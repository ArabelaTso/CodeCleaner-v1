{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/__config__.py", "fn_id": "", "content": "class DisplayModes(Enum):\n    stdout = \"stdout\"\n    dicts = \"dicts\"\n", "class_fn": true, "question_id": "numpy/numpy.__config__/DisplayModes", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_typing/_array_like.py", "fn_id": "", "content": "@runtime_checkable\nclass _SupportsArrayFunc(Protocol):\n    \"\"\"A protocol class representing `~class.__array_function__`.\"\"\"\n    def __array_function__(\n        self,\n        func: Callable[..., Any],\n        types: Collection[type[Any]],\n        args: tuple[Any, ...],\n        kwargs: dict[str, Any],\n    ) -> object: ...\n", "class_fn": true, "question_id": "numpy/numpy._typing._array_like/_SupportsArrayFunc", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_utils/_pep440.py", "fn_id": "", "content": "class LegacyVersion(_BaseVersion):\n\n    def __init__(self, version):\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)\n\n    def __str__(self):\n        return self._version\n\n    def __repr__(self):\n        return \"<LegacyVersion({0})>\".format(repr(str(self)))\n\n    @property\n    def public(self):\n        return self._version\n\n    @property\n    def base_version(self):\n        return self._version\n\n    @property\n    def local(self):\n        return None\n\n    @property\n    def is_prerelease(self):\n        return False\n\n    @property\n    def is_postrelease(self):\n        return False\n", "class_fn": true, "question_id": "numpy/numpy._utils._pep440/LegacyVersion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/array_api/_typing.py", "fn_id": "", "content": "class NestedSequence(Protocol[_T_co]):\n    def __getitem__(self, key: int, /) -> _T_co | NestedSequence[_T_co]: ...\n    def __len__(self, /) -> int: ...\n", "class_fn": true, "question_id": "numpy/numpy.array_api._typing/NestedSequence", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/_exceptions.py", "fn_id": "", "content": "class UFuncTypeError(TypeError):\n    \"\"\" Base class for all ufunc exceptions \"\"\"\n    def __init__(self, ufunc):\n        self.ufunc = ufunc\n", "class_fn": true, "question_id": "numpy/numpy.core._exceptions/UFuncTypeError", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/_exceptions.py", "fn_id": "", "content": "@_display_as_base\nclass _UFuncCastingError(UFuncTypeError):\n    def __init__(self, ufunc, casting, from_, to):\n        super().__init__(ufunc)\n        self.casting = casting\n        self.from_ = from_\n        self.to = to\n", "class_fn": true, "question_id": "numpy/numpy.core._exceptions/_UFuncCastingError", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/_exceptions.py", "fn_id": "", "content": "@_display_as_base\nclass _UFuncOutputCastingError(_UFuncCastingError):\n    \"\"\" Thrown when a ufunc output cannot be casted \"\"\"\n    def __init__(self, ufunc, casting, from_, to, i):\n        super().__init__(ufunc, casting, from_, to)\n        self.out_i = i\n\n    def __str__(self):\n        # only show the number if more than one output exists\n        i_str = \"{} \".format(self.out_i) if self.ufunc.nout != 1 else \"\"\n        return (\n            \"Cannot cast ufunc {!r} output {}from {!r} to {!r} with casting \"\n            \"rule {!r}\"\n        ).format(\n            self.ufunc.__name__, i_str, self.from_, self.to, self.casting\n        )\n", "class_fn": true, "question_id": "numpy/numpy.core._exceptions/_UFuncOutputCastingError", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/_internal.py", "fn_id": "", "content": "class dummy_ctype:\n    def __init__(self, cls):\n        self._cls = cls\n    def __mul__(self, other):\n        return self\n    def __call__(self, *other):\n        return self._cls(other)\n    def __eq__(self, other):\n        return self._cls == other._cls\n    def __ne__(self, other):\n        return self._cls != other._cls\n", "class_fn": true, "question_id": "numpy/numpy.core._internal/dummy_ctype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/arrayprint.py", "fn_id": "", "content": "class DatetimeFormat(_TimelikeFormat):\n    def __init__(self, x, unit=None, timezone=None, casting='same_kind',\n                 legacy=False):\n        # Get the unit from the dtype\n        if unit is None:\n            if x.dtype.kind == 'M':\n                unit = datetime_data(x.dtype)[0]\n            else:\n                unit = 's'\n\n        if timezone is None:\n            timezone = 'naive'\n        self.timezone = timezone\n        self.unit = unit\n        self.casting = casting\n        self.legacy = legacy\n\n        # must be called after the above are configured\n        super().__init__(x)\n\n    def __call__(self, x):\n        if self.legacy <= 113:\n            return self._format_non_nat(x)\n        return super().__call__(x)\n\n    def _format_non_nat(self, x):\n        return \"'%s'\" % datetime_as_string(x,\n                                    unit=self.unit,\n                                    timezone=self.timezone,\n                                    casting=self.casting)\n", "class_fn": true, "question_id": "numpy/numpy.core.arrayprint/DatetimeFormat", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/arrayprint.py", "fn_id": "", "content": "class SubArrayFormat:\n    def __init__(self, format_function, **options):\n        self.format_function = format_function\n        self.threshold = options['threshold']\n        self.edge_items = options['edgeitems']\n\n    def __call__(self, a):\n        self.summary_insert = \"...\" if a.size > self.threshold else \"\"\n        return self.format_array(a)\n\n    def format_array(self, a):\n        if np.ndim(a) == 0:\n            return self.format_function(a)\n\n        if self.summary_insert and a.shape[0] > 2*self.edge_items:\n            formatted = (\n                [self.format_array(a_) for a_ in a[:self.edge_items]]\n                + [self.summary_insert]\n                + [self.format_array(a_) for a_ in a[-self.edge_items:]]\n            )\n        else:\n            formatted = [self.format_array(a_) for a_ in a]\n\n        return \"[\" + \", \".join(formatted) + \"]\"\n", "class_fn": true, "question_id": "numpy/numpy.core.arrayprint/SubArrayFormat", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/tests/test_machar.py", "fn_id": "", "content": "class TestMachAr:\n    def _run_machar_highprec(self):\n        # Instantiate MachAr instance with high enough precision to cause\n        # underflow\n        try:\n            hiprec = ntypes.float96\n            MachAr(lambda v: array(v, hiprec))\n        except AttributeError:\n            # Fixme, this needs to raise a 'skip' exception.\n            \"Skipping test: no ntypes.float96 available on this platform.\"\n\n    def test_underlow(self):\n        # Regression test for #759:\n        # instantiating MachAr for dtype = np.float96 raises spurious warning.\n        with errstate(all='raise'):\n            try:\n                self._run_machar_highprec()\n            except FloatingPointError as e:\n                msg = \"Caught %s exception, should not have been raised.\" % e\n                raise AssertionError(msg)\n", "class_fn": true, "question_id": "numpy/numpy.core.tests.test_machar/TestMachAr", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ctypeslib.py", "fn_id": "", "content": "class _ndptr(_ndptr_base):\n    @classmethod\n    def from_param(cls, obj):\n        if not isinstance(obj, ndarray):\n            raise TypeError(\"argument must be an ndarray\")\n        if cls._dtype_ is not None \\\n               and obj.dtype != cls._dtype_:\n            raise TypeError(\"array must have data type %s\" % cls._dtype_)\n        if cls._ndim_ is not None \\\n               and obj.ndim != cls._ndim_:\n            raise TypeError(\"array must have %d dimension(s)\" % cls._ndim_)\n        if cls._shape_ is not None \\\n               and obj.shape != cls._shape_:\n            raise TypeError(\"array must have shape %s\" % str(cls._shape_))\n        if cls._flags_ is not None \\\n               and ((obj.flags.num & cls._flags_) != cls._flags_):\n            raise TypeError(\"array must have flags %s\" %\n                    _flags_fromnum(cls._flags_))\n        return obj.ctypes\n", "class_fn": true, "question_id": "numpy/numpy.ctypeslib/_ndptr", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/_shell_utils.py", "fn_id": "", "content": "class WindowsParser:\n    \"\"\"\n    The parsing behavior used by `subprocess.call(\"string\")` on Windows, which\n    matches the Microsoft C/C++ runtime.\n\n    Note that this is _not_ the behavior of cmd.\n    \"\"\"\n    @staticmethod\n    def join(argv):\n        # note that list2cmdline is specific to the windows syntax\n        return subprocess.list2cmdline(argv)\n\n    @staticmethod\n    def split(cmd):\n        import ctypes  # guarded import for systems without ctypes\n        try:\n            ctypes.windll\n        except AttributeError:\n            raise NotImplementedError\n\n        # Windows has special parsing rules for the executable (no quotes),\n        # that we do not care about - insert a dummy element\n        if not cmd:\n            return []\n        cmd = 'dummy ' + cmd\n\n        CommandLineToArgvW = ctypes.windll.shell32.CommandLineToArgvW\n        CommandLineToArgvW.restype = ctypes.POINTER(ctypes.c_wchar_p)\n        CommandLineToArgvW.argtypes = (ctypes.c_wchar_p, ctypes.POINTER(ctypes.c_int))\n\n        nargs = ctypes.c_int()\n        lpargs = CommandLineToArgvW(cmd, ctypes.byref(nargs))\n        args = [lpargs[i] for i in range(nargs.value)]\n        assert not ctypes.windll.kernel32.LocalFree(lpargs)\n\n        # strip the element we inserted\n        assert args[0] == \"dummy\"\n        return args[1:]\n", "class_fn": true, "question_id": "numpy/numpy.distutils._shell_utils/WindowsParser", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/build_py.py", "fn_id": "", "content": "class build_py(old_build_py):\n\n    def run(self):\n        build_src = self.get_finalized_command('build_src')\n        if build_src.py_modules_dict and self.packages is None:\n            self.packages = list(build_src.py_modules_dict.keys ())\n        old_build_py.run(self)\n\n    def find_package_modules(self, package, package_dir):\n        modules = old_build_py.find_package_modules(self, package, package_dir)\n\n        # Find build_src generated *.py files.\n        build_src = self.get_finalized_command('build_src')\n        modules += build_src.py_modules_dict.get(package, [])\n\n        return modules\n\n    def find_modules(self):\n        old_py_modules = self.py_modules[:]\n        new_py_modules = [_m for _m in self.py_modules if is_string(_m)]\n        self.py_modules[:] = new_py_modules\n        modules = old_build_py.find_modules(self)\n        self.py_modules[:] = old_py_modules\n\n        return modules\n\n    # XXX: Fix find_source_files for item in py_modules such that item is 3-tuple\n    # and item[2] is source file.\n", "class_fn": true, "question_id": "numpy/numpy.distutils.command.build_py/build_py", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/config_compiler.py", "fn_id": "", "content": "class config_cc(Command):\n    \"\"\" Distutils command to hold user specified options\n    to C/C++ compilers.\n    \"\"\"\n\n    description = \"specify C/C++ compiler information\"\n\n    user_options = [\n        ('compiler=', None, \"specify C/C++ compiler type\"),\n        ]\n\n    def initialize_options(self):\n        self.compiler = None\n\n    def finalize_options(self):\n        log.info('unifing config_cc, config, build_clib, build_ext, build commands --compiler options')\n        build_clib = self.get_finalized_command('build_clib')\n        build_ext = self.get_finalized_command('build_ext')\n        config = self.get_finalized_command('config')\n        build = self.get_finalized_command('build')\n        cmd_list = [self, config, build_clib, build_ext, build]\n        for a in ['compiler']:\n            l = []\n            for c in cmd_list:\n                v = getattr(c, a)\n                if v is not None:\n                    if not isinstance(v, str): v = v.compiler_type\n                    if v not in l: l.append(v)\n            if not l: v1 = None\n            else: v1 = l[0]\n            if len(l)>1:\n                log.warn('  commands have different --%s options: %s'\\\n                         ', using first in list as default' % (a, l))\n            if v1:\n                for c in cmd_list:\n                    if getattr(c, a) is None: setattr(c, a, v1)\n        return\n\n    def run(self):\n        # Do nothing.\n        return\n", "class_fn": true, "question_id": "numpy/numpy.distutils.command.config_compiler/config_cc", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/install_clib.py", "fn_id": "", "content": "class install_clib(Command):\n    description = \"Command to install installable C libraries\"\n\n    user_options = []\n\n    def initialize_options(self):\n        self.install_dir = None\n        self.outfiles = []\n\n    def finalize_options(self):\n        self.set_undefined_options('install', ('install_lib', 'install_dir'))\n\n    def run (self):\n        build_clib_cmd = get_cmd(\"build_clib\")\n        if not build_clib_cmd.build_clib:\n            # can happen if the user specified `--skip-build`\n            build_clib_cmd.finalize_options()\n        build_dir = build_clib_cmd.build_clib\n\n        # We need the compiler to get the library name -> filename association\n        if not build_clib_cmd.compiler:\n            compiler = new_compiler(compiler=None)\n            compiler.customize(self.distribution)\n        else:\n            compiler = build_clib_cmd.compiler\n\n        for l in self.distribution.installed_libraries:\n            target_dir = os.path.join(self.install_dir, l.target_dir)\n            name = compiler.library_filename(l.name)\n            source = os.path.join(build_dir, name)\n            self.mkpath(target_dir)\n            self.outfiles.append(self.copy_file(source, target_dir)[0])\n\n    def get_outputs(self):\n        return self.outfiles\n", "class_fn": true, "question_id": "numpy/numpy.distutils.command.install_clib/install_clib", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/sdist.py", "fn_id": "", "content": "class sdist(old_sdist):\n\n    def add_defaults (self):\n        old_sdist.add_defaults(self)\n\n        dist = self.distribution\n\n        if dist.has_data_files():\n            for data in dist.data_files:\n                self.filelist.extend(get_data_files(data))\n\n        if dist.has_headers():\n            headers = []\n            for h in dist.headers:\n                if isinstance(h, str): headers.append(h)\n                else: headers.append(h[1])\n            self.filelist.extend(headers)\n\n        return\n", "class_fn": true, "question_id": "numpy/numpy.distutils.command.sdist/sdist", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/arm.py", "fn_id": "", "content": "class ArmFlangCompiler(FCompiler):\n    compiler_type = 'arm'\n    description = 'Arm Compiler'\n    version_pattern = r'\\s*Arm.*version (?P<version>[\\d.-]+).*'\n\n    ar_exe = 'lib.exe'\n    possible_executables = ['armflang']\n\n    executables = {\n        'version_cmd': [\"\", \"--version\"],\n        'compiler_f77': [\"armflang\", \"-fPIC\"],\n        'compiler_fix': [\"armflang\", \"-fPIC\", \"-ffixed-form\"],\n        'compiler_f90': [\"armflang\", \"-fPIC\"],\n        'linker_so': [\"armflang\", \"-fPIC\", \"-shared\"],\n        'archiver': [\"ar\", \"-cr\"],\n        'ranlib':  None\n    }\n\n    pic_flags = [\"-fPIC\", \"-DPIC\"]\n    c_compiler = 'arm'\n    module_dir_switch = '-module '  # Don't remove ending space!\n\n    def get_libraries(self):\n        opt = FCompiler.get_libraries(self)\n        opt.extend(['flang', 'flangrti', 'ompstub'])\n        return opt\n\n    @functools.lru_cache(maxsize=128)\n    def get_library_dirs(self):\n        \"\"\"List of compiler library directories.\"\"\"\n        opt = FCompiler.get_library_dirs(self)\n        flang_dir = dirname(self.executables['compiler_f77'][0])\n        opt.append(normpath(join(flang_dir, '..', 'lib')))\n\n        return opt\n\n    def get_flags(self):\n        return []\n\n    def get_flags_free(self):\n        return []\n\n    def get_flags_debug(self):\n        return ['-g']\n\n    def get_flags_opt(self):\n        return ['-O3']\n\n    def get_flags_arch(self):\n        return []\n\n    def runtime_library_dir_option(self, dir):\n        return '-Wl,-rpath=%s' % dir\n", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.arm/ArmFlangCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/fujitsu.py", "fn_id": "", "content": "class FujitsuFCompiler(FCompiler):\n    compiler_type = 'fujitsu'\n    description = 'Fujitsu Fortran Compiler'\n\n    possible_executables = ['frt']\n    version_pattern = r'frt \\(FRT\\) (?P<version>[a-z\\d.]+)'\n    # $ frt --version\n    # frt (FRT) x.x.x yyyymmdd\n\n    executables = {\n        'version_cmd'  : [\"<F77>\", \"--version\"],\n        'compiler_f77' : [\"frt\", \"-Fixed\"],\n        'compiler_fix' : [\"frt\", \"-Fixed\"],\n        'compiler_f90' : [\"frt\"],\n        'linker_so'    : [\"frt\", \"-shared\"],\n        'archiver'     : [\"ar\", \"-cr\"],\n        'ranlib'       : [\"ranlib\"]\n        }\n    pic_flags = ['-KPIC']\n    module_dir_switch = '-M'\n    module_include_switch = '-I'\n\n    def get_flags_opt(self):\n        return ['-O3']\n    def get_flags_debug(self):\n        return ['-g']\n    def runtime_library_dir_option(self, dir):\n        return f'-Wl,-rpath={dir}'\n    def get_libraries(self):\n        return ['fj90f', 'fj90i', 'fjsrcinfo']\n", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.fujitsu/FujitsuFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/intel.py", "fn_id": "", "content": "class BaseIntelFCompiler(FCompiler):\n    def update_executables(self):\n        f = dummy_fortran_file()\n        self.executables['version_cmd'] = ['<F77>', '-FI', '-V', '-c',\n                                           f + '.f', '-o', f + '.o']\n\n    def runtime_library_dir_option(self, dir):\n        # TODO: could use -Xlinker here, if it's supported\n        assert \",\" not in dir\n\n        return '-Wl,-rpath=%s' % dir\n", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.intel/BaseIntelFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/intel.py", "fn_id": "", "content": "class IntelFCompiler(BaseIntelFCompiler):\n\n    compiler_type = 'intel'\n    compiler_aliases = ('ifort',)\n    description = 'Intel Fortran Compiler for 32-bit apps'\n    version_match = intel_version_match('32-bit|IA-32')\n\n    possible_executables = ['ifort', 'ifc']\n\n    executables = {\n        'version_cmd'  : None,          # set by update_executables\n        'compiler_f77' : [None, \"-72\", \"-w90\", \"-w95\"],\n        'compiler_f90' : [None],\n        'compiler_fix' : [None, \"-FI\"],\n        'linker_so'    : [\"<F90>\", \"-shared\"],\n        'archiver'     : [\"ar\", \"-cr\"],\n        'ranlib'       : [\"ranlib\"]\n        }\n\n    pic_flags = ['-fPIC']\n    module_dir_switch = '-module '  # Don't remove ending space!\n    module_include_switch = '-I'\n\n    def get_flags_free(self):\n        return ['-FR']\n\n    def get_flags(self):\n        return ['-fPIC']\n\n    def get_flags_opt(self):  # Scipy test failures with -O2\n        v = self.get_version()\n        mpopt = 'openmp' if v and v < '15' else 'qopenmp'\n        return ['-fp-model', 'strict', '-O1',\n                '-assume', 'minus0', '-{}'.format(mpopt)]\n\n    def get_flags_arch(self):\n        return []\n\n    def get_flags_linker_so(self):\n        opt = FCompiler.get_flags_linker_so(self)\n        v = self.get_version()\n        if v and v >= '8.0':\n            opt.append('-nofor_main')\n        if sys.platform == 'darwin':\n            # Here, it's -dynamiclib\n            try:\n                idx = opt.index('-shared')\n                opt.remove('-shared')\n            except ValueError:\n                idx = 0\n            opt[idx:idx] = ['-dynamiclib', '-Wl,-undefined,dynamic_lookup']\n        return opt\n", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.intel/IntelFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/intel.py", "fn_id": "", "content": "class IntelVisualFCompiler(BaseIntelFCompiler):\n    compiler_type = 'intelv'\n    description = 'Intel Visual Fortran Compiler for 32-bit apps'\n    version_match = intel_version_match('32-bit|IA-32')\n\n    def update_executables(self):\n        f = dummy_fortran_file()\n        self.executables['version_cmd'] = ['<F77>', '/FI', '/c',\n                                           f + '.f', '/o', f + '.o']\n\n    ar_exe = 'lib.exe'\n    possible_executables = ['ifort', 'ifl']\n\n    executables = {\n        'version_cmd'  : None,\n        'compiler_f77' : [None],\n        'compiler_fix' : [None],\n        'compiler_f90' : [None],\n        'linker_so'    : [None],\n        'archiver'     : [ar_exe, \"/verbose\", \"/OUT:\"],\n        'ranlib'       : None\n        }\n\n    compile_switch = '/c '\n    object_switch = '/Fo'     # No space after /Fo!\n    library_switch = '/OUT:'  # No space after /OUT:!\n    module_dir_switch = '/module:'  # No space after /module:\n    module_include_switch = '/I'\n\n    def get_flags(self):\n        opt = ['/nologo', '/MD', '/nbs', '/names:lowercase', \n               '/assume:underscore', '/fpp']\n        return opt\n\n    def get_flags_free(self):\n        return []\n\n    def get_flags_debug(self):\n        return ['/4Yb', '/d2']\n\n    def get_flags_opt(self):\n        return ['/O1', '/assume:minus0']  # Scipy test failures with /O2\n\n    def get_flags_arch(self):\n        return [\"/arch:IA32\", \"/QaxSSE3\"]\n\n    def runtime_library_dir_option(self, dir):\n        raise NotImplementedError\n", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.intel/IntelVisualFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/nag.py", "fn_id": "", "content": "class BaseNAGFCompiler(FCompiler):\n    version_pattern = r'NAG.* Release (?P<version>[^(\\s]*)'\n\n    def version_match(self, version_string):\n        m = re.search(self.version_pattern, version_string)\n        if m:\n            return m.group('version')\n        else:\n            return None\n\n    def get_flags_linker_so(self):\n        return [\"-Wl,-shared\"]\n    def get_flags_opt(self):\n        return ['-O4']\n    def get_flags_arch(self):\n        return []\n", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.nag/BaseNAGFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/none.py", "fn_id": "", "content": "class NoneFCompiler(FCompiler):\n\n    compiler_type = 'none'\n    description = 'Fake Fortran compiler'\n\n    executables = {'compiler_f77': None,\n                   'compiler_f90': None,\n                   'compiler_fix': None,\n                   'linker_so': None,\n                   'linker_exe': None,\n                   'archiver': None,\n                   'ranlib': None,\n                   'version_cmd': None,\n                   }\n\n    def find_executables(self):\n        pass\n", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.none/NoneFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/pg.py", "fn_id": "", "content": "class PGroupFCompiler(FCompiler):\n\n    compiler_type = 'pg'\n    description = 'Portland Group Fortran Compiler'\n    version_pattern = r'\\s*pg(f77|f90|hpf|fortran) (?P<version>[\\d.-]+).*'\n\n    if platform == 'darwin':\n        executables = {\n            'version_cmd': [\"<F77>\", \"-V\"],\n            'compiler_f77': [\"pgfortran\", \"-dynamiclib\"],\n            'compiler_fix': [\"pgfortran\", \"-Mfixed\", \"-dynamiclib\"],\n            'compiler_f90': [\"pgfortran\", \"-dynamiclib\"],\n            'linker_so': [\"libtool\"],\n            'archiver': [\"ar\", \"-cr\"],\n            'ranlib': [\"ranlib\"]\n        }\n        pic_flags = ['']\n    else:\n        executables = {\n            'version_cmd': [\"<F77>\", \"-V\"],\n            'compiler_f77': [\"pgfortran\"],\n            'compiler_fix': [\"pgfortran\", \"-Mfixed\"],\n            'compiler_f90': [\"pgfortran\"],\n            'linker_so': [\"<F90>\"],\n            'archiver': [\"ar\", \"-cr\"],\n            'ranlib': [\"ranlib\"]\n        }\n        pic_flags = ['-fpic']\n\n    module_dir_switch = '-module '\n    module_include_switch = '-I'\n\n    def get_flags(self):\n        opt = ['-Minform=inform', '-Mnosecond_underscore']\n        return self.pic_flags + opt\n\n    def get_flags_opt(self):\n        return ['-fast']\n\n    def get_flags_debug(self):\n        return ['-g']\n\n    if platform == 'darwin':\n        def get_flags_linker_so(self):\n            return [\"-dynamic\", '-undefined', 'dynamic_lookup']\n\n    else:\n        def get_flags_linker_so(self):\n            return [\"-shared\", '-fpic']\n\n    def runtime_library_dir_option(self, dir):\n        return '-R%s' % dir\n", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.pg/PGroupFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/vast.py", "fn_id": "", "content": "class VastFCompiler(GnuFCompiler):\n    compiler_type = 'vast'\n    compiler_aliases = ()\n    description = 'Pacific-Sierra Research Fortran 90 Compiler'\n    version_pattern = (r'\\s*Pacific-Sierra Research vf90 '\n                       r'(Personal|Professional)\\s+(?P<version>[^\\s]*)')\n\n    # VAST f90 does not support -o with -c. So, object files are created\n    # to the current directory and then moved to build directory\n    object_switch = ' && function _mvfile { mv -v `basename $1` $1 ; } && _mvfile '\n\n    executables = {\n        'version_cmd'  : [\"vf90\", \"-v\"],\n        'compiler_f77' : [\"g77\"],\n        'compiler_fix' : [\"f90\", \"-Wv,-ya\"],\n        'compiler_f90' : [\"f90\"],\n        'linker_so'    : [\"<F90>\"],\n        'archiver'     : [\"ar\", \"-cr\"],\n        'ranlib'       : [\"ranlib\"]\n        }\n    module_dir_switch = None  #XXX Fix me\n    module_include_switch = None #XXX Fix me\n\n    def find_executables(self):\n        pass\n\n    def get_version_cmd(self):\n        f90 = self.compiler_f90[0]\n        d, b = os.path.split(f90)\n        vf90 = os.path.join(d, 'v'+b)\n        return vf90\n\n    def get_flags_arch(self):\n        vast_version = self.get_version()\n        gnu = GnuFCompiler()\n        gnu.customize(None)\n        self.version = gnu.get_version()\n        opt = GnuFCompiler.get_flags_arch(self)\n        self.version = vast_version\n        return opt\n", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.vast/VastFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/intelccompiler.py", "fn_id": "", "content": "class IntelEM64TCCompiler(UnixCCompiler):\n    \"\"\"\n    A modified Intel x86_64 compiler compatible with a 64bit GCC-built Python.\n    \"\"\"\n    compiler_type = 'intelem'\n    cc_exe = 'icc -m64'\n    cc_args = '-fPIC'\n\n    def __init__(self, verbose=0, dry_run=0, force=0):\n        UnixCCompiler.__init__(self, verbose, dry_run, force)\n\n        v = self.get_version()\n        mpopt = 'openmp' if v and v < '15' else 'qopenmp'\n        self.cc_exe = ('icc -std=c99 -m64 -fPIC -fp-model strict -O3 '\n                       '-fomit-frame-pointer -{}').format(mpopt)\n        compiler = self.cc_exe\n\n        if platform.system() == 'Darwin':\n            shared_flag = '-Wl,-undefined,dynamic_lookup'\n        else:\n            shared_flag = '-shared'\n        self.set_executables(compiler=compiler,\n                             compiler_so=compiler,\n                             compiler_cxx=compiler,\n                             archiver='xiar' + ' cru',\n                             linker_exe=compiler + ' -shared-intel',\n                             linker_so=compiler + ' ' + shared_flag +\n                             ' -shared-intel')\n", "class_fn": true, "question_id": "numpy/numpy.distutils.intelccompiler/IntelEM64TCCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/misc_util.py", "fn_id": "", "content": "class InstallableLib:\n    \"\"\"\n    Container to hold information on an installable library.\n\n    Parameters\n    ----------\n    name : str\n        Name of the installed library.\n    build_info : dict\n        Dictionary holding build information.\n    target_dir : str\n        Absolute path specifying where to install the library.\n\n    See Also\n    --------\n    Configuration.add_installed_library\n\n    Notes\n    -----\n    The three parameters are stored as attributes with the same names.\n\n    \"\"\"\n    def __init__(self, name, build_info, target_dir):\n        self.name = name\n        self.build_info = build_info\n        self.target_dir = target_dir\n", "class_fn": true, "question_id": "numpy/numpy.distutils.misc_util/InstallableLib", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/npy_pkg_config.py", "fn_id": "", "content": "class PkgNotFound(OSError):\n    \"\"\"Exception raised when a package can not be located.\"\"\"\n    def __init__(self, msg):\n        self.msg = msg\n\n    def __str__(self):\n        return self.msg\n", "class_fn": true, "question_id": "numpy/numpy.distutils.npy_pkg_config/PkgNotFound", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/pathccompiler.py", "fn_id": "", "content": "class PathScaleCCompiler(UnixCCompiler):\n\n    \"\"\"\n    PathScale compiler compatible with an gcc built Python.\n    \"\"\"\n\n    compiler_type = 'pathcc'\n    cc_exe = 'pathcc'\n    cxx_exe = 'pathCC'\n\n    def __init__ (self, verbose=0, dry_run=0, force=0):\n        UnixCCompiler.__init__ (self, verbose, dry_run, force)\n        cc_compiler = self.cc_exe\n        cxx_compiler = self.cxx_exe\n        self.set_executables(compiler=cc_compiler,\n                             compiler_so=cc_compiler,\n                             compiler_cxx=cxx_compiler,\n                             linker_exe=cc_compiler,\n                             linker_so=cc_compiler + ' -shared')\n", "class_fn": true, "question_id": "numpy/numpy.distutils.pathccompiler/PathScaleCCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class accelerate_lapack_info(accelerate_info):\n    def _calc_info(self):\n        return super()._calc_info()\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/accelerate_lapack_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class armpl_info(system_info):\n    section = 'armpl'\n    dir_env_var = 'ARMPL_DIR'\n    _lib_armpl = ['armpl_lp64_mp']\n\n    def calc_info(self):\n        lib_dirs = self.get_lib_dirs()\n        incl_dirs = self.get_include_dirs()\n        armpl_libs = self.get_libs('armpl_libs', self._lib_armpl)\n        info = self.check_libs2(lib_dirs, armpl_libs)\n        if info is None:\n            return\n        dict_append(info,\n                    define_macros=[('SCIPY_MKL_H', None),\n                                   ('HAVE_CBLAS', None)],\n                    include_dirs=incl_dirs)\n        self.set_info(**info)\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/armpl_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class atlas_3_10_info(atlas_info):\n    _lib_names = ['satlas']\n    _lib_atlas = _lib_names\n    _lib_lapack = _lib_names\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/atlas_3_10_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class atlas_threads_info(atlas_info):\n    dir_env_var = ['PTATLAS', 'ATLAS']\n    _lib_names = ['ptf77blas', 'ptcblas']\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/atlas_threads_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class blas_ilp64_plain_opt_info(blas_ilp64_opt_info):\n    symbol_prefix = ''\n    symbol_suffix = ''\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/blas_ilp64_plain_opt_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class cblas_info(system_info):\n    section = 'cblas'\n    dir_env_var = 'CBLAS'\n    # No default as it's used only in blas_info\n    _lib_names = []\n    notfounderror = BlasNotFoundError\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/cblas_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class djbfft_info(system_info):\n    section = 'djbfft'\n    dir_env_var = 'DJBFFT'\n    notfounderror = DJBFFTNotFoundError\n\n    def get_paths(self, section, key):\n        pre_dirs = system_info.get_paths(self, section, key)\n        dirs = []\n        for d in pre_dirs:\n            dirs.extend(self.combine_paths(d, ['djbfft']) + [d])\n        return [d for d in dirs if os.path.isdir(d)]\n\n    def calc_info(self):\n        lib_dirs = self.get_lib_dirs()\n        incl_dirs = self.get_include_dirs()\n        info = None\n        for d in lib_dirs:\n            p = self.combine_paths(d, ['djbfft.a'])\n            if p:\n                info = {'extra_objects': p}\n                break\n            p = self.combine_paths(d, ['libdjbfft.a', 'libdjbfft' + so_ext])\n            if p:\n                info = {'libraries': ['djbfft'], 'library_dirs': [d]}\n                break\n        if info is None:\n            return\n        for d in incl_dirs:\n            if len(self.combine_paths(d, ['fftc8.h', 'fftfreq.h'])) == 2:\n                dict_append(info, include_dirs=[d],\n                            define_macros=[('SCIPY_DJBFFT_H', None)])\n                self.set_info(**info)\n                return\n        return\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/djbfft_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class fftw2_info(fftw_info):\n    #variables to override\n    section = 'fftw'\n    dir_env_var = 'FFTW'\n    notfounderror = FFTWNotFoundError\n    ver_info = [{'name':'fftw2',\n                    'libs':['rfftw', 'fftw'],\n                    'includes':['fftw.h', 'rfftw.h'],\n                    'macros':[('SCIPY_FFTW_H', None)]}\n                  ]\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/fftw2_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class fftw_info(system_info):\n    #variables to override\n    section = 'fftw'\n    dir_env_var = 'FFTW'\n    notfounderror = FFTWNotFoundError\n    ver_info = [{'name':'fftw3',\n                    'libs':['fftw3'],\n                    'includes':['fftw3.h'],\n                    'macros':[('SCIPY_FFTW3_H', None)]},\n                  {'name':'fftw2',\n                    'libs':['rfftw', 'fftw'],\n                    'includes':['fftw.h', 'rfftw.h'],\n                    'macros':[('SCIPY_FFTW_H', None)]}]\n\n    def calc_ver_info(self, ver_param):\n        \"\"\"Returns True on successful version detection, else False\"\"\"\n        lib_dirs = self.get_lib_dirs()\n        incl_dirs = self.get_include_dirs()\n\n        opt = self.get_option_single(self.section + '_libs', 'libraries')\n        libs = self.get_libs(opt, ver_param['libs'])\n        info = self.check_libs(lib_dirs, libs)\n        if info is not None:\n            flag = 0\n            for d in incl_dirs:\n                if len(self.combine_paths(d, ver_param['includes'])) \\\n                   == len(ver_param['includes']):\n                    dict_append(info, include_dirs=[d])\n                    flag = 1\n                    break\n            if flag:\n                dict_append(info, define_macros=ver_param['macros'])\n            else:\n                info = None\n        if info is not None:\n            self.set_info(**info)\n            return True\n        else:\n            log.info('  %s not found' % (ver_param['name']))\n            return False\n\n    def calc_info(self):\n        for i in self.ver_info:\n            if self.calc_ver_info(i):\n                break\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/fftw_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class gdk_2_info(_pkg_config_info):\n    section = 'gdk_2'\n    append_config_exe = 'gdk-2.0'\n    version_macro_name = 'GDK_VERSION'\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/gdk_2_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class gdk_pixbuf_xlib_2_info(_pkg_config_info):\n    section = 'gdk_pixbuf_xlib_2'\n    append_config_exe = 'gdk-pixbuf-xlib-2.0'\n    version_macro_name = 'GDK_PIXBUF_XLIB_VERSION'\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/gdk_pixbuf_xlib_2_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class gtkp_x11_2_info(_pkg_config_info):\n    section = 'gtkp_x11_2'\n    append_config_exe = 'gtk+-x11-2.0'\n    version_macro_name = 'GTK_X11_VERSION'\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/gtkp_x11_2_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class lapack_ilp64_plain_opt_info(lapack_ilp64_opt_info):\n    # Same as lapack_ilp64_opt_info, but fix symbol names\n    symbol_prefix = ''\n    symbol_suffix = ''\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/lapack_ilp64_plain_opt_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class numerix_info(system_info):\n    section = 'numerix'\n\n    def calc_info(self):\n        which = None, None\n        if os.getenv(\"NUMERIX\"):\n            which = os.getenv(\"NUMERIX\"), \"environment var\"\n        # If all the above fail, default to numpy.\n        if which[0] is None:\n            which = \"numpy\", \"defaulted\"\n            try:\n                import numpy  # noqa: F401\n                which = \"numpy\", \"defaulted\"\n            except ImportError as e:\n                msg1 = str(e)\n                try:\n                    import Numeric  # noqa: F401\n                    which = \"numeric\", \"defaulted\"\n                except ImportError as e:\n                    msg2 = str(e)\n                    try:\n                        import numarray  # noqa: F401\n                        which = \"numarray\", \"defaulted\"\n                    except ImportError as e:\n                        msg3 = str(e)\n                        log.info(msg1)\n                        log.info(msg2)\n                        log.info(msg3)\n        which = which[0].strip().lower(), which[1]\n        if which[0] not in [\"numeric\", \"numarray\", \"numpy\"]:\n            raise ValueError(\"numerix selector must be either 'Numeric' \"\n                             \"or 'numarray' or 'numpy' but the value obtained\"\n                             \" from the %s was '%s'.\" % (which[1], which[0]))\n        os.environ['NUMERIX'] = which[0]\n        self.set_info(**get_info(which[0]))\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/numerix_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class openblas_ilp64_info(openblas_info):\n    section = 'openblas_ilp64'\n    dir_env_var = 'OPENBLAS_ILP64'\n    _lib_names = ['openblas64']\n    _require_symbols = ['dgemm_', 'cblas_dgemm']\n    notfounderror = BlasILP64NotFoundError\n\n    def _calc_info(self):\n        info = super()._calc_info()\n        if info is not None:\n            info['define_macros'] += [('HAVE_BLAS_ILP64', None)]\n        return info\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/openblas_ilp64_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class sfftw_threads_info(fftw_info):\n    section = 'fftw'\n    dir_env_var = 'FFTW'\n    ver_info = [{'name':'sfftw threads',\n                    'libs':['srfftw_threads', 'sfftw_threads'],\n                    'includes':['sfftw_threads.h', 'srfftw_threads.h'],\n                    'macros':[('SCIPY_SFFTW_THREADS_H', None)]}]\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/sfftw_threads_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class wx_info(_pkg_config_info):\n    section = 'wx'\n    config_env_var = 'WX_CONFIG'\n    default_config_exe = 'wx-config'\n    append_config_exe = ''\n    version_macro_name = 'WX_VERSION'\n    release_macro_name = 'WX_RELEASE'\n    version_flag = '--version'\n    cflags_flag = '--cxxflags'\n", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/wx_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/tests/test_ccompiler_opt.py", "fn_id": "", "content": "class FakeCCompilerOpt(CCompilerOpt):\n    fake_info = \"\"\n    def __init__(self, trap_files=\"\", trap_flags=\"\", *args, **kwargs):\n        self.fake_trap_files = trap_files\n        self.fake_trap_flags = trap_flags\n        CCompilerOpt.__init__(self, None, **kwargs)\n\n    def __repr__(self):\n        return textwrap.dedent(\"\"\"\\\n            <<<<\n            march    : {}\n            compiler : {}\n            ----------------\n            {}\n            >>>>\n        \"\"\").format(self.cc_march, self.cc_name, self.report())\n\n    def dist_compile(self, sources, flags, **kwargs):\n        assert(isinstance(sources, list))\n        assert(isinstance(flags, list))\n        if self.fake_trap_files:\n            for src in sources:\n                if re.match(self.fake_trap_files, src):\n                    self.dist_error(\"source is trapped by a fake interface\")\n        if self.fake_trap_flags:\n            for f in flags:\n                if re.match(self.fake_trap_flags, f):\n                    self.dist_error(\"flag is trapped by a fake interface\")\n        # fake objects\n        return zip(sources, [' '.join(flags)] * len(sources))\n\n    def dist_info(self):\n        return FakeCCompilerOpt.fake_info\n\n    @staticmethod\n    def dist_log(*args, stderr=False):\n        pass\n", "class_fn": true, "question_id": "numpy/numpy.distutils.tests.test_ccompiler_opt/FakeCCompilerOpt", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/tests/test_fcompiler_gnu.py", "fn_id": "", "content": "class TestG77Versions:\n    def test_g77_version(self):\n        fc = numpy.distutils.fcompiler.new_fcompiler(compiler='gnu')\n        for vs, version in g77_version_strings:\n            v = fc.version_match(vs)\n            assert_(v == version, (vs, v))\n\n    def test_not_g77(self):\n        fc = numpy.distutils.fcompiler.new_fcompiler(compiler='gnu')\n        for vs, _ in gfortran_version_strings:\n            v = fc.version_match(vs)\n            assert_(v is None, (vs, v))\n", "class_fn": true, "question_id": "numpy/numpy.distutils.tests.test_fcompiler_gnu/TestG77Versions", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/tests/test_fcompiler_intel.py", "fn_id": "", "content": "class TestIntelFCompilerVersions:\n    def test_32bit_version(self):\n        fc = numpy.distutils.fcompiler.new_fcompiler(compiler='intel')\n        for vs, version in intel_32bit_version_strings:\n            v = fc.version_match(vs)\n            assert_(v == version)\n", "class_fn": true, "question_id": "numpy/numpy.distutils.tests.test_fcompiler_intel/TestIntelFCompilerVersions", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/tests/test_misc_util.py", "fn_id": "", "content": "class TestSharedExtension:\n\n    def test_get_shared_lib_extension(self):\n        import sys\n        ext = get_shared_lib_extension(is_python_ext=False)\n        if sys.platform.startswith('linux'):\n            assert_equal(ext, '.so')\n        elif sys.platform.startswith('gnukfreebsd'):\n            assert_equal(ext, '.so')\n        elif sys.platform.startswith('darwin'):\n            assert_equal(ext, '.dylib')\n        elif sys.platform.startswith('win'):\n            assert_equal(ext, '.dll')\n        # just check for no crash\n        assert_(get_shared_lib_extension(is_python_ext=True))\n", "class_fn": true, "question_id": "numpy/numpy.distutils.tests.test_misc_util/TestSharedExtension", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/exceptions.py", "fn_id": "", "content": "class TooHardError(RuntimeError):\n    \"\"\"max_work was exceeded.\n\n    This is raised whenever the maximum number of candidate solutions\n    to consider specified by the ``max_work`` parameter is exceeded.\n    Assigning a finite number to max_work may have caused the operation\n    to fail.\n\n    \"\"\"\n\n    pass\n", "class_fn": true, "question_id": "numpy/numpy.exceptions/TooHardError", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/f2py/f2py2e.py", "fn_id": "", "content": "class CombineIncludePaths(argparse.Action):\n    def __call__(self, parser, namespace, values, option_string=None):\n        include_paths_set = set(getattr(namespace, 'include_paths', []) or [])\n        if option_string == \"--include_paths\":\n            outmess(\"Use --include-paths or -I instead of --include_paths which will be removed\")\n        if option_string == \"--include-paths\" or option_string == \"--include_paths\":\n            include_paths_set.update(values.split(':'))\n        else:\n            include_paths_set.add(values)\n        setattr(namespace, 'include_paths', list(include_paths_set))\n", "class_fn": true, "question_id": "numpy/numpy.f2py.f2py2e/CombineIncludePaths", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/fft/tests/test_helper.py", "fn_id": "", "content": "class TestFFTFreq:\n\n    def test_definition(self):\n        x = [0, 1, 2, 3, 4, -4, -3, -2, -1]\n        assert_array_almost_equal(9*fft.fftfreq(9), x)\n        assert_array_almost_equal(9*pi*fft.fftfreq(9, pi), x)\n        x = [0, 1, 2, 3, 4, -5, -4, -3, -2, -1]\n        assert_array_almost_equal(10*fft.fftfreq(10), x)\n        assert_array_almost_equal(10*pi*fft.fftfreq(10, pi), x)\n", "class_fn": true, "question_id": "numpy/numpy.fft.tests.test_helper/TestFFTFreq", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/index_tricks.py", "fn_id": "", "content": "class CClass(AxisConcatenator):\n    \"\"\"\n    Translates slice objects to concatenation along the second axis.\n\n    This is short-hand for ``np.r_['-1,2,0', index expression]``, which is\n    useful because of its common occurrence. In particular, arrays will be\n    stacked along their last axis after being upgraded to at least 2-D with\n    1's post-pended to the shape (column vectors made out of 1-D arrays).\n\n    See Also\n    --------\n    column_stack : Stack 1-D arrays as columns into a 2-D array.\n    r_ : For more detailed documentation.\n\n    Examples\n    --------\n    >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\n    array([[1, 4],\n           [2, 5],\n           [3, 6]])\n    >>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\n    array([[1, 2, 3, ..., 4, 5, 6]])\n\n    \"\"\"\n\n    def __init__(self):\n        AxisConcatenator.__init__(self, -1, ndmin=2, trans1d=0)\n", "class_fn": true, "question_id": "numpy/numpy.lib.index_tricks/CClass", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/index_tricks.py", "fn_id": "", "content": "class OGridClass(nd_grid):\n    \"\"\"\n    An instance which returns an open multi-dimensional \"meshgrid\".\n\n    An instance which returns an open (i.e. not fleshed out) mesh-grid\n    when indexed, so that only one dimension of each returned array is\n    greater than 1.  The dimension and number of the output arrays are\n    equal to the number of indexing dimensions.  If the step length is\n    not a complex number, then the stop is not inclusive.\n\n    However, if the step length is a **complex number** (e.g. 5j), then\n    the integer part of its magnitude is interpreted as specifying the\n    number of points to create between the start and stop values, where\n    the stop value **is inclusive**.\n\n    Returns\n    -------\n    mesh-grid\n        `ndarrays` with only one dimension not equal to 1\n\n    See Also\n    --------\n    mgrid : like `ogrid` but returns dense (or fleshed out) mesh grids\n    meshgrid: return coordinate matrices from coordinate vectors\n    r_ : array concatenator\n    :ref:`how-to-partition`\n\n    Examples\n    --------\n    >>> from numpy import ogrid\n    >>> ogrid[-1:1:5j]\n    array([-1. , -0.5,  0. ,  0.5,  1. ])\n    >>> ogrid[0:5,0:5]\n    [array([[0],\n            [1],\n            [2],\n            [3],\n            [4]]), array([[0, 1, 2, 3, 4]])]\n\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(sparse=True)\n", "class_fn": true, "question_id": "numpy/numpy.lib.index_tricks/OGridClass", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/tests/test_type_check.py", "fn_id": "", "content": "class TestArrayConversion:\n\n    def test_asfarray(self):\n        a = asfarray(np.array([1, 2, 3]))\n        assert_equal(a.__class__, np.ndarray)\n        assert_(np.issubdtype(a.dtype, np.floating))\n\n        # previously this would infer dtypes from arrays, unlike every single\n        # other numpy function\n        assert_raises(TypeError,\n            asfarray, np.array([1, 2, 3]), dtype=np.array(1.0))\n", "class_fn": true, "question_id": "numpy/numpy.lib.tests.test_type_check/TestArrayConversion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/tests/test_type_check.py", "fn_id": "", "content": "class TestIscomplex:\n\n    def test_fail(self):\n        z = np.array([-1, 0, 1])\n        res = iscomplex(z)\n        assert_(not np.any(res, axis=0))\n\n    def test_pass(self):\n        z = np.array([-1j, 1, 0])\n        res = iscomplex(z)\n        assert_array_equal(res, [1, 0, 0])\n", "class_fn": true, "question_id": "numpy/numpy.lib.tests.test_type_check/TestIscomplex", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/tests/test_type_check.py", "fn_id": "", "content": "class TestIsinf:\n    # Fixme, wrong place, isinf now ufunc\n\n    def test_goodvalues(self):\n        z = np.array((-1., 0., 1.))\n        res = np.isinf(z) == 0\n        assert_all(np.all(res, axis=0))\n\n    def test_posinf(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            assert_all(np.isinf(np.array((1.,))/0.) == 1)\n\n    def test_posinf_scalar(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            assert_all(np.isinf(np.array(1.,)/0.) == 1)\n\n    def test_neginf(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            assert_all(np.isinf(np.array((-1.,))/0.) == 1)\n\n    def test_neginf_scalar(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            assert_all(np.isinf(np.array(-1.)/0.) == 1)\n\n    def test_ind(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            assert_all(np.isinf(np.array((0.,))/0.) == 0)\n", "class_fn": true, "question_id": "numpy/numpy.lib.tests.test_type_check/TestIsinf", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/tests/test_type_check.py", "fn_id": "", "content": "class TestIsposinf:\n\n    def test_generic(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            vals = isposinf(np.array((-1., 0, 1))/0.)\n        assert_(vals[0] == 0)\n        assert_(vals[1] == 0)\n        assert_(vals[2] == 1)\n", "class_fn": true, "question_id": "numpy/numpy.lib.tests.test_type_check/TestIsposinf", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/tests/test_type_check.py", "fn_id": "", "content": "class TestMintypecode:\n\n    def test_default_1(self):\n        for itype in '1bcsuwil':\n            assert_equal(mintypecode(itype), 'd')\n        assert_equal(mintypecode('f'), 'f')\n        assert_equal(mintypecode('d'), 'd')\n        assert_equal(mintypecode('F'), 'F')\n        assert_equal(mintypecode('D'), 'D')\n\n    def test_default_2(self):\n        for itype in '1bcsuwil':\n            assert_equal(mintypecode(itype+'f'), 'f')\n            assert_equal(mintypecode(itype+'d'), 'd')\n            assert_equal(mintypecode(itype+'F'), 'F')\n            assert_equal(mintypecode(itype+'D'), 'D')\n        assert_equal(mintypecode('ff'), 'f')\n        assert_equal(mintypecode('fd'), 'd')\n        assert_equal(mintypecode('fF'), 'F')\n        assert_equal(mintypecode('fD'), 'D')\n        assert_equal(mintypecode('df'), 'd')\n        assert_equal(mintypecode('dd'), 'd')\n        #assert_equal(mintypecode('dF',savespace=1),'F')\n        assert_equal(mintypecode('dF'), 'D')\n        assert_equal(mintypecode('dD'), 'D')\n        assert_equal(mintypecode('Ff'), 'F')\n        #assert_equal(mintypecode('Fd',savespace=1),'F')\n        assert_equal(mintypecode('Fd'), 'D')\n        assert_equal(mintypecode('FF'), 'F')\n        assert_equal(mintypecode('FD'), 'D')\n        assert_equal(mintypecode('Df'), 'D')\n        assert_equal(mintypecode('Dd'), 'D')\n        assert_equal(mintypecode('DF'), 'D')\n        assert_equal(mintypecode('DD'), 'D')\n\n    def test_default_3(self):\n        assert_equal(mintypecode('fdF'), 'D')\n        #assert_equal(mintypecode('fdF',savespace=1),'F')\n        assert_equal(mintypecode('fdD'), 'D')\n        assert_equal(mintypecode('fFD'), 'D')\n        assert_equal(mintypecode('dFD'), 'D')\n\n        assert_equal(mintypecode('ifd'), 'd')\n        assert_equal(mintypecode('ifF'), 'F')\n        assert_equal(mintypecode('ifD'), 'D')\n        assert_equal(mintypecode('idF'), 'D')\n        #assert_equal(mintypecode('idF',savespace=1),'F')\n        assert_equal(mintypecode('idD'), 'D')\n", "class_fn": true, "question_id": "numpy/numpy.lib.tests.test_type_check/TestMintypecode", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/utils.py", "fn_id": "", "content": "class _Deprecate:\n    \"\"\"\n    Decorator class to deprecate old functions.\n\n    Refer to `deprecate` for details.\n\n    See Also\n    --------\n    deprecate\n\n    \"\"\"\n\n    def __init__(self, old_name=None, new_name=None, message=None):\n        self.old_name = old_name\n        self.new_name = new_name\n        self.message = message\n\n    def __call__(self, func, *args, **kwargs):\n        \"\"\"\n        Decorator call.  Refer to ``decorate``.\n\n        \"\"\"\n        old_name = self.old_name\n        new_name = self.new_name\n        message = self.message\n\n        if old_name is None:\n            old_name = func.__name__\n        if new_name is None:\n            depdoc = \"`%s` is deprecated!\" % old_name\n        else:\n            depdoc = \"`%s` is deprecated, use `%s` instead!\" % \\\n                     (old_name, new_name)\n\n        if message is not None:\n            depdoc += \"\\n\" + message\n\n        @functools.wraps(func)\n        def newfunc(*args, **kwds):\n            warnings.warn(depdoc, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwds)\n\n        newfunc.__name__ = old_name\n        doc = func.__doc__\n        if doc is None:\n            doc = depdoc\n        else:\n            lines = doc.expandtabs().split('\\n')\n            indent = _get_indent(lines[1:])\n            if lines[0].lstrip():\n                # Indent the original first line to let inspect.cleandoc()\n                # dedent the docstring despite the deprecation notice.\n                doc = indent * ' ' + doc\n            else:\n                # Remove the same leading blank lines as cleandoc() would.\n                skip = len(lines[0]) + 1\n                for line in lines[1:]:\n                    if len(line) > indent:\n                        break\n                    skip += len(line) + 1\n                doc = doc[skip:]\n            depdoc = textwrap.indent(depdoc, ' ' * indent)\n            doc = '\\n\\n'.join([depdoc, doc])\n        newfunc.__doc__ = doc\n\n        return newfunc\n", "class_fn": true, "question_id": "numpy/numpy.lib.utils/_Deprecate", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/core.py", "fn_id": "", "content": "class _DomainGreaterEqual:\n    \"\"\"\n    DomainGreaterEqual(v)(x) is True where x < v.\n\n    \"\"\"\n\n    def __init__(self, critical_value):\n        \"DomainGreaterEqual(v)(x) = true where x < v\"\n        self.critical_value = critical_value\n\n    def __call__(self, x):\n        \"Executes the call behavior.\"\n        with np.errstate(invalid='ignore'):\n            return umath.less(x, self.critical_value)\n", "class_fn": true, "question_id": "numpy/numpy.ma.core/_DomainGreaterEqual", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/core.py", "fn_id": "", "content": "class _MaskedPrintOption:\n    \"\"\"\n    Handle the string used to represent missing data in a masked array.\n\n    \"\"\"\n\n    def __init__(self, display):\n        \"\"\"\n        Create the masked_print_option object.\n\n        \"\"\"\n        self._display = display\n        self._enabled = True\n\n    def display(self):\n        \"\"\"\n        Display the string to print for masked values.\n\n        \"\"\"\n        return self._display\n\n    def set_display(self, s):\n        \"\"\"\n        Set the string to print for masked values.\n\n        \"\"\"\n        self._display = s\n\n    def enabled(self):\n        \"\"\"\n        Is the use of the display value enabled?\n\n        \"\"\"\n        return self._enabled\n\n    def enable(self, shrink=1):\n        \"\"\"\n        Set the enabling shrink to `shrink`.\n\n        \"\"\"\n        self._enabled = shrink\n\n    def __str__(self):\n        return str(self._display)\n\n    __repr__ = __str__\n", "class_fn": true, "question_id": "numpy/numpy.ma.core/_MaskedPrintOption", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/extras.py", "fn_id": "", "content": "class MAxisConcatenator(AxisConcatenator):\n    \"\"\"\n    Translate slice objects to concatenation along an axis.\n\n    For documentation on usage, see `mr_class`.\n\n    See Also\n    --------\n    mr_class\n\n    \"\"\"\n    concatenate = staticmethod(concatenate)\n\n    @classmethod\n    def makemat(cls, arr):\n        # There used to be a view as np.matrix here, but we may eventually\n        # deprecate that class. In preparation, we use the unmasked version\n        # to construct the matrix (with copy=False for backwards compatibility\n        # with the .view)\n        data = super().makemat(arr.data, copy=False)\n        return array(data, mask=arr.mask)\n\n    def __getitem__(self, key):\n        # matrix builder syntax, like 'a, b; c, d'\n        if isinstance(key, str):\n            raise MAError(\"Unavailable for masked array.\")\n\n        return super().__getitem__(key)\n", "class_fn": true, "question_id": "numpy/numpy.ma.extras/MAxisConcatenator", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/extras.py", "fn_id": "", "content": "class _fromnxfunction_args(_fromnxfunction):\n    \"\"\"\n    A version of `_fromnxfunction` that is called with multiple array\n    arguments. The first non-array-like input marks the beginning of the\n    arguments that are passed verbatim for both the data and mask calls.\n    Array arguments are processed independently and the results are\n    returned in a list. If only one array is found, the return value is\n    just the processed array instead of a list.\n    \"\"\"\n    def __call__(self, *args, **params):\n        func = getattr(np, self.__name__)\n        arrays = []\n        args = list(args)\n        while len(args) > 0 and issequence(args[0]):\n            arrays.append(args.pop(0))\n        res = []\n        for x in arrays:\n            _d = func(np.asarray(x), *args, **params)\n            _m = func(getmaskarray(x), *args, **params)\n            res.append(masked_array(_d, mask=_m))\n        if len(arrays) == 1:\n            return res[0]\n        return res\n", "class_fn": true, "question_id": "numpy/numpy.ma.extras/_fromnxfunction_args", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/extras.py", "fn_id": "", "content": "class mr_class(MAxisConcatenator):\n    \"\"\"\n    Translate slice objects to concatenation along the first axis.\n\n    This is the masked array version of `lib.index_tricks.RClass`.\n\n    See Also\n    --------\n    lib.index_tricks.RClass\n\n    Examples\n    --------\n    >>> np.ma.mr_[np.ma.array([1,2,3]), 0, 0, np.ma.array([4,5,6])]\n    masked_array(data=[1, 2, 3, ..., 4, 5, 6],\n                 mask=False,\n           fill_value=999999)\n\n    \"\"\"\n    def __init__(self):\n        MAxisConcatenator.__init__(self, 0)\n", "class_fn": true, "question_id": "numpy/numpy.ma.extras/mr_class", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/tests/test_subclassing.py", "fn_id": "", "content": "class CSAIterator:\n    \"\"\"\n    Flat iterator object that uses its own setter/getter\n    (works around ndarray.flat not propagating subclass setters/getters\n    see https://github.com/numpy/numpy/issues/4564)\n    roughly following MaskedIterator\n    \"\"\"\n    def __init__(self, a):\n        self._original = a\n        self._dataiter = a.view(np.ndarray).flat\n\n    def __iter__(self):\n        return self\n\n    def __getitem__(self, indx):\n        out = self._dataiter.__getitem__(indx)\n        if not isinstance(out, np.ndarray):\n            out = out.__array__()\n        out = out.view(type(self._original))\n        return out\n\n    def __setitem__(self, index, value):\n        self._dataiter[index] = self._original._validate_input(value)\n\n    def __next__(self):\n        return next(self._dataiter).__array__().view(type(self._original))\n", "class_fn": true, "question_id": "numpy/numpy.ma.tests.test_subclassing/CSAIterator", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/tests/test_subclassing.py", "fn_id": "", "content": "class SubArray(np.ndarray):\n    # Defines a generic np.ndarray subclass, that stores some metadata\n    # in the  dictionary `info`.\n    def __new__(cls,arr,info={}):\n        x = np.asanyarray(arr).view(cls)\n        x.info = info.copy()\n        return x\n\n    def __array_finalize__(self, obj):\n        super().__array_finalize__(obj)\n        self.info = getattr(obj, 'info', {}).copy()\n        return\n\n    def __add__(self, other):\n        result = super().__add__(other)\n        result.info['added'] = result.info.get('added', 0) + 1\n        return result\n\n    def __iadd__(self, other):\n        result = super().__iadd__(other)\n        result.info['iadded'] = result.info.get('iadded', 0) + 1\n        return result\n", "class_fn": true, "question_id": "numpy/numpy.ma.tests.test_subclassing/SubArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/tests/test_subclassing.py", "fn_id": "", "content": "class WrappedArray(NDArrayOperatorsMixin):\n    \"\"\"\n    Wrapping a MaskedArray rather than subclassing to test that\n    ufunc deferrals are commutative.\n    See: https://github.com/numpy/numpy/issues/15200)\n    \"\"\"\n    __slots__ = ('_array', 'attrs')\n    __array_priority__ = 20\n\n    def __init__(self, array, **attrs):\n        self._array = array\n        self.attrs = attrs\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(\\n{self._array}\\n{self.attrs}\\n)\"\n\n    def __array__(self):\n        return np.asarray(self._array)\n\n    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n        if method == '__call__':\n            inputs = [arg._array if isinstance(arg, self.__class__) else arg\n                      for arg in inputs]\n            return self.__class__(ufunc(*inputs, **kwargs), **self.attrs)\n        else:\n            return NotImplemented\n", "class_fn": true, "question_id": "numpy/numpy.ma.tests.test_subclassing/WrappedArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/matrixlib/tests/test_defmatrix.py", "fn_id": "", "content": "class TestCtor:\n    def test_basic(self):\n        A = np.array([[1, 2], [3, 4]])\n        mA = matrix(A)\n        assert_(np.all(mA.A == A))\n\n        B = bmat(\"A,A;A,A\")\n        C = bmat([[A, A], [A, A]])\n        D = np.array([[1, 2, 1, 2],\n                      [3, 4, 3, 4],\n                      [1, 2, 1, 2],\n                      [3, 4, 3, 4]])\n        assert_(np.all(B.A == D))\n        assert_(np.all(C.A == D))\n\n        E = np.array([[5, 6], [7, 8]])\n        AEresult = matrix([[1, 2, 5, 6], [3, 4, 7, 8]])\n        assert_(np.all(bmat([A, E]) == AEresult))\n\n        vec = np.arange(5)\n        mvec = matrix(vec)\n        assert_(mvec.shape == (1, 5))\n\n    def test_exceptions(self):\n        # Check for ValueError when called with invalid string data.\n        assert_raises(ValueError, matrix, \"invalid\")\n\n    def test_bmat_nondefault_str(self):\n        A = np.array([[1, 2], [3, 4]])\n        B = np.array([[5, 6], [7, 8]])\n        Aresult = np.array([[1, 2, 1, 2],\n                            [3, 4, 3, 4],\n                            [1, 2, 1, 2],\n                            [3, 4, 3, 4]])\n        mixresult = np.array([[1, 2, 5, 6],\n                              [3, 4, 7, 8],\n                              [5, 6, 1, 2],\n                              [7, 8, 3, 4]])\n        assert_(np.all(bmat(\"A,A;A,A\") == Aresult))\n        assert_(np.all(bmat(\"A,A;A,A\", ldict={'A':B}) == Aresult))\n        assert_raises(TypeError, bmat, \"A,A;A,A\", gdict={'A':B})\n        assert_(\n            np.all(bmat(\"A,A;A,A\", ldict={'A':A}, gdict={'A':B}) == Aresult))\n        b2 = bmat(\"A,B;C,D\", ldict={'A':A,'B':B}, gdict={'C':B,'D':A})\n        assert_(np.all(b2 == mixresult))\n", "class_fn": true, "question_id": "numpy/numpy.matrixlib.tests.test_defmatrix/TestCtor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/matrixlib/tests/test_defmatrix.py", "fn_id": "", "content": "class TestPower:\n    def test_returntype(self):\n        a = np.array([[0, 1], [0, 0]])\n        assert_(type(matrix_power(a, 2)) is np.ndarray)\n        a = mat(a)\n        assert_(type(matrix_power(a, 2)) is matrix)\n\n    def test_list(self):\n        assert_array_equal(matrix_power([[0, 1], [0, 0]], 2), [[0, 0], [0, 0]])\n", "class_fn": true, "question_id": "numpy/numpy.matrixlib.tests.test_defmatrix/TestPower", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/matrixlib/tests/test_masked_matrix.py", "fn_id": "", "content": "class TestSubclassing:\n    # Test suite for masked subclasses of ndarray.\n\n    def setup_method(self):\n        x = np.arange(5, dtype='float')\n        mx = MMatrix(x, mask=[0, 1, 0, 0, 0])\n        self.data = (x, mx)\n\n    def test_maskedarray_subclassing(self):\n        # Tests subclassing MaskedArray\n        (x, mx) = self.data\n        assert_(isinstance(mx._data, np.matrix))\n\n    def test_masked_unary_operations(self):\n        # Tests masked_unary_operation\n        (x, mx) = self.data\n        with np.errstate(divide='ignore'):\n            assert_(isinstance(log(mx), MMatrix))\n            assert_equal(log(x), np.log(x))\n\n    def test_masked_binary_operations(self):\n        # Tests masked_binary_operation\n        (x, mx) = self.data\n        # Result should be a MMatrix\n        assert_(isinstance(add(mx, mx), MMatrix))\n        assert_(isinstance(add(mx, x), MMatrix))\n        # Result should work\n        assert_equal(add(mx, x), mx+x)\n        assert_(isinstance(add(mx, mx)._data, np.matrix))\n        with assert_warns(DeprecationWarning):\n            assert_(isinstance(add.outer(mx, mx), MMatrix))\n        assert_(isinstance(hypot(mx, mx), MMatrix))\n        assert_(isinstance(hypot(mx, x), MMatrix))\n\n    def test_masked_binary_operations2(self):\n        # Tests domained_masked_binary_operation\n        (x, mx) = self.data\n        xmx = masked_array(mx.data.__array__(), mask=mx.mask)\n        assert_(isinstance(divide(mx, mx), MMatrix))\n        assert_(isinstance(divide(mx, x), MMatrix))\n        assert_equal(divide(mx, mx), divide(xmx, xmx))\n", "class_fn": true, "question_id": "numpy/numpy.matrixlib.tests.test_masked_matrix/TestSubclassing", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/matrixlib/tests/test_regression.py", "fn_id": "", "content": "class TestRegression:\n    def test_kron_matrix(self):\n        # Ticket #71\n        x = np.matrix('[1 0; 1 0]')\n        assert_equal(type(np.kron(x, x)), type(x))\n\n    def test_matrix_properties(self):\n        # Ticket #125\n        a = np.matrix([1.0], dtype=float)\n        assert_(type(a.real) is np.matrix)\n        assert_(type(a.imag) is np.matrix)\n        c, d = np.matrix([0.0]).nonzero()\n        assert_(type(c) is np.ndarray)\n        assert_(type(d) is np.ndarray)\n\n    def test_matrix_multiply_by_1d_vector(self):\n        # Ticket #473\n        def mul():\n            np.mat(np.eye(2))*np.ones(2)\n\n        assert_raises(ValueError, mul)\n\n    def test_matrix_std_argmax(self):\n        # Ticket #83\n        x = np.asmatrix(np.random.uniform(0, 1, (3, 3)))\n        assert_equal(x.std().shape, ())\n        assert_equal(x.argmax().shape, ())\n", "class_fn": true, "question_id": "numpy/numpy.matrixlib.tests.test_regression/TestRegression", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/laguerre.py", "fn_id": "", "content": "class Laguerre(ABCPolyBase):\n    \"\"\"A Laguerre series class.\n\n    The Laguerre class provides the standard Python numerical methods\n    '+', '-', '*', '//', '%', 'divmod', '**', and '()' as well as the\n    attributes and methods listed in the `ABCPolyBase` documentation.\n\n    Parameters\n    ----------\n    coef : array_like\n        Laguerre coefficients in order of increasing degree, i.e,\n        ``(1, 2, 3)`` gives ``1*L_0(x) + 2*L_1(X) + 3*L_2(x)``.\n    domain : (2,) array_like, optional\n        Domain to use. The interval ``[domain[0], domain[1]]`` is mapped\n        to the interval ``[window[0], window[1]]`` by shifting and scaling.\n        The default value is [0, 1].\n    window : (2,) array_like, optional\n        Window, see `domain` for its use. The default value is [0, 1].\n\n        .. versionadded:: 1.6.0\n    symbol : str, optional\n        Symbol used to represent the independent variable in string\n        representations of the polynomial expression, e.g. for printing.\n        The symbol must be a valid Python identifier. Default value is 'x'.\n\n        .. versionadded:: 1.24\n\n    \"\"\"\n    # Virtual Functions\n    _add = staticmethod(lagadd)\n    _sub = staticmethod(lagsub)\n    _mul = staticmethod(lagmul)\n    _div = staticmethod(lagdiv)\n    _pow = staticmethod(lagpow)\n    _val = staticmethod(lagval)\n    _int = staticmethod(lagint)\n    _der = staticmethod(lagder)\n    _fit = staticmethod(lagfit)\n    _line = staticmethod(lagline)\n    _roots = staticmethod(lagroots)\n    _fromroots = staticmethod(lagfromroots)\n\n    # Virtual properties\n    domain = np.array(lagdomain)\n    window = np.array(lagdomain)\n    basis_name = 'L'\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.laguerre/Laguerre", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_chebyshev.py", "fn_id": "", "content": "class TestConstants:\n\n    def test_chebdomain(self):\n        assert_equal(cheb.chebdomain, [-1, 1])\n\n    def test_chebzero(self):\n        assert_equal(cheb.chebzero, [0])\n\n    def test_chebone(self):\n        assert_equal(cheb.chebone, [1])\n\n    def test_chebx(self):\n        assert_equal(cheb.chebx, [0, 1])\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_chebyshev/TestConstants", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_chebyshev.py", "fn_id": "", "content": "class TestInterpolate:\n\n    def f(self, x):\n        return x * (x - 1) * (x - 2)\n\n    def test_raises(self):\n        assert_raises(ValueError, cheb.chebinterpolate, self.f, -1)\n        assert_raises(TypeError, cheb.chebinterpolate, self.f, 10.)\n\n    def test_dimensions(self):\n        for deg in range(1, 5):\n            assert_(cheb.chebinterpolate(self.f, deg).shape == (deg + 1,))\n\n    def test_approximation(self):\n\n        def powx(x, p):\n            return x**p\n\n        x = np.linspace(-1, 1, 10)\n        for deg in range(0, 10):\n            for p in range(0, deg + 1):\n                c = cheb.chebinterpolate(powx, deg, (p,))\n                assert_almost_equal(cheb.chebval(x, c), powx(x, p), decimal=12)\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_chebyshev/TestInterpolate", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_hermite.py", "fn_id": "", "content": "class TestCompanion:\n\n    def test_raises(self):\n        assert_raises(ValueError, herm.hermcompanion, [])\n        assert_raises(ValueError, herm.hermcompanion, [1])\n\n    def test_dimensions(self):\n        for i in range(1, 5):\n            coef = [0]*i + [1]\n            assert_(herm.hermcompanion(coef).shape == (i, i))\n\n    def test_linear_root(self):\n        assert_(herm.hermcompanion([1, 2])[0, 0] == -.25)\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_hermite/TestCompanion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_hermite.py", "fn_id": "", "content": "class TestGauss:\n\n    def test_100(self):\n        x, w = herm.hermgauss(100)\n\n        # test orthogonality. Note that the results need to be normalized,\n        # otherwise the huge values that can arise from fast growing\n        # functions like Laguerre can be very confusing.\n        v = herm.hermvander(x, 99)\n        vv = np.dot(v.T * w, v)\n        vd = 1/np.sqrt(vv.diagonal())\n        vv = vd[:, None] * vv * vd\n        assert_almost_equal(vv, np.eye(100))\n\n        # check that the integral of 1 is correct\n        tgt = np.sqrt(np.pi)\n        assert_almost_equal(w.sum(), tgt)\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_hermite/TestGauss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_hermite_e.py", "fn_id": "", "content": "class TestCompanion:\n\n    def test_raises(self):\n        assert_raises(ValueError, herme.hermecompanion, [])\n        assert_raises(ValueError, herme.hermecompanion, [1])\n\n    def test_dimensions(self):\n        for i in range(1, 5):\n            coef = [0]*i + [1]\n            assert_(herme.hermecompanion(coef).shape == (i, i))\n\n    def test_linear_root(self):\n        assert_(herme.hermecompanion([1, 2])[0, 0] == -.5)\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_hermite_e/TestCompanion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_hermite_e.py", "fn_id": "", "content": "class TestGauss:\n\n    def test_100(self):\n        x, w = herme.hermegauss(100)\n\n        # test orthogonality. Note that the results need to be normalized,\n        # otherwise the huge values that can arise from fast growing\n        # functions like Laguerre can be very confusing.\n        v = herme.hermevander(x, 99)\n        vv = np.dot(v.T * w, v)\n        vd = 1/np.sqrt(vv.diagonal())\n        vv = vd[:, None] * vv * vd\n        assert_almost_equal(vv, np.eye(100))\n\n        # check that the integral of 1 is correct\n        tgt = np.sqrt(2*np.pi)\n        assert_almost_equal(w.sum(), tgt)\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_hermite_e/TestGauss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_laguerre.py", "fn_id": "", "content": "class TestCompanion:\n\n    def test_raises(self):\n        assert_raises(ValueError, lag.lagcompanion, [])\n        assert_raises(ValueError, lag.lagcompanion, [1])\n\n    def test_dimensions(self):\n        for i in range(1, 5):\n            coef = [0]*i + [1]\n            assert_(lag.lagcompanion(coef).shape == (i, i))\n\n    def test_linear_root(self):\n        assert_(lag.lagcompanion([1, 2])[0, 0] == 1.5)\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_laguerre/TestCompanion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_laguerre.py", "fn_id": "", "content": "class TestGauss:\n\n    def test_100(self):\n        x, w = lag.laggauss(100)\n\n        # test orthogonality. Note that the results need to be normalized,\n        # otherwise the huge values that can arise from fast growing\n        # functions like Laguerre can be very confusing.\n        v = lag.lagvander(x, 99)\n        vv = np.dot(v.T * w, v)\n        vd = 1/np.sqrt(vv.diagonal())\n        vv = vd[:, None] * vv * vd\n        assert_almost_equal(vv, np.eye(100))\n\n        # check that the integral of 1 is correct\n        tgt = 1.0\n        assert_almost_equal(w.sum(), tgt)\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_laguerre/TestGauss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_legendre.py", "fn_id": "", "content": "class TestCompanion:\n\n    def test_raises(self):\n        assert_raises(ValueError, leg.legcompanion, [])\n        assert_raises(ValueError, leg.legcompanion, [1])\n\n    def test_dimensions(self):\n        for i in range(1, 5):\n            coef = [0]*i + [1]\n            assert_(leg.legcompanion(coef).shape == (i, i))\n\n    def test_linear_root(self):\n        assert_(leg.legcompanion([1, 2])[0, 0] == -.5)\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_legendre/TestCompanion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_legendre.py", "fn_id": "", "content": "class TestGauss:\n\n    def test_100(self):\n        x, w = leg.leggauss(100)\n\n        # test orthogonality. Note that the results need to be normalized,\n        # otherwise the huge values that can arise from fast growing\n        # functions like Laguerre can be very confusing.\n        v = leg.legvander(x, 99)\n        vv = np.dot(v.T * w, v)\n        vd = 1/np.sqrt(vv.diagonal())\n        vv = vd[:, None] * vv * vd\n        assert_almost_equal(vv, np.eye(100))\n\n        # check that the integral of 1 is correct\n        tgt = 2.0\n        assert_almost_equal(w.sum(), tgt)\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_legendre/TestGauss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_polynomial.py", "fn_id": "", "content": "class TestCompanion:\n\n    def test_raises(self):\n        assert_raises(ValueError, poly.polycompanion, [])\n        assert_raises(ValueError, poly.polycompanion, [1])\n\n    def test_dimensions(self):\n        for i in range(1, 5):\n            coef = [0]*i + [1]\n            assert_(poly.polycompanion(coef).shape == (i, i))\n\n    def test_linear_root(self):\n        assert_(poly.polycompanion([1, 2])[0, 0] == -.5)\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_polynomial/TestCompanion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_polynomial.py", "fn_id": "", "content": "class TestVander:\n    # some random values in [-1, 1)\n    x = np.random.random((3, 5))*2 - 1\n\n    def test_polyvander(self):\n        # check for 1d x\n        x = np.arange(3)\n        v = poly.polyvander(x, 3)\n        assert_(v.shape == (3, 4))\n        for i in range(4):\n            coef = [0]*i + [1]\n            assert_almost_equal(v[..., i], poly.polyval(x, coef))\n\n        # check for 2d x\n        x = np.array([[1, 2], [3, 4], [5, 6]])\n        v = poly.polyvander(x, 3)\n        assert_(v.shape == (3, 2, 4))\n        for i in range(4):\n            coef = [0]*i + [1]\n            assert_almost_equal(v[..., i], poly.polyval(x, coef))\n\n    def test_polyvander2d(self):\n        # also tests polyval2d for non-square coefficient array\n        x1, x2, x3 = self.x\n        c = np.random.random((2, 3))\n        van = poly.polyvander2d(x1, x2, [1, 2])\n        tgt = poly.polyval2d(x1, x2, c)\n        res = np.dot(van, c.flat)\n        assert_almost_equal(res, tgt)\n\n        # check shape\n        van = poly.polyvander2d([x1], [x2], [1, 2])\n        assert_(van.shape == (1, 5, 6))\n\n    def test_polyvander3d(self):\n        # also tests polyval3d for non-square coefficient array\n        x1, x2, x3 = self.x\n        c = np.random.random((2, 3, 4))\n        van = poly.polyvander3d(x1, x2, x3, [1, 2, 3])\n        tgt = poly.polyval3d(x1, x2, x3, c)\n        res = np.dot(van, c.flat)\n        assert_almost_equal(res, tgt)\n\n        # check shape\n        van = poly.polyvander3d([x1], [x2], [x3], [1, 2, 3])\n        assert_(van.shape == (1, 5, 24))\n\n    def test_polyvandernegdeg(self):\n        x = np.arange(3)\n        assert_raises(ValueError, poly.polyvander, x, -1)\n", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_polynomial/TestVander", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/testing/_private/utils.py", "fn_id": "", "content": "class IgnoreException(Exception):\n    \"Ignoring this exception due to disabled feature\"\n    pass\n", "class_fn": true, "question_id": "numpy/numpy.testing._private.utils/IgnoreException", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/testing/print_coercion_tables.py", "fn_id": "", "content": "class GenericObject:\n    def __init__(self, v):\n        self.v = v\n\n    def __add__(self, other):\n        return self\n\n    def __radd__(self, other):\n        return self\n\n    dtype = np.dtype('O')\n", "class_fn": true, "question_id": "numpy/numpy.testing.print_coercion_tables/GenericObject", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_typing/__init__.py", "fn_id": "", "content": "class _128Bit(_256Bit):  # type: ignore[misc]\n    pass\n", "class_fn": true, "question_id": "numpy/numpy._typing/_128Bit", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_typing/__init__.py", "fn_id": "", "content": "class _32Bit(_64Bit):  # type: ignore[misc]\n    pass\n", "class_fn": true, "question_id": "numpy/numpy._typing/_32Bit", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_typing/__init__.py", "fn_id": "", "content": "class _8Bit(_16Bit):  # type: ignore[misc]\n    pass\n", "class_fn": true, "question_id": "numpy/numpy._typing/_8Bit", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_typing/_dtype_like.py", "fn_id": "", "content": "class _DTypeDictBase(TypedDict):\n    names: Sequence[str]\n    formats: Sequence[_DTypeLikeNested]\n", "class_fn": true, "question_id": "numpy/numpy._typing._dtype_like/_DTypeDictBase", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_utils/_pep440.py", "fn_id": "", "content": "class Version(_BaseVersion):\n\n    _regex = re.compile(\n        r\"^\\s*\" + VERSION_PATTERN + r\"\\s*$\",\n        re.VERBOSE | re.IGNORECASE,\n    )\n\n    def __init__(self, version):\n        # Validate the version and parse it into pieces\n        match = self._regex.search(version)\n        if not match:\n            raise InvalidVersion(\"Invalid version: '{0}'\".format(version))\n\n        # Store the parsed out pieces of the version\n        self._version = _Version(\n            epoch=int(match.group(\"epoch\")) if match.group(\"epoch\") else 0,\n            release=tuple(int(i) for i in match.group(\"release\").split(\".\")),\n            pre=_parse_letter_version(\n                match.group(\"pre_l\"),\n                match.group(\"pre_n\"),\n            ),\n            post=_parse_letter_version(\n                match.group(\"post_l\"),\n                match.group(\"post_n1\") or match.group(\"post_n2\"),\n            ),\n            dev=_parse_letter_version(\n                match.group(\"dev_l\"),\n                match.group(\"dev_n\"),\n            ),\n            local=_parse_local_version(match.group(\"local\")),\n        )\n\n        # Generate a key which will be used for sorting\n        self._key = _cmpkey(\n            self._version.epoch,\n            self._version.release,\n            self._version.pre,\n            self._version.post,\n            self._version.dev,\n            self._version.local,\n        )\n\n    def __repr__(self):\n        return \"<Version({0})>\".format(repr(str(self)))\n\n    def __str__(self):\n        parts = []\n\n        # Epoch\n        if self._version.epoch != 0:\n            parts.append(\"{0}!\".format(self._version.epoch))\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self._version.release))\n\n        # Pre-release\n        if self._version.pre is not None:\n            parts.append(\"\".join(str(x) for x in self._version.pre))\n\n        # Post-release\n        if self._version.post is not None:\n            parts.append(\".post{0}\".format(self._version.post[1]))\n\n        # Development release\n        if self._version.dev is not None:\n            parts.append(\".dev{0}\".format(self._version.dev[1]))\n\n        # Local version segment\n        if self._version.local is not None:\n            parts.append(\n                \"+{0}\".format(\".\".join(str(x) for x in self._version.local))\n            )\n\n        return \"\".join(parts)\n\n    @property\n    def public(self):\n        return str(self).split(\"+\", 1)[0]\n\n    @property\n    def base_version(self):\n        parts = []\n\n        # Epoch\n        if self._version.epoch != 0:\n            parts.append(\"{0}!\".format(self._version.epoch))\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self._version.release))\n\n        return \"\".join(parts)\n\n    @property\n    def local(self):\n        version_string = str(self)\n        if \"+\" in version_string:\n            return version_string.split(\"+\", 1)[1]\n\n    @property\n    def is_prerelease(self):\n        return bool(self._version.dev or self._version.pre)\n\n    @property\n    def is_postrelease(self):\n        return bool(self._version.post)\n", "class_fn": true, "question_id": "numpy/numpy._utils._pep440/Version", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/array_api/_set_functions.py", "fn_id": "", "content": "class UniqueAllResult(NamedTuple):\n    values: Array\n    indices: Array\n    inverse_indices: Array\n    counts: Array\n", "class_fn": true, "question_id": "numpy/numpy.array_api._set_functions/UniqueAllResult", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/array_api/linalg.py", "fn_id": "", "content": "class EighResult(NamedTuple):\n    eigenvalues: Array\n    eigenvectors: Array\n", "class_fn": true, "question_id": "numpy/numpy.array_api.linalg/EighResult", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/array_api/linalg.py", "fn_id": "", "content": "class SlogdetResult(NamedTuple):\n    sign: Array\n    logabsdet: Array\n", "class_fn": true, "question_id": "numpy/numpy.array_api.linalg/SlogdetResult", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/_ufunc_config.py", "fn_id": "", "content": "class _unspecified:\n    pass\n", "class_fn": true, "question_id": "numpy/numpy.core._ufunc_config/_unspecified", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/numerictypes.py", "fn_id": "", "content": "class _typedict(dict):\n    \"\"\"\n    Base object for a dictionary for look-up with any alias for an array dtype.\n\n    Instances of `_typedict` can not be used as dictionaries directly,\n    first they have to be populated.\n\n    \"\"\"\n\n    def __getitem__(self, obj):\n        return dict.__getitem__(self, obj2sctype(obj))\n", "class_fn": true, "question_id": "numpy/numpy.core.numerictypes/_typedict", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/ccompiler_opt.py", "fn_id": "", "content": "class CCompilerOpt(_Config, _Distutils, _Cache, _CCompiler, _Feature, _Parse):\n    \"\"\"\n    A helper class for `CCompiler` aims to provide extra build options\n    to effectively control of compiler optimizations that are directly\n    related to CPU features.\n    \"\"\"\n    def __init__(self, ccompiler, cpu_baseline=\"min\", cpu_dispatch=\"max\", cache_path=None):\n        _Config.__init__(self)\n        _Distutils.__init__(self, ccompiler)\n        _Cache.__init__(self, cache_path, self.dist_info(), cpu_baseline, cpu_dispatch)\n        _CCompiler.__init__(self)\n        _Feature.__init__(self)\n        if not self.cc_noopt and self.cc_has_native:\n            self.dist_log(\n                \"native flag is specified through environment variables. \"\n                \"force cpu-baseline='native'\"\n            )\n            cpu_baseline = \"native\"\n        _Parse.__init__(self, cpu_baseline, cpu_dispatch)\n        # keep the requested features untouched, need it later for report\n        # and trace purposes\n        self._requested_baseline = cpu_baseline\n        self._requested_dispatch = cpu_dispatch\n        # key is the dispatch-able source and value is a tuple\n        # contains two items (has_baseline[boolean], dispatched-features[list])\n        self.sources_status = getattr(self, \"sources_status\", {})\n        # every instance should has a separate one\n        self.cache_private.add(\"sources_status\")\n        # set it at the end to make sure the cache writing was done after init\n        # this class\n        self.hit_cache = hasattr(self, \"hit_cache\")\n\n    def is_cached(self):\n        \"\"\"\n        Returns True if the class loaded from the cache file\n        \"\"\"\n        return self.cache_infile and self.hit_cache\n\n    def cpu_baseline_flags(self):\n        \"\"\"\n        Returns a list of final CPU baseline compiler flags\n        \"\"\"\n        return self.parse_baseline_flags\n\n    def cpu_baseline_names(self):\n        \"\"\"\n        return a list of final CPU baseline feature names\n        \"\"\"\n        return self.parse_baseline_names\n\n    def cpu_dispatch_names(self):\n        \"\"\"\n        return a list of final CPU dispatch feature names\n        \"\"\"\n        return self.parse_dispatch_names\n\n    def try_dispatch(self, sources, src_dir=None, ccompiler=None, **kwargs):\n        \"\"\"\n        Compile one or more dispatch-able sources and generates object files,\n        also generates abstract C config headers and macros that\n        used later for the final runtime dispatching process.\n\n        The mechanism behind it is to takes each source file that specified\n        in 'sources' and branching it into several files depend on\n        special configuration statements that must be declared in the\n        top of each source which contains targeted CPU features,\n        then it compiles every branched source with the proper compiler flags.\n\n        Parameters\n        ----------\n        sources : list\n            Must be a list of dispatch-able sources file paths,\n            and configuration statements must be declared inside\n            each file.\n\n        src_dir : str\n            Path of parent directory for the generated headers and wrapped sources.\n            If None(default) the files will generated in-place.\n\n        ccompiler : CCompiler\n            Distutils `CCompiler` instance to be used for compilation.\n            If None (default), the provided instance during the initialization\n            will be used instead.\n\n        **kwargs : any\n            Arguments to pass on to the `CCompiler.compile()`\n\n        Returns\n        -------\n        list : generated object files\n\n        Raises\n        ------\n        CompileError\n            Raises by `CCompiler.compile()` on compiling failure.\n        DistutilsError\n            Some errors during checking the sanity of configuration statements.\n\n        See Also\n        --------\n        parse_targets :\n            Parsing the configuration statements of dispatch-able sources.\n        \"\"\"\n        to_compile = {}\n        baseline_flags = self.cpu_baseline_flags()\n        include_dirs = kwargs.setdefault(\"include_dirs\", [])\n\n        for src in sources:\n            output_dir = os.path.dirname(src)\n            if src_dir:\n                if not output_dir.startswith(src_dir):\n                    output_dir = os.path.join(src_dir, output_dir)\n                if output_dir not in include_dirs:\n                    # To allow including the generated config header(*.dispatch.h)\n                    # by the dispatch-able sources\n                    include_dirs.append(output_dir)\n\n            has_baseline, targets, extra_flags = self.parse_targets(src)\n            nochange = self._generate_config(output_dir, src, targets, has_baseline)\n            for tar in targets:\n                tar_src = self._wrap_target(output_dir, src, tar, nochange=nochange)\n                flags = tuple(extra_flags + self.feature_flags(tar))\n                to_compile.setdefault(flags, []).append(tar_src)\n\n            if has_baseline:\n                flags = tuple(extra_flags + baseline_flags)\n                to_compile.setdefault(flags, []).append(src)\n\n            self.sources_status[src] = (has_baseline, targets)\n\n        # For these reasons, the sources are compiled in a separate loop:\n        # - Gathering all sources with the same flags to benefit from\n        #   the parallel compiling as much as possible.\n        # - To generate all config headers of the dispatchable sources,\n        #   before the compilation in case if there are dependency relationships\n        #   among them.\n        objects = []\n        for flags, srcs in to_compile.items():\n            objects += self.dist_compile(\n                srcs, list(flags), ccompiler=ccompiler, **kwargs\n            )\n        return objects\n\n    def generate_dispatch_header(self, header_path):\n        \"\"\"\n        Generate the dispatch header which contains the #definitions and headers\n        for platform-specific instruction-sets for the enabled CPU baseline and\n        dispatch-able features.\n\n        Its highly recommended to take a look at the generated header\n        also the generated source files via `try_dispatch()`\n        in order to get the full picture.\n        \"\"\"\n        self.dist_log(\"generate CPU dispatch header: (%s)\" % header_path)\n\n        baseline_names = self.cpu_baseline_names()\n        dispatch_names = self.cpu_dispatch_names()\n        baseline_len = len(baseline_names)\n        dispatch_len = len(dispatch_names)\n\n        header_dir = os.path.dirname(header_path)\n        if not os.path.exists(header_dir):\n            self.dist_log(\n                f\"dispatch header dir {header_dir} does not exist, creating it\",\n                stderr=True\n            )\n            os.makedirs(header_dir)\n\n        with open(header_path, 'w') as f:\n            baseline_calls = ' \\\\\\n'.join([\n                (\n                    \"\\t%sWITH_CPU_EXPAND_(MACRO_TO_CALL(%s, __VA_ARGS__))\"\n                ) % (self.conf_c_prefix, f)\n                for f in baseline_names\n            ])\n            dispatch_calls = ' \\\\\\n'.join([\n                (\n                    \"\\t%sWITH_CPU_EXPAND_(MACRO_TO_CALL(%s, __VA_ARGS__))\"\n                ) % (self.conf_c_prefix, f)\n                for f in dispatch_names\n            ])\n            f.write(textwrap.dedent(\"\"\"\\\n                /*\n                 * AUTOGENERATED DON'T EDIT\n                 * Please make changes to the code generator (distutils/ccompiler_opt.py)\n                */\n                #define {pfx}WITH_CPU_BASELINE  \"{baseline_str}\"\n                #define {pfx}WITH_CPU_DISPATCH  \"{dispatch_str}\"\n                #define {pfx}WITH_CPU_BASELINE_N {baseline_len}\n                #define {pfx}WITH_CPU_DISPATCH_N {dispatch_len}\n                #define {pfx}WITH_CPU_EXPAND_(X) X\n                #define {pfx}WITH_CPU_BASELINE_CALL(MACRO_TO_CALL, ...) \\\\\n                {baseline_calls}\n                #define {pfx}WITH_CPU_DISPATCH_CALL(MACRO_TO_CALL, ...) \\\\\n                {dispatch_calls}\n            \"\"\").format(\n                pfx=self.conf_c_prefix, baseline_str=\" \".join(baseline_names),\n                dispatch_str=\" \".join(dispatch_names), baseline_len=baseline_len,\n                dispatch_len=dispatch_len, baseline_calls=baseline_calls,\n                dispatch_calls=dispatch_calls\n            ))\n            baseline_pre = ''\n            for name in baseline_names:\n                baseline_pre += self.feature_c_preprocessor(name, tabs=1) + '\\n'\n\n            dispatch_pre = ''\n            for name in dispatch_names:\n                dispatch_pre += textwrap.dedent(\"\"\"\\\n                #ifdef {pfx}CPU_TARGET_{name}\n                {pre}\n                #endif /*{pfx}CPU_TARGET_{name}*/\n                \"\"\").format(\n                    pfx=self.conf_c_prefix_, name=name, pre=self.feature_c_preprocessor(\n                    name, tabs=1\n                ))\n\n            f.write(textwrap.dedent(\"\"\"\\\n            /******* baseline features *******/\n            {baseline_pre}\n            /******* dispatch features *******/\n            {dispatch_pre}\n            \"\"\").format(\n                pfx=self.conf_c_prefix_, baseline_pre=baseline_pre,\n                dispatch_pre=dispatch_pre\n            ))\n\n    def report(self, full=False):\n        report = []\n        platform_rows = []\n        baseline_rows = []\n        dispatch_rows = []\n        report.append((\"Platform\", platform_rows))\n        report.append((\"\", \"\"))\n        report.append((\"CPU baseline\", baseline_rows))\n        report.append((\"\", \"\"))\n        report.append((\"CPU dispatch\", dispatch_rows))\n\n        ########## platform ##########\n        platform_rows.append((\"Architecture\", (\n            \"unsupported\" if self.cc_on_noarch else self.cc_march)\n        ))\n        platform_rows.append((\"Compiler\", (\n            \"unix-like\"   if self.cc_is_nocc   else self.cc_name)\n        ))\n        ########## baseline ##########\n        if self.cc_noopt:\n            baseline_rows.append((\"Requested\", \"optimization disabled\"))\n        else:\n            baseline_rows.append((\"Requested\", repr(self._requested_baseline)))\n\n        baseline_names = self.cpu_baseline_names()\n        baseline_rows.append((\n            \"Enabled\", (' '.join(baseline_names) if baseline_names else \"none\")\n        ))\n        baseline_flags = self.cpu_baseline_flags()\n        baseline_rows.append((\n            \"Flags\", (' '.join(baseline_flags) if baseline_flags else \"none\")\n        ))\n        extra_checks = []\n        for name in baseline_names:\n            extra_checks += self.feature_extra_checks(name)\n        baseline_rows.append((\n            \"Extra checks\", (' '.join(extra_checks) if extra_checks else \"none\")\n        ))\n\n        ########## dispatch ##########\n        if self.cc_noopt:\n            baseline_rows.append((\"Requested\", \"optimization disabled\"))\n        else:\n            dispatch_rows.append((\"Requested\", repr(self._requested_dispatch)))\n\n        dispatch_names = self.cpu_dispatch_names()\n        dispatch_rows.append((\n            \"Enabled\", (' '.join(dispatch_names) if dispatch_names else \"none\")\n        ))\n        ########## Generated ##########\n        # TODO:\n        # - collect object names from 'try_dispatch()'\n        #   then get size of each object and printed\n        # - give more details about the features that not\n        #   generated due compiler support\n        # - find a better output's design.\n        #\n        target_sources = {}\n        for source, (_, targets) in self.sources_status.items():\n            for tar in targets:\n                target_sources.setdefault(tar, []).append(source)\n\n        if not full or not target_sources:\n            generated = \"\"\n            for tar in self.feature_sorted(target_sources):\n                sources = target_sources[tar]\n                name = tar if isinstance(tar, str) else '(%s)' % ' '.join(tar)\n                generated += name + \"[%d] \" % len(sources)\n            dispatch_rows.append((\"Generated\", generated[:-1] if generated else \"none\"))\n        else:\n            dispatch_rows.append((\"Generated\", ''))\n            for tar in self.feature_sorted(target_sources):\n                sources = target_sources[tar]\n                pretty_name = tar if isinstance(tar, str) else '(%s)' % ' '.join(tar)\n                flags = ' '.join(self.feature_flags(tar))\n                implies = ' '.join(self.feature_sorted(self.feature_implies(tar)))\n                detect = ' '.join(self.feature_detect(tar))\n                extra_checks = []\n                for name in ((tar,) if isinstance(tar, str) else tar):\n                    extra_checks += self.feature_extra_checks(name)\n                extra_checks = (' '.join(extra_checks) if extra_checks else \"none\")\n\n                dispatch_rows.append(('', ''))\n                dispatch_rows.append((pretty_name, implies))\n                dispatch_rows.append((\"Flags\", flags))\n                dispatch_rows.append((\"Extra checks\", extra_checks))\n                dispatch_rows.append((\"Detect\", detect))\n                for src in sources:\n                    dispatch_rows.append((\"\", src))\n\n        ###############################\n        # TODO: add support for 'markdown' format\n        text = []\n        secs_len = [len(secs) for secs, _ in report]\n        cols_len = [len(col) for _, rows in report for col, _ in rows]\n        tab = ' ' * 2\n        pad =  max(max(secs_len), max(cols_len))\n        for sec, rows in report:\n            if not sec:\n                text.append(\"\") # empty line\n                continue\n            sec += ' ' * (pad - len(sec))\n            text.append(sec + tab + ': ')\n            for col, val in rows:\n                col += ' ' * (pad - len(col))\n                text.append(tab + col + ': ' + val)\n\n        return '\\n'.join(text)\n\n    def _wrap_target(self, output_dir, dispatch_src, target, nochange=False):\n        assert(isinstance(target, (str, tuple)))\n        if isinstance(target, str):\n            ext_name = target_name = target\n        else:\n            # multi-target\n            ext_name = '.'.join(target)\n            target_name = '__'.join(target)\n\n        wrap_path = os.path.join(output_dir, os.path.basename(dispatch_src))\n        wrap_path = \"{0}.{2}{1}\".format(*os.path.splitext(wrap_path), ext_name.lower())\n        if nochange and os.path.exists(wrap_path):\n            return wrap_path\n\n        self.dist_log(\"wrap dispatch-able target -> \", wrap_path)\n        # sorting for readability\n        features = self.feature_sorted(self.feature_implies_c(target))\n        target_join = \"#define %sCPU_TARGET_\" % self.conf_c_prefix_\n        target_defs = [target_join + f for f in features]\n        target_defs = '\\n'.join(target_defs)\n\n        with open(wrap_path, \"w\") as fd:\n            fd.write(textwrap.dedent(\"\"\"\\\n            /**\n             * AUTOGENERATED DON'T EDIT\n             * Please make changes to the code generator \\\n             (distutils/ccompiler_opt.py)\n             */\n            #define {pfx}CPU_TARGET_MODE\n            #define {pfx}CPU_TARGET_CURRENT {target_name}\n            {target_defs}\n            #include \"{path}\"\n            \"\"\").format(\n                pfx=self.conf_c_prefix_, target_name=target_name,\n                path=os.path.abspath(dispatch_src), target_defs=target_defs\n            ))\n        return wrap_path\n\n    def _generate_config(self, output_dir, dispatch_src, targets, has_baseline=False):\n        config_path = os.path.basename(dispatch_src)\n        config_path = os.path.splitext(config_path)[0] + '.h'\n        config_path = os.path.join(output_dir, config_path)\n        # check if targets didn't change to avoid recompiling\n        cache_hash = self.cache_hash(targets, has_baseline)\n        try:\n            with open(config_path) as f:\n                last_hash = f.readline().split(\"cache_hash:\")\n                if len(last_hash) == 2 and int(last_hash[1]) == cache_hash:\n                    return True\n        except OSError:\n            pass\n\n        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n\n        self.dist_log(\"generate dispatched config -> \", config_path)\n        dispatch_calls = []\n        for tar in targets:\n            if isinstance(tar, str):\n                target_name = tar\n            else: # multi target\n                target_name = '__'.join([t for t in tar])\n            req_detect = self.feature_detect(tar)\n            req_detect = '&&'.join([\n                \"CHK(%s)\" % f for f in req_detect\n            ])\n            dispatch_calls.append(\n                \"\\t%sCPU_DISPATCH_EXPAND_(CB((%s), %s, __VA_ARGS__))\" % (\n                self.conf_c_prefix_, req_detect, target_name\n            ))\n        dispatch_calls = ' \\\\\\n'.join(dispatch_calls)\n\n        if has_baseline:\n            baseline_calls = (\n                \"\\t%sCPU_DISPATCH_EXPAND_(CB(__VA_ARGS__))\"\n            ) % self.conf_c_prefix_\n        else:\n            baseline_calls = ''\n\n        with open(config_path, \"w\") as fd:\n            fd.write(textwrap.dedent(\"\"\"\\\n            // cache_hash:{cache_hash}\n            /**\n             * AUTOGENERATED DON'T EDIT\n             * Please make changes to the code generator (distutils/ccompiler_opt.py)\n             */\n            #ifndef {pfx}CPU_DISPATCH_EXPAND_\n                #define {pfx}CPU_DISPATCH_EXPAND_(X) X\n            #endif\n            #undef {pfx}CPU_DISPATCH_BASELINE_CALL\n            #undef {pfx}CPU_DISPATCH_CALL\n            #define {pfx}CPU_DISPATCH_BASELINE_CALL(CB, ...) \\\\\n            {baseline_calls}\n            #define {pfx}CPU_DISPATCH_CALL(CHK, CB, ...) \\\\\n            {dispatch_calls}\n            \"\"\").format(\n                pfx=self.conf_c_prefix_, baseline_calls=baseline_calls,\n                dispatch_calls=dispatch_calls, cache_hash=cache_hash\n            ))\n        return False\n", "class_fn": true, "question_id": "numpy/numpy.distutils.ccompiler_opt/CCompilerOpt", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/ccompiler_opt.py", "fn_id": "", "content": "class _Config:\n    \"\"\"An abstract class holds all configurable attributes of `CCompilerOpt`,\n    these class attributes can be used to change the default behavior\n    of `CCompilerOpt` in order to fit other requirements.\n\n    Attributes\n    ----------\n    conf_nocache : bool\n        Set True to disable memory and file cache.\n        Default is False.\n\n    conf_noopt : bool\n        Set True to forces the optimization to be disabled,\n        in this case `CCompilerOpt` tends to generate all\n        expected headers in order to 'not' break the build.\n        Default is False.\n\n    conf_cache_factors : list\n        Add extra factors to the primary caching factors. The caching factors\n        are utilized to determine if there are changes had happened that\n        requires to discard the cache and re-updating it. The primary factors\n        are the arguments of `CCompilerOpt` and `CCompiler`'s properties(type, flags, etc).\n        Default is list of two items, containing the time of last modification\n        of `ccompiler_opt` and value of attribute \"conf_noopt\"\n\n    conf_tmp_path : str,\n        The path of temporary directory. Default is auto-created\n        temporary directory via ``tempfile.mkdtemp()``.\n\n    conf_check_path : str\n        The path of testing files. Each added CPU feature must have a\n        **C** source file contains at least one intrinsic or instruction that\n        related to this feature, so it can be tested against the compiler.\n        Default is ``./distutils/checks``.\n\n    conf_target_groups : dict\n        Extra tokens that can be reached from dispatch-able sources through\n        the special mark ``@targets``. Default is an empty dictionary.\n\n        **Notes**:\n            - case-insensitive for tokens and group names\n            - sign '#' must stick in the begin of group name and only within ``@targets``\n\n        **Example**:\n            .. code-block:: console\n\n                $ \"@targets #avx_group other_tokens\" > group_inside.c\n\n            >>> CCompilerOpt.conf_target_groups[\"avx_group\"] = \\\\\n            \"$werror $maxopt avx2 avx512f avx512_skx\"\n            >>> cco = CCompilerOpt(cc_instance)\n            >>> cco.try_dispatch([\"group_inside.c\"])\n\n    conf_c_prefix : str\n        The prefix of public C definitions. Default is ``\"NPY_\"``.\n\n    conf_c_prefix_ : str\n        The prefix of internal C definitions. Default is ``\"NPY__\"``.\n\n    conf_cc_flags : dict\n        Nested dictionaries defining several compiler flags\n        that linked to some major functions, the main key\n        represent the compiler name and sub-keys represent\n        flags names. Default is already covers all supported\n        **C** compilers.\n\n        Sub-keys explained as follows:\n\n        \"native\": str or None\n            used by argument option `native`, to detect the current\n            machine support via the compiler.\n        \"werror\": str or None\n            utilized to treat warning as errors during testing CPU features\n            against the compiler and also for target's policy `$werror`\n            via dispatch-able sources.\n        \"maxopt\": str or None\n            utilized for target's policy '$maxopt' and the value should\n            contains the maximum acceptable optimization by the compiler.\n            e.g. in gcc `'-O3'`\n\n        **Notes**:\n            * case-sensitive for compiler names and flags\n            * use space to separate multiple flags\n            * any flag will tested against the compiler and it will skipped\n              if it's not applicable.\n\n    conf_min_features : dict\n        A dictionary defines the used CPU features for\n        argument option `'min'`, the key represent the CPU architecture\n        name e.g. `'x86'`. Default values provide the best effort\n        on wide range of users platforms.\n\n        **Note**: case-sensitive for architecture names.\n\n    conf_features : dict\n        Nested dictionaries used for identifying the CPU features.\n        the primary key is represented as a feature name or group name\n        that gathers several features. Default values covers all\n        supported features but without the major options like \"flags\",\n        these undefined options handle it by method `conf_features_partial()`.\n        Default value is covers almost all CPU features for *X86*, *IBM/Power64*\n        and *ARM 7/8*.\n\n        Sub-keys explained as follows:\n\n        \"implies\" : str or list, optional,\n            List of CPU feature names to be implied by it,\n            the feature name must be defined within `conf_features`.\n            Default is None.\n\n        \"flags\": str or list, optional\n            List of compiler flags. Default is None.\n\n        \"detect\": str or list, optional\n            List of CPU feature names that required to be detected\n            in runtime. By default, its the feature name or features\n            in \"group\" if its specified.\n\n        \"implies_detect\": bool, optional\n            If True, all \"detect\" of implied features will be combined.\n            Default is True. see `feature_detect()`.\n\n        \"group\": str or list, optional\n            Same as \"implies\" but doesn't require the feature name to be\n            defined within `conf_features`.\n\n        \"interest\": int, required\n            a key for sorting CPU features\n\n        \"headers\": str or list, optional\n            intrinsics C header file\n\n        \"disable\": str, optional\n            force disable feature, the string value should contains the\n            reason of disabling.\n\n        \"autovec\": bool or None, optional\n            True or False to declare that CPU feature can be auto-vectorized\n            by the compiler.\n            By default(None), treated as True if the feature contains at\n            least one applicable flag. see `feature_can_autovec()`\n\n        \"extra_checks\": str or list, optional\n            Extra test case names for the CPU feature that need to be tested\n            against the compiler.\n\n            Each test case must have a C file named ``extra_xxxx.c``, where\n            ``xxxx`` is the case name in lower case, under 'conf_check_path'.\n            It should contain at least one intrinsic or function related to the test case.\n\n            If the compiler able to successfully compile the C file then `CCompilerOpt`\n            will add a C ``#define`` for it into the main dispatch header, e.g.\n            ``#define {conf_c_prefix}_XXXX`` where ``XXXX`` is the case name in upper case.\n\n        **NOTES**:\n            * space can be used as separator with options that supports \"str or list\"\n            * case-sensitive for all values and feature name must be in upper-case.\n            * if flags aren't applicable, its will skipped rather than disable the\n              CPU feature\n            * the CPU feature will disabled if the compiler fail to compile\n              the test file\n    \"\"\"\n    conf_nocache = False\n    conf_noopt = False\n    conf_cache_factors = None\n    conf_tmp_path = None\n    conf_check_path = os.path.join(\n        os.path.dirname(os.path.realpath(__file__)), \"checks\"\n    )\n    conf_target_groups = {}\n    conf_c_prefix = 'NPY_'\n    conf_c_prefix_ = 'NPY__'\n    conf_cc_flags = dict(\n        gcc = dict(\n            # native should always fail on arm and ppc64,\n            # native usually works only with x86\n            native = '-march=native',\n            opt = '-O3',\n            werror = '-Werror',\n        ),\n        clang = dict(\n            native = '-march=native',\n            opt = \"-O3\",\n            # One of the following flags needs to be applicable for Clang to\n            # guarantee the sanity of the testing process, however in certain\n            # cases `-Werror` gets skipped during the availability test due to\n            # \"unused arguments\" warnings.\n            # see https://github.com/numpy/numpy/issues/19624\n            werror = '-Werror=switch -Werror',\n        ),\n        icc = dict(\n            native = '-xHost',\n            opt = '-O3',\n            werror = '-Werror',\n        ),\n        iccw = dict(\n            native = '/QxHost',\n            opt = '/O3',\n            werror = '/Werror',\n        ),\n        msvc = dict(\n            native = None,\n            opt = '/O2',\n            werror = '/WX',\n        ),\n        fcc = dict(\n            native = '-mcpu=a64fx',\n            opt = None,\n            werror = None,\n        )\n    )\n    conf_min_features = dict(\n        x86 = \"SSE SSE2\",\n        x64 = \"SSE SSE2 SSE3\",\n        ppc64 = '', # play it safe\n        ppc64le = \"VSX VSX2\",\n        s390x = '',\n        armhf = '', # play it safe\n        aarch64 = \"NEON NEON_FP16 NEON_VFPV4 ASIMD\"\n    )\n    conf_features = dict(\n        # X86\n        SSE = dict(\n            interest=1, headers=\"xmmintrin.h\",\n            # enabling SSE without SSE2 is useless also\n            # it's non-optional for x86_64\n            implies=\"SSE2\"\n        ),\n        SSE2   = dict(interest=2, implies=\"SSE\", headers=\"emmintrin.h\"),\n        SSE3   = dict(interest=3, implies=\"SSE2\", headers=\"pmmintrin.h\"),\n        SSSE3  = dict(interest=4, implies=\"SSE3\", headers=\"tmmintrin.h\"),\n        SSE41  = dict(interest=5, implies=\"SSSE3\", headers=\"smmintrin.h\"),\n        POPCNT = dict(interest=6, implies=\"SSE41\", headers=\"popcntintrin.h\"),\n        SSE42  = dict(interest=7, implies=\"POPCNT\"),\n        AVX    = dict(\n            interest=8, implies=\"SSE42\", headers=\"immintrin.h\",\n            implies_detect=False\n        ),\n        XOP    = dict(interest=9, implies=\"AVX\", headers=\"x86intrin.h\"),\n        FMA4   = dict(interest=10, implies=\"AVX\", headers=\"x86intrin.h\"),\n        F16C   = dict(interest=11, implies=\"AVX\"),\n        FMA3   = dict(interest=12, implies=\"F16C\"),\n        AVX2   = dict(interest=13, implies=\"F16C\"),\n        AVX512F = dict(\n            interest=20, implies=\"FMA3 AVX2\", implies_detect=False,\n            extra_checks=\"AVX512F_REDUCE\"\n        ),\n        AVX512CD = dict(interest=21, implies=\"AVX512F\"),\n        AVX512_KNL = dict(\n            interest=40, implies=\"AVX512CD\", group=\"AVX512ER AVX512PF\",\n            detect=\"AVX512_KNL\", implies_detect=False\n        ),\n        AVX512_KNM = dict(\n            interest=41, implies=\"AVX512_KNL\",\n            group=\"AVX5124FMAPS AVX5124VNNIW AVX512VPOPCNTDQ\",\n            detect=\"AVX512_KNM\", implies_detect=False\n        ),\n        AVX512_SKX = dict(\n            interest=42, implies=\"AVX512CD\", group=\"AVX512VL AVX512BW AVX512DQ\",\n            detect=\"AVX512_SKX\", implies_detect=False,\n            extra_checks=\"AVX512BW_MASK AVX512DQ_MASK\"\n        ),\n        AVX512_CLX = dict(\n            interest=43, implies=\"AVX512_SKX\", group=\"AVX512VNNI\",\n            detect=\"AVX512_CLX\"\n        ),\n        AVX512_CNL = dict(\n            interest=44, implies=\"AVX512_SKX\", group=\"AVX512IFMA AVX512VBMI\",\n            detect=\"AVX512_CNL\", implies_detect=False\n        ),\n        AVX512_ICL = dict(\n            interest=45, implies=\"AVX512_CLX AVX512_CNL\",\n            group=\"AVX512VBMI2 AVX512BITALG AVX512VPOPCNTDQ\",\n            detect=\"AVX512_ICL\", implies_detect=False\n        ),\n        AVX512_SPR = dict(\n            interest=46, implies=\"AVX512_ICL\", group=\"AVX512FP16\",\n            detect=\"AVX512_SPR\", implies_detect=False\n        ),\n        # IBM/Power\n        ## Power7/ISA 2.06\n        VSX = dict(interest=1, headers=\"altivec.h\", extra_checks=\"VSX_ASM\"),\n        ## Power8/ISA 2.07\n        VSX2 = dict(interest=2, implies=\"VSX\", implies_detect=False),\n        ## Power9/ISA 3.00\n        VSX3 = dict(interest=3, implies=\"VSX2\", implies_detect=False,\n                    extra_checks=\"VSX3_HALF_DOUBLE\"),\n        ## Power10/ISA 3.1\n        VSX4 = dict(interest=4, implies=\"VSX3\", implies_detect=False,\n                    extra_checks=\"VSX4_MMA\"),\n        # IBM/Z\n        ## VX(z13) support\n        VX = dict(interest=1, headers=\"vecintrin.h\"),\n        ## Vector-Enhancements Facility\n        VXE = dict(interest=2, implies=\"VX\", implies_detect=False),\n        ## Vector-Enhancements Facility 2\n        VXE2 = dict(interest=3, implies=\"VXE\", implies_detect=False),\n        # ARM\n        NEON  = dict(interest=1, headers=\"arm_neon.h\"),\n        NEON_FP16 = dict(interest=2, implies=\"NEON\"),\n        ## FMA\n        NEON_VFPV4 = dict(interest=3, implies=\"NEON_FP16\"),\n        ## Advanced SIMD\n        ASIMD = dict(interest=4, implies=\"NEON_FP16 NEON_VFPV4\", implies_detect=False),\n        ## ARMv8.2 half-precision & vector arithm\n        ASIMDHP = dict(interest=5, implies=\"ASIMD\"),\n        ## ARMv8.2 dot product\n        ASIMDDP = dict(interest=6, implies=\"ASIMD\"),\n        ## ARMv8.2 Single & half-precision Multiply\n        ASIMDFHM = dict(interest=7, implies=\"ASIMDHP\"),\n    )\n    def conf_features_partial(self):\n        \"\"\"Return a dictionary of supported CPU features by the platform,\n        and accumulate the rest of undefined options in `conf_features`,\n        the returned dict has same rules and notes in\n        class attribute `conf_features`, also its override\n        any options that been set in 'conf_features'.\n        \"\"\"\n        if self.cc_noopt:\n            # optimization is disabled\n            return {}\n\n        on_x86 = self.cc_on_x86 or self.cc_on_x64\n        is_unix = self.cc_is_gcc or self.cc_is_clang or self.cc_is_fcc\n\n        if on_x86 and is_unix: return dict(\n            SSE    = dict(flags=\"-msse\"),\n            SSE2   = dict(flags=\"-msse2\"),\n            SSE3   = dict(flags=\"-msse3\"),\n            SSSE3  = dict(flags=\"-mssse3\"),\n            SSE41  = dict(flags=\"-msse4.1\"),\n            POPCNT = dict(flags=\"-mpopcnt\"),\n            SSE42  = dict(flags=\"-msse4.2\"),\n            AVX    = dict(flags=\"-mavx\"),\n            F16C   = dict(flags=\"-mf16c\"),\n            XOP    = dict(flags=\"-mxop\"),\n            FMA4   = dict(flags=\"-mfma4\"),\n            FMA3   = dict(flags=\"-mfma\"),\n            AVX2   = dict(flags=\"-mavx2\"),\n            AVX512F = dict(flags=\"-mavx512f -mno-mmx\"),\n            AVX512CD = dict(flags=\"-mavx512cd\"),\n            AVX512_KNL = dict(flags=\"-mavx512er -mavx512pf\"),\n            AVX512_KNM = dict(\n                flags=\"-mavx5124fmaps -mavx5124vnniw -mavx512vpopcntdq\"\n            ),\n            AVX512_SKX = dict(flags=\"-mavx512vl -mavx512bw -mavx512dq\"),\n            AVX512_CLX = dict(flags=\"-mavx512vnni\"),\n            AVX512_CNL = dict(flags=\"-mavx512ifma -mavx512vbmi\"),\n            AVX512_ICL = dict(\n                flags=\"-mavx512vbmi2 -mavx512bitalg -mavx512vpopcntdq\"\n            ),\n            AVX512_SPR = dict(flags=\"-mavx512fp16\"),\n        )\n        if on_x86 and self.cc_is_icc: return dict(\n            SSE    = dict(flags=\"-msse\"),\n            SSE2   = dict(flags=\"-msse2\"),\n            SSE3   = dict(flags=\"-msse3\"),\n            SSSE3  = dict(flags=\"-mssse3\"),\n            SSE41  = dict(flags=\"-msse4.1\"),\n            POPCNT = {},\n            SSE42  = dict(flags=\"-msse4.2\"),\n            AVX    = dict(flags=\"-mavx\"),\n            F16C   = {},\n            XOP    = dict(disable=\"Intel Compiler doesn't support it\"),\n            FMA4   = dict(disable=\"Intel Compiler doesn't support it\"),\n            # Intel Compiler doesn't support AVX2 or FMA3 independently\n            FMA3 = dict(\n                implies=\"F16C AVX2\", flags=\"-march=core-avx2\"\n            ),\n            AVX2 = dict(implies=\"FMA3\", flags=\"-march=core-avx2\"),\n            # Intel Compiler doesn't support AVX512F or AVX512CD independently\n            AVX512F = dict(\n                implies=\"AVX2 AVX512CD\", flags=\"-march=common-avx512\"\n            ),\n            AVX512CD = dict(\n                implies=\"AVX2 AVX512F\", flags=\"-march=common-avx512\"\n            ),\n            AVX512_KNL = dict(flags=\"-xKNL\"),\n            AVX512_KNM = dict(flags=\"-xKNM\"),\n            AVX512_SKX = dict(flags=\"-xSKYLAKE-AVX512\"),\n            AVX512_CLX = dict(flags=\"-xCASCADELAKE\"),\n            AVX512_CNL = dict(flags=\"-xCANNONLAKE\"),\n            AVX512_ICL = dict(flags=\"-xICELAKE-CLIENT\"),\n            AVX512_SPR = dict(disable=\"Not supported yet\")\n        )\n        if on_x86 and self.cc_is_iccw: return dict(\n            SSE    = dict(flags=\"/arch:SSE\"),\n            SSE2   = dict(flags=\"/arch:SSE2\"),\n            SSE3   = dict(flags=\"/arch:SSE3\"),\n            SSSE3  = dict(flags=\"/arch:SSSE3\"),\n            SSE41  = dict(flags=\"/arch:SSE4.1\"),\n            POPCNT = {},\n            SSE42  = dict(flags=\"/arch:SSE4.2\"),\n            AVX    = dict(flags=\"/arch:AVX\"),\n            F16C   = {},\n            XOP    = dict(disable=\"Intel Compiler doesn't support it\"),\n            FMA4   = dict(disable=\"Intel Compiler doesn't support it\"),\n            # Intel Compiler doesn't support FMA3 or AVX2 independently\n            FMA3 = dict(\n                implies=\"F16C AVX2\", flags=\"/arch:CORE-AVX2\"\n            ),\n            AVX2 = dict(\n                implies=\"FMA3\", flags=\"/arch:CORE-AVX2\"\n            ),\n            # Intel Compiler doesn't support AVX512F or AVX512CD independently\n            AVX512F = dict(\n                implies=\"AVX2 AVX512CD\", flags=\"/Qx:COMMON-AVX512\"\n            ),\n            AVX512CD = dict(\n                implies=\"AVX2 AVX512F\", flags=\"/Qx:COMMON-AVX512\"\n            ),\n            AVX512_KNL = dict(flags=\"/Qx:KNL\"),\n            AVX512_KNM = dict(flags=\"/Qx:KNM\"),\n            AVX512_SKX = dict(flags=\"/Qx:SKYLAKE-AVX512\"),\n            AVX512_CLX = dict(flags=\"/Qx:CASCADELAKE\"),\n            AVX512_CNL = dict(flags=\"/Qx:CANNONLAKE\"),\n            AVX512_ICL = dict(flags=\"/Qx:ICELAKE-CLIENT\"),\n            AVX512_SPR = dict(disable=\"Not supported yet\")\n        )\n        if on_x86 and self.cc_is_msvc: return dict(\n            SSE = dict(flags=\"/arch:SSE\") if self.cc_on_x86 else {},\n            SSE2 = dict(flags=\"/arch:SSE2\") if self.cc_on_x86 else {},\n            SSE3   = {},\n            SSSE3  = {},\n            SSE41  = {},\n            POPCNT = dict(headers=\"nmmintrin.h\"),\n            SSE42  = {},\n            AVX    = dict(flags=\"/arch:AVX\"),\n            F16C   = {},\n            XOP    = dict(headers=\"ammintrin.h\"),\n            FMA4   = dict(headers=\"ammintrin.h\"),\n            # MSVC doesn't support FMA3 or AVX2 independently\n            FMA3 = dict(\n                implies=\"F16C AVX2\", flags=\"/arch:AVX2\"\n            ),\n            AVX2 = dict(\n                implies=\"F16C FMA3\", flags=\"/arch:AVX2\"\n            ),\n            # MSVC doesn't support AVX512F or AVX512CD independently,\n            # always generate instructions belong to (VL/VW/DQ)\n            AVX512F = dict(\n                implies=\"AVX2 AVX512CD AVX512_SKX\", flags=\"/arch:AVX512\"\n            ),\n            AVX512CD = dict(\n                implies=\"AVX512F AVX512_SKX\", flags=\"/arch:AVX512\"\n            ),\n            AVX512_KNL = dict(\n                disable=\"MSVC compiler doesn't support it\"\n            ),\n            AVX512_KNM = dict(\n                disable=\"MSVC compiler doesn't support it\"\n            ),\n            AVX512_SKX = dict(flags=\"/arch:AVX512\"),\n            AVX512_CLX = {},\n            AVX512_CNL = {},\n            AVX512_ICL = {},\n            AVX512_SPR= dict(\n                disable=\"MSVC compiler doesn't support it\"\n            )\n        )\n\n        on_power = self.cc_on_ppc64le or self.cc_on_ppc64\n        if on_power:\n            partial = dict(\n                VSX = dict(\n                    implies=(\"VSX2\" if self.cc_on_ppc64le else \"\"),\n                    flags=\"-mvsx\"\n                ),\n                VSX2 = dict(\n                    flags=\"-mcpu=power8\", implies_detect=False\n                ),\n                VSX3 = dict(\n                    flags=\"-mcpu=power9 -mtune=power9\", implies_detect=False\n                ),\n                VSX4 = dict(\n                    flags=\"-mcpu=power10 -mtune=power10\", implies_detect=False\n                )\n            )\n            if self.cc_is_clang:\n                partial[\"VSX\"][\"flags\"]  = \"-maltivec -mvsx\"\n                partial[\"VSX2\"][\"flags\"] = \"-mcpu=power8\"\n                partial[\"VSX3\"][\"flags\"] = \"-mcpu=power9\"\n                partial[\"VSX4\"][\"flags\"] = \"-mcpu=power10\"\n\n            return partial\n\n        on_zarch = self.cc_on_s390x\n        if on_zarch:\n            partial = dict(\n                VX = dict(\n                    flags=\"-march=arch11 -mzvector\"\n                ),\n                VXE = dict(\n                    flags=\"-march=arch12\", implies_detect=False\n                ),\n                VXE2 = dict(\n                    flags=\"-march=arch13\", implies_detect=False\n                )\n            )\n\n            return partial\n\n\n        if self.cc_on_aarch64 and is_unix: return dict(\n            NEON = dict(\n                implies=\"NEON_FP16 NEON_VFPV4 ASIMD\", autovec=True\n            ),\n            NEON_FP16 = dict(\n                implies=\"NEON NEON_VFPV4 ASIMD\", autovec=True\n            ),\n            NEON_VFPV4 = dict(\n                implies=\"NEON NEON_FP16 ASIMD\", autovec=True\n            ),\n            ASIMD = dict(\n                implies=\"NEON NEON_FP16 NEON_VFPV4\", autovec=True\n            ),\n            ASIMDHP = dict(\n                flags=\"-march=armv8.2-a+fp16\"\n            ),\n            ASIMDDP = dict(\n                flags=\"-march=armv8.2-a+dotprod\"\n            ),\n            ASIMDFHM = dict(\n                flags=\"-march=armv8.2-a+fp16fml\"\n            ),\n        )\n        if self.cc_on_armhf and is_unix: return dict(\n            NEON = dict(\n                flags=\"-mfpu=neon\"\n            ),\n            NEON_FP16 = dict(\n                flags=\"-mfpu=neon-fp16 -mfp16-format=ieee\"\n            ),\n            NEON_VFPV4 = dict(\n                flags=\"-mfpu=neon-vfpv4\",\n            ),\n            ASIMD = dict(\n                flags=\"-mfpu=neon-fp-armv8 -march=armv8-a+simd\",\n            ),\n            ASIMDHP = dict(\n                flags=\"-march=armv8.2-a+fp16\"\n            ),\n            ASIMDDP = dict(\n                flags=\"-march=armv8.2-a+dotprod\",\n            ),\n            ASIMDFHM = dict(\n                flags=\"-march=armv8.2-a+fp16fml\"\n            )\n        )\n        # TODO: ARM MSVC\n        return {}\n\n    def __init__(self):\n        if self.conf_tmp_path is None:\n            import shutil\n            import tempfile\n            tmp = tempfile.mkdtemp()\n            def rm_temp():\n                try:\n                    shutil.rmtree(tmp)\n                except OSError:\n                    pass\n            atexit.register(rm_temp)\n            self.conf_tmp_path = tmp\n\n        if self.conf_cache_factors is None:\n            self.conf_cache_factors = [\n                os.path.getmtime(__file__),\n                self.conf_nocache\n            ]\n", "class_fn": true, "question_id": "numpy/numpy.distutils.ccompiler_opt/_Config", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/ccompiler_opt.py", "fn_id": "", "content": "class _Parse:\n    \"\"\"A helper class that parsing main arguments of `CCompilerOpt`,\n    also parsing configuration statements in dispatch-able sources.\n\n    Parameters\n    ----------\n    cpu_baseline : str or None\n        minimal set of required CPU features or special options.\n\n    cpu_dispatch : str or None\n        dispatched set of additional CPU features or special options.\n\n    Special options can be:\n        - **MIN**: Enables the minimum CPU features that utilized via `_Config.conf_min_features`\n        - **MAX**: Enables all supported CPU features by the Compiler and platform.\n        - **NATIVE**: Enables all CPU features that supported by the current machine.\n        - **NONE**: Enables nothing\n        - **Operand +/-**: remove or add features, useful with options **MAX**, **MIN** and **NATIVE**.\n            NOTE: operand + is only added for nominal reason.\n\n    NOTES:\n        - Case-insensitive among all CPU features and special options.\n        - Comma or space can be used as a separator.\n        - If the CPU feature is not supported by the user platform or compiler,\n          it will be skipped rather than raising a fatal error.\n        - Any specified CPU features to 'cpu_dispatch' will be skipped if its part of CPU baseline features\n        - 'cpu_baseline' force enables implied features.\n\n    Attributes\n    ----------\n    parse_baseline_names : list\n        Final CPU baseline's feature names(sorted from low to high)\n    parse_baseline_flags : list\n        Compiler flags of baseline features\n    parse_dispatch_names : list\n        Final CPU dispatch-able feature names(sorted from low to high)\n    parse_target_groups : dict\n        Dictionary containing initialized target groups that configured\n        through class attribute `conf_target_groups`.\n\n        The key is represent the group name and value is a tuple\n        contains three items :\n            - bool, True if group has the 'baseline' option.\n            - list, list of CPU features.\n            - list, list of extra compiler flags.\n\n    \"\"\"\n    def __init__(self, cpu_baseline, cpu_dispatch):\n        self._parse_policies = dict(\n            # POLICY NAME, (HAVE, NOT HAVE, [DEB])\n            KEEP_BASELINE = (\n                None, self._parse_policy_not_keepbase,\n                []\n            ),\n            KEEP_SORT = (\n                self._parse_policy_keepsort,\n                self._parse_policy_not_keepsort,\n                []\n            ),\n            MAXOPT = (\n                self._parse_policy_maxopt, None,\n                []\n            ),\n            WERROR = (\n                self._parse_policy_werror, None,\n                []\n            ),\n            AUTOVEC = (\n                self._parse_policy_autovec, None,\n                [\"MAXOPT\"]\n            )\n        )\n        if hasattr(self, \"parse_is_cached\"):\n            return\n\n        self.parse_baseline_names = []\n        self.parse_baseline_flags = []\n        self.parse_dispatch_names = []\n        self.parse_target_groups = {}\n\n        if self.cc_noopt:\n            # skip parsing baseline and dispatch args and keep parsing target groups\n            cpu_baseline = cpu_dispatch = None\n\n        self.dist_log(\"check requested baseline\")\n        if cpu_baseline is not None:\n            cpu_baseline = self._parse_arg_features(\"cpu_baseline\", cpu_baseline)\n            baseline_names = self.feature_names(cpu_baseline)\n            self.parse_baseline_flags = self.feature_flags(baseline_names)\n            self.parse_baseline_names = self.feature_sorted(\n                self.feature_implies_c(baseline_names)\n            )\n\n        self.dist_log(\"check requested dispatch-able features\")\n        if cpu_dispatch is not None:\n            cpu_dispatch_ = self._parse_arg_features(\"cpu_dispatch\", cpu_dispatch)\n            cpu_dispatch = {\n                f for f in cpu_dispatch_\n                if f not in self.parse_baseline_names\n            }\n            conflict_baseline = cpu_dispatch_.difference(cpu_dispatch)\n            self.parse_dispatch_names = self.feature_sorted(\n                self.feature_names(cpu_dispatch)\n            )\n            if len(conflict_baseline) > 0:\n                self.dist_log(\n                    \"skip features\", conflict_baseline, \"since its part of baseline\"\n                )\n\n        self.dist_log(\"initialize targets groups\")\n        for group_name, tokens in self.conf_target_groups.items():\n            self.dist_log(\"parse target group\", group_name)\n            GROUP_NAME = group_name.upper()\n            if not tokens or not tokens.strip():\n                # allow empty groups, useful in case if there's a need\n                # to disable certain group since '_parse_target_tokens()'\n                # requires at least one valid target\n                self.parse_target_groups[GROUP_NAME] = (\n                    False, [], []\n                )\n                continue\n            has_baseline, features, extra_flags = \\\n                self._parse_target_tokens(tokens)\n            self.parse_target_groups[GROUP_NAME] = (\n                has_baseline, features, extra_flags\n            )\n\n        self.parse_is_cached = True\n\n    def parse_targets(self, source):\n        \"\"\"\n        Fetch and parse configuration statements that required for\n        defining the targeted CPU features, statements should be declared\n        in the top of source in between **C** comment and start\n        with a special mark **@targets**.\n\n        Configuration statements are sort of keywords representing\n        CPU features names, group of statements and policies, combined\n        together to determine the required optimization.\n\n        Parameters\n        ----------\n        source : str\n            the path of **C** source file.\n\n        Returns\n        -------\n        - bool, True if group has the 'baseline' option\n        - list, list of CPU features\n        - list, list of extra compiler flags\n        \"\"\"\n        self.dist_log(\"looking for '@targets' inside -> \", source)\n        # get lines between /*@targets and */\n        with open(source) as fd:\n            tokens = \"\"\n            max_to_reach = 1000 # good enough, isn't?\n            start_with = \"@targets\"\n            start_pos = -1\n            end_with = \"*/\"\n            end_pos = -1\n            for current_line, line in enumerate(fd):\n                if current_line == max_to_reach:\n                    self.dist_fatal(\"reached the max of lines\")\n                    break\n                if start_pos == -1:\n                    start_pos = line.find(start_with)\n                    if start_pos == -1:\n                        continue\n                    start_pos += len(start_with)\n                tokens += line\n                end_pos = line.find(end_with)\n                if end_pos != -1:\n                    end_pos += len(tokens) - len(line)\n                    break\n\n        if start_pos == -1:\n            self.dist_fatal(\"expected to find '%s' within a C comment\" % start_with)\n        if end_pos == -1:\n            self.dist_fatal(\"expected to end with '%s'\" % end_with)\n\n        tokens = tokens[start_pos:end_pos]\n        return self._parse_target_tokens(tokens)\n\n    _parse_regex_arg = re.compile(r'\\s|,|([+-])')\n    def _parse_arg_features(self, arg_name, req_features):\n        if not isinstance(req_features, str):\n            self.dist_fatal(\"expected a string in '%s'\" % arg_name)\n\n        final_features = set()\n        # space and comma can be used as a separator\n        tokens = list(filter(None, re.split(self._parse_regex_arg, req_features)))\n        append = True # append is the default\n        for tok in tokens:\n            if tok[0] in (\"#\", \"$\"):\n                self.dist_fatal(\n                    arg_name, \"target groups and policies \"\n                    \"aren't allowed from arguments, \"\n                    \"only from dispatch-able sources\"\n                )\n            if tok == '+':\n                append = True\n                continue\n            if tok == '-':\n                append = False\n                continue\n\n            TOK = tok.upper() # we use upper-case internally\n            features_to = set()\n            if TOK == \"NONE\":\n                pass\n            elif TOK == \"NATIVE\":\n                native = self.cc_flags[\"native\"]\n                if not native:\n                    self.dist_fatal(arg_name,\n                        \"native option isn't supported by the compiler\"\n                    )\n                features_to = self.feature_names(\n                    force_flags=native, macros=[(\"DETECT_FEATURES\", 1)]\n                )\n            elif TOK == \"MAX\":\n                features_to = self.feature_supported.keys()\n            elif TOK == \"MIN\":\n                features_to = self.feature_min\n            else:\n                if TOK in self.feature_supported:\n                    features_to.add(TOK)\n                else:\n                    if not self.feature_is_exist(TOK):\n                        self.dist_fatal(arg_name,\n                            \", '%s' isn't a known feature or option\" % tok\n                        )\n            if append:\n                final_features = final_features.union(features_to)\n            else:\n                final_features = final_features.difference(features_to)\n\n            append = True # back to default\n\n        return final_features\n\n    _parse_regex_target = re.compile(r'\\s|[*,/]|([()])')\n    def _parse_target_tokens(self, tokens):\n        assert(isinstance(tokens, str))\n        final_targets = [] # to keep it sorted as specified\n        extra_flags = []\n        has_baseline = False\n\n        skipped  = set()\n        policies = set()\n        multi_target = None\n\n        tokens = list(filter(None, re.split(self._parse_regex_target, tokens)))\n        if not tokens:\n            self.dist_fatal(\"expected one token at least\")\n\n        for tok in tokens:\n            TOK = tok.upper()\n            ch = tok[0]\n            if ch in ('+', '-'):\n                self.dist_fatal(\n                    \"+/- are 'not' allowed from target's groups or @targets, \"\n                    \"only from cpu_baseline and cpu_dispatch parms\"\n                )\n            elif ch == '$':\n                if multi_target is not None:\n                    self.dist_fatal(\n                        \"policies aren't allowed inside multi-target '()'\"\n                        \", only CPU features\"\n                    )\n                policies.add(self._parse_token_policy(TOK))\n            elif ch == '#':\n                if multi_target is not None:\n                    self.dist_fatal(\n                        \"target groups aren't allowed inside multi-target '()'\"\n                        \", only CPU features\"\n                    )\n                has_baseline, final_targets, extra_flags = \\\n                self._parse_token_group(TOK, has_baseline, final_targets, extra_flags)\n            elif ch == '(':\n                if multi_target is not None:\n                    self.dist_fatal(\"unclosed multi-target, missing ')'\")\n                multi_target = set()\n            elif ch == ')':\n                if multi_target is None:\n                    self.dist_fatal(\"multi-target opener '(' wasn't found\")\n                targets = self._parse_multi_target(multi_target)\n                if targets is None:\n                    skipped.add(tuple(multi_target))\n                else:\n                    if len(targets) == 1:\n                        targets = targets[0]\n                    if targets and targets not in final_targets:\n                        final_targets.append(targets)\n                multi_target = None # back to default\n            else:\n                if TOK == \"BASELINE\":\n                    if multi_target is not None:\n                        self.dist_fatal(\"baseline isn't allowed inside multi-target '()'\")\n                    has_baseline = True\n                    continue\n\n                if multi_target is not None:\n                    multi_target.add(TOK)\n                    continue\n\n                if not self.feature_is_exist(TOK):\n                    self.dist_fatal(\"invalid target name '%s'\" % TOK)\n\n                is_enabled = (\n                    TOK in self.parse_baseline_names or\n                    TOK in self.parse_dispatch_names\n                )\n                if  is_enabled:\n                    if TOK not in final_targets:\n                        final_targets.append(TOK)\n                    continue\n\n                skipped.add(TOK)\n\n        if multi_target is not None:\n            self.dist_fatal(\"unclosed multi-target, missing ')'\")\n        if skipped:\n            self.dist_log(\n                \"skip targets\", skipped,\n                \"not part of baseline or dispatch-able features\"\n            )\n\n        final_targets = self.feature_untied(final_targets)\n\n        # add polices dependencies\n        for p in list(policies):\n            _, _, deps = self._parse_policies[p]\n            for d in deps:\n                if d in policies:\n                    continue\n                self.dist_log(\n                    \"policy '%s' force enables '%s'\" % (\n                    p, d\n                ))\n                policies.add(d)\n\n        # release policies filtrations\n        for p, (have, nhave, _) in self._parse_policies.items():\n            func = None\n            if p in policies:\n                func = have\n                self.dist_log(\"policy '%s' is ON\" % p)\n            else:\n                func = nhave\n            if not func:\n                continue\n            has_baseline, final_targets, extra_flags = func(\n                has_baseline, final_targets, extra_flags\n            )\n\n        return has_baseline, final_targets, extra_flags\n\n    def _parse_token_policy(self, token):\n        \"\"\"validate policy token\"\"\"\n        if len(token) <= 1 or token[-1:] == token[0]:\n            self.dist_fatal(\"'$' must stuck in the begin of policy name\")\n        token = token[1:]\n        if token not in self._parse_policies:\n            self.dist_fatal(\n                \"'%s' is an invalid policy name, available policies are\" % token,\n                self._parse_policies.keys()\n            )\n        return token\n\n    def _parse_token_group(self, token, has_baseline, final_targets, extra_flags):\n        \"\"\"validate group token\"\"\"\n        if len(token) <= 1 or token[-1:] == token[0]:\n            self.dist_fatal(\"'#' must stuck in the begin of group name\")\n\n        token = token[1:]\n        ghas_baseline, gtargets, gextra_flags = self.parse_target_groups.get(\n            token, (False, None, [])\n        )\n        if gtargets is None:\n            self.dist_fatal(\n                \"'%s' is an invalid target group name, \" % token + \\\n                \"available target groups are\",\n                self.parse_target_groups.keys()\n            )\n        if ghas_baseline:\n            has_baseline = True\n        # always keep sorting as specified\n        final_targets += [f for f in gtargets if f not in final_targets]\n        extra_flags += [f for f in gextra_flags if f not in extra_flags]\n        return has_baseline, final_targets, extra_flags\n\n    def _parse_multi_target(self, targets):\n        \"\"\"validate multi targets that defined between parentheses()\"\"\"\n        # remove any implied features and keep the origins\n        if not targets:\n            self.dist_fatal(\"empty multi-target '()'\")\n        if not all([\n            self.feature_is_exist(tar) for tar in targets\n        ]) :\n            self.dist_fatal(\"invalid target name in multi-target\", targets)\n        if not all([\n            (\n                tar in self.parse_baseline_names or\n                tar in self.parse_dispatch_names\n            )\n            for tar in targets\n        ]) :\n            return None\n        targets = self.feature_ahead(targets)\n        if not targets:\n            return None\n        # force sort multi targets, so it can be comparable\n        targets = self.feature_sorted(targets)\n        targets = tuple(targets) # hashable\n        return targets\n\n    def _parse_policy_not_keepbase(self, has_baseline, final_targets, extra_flags):\n        \"\"\"skip all baseline features\"\"\"\n        skipped = []\n        for tar in final_targets[:]:\n            is_base = False\n            if isinstance(tar, str):\n                is_base = tar in self.parse_baseline_names\n            else:\n                # multi targets\n                is_base = all([\n                    f in self.parse_baseline_names\n                    for f in tar\n                ])\n            if is_base:\n                skipped.append(tar)\n                final_targets.remove(tar)\n\n        if skipped:\n            self.dist_log(\"skip baseline features\", skipped)\n\n        return has_baseline, final_targets, extra_flags\n\n    def _parse_policy_keepsort(self, has_baseline, final_targets, extra_flags):\n        \"\"\"leave a notice that $keep_sort is on\"\"\"\n        self.dist_log(\n            \"policy 'keep_sort' is on, dispatch-able targets\", final_targets, \"\\n\"\n            \"are 'not' sorted depend on the highest interest but\"\n            \"as specified in the dispatch-able source or the extra group\"\n        )\n        return has_baseline, final_targets, extra_flags\n\n    def _parse_policy_not_keepsort(self, has_baseline, final_targets, extra_flags):\n        \"\"\"sorted depend on the highest interest\"\"\"\n        final_targets = self.feature_sorted(final_targets, reverse=True)\n        return has_baseline, final_targets, extra_flags\n\n    def _parse_policy_maxopt(self, has_baseline, final_targets, extra_flags):\n        \"\"\"append the compiler optimization flags\"\"\"\n        if self.cc_has_debug:\n            self.dist_log(\"debug mode is detected, policy 'maxopt' is skipped.\")\n        elif self.cc_noopt:\n            self.dist_log(\"optimization is disabled, policy 'maxopt' is skipped.\")\n        else:\n            flags = self.cc_flags[\"opt\"]\n            if not flags:\n                self.dist_log(\n                    \"current compiler doesn't support optimization flags, \"\n                    \"policy 'maxopt' is skipped\", stderr=True\n                )\n            else:\n                extra_flags += flags\n        return has_baseline, final_targets, extra_flags\n\n    def _parse_policy_werror(self, has_baseline, final_targets, extra_flags):\n        \"\"\"force warnings to treated as errors\"\"\"\n        flags = self.cc_flags[\"werror\"]\n        if not flags:\n            self.dist_log(\n                \"current compiler doesn't support werror flags, \"\n                \"warnings will 'not' treated as errors\", stderr=True\n            )\n        else:\n            self.dist_log(\"compiler warnings are treated as errors\")\n            extra_flags += flags\n        return has_baseline, final_targets, extra_flags\n\n    def _parse_policy_autovec(self, has_baseline, final_targets, extra_flags):\n        \"\"\"skip features that has no auto-vectorized support by compiler\"\"\"\n        skipped = []\n        for tar in final_targets[:]:\n            if isinstance(tar, str):\n                can = self.feature_can_autovec(tar)\n            else: # multiple target\n                can = all([\n                    self.feature_can_autovec(t)\n                    for t in tar\n                ])\n            if not can:\n                final_targets.remove(tar)\n                skipped.append(tar)\n\n        if skipped:\n            self.dist_log(\"skip non auto-vectorized features\", skipped)\n\n        return has_baseline, final_targets, extra_flags\n", "class_fn": true, "question_id": "numpy/numpy.distutils.ccompiler_opt/_Parse", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/build_ext.py", "fn_id": "", "content": "class build_ext (old_build_ext):\n\n    description = \"build C/C++/F extensions (compile/link to build directory)\"\n\n    user_options = old_build_ext.user_options + [\n        ('fcompiler=', None,\n         \"specify the Fortran compiler type\"),\n        ('parallel=', 'j',\n         \"number of parallel jobs\"),\n        ('warn-error', None,\n         \"turn all warnings into errors (-Werror)\"),\n        ('cpu-baseline=', None,\n         \"specify a list of enabled baseline CPU optimizations\"),\n        ('cpu-dispatch=', None,\n         \"specify a list of dispatched CPU optimizations\"),\n        ('disable-optimization', None,\n         \"disable CPU optimized code(dispatch,simd,fast...)\"),\n        ('simd-test=', None,\n         \"specify a list of CPU optimizations to be tested against NumPy SIMD interface\"),\n    ]\n\n    help_options = old_build_ext.help_options + [\n        ('help-fcompiler', None, \"list available Fortran compilers\",\n         show_fortran_compilers),\n    ]\n\n    boolean_options = old_build_ext.boolean_options + ['warn-error', 'disable-optimization']\n\n    def initialize_options(self):\n        old_build_ext.initialize_options(self)\n        self.fcompiler = None\n        self.parallel = None\n        self.warn_error = None\n        self.cpu_baseline = None\n        self.cpu_dispatch = None\n        self.disable_optimization = None\n        self.simd_test = None\n\n    def finalize_options(self):\n        if self.parallel:\n            try:\n                self.parallel = int(self.parallel)\n            except ValueError as e:\n                raise ValueError(\"--parallel/-j argument must be an integer\") from e\n\n        # Ensure that self.include_dirs and self.distribution.include_dirs\n        # refer to the same list object. finalize_options will modify\n        # self.include_dirs, but self.distribution.include_dirs is used\n        # during the actual build.\n        # self.include_dirs is None unless paths are specified with\n        # --include-dirs.\n        # The include paths will be passed to the compiler in the order:\n        # numpy paths, --include-dirs paths, Python include path.\n        if isinstance(self.include_dirs, str):\n            self.include_dirs = self.include_dirs.split(os.pathsep)\n        incl_dirs = self.include_dirs or []\n        if self.distribution.include_dirs is None:\n            self.distribution.include_dirs = []\n        self.include_dirs = self.distribution.include_dirs\n        self.include_dirs.extend(incl_dirs)\n\n        old_build_ext.finalize_options(self)\n        self.set_undefined_options('build',\n                                        ('parallel', 'parallel'),\n                                        ('warn_error', 'warn_error'),\n                                        ('cpu_baseline', 'cpu_baseline'),\n                                        ('cpu_dispatch', 'cpu_dispatch'),\n                                        ('disable_optimization', 'disable_optimization'),\n                                        ('simd_test', 'simd_test')\n                                  )\n        CCompilerOpt.conf_target_groups[\"simd_test\"] = self.simd_test\n\n    def run(self):\n        if not self.extensions:\n            return\n\n        # Make sure that extension sources are complete.\n        self.run_command('build_src')\n\n        if self.distribution.has_c_libraries():\n            if self.inplace:\n                if self.distribution.have_run.get('build_clib'):\n                    log.warn('build_clib already run, it is too late to '\n                             'ensure in-place build of build_clib')\n                    build_clib = self.distribution.get_command_obj(\n                        'build_clib')\n                else:\n                    build_clib = self.distribution.get_command_obj(\n                        'build_clib')\n                    build_clib.inplace = 1\n                    build_clib.ensure_finalized()\n                    build_clib.run()\n                    self.distribution.have_run['build_clib'] = 1\n\n            else:\n                self.run_command('build_clib')\n                build_clib = self.get_finalized_command('build_clib')\n            self.library_dirs.append(build_clib.build_clib)\n        else:\n            build_clib = None\n\n        # Not including C libraries to the list of\n        # extension libraries automatically to prevent\n        # bogus linking commands. Extensions must\n        # explicitly specify the C libraries that they use.\n\n        from distutils.ccompiler import new_compiler\n        from numpy.distutils.fcompiler import new_fcompiler\n\n        compiler_type = self.compiler\n        # Initialize C compiler:\n        self.compiler = new_compiler(compiler=compiler_type,\n                                     verbose=self.verbose,\n                                     dry_run=self.dry_run,\n                                     force=self.force)\n        self.compiler.customize(self.distribution)\n        self.compiler.customize_cmd(self)\n\n        if self.warn_error:\n            self.compiler.compiler.append('-Werror')\n            self.compiler.compiler_so.append('-Werror')\n\n        self.compiler.show_customization()\n\n        if not self.disable_optimization:\n            dispatch_hpath = os.path.join(\"numpy\", \"distutils\", \"include\", \"npy_cpu_dispatch_config.h\")\n            dispatch_hpath = os.path.join(self.get_finalized_command(\"build_src\").build_src, dispatch_hpath)\n            opt_cache_path = os.path.abspath(\n                os.path.join(self.build_temp, 'ccompiler_opt_cache_ext.py')\n            )\n            if hasattr(self, \"compiler_opt\"):\n                # By default `CCompilerOpt` update the cache at the exit of\n                # the process, which may lead to duplicate building\n                # (see build_extension()/force_rebuild) if run() called\n                # multiple times within the same os process/thread without\n                # giving the chance the previous instances of `CCompilerOpt`\n                # to update the cache.\n                self.compiler_opt.cache_flush()\n\n            self.compiler_opt = new_ccompiler_opt(\n                compiler=self.compiler, dispatch_hpath=dispatch_hpath,\n                cpu_baseline=self.cpu_baseline, cpu_dispatch=self.cpu_dispatch,\n                cache_path=opt_cache_path\n            )\n            def report(copt):\n                log.info(\"\\n########### EXT COMPILER OPTIMIZATION ###########\")\n                log.info(copt.report(full=True))\n\n            import atexit\n            atexit.register(report, self.compiler_opt)\n\n        # Setup directory for storing generated extra DLL files on Windows\n        self.extra_dll_dir = os.path.join(self.build_temp, '.libs')\n        if not os.path.isdir(self.extra_dll_dir):\n            os.makedirs(self.extra_dll_dir)\n\n        # Create mapping of libraries built by build_clib:\n        clibs = {}\n        if build_clib is not None:\n            for libname, build_info in build_clib.libraries or []:\n                if libname in clibs and clibs[libname] != build_info:\n                    log.warn('library %r defined more than once,'\n                             ' overwriting build_info\\n%s... \\nwith\\n%s...'\n                             % (libname, repr(clibs[libname])[:300], repr(build_info)[:300]))\n                clibs[libname] = build_info\n        # .. and distribution libraries:\n        for libname, build_info in self.distribution.libraries or []:\n            if libname in clibs:\n                # build_clib libraries have a precedence before distribution ones\n                continue\n            clibs[libname] = build_info\n\n        # Determine if C++/Fortran 77/Fortran 90 compilers are needed.\n        # Update extension libraries, library_dirs, and macros.\n        all_languages = set()\n        for ext in self.extensions:\n            ext_languages = set()\n            c_libs = []\n            c_lib_dirs = []\n            macros = []\n            for libname in ext.libraries:\n                if libname in clibs:\n                    binfo = clibs[libname]\n                    c_libs += binfo.get('libraries', [])\n                    c_lib_dirs += binfo.get('library_dirs', [])\n                    for m in binfo.get('macros', []):\n                        if m not in macros:\n                            macros.append(m)\n\n                for l in clibs.get(libname, {}).get('source_languages', []):\n                    ext_languages.add(l)\n            if c_libs:\n                new_c_libs = ext.libraries + c_libs\n                log.info('updating extension %r libraries from %r to %r'\n                         % (ext.name, ext.libraries, new_c_libs))\n                ext.libraries = new_c_libs\n                ext.library_dirs = ext.library_dirs + c_lib_dirs\n            if macros:\n                log.info('extending extension %r defined_macros with %r'\n                         % (ext.name, macros))\n                ext.define_macros = ext.define_macros + macros\n\n            # determine extension languages\n            if has_f_sources(ext.sources):\n                ext_languages.add('f77')\n            if has_cxx_sources(ext.sources):\n                ext_languages.add('c++')\n            l = ext.language or self.compiler.detect_language(ext.sources)\n            if l:\n                ext_languages.add(l)\n\n            # reset language attribute for choosing proper linker\n            #\n            # When we build extensions with multiple languages, we have to\n            # choose a linker. The rules here are:\n            #   1. if there is Fortran code, always prefer the Fortran linker,\n            #   2. otherwise prefer C++ over C,\n            #   3. Users can force a particular linker by using\n            #          `language='c'`  # or 'c++', 'f90', 'f77'\n            #      in their config.add_extension() calls.\n            if 'c++' in ext_languages:\n                ext_language = 'c++'\n            else:\n                ext_language = 'c'  # default\n\n            has_fortran = False\n            if 'f90' in ext_languages:\n                ext_language = 'f90'\n                has_fortran = True\n            elif 'f77' in ext_languages:\n                ext_language = 'f77'\n                has_fortran = True\n\n            if not ext.language or has_fortran:\n                if l and l != ext_language and ext.language:\n                    log.warn('resetting extension %r language from %r to %r.' %\n                             (ext.name, l, ext_language))\n\n            ext.language = ext_language\n\n            # global language\n            all_languages.update(ext_languages)\n\n        need_f90_compiler = 'f90' in all_languages\n        need_f77_compiler = 'f77' in all_languages\n        need_cxx_compiler = 'c++' in all_languages\n\n        # Initialize C++ compiler:\n        if need_cxx_compiler:\n            self._cxx_compiler = new_compiler(compiler=compiler_type,\n                                              verbose=self.verbose,\n                                              dry_run=self.dry_run,\n                                              force=self.force)\n            compiler = self._cxx_compiler\n            compiler.customize(self.distribution, need_cxx=need_cxx_compiler)\n            compiler.customize_cmd(self)\n            compiler.show_customization()\n            self._cxx_compiler = compiler.cxx_compiler()\n        else:\n            self._cxx_compiler = None\n\n        # Initialize Fortran 77 compiler:\n        if need_f77_compiler:\n            ctype = self.fcompiler\n            self._f77_compiler = new_fcompiler(compiler=self.fcompiler,\n                                               verbose=self.verbose,\n                                               dry_run=self.dry_run,\n                                               force=self.force,\n                                               requiref90=False,\n                                               c_compiler=self.compiler)\n            fcompiler = self._f77_compiler\n            if fcompiler:\n                ctype = fcompiler.compiler_type\n                fcompiler.customize(self.distribution)\n            if fcompiler and fcompiler.get_version():\n                fcompiler.customize_cmd(self)\n                fcompiler.show_customization()\n            else:\n                self.warn('f77_compiler=%s is not available.' %\n                          (ctype))\n                self._f77_compiler = None\n        else:\n            self._f77_compiler = None\n\n        # Initialize Fortran 90 compiler:\n        if need_f90_compiler:\n            ctype = self.fcompiler\n            self._f90_compiler = new_fcompiler(compiler=self.fcompiler,\n                                               verbose=self.verbose,\n                                               dry_run=self.dry_run,\n                                               force=self.force,\n                                               requiref90=True,\n                                               c_compiler=self.compiler)\n            fcompiler = self._f90_compiler\n            if fcompiler:\n                ctype = fcompiler.compiler_type\n                fcompiler.customize(self.distribution)\n            if fcompiler and fcompiler.get_version():\n                fcompiler.customize_cmd(self)\n                fcompiler.show_customization()\n            else:\n                self.warn('f90_compiler=%s is not available.' %\n                          (ctype))\n                self._f90_compiler = None\n        else:\n            self._f90_compiler = None\n\n        # Build extensions\n        self.build_extensions()\n\n        # Copy over any extra DLL files\n        # FIXME: In the case where there are more than two packages,\n        # we blindly assume that both packages need all of the libraries,\n        # resulting in a larger wheel than is required. This should be fixed,\n        # but it's so rare that I won't bother to handle it.\n        pkg_roots = {\n            self.get_ext_fullname(ext.name).split('.')[0]\n            for ext in self.extensions\n        }\n        for pkg_root in pkg_roots:\n            shared_lib_dir = os.path.join(pkg_root, '.libs')\n            if not self.inplace:\n                shared_lib_dir = os.path.join(self.build_lib, shared_lib_dir)\n            for fn in os.listdir(self.extra_dll_dir):\n                if not os.path.isdir(shared_lib_dir):\n                    os.makedirs(shared_lib_dir)\n                if not fn.lower().endswith('.dll'):\n                    continue\n                runtime_lib = os.path.join(self.extra_dll_dir, fn)\n                copy_file(runtime_lib, shared_lib_dir)\n\n    def swig_sources(self, sources, extensions=None):\n        # Do nothing. Swig sources have been handled in build_src command.\n        return sources\n\n    def build_extension(self, ext):\n        sources = ext.sources\n        if sources is None or not is_sequence(sources):\n            raise DistutilsSetupError(\n                (\"in 'ext_modules' option (extension '%s'), \" +\n                 \"'sources' must be present and must be \" +\n                 \"a list of source filenames\") % ext.name)\n        sources = list(sources)\n\n        if not sources:\n            return\n\n        fullname = self.get_ext_fullname(ext.name)\n        if self.inplace:\n            modpath = fullname.split('.')\n            package = '.'.join(modpath[0:-1])\n            base = modpath[-1]\n            build_py = self.get_finalized_command('build_py')\n            package_dir = build_py.get_package_dir(package)\n            ext_filename = os.path.join(package_dir,\n                                        self.get_ext_filename(base))\n        else:\n            ext_filename = os.path.join(self.build_lib,\n                                        self.get_ext_filename(fullname))\n        depends = sources + ext.depends\n\n        force_rebuild = self.force\n        if not self.disable_optimization and not self.compiler_opt.is_cached():\n            log.debug(\"Detected changes on compiler optimizations\")\n            force_rebuild = True\n        if not (force_rebuild or newer_group(depends, ext_filename, 'newer')):\n            log.debug(\"skipping '%s' extension (up-to-date)\", ext.name)\n            return\n        else:\n            log.info(\"building '%s' extension\", ext.name)\n\n        extra_args = ext.extra_compile_args or []\n        extra_cflags = getattr(ext, 'extra_c_compile_args', None) or []\n        extra_cxxflags = getattr(ext, 'extra_cxx_compile_args', None) or []\n\n        macros = ext.define_macros[:]\n        for undef in ext.undef_macros:\n            macros.append((undef,))\n\n        c_sources, cxx_sources, f_sources, fmodule_sources = \\\n            filter_sources(ext.sources)\n\n        if self.compiler.compiler_type == 'msvc':\n            if cxx_sources:\n                # Needed to compile kiva.agg._agg extension.\n                extra_args.append('/Zm1000')\n                extra_cflags += extra_cxxflags\n            # this hack works around the msvc compiler attributes\n            # problem, msvc uses its own convention :(\n            c_sources += cxx_sources\n            cxx_sources = []\n\n        # Set Fortran/C++ compilers for compilation and linking.\n        if ext.language == 'f90':\n            fcompiler = self._f90_compiler\n        elif ext.language == 'f77':\n            fcompiler = self._f77_compiler\n        else:  # in case ext.language is c++, for instance\n            fcompiler = self._f90_compiler or self._f77_compiler\n        if fcompiler is not None:\n            fcompiler.extra_f77_compile_args = (ext.extra_f77_compile_args or []) if hasattr(\n                ext, 'extra_f77_compile_args') else []\n            fcompiler.extra_f90_compile_args = (ext.extra_f90_compile_args or []) if hasattr(\n                ext, 'extra_f90_compile_args') else []\n        cxx_compiler = self._cxx_compiler\n\n        # check for the availability of required compilers\n        if cxx_sources and cxx_compiler is None:\n            raise DistutilsError(\"extension %r has C++ sources\"\n                                 \"but no C++ compiler found\" % (ext.name))\n        if (f_sources or fmodule_sources) and fcompiler is None:\n            raise DistutilsError(\"extension %r has Fortran sources \"\n                                 \"but no Fortran compiler found\" % (ext.name))\n        if ext.language in ['f77', 'f90'] and fcompiler is None:\n            self.warn(\"extension %r has Fortran libraries \"\n                      \"but no Fortran linker found, using default linker\" % (ext.name))\n        if ext.language == 'c++' and cxx_compiler is None:\n            self.warn(\"extension %r has C++ libraries \"\n                      \"but no C++ linker found, using default linker\" % (ext.name))\n\n        kws = {'depends': ext.depends}\n        output_dir = self.build_temp\n\n        include_dirs = ext.include_dirs + get_numpy_include_dirs()\n\n        # filtering C dispatch-table sources when optimization is not disabled,\n        # otherwise treated as normal sources.\n        copt_c_sources = []\n        copt_cxx_sources = []\n        copt_baseline_flags = []\n        copt_macros = []\n        if not self.disable_optimization:\n            bsrc_dir = self.get_finalized_command(\"build_src\").build_src\n            dispatch_hpath = os.path.join(\"numpy\", \"distutils\", \"include\")\n            dispatch_hpath = os.path.join(bsrc_dir, dispatch_hpath)\n            include_dirs.append(dispatch_hpath)\n\n            # copt_build_src = None if self.inplace else bsrc_dir\n            # Always generate the generated config files and\n            # dispatch-able sources inside the build directory,\n            # even if the build option `inplace` is enabled.\n            # This approach prevents conflicts with Meson-generated\n            # config headers. Since `spin build --clean` will not remove\n            # these headers, they might overwrite the generated Meson headers,\n            # causing compatibility issues. Maintaining separate directories\n            # ensures compatibility between distutils dispatch config headers\n            # and Meson headers, avoiding build disruptions.\n            # See gh-24450 for more details.\n            copt_build_src = bsrc_dir\n            for _srcs, _dst, _ext in (\n                ((c_sources,), copt_c_sources, ('.dispatch.c',)),\n                ((c_sources, cxx_sources), copt_cxx_sources,\n                    ('.dispatch.cpp', '.dispatch.cxx'))\n            ):\n                for _src in _srcs:\n                    _dst += [\n                        _src.pop(_src.index(s))\n                        for s in _src[:] if s.endswith(_ext)\n                    ]\n            copt_baseline_flags = self.compiler_opt.cpu_baseline_flags()\n        else:\n            copt_macros.append((\"NPY_DISABLE_OPTIMIZATION\", 1))\n\n        c_objects = []\n        if copt_cxx_sources:\n            log.info(\"compiling C++ dispatch-able sources\")\n            c_objects += self.compiler_opt.try_dispatch(\n                copt_cxx_sources,\n                output_dir=output_dir,\n                src_dir=copt_build_src,\n                macros=macros + copt_macros,\n                include_dirs=include_dirs,\n                debug=self.debug,\n                extra_postargs=extra_args + extra_cxxflags,\n                ccompiler=cxx_compiler,\n                **kws\n            )\n        if copt_c_sources:\n            log.info(\"compiling C dispatch-able sources\")\n            c_objects += self.compiler_opt.try_dispatch(\n                copt_c_sources,\n                output_dir=output_dir,\n                src_dir=copt_build_src,\n                macros=macros + copt_macros,\n                include_dirs=include_dirs,\n                debug=self.debug,\n                extra_postargs=extra_args + extra_cflags,\n                **kws)\n        if c_sources:\n            log.info(\"compiling C sources\")\n            c_objects += self.compiler.compile(\n                c_sources,\n                output_dir=output_dir,\n                macros=macros + copt_macros,\n                include_dirs=include_dirs,\n                debug=self.debug,\n                extra_postargs=(extra_args + copt_baseline_flags +\n                                extra_cflags),\n                **kws)\n        if cxx_sources:\n            log.info(\"compiling C++ sources\")\n            c_objects += cxx_compiler.compile(\n                cxx_sources,\n                output_dir=output_dir,\n                macros=macros + copt_macros,\n                include_dirs=include_dirs,\n                debug=self.debug,\n                extra_postargs=(extra_args + copt_baseline_flags +\n                                extra_cxxflags),\n                **kws)\n\n        extra_postargs = []\n        f_objects = []\n        if fmodule_sources:\n            log.info(\"compiling Fortran 90 module sources\")\n            module_dirs = ext.module_dirs[:]\n            module_build_dir = os.path.join(\n                self.build_temp, os.path.dirname(\n                    self.get_ext_filename(fullname)))\n\n            self.mkpath(module_build_dir)\n            if fcompiler.module_dir_switch is None:\n                existing_modules = glob('*.mod')\n            extra_postargs += fcompiler.module_options(\n                module_dirs, module_build_dir)\n            f_objects += fcompiler.compile(fmodule_sources,\n                                           output_dir=self.build_temp,\n                                           macros=macros,\n                                           include_dirs=include_dirs,\n                                           debug=self.debug,\n                                           extra_postargs=extra_postargs,\n                                           depends=ext.depends)\n\n            if fcompiler.module_dir_switch is None:\n                for f in glob('*.mod'):\n                    if f in existing_modules:\n                        continue\n                    t = os.path.join(module_build_dir, f)\n                    if os.path.abspath(f) == os.path.abspath(t):\n                        continue\n                    if os.path.isfile(t):\n                        os.remove(t)\n                    try:\n                        self.move_file(f, module_build_dir)\n                    except DistutilsFileError:\n                        log.warn('failed to move %r to %r' %\n                                 (f, module_build_dir))\n        if f_sources:\n            log.info(\"compiling Fortran sources\")\n            f_objects += fcompiler.compile(f_sources,\n                                           output_dir=self.build_temp,\n                                           macros=macros,\n                                           include_dirs=include_dirs,\n                                           debug=self.debug,\n                                           extra_postargs=extra_postargs,\n                                           depends=ext.depends)\n\n        if f_objects and not fcompiler.can_ccompiler_link(self.compiler):\n            unlinkable_fobjects = f_objects\n            objects = c_objects\n        else:\n            unlinkable_fobjects = []\n            objects = c_objects + f_objects\n\n        if ext.extra_objects:\n            objects.extend(ext.extra_objects)\n        extra_args = ext.extra_link_args or []\n        libraries = self.get_libraries(ext)[:]\n        library_dirs = ext.library_dirs[:]\n\n        linker = self.compiler.link_shared_object\n        # Always use system linker when using MSVC compiler.\n        if self.compiler.compiler_type in ('msvc', 'intelw', 'intelemw'):\n            # expand libraries with fcompiler libraries as we are\n            # not using fcompiler linker\n            self._libs_with_msvc_and_fortran(\n                fcompiler, libraries, library_dirs)\n            if ext.runtime_library_dirs:\n                # gcc adds RPATH to the link. On windows, copy the dll into\n                # self.extra_dll_dir instead.\n                for d in ext.runtime_library_dirs:\n                    for f in glob(d + '/*.dll'):\n                        copy_file(f, self.extra_dll_dir)\n                ext.runtime_library_dirs = []\n\n        elif ext.language in ['f77', 'f90'] and fcompiler is not None:\n            linker = fcompiler.link_shared_object\n        if ext.language == 'c++' and cxx_compiler is not None:\n            linker = cxx_compiler.link_shared_object\n\n        if fcompiler is not None:\n            objects, libraries = self._process_unlinkable_fobjects(\n                    objects, libraries,\n                    fcompiler, library_dirs,\n                    unlinkable_fobjects)\n\n        linker(objects, ext_filename,\n               libraries=libraries,\n               library_dirs=library_dirs,\n               runtime_library_dirs=ext.runtime_library_dirs,\n               extra_postargs=extra_args,\n               export_symbols=self.get_export_symbols(ext),\n               debug=self.debug,\n               build_temp=self.build_temp,\n               target_lang=ext.language)\n\n    def _add_dummy_mingwex_sym(self, c_sources):\n        build_src = self.get_finalized_command(\"build_src\").build_src\n        build_clib = self.get_finalized_command(\"build_clib\").build_clib\n        objects = self.compiler.compile([os.path.join(build_src,\n                                                      \"gfortran_vs2003_hack.c\")],\n                                        output_dir=self.build_temp)\n        self.compiler.create_static_lib(\n            objects, \"_gfortran_workaround\", output_dir=build_clib, debug=self.debug)\n\n    def _process_unlinkable_fobjects(self, objects, libraries,\n                                     fcompiler, library_dirs,\n                                     unlinkable_fobjects):\n        libraries = list(libraries)\n        objects = list(objects)\n        unlinkable_fobjects = list(unlinkable_fobjects)\n\n        # Expand possible fake static libraries to objects;\n        # make sure to iterate over a copy of the list as\n        # \"fake\" libraries will be removed as they are\n        # encountered\n        for lib in libraries[:]:\n            for libdir in library_dirs:\n                fake_lib = os.path.join(libdir, lib + '.fobjects')\n                if os.path.isfile(fake_lib):\n                    # Replace fake static library\n                    libraries.remove(lib)\n                    with open(fake_lib) as f:\n                        unlinkable_fobjects.extend(f.read().splitlines())\n\n                    # Expand C objects\n                    c_lib = os.path.join(libdir, lib + '.cobjects')\n                    with open(c_lib) as f:\n                        objects.extend(f.read().splitlines())\n\n        # Wrap unlinkable objects to a linkable one\n        if unlinkable_fobjects:\n            fobjects = [os.path.abspath(obj) for obj in unlinkable_fobjects]\n            wrapped = fcompiler.wrap_unlinkable_objects(\n                    fobjects, output_dir=self.build_temp,\n                    extra_dll_dir=self.extra_dll_dir)\n            objects.extend(wrapped)\n\n        return objects, libraries\n\n    def _libs_with_msvc_and_fortran(self, fcompiler, c_libraries,\n                                    c_library_dirs):\n        if fcompiler is None:\n            return\n\n        for libname in c_libraries:\n            if libname.startswith('msvc'):\n                continue\n            fileexists = False\n            for libdir in c_library_dirs or []:\n                libfile = os.path.join(libdir, '%s.lib' % (libname))\n                if os.path.isfile(libfile):\n                    fileexists = True\n                    break\n            if fileexists:\n                continue\n            # make g77-compiled static libs available to MSVC\n            fileexists = False\n            for libdir in c_library_dirs:\n                libfile = os.path.join(libdir, 'lib%s.a' % (libname))\n                if os.path.isfile(libfile):\n                    # copy libname.a file to name.lib so that MSVC linker\n                    # can find it\n                    libfile2 = os.path.join(self.build_temp, libname + '.lib')\n                    copy_file(libfile, libfile2)\n                    if self.build_temp not in c_library_dirs:\n                        c_library_dirs.append(self.build_temp)\n                    fileexists = True\n                    break\n            if fileexists:\n                continue\n            log.warn('could not find library %r in directories %s'\n                     % (libname, c_library_dirs))\n\n        # Always use system linker when using MSVC compiler.\n        f_lib_dirs = []\n        for dir in fcompiler.library_dirs:\n            # correct path when compiling in Cygwin but with normal Win\n            # Python\n            if dir.startswith('/usr/lib'):\n                try:\n                    dir = subprocess.check_output(['cygpath', '-w', dir])\n                except (OSError, subprocess.CalledProcessError):\n                    pass\n                else:\n                    dir = filepath_from_subprocess_output(dir)\n            f_lib_dirs.append(dir)\n        c_library_dirs.extend(f_lib_dirs)\n\n        # make g77-compiled static libs available to MSVC\n        for lib in fcompiler.libraries:\n            if not lib.startswith('msvc'):\n                c_libraries.append(lib)\n                p = combine_paths(f_lib_dirs, 'lib' + lib + '.a')\n                if p:\n                    dst_name = os.path.join(self.build_temp, lib + '.lib')\n                    if not os.path.isfile(dst_name):\n                        copy_file(p[0], dst_name)\n                    if self.build_temp not in c_library_dirs:\n                        c_library_dirs.append(self.build_temp)\n\n    def get_source_files(self):\n        self.check_extensions_list(self.extensions)\n        filenames = []\n        for ext in self.extensions:\n            filenames.extend(get_ext_source_files(ext))\n        return filenames\n\n    def get_outputs(self):\n        self.check_extensions_list(self.extensions)\n\n        outputs = []\n        for ext in self.extensions:\n            if not ext.sources:\n                continue\n            fullname = self.get_ext_fullname(ext.name)\n            outputs.append(os.path.join(self.build_lib,\n                                        self.get_ext_filename(fullname)))\n        return outputs\n", "class_fn": true, "question_id": "numpy/numpy.distutils.command.build_ext/build_ext", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/config_compiler.py", "fn_id": "", "content": "class config_fc(Command):\n    \"\"\" Distutils command to hold user specified options\n    to Fortran compilers.\n\n    config_fc command is used by the FCompiler.customize() method.\n    \"\"\"\n\n    description = \"specify Fortran 77/Fortran 90 compiler information\"\n\n    user_options = [\n        ('fcompiler=', None, \"specify Fortran compiler type\"),\n        ('f77exec=', None, \"specify F77 compiler command\"),\n        ('f90exec=', None, \"specify F90 compiler command\"),\n        ('f77flags=', None, \"specify F77 compiler flags\"),\n        ('f90flags=', None, \"specify F90 compiler flags\"),\n        ('opt=', None, \"specify optimization flags\"),\n        ('arch=', None, \"specify architecture specific optimization flags\"),\n        ('debug', 'g', \"compile with debugging information\"),\n        ('noopt', None, \"compile without optimization\"),\n        ('noarch', None, \"compile without arch-dependent optimization\"),\n        ]\n\n    help_options = [\n        ('help-fcompiler', None, \"list available Fortran compilers\",\n         show_fortran_compilers),\n        ]\n\n    boolean_options = ['debug', 'noopt', 'noarch']\n\n    def initialize_options(self):\n        self.fcompiler = None\n        self.f77exec = None\n        self.f90exec = None\n        self.f77flags = None\n        self.f90flags = None\n        self.opt = None\n        self.arch = None\n        self.debug = None\n        self.noopt = None\n        self.noarch = None\n\n    def finalize_options(self):\n        log.info('unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options')\n        build_clib = self.get_finalized_command('build_clib')\n        build_ext = self.get_finalized_command('build_ext')\n        config = self.get_finalized_command('config')\n        build = self.get_finalized_command('build')\n        cmd_list = [self, config, build_clib, build_ext, build]\n        for a in ['fcompiler']:\n            l = []\n            for c in cmd_list:\n                v = getattr(c, a)\n                if v is not None:\n                    if not isinstance(v, str): v = v.compiler_type\n                    if v not in l: l.append(v)\n            if not l: v1 = None\n            else: v1 = l[0]\n            if len(l)>1:\n                log.warn('  commands have different --%s options: %s'\\\n                         ', using first in list as default' % (a, l))\n            if v1:\n                for c in cmd_list:\n                    if getattr(c, a) is None: setattr(c, a, v1)\n\n    def run(self):\n        # Do nothing.\n        return\n", "class_fn": true, "question_id": "numpy/numpy.distutils.command.config_compiler/config_fc", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/cpuinfo.py", "fn_id": "", "content": "class LinuxCPUInfo(CPUInfoBase):\n\n    info = None\n\n    def __init__(self):\n        if self.info is not None:\n            return\n        info = [ {} ]\n        ok, output = getoutput('uname -m')\n        if ok:\n            info[0]['uname_m'] = output.strip()\n        try:\n            fo = open('/proc/cpuinfo')\n        except OSError as e:\n            warnings.warn(str(e), UserWarning, stacklevel=2)\n        else:\n            for line in fo:\n                name_value = [s.strip() for s in line.split(':', 1)]\n                if len(name_value) != 2:\n                    continue\n                name, value = name_value\n                if not info or name in info[-1]: # next processor\n                    info.append({})\n                info[-1][name] = value\n            fo.close()\n        self.__class__.info = info\n\n    def _not_impl(self): pass\n\n    # Athlon\n\n    def _is_AMD(self):\n        return self.info[0]['vendor_id']=='AuthenticAMD'\n\n    def _is_AthlonK6_2(self):\n        return self._is_AMD() and self.info[0]['model'] == '2'\n\n    def _is_AthlonK6_3(self):\n        return self._is_AMD() and self.info[0]['model'] == '3'\n\n    def _is_AthlonK6(self):\n        return re.match(r'.*?AMD-K6', self.info[0]['model name']) is not None\n\n    def _is_AthlonK7(self):\n        return re.match(r'.*?AMD-K7', self.info[0]['model name']) is not None\n\n    def _is_AthlonMP(self):\n        return re.match(r'.*?Athlon\\(tm\\) MP\\b',\n                        self.info[0]['model name']) is not None\n\n    def _is_AMD64(self):\n        return self.is_AMD() and self.info[0]['family'] == '15'\n\n    def _is_Athlon64(self):\n        return re.match(r'.*?Athlon\\(tm\\) 64\\b',\n                        self.info[0]['model name']) is not None\n\n    def _is_AthlonHX(self):\n        return re.match(r'.*?Athlon HX\\b',\n                        self.info[0]['model name']) is not None\n\n    def _is_Opteron(self):\n        return re.match(r'.*?Opteron\\b',\n                        self.info[0]['model name']) is not None\n\n    def _is_Hammer(self):\n        return re.match(r'.*?Hammer\\b',\n                        self.info[0]['model name']) is not None\n\n    # Alpha\n\n    def _is_Alpha(self):\n        return self.info[0]['cpu']=='Alpha'\n\n    def _is_EV4(self):\n        return self.is_Alpha() and self.info[0]['cpu model'] == 'EV4'\n\n    def _is_EV5(self):\n        return self.is_Alpha() and self.info[0]['cpu model'] == 'EV5'\n\n    def _is_EV56(self):\n        return self.is_Alpha() and self.info[0]['cpu model'] == 'EV56'\n\n    def _is_PCA56(self):\n        return self.is_Alpha() and self.info[0]['cpu model'] == 'PCA56'\n\n    # Intel\n\n    #XXX\n    _is_i386 = _not_impl\n\n    def _is_Intel(self):\n        return self.info[0]['vendor_id']=='GenuineIntel'\n\n    def _is_i486(self):\n        return self.info[0]['cpu']=='i486'\n\n    def _is_i586(self):\n        return self.is_Intel() and self.info[0]['cpu family'] == '5'\n\n    def _is_i686(self):\n        return self.is_Intel() and self.info[0]['cpu family'] == '6'\n\n    def _is_Celeron(self):\n        return re.match(r'.*?Celeron',\n                        self.info[0]['model name']) is not None\n\n    def _is_Pentium(self):\n        return re.match(r'.*?Pentium',\n                        self.info[0]['model name']) is not None\n\n    def _is_PentiumII(self):\n        return re.match(r'.*?Pentium.*?II\\b',\n                        self.info[0]['model name']) is not None\n\n    def _is_PentiumPro(self):\n        return re.match(r'.*?PentiumPro\\b',\n                        self.info[0]['model name']) is not None\n\n    def _is_PentiumMMX(self):\n        return re.match(r'.*?Pentium.*?MMX\\b',\n                        self.info[0]['model name']) is not None\n\n    def _is_PentiumIII(self):\n        return re.match(r'.*?Pentium.*?III\\b',\n                        self.info[0]['model name']) is not None\n\n    def _is_PentiumIV(self):\n        return re.match(r'.*?Pentium.*?(IV|4)\\b',\n                        self.info[0]['model name']) is not None\n\n    def _is_PentiumM(self):\n        return re.match(r'.*?Pentium.*?M\\b',\n                        self.info[0]['model name']) is not None\n\n    def _is_Prescott(self):\n        return self.is_PentiumIV() and self.has_sse3()\n\n    def _is_Nocona(self):\n        return (self.is_Intel()\n                and (self.info[0]['cpu family'] == '6'\n                     or self.info[0]['cpu family'] == '15')\n                and (self.has_sse3() and not self.has_ssse3())\n                and re.match(r'.*?\\blm\\b', self.info[0]['flags']) is not None)\n\n    def _is_Core2(self):\n        return (self.is_64bit() and self.is_Intel() and\n                re.match(r'.*?Core\\(TM\\)2\\b',\n                         self.info[0]['model name']) is not None)\n\n    def _is_Itanium(self):\n        return re.match(r'.*?Itanium\\b',\n                        self.info[0]['family']) is not None\n\n    def _is_XEON(self):\n        return re.match(r'.*?XEON\\b',\n                        self.info[0]['model name'], re.IGNORECASE) is not None\n\n    _is_Xeon = _is_XEON\n\n    # Varia\n\n    def _is_singleCPU(self):\n        return len(self.info) == 1\n\n    def _getNCPUs(self):\n        return len(self.info)\n\n    def _has_fdiv_bug(self):\n        return self.info[0]['fdiv_bug']=='yes'\n\n    def _has_f00f_bug(self):\n        return self.info[0]['f00f_bug']=='yes'\n\n    def _has_mmx(self):\n        return re.match(r'.*?\\bmmx\\b', self.info[0]['flags']) is not None\n\n    def _has_sse(self):\n        return re.match(r'.*?\\bsse\\b', self.info[0]['flags']) is not None\n\n    def _has_sse2(self):\n        return re.match(r'.*?\\bsse2\\b', self.info[0]['flags']) is not None\n\n    def _has_sse3(self):\n        return re.match(r'.*?\\bpni\\b', self.info[0]['flags']) is not None\n\n    def _has_ssse3(self):\n        return re.match(r'.*?\\bssse3\\b', self.info[0]['flags']) is not None\n\n    def _has_3dnow(self):\n        return re.match(r'.*?\\b3dnow\\b', self.info[0]['flags']) is not None\n\n    def _has_3dnowext(self):\n        return re.match(r'.*?\\b3dnowext\\b', self.info[0]['flags']) is not None\n", "class_fn": true, "question_id": "numpy/numpy.distutils.cpuinfo/LinuxCPUInfo", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/extension.py", "fn_id": "", "content": "class Extension(old_Extension):\n    \"\"\"\n    Parameters\n    ----------\n    name : str\n        Extension name.\n    sources : list of str\n        List of source file locations relative to the top directory of\n        the package.\n    extra_compile_args : list of str\n        Extra command line arguments to pass to the compiler.\n    extra_f77_compile_args : list of str\n        Extra command line arguments to pass to the fortran77 compiler.\n    extra_f90_compile_args : list of str\n        Extra command line arguments to pass to the fortran90 compiler.\n    \"\"\"\n    def __init__(\n            self, name, sources,\n            include_dirs=None,\n            define_macros=None,\n            undef_macros=None,\n            library_dirs=None,\n            libraries=None,\n            runtime_library_dirs=None,\n            extra_objects=None,\n            extra_compile_args=None,\n            extra_link_args=None,\n            export_symbols=None,\n            swig_opts=None,\n            depends=None,\n            language=None,\n            f2py_options=None,\n            module_dirs=None,\n            extra_c_compile_args=None,\n            extra_cxx_compile_args=None,\n            extra_f77_compile_args=None,\n            extra_f90_compile_args=None,):\n\n        old_Extension.__init__(\n                self, name, [],\n                include_dirs=include_dirs,\n                define_macros=define_macros,\n                undef_macros=undef_macros,\n                library_dirs=library_dirs,\n                libraries=libraries,\n                runtime_library_dirs=runtime_library_dirs,\n                extra_objects=extra_objects,\n                extra_compile_args=extra_compile_args,\n                extra_link_args=extra_link_args,\n                export_symbols=export_symbols)\n\n        # Avoid assert statements checking that sources contains strings:\n        self.sources = sources\n\n        # Python 2.4 distutils new features\n        self.swig_opts = swig_opts or []\n        # swig_opts is assumed to be a list. Here we handle the case where it\n        # is specified as a string instead.\n        if isinstance(self.swig_opts, str):\n            import warnings\n            msg = \"swig_opts is specified as a string instead of a list\"\n            warnings.warn(msg, SyntaxWarning, stacklevel=2)\n            self.swig_opts = self.swig_opts.split()\n\n        # Python 2.3 distutils new features\n        self.depends = depends or []\n        self.language = language\n\n        # numpy_distutils features\n        self.f2py_options = f2py_options or []\n        self.module_dirs = module_dirs or []\n        self.extra_c_compile_args = extra_c_compile_args or []\n        self.extra_cxx_compile_args = extra_cxx_compile_args or []\n        self.extra_f77_compile_args = extra_f77_compile_args or []\n        self.extra_f90_compile_args = extra_f90_compile_args or []\n\n        return\n\n    def has_cxx_sources(self):\n        for source in self.sources:\n            if cxx_ext_re(str(source)):\n                return True\n        return False\n\n    def has_f2py_sources(self):\n        for source in self.sources:\n            if fortran_pyf_ext_re(source):\n                return True\n        return False\n", "class_fn": true, "question_id": "numpy/numpy.distutils.extension/Extension", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/absoft.py", "fn_id": "", "content": "class AbsoftFCompiler(FCompiler):\n\n    compiler_type = 'absoft'\n    description = 'Absoft Corp Fortran Compiler'\n    #version_pattern = r'FORTRAN 77 Compiler (?P<version>[^\\s*,]*).*?Absoft Corp'\n    version_pattern = r'(f90:.*?(Absoft Pro FORTRAN Version|FORTRAN 77 Compiler|Absoft Fortran Compiler Version|Copyright Absoft Corporation.*?Version))'+\\\n                       r' (?P<version>[^\\s*,]*)(.*?Absoft Corp|)'\n\n    # on windows: f90 -V -c dummy.f\n    # f90: Copyright Absoft Corporation 1994-1998 mV2; Cray Research, Inc. 1994-1996 CF90 (2.x.x.x  f36t87) Version 2.3 Wed Apr 19, 2006  13:05:16\n\n    # samt5735(8)$ f90 -V -c dummy.f\n    # f90: Copyright Absoft Corporation 1994-2002; Absoft Pro FORTRAN Version 8.0\n    # Note that fink installs g77 as f77, so need to use f90 for detection.\n\n    executables = {\n        'version_cmd'  : None,          # set by update_executables\n        'compiler_f77' : [\"f77\"],\n        'compiler_fix' : [\"f90\"],\n        'compiler_f90' : [\"f90\"],\n        'linker_so'    : [\"<F90>\"],\n        'archiver'     : [\"ar\", \"-cr\"],\n        'ranlib'       : [\"ranlib\"]\n        }\n\n    if os.name=='nt':\n        library_switch = '/out:'      #No space after /out:!\n\n    module_dir_switch = None\n    module_include_switch = '-p'\n\n    def update_executables(self):\n        f = cyg2win32(dummy_fortran_file())\n        self.executables['version_cmd'] = ['<F90>', '-V', '-c',\n                                           f+'.f', '-o', f+'.o']\n\n    def get_flags_linker_so(self):\n        if os.name=='nt':\n            opt = ['/dll']\n        # The \"-K shared\" switches are being left in for pre-9.0 versions\n        # of Absoft though I don't think versions earlier than 9 can\n        # actually be used to build shared libraries.  In fact, version\n        # 8 of Absoft doesn't recognize \"-K shared\" and will fail.\n        elif self.get_version() >= '9.0':\n            opt = ['-shared']\n        else:\n            opt = [\"-K\", \"shared\"]\n        return opt\n\n    def library_dir_option(self, dir):\n        if os.name=='nt':\n            return ['-link', '/PATH:%s' % (dir)]\n        return \"-L\" + dir\n\n    def library_option(self, lib):\n        if os.name=='nt':\n            return '%s.lib' % (lib)\n        return \"-l\" + lib\n\n    def get_library_dirs(self):\n        opt = FCompiler.get_library_dirs(self)\n        d = os.environ.get('ABSOFT')\n        if d:\n            if self.get_version() >= '10.0':\n                # use shared libraries, the static libraries were not compiled -fPIC\n                prefix = 'sh'\n            else:\n                prefix = ''\n            if cpu.is_64bit():\n                suffix = '64'\n            else:\n                suffix = ''\n            opt.append(os.path.join(d, '%slib%s' % (prefix, suffix)))\n        return opt\n\n    def get_libraries(self):\n        opt = FCompiler.get_libraries(self)\n        if self.get_version() >= '11.0':\n            opt.extend(['af90math', 'afio', 'af77math', 'amisc'])\n        elif self.get_version() >= '10.0':\n            opt.extend(['af90math', 'afio', 'af77math', 'U77'])\n        elif self.get_version() >= '8.0':\n            opt.extend(['f90math', 'fio', 'f77math', 'U77'])\n        else:\n            opt.extend(['fio', 'f90math', 'fmath', 'U77'])\n        if os.name =='nt':\n            opt.append('COMDLG32')\n        return opt\n\n    def get_flags(self):\n        opt = FCompiler.get_flags(self)\n        if os.name != 'nt':\n            opt.extend(['-s'])\n            if self.get_version():\n                if self.get_version()>='8.2':\n                    opt.append('-fpic')\n        return opt\n\n    def get_flags_f77(self):\n        opt = FCompiler.get_flags_f77(self)\n        opt.extend(['-N22', '-N90', '-N110'])\n        v = self.get_version()\n        if os.name == 'nt':\n            if v and v>='8.0':\n                opt.extend(['-f', '-N15'])\n        else:\n            opt.append('-f')\n            if v:\n                if v<='4.6':\n                    opt.append('-B108')\n                else:\n                    # Though -N15 is undocumented, it works with\n                    # Absoft 8.0 on Linux\n                    opt.append('-N15')\n        return opt\n\n    def get_flags_f90(self):\n        opt = FCompiler.get_flags_f90(self)\n        opt.extend([\"-YCFRL=1\", \"-YCOM_NAMES=LCS\", \"-YCOM_PFX\", \"-YEXT_PFX\",\n                    \"-YCOM_SFX=_\", \"-YEXT_SFX=_\", \"-YEXT_NAMES=LCS\"])\n        if self.get_version():\n            if self.get_version()>'4.6':\n                opt.extend([\"-YDEALLOC=ALL\"])\n        return opt\n\n    def get_flags_fix(self):\n        opt = FCompiler.get_flags_fix(self)\n        opt.extend([\"-YCFRL=1\", \"-YCOM_NAMES=LCS\", \"-YCOM_PFX\", \"-YEXT_PFX\",\n                    \"-YCOM_SFX=_\", \"-YEXT_SFX=_\", \"-YEXT_NAMES=LCS\"])\n        opt.extend([\"-f\", \"fixed\"])\n        return opt\n\n    def get_flags_opt(self):\n        opt = ['-O']\n        return opt\n", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.absoft/AbsoftFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/gnu.py", "fn_id": "", "content": "class GnuFCompiler(FCompiler):\n    compiler_type = 'gnu'\n    compiler_aliases = ('g77', )\n    description = 'GNU Fortran 77 compiler'\n\n    def gnu_version_match(self, version_string):\n        \"\"\"Handle the different versions of GNU fortran compilers\"\"\"\n        # Strip warning(s) that may be emitted by gfortran\n        while version_string.startswith('gfortran: warning'):\n            version_string =\\\n                version_string[version_string.find('\\n') + 1:].strip()\n\n        # Gfortran versions from after 2010 will output a simple string\n        # (usually \"x.y\", \"x.y.z\" or \"x.y.z-q\") for ``-dumpversion``; older\n        # gfortrans may still return long version strings (``-dumpversion`` was\n        # an alias for ``--version``)\n        if len(version_string) <= 20:\n            # Try to find a valid version string\n            m = re.search(r'([0-9.]+)', version_string)\n            if m:\n                # g77 provides a longer version string that starts with GNU\n                # Fortran\n                if version_string.startswith('GNU Fortran'):\n                    return ('g77', m.group(1))\n\n                # gfortran only outputs a version string such as #.#.#, so check\n                # if the match is at the start of the string\n                elif m.start() == 0:\n                    return ('gfortran', m.group(1))\n        else:\n            # Output probably from --version, try harder:\n            m = re.search(r'GNU Fortran\\s+95.*?([0-9-.]+)', version_string)\n            if m:\n                return ('gfortran', m.group(1))\n            m = re.search(\n                r'GNU Fortran.*?\\-?([0-9-.]+\\.[0-9-.]+)', version_string)\n            if m:\n                v = m.group(1)\n                if v.startswith('0') or v.startswith('2') or v.startswith('3'):\n                    # the '0' is for early g77's\n                    return ('g77', v)\n                else:\n                    # at some point in the 4.x series, the ' 95' was dropped\n                    # from the version string\n                    return ('gfortran', v)\n\n        # If still nothing, raise an error to make the problem easy to find.\n        err = 'A valid Fortran version was not found in this string:\\n'\n        raise ValueError(err + version_string)\n\n    def version_match(self, version_string):\n        v = self.gnu_version_match(version_string)\n        if not v or v[0] != 'g77':\n            return None\n        return v[1]\n\n    possible_executables = ['g77', 'f77']\n    executables = {\n        'version_cmd'  : [None, \"-dumpversion\"],\n        'compiler_f77' : [None, \"-g\", \"-Wall\", \"-fno-second-underscore\"],\n        'compiler_f90' : None,  # Use --fcompiler=gnu95 for f90 codes\n        'compiler_fix' : None,\n        'linker_so'    : [None, \"-g\", \"-Wall\"],\n        'archiver'     : [\"ar\", \"-cr\"],\n        'ranlib'       : [\"ranlib\"],\n        'linker_exe'   : [None, \"-g\", \"-Wall\"]\n    }\n    module_dir_switch = None\n    module_include_switch = None\n\n    # Cygwin: f771: warning: -fPIC ignored for target (all code is\n    # position independent)\n    if os.name != 'nt' and sys.platform != 'cygwin':\n        pic_flags = ['-fPIC']\n\n    # use -mno-cygwin for g77 when Python is not Cygwin-Python\n    if sys.platform == 'win32':\n        for key in ['version_cmd', 'compiler_f77', 'linker_so', 'linker_exe']:\n            executables[key].append('-mno-cygwin')\n\n    g2c = 'g2c'\n    suggested_f90_compiler = 'gnu95'\n\n    def get_flags_linker_so(self):\n        opt = self.linker_so[1:]\n        if sys.platform == 'darwin':\n            target = os.environ.get('MACOSX_DEPLOYMENT_TARGET', None)\n            # If MACOSX_DEPLOYMENT_TARGET is set, we simply trust the value\n            # and leave it alone.  But, distutils will complain if the\n            # environment's value is different from the one in the Python\n            # Makefile used to build Python.  We let distutils handle this\n            # error checking.\n            if not target:\n                # If MACOSX_DEPLOYMENT_TARGET is not set in the environment,\n                # we try to get it first from sysconfig and then\n                # fall back to setting it to 10.9 This is a reasonable default\n                # even when using the official Python dist and those derived\n                # from it.\n                import sysconfig\n                target = sysconfig.get_config_var('MACOSX_DEPLOYMENT_TARGET')\n                if not target:\n                    target = '10.9'\n                    s = f'Env. variable MACOSX_DEPLOYMENT_TARGET set to {target}'\n                    warnings.warn(s, stacklevel=2)\n                os.environ['MACOSX_DEPLOYMENT_TARGET'] = str(target)\n            opt.extend(['-undefined', 'dynamic_lookup', '-bundle'])\n        else:\n            opt.append(\"-shared\")\n        if sys.platform.startswith('sunos'):\n            # SunOS often has dynamically loaded symbols defined in the\n            # static library libg2c.a  The linker doesn't like this.  To\n            # ignore the problem, use the -mimpure-text flag.  It isn't\n            # the safest thing, but seems to work. 'man gcc' says:\n            # \".. Instead of using -mimpure-text, you should compile all\n            #  source code with -fpic or -fPIC.\"\n            opt.append('-mimpure-text')\n        return opt\n\n    def get_libgcc_dir(self):\n        try:\n            output = subprocess.check_output(self.compiler_f77 +\n                                            ['-print-libgcc-file-name'])\n        except (OSError, subprocess.CalledProcessError):\n            pass\n        else:\n            output = filepath_from_subprocess_output(output)\n            return os.path.dirname(output)\n        return None\n\n    def get_libgfortran_dir(self):\n        if sys.platform[:5] == 'linux':\n            libgfortran_name = 'libgfortran.so'\n        elif sys.platform == 'darwin':\n            libgfortran_name = 'libgfortran.dylib'\n        else:\n            libgfortran_name = None\n\n        libgfortran_dir = None\n        if libgfortran_name:\n            find_lib_arg = ['-print-file-name={0}'.format(libgfortran_name)]\n            try:\n                output = subprocess.check_output(\n                                       self.compiler_f77 + find_lib_arg)\n            except (OSError, subprocess.CalledProcessError):\n                pass\n            else:\n                output = filepath_from_subprocess_output(output)\n                libgfortran_dir = os.path.dirname(output)\n        return libgfortran_dir\n\n    def get_library_dirs(self):\n        opt = []\n        if sys.platform[:5] != 'linux':\n            d = self.get_libgcc_dir()\n            if d:\n                # if windows and not cygwin, libg2c lies in a different folder\n                if sys.platform == 'win32' and not d.startswith('/usr/lib'):\n                    d = os.path.normpath(d)\n                    path = os.path.join(d, \"lib%s.a\" % self.g2c)\n                    if not os.path.exists(path):\n                        root = os.path.join(d, *((os.pardir, ) * 4))\n                        d2 = os.path.abspath(os.path.join(root, 'lib'))\n                        path = os.path.join(d2, \"lib%s.a\" % self.g2c)\n                        if os.path.exists(path):\n                            opt.append(d2)\n                opt.append(d)\n        # For Macports / Linux, libgfortran and libgcc are not co-located\n        lib_gfortran_dir = self.get_libgfortran_dir()\n        if lib_gfortran_dir:\n            opt.append(lib_gfortran_dir)\n        return opt\n\n    def get_libraries(self):\n        opt = []\n        d = self.get_libgcc_dir()\n        if d is not None:\n            g2c = self.g2c + '-pic'\n            f = self.static_lib_format % (g2c, self.static_lib_extension)\n            if not os.path.isfile(os.path.join(d, f)):\n                g2c = self.g2c\n        else:\n            g2c = self.g2c\n\n        if g2c is not None:\n            opt.append(g2c)\n        c_compiler = self.c_compiler\n        if sys.platform == 'win32' and c_compiler and \\\n                c_compiler.compiler_type == 'msvc':\n            opt.append('gcc')\n        if sys.platform == 'darwin':\n            opt.append('cc_dynamic')\n        return opt\n\n    def get_flags_debug(self):\n        return ['-g']\n\n    def get_flags_opt(self):\n        v = self.get_version()\n        if v and v <= '3.3.3':\n            # With this compiler version building Fortran BLAS/LAPACK\n            # with -O3 caused failures in lib.lapack heevr,syevr tests.\n            opt = ['-O2']\n        else:\n            opt = ['-O3']\n        opt.append('-funroll-loops')\n        return opt\n\n    def _c_arch_flags(self):\n        \"\"\" Return detected arch flags from CFLAGS \"\"\"\n        import sysconfig\n        try:\n            cflags = sysconfig.get_config_vars()['CFLAGS']\n        except KeyError:\n            return []\n        arch_re = re.compile(r\"-arch\\s+(\\w+)\")\n        arch_flags = []\n        for arch in arch_re.findall(cflags):\n            arch_flags += ['-arch', arch]\n        return arch_flags\n\n    def get_flags_arch(self):\n        return []\n\n    def runtime_library_dir_option(self, dir):\n        if sys.platform == 'win32' or sys.platform == 'cygwin':\n            # Linux/Solaris/Unix support RPATH, Windows does not\n            raise NotImplementedError\n\n        # TODO: could use -Xlinker here, if it's supported\n        assert \",\" not in dir\n\n        if sys.platform == 'darwin':\n            return f'-Wl,-rpath,{dir}'\n        elif sys.platform.startswith(('aix', 'os400')):\n            # AIX RPATH is called LIBPATH\n            return f'-Wl,-blibpath:{dir}'\n        else:\n            return f'-Wl,-rpath={dir}'\n", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.gnu/GnuFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_config/config.py", "fn_id": "", "content": "class CallableDynamicDoc(Generic[T]):\n    def __init__(self, func: Callable[..., T], doc_tmpl: str) -> None:\n        self.__doc_tmpl__ = doc_tmpl\n        self.__func__ = func\n\n    def __call__(self, *args, **kwds) -> T:\n        return self.__func__(*args, **kwds)\n\n    # error: Signature of \"__doc__\" incompatible with supertype \"object\"\n    @property\n    def __doc__(self) -> str:  # type: ignore[override]\n        opts_desc = _describe_option(\"all\", _print_desc=False)\n        opts_list = pp_options_list(list(_registered_options.keys()))\n        return self.__doc_tmpl__.format(opts_desc=opts_desc, opts_list=opts_list)\n", "class_fn": true, "question_id": "pandas/pandas._config.config/CallableDynamicDoc", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_testing/__init__.py", "fn_id": "", "content": "class SubclassedDataFrame(DataFrame):\n    _metadata = [\"testattr\"]\n\n    @property\n    def _constructor(self):\n        return lambda *args, **kwargs: SubclassedDataFrame(*args, **kwargs)\n\n    @property\n    def _constructor_sliced(self):\n        return lambda *args, **kwargs: SubclassedSeries(*args, **kwargs)\n", "class_fn": true, "question_id": "pandas/pandas._testing/SubclassedDataFrame", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_typing.py", "fn_id": "", "content": "class ReadBuffer(BaseBuffer, Protocol[AnyStr_co]):\n    def read(self, __n: int = ...) -> AnyStr_co:\n        # for BytesIOWrapper, gzip.GzipFile, bz2.BZ2File\n        ...\n", "class_fn": true, "question_id": "pandas/pandas._typing/ReadBuffer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_typing.py", "fn_id": "", "content": "class SequenceNotStr(Protocol[_T_co]):\n    @overload\n    def __getitem__(self, index: SupportsIndex, /) -> _T_co:\n        ...\n\n    @overload\n    def __getitem__(self, index: slice, /) -> Sequence[_T_co]:\n        ...\n\n    def __contains__(self, value: object, /) -> bool:\n        ...\n\n    def __len__(self) -> int:\n        ...\n\n    def __iter__(self) -> Iterator[_T_co]:\n        ...\n\n    def index(self, value: Any, /, start: int = 0, stop: int = ...) -> int:\n        ...\n\n    def count(self, value: Any, /) -> int:\n        ...\n\n    def __reversed__(self) -> Iterator[_T_co]:\n        ...\n", "class_fn": true, "question_id": "pandas/pandas._typing/SequenceNotStr", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/compat/compressors.py", "fn_id": "", "content": "    class BZ2File(bz2.BZ2File):\n        if not PY310:\n\n            def write(self, b) -> int:\n                # Workaround issue where `bz2.BZ2File` expects `len`\n                # to return the number of bytes in `b` by converting\n                # `b` into something that meets that constraint with\n                # minimal copying.\n                #\n                # Note: This is fixed in Python 3.10.\n                return super().write(flatten_buffer(b))\n", "class_fn": true, "question_id": "pandas/pandas.compat.compressors/BZ2File", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/compat/pickle_compat.py", "fn_id": "", "content": "class Unpickler(pkl._Unpickler):\n    def find_class(self, module, name):\n        # override superclass\n        key = (module, name)\n        module, name = _class_locations_map.get(key, key)\n        return super().find_class(module, name)\n", "class_fn": true, "question_id": "pandas/pandas.compat.pickle_compat/Unpickler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/apply.py", "fn_id": "", "content": "class NDFrameApply(Apply):\n    \"\"\"\n    Methods shared by FrameApply and SeriesApply but\n    not GroupByApply or ResamplerWindowApply\n    \"\"\"\n\n    obj: DataFrame | Series\n\n    @property\n    def index(self) -> Index:\n        return self.obj.index\n\n    @property\n    def agg_axis(self) -> Index:\n        return self.obj._get_agg_axis(self.axis)\n\n    def agg_or_apply_list_like(\n        self, op_name: Literal[\"agg\", \"apply\"]\n    ) -> DataFrame | Series:\n        obj = self.obj\n        kwargs = self.kwargs\n\n        if op_name == \"apply\":\n            if isinstance(self, FrameApply):\n                by_row = self.by_row\n\n            elif isinstance(self, SeriesApply):\n                by_row = \"_compat\" if self.by_row else False\n            else:\n                by_row = False\n            kwargs = {**kwargs, \"by_row\": by_row}\n\n        if getattr(obj, \"axis\", 0) == 1:\n            raise NotImplementedError(\"axis other than 0 is not supported\")\n\n        keys, results = self.compute_list_like(op_name, obj, kwargs)\n        result = self.wrap_results_list_like(keys, results)\n        return result\n\n    def agg_or_apply_dict_like(\n        self, op_name: Literal[\"agg\", \"apply\"]\n    ) -> DataFrame | Series:\n        assert op_name in [\"agg\", \"apply\"]\n        obj = self.obj\n\n        kwargs = {}\n        if op_name == \"apply\":\n            by_row = \"_compat\" if self.by_row else False\n            kwargs.update({\"by_row\": by_row})\n\n        if getattr(obj, \"axis\", 0) == 1:\n            raise NotImplementedError(\"axis other than 0 is not supported\")\n\n        selection = None\n        result_index, result_data = self.compute_dict_like(\n            op_name, obj, selection, kwargs\n        )\n        result = self.wrap_results_dict_like(obj, result_index, result_data)\n        return result\n", "class_fn": true, "question_id": "pandas/pandas.core.apply/NDFrameApply", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/base.py", "fn_id": "", "content": "class ExtensionArraySupportsAnyAll(ExtensionArray):\n    def any(self, *, skipna: bool = True) -> bool:\n        raise AbstractMethodError(self)\n\n    def all(self, *, skipna: bool = True) -> bool:\n        raise AbstractMethodError(self)\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.base/ExtensionArraySupportsAnyAll", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/floating.py", "fn_id": "", "content": "class FloatingArray(NumericArray):\n    \"\"\"\n    Array of floating (optional missing) values.\n\n    .. warning::\n\n       FloatingArray is currently experimental, and its API or internal\n       implementation may change without warning. Especially the behaviour\n       regarding NaN (distinct from NA missing values) is subject to change.\n\n    We represent a FloatingArray with 2 numpy arrays:\n\n    - data: contains a numpy float array of the appropriate dtype\n    - mask: a boolean array holding a mask on the data, True is missing\n\n    To construct an FloatingArray from generic array-like input, use\n    :func:`pandas.array` with one of the float dtypes (see examples).\n\n    See :ref:`integer_na` for more.\n\n    Parameters\n    ----------\n    values : numpy.ndarray\n        A 1-d float-dtype array.\n    mask : numpy.ndarray\n        A 1-d boolean-dtype array indicating missing values.\n    copy : bool, default False\n        Whether to copy the `values` and `mask`.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Returns\n    -------\n    FloatingArray\n\n    Examples\n    --------\n    Create an FloatingArray with :func:`pandas.array`:\n\n    >>> pd.array([0.1, None, 0.3], dtype=pd.Float32Dtype())\n    <FloatingArray>\n    [0.1, <NA>, 0.3]\n    Length: 3, dtype: Float32\n\n    String aliases for the dtypes are also available. They are capitalized.\n\n    >>> pd.array([0.1, None, 0.3], dtype=\"Float32\")\n    <FloatingArray>\n    [0.1, <NA>, 0.3]\n    Length: 3, dtype: Float32\n    \"\"\"\n\n    _dtype_cls = FloatingDtype\n\n    # The value used to fill '_data' to avoid upcasting\n    _internal_fill_value = np.nan\n    # Fill values used for any/all\n    # Incompatible types in assignment (expression has type \"float\", base class\n    # \"BaseMaskedArray\" defined the type as \"<typing special form>\")\n    _truthy_value = 1.0  # type: ignore[assignment]\n    _falsey_value = 0.0  # type: ignore[assignment]\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.floating/FloatingArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/integer.py", "fn_id": "", "content": "@register_extension_dtype\nclass Int32Dtype(IntegerDtype):\n    type = np.int32\n    name: ClassVar[str] = \"Int32\"\n    __doc__ = _dtype_docstring.format(dtype=\"int32\")\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.integer/Int32Dtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/integer.py", "fn_id": "", "content": "class IntegerArray(NumericArray):\n    \"\"\"\n    Array of integer (optional missing) values.\n\n    Uses :attr:`pandas.NA` as the missing value.\n\n    .. warning::\n\n       IntegerArray is currently experimental, and its API or internal\n       implementation may change without warning.\n\n    We represent an IntegerArray with 2 numpy arrays:\n\n    - data: contains a numpy integer array of the appropriate dtype\n    - mask: a boolean array holding a mask on the data, True is missing\n\n    To construct an IntegerArray from generic array-like input, use\n    :func:`pandas.array` with one of the integer dtypes (see examples).\n\n    See :ref:`integer_na` for more.\n\n    Parameters\n    ----------\n    values : numpy.ndarray\n        A 1-d integer-dtype array.\n    mask : numpy.ndarray\n        A 1-d boolean-dtype array indicating missing values.\n    copy : bool, default False\n        Whether to copy the `values` and `mask`.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Returns\n    -------\n    IntegerArray\n\n    Examples\n    --------\n    Create an IntegerArray with :func:`pandas.array`.\n\n    >>> int_array = pd.array([1, None, 3], dtype=pd.Int32Dtype())\n    >>> int_array\n    <IntegerArray>\n    [1, <NA>, 3]\n    Length: 3, dtype: Int32\n\n    String aliases for the dtypes are also available. They are capitalized.\n\n    >>> pd.array([1, None, 3], dtype='Int32')\n    <IntegerArray>\n    [1, <NA>, 3]\n    Length: 3, dtype: Int32\n\n    >>> pd.array([1, None, 3], dtype='UInt16')\n    <IntegerArray>\n    [1, <NA>, 3]\n    Length: 3, dtype: UInt16\n    \"\"\"\n\n    _dtype_cls = IntegerDtype\n\n    # The value used to fill '_data' to avoid upcasting\n    _internal_fill_value = 1\n    # Fill values used for any/all\n    # Incompatible types in assignment (expression has type \"int\", base class\n    # \"BaseMaskedArray\" defined the type as \"<typing special form>\")\n    _truthy_value = 1  # type: ignore[assignment]\n    _falsey_value = 0  # type: ignore[assignment]\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.integer/IntegerArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/integer.py", "fn_id": "", "content": "@register_extension_dtype\nclass UInt32Dtype(IntegerDtype):\n    type = np.uint32\n    name: ClassVar[str] = \"UInt32\"\n    __doc__ = _dtype_docstring.format(dtype=\"uint32\")\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.integer/UInt32Dtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/numeric.py", "fn_id": "", "content": "class NumericArray(BaseMaskedArray):\n    \"\"\"\n    Base class for IntegerArray and FloatingArray.\n    \"\"\"\n\n    _dtype_cls: type[NumericDtype]\n\n    def __init__(\n        self, values: np.ndarray, mask: npt.NDArray[np.bool_], copy: bool = False\n    ) -> None:\n        checker = self._dtype_cls._checker\n        if not (isinstance(values, np.ndarray) and checker(values.dtype)):\n            descr = (\n                \"floating\"\n                if self._dtype_cls.kind == \"f\"  # type: ignore[comparison-overlap]\n                else \"integer\"\n            )\n            raise TypeError(\n                f\"values should be {descr} numpy array. Use \"\n                \"the 'pd.array' function instead\"\n            )\n        if values.dtype == np.float16:\n            # If we don't raise here, then accessing self.dtype would raise\n            raise TypeError(\"FloatingArray does not support np.float16 dtype.\")\n\n        super().__init__(values, mask, copy=copy)\n\n    @cache_readonly\n    def dtype(self) -> NumericDtype:\n        mapping = self._dtype_cls._get_dtype_mapping()\n        return mapping[self._data.dtype]\n\n    @classmethod\n    def _coerce_to_array(\n        cls, value, *, dtype: DtypeObj, copy: bool = False\n    ) -> tuple[np.ndarray, np.ndarray]:\n        dtype_cls = cls._dtype_cls\n        default_dtype = dtype_cls._default_np_dtype\n        values, mask, _, _ = _coerce_to_data_and_mask(\n            value, dtype, copy, dtype_cls, default_dtype\n        )\n        return values, mask\n\n    @classmethod\n    def _from_sequence_of_strings(\n        cls, strings, *, dtype: Dtype | None = None, copy: bool = False\n    ) -> Self:\n        from pandas.core.tools.numeric import to_numeric\n\n        scalars = to_numeric(strings, errors=\"raise\", dtype_backend=\"numpy_nullable\")\n        return cls._from_sequence(scalars, dtype=dtype, copy=copy)\n\n    _HANDLED_TYPES = (np.ndarray, numbers.Number)\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.numeric/NumericArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/base.py", "fn_id": "", "content": "class NoNewAttributesMixin:\n    \"\"\"\n    Mixin which prevents adding new attributes.\n\n    Prevents additional attributes via xxx.attribute = \"something\" after a\n    call to `self.__freeze()`. Mainly used to prevent the user from using\n    wrong attributes on an accessor (`Series.cat/.str/.dt`).\n\n    If you really want to add a new attribute at a later time, you need to use\n    `object.__setattr__(self, key, value)`.\n    \"\"\"\n\n    def _freeze(self) -> None:\n        \"\"\"\n        Prevents setting additional attributes.\n        \"\"\"\n        object.__setattr__(self, \"__frozen\", True)\n\n    # prevent adding any attribute via s.xxx.new_attribute = ...\n    def __setattr__(self, key: str, value) -> None:\n        # _cache is used by a decorator\n        # We need to check both 1.) cls.__dict__ and 2.) getattr(self, key)\n        # because\n        # 1.) getattr is false for attributes that raise errors\n        # 2.) cls.__dict__ doesn't traverse into base classes\n        if getattr(self, \"__frozen\", False) and not (\n            key == \"_cache\"\n            or key in type(self).__dict__\n            or getattr(self, key, None) is not None\n        ):\n            raise AttributeError(f\"You cannot add any new attribute '{key}'\")\n        object.__setattr__(self, key, value)\n", "class_fn": true, "question_id": "pandas/pandas.core.base/NoNewAttributesMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/engines.py", "fn_id": "", "content": "class NumExprEngine(AbstractEngine):\n    \"\"\"NumExpr engine class\"\"\"\n\n    has_neg_frac = True\n\n    def _evaluate(self):\n        import numexpr as ne\n\n        # convert the expression to a valid numexpr expression\n        s = self.convert()\n\n        env = self.expr.env\n        scope = env.full_scope\n        _check_ne_builtin_clash(self.expr)\n        return ne.evaluate(s, local_dict=scope)\n", "class_fn": true, "question_id": "pandas/pandas.core.computation.engines/NumExprEngine", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/expr.py", "fn_id": "", "content": "@disallow(\n    (_unsupported_nodes | _python_not_supported)\n    - (_boolop_nodes | frozenset([\"BoolOp\", \"Attribute\", \"In\", \"NotIn\", \"Tuple\"]))\n)\nclass PandasExprVisitor(BaseExprVisitor):\n    def __init__(\n        self,\n        env,\n        engine,\n        parser,\n        preparser=partial(\n            _preparse,\n            f=_compose(_replace_locals, _replace_booleans, clean_backtick_quoted_toks),\n        ),\n    ) -> None:\n        super().__init__(env, engine, parser, preparser)\n", "class_fn": true, "question_id": "pandas/pandas.core.computation.expr/PandasExprVisitor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/ops.py", "fn_id": "", "content": "class FuncNode:\n    def __init__(self, name: str) -> None:\n        if name not in MATHOPS:\n            raise ValueError(f'\"{name}\" is not a supported function')\n        self.name = name\n        self.func = getattr(np, name)\n\n    def __call__(self, *args) -> MathCall:\n        return MathCall(self, args)\n", "class_fn": true, "question_id": "pandas/pandas.core.computation.ops/FuncNode", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/ops.py", "fn_id": "", "content": "class UnaryOp(Op):\n    \"\"\"\n    Hold a unary operator and its operands.\n\n    Parameters\n    ----------\n    op : str\n        The token used to represent the operator.\n    operand : Term or Op\n        The Term or Op operand to the operator.\n\n    Raises\n    ------\n    ValueError\n        * If no function associated with the passed operator token is found.\n    \"\"\"\n\n    def __init__(self, op: Literal[\"+\", \"-\", \"~\", \"not\"], operand) -> None:\n        super().__init__(op, (operand,))\n        self.operand = operand\n\n        try:\n            self.func = _unary_ops_dict[op]\n        except KeyError as err:\n            raise ValueError(\n                f\"Invalid unary operator {repr(op)}, \"\n                f\"valid operators are {UNARY_OPS_SYMS}\"\n            ) from err\n\n    def __call__(self, env) -> MathCall:\n        operand = self.operand(env)\n        # error: Cannot call function of unknown type\n        return self.func(operand)  # type: ignore[operator]\n\n    def __repr__(self) -> str:\n        return pprint_thing(f\"{self.op}({self.operand})\")\n\n    @property\n    def return_type(self) -> np.dtype:\n        operand = self.operand\n        if operand.return_type == np.dtype(\"bool\"):\n            return np.dtype(\"bool\")\n        if isinstance(operand, Op) and (\n            operand.op in _cmp_ops_dict or operand.op in _bool_ops_dict\n        ):\n            return np.dtype(\"bool\")\n        return np.dtype(\"int\")\n", "class_fn": true, "question_id": "pandas/pandas.core.computation.ops/UnaryOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/pytables.py", "fn_id": "", "content": "class FilterBinOp(BinOp):\n    filter: tuple[Any, Any, Index] | None = None\n\n    def __repr__(self) -> str:\n        if self.filter is None:\n            return \"Filter: Not Initialized\"\n        return pprint_thing(f\"[Filter : [{self.filter[0]}] -> [{self.filter[1]}]\")\n\n    def invert(self) -> Self:\n        \"\"\"invert the filter\"\"\"\n        if self.filter is not None:\n            self.filter = (\n                self.filter[0],\n                self.generate_filter_op(invert=True),\n                self.filter[2],\n            )\n        return self\n\n    def format(self):\n        \"\"\"return the actual filter format\"\"\"\n        return [self.filter]\n\n    # error: Signature of \"evaluate\" incompatible with supertype \"BinOp\"\n    def evaluate(self) -> Self | None:  # type: ignore[override]\n        if not self.is_valid:\n            raise ValueError(f\"query term is not valid [{self}]\")\n\n        rhs = self.conform(self.rhs)\n        values = list(rhs)\n\n        if self.is_in_table:\n            # if too many values to create the expression, use a filter instead\n            if self.op in [\"==\", \"!=\"] and len(values) > self._max_selectors:\n                filter_op = self.generate_filter_op()\n                self.filter = (self.lhs, filter_op, Index(values))\n\n                return self\n            return None\n\n        # equality conditions\n        if self.op in [\"==\", \"!=\"]:\n            filter_op = self.generate_filter_op()\n            self.filter = (self.lhs, filter_op, Index(values))\n\n        else:\n            raise TypeError(\n                f\"passing a filterable condition to a non-table indexer [{self}]\"\n            )\n\n        return self\n\n    def generate_filter_op(self, invert: bool = False):\n        if (self.op == \"!=\" and not invert) or (self.op == \"==\" and invert):\n            return lambda axis, vals: ~axis.isin(vals)\n        else:\n            return lambda axis, vals: axis.isin(vals)\n", "class_fn": true, "question_id": "pandas/pandas.core.computation.pytables/FilterBinOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/pytables.py", "fn_id": "", "content": "class PyTablesScope(_scope.Scope):\n    __slots__ = (\"queryables\",)\n\n    queryables: dict[str, Any]\n\n    def __init__(\n        self,\n        level: int,\n        global_dict=None,\n        local_dict=None,\n        queryables: dict[str, Any] | None = None,\n    ) -> None:\n        super().__init__(level + 1, global_dict=global_dict, local_dict=local_dict)\n        self.queryables = queryables or {}\n", "class_fn": true, "question_id": "pandas/pandas.core.computation.pytables/PyTablesScope", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/pytables.py", "fn_id": "", "content": "class UnaryOp(ops.UnaryOp):\n    def prune(self, klass):\n        if self.op != \"~\":\n            raise NotImplementedError(\"UnaryOp only support invert type ops\")\n\n        operand = self.operand\n        operand = operand.prune(klass)\n\n        if operand is not None and (\n            issubclass(klass, ConditionBinOp)\n            and operand.condition is not None\n            or not issubclass(klass, ConditionBinOp)\n            and issubclass(klass, FilterBinOp)\n            and operand.filter is not None\n        ):\n            return operand.invert()\n        return None\n", "class_fn": true, "question_id": "pandas/pandas.core.computation.pytables/UnaryOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/dtypes/dtypes.py", "fn_id": "", "content": "class PandasExtensionDtype(ExtensionDtype):\n    \"\"\"\n    A np.dtype duck-typed class, suitable for holding a custom dtype.\n\n    THIS IS NOT A REAL NUMPY DTYPE\n    \"\"\"\n\n    type: Any\n    kind: Any\n    # The Any type annotations above are here only because mypy seems to have a\n    # problem dealing with multiple inheritance from PandasExtensionDtype\n    # and ExtensionDtype's @properties in the subclasses below. The kind and\n    # type variables in those subclasses are explicitly typed below.\n    subdtype = None\n    str: str_type\n    num = 100\n    shape: tuple[int, ...] = ()\n    itemsize = 8\n    base: DtypeObj | None = None\n    isbuiltin = 0\n    isnative = 0\n    _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}\n\n    def __repr__(self) -> str_type:\n        \"\"\"\n        Return a string representation for a particular object.\n        \"\"\"\n        return str(self)\n\n    def __hash__(self) -> int:\n        raise NotImplementedError(\"sub-classes should implement an __hash__ method\")\n\n    def __getstate__(self) -> dict[str_type, Any]:\n        # pickle support; we don't want to pickle the cache\n        return {k: getattr(self, k, None) for k in self._metadata}\n\n    @classmethod\n    def reset_cache(cls) -> None:\n        \"\"\"clear the cache\"\"\"\n        cls._cache_dtypes = {}\n", "class_fn": true, "question_id": "pandas/pandas.core.dtypes.dtypes/PandasExtensionDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/indexing.py", "fn_id": "", "content": "class GroupByNthSelector:\n    \"\"\"\n    Dynamically substituted for GroupBy.nth to enable both call and index\n    \"\"\"\n\n    def __init__(self, groupby_object: groupby.GroupBy) -> None:\n        self.groupby_object = groupby_object\n\n    def __call__(\n        self,\n        n: PositionalIndexer | tuple,\n        dropna: Literal[\"any\", \"all\", None] = None,\n    ) -> DataFrame | Series:\n        return self.groupby_object._nth(n, dropna)\n\n    def __getitem__(self, n: PositionalIndexer | tuple) -> DataFrame | Series:\n        return self.groupby_object._nth(n)\n", "class_fn": true, "question_id": "pandas/pandas.core.groupby.indexing/GroupByNthSelector", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/ops.py", "fn_id": "", "content": "class FrameSplitter(DataSplitter):\n    def _chop(self, sdata: DataFrame, slice_obj: slice) -> DataFrame:\n        # Fastpath equivalent to:\n        # if self.axis == 0:\n        #     return sdata.iloc[slice_obj]\n        # else:\n        #     return sdata.iloc[:, slice_obj]\n        mgr = sdata._mgr.get_slice(slice_obj, axis=1 - self.axis)\n        df = sdata._constructor_from_mgr(mgr, axes=mgr.axes)\n        return df.__finalize__(sdata, method=\"groupby\")\n", "class_fn": true, "question_id": "pandas/pandas.core.groupby.ops/FrameSplitter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexers/objects.py", "fn_id": "", "content": "class ExpandingIndexer(BaseIndexer):\n    \"\"\"Calculate expanding window bounds, mimicking df.expanding()\"\"\"\n\n    @Appender(get_window_bounds_doc)\n    def get_window_bounds(\n        self,\n        num_values: int = 0,\n        min_periods: int | None = None,\n        center: bool | None = None,\n        closed: str | None = None,\n        step: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        return (\n            np.zeros(num_values, dtype=np.int64),\n            np.arange(1, num_values + 1, dtype=np.int64),\n        )\n", "class_fn": true, "question_id": "pandas/pandas.core.indexers.objects/ExpandingIndexer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexers/objects.py", "fn_id": "", "content": "class FixedWindowIndexer(BaseIndexer):\n    \"\"\"Creates window boundaries that are of fixed length.\"\"\"\n\n    @Appender(get_window_bounds_doc)\n    def get_window_bounds(\n        self,\n        num_values: int = 0,\n        min_periods: int | None = None,\n        center: bool | None = None,\n        closed: str | None = None,\n        step: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if center or self.window_size == 0:\n            offset = (self.window_size - 1) // 2\n        else:\n            offset = 0\n\n        end = np.arange(1 + offset, num_values + 1 + offset, step, dtype=\"int64\")\n        start = end - self.window_size\n        if closed in [\"left\", \"both\"]:\n            start -= 1\n        if closed in [\"left\", \"neither\"]:\n            end -= 1\n\n        end = np.clip(end, 0, num_values)\n        start = np.clip(start, 0, num_values)\n\n        return start, end\n", "class_fn": true, "question_id": "pandas/pandas.core.indexers.objects/FixedWindowIndexer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/accessors.py", "fn_id": "", "content": "@delegate_names(\n    delegate=PeriodArray, accessors=PeriodArray._datetimelike_ops, typ=\"property\"\n)\n@delegate_names(\n    delegate=PeriodArray, accessors=PeriodArray._datetimelike_methods, typ=\"method\"\n)\nclass PeriodProperties(Properties):\n    \"\"\"\n    Accessor object for datetimelike properties of the Series values.\n\n    Returns a Series indexed like the original Series.\n    Raises TypeError if the Series does not contain datetimelike values.\n\n    Examples\n    --------\n    >>> seconds_series = pd.Series(\n    ...     pd.period_range(\n    ...         start=\"2000-01-01 00:00:00\", end=\"2000-01-01 00:00:03\", freq=\"s\"\n    ...     )\n    ... )\n    >>> seconds_series\n    0    2000-01-01 00:00:00\n    1    2000-01-01 00:00:01\n    2    2000-01-01 00:00:02\n    3    2000-01-01 00:00:03\n    dtype: period[s]\n    >>> seconds_series.dt.second\n    0    0\n    1    1\n    2    2\n    3    3\n    dtype: int64\n\n    >>> hours_series = pd.Series(\n    ...     pd.period_range(start=\"2000-01-01 00:00\", end=\"2000-01-01 03:00\", freq=\"h\")\n    ... )\n    >>> hours_series\n    0    2000-01-01 00:00\n    1    2000-01-01 01:00\n    2    2000-01-01 02:00\n    3    2000-01-01 03:00\n    dtype: period[h]\n    >>> hours_series.dt.hour\n    0    0\n    1    1\n    2    2\n    3    3\n    dtype: int64\n\n    >>> quarters_series = pd.Series(\n    ...     pd.period_range(start=\"2000-01-01\", end=\"2000-12-31\", freq=\"Q-DEC\")\n    ... )\n    >>> quarters_series\n    0    2000Q1\n    1    2000Q2\n    2    2000Q3\n    3    2000Q4\n    dtype: period[Q-DEC]\n    >>> quarters_series.dt.quarter\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n    \"\"\"\n", "class_fn": true, "question_id": "pandas/pandas.core.indexes.accessors/PeriodProperties", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/multi.py", "fn_id": "", "content": "class MultiIndexPyIntEngine(libindex.BaseMultiIndexCodesEngine, libindex.ObjectEngine):\n    \"\"\"\n    This class manages those (extreme) cases in which the number of possible\n    label combinations overflows the 64 bits integers, and uses an ObjectEngine\n    containing Python integers.\n    \"\"\"\n\n    _base = libindex.ObjectEngine\n\n    def _codes_to_ints(self, codes):\n        \"\"\"\n        Transform combination(s) of uint64 in one Python integer (each), in a\n        strictly monotonic way (i.e. respecting the lexicographic order of\n        integer combinations): see BaseMultiIndexCodesEngine documentation.\n\n        Parameters\n        ----------\n        codes : 1- or 2-dimensional array of dtype uint64\n            Combinations of integers (one per row)\n\n        Returns\n        -------\n        int, or 1-dimensional array of dtype object\n            Integer(s) representing one combination (each).\n        \"\"\"\n        # Shift the representation of each level by the pre-calculated number\n        # of bits. Since this can overflow uint64, first make sure we are\n        # working with Python integers:\n        codes = codes.astype(\"object\") << self.offsets\n\n        # Now sum and OR are in fact interchangeable. This is a simple\n        # composition of the (disjunct) significant bits of each level (i.e.\n        # each column in \"codes\") in a single positive integer (per row):\n        if codes.ndim == 1:\n            # Single key\n            return np.bitwise_or.reduce(codes)\n\n        # Multiple keys\n        return np.bitwise_or.reduce(codes, axis=1)\n", "class_fn": true, "question_id": "pandas/pandas.core.indexes.multi/MultiIndexPyIntEngine", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexing.py", "fn_id": "", "content": "class _ScalarAccessIndexer(NDFrameIndexerBase):\n    \"\"\"\n    Access scalars quickly.\n    \"\"\"\n\n    # sub-classes need to set _takeable\n    _takeable: bool\n\n    def _convert_key(self, key):\n        raise AbstractMethodError(self)\n\n    def __getitem__(self, key):\n        if not isinstance(key, tuple):\n            # we could have a convertible item here (e.g. Timestamp)\n            if not is_list_like_indexer(key):\n                key = (key,)\n            else:\n                raise ValueError(\"Invalid call for scalar access (getting)!\")\n\n        key = self._convert_key(key)\n        return self.obj._get_value(*key, takeable=self._takeable)\n\n    def __setitem__(self, key, value) -> None:\n        if isinstance(key, tuple):\n            key = tuple(com.apply_if_callable(x, self.obj) for x in key)\n        else:\n            # scalar callable may return tuple\n            key = com.apply_if_callable(key, self.obj)\n\n        if not isinstance(key, tuple):\n            key = _tuplify(self.ndim, key)\n        key = list(self._convert_key(key))\n        if len(key) != self.ndim:\n            raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n\n        self.obj._set_value(*key, value=value, takeable=self._takeable)\n", "class_fn": true, "question_id": "pandas/pandas.core.indexing/_ScalarAccessIndexer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/interchange/buffer.py", "fn_id": "", "content": "class PandasBufferPyarrow(Buffer):\n    \"\"\"\n    Data in the buffer is guaranteed to be contiguous in memory.\n    \"\"\"\n\n    def __init__(\n        self,\n        buffer: pa.Buffer,\n        *,\n        length: int,\n    ) -> None:\n        \"\"\"\n        Handle pyarrow chunked arrays.\n        \"\"\"\n        self._buffer = buffer\n        self._length = length\n\n    @property\n    def bufsize(self) -> int:\n        \"\"\"\n        Buffer size in bytes.\n        \"\"\"\n        return self._buffer.size\n\n    @property\n    def ptr(self) -> int:\n        \"\"\"\n        Pointer to start of the buffer as an integer.\n        \"\"\"\n        return self._buffer.address\n\n    def __dlpack__(self) -> Any:\n        \"\"\"\n        Represent this structure as DLPack interface.\n        \"\"\"\n        raise NotImplementedError()\n\n    def __dlpack_device__(self) -> tuple[DlpackDeviceType, int | None]:\n        \"\"\"\n        Device type and device ID for where the data in the buffer resides.\n        \"\"\"\n        return (DlpackDeviceType.CPU, None)\n\n    def __repr__(self) -> str:\n        return (\n            \"PandasBuffer[pyarrow](\"\n            + str(\n                {\n                    \"bufsize\": self.bufsize,\n                    \"ptr\": self.ptr,\n                    \"device\": \"CPU\",\n                }\n            )\n            + \")\"\n        )\n", "class_fn": true, "question_id": "pandas/pandas.core.interchange.buffer/PandasBufferPyarrow", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/interchange/dataframe_protocol.py", "fn_id": "", "content": "class ColumnBuffers(TypedDict):\n    # first element is a buffer containing the column data;\n    # second element is the data buffer's associated dtype\n    data: tuple[Buffer, Any]\n\n    # first element is a buffer containing mask values indicating missing data;\n    # second element is the mask value buffer's associated dtype.\n    # None if the null representation is not a bit or byte mask\n    validity: tuple[Buffer, Any] | None\n\n    # first element is a buffer containing the offset values for\n    # variable-size binary data (e.g., variable-length strings);\n    # second element is the offsets buffer's associated dtype.\n    # None if the data buffer does not have an associated offsets buffer\n    offsets: tuple[Buffer, Any] | None\n", "class_fn": true, "question_id": "pandas/pandas.core.interchange.dataframe_protocol/ColumnBuffers", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/interchange/dataframe_protocol.py", "fn_id": "", "content": "class DtypeKind(enum.IntEnum):\n    \"\"\"\n    Integer enum for data types.\n\n    Attributes\n    ----------\n    INT : int\n        Matches to signed integer data type.\n    UINT : int\n        Matches to unsigned integer data type.\n    FLOAT : int\n        Matches to floating point data type.\n    BOOL : int\n        Matches to boolean data type.\n    STRING : int\n        Matches to string data type (UTF-8 encoded).\n    DATETIME : int\n        Matches to datetime data type.\n    CATEGORICAL : int\n        Matches to categorical data type.\n    \"\"\"\n\n    INT = 0\n    UINT = 1\n    FLOAT = 2\n    BOOL = 20\n    STRING = 21  # UTF-8\n    DATETIME = 22\n    CATEGORICAL = 23\n", "class_fn": true, "question_id": "pandas/pandas.core.interchange.dataframe_protocol/DtypeKind", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/internals/array_manager.py", "fn_id": "", "content": "class NullArrayProxy:\n    \"\"\"\n    Proxy object for an all-NA array.\n\n    Only stores the length of the array, and not the dtype. The dtype\n    will only be known when actually concatenating (after determining the\n    common dtype, for which this proxy is ignored).\n    Using this object avoids that the internals/concat.py needs to determine\n    the proper dtype and array type.\n    \"\"\"\n\n    ndim = 1\n\n    def __init__(self, n: int) -> None:\n        self.n = n\n\n    @property\n    def shape(self) -> tuple[int]:\n        return (self.n,)\n\n    def to_array(self, dtype: DtypeObj) -> ArrayLike:\n        \"\"\"\n        Helper function to create the actual all-NA array from the NullArrayProxy\n        object.\n\n        Parameters\n        ----------\n        arr : NullArrayProxy\n        dtype : the dtype for the resulting array\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n        \"\"\"\n        if isinstance(dtype, ExtensionDtype):\n            empty = dtype.construct_array_type()._from_sequence([], dtype=dtype)\n            indexer = -np.ones(self.n, dtype=np.intp)\n            return empty.take(indexer, allow_fill=True)\n        else:\n            # when introducing missing values, int becomes float, bool becomes object\n            dtype = ensure_dtype_can_hold_na(dtype)\n            fill_value = na_value_for_dtype(dtype)\n            arr = np.empty(self.n, dtype=dtype)\n            arr.fill(fill_value)\n            return ensure_wrapped_if_datetimelike(arr)\n", "class_fn": true, "question_id": "pandas/pandas.core.internals.array_manager/NullArrayProxy", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/internals/blocks.py", "fn_id": "", "content": "class DatetimeLikeBlock(NDArrayBackedExtensionBlock):\n    \"\"\"Block for datetime64[ns], timedelta64[ns].\"\"\"\n\n    __slots__ = ()\n    is_numeric = False\n    values: DatetimeArray | TimedeltaArray\n", "class_fn": true, "question_id": "pandas/pandas.core.internals.blocks/DatetimeLikeBlock", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/internals/blocks.py", "fn_id": "", "content": "class NumpyBlock(Block):\n    values: np.ndarray\n    __slots__ = ()\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\"return a boolean if I am possibly a view\"\"\"\n        return self.values.base is not None\n\n    @property\n    def array_values(self) -> ExtensionArray:\n        return NumpyExtensionArray(self.values)\n\n    def get_values(self, dtype: DtypeObj | None = None) -> np.ndarray:\n        if dtype == _dtype_obj:\n            return self.values.astype(_dtype_obj)\n        return self.values\n\n    @cache_readonly\n    def is_numeric(self) -> bool:  # type: ignore[override]\n        dtype = self.values.dtype\n        kind = dtype.kind\n\n        return kind in \"fciub\"\n", "class_fn": true, "question_id": "pandas/pandas.core.internals.blocks/NumpyBlock", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/methods/selectn.py", "fn_id": "", "content": "class SelectN:\n    def __init__(self, obj, n: int, keep: str) -> None:\n        self.obj = obj\n        self.n = n\n        self.keep = keep\n\n        if self.keep not in (\"first\", \"last\", \"all\"):\n            raise ValueError('keep must be either \"first\", \"last\" or \"all\"')\n\n    def compute(self, method: str) -> DataFrame | Series:\n        raise NotImplementedError\n\n    @final\n    def nlargest(self):\n        return self.compute(\"nlargest\")\n\n    @final\n    def nsmallest(self):\n        return self.compute(\"nsmallest\")\n\n    @final\n    @staticmethod\n    def is_valid_dtype_n_method(dtype: DtypeObj) -> bool:\n        \"\"\"\n        Helper function to determine if dtype is valid for\n        nsmallest/nlargest methods\n        \"\"\"\n        if is_numeric_dtype(dtype):\n            return not is_complex_dtype(dtype)\n        return needs_i8_conversion(dtype)\n", "class_fn": true, "question_id": "pandas/pandas.core.methods.selectn/SelectN", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/resample.py", "fn_id": "", "content": "class DatetimeIndexResamplerGroupby(  # type: ignore[misc]\n    _GroupByMixin, DatetimeIndexResampler\n):\n    \"\"\"\n    Provides a resample of a groupby implementation\n    \"\"\"\n\n    @property\n    def _resampler_cls(self):\n        return DatetimeIndexResampler\n", "class_fn": true, "question_id": "pandas/pandas.core.resample/DatetimeIndexResamplerGroupby", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/resample.py", "fn_id": "", "content": "class TimedeltaIndexResamplerGroupby(  # type: ignore[misc]\n    _GroupByMixin, TimedeltaIndexResampler\n):\n    \"\"\"\n    Provides a resample of a groupby implementation.\n    \"\"\"\n\n    @property\n    def _resampler_cls(self):\n        return TimedeltaIndexResampler\n", "class_fn": true, "question_id": "pandas/pandas.core.resample/TimedeltaIndexResamplerGroupby", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/window/expanding.py", "fn_id": "", "content": "class ExpandingGroupby(BaseWindowGroupby, Expanding):\n    \"\"\"\n    Provide a expanding groupby implementation.\n    \"\"\"\n\n    _attributes = Expanding._attributes + BaseWindowGroupby._attributes\n\n    def _get_window_indexer(self) -> GroupbyIndexer:\n        \"\"\"\n        Return an indexer class that will compute the window start and end bounds\n\n        Returns\n        -------\n        GroupbyIndexer\n        \"\"\"\n        window_indexer = GroupbyIndexer(\n            groupby_indices=self._grouper.indices,\n            window_indexer=ExpandingIndexer,\n        )\n        return window_indexer\n", "class_fn": true, "question_id": "pandas/pandas.core.window.expanding/ExpandingGroupby", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/errors/__init__.py", "fn_id": "", "content": "class PyperclipWindowsException(PyperclipException):\n    \"\"\"\n    Exception raised when clipboard functionality is unsupported by Windows.\n\n    Access to the clipboard handle would be denied due to some other\n    window process is accessing it.\n    \"\"\"\n\n    def __init__(self, message: str) -> None:\n        # attr only exists on Windows, so typing fails on other platforms\n        message += f\" ({ctypes.WinError()})\"  # type: ignore[attr-defined]\n        super().__init__(message)\n", "class_fn": true, "question_id": "pandas/pandas.errors/PyperclipWindowsException", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/common.py", "fn_id": "", "content": "@dataclasses.dataclass\nclass IOArgs:\n    \"\"\"\n    Return value of io/common.py:_get_filepath_or_buffer.\n    \"\"\"\n\n    filepath_or_buffer: str | BaseBuffer\n    encoding: str\n    mode: str\n    compression: CompressionDict\n    should_close: bool = False\n", "class_fn": true, "question_id": "pandas/pandas.io.common/IOArgs", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/common.py", "fn_id": "", "content": "class _BytesIOWrapper:\n    # Wrapper that wraps a StringIO buffer and reads bytes from it\n    # Created for compat with pyarrow read_csv\n    def __init__(self, buffer: StringIO | TextIOBase, encoding: str = \"utf-8\") -> None:\n        self.buffer = buffer\n        self.encoding = encoding\n        # Because a character can be represented by more than 1 byte,\n        # it is possible that reading will produce more bytes than n\n        # We store the extra bytes in this overflow variable, and append the\n        # overflow to the front of the bytestring the next time reading is performed\n        self.overflow = b\"\"\n\n    def __getattr__(self, attr: str):\n        return getattr(self.buffer, attr)\n\n    def read(self, n: int | None = -1) -> bytes:\n        assert self.buffer is not None\n        bytestring = self.buffer.read(n).encode(self.encoding)\n        # When n=-1/n greater than remaining bytes: Read entire file/rest of file\n        combined_bytestring = self.overflow + bytestring\n        if n is None or n < 0 or n >= len(combined_bytestring):\n            self.overflow = b\"\"\n            return combined_bytestring\n        else:\n            to_return = combined_bytestring[:n]\n            self.overflow = combined_bytestring[n:]\n            return to_return\n", "class_fn": true, "question_id": "pandas/pandas.io.common/_BytesIOWrapper", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/common.py", "fn_id": "", "content": "class _IOWrapper:\n    # TextIOWrapper is overly strict: it request that the buffer has seekable, readable,\n    # and writable. If we have a read-only buffer, we shouldn't need writable and vice\n    # versa. Some buffers, are seek/read/writ-able but they do not have the \"-able\"\n    # methods, e.g., tempfile.SpooledTemporaryFile.\n    # If a buffer does not have the above \"-able\" methods, we simple assume they are\n    # seek/read/writ-able.\n    def __init__(self, buffer: BaseBuffer) -> None:\n        self.buffer = buffer\n\n    def __getattr__(self, name: str):\n        return getattr(self.buffer, name)\n\n    def readable(self) -> bool:\n        if hasattr(self.buffer, \"readable\"):\n            return self.buffer.readable()\n        return True\n\n    def seekable(self) -> bool:\n        if hasattr(self.buffer, \"seekable\"):\n            return self.buffer.seekable()\n        return True\n\n    def writable(self) -> bool:\n        if hasattr(self.buffer, \"writable\"):\n            return self.buffer.writable()\n        return True\n", "class_fn": true, "question_id": "pandas/pandas.io.common/_IOWrapper", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/format.py", "fn_id": "", "content": "class _Datetime64Formatter(_GenericArrayFormatter):\n    values: DatetimeArray\n\n    def __init__(\n        self,\n        values: DatetimeArray,\n        nat_rep: str = \"NaT\",\n        date_format: None = None,\n        **kwargs,\n    ) -> None:\n        super().__init__(values, **kwargs)\n        self.nat_rep = nat_rep\n        self.date_format = date_format\n\n    def _format_strings(self) -> list[str]:\n        \"\"\"we by definition have DO NOT have a TZ\"\"\"\n        values = self.values\n\n        if self.formatter is not None:\n            return [self.formatter(x) for x in values]\n\n        fmt_values = values._format_native_types(\n            na_rep=self.nat_rep, date_format=self.date_format\n        )\n        return fmt_values.tolist()\n", "class_fn": true, "question_id": "pandas/pandas.io.formats.format/_Datetime64Formatter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/format.py", "fn_id": "", "content": "class _IntArrayFormatter(_GenericArrayFormatter):\n    def _format_strings(self) -> list[str]:\n        if self.leading_space is False:\n            formatter_str = lambda x: f\"{x:d}\".format(x=x)\n        else:\n            formatter_str = lambda x: f\"{x: d}\".format(x=x)\n        formatter = self.formatter or formatter_str\n        fmt_values = [formatter(x) for x in self.values]\n        return fmt_values\n", "class_fn": true, "question_id": "pandas/pandas.io.formats.format/_IntArrayFormatter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/info.py", "fn_id": "", "content": "class DataFrameInfo(_BaseInfo):\n    \"\"\"\n    Class storing dataframe-specific info.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: DataFrame,\n        memory_usage: bool | str | None = None,\n    ) -> None:\n        self.data: DataFrame = data\n        self.memory_usage = _initialize_memory_usage(memory_usage)\n\n    @property\n    def dtype_counts(self) -> Mapping[str, int]:\n        return _get_dataframe_dtype_counts(self.data)\n\n    @property\n    def dtypes(self) -> Iterable[Dtype]:\n        \"\"\"\n        Dtypes.\n\n        Returns\n        -------\n        dtypes\n            Dtype of each of the DataFrame's columns.\n        \"\"\"\n        return self.data.dtypes\n\n    @property\n    def ids(self) -> Index:\n        \"\"\"\n        Column names.\n\n        Returns\n        -------\n        ids : Index\n            DataFrame's column names.\n        \"\"\"\n        return self.data.columns\n\n    @property\n    def col_count(self) -> int:\n        \"\"\"Number of columns to be summarized.\"\"\"\n        return len(self.ids)\n\n    @property\n    def non_null_counts(self) -> Sequence[int]:\n        \"\"\"Sequence of non-null counts for all columns or column (if series).\"\"\"\n        return self.data.count()\n\n    @property\n    def memory_usage_bytes(self) -> int:\n        deep = self.memory_usage == \"deep\"\n        return self.data.memory_usage(index=True, deep=deep).sum()\n\n    def render(\n        self,\n        *,\n        buf: WriteBuffer[str] | None,\n        max_cols: int | None,\n        verbose: bool | None,\n        show_counts: bool | None,\n    ) -> None:\n        printer = _DataFrameInfoPrinter(\n            info=self,\n            max_cols=max_cols,\n            verbose=verbose,\n            show_counts=show_counts,\n        )\n        printer.to_buffer(buf)\n", "class_fn": true, "question_id": "pandas/pandas.io.formats.info/DataFrameInfo", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/info.py", "fn_id": "", "content": "class _DataFrameTableBuilderNonVerbose(_DataFrameTableBuilder):\n    \"\"\"\n    Dataframe info table builder for non-verbose output.\n    \"\"\"\n\n    def _fill_non_empty_info(self) -> None:\n        \"\"\"Add lines to the info table, pertaining to non-empty dataframe.\"\"\"\n        self.add_object_type_line()\n        self.add_index_range_line()\n        self.add_columns_summary_line()\n        self.add_dtypes_line()\n        if self.display_memory_usage:\n            self.add_memory_usage_line()\n\n    def add_columns_summary_line(self) -> None:\n        self._lines.append(self.ids._summary(name=\"Columns\"))\n", "class_fn": true, "question_id": "pandas/pandas.io.formats.info/_DataFrameTableBuilderNonVerbose", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/info.py", "fn_id": "", "content": "class _SeriesTableBuilder(_TableBuilderAbstract):\n    \"\"\"\n    Abstract builder for series info table.\n\n    Parameters\n    ----------\n    info : SeriesInfo.\n        Instance of SeriesInfo.\n    \"\"\"\n\n    def __init__(self, *, info: SeriesInfo) -> None:\n        self.info: SeriesInfo = info\n\n    def get_lines(self) -> list[str]:\n        self._lines = []\n        self._fill_non_empty_info()\n        return self._lines\n\n    @property\n    def data(self) -> Series:\n        \"\"\"Series.\"\"\"\n        return self.info.data\n\n    def add_memory_usage_line(self) -> None:\n        \"\"\"Add line containing memory usage.\"\"\"\n        self._lines.append(f\"memory usage: {self.memory_usage_string}\")\n\n    @abstractmethod\n    def _fill_non_empty_info(self) -> None:\n        \"\"\"Add lines to the info table, pertaining to non-empty series.\"\"\"\n", "class_fn": true, "question_id": "pandas/pandas.io.formats.info/_SeriesTableBuilder", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/info.py", "fn_id": "", "content": "class _TableBuilderAbstract(ABC):\n    \"\"\"\n    Abstract builder for info table.\n    \"\"\"\n\n    _lines: list[str]\n    info: _BaseInfo\n\n    @abstractmethod\n    def get_lines(self) -> list[str]:\n        \"\"\"Product in a form of list of lines (strings).\"\"\"\n\n    @property\n    def data(self) -> DataFrame | Series:\n        return self.info.data\n\n    @property\n    def dtypes(self) -> Iterable[Dtype]:\n        \"\"\"Dtypes of each of the DataFrame's columns.\"\"\"\n        return self.info.dtypes\n\n    @property\n    def dtype_counts(self) -> Mapping[str, int]:\n        \"\"\"Mapping dtype - number of counts.\"\"\"\n        return self.info.dtype_counts\n\n    @property\n    def display_memory_usage(self) -> bool:\n        \"\"\"Whether to display memory usage.\"\"\"\n        return bool(self.info.memory_usage)\n\n    @property\n    def memory_usage_string(self) -> str:\n        \"\"\"Memory usage string with proper size qualifier.\"\"\"\n        return self.info.memory_usage_string\n\n    @property\n    def non_null_counts(self) -> Sequence[int]:\n        return self.info.non_null_counts\n\n    def add_object_type_line(self) -> None:\n        \"\"\"Add line with string representation of dataframe to the table.\"\"\"\n        self._lines.append(str(type(self.data)))\n\n    def add_index_range_line(self) -> None:\n        \"\"\"Add line with range of indices to the table.\"\"\"\n        self._lines.append(self.data.index._summary())\n\n    def add_dtypes_line(self) -> None:\n        \"\"\"Add summary line with dtypes present in dataframe.\"\"\"\n        collected_dtypes = [\n            f\"{key}({val:d})\" for key, val in sorted(self.dtype_counts.items())\n        ]\n        self._lines.append(f\"dtypes: {', '.join(collected_dtypes)}\")\n", "class_fn": true, "question_id": "pandas/pandas.io.formats.info/_TableBuilderAbstract", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/json/_json.py", "fn_id": "", "content": "class FrameWriter(Writer):\n    _default_orient = \"columns\"\n\n    @property\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        if not self.index and self.orient == \"split\":\n            obj_to_write = self.obj.to_dict(orient=\"split\")\n            del obj_to_write[\"index\"]\n        else:\n            obj_to_write = self.obj\n        return obj_to_write\n\n    def _format_axes(self) -> None:\n        \"\"\"\n        Try to format axes if they are datelike.\n        \"\"\"\n        if not self.obj.index.is_unique and self.orient in (\"index\", \"columns\"):\n            raise ValueError(\n                f\"DataFrame index must be unique for orient='{self.orient}'.\"\n            )\n        if not self.obj.columns.is_unique and self.orient in (\n            \"index\",\n            \"columns\",\n            \"records\",\n        ):\n            raise ValueError(\n                f\"DataFrame columns must be unique for orient='{self.orient}'.\"\n            )\n", "class_fn": true, "question_id": "pandas/pandas.io.json._json/FrameWriter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/json/_json.py", "fn_id": "", "content": "class Writer(ABC):\n    _default_orient: str\n\n    def __init__(\n        self,\n        obj: NDFrame,\n        orient: str | None,\n        date_format: str,\n        double_precision: int,\n        ensure_ascii: bool,\n        date_unit: str,\n        index: bool,\n        default_handler: Callable[[Any], JSONSerializable] | None = None,\n        indent: int = 0,\n    ) -> None:\n        self.obj = obj\n\n        if orient is None:\n            orient = self._default_orient\n\n        self.orient = orient\n        self.date_format = date_format\n        self.double_precision = double_precision\n        self.ensure_ascii = ensure_ascii\n        self.date_unit = date_unit\n        self.default_handler = default_handler\n        self.index = index\n        self.indent = indent\n\n        self.is_copy = None\n        self._format_axes()\n\n    def _format_axes(self) -> None:\n        raise AbstractMethodError(self)\n\n    def write(self) -> str:\n        iso_dates = self.date_format == \"iso\"\n        return ujson_dumps(\n            self.obj_to_write,\n            orient=self.orient,\n            double_precision=self.double_precision,\n            ensure_ascii=self.ensure_ascii,\n            date_unit=self.date_unit,\n            iso_dates=iso_dates,\n            default_handler=self.default_handler,\n            indent=self.indent,\n        )\n\n    @property\n    @abstractmethod\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        \"\"\"Object to write in JSON format.\"\"\"\n", "class_fn": true, "question_id": "pandas/pandas.io.json._json/Writer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/parsers/readers.py", "fn_id": "", "content": "class _Fwf_Defaults(TypedDict):\n    colspecs: Literal[\"infer\"]\n    infer_nrows: Literal[100]\n    widths: None\n", "class_fn": true, "question_id": "pandas/pandas.io.parsers.readers/_Fwf_Defaults", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/pytables.py", "fn_id": "", "content": "class AppendableSeriesTable(AppendableFrameTable):\n    \"\"\"support the new appendable table formats\"\"\"\n\n    pandas_kind = \"series_table\"\n    table_type = \"appendable_series\"\n    ndim = 2\n    obj_type = Series\n\n    @property\n    def is_transposed(self) -> bool:\n        return False\n\n    @classmethod\n    def get_object(cls, obj, transposed: bool):\n        return obj\n\n    # error: Signature of \"write\" incompatible with supertype \"Fixed\"\n    def write(self, obj, data_columns=None, **kwargs) -> None:  # type: ignore[override]\n        \"\"\"we are going to write this as a frame table\"\"\"\n        if not isinstance(obj, DataFrame):\n            name = obj.name or \"values\"\n            obj = obj.to_frame(name)\n        super().write(obj=obj, data_columns=obj.columns.tolist(), **kwargs)\n\n    def read(\n        self,\n        where=None,\n        columns=None,\n        start: int | None = None,\n        stop: int | None = None,\n    ) -> Series:\n        is_multi_index = self.is_multi_index\n        if columns is not None and is_multi_index:\n            assert isinstance(self.levels, list)  # needed for mypy\n            for n in self.levels:\n                if n not in columns:\n                    columns.insert(0, n)\n        s = super().read(where=where, columns=columns, start=start, stop=stop)\n        if is_multi_index:\n            s.set_index(self.levels, inplace=True)\n\n        s = s.iloc[:, 0]\n\n        # remove the default name\n        if s.name == \"values\":\n            s.name = None\n        return s\n", "class_fn": true, "question_id": "pandas/pandas.io.pytables/AppendableSeriesTable", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/pytables.py", "fn_id": "", "content": "class GenericIndexCol(IndexCol):\n    \"\"\"an index which is not represented in the data of the table\"\"\"\n\n    @property\n    def is_indexed(self) -> bool:\n        return False\n\n    def convert(\n        self, values: np.ndarray, nan_rep, encoding: str, errors: str\n    ) -> tuple[Index, Index]:\n        \"\"\"\n        Convert the data from this selection to the appropriate pandas type.\n\n        Parameters\n        ----------\n        values : np.ndarray\n        nan_rep : str\n        encoding : str\n        errors : str\n        \"\"\"\n        assert isinstance(values, np.ndarray), type(values)\n\n        index = RangeIndex(len(values))\n        return index, index\n\n    def set_attr(self) -> None:\n        pass\n", "class_fn": true, "question_id": "pandas/pandas.io.pytables/GenericIndexCol", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/sas/sas7bdat.py", "fn_id": "", "content": "class _Column:\n    col_id: int\n    name: str | bytes\n    label: str | bytes\n    format: str | bytes\n    ctype: bytes\n    length: int\n\n    def __init__(\n        self,\n        col_id: int,\n        # These can be bytes when convert_header_text is False\n        name: str | bytes,\n        label: str | bytes,\n        format: str | bytes,\n        ctype: bytes,\n        length: int,\n    ) -> None:\n        self.col_id = col_id\n        self.name = name\n        self.label = label\n        self.format = format\n        self.ctype = ctype\n        self.length = length\n", "class_fn": true, "question_id": "pandas/pandas.io.sas.sas7bdat/_Column", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/sql.py", "fn_id": "", "content": "class PandasSQL(PandasObject, ABC):\n    \"\"\"\n    Subclasses Should define read_query and to_sql.\n    \"\"\"\n\n    def __enter__(self) -> Self:\n        return self\n\n    def __exit__(self, *args) -> None:\n        pass\n\n    def read_table(\n        self,\n        table_name: str,\n        index_col: str | list[str] | None = None,\n        coerce_float: bool = True,\n        parse_dates=None,\n        columns=None,\n        schema: str | None = None,\n        chunksize: int | None = None,\n        dtype_backend: DtypeBackend | Literal[\"numpy\"] = \"numpy\",\n    ) -> DataFrame | Iterator[DataFrame]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def read_query(\n        self,\n        sql: str,\n        index_col: str | list[str] | None = None,\n        coerce_float: bool = True,\n        parse_dates=None,\n        params=None,\n        chunksize: int | None = None,\n        dtype: DtypeArg | None = None,\n        dtype_backend: DtypeBackend | Literal[\"numpy\"] = \"numpy\",\n    ) -> DataFrame | Iterator[DataFrame]:\n        pass\n\n    @abstractmethod\n    def to_sql(\n        self,\n        frame,\n        name: str,\n        if_exists: Literal[\"fail\", \"replace\", \"append\"] = \"fail\",\n        index: bool = True,\n        index_label=None,\n        schema=None,\n        chunksize: int | None = None,\n        dtype: DtypeArg | None = None,\n        method: Literal[\"multi\"] | Callable | None = None,\n        engine: str = \"auto\",\n        **engine_kwargs,\n    ) -> int | None:\n        pass\n\n    @abstractmethod\n    def execute(self, sql: str | Select | TextClause, params=None):\n        pass\n\n    @abstractmethod\n    def has_table(self, name: str, schema: str | None = None) -> bool:\n        pass\n\n    @abstractmethod\n    def _create_sql_schema(\n        self,\n        frame: DataFrame,\n        table_name: str,\n        keys: list[str] | None = None,\n        dtype: DtypeArg | None = None,\n        schema: str | None = None,\n    ) -> str:\n        pass\n", "class_fn": true, "question_id": "pandas/pandas.io.sql/PandasSQL", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/arithmetic/test_categorical.py", "fn_id": "", "content": "class TestCategoricalComparisons:\n    def test_categorical_nan_equality(self):\n        cat = Series(Categorical([\"a\", \"b\", \"c\", np.nan]))\n        expected = Series([True, True, True, False])\n        result = cat == cat\n        tm.assert_series_equal(result, expected)\n\n    def test_categorical_tuple_equality(self):\n        # GH 18050\n        ser = Series([(0, 0), (0, 1), (0, 0), (1, 0), (1, 1)])\n        expected = Series([True, False, True, False, False])\n        result = ser == (0, 0)\n        tm.assert_series_equal(result, expected)\n\n        result = ser.astype(\"category\") == (0, 0)\n        tm.assert_series_equal(result, expected)\n", "class_fn": true, "question_id": "pandas/pandas.tests.arithmetic.test_categorical/TestCategoricalComparisons", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/arrays/numpy_/test_indexing.py", "fn_id": "", "content": "class TestSearchsorted:\n    def test_searchsorted_string(self, string_dtype):\n        arr = pd.array([\"a\", \"b\", \"c\"], dtype=string_dtype)\n\n        result = arr.searchsorted(\"a\", side=\"left\")\n        assert is_scalar(result)\n        assert result == 0\n\n        result = arr.searchsorted(\"a\", side=\"right\")\n        assert is_scalar(result)\n        assert result == 1\n\n    def test_searchsorted_numeric_dtypes_scalar(self, any_real_numpy_dtype):\n        arr = pd.array([1, 3, 90], dtype=any_real_numpy_dtype)\n        result = arr.searchsorted(30)\n        assert is_scalar(result)\n        assert result == 2\n\n        result = arr.searchsorted([30])\n        expected = np.array([2], dtype=np.intp)\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_searchsorted_numeric_dtypes_vector(self, any_real_numpy_dtype):\n        arr = pd.array([1, 3, 90], dtype=any_real_numpy_dtype)\n        result = arr.searchsorted([2, 30])\n        expected = np.array([1, 2], dtype=np.intp)\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_searchsorted_sorter(self, any_real_numpy_dtype):\n        arr = pd.array([3, 1, 2], dtype=any_real_numpy_dtype)\n        result = arr.searchsorted([0, 3], sorter=np.argsort(arr))\n        expected = np.array([0, 2], dtype=np.intp)\n        tm.assert_numpy_array_equal(result, expected)\n", "class_fn": true, "question_id": "pandas/pandas.tests.arrays.numpy_.test_indexing/TestSearchsorted", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/extension/date/array.py", "fn_id": "", "content": "@register_extension_dtype\nclass DateDtype(ExtensionDtype):\n    @property\n    def type(self):\n        return dt.date\n\n    @property\n    def name(self):\n        return \"DateDtype\"\n\n    @classmethod\n    def construct_from_string(cls, string: str):\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n\n        if string == cls.__name__:\n            return cls()\n        else:\n            raise TypeError(f\"Cannot construct a '{cls.__name__}' from '{string}'\")\n\n    @classmethod\n    def construct_array_type(cls):\n        return DateArray\n\n    @property\n    def na_value(self):\n        return dt.date.min\n\n    def __repr__(self) -> str:\n        return self.name\n", "class_fn": true, "question_id": "pandas/pandas.tests.extension.date.array/DateDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/extension/list/array.py", "fn_id": "", "content": "class ListDtype(ExtensionDtype):\n    type = list\n    name = \"list\"\n    na_value = np.nan\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[ListArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        return ListArray\n", "class_fn": true, "question_id": "pandas/pandas.tests.extension.list.array/ListDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/frame/test_alter_axes.py", "fn_id": "", "content": "class TestDataFrameAlterAxes:\n    # Tests for setting index/columns attributes directly (i.e. __setattr__)\n\n    def test_set_axis_setattr_index(self):\n        # GH 6785\n        # set the index manually\n\n        df = DataFrame([{\"ts\": datetime(2014, 4, 1, tzinfo=pytz.utc), \"foo\": 1}])\n        expected = df.set_index(\"ts\")\n        df.index = df[\"ts\"]\n        df.pop(\"ts\")\n        tm.assert_frame_equal(df, expected)\n\n    # Renaming\n\n    def test_assign_columns(self, float_frame):\n        float_frame[\"hi\"] = \"there\"\n\n        df = float_frame.copy()\n        df.columns = [\"foo\", \"bar\", \"baz\", \"quux\", \"foo2\"]\n        tm.assert_series_equal(float_frame[\"C\"], df[\"baz\"], check_names=False)\n        tm.assert_series_equal(float_frame[\"hi\"], df[\"foo2\"], check_names=False)\n", "class_fn": true, "question_id": "pandas/pandas.tests.frame.test_alter_axes/TestDataFrameAlterAxes", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/datetimes/methods/test_to_frame.py", "fn_id": "", "content": "class TestToFrame:\n    def test_to_frame_datetime_tz(self):\n        # GH#25809\n        idx = date_range(start=\"2019-01-01\", end=\"2019-01-30\", freq=\"D\", tz=\"UTC\")\n        result = idx.to_frame()\n        expected = DataFrame(idx, index=idx)\n        tm.assert_frame_equal(result, expected)\n\n    def test_to_frame_respects_none_name(self):\n        # GH#44212 if we explicitly pass name=None, then that should be respected,\n        #  not changed to 0\n        # GH-45448 this is first deprecated to only change in the future\n        idx = date_range(start=\"2019-01-01\", end=\"2019-01-30\", freq=\"D\", tz=\"UTC\")\n        result = idx.to_frame(name=None)\n        exp_idx = Index([None], dtype=object)\n        tm.assert_index_equal(exp_idx, result.columns)\n\n        result = idx.rename(\"foo\").to_frame(name=None)\n        exp_idx = Index([None], dtype=object)\n        tm.assert_index_equal(exp_idx, result.columns)\n", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.datetimes.methods.test_to_frame/TestToFrame", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/datetimes/test_npfuncs.py", "fn_id": "", "content": "class TestSplit:\n    def test_split_non_utc(self):\n        # GH#14042\n        indices = date_range(\"2016-01-01 00:00:00+0200\", freq=\"s\", periods=10)\n        result = np.split(indices, indices_or_sections=[])[0]\n        expected = indices._with_freq(None)\n        tm.assert_index_equal(result, expected)\n", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.datetimes.test_npfuncs/TestSplit", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/multi/test_get_level_values.py", "fn_id": "", "content": "class TestGetLevelValues:\n    def test_get_level_values_box_datetime64(self):\n        dates = date_range(\"1/1/2000\", periods=4)\n        levels = [dates, [0, 1]]\n        codes = [[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]]\n\n        index = MultiIndex(levels=levels, codes=codes)\n\n        assert isinstance(index.get_level_values(0)[0], Timestamp)\n", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.multi.test_get_level_values/TestGetLevelValues", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/period/methods/test_factorize.py", "fn_id": "", "content": "class TestFactorize:\n    def test_factorize_period(self):\n        idx1 = PeriodIndex(\n            [\"2014-01\", \"2014-01\", \"2014-02\", \"2014-02\", \"2014-03\", \"2014-03\"],\n            freq=\"M\",\n        )\n\n        exp_arr = np.array([0, 0, 1, 1, 2, 2], dtype=np.intp)\n        exp_idx = PeriodIndex([\"2014-01\", \"2014-02\", \"2014-03\"], freq=\"M\")\n\n        arr, idx = idx1.factorize()\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)\n\n        arr, idx = idx1.factorize(sort=True)\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)\n\n    def test_factorize_period_nonmonotonic(self):\n        idx2 = PeriodIndex(\n            [\"2014-03\", \"2014-03\", \"2014-02\", \"2014-01\", \"2014-03\", \"2014-01\"],\n            freq=\"M\",\n        )\n        exp_idx = PeriodIndex([\"2014-01\", \"2014-02\", \"2014-03\"], freq=\"M\")\n\n        exp_arr = np.array([2, 2, 1, 0, 2, 0], dtype=np.intp)\n        arr, idx = idx2.factorize(sort=True)\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)\n\n        exp_arr = np.array([0, 0, 1, 2, 0, 2], dtype=np.intp)\n        exp_idx = PeriodIndex([\"2014-03\", \"2014-02\", \"2014-01\"], freq=\"M\")\n        arr, idx = idx2.factorize()\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)\n", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.period.methods.test_factorize/TestFactorize", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/timedeltas/methods/test_factorize.py", "fn_id": "", "content": "class TestTimedeltaIndexFactorize:\n    def test_factorize(self):\n        idx1 = TimedeltaIndex([\"1 day\", \"1 day\", \"2 day\", \"2 day\", \"3 day\", \"3 day\"])\n\n        exp_arr = np.array([0, 0, 1, 1, 2, 2], dtype=np.intp)\n        exp_idx = TimedeltaIndex([\"1 day\", \"2 day\", \"3 day\"])\n\n        arr, idx = idx1.factorize()\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)\n        assert idx.freq == exp_idx.freq\n\n        arr, idx = idx1.factorize(sort=True)\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)\n        assert idx.freq == exp_idx.freq\n\n    def test_factorize_preserves_freq(self):\n        # GH#38120 freq should be preserved\n        idx3 = timedelta_range(\"1 day\", periods=4, freq=\"s\")\n        exp_arr = np.array([0, 1, 2, 3], dtype=np.intp)\n        arr, idx = idx3.factorize()\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, idx3)\n        assert idx.freq == idx3.freq\n\n        arr, idx = factorize(idx3)\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, idx3)\n        assert idx.freq == idx3.freq\n", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.timedeltas.methods.test_factorize/TestTimedeltaIndexFactorize", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/timedeltas/test_arithmetic.py", "fn_id": "", "content": "class TestTimedeltaIndexArithmetic:\n    def test_arithmetic_zero_freq(self):\n        # GH#51575 don't get a .freq with freq.n = 0\n        tdi = timedelta_range(0, periods=100, freq=\"ns\")\n        result = tdi / 2\n        assert result.freq is None\n        expected = tdi[:50].repeat(2)\n        tm.assert_index_equal(result, expected)\n\n        result2 = tdi // 2\n        assert result2.freq is None\n        expected2 = expected\n        tm.assert_index_equal(result2, expected2)\n\n        result3 = tdi * 0\n        assert result3.freq is None\n        expected3 = tdi[:1].repeat(100)\n        tm.assert_index_equal(result3, expected3)\n\n    def test_tdi_division(self, index_or_series):\n        # doc example\n\n        scalar = Timedelta(days=31)\n        td = index_or_series(\n            [scalar, scalar, scalar + Timedelta(minutes=5, seconds=3), NaT],\n            dtype=\"m8[ns]\",\n        )\n\n        result = td / np.timedelta64(1, \"D\")\n        expected = index_or_series(\n            [31, 31, (31 * 86400 + 5 * 60 + 3) / 86400.0, np.nan]\n        )\n        tm.assert_equal(result, expected)\n\n        result = td / np.timedelta64(1, \"s\")\n        expected = index_or_series(\n            [31 * 86400, 31 * 86400, 31 * 86400 + 5 * 60 + 3, np.nan]\n        )\n        tm.assert_equal(result, expected)\n", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.timedeltas.test_arithmetic/TestTimedeltaIndexArithmetic", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/timedeltas/test_pickle.py", "fn_id": "", "content": "class TestPickle:\n    def test_pickle_after_set_freq(self):\n        tdi = timedelta_range(\"1 day\", periods=4, freq=\"s\")\n        tdi = tdi._with_freq(None)\n\n        res = tm.round_trip_pickle(tdi)\n        tm.assert_index_equal(res, tdi)\n", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.timedeltas.test_pickle/TestPickle", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/scalar/timestamp/methods/test_to_julian_date.py", "fn_id": "", "content": "class TestTimestampToJulianDate:\n    def test_compare_1700(self):\n        ts = Timestamp(\"1700-06-23\")\n        res = ts.to_julian_date()\n        assert res == 2_342_145.5\n\n    def test_compare_2000(self):\n        ts = Timestamp(\"2000-04-12\")\n        res = ts.to_julian_date()\n        assert res == 2_451_646.5\n\n    def test_compare_2100(self):\n        ts = Timestamp(\"2100-08-12\")\n        res = ts.to_julian_date()\n        assert res == 2_488_292.5\n\n    def test_compare_hour01(self):\n        ts = Timestamp(\"2000-08-12T01:00:00\")\n        res = ts.to_julian_date()\n        assert res == 2_451_768.5416666666666666\n\n    def test_compare_hour13(self):\n        ts = Timestamp(\"2000-08-12T13:00:00\")\n        res = ts.to_julian_date()\n        assert res == 2_451_769.0416666666666666\n", "class_fn": true, "question_id": "pandas/pandas.tests.scalar.timestamp.methods.test_to_julian_date/TestTimestampToJulianDate", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/series/methods/test_autocorr.py", "fn_id": "", "content": "class TestAutoCorr:\n    def test_autocorr(self, datetime_series):\n        # Just run the function\n        corr1 = datetime_series.autocorr()\n\n        # Now run it with the lag parameter\n        corr2 = datetime_series.autocorr(lag=1)\n\n        # corr() with lag needs Series of at least length 2\n        if len(datetime_series) <= 2:\n            assert np.isnan(corr1)\n            assert np.isnan(corr2)\n        else:\n            assert corr1 == corr2\n\n        # Choose a random lag between 1 and length of Series - 2\n        # and compare the result with the Series corr() function\n        n = 1 + np.random.default_rng(2).integers(max(1, len(datetime_series) - 2))\n        corr1 = datetime_series.corr(datetime_series.shift(n))\n        corr2 = datetime_series.autocorr(lag=n)\n\n        # corr() with lag needs Series of at least length 2\n        if len(datetime_series) <= 2:\n            assert np.isnan(corr1)\n            assert np.isnan(corr2)\n        else:\n            assert corr1 == corr2\n", "class_fn": true, "question_id": "pandas/pandas.tests.series.methods.test_autocorr/TestAutoCorr", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/series/methods/test_dtypes.py", "fn_id": "", "content": "class TestSeriesDtypes:\n    def test_dtype(self, datetime_series):\n        assert datetime_series.dtype == np.dtype(\"float64\")\n        assert datetime_series.dtypes == np.dtype(\"float64\")\n", "class_fn": true, "question_id": "pandas/pandas.tests.series.methods.test_dtypes/TestSeriesDtypes", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/series/methods/test_is_monotonic.py", "fn_id": "", "content": "class TestIsMonotonic:\n    def test_is_monotonic_numeric(self):\n        ser = Series(np.random.default_rng(2).integers(0, 10, size=1000))\n        assert not ser.is_monotonic_increasing\n        ser = Series(np.arange(1000))\n        assert ser.is_monotonic_increasing is True\n        assert ser.is_monotonic_increasing is True\n        ser = Series(np.arange(1000, 0, -1))\n        assert ser.is_monotonic_decreasing is True\n\n    def test_is_monotonic_dt64(self):\n        ser = Series(date_range(\"20130101\", periods=10))\n        assert ser.is_monotonic_increasing is True\n        assert ser.is_monotonic_increasing is True\n\n        ser = Series(list(reversed(ser)))\n        assert ser.is_monotonic_increasing is False\n        assert ser.is_monotonic_decreasing is True\n", "class_fn": true, "question_id": "pandas/pandas.tests.series.methods.test_is_monotonic/TestIsMonotonic", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/series/test_iteration.py", "fn_id": "", "content": "class TestIteration:\n    def test_keys(self, datetime_series):\n        assert datetime_series.keys() is datetime_series.index\n\n    def test_iter_datetimes(self, datetime_series):\n        for i, val in enumerate(datetime_series):\n            # pylint: disable-next=unnecessary-list-index-lookup\n            assert val == datetime_series.iloc[i]\n\n    def test_iter_strings(self, string_series):\n        for i, val in enumerate(string_series):\n            # pylint: disable-next=unnecessary-list-index-lookup\n            assert val == string_series.iloc[i]\n\n    def test_iteritems_datetimes(self, datetime_series):\n        for idx, val in datetime_series.items():\n            assert val == datetime_series[idx]\n\n    def test_iteritems_strings(self, string_series):\n        for idx, val in string_series.items():\n            assert val == string_series[idx]\n\n        # assert is lazy (generators don't define reverse, lists do)\n        assert not hasattr(string_series.items(), \"reverse\")\n\n    def test_items_datetimes(self, datetime_series):\n        for idx, val in datetime_series.items():\n            assert val == datetime_series[idx]\n\n    def test_items_strings(self, string_series):\n        for idx, val in string_series.items():\n            assert val == string_series[idx]\n\n        # assert is lazy (generators don't define reverse, lists do)\n        assert not hasattr(string_series.items(), \"reverse\")\n", "class_fn": true, "question_id": "pandas/pandas.tests.series.test_iteration/TestIteration", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tseries/frequencies.py", "fn_id": "", "content": "class _TimedeltaFrequencyInferer(_FrequencyInferer):\n    def _infer_daily_rule(self):\n        if self.is_unique:\n            return self._get_daily_rule()\n", "class_fn": true, "question_id": "pandas/pandas.tseries.frequencies/_TimedeltaFrequencyInferer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/util/_decorators.py", "fn_id": "", "content": "class Appender:\n    \"\"\"\n    A function decorator that will append an addendum to the docstring\n    of the target function.\n\n    This decorator should be robust even if func.__doc__ is None\n    (for example, if -OO was passed to the interpreter).\n\n    Usage: construct a docstring.Appender with a string to be joined to\n    the original docstring. An optional 'join' parameter may be supplied\n    which will be used to join the docstring and addendum. e.g.\n\n    add_copyright = Appender(\"Copyright (c) 2009\", join='\\n')\n\n    @add_copyright\n    def my_dog(has='fleas'):\n        \"This docstring will have a copyright below\"\n        pass\n    \"\"\"\n\n    addendum: str | None\n\n    def __init__(self, addendum: str | None, join: str = \"\", indents: int = 0) -> None:\n        if indents > 0:\n            self.addendum = indent(addendum, indents=indents)\n        else:\n            self.addendum = addendum\n        self.join = join\n\n    def __call__(self, func: T) -> T:\n        func.__doc__ = func.__doc__ if func.__doc__ else \"\"\n        self.addendum = self.addendum if self.addendum else \"\"\n        docitems = [func.__doc__, self.addendum]\n        func.__doc__ = dedent(self.join.join(docitems))\n        return func\n", "class_fn": true, "question_id": "pandas/pandas.util._decorators/Appender", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/util/version/__init__.py", "fn_id": "", "content": "class LegacyVersion(_BaseVersion):\n    def __init__(self, version: str) -> None:\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)\n\n        warnings.warn(\n            \"Creating a LegacyVersion has been deprecated and will be \"\n            \"removed in the next major release.\",\n            DeprecationWarning,\n        )\n\n    def __str__(self) -> str:\n        return self._version\n\n    def __repr__(self) -> str:\n        return f\"<LegacyVersion('{self}')>\"\n\n    @property\n    def public(self) -> str:\n        return self._version\n\n    @property\n    def base_version(self) -> str:\n        return self._version\n\n    @property\n    def epoch(self) -> int:\n        return -1\n\n    @property\n    def release(self) -> None:\n        return None\n\n    @property\n    def pre(self) -> None:\n        return None\n\n    @property\n    def post(self) -> None:\n        return None\n\n    @property\n    def dev(self) -> None:\n        return None\n\n    @property\n    def local(self) -> None:\n        return None\n\n    @property\n    def is_prerelease(self) -> bool:\n        return False\n\n    @property\n    def is_postrelease(self) -> bool:\n        return False\n\n    @property\n    def is_devrelease(self) -> bool:\n        return False\n", "class_fn": true, "question_id": "pandas/pandas.util.version/LegacyVersion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_config/config.py", "fn_id": "", "content": "class DeprecatedOption(NamedTuple):\n    key: str\n    msg: str | None\n    rkey: str | None\n    removal_ver: str | None\n", "class_fn": true, "question_id": "pandas/pandas._config.config/DeprecatedOption", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_version.py", "fn_id": "", "content": "class NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n", "class_fn": true, "question_id": "pandas/pandas._version/NotThisMethod", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/apply.py", "fn_id": "", "content": "class Apply(metaclass=abc.ABCMeta):\n    axis: AxisInt\n\n    def __init__(\n        self,\n        obj: AggObjType,\n        func: AggFuncType,\n        raw: bool,\n        result_type: str | None,\n        *,\n        by_row: Literal[False, \"compat\", \"_compat\"] = \"compat\",\n        engine: str = \"python\",\n        engine_kwargs: dict[str, bool] | None = None,\n        args,\n        kwargs,\n    ) -> None:\n        self.obj = obj\n        self.raw = raw\n\n        assert by_row is False or by_row in [\"compat\", \"_compat\"]\n        self.by_row = by_row\n\n        self.args = args or ()\n        self.kwargs = kwargs or {}\n\n        self.engine = engine\n        self.engine_kwargs = {} if engine_kwargs is None else engine_kwargs\n\n        if result_type not in [None, \"reduce\", \"broadcast\", \"expand\"]:\n            raise ValueError(\n                \"invalid value for result_type, must be one \"\n                \"of {None, 'reduce', 'broadcast', 'expand'}\"\n            )\n\n        self.result_type = result_type\n\n        self.func = func\n\n    @abc.abstractmethod\n    def apply(self) -> DataFrame | Series:\n        pass\n\n    @abc.abstractmethod\n    def agg_or_apply_list_like(\n        self, op_name: Literal[\"agg\", \"apply\"]\n    ) -> DataFrame | Series:\n        pass\n\n    @abc.abstractmethod\n    def agg_or_apply_dict_like(\n        self, op_name: Literal[\"agg\", \"apply\"]\n    ) -> DataFrame | Series:\n        pass\n\n    def agg(self) -> DataFrame | Series | None:\n        \"\"\"\n        Provide an implementation for the aggregators.\n\n        Returns\n        -------\n        Result of aggregation, or None if agg cannot be performed by\n        this method.\n        \"\"\"\n        obj = self.obj\n        func = self.func\n        args = self.args\n        kwargs = self.kwargs\n\n        if isinstance(func, str):\n            return self.apply_str()\n\n        if is_dict_like(func):\n            return self.agg_dict_like()\n        elif is_list_like(func):\n            # we require a list, but not a 'str'\n            return self.agg_list_like()\n\n        if callable(func):\n            f = com.get_cython_func(func)\n            if f and not args and not kwargs:\n                warn_alias_replacement(obj, func, f)\n                return getattr(obj, f)()\n\n        # caller can react\n        return None\n\n    def transform(self) -> DataFrame | Series:\n        \"\"\"\n        Transform a DataFrame or Series.\n\n        Returns\n        -------\n        DataFrame or Series\n            Result of applying ``func`` along the given axis of the\n            Series or DataFrame.\n\n        Raises\n        ------\n        ValueError\n            If the transform function fails or does not transform.\n        \"\"\"\n        obj = self.obj\n        func = self.func\n        axis = self.axis\n        args = self.args\n        kwargs = self.kwargs\n\n        is_series = obj.ndim == 1\n\n        if obj._get_axis_number(axis) == 1:\n            assert not is_series\n            return obj.T.transform(func, 0, *args, **kwargs).T\n\n        if is_list_like(func) and not is_dict_like(func):\n            func = cast(list[AggFuncTypeBase], func)\n            # Convert func equivalent dict\n            if is_series:\n                func = {com.get_callable_name(v) or v: v for v in func}\n            else:\n                func = {col: func for col in obj}\n\n        if is_dict_like(func):\n            func = cast(AggFuncTypeDict, func)\n            return self.transform_dict_like(func)\n\n        # func is either str or callable\n        func = cast(AggFuncTypeBase, func)\n        try:\n            result = self.transform_str_or_callable(func)\n        except TypeError:\n            raise\n        except Exception as err:\n            raise ValueError(\"Transform function failed\") from err\n\n        # Functions that transform may return empty Series/DataFrame\n        # when the dtype is not appropriate\n        if (\n            isinstance(result, (ABCSeries, ABCDataFrame))\n            and result.empty\n            and not obj.empty\n        ):\n            raise ValueError(\"Transform function failed\")\n        # error: Argument 1 to \"__get__\" of \"AxisProperty\" has incompatible type\n        # \"Union[Series, DataFrame, GroupBy[Any], SeriesGroupBy,\n        # DataFrameGroupBy, BaseWindow, Resampler]\"; expected \"Union[DataFrame,\n        # Series]\"\n        if not isinstance(result, (ABCSeries, ABCDataFrame)) or not result.index.equals(\n            obj.index  # type: ignore[arg-type]\n        ):\n            raise ValueError(\"Function did not transform\")\n\n        return result\n\n    def transform_dict_like(self, func) -> DataFrame:\n        \"\"\"\n        Compute transform in the case of a dict-like func\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        obj = self.obj\n        args = self.args\n        kwargs = self.kwargs\n\n        # transform is currently only for Series/DataFrame\n        assert isinstance(obj, ABCNDFrame)\n\n        if len(func) == 0:\n            raise ValueError(\"No transform functions were provided\")\n\n        func = self.normalize_dictlike_arg(\"transform\", obj, func)\n\n        results: dict[Hashable, DataFrame | Series] = {}\n        for name, how in func.items():\n            colg = obj._gotitem(name, ndim=1)\n            results[name] = colg.transform(how, 0, *args, **kwargs)\n        return concat(results, axis=1)\n\n    def transform_str_or_callable(self, func) -> DataFrame | Series:\n        \"\"\"\n        Compute transform in the case of a string or callable func\n        \"\"\"\n        obj = self.obj\n        args = self.args\n        kwargs = self.kwargs\n\n        if isinstance(func, str):\n            return self._apply_str(obj, func, *args, **kwargs)\n\n        if not args and not kwargs:\n            f = com.get_cython_func(func)\n            if f:\n                warn_alias_replacement(obj, func, f)\n                return getattr(obj, f)()\n\n        # Two possible ways to use a UDF - apply or call directly\n        try:\n            return obj.apply(func, args=args, **kwargs)\n        except Exception:\n            return func(obj, *args, **kwargs)\n\n    def agg_list_like(self) -> DataFrame | Series:\n        \"\"\"\n        Compute aggregation in the case of a list-like argument.\n\n        Returns\n        -------\n        Result of aggregation.\n        \"\"\"\n        return self.agg_or_apply_list_like(op_name=\"agg\")\n\n    def compute_list_like(\n        self,\n        op_name: Literal[\"agg\", \"apply\"],\n        selected_obj: Series | DataFrame,\n        kwargs: dict[str, Any],\n    ) -> tuple[list[Hashable] | Index, list[Any]]:\n        \"\"\"\n        Compute agg/apply results for like-like input.\n\n        Parameters\n        ----------\n        op_name : {\"agg\", \"apply\"}\n            Operation being performed.\n        selected_obj : Series or DataFrame\n            Data to perform operation on.\n        kwargs : dict\n            Keyword arguments to pass to the functions.\n\n        Returns\n        -------\n        keys : list[Hashable] or Index\n            Index labels for result.\n        results : list\n            Data for result. When aggregating with a Series, this can contain any\n            Python objects.\n        \"\"\"\n        func = cast(list[AggFuncTypeBase], self.func)\n        obj = self.obj\n\n        results = []\n        keys = []\n\n        # degenerate case\n        if selected_obj.ndim == 1:\n            for a in func:\n                colg = obj._gotitem(selected_obj.name, ndim=1, subset=selected_obj)\n                args = (\n                    [self.axis, *self.args]\n                    if include_axis(op_name, colg)\n                    else self.args\n                )\n                new_res = getattr(colg, op_name)(a, *args, **kwargs)\n                results.append(new_res)\n\n                # make sure we find a good name\n                name = com.get_callable_name(a) or a\n                keys.append(name)\n\n        else:\n            indices = []\n            for index, col in enumerate(selected_obj):\n                colg = obj._gotitem(col, ndim=1, subset=selected_obj.iloc[:, index])\n                args = (\n                    [self.axis, *self.args]\n                    if include_axis(op_name, colg)\n                    else self.args\n                )\n                new_res = getattr(colg, op_name)(func, *args, **kwargs)\n                results.append(new_res)\n                indices.append(index)\n            # error: Incompatible types in assignment (expression has type \"Any |\n            # Index\", variable has type \"list[Any | Callable[..., Any] | str]\")\n            keys = selected_obj.columns.take(indices)  # type: ignore[assignment]\n\n        return keys, results\n\n    def wrap_results_list_like(\n        self, keys: Iterable[Hashable], results: list[Series | DataFrame]\n    ):\n        from pandas.core.reshape.concat import concat\n\n        obj = self.obj\n\n        try:\n            return concat(results, keys=keys, axis=1, sort=False)\n        except TypeError as err:\n            # we are concatting non-NDFrame objects,\n            # e.g. a list of scalars\n            from pandas import Series\n\n            result = Series(results, index=keys, name=obj.name)\n            if is_nested_object(result):\n                raise ValueError(\n                    \"cannot combine transform and aggregation operations\"\n                ) from err\n            return result\n\n    def agg_dict_like(self) -> DataFrame | Series:\n        \"\"\"\n        Compute aggregation in the case of a dict-like argument.\n\n        Returns\n        -------\n        Result of aggregation.\n        \"\"\"\n        return self.agg_or_apply_dict_like(op_name=\"agg\")\n\n    def compute_dict_like(\n        self,\n        op_name: Literal[\"agg\", \"apply\"],\n        selected_obj: Series | DataFrame,\n        selection: Hashable | Sequence[Hashable],\n        kwargs: dict[str, Any],\n    ) -> tuple[list[Hashable], list[Any]]:\n        \"\"\"\n        Compute agg/apply results for dict-like input.\n\n        Parameters\n        ----------\n        op_name : {\"agg\", \"apply\"}\n            Operation being performed.\n        selected_obj : Series or DataFrame\n            Data to perform operation on.\n        selection : hashable or sequence of hashables\n            Used by GroupBy, Window, and Resample if selection is applied to the object.\n        kwargs : dict\n            Keyword arguments to pass to the functions.\n\n        Returns\n        -------\n        keys : list[hashable]\n            Index labels for result.\n        results : list\n            Data for result. When aggregating with a Series, this can contain any\n            Python object.\n        \"\"\"\n        from pandas.core.groupby.generic import (\n            DataFrameGroupBy,\n            SeriesGroupBy,\n        )\n\n        obj = self.obj\n        is_groupby = isinstance(obj, (DataFrameGroupBy, SeriesGroupBy))\n        func = cast(AggFuncTypeDict, self.func)\n        func = self.normalize_dictlike_arg(op_name, selected_obj, func)\n\n        is_non_unique_col = (\n            selected_obj.ndim == 2\n            and selected_obj.columns.nunique() < len(selected_obj.columns)\n        )\n\n        if selected_obj.ndim == 1:\n            # key only used for output\n            colg = obj._gotitem(selection, ndim=1)\n            results = [getattr(colg, op_name)(how, **kwargs) for _, how in func.items()]\n            keys = list(func.keys())\n        elif not is_groupby and is_non_unique_col:\n            # key used for column selection and output\n            # GH#51099\n            results = []\n            keys = []\n            for key, how in func.items():\n                indices = selected_obj.columns.get_indexer_for([key])\n                labels = selected_obj.columns.take(indices)\n                label_to_indices = defaultdict(list)\n                for index, label in zip(indices, labels):\n                    label_to_indices[label].append(index)\n\n                key_data = [\n                    getattr(selected_obj._ixs(indice, axis=1), op_name)(how, **kwargs)\n                    for label, indices in label_to_indices.items()\n                    for indice in indices\n                ]\n\n                keys += [key] * len(key_data)\n                results += key_data\n        else:\n            # key used for column selection and output\n            results = [\n                getattr(obj._gotitem(key, ndim=1), op_name)(how, **kwargs)\n                for key, how in func.items()\n            ]\n            keys = list(func.keys())\n\n        return keys, results\n\n    def wrap_results_dict_like(\n        self,\n        selected_obj: Series | DataFrame,\n        result_index: list[Hashable],\n        result_data: list,\n    ):\n        from pandas import Index\n        from pandas.core.reshape.concat import concat\n\n        obj = self.obj\n\n        # Avoid making two isinstance calls in all and any below\n        is_ndframe = [isinstance(r, ABCNDFrame) for r in result_data]\n\n        if all(is_ndframe):\n            results = dict(zip(result_index, result_data))\n            keys_to_use: Iterable[Hashable]\n            keys_to_use = [k for k in result_index if not results[k].empty]\n            # Have to check, if at least one DataFrame is not empty.\n            keys_to_use = keys_to_use if keys_to_use != [] else result_index\n            if selected_obj.ndim == 2:\n                # keys are columns, so we can preserve names\n                ktu = Index(keys_to_use)\n                ktu._set_names(selected_obj.columns.names)\n                keys_to_use = ktu\n\n            axis: AxisInt = 0 if isinstance(obj, ABCSeries) else 1\n            result = concat(\n                {k: results[k] for k in keys_to_use},\n                axis=axis,\n                keys=keys_to_use,\n            )\n        elif any(is_ndframe):\n            # There is a mix of NDFrames and scalars\n            raise ValueError(\n                \"cannot perform both aggregation \"\n                \"and transformation operations \"\n                \"simultaneously\"\n            )\n        else:\n            from pandas import Series\n\n            # we have a list of scalars\n            # GH 36212 use name only if obj is a series\n            if obj.ndim == 1:\n                obj = cast(\"Series\", obj)\n                name = obj.name\n            else:\n                name = None\n\n            result = Series(result_data, index=result_index, name=name)\n\n        return result\n\n    def apply_str(self) -> DataFrame | Series:\n        \"\"\"\n        Compute apply in case of a string.\n\n        Returns\n        -------\n        result: Series or DataFrame\n        \"\"\"\n        # Caller is responsible for checking isinstance(self.f, str)\n        func = cast(str, self.func)\n\n        obj = self.obj\n\n        from pandas.core.groupby.generic import (\n            DataFrameGroupBy,\n            SeriesGroupBy,\n        )\n\n        # Support for `frame.transform('method')`\n        # Some methods (shift, etc.) require the axis argument, others\n        # don't, so inspect and insert if necessary.\n        method = getattr(obj, func, None)\n        if callable(method):\n            sig = inspect.getfullargspec(method)\n            arg_names = (*sig.args, *sig.kwonlyargs)\n            if self.axis != 0 and (\n                \"axis\" not in arg_names or func in (\"corrwith\", \"skew\")\n            ):\n                raise ValueError(f\"Operation {func} does not support axis=1\")\n            if \"axis\" in arg_names:\n                if isinstance(obj, (SeriesGroupBy, DataFrameGroupBy)):\n                    # Try to avoid FutureWarning for deprecated axis keyword;\n                    # If self.axis matches the axis we would get by not passing\n                    #  axis, we safely exclude the keyword.\n\n                    default_axis = 0\n                    if func in [\"idxmax\", \"idxmin\"]:\n                        # DataFrameGroupBy.idxmax, idxmin axis defaults to self.axis,\n                        # whereas other axis keywords default to 0\n                        default_axis = self.obj.axis\n\n                    if default_axis != self.axis:\n                        self.kwargs[\"axis\"] = self.axis\n                else:\n                    self.kwargs[\"axis\"] = self.axis\n        return self._apply_str(obj, func, *self.args, **self.kwargs)\n\n    def apply_list_or_dict_like(self) -> DataFrame | Series:\n        \"\"\"\n        Compute apply in case of a list-like or dict-like.\n\n        Returns\n        -------\n        result: Series, DataFrame, or None\n            Result when self.func is a list-like or dict-like, None otherwise.\n        \"\"\"\n\n        if self.engine == \"numba\":\n            raise NotImplementedError(\n                \"The 'numba' engine doesn't support list-like/\"\n                \"dict likes of callables yet.\"\n            )\n\n        if self.axis == 1 and isinstance(self.obj, ABCDataFrame):\n            return self.obj.T.apply(self.func, 0, args=self.args, **self.kwargs).T\n\n        func = self.func\n        kwargs = self.kwargs\n\n        if is_dict_like(func):\n            result = self.agg_or_apply_dict_like(op_name=\"apply\")\n        else:\n            result = self.agg_or_apply_list_like(op_name=\"apply\")\n\n        result = reconstruct_and_relabel_result(result, func, **kwargs)\n\n        return result\n\n    def normalize_dictlike_arg(\n        self, how: str, obj: DataFrame | Series, func: AggFuncTypeDict\n    ) -> AggFuncTypeDict:\n        \"\"\"\n        Handler for dict-like argument.\n\n        Ensures that necessary columns exist if obj is a DataFrame, and\n        that a nested renamer is not passed. Also normalizes to all lists\n        when values consists of a mix of list and non-lists.\n        \"\"\"\n        assert how in (\"apply\", \"agg\", \"transform\")\n\n        # Can't use func.values(); wouldn't work for a Series\n        if (\n            how == \"agg\"\n            and isinstance(obj, ABCSeries)\n            and any(is_list_like(v) for _, v in func.items())\n        ) or (any(is_dict_like(v) for _, v in func.items())):\n            # GH 15931 - deprecation of renaming keys\n            raise SpecificationError(\"nested renamer is not supported\")\n\n        if obj.ndim != 1:\n            # Check for missing columns on a frame\n            from pandas import Index\n\n            cols = Index(list(func.keys())).difference(obj.columns, sort=True)\n            if len(cols) > 0:\n                raise KeyError(f\"Column(s) {list(cols)} do not exist\")\n\n        aggregator_types = (list, tuple, dict)\n\n        # if we have a dict of any non-scalars\n        # eg. {'A' : ['mean']}, normalize all to\n        # be list-likes\n        # Cannot use func.values() because arg may be a Series\n        if any(isinstance(x, aggregator_types) for _, x in func.items()):\n            new_func: AggFuncTypeDict = {}\n            for k, v in func.items():\n                if not isinstance(v, aggregator_types):\n                    new_func[k] = [v]\n                else:\n                    new_func[k] = v\n            func = new_func\n        return func\n\n    def _apply_str(self, obj, func: str, *args, **kwargs):\n        \"\"\"\n        if arg is a string, then try to operate on it:\n        - try to find a function (or attribute) on obj\n        - try to find a numpy function\n        - raise\n        \"\"\"\n        assert isinstance(func, str)\n\n        if hasattr(obj, func):\n            f = getattr(obj, func)\n            if callable(f):\n                return f(*args, **kwargs)\n\n            # people may aggregate on a non-callable attribute\n            # but don't let them think they can pass args to it\n            assert len(args) == 0\n            assert len([kwarg for kwarg in kwargs if kwarg not in [\"axis\"]]) == 0\n            return f\n        elif hasattr(np, func) and hasattr(obj, \"__array__\"):\n            # in particular exclude Window\n            f = getattr(np, func)\n            return f(obj, *args, **kwargs)\n        else:\n            msg = f\"'{func}' is not a valid function for '{type(obj).__name__}' object\"\n            raise AttributeError(msg)\n", "class_fn": true, "question_id": "pandas/pandas.core.apply/Apply", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/apply.py", "fn_id": "", "content": "class FrameRowApply(FrameApply):\n    axis: AxisInt = 0\n\n    @property\n    def series_generator(self) -> Generator[Series, None, None]:\n        return (self.obj._ixs(i, axis=1) for i in range(len(self.columns)))\n\n    @staticmethod\n    @functools.cache\n    def generate_numba_apply_func(\n        func, nogil=True, nopython=True, parallel=False\n    ) -> Callable[[npt.NDArray, Index, Index], dict[int, Any]]:\n        numba = import_optional_dependency(\"numba\")\n        from pandas import Series\n\n        # Import helper from extensions to cast string object -> np strings\n        # Note: This also has the side effect of loading our numba extensions\n        from pandas.core._numba.extensions import maybe_cast_str\n\n        jitted_udf = numba.extending.register_jitable(func)\n\n        # Currently the parallel argument doesn't get passed through here\n        # (it's disabled) since the dicts in numba aren't thread-safe.\n        @numba.jit(nogil=nogil, nopython=nopython, parallel=parallel)\n        def numba_func(values, col_names, df_index):\n            results = {}\n            for j in range(values.shape[1]):\n                # Create the series\n                ser = Series(\n                    values[:, j], index=df_index, name=maybe_cast_str(col_names[j])\n                )\n                results[j] = jitted_udf(ser)\n            return results\n\n        return numba_func\n\n    def apply_with_numba(self) -> dict[int, Any]:\n        nb_func = self.generate_numba_apply_func(\n            cast(Callable, self.func), **self.engine_kwargs\n        )\n        from pandas.core._numba.extensions import set_numba_data\n\n        index = self.obj.index\n        if index.dtype == \"string\":\n            index = index.astype(object)\n\n        columns = self.obj.columns\n        if columns.dtype == \"string\":\n            columns = columns.astype(object)\n\n        # Convert from numba dict to regular dict\n        # Our isinstance checks in the df constructor don't pass for numbas typed dict\n        with set_numba_data(index) as index, set_numba_data(columns) as columns:\n            res = dict(nb_func(self.values, columns, index))\n        return res\n\n    @property\n    def result_index(self) -> Index:\n        return self.columns\n\n    @property\n    def result_columns(self) -> Index:\n        return self.index\n\n    def wrap_results_for_axis(\n        self, results: ResType, res_index: Index\n    ) -> DataFrame | Series:\n        \"\"\"return the results for the rows\"\"\"\n\n        if self.result_type == \"reduce\":\n            # e.g. test_apply_dict GH#8735\n            res = self.obj._constructor_sliced(results)\n            res.index = res_index\n            return res\n\n        elif self.result_type is None and all(\n            isinstance(x, dict) for x in results.values()\n        ):\n            # Our operation was a to_dict op e.g.\n            #  test_apply_dict GH#8735, test_apply_reduce_to_dict GH#25196 #37544\n            res = self.obj._constructor_sliced(results)\n            res.index = res_index\n            return res\n\n        try:\n            result = self.obj._constructor(data=results)\n        except ValueError as err:\n            if \"All arrays must be of the same length\" in str(err):\n                # e.g. result = [[2, 3], [1.5], ['foo', 'bar']]\n                #  see test_agg_listlike_result GH#29587\n                res = self.obj._constructor_sliced(results)\n                res.index = res_index\n                return res\n            else:\n                raise\n\n        if not isinstance(results[0], ABCSeries):\n            if len(result.index) == len(self.res_columns):\n                result.index = self.res_columns\n\n        if len(result.columns) == len(res_index):\n            result.columns = res_index\n\n        return result\n", "class_fn": true, "question_id": "pandas/pandas.core.apply/FrameRowApply", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arraylike.py", "fn_id": "", "content": "class OpsMixin:\n    # -------------------------------------------------------------\n    # Comparisons\n\n    def _cmp_method(self, other, op):\n        return NotImplemented\n\n    @unpack_zerodim_and_defer(\"__eq__\")\n    def __eq__(self, other):\n        return self._cmp_method(other, operator.eq)\n\n    @unpack_zerodim_and_defer(\"__ne__\")\n    def __ne__(self, other):\n        return self._cmp_method(other, operator.ne)\n\n    @unpack_zerodim_and_defer(\"__lt__\")\n    def __lt__(self, other):\n        return self._cmp_method(other, operator.lt)\n\n    @unpack_zerodim_and_defer(\"__le__\")\n    def __le__(self, other):\n        return self._cmp_method(other, operator.le)\n\n    @unpack_zerodim_and_defer(\"__gt__\")\n    def __gt__(self, other):\n        return self._cmp_method(other, operator.gt)\n\n    @unpack_zerodim_and_defer(\"__ge__\")\n    def __ge__(self, other):\n        return self._cmp_method(other, operator.ge)\n\n    # -------------------------------------------------------------\n    # Logical Methods\n\n    def _logical_method(self, other, op):\n        return NotImplemented\n\n    @unpack_zerodim_and_defer(\"__and__\")\n    def __and__(self, other):\n        return self._logical_method(other, operator.and_)\n\n    @unpack_zerodim_and_defer(\"__rand__\")\n    def __rand__(self, other):\n        return self._logical_method(other, roperator.rand_)\n\n    @unpack_zerodim_and_defer(\"__or__\")\n    def __or__(self, other):\n        return self._logical_method(other, operator.or_)\n\n    @unpack_zerodim_and_defer(\"__ror__\")\n    def __ror__(self, other):\n        return self._logical_method(other, roperator.ror_)\n\n    @unpack_zerodim_and_defer(\"__xor__\")\n    def __xor__(self, other):\n        return self._logical_method(other, operator.xor)\n\n    @unpack_zerodim_and_defer(\"__rxor__\")\n    def __rxor__(self, other):\n        return self._logical_method(other, roperator.rxor)\n\n    # -------------------------------------------------------------\n    # Arithmetic Methods\n\n    def _arith_method(self, other, op):\n        return NotImplemented\n\n    @unpack_zerodim_and_defer(\"__add__\")\n    def __add__(self, other):\n        \"\"\"\n        Get Addition of DataFrame and other, column-wise.\n\n        Equivalent to ``DataFrame.add(other)``.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, dict or DataFrame\n            Object to be added to the DataFrame.\n\n        Returns\n        -------\n        DataFrame\n            The result of adding ``other`` to DataFrame.\n\n        See Also\n        --------\n        DataFrame.add : Add a DataFrame and another object, with option for index-\n            or column-oriented addition.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'height': [1.5, 2.6], 'weight': [500, 800]},\n        ...                   index=['elk', 'moose'])\n        >>> df\n               height  weight\n        elk       1.5     500\n        moose     2.6     800\n\n        Adding a scalar affects all rows and columns.\n\n        >>> df[['height', 'weight']] + 1.5\n               height  weight\n        elk       3.0   501.5\n        moose     4.1   801.5\n\n        Each element of a list is added to a column of the DataFrame, in order.\n\n        >>> df[['height', 'weight']] + [0.5, 1.5]\n               height  weight\n        elk       2.0   501.5\n        moose     3.1   801.5\n\n        Keys of a dictionary are aligned to the DataFrame, based on column names;\n        each value in the dictionary is added to the corresponding column.\n\n        >>> df[['height', 'weight']] + {'height': 0.5, 'weight': 1.5}\n               height  weight\n        elk       2.0   501.5\n        moose     3.1   801.5\n\n        When `other` is a :class:`Series`, the index of `other` is aligned with the\n        columns of the DataFrame.\n\n        >>> s1 = pd.Series([0.5, 1.5], index=['weight', 'height'])\n        >>> df[['height', 'weight']] + s1\n               height  weight\n        elk       3.0   500.5\n        moose     4.1   800.5\n\n        Even when the index of `other` is the same as the index of the DataFrame,\n        the :class:`Series` will not be reoriented. If index-wise alignment is desired,\n        :meth:`DataFrame.add` should be used with `axis='index'`.\n\n        >>> s2 = pd.Series([0.5, 1.5], index=['elk', 'moose'])\n        >>> df[['height', 'weight']] + s2\n               elk  height  moose  weight\n        elk    NaN     NaN    NaN     NaN\n        moose  NaN     NaN    NaN     NaN\n\n        >>> df[['height', 'weight']].add(s2, axis='index')\n               height  weight\n        elk       2.0   500.5\n        moose     4.1   801.5\n\n        When `other` is a :class:`DataFrame`, both columns names and the\n        index are aligned.\n\n        >>> other = pd.DataFrame({'height': [0.2, 0.4, 0.6]},\n        ...                      index=['elk', 'moose', 'deer'])\n        >>> df[['height', 'weight']] + other\n               height  weight\n        deer      NaN     NaN\n        elk       1.7     NaN\n        moose     3.0     NaN\n        \"\"\"\n        return self._arith_method(other, operator.add)\n\n    @unpack_zerodim_and_defer(\"__radd__\")\n    def __radd__(self, other):\n        return self._arith_method(other, roperator.radd)\n\n    @unpack_zerodim_and_defer(\"__sub__\")\n    def __sub__(self, other):\n        return self._arith_method(other, operator.sub)\n\n    @unpack_zerodim_and_defer(\"__rsub__\")\n    def __rsub__(self, other):\n        return self._arith_method(other, roperator.rsub)\n\n    @unpack_zerodim_and_defer(\"__mul__\")\n    def __mul__(self, other):\n        return self._arith_method(other, operator.mul)\n\n    @unpack_zerodim_and_defer(\"__rmul__\")\n    def __rmul__(self, other):\n        return self._arith_method(other, roperator.rmul)\n\n    @unpack_zerodim_and_defer(\"__truediv__\")\n    def __truediv__(self, other):\n        return self._arith_method(other, operator.truediv)\n\n    @unpack_zerodim_and_defer(\"__rtruediv__\")\n    def __rtruediv__(self, other):\n        return self._arith_method(other, roperator.rtruediv)\n\n    @unpack_zerodim_and_defer(\"__floordiv__\")\n    def __floordiv__(self, other):\n        return self._arith_method(other, operator.floordiv)\n\n    @unpack_zerodim_and_defer(\"__rfloordiv\")\n    def __rfloordiv__(self, other):\n        return self._arith_method(other, roperator.rfloordiv)\n\n    @unpack_zerodim_and_defer(\"__mod__\")\n    def __mod__(self, other):\n        return self._arith_method(other, operator.mod)\n\n    @unpack_zerodim_and_defer(\"__rmod__\")\n    def __rmod__(self, other):\n        return self._arith_method(other, roperator.rmod)\n\n    @unpack_zerodim_and_defer(\"__divmod__\")\n    def __divmod__(self, other):\n        return self._arith_method(other, divmod)\n\n    @unpack_zerodim_and_defer(\"__rdivmod__\")\n    def __rdivmod__(self, other):\n        return self._arith_method(other, roperator.rdivmod)\n\n    @unpack_zerodim_and_defer(\"__pow__\")\n    def __pow__(self, other):\n        return self._arith_method(other, operator.pow)\n\n    @unpack_zerodim_and_defer(\"__rpow__\")\n    def __rpow__(self, other):\n        return self._arith_method(other, roperator.rpow)\n", "class_fn": true, "question_id": "pandas/pandas.core.arraylike/OpsMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/arrow/accessors.py", "fn_id": "", "content": "class ListAccessor(ArrowAccessor):\n    \"\"\"\n    Accessor object for list data properties of the Series values.\n\n    Parameters\n    ----------\n    data : Series\n        Series containing Arrow list data.\n    \"\"\"\n\n    def __init__(self, data=None) -> None:\n        super().__init__(\n            data,\n            validation_msg=\"Can only use the '.list' accessor with \"\n            \"'list[pyarrow]' dtype, not {dtype}.\",\n        )\n\n    def _is_valid_pyarrow_dtype(self, pyarrow_dtype) -> bool:\n        return (\n            pa.types.is_list(pyarrow_dtype)\n            or pa.types.is_fixed_size_list(pyarrow_dtype)\n            or pa.types.is_large_list(pyarrow_dtype)\n        )\n\n    def len(self) -> Series:\n        \"\"\"\n        Return the length of each list in the Series.\n\n        Returns\n        -------\n        pandas.Series\n            The length of each list.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> s = pd.Series(\n        ...     [\n        ...         [1, 2, 3],\n        ...         [3],\n        ...     ],\n        ...     dtype=pd.ArrowDtype(pa.list_(\n        ...         pa.int64()\n        ...     ))\n        ... )\n        >>> s.list.len()\n        0    3\n        1    1\n        dtype: int32[pyarrow]\n        \"\"\"\n        from pandas import Series\n\n        value_lengths = pc.list_value_length(self._pa_array)\n        return Series(value_lengths, dtype=ArrowDtype(value_lengths.type))\n\n    def __getitem__(self, key: int | slice) -> Series:\n        \"\"\"\n        Index or slice lists in the Series.\n\n        Parameters\n        ----------\n        key : int | slice\n            Index or slice of indices to access from each list.\n\n        Returns\n        -------\n        pandas.Series\n            The list at requested index.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> s = pd.Series(\n        ...     [\n        ...         [1, 2, 3],\n        ...         [3],\n        ...     ],\n        ...     dtype=pd.ArrowDtype(pa.list_(\n        ...         pa.int64()\n        ...     ))\n        ... )\n        >>> s.list[0]\n        0    1\n        1    3\n        dtype: int64[pyarrow]\n        \"\"\"\n        from pandas import Series\n\n        if isinstance(key, int):\n            # TODO: Support negative key but pyarrow does not allow\n            # element index to be an array.\n            # if key < 0:\n            #     key = pc.add(key, pc.list_value_length(self._pa_array))\n            element = pc.list_element(self._pa_array, key)\n            return Series(element, dtype=ArrowDtype(element.type))\n        elif isinstance(key, slice):\n            if pa_version_under11p0:\n                raise NotImplementedError(\n                    f\"List slice not supported by pyarrow {pa.__version__}.\"\n                )\n\n            # TODO: Support negative start/stop/step, ideally this would be added\n            # upstream in pyarrow.\n            start, stop, step = key.start, key.stop, key.step\n            if start is None:\n                # TODO: When adding negative step support\n                #  this should be setto last element of array\n                # when step is negative.\n                start = 0\n            if step is None:\n                step = 1\n            sliced = pc.list_slice(self._pa_array, start, stop, step)\n            return Series(sliced, dtype=ArrowDtype(sliced.type))\n        else:\n            raise ValueError(f\"key must be an int or slice, got {type(key).__name__}\")\n\n    def __iter__(self) -> Iterator:\n        raise TypeError(f\"'{type(self).__name__}' object is not iterable\")\n\n    def flatten(self) -> Series:\n        \"\"\"\n        Flatten list values.\n\n        Returns\n        -------\n        pandas.Series\n            The data from all lists in the series flattened.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> s = pd.Series(\n        ...     [\n        ...         [1, 2, 3],\n        ...         [3],\n        ...     ],\n        ...     dtype=pd.ArrowDtype(pa.list_(\n        ...         pa.int64()\n        ...     ))\n        ... )\n        >>> s.list.flatten()\n        0    1\n        1    2\n        2    3\n        3    3\n        dtype: int64[pyarrow]\n        \"\"\"\n        from pandas import Series\n\n        flattened = pc.list_flatten(self._pa_array)\n        return Series(flattened, dtype=ArrowDtype(flattened.type))\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.arrow.accessors/ListAccessor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/base.py", "fn_id": "", "content": "class ExtensionArray:\n    \"\"\"\n    Abstract base class for custom 1-D array types.\n\n    pandas will recognize instances of this class as proper arrays\n    with a custom type and will not attempt to coerce them to objects. They\n    may be stored directly inside a :class:`DataFrame` or :class:`Series`.\n\n    Attributes\n    ----------\n    dtype\n    nbytes\n    ndim\n    shape\n\n    Methods\n    -------\n    argsort\n    astype\n    copy\n    dropna\n    duplicated\n    factorize\n    fillna\n    equals\n    insert\n    interpolate\n    isin\n    isna\n    ravel\n    repeat\n    searchsorted\n    shift\n    take\n    tolist\n    unique\n    view\n    _accumulate\n    _concat_same_type\n    _explode\n    _formatter\n    _from_factorized\n    _from_sequence\n    _from_sequence_of_strings\n    _hash_pandas_object\n    _pad_or_backfill\n    _reduce\n    _values_for_argsort\n    _values_for_factorize\n\n    Notes\n    -----\n    The interface includes the following abstract methods that must be\n    implemented by subclasses:\n\n    * _from_sequence\n    * _from_factorized\n    * __getitem__\n    * __len__\n    * __eq__\n    * dtype\n    * nbytes\n    * isna\n    * take\n    * copy\n    * _concat_same_type\n    * interpolate\n\n    A default repr displaying the type, (truncated) data, length,\n    and dtype is provided. It can be customized or replaced by\n    by overriding:\n\n    * __repr__ : A default repr for the ExtensionArray.\n    * _formatter : Print scalars inside a Series or DataFrame.\n\n    Some methods require casting the ExtensionArray to an ndarray of Python\n    objects with ``self.astype(object)``, which may be expensive. When\n    performance is a concern, we highly recommend overriding the following\n    methods:\n\n    * fillna\n    * _pad_or_backfill\n    * dropna\n    * unique\n    * factorize / _values_for_factorize\n    * argsort, argmax, argmin / _values_for_argsort\n    * searchsorted\n    * map\n\n    The remaining methods implemented on this class should be performant,\n    as they only compose abstract methods. Still, a more efficient\n    implementation may be available, and these methods can be overridden.\n\n    One can implement methods to handle array accumulations or reductions.\n\n    * _accumulate\n    * _reduce\n\n    One can implement methods to handle parsing from strings that will be used\n    in methods such as ``pandas.io.parsers.read_csv``.\n\n    * _from_sequence_of_strings\n\n    This class does not inherit from 'abc.ABCMeta' for performance reasons.\n    Methods and properties required by the interface raise\n    ``pandas.errors.AbstractMethodError`` and no ``register`` method is\n    provided for registering virtual subclasses.\n\n    ExtensionArrays are limited to 1 dimension.\n\n    They may be backed by none, one, or many NumPy arrays. For example,\n    ``pandas.Categorical`` is an extension array backed by two arrays,\n    one for codes and one for categories. An array of IPv6 address may\n    be backed by a NumPy structured array with two fields, one for the\n    lower 64 bits and one for the upper 64 bits. Or they may be backed\n    by some other storage type, like Python lists. Pandas makes no\n    assumptions on how the data are stored, just that it can be converted\n    to a NumPy array.\n    The ExtensionArray interface does not impose any rules on how this data\n    is stored. However, currently, the backing data cannot be stored in\n    attributes called ``.values`` or ``._values`` to ensure full compatibility\n    with pandas internals. But other names as ``.data``, ``._data``,\n    ``._items``, ... can be freely used.\n\n    If implementing NumPy's ``__array_ufunc__`` interface, pandas expects\n    that\n\n    1. You defer by returning ``NotImplemented`` when any Series are present\n       in `inputs`. Pandas will extract the arrays and call the ufunc again.\n    2. You define a ``_HANDLED_TYPES`` tuple as an attribute on the class.\n       Pandas inspect this to determine whether the ufunc is valid for the\n       types present.\n\n    See :ref:`extending.extension.ufunc` for more.\n\n    By default, ExtensionArrays are not hashable.  Immutable subclasses may\n    override this behavior.\n\n    Examples\n    --------\n    Please see the following:\n\n    https://github.com/pandas-dev/pandas/blob/main/pandas/tests/extension/list/array.py\n    \"\"\"\n\n    # '_typ' is for pandas.core.dtypes.generic.ABCExtensionArray.\n    # Don't override this.\n    _typ = \"extension\"\n\n    # similar to __array_priority__, positions ExtensionArray after Index,\n    #  Series, and DataFrame.  EA subclasses may override to choose which EA\n    #  subclass takes priority. If overriding, the value should always be\n    #  strictly less than 2000 to be below Index.__pandas_priority__.\n    __pandas_priority__ = 1000\n\n    # ------------------------------------------------------------------------\n    # Constructors\n    # ------------------------------------------------------------------------\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype: Dtype | None = None, copy: bool = False):\n        \"\"\"\n        Construct a new ExtensionArray from a sequence of scalars.\n\n        Parameters\n        ----------\n        scalars : Sequence\n            Each element will be an instance of the scalar type for this\n            array, ``cls.dtype.type`` or be converted into this type in this method.\n        dtype : dtype, optional\n            Construct for this particular dtype. This should be a Dtype\n            compatible with the ExtensionArray.\n        copy : bool, default False\n            If True, copy the underlying data.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> pd.arrays.IntegerArray._from_sequence([4, 5])\n        <IntegerArray>\n        [4, 5]\n        Length: 2, dtype: Int64\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _from_scalars(cls, scalars, *, dtype: DtypeObj) -> Self:\n        \"\"\"\n        Strict analogue to _from_sequence, allowing only sequences of scalars\n        that should be specifically inferred to the given dtype.\n\n        Parameters\n        ----------\n        scalars : sequence\n        dtype : ExtensionDtype\n\n        Raises\n        ------\n        TypeError or ValueError\n\n        Notes\n        -----\n        This is called in a try/except block when casting the result of a\n        pointwise operation.\n        \"\"\"\n        try:\n            return cls._from_sequence(scalars, dtype=dtype, copy=False)\n        except (ValueError, TypeError):\n            raise\n        except Exception:\n            warnings.warn(\n                \"_from_scalars should only raise ValueError or TypeError. \"\n                \"Consider overriding _from_scalars where appropriate.\",\n                stacklevel=find_stack_level(),\n            )\n            raise\n\n    @classmethod\n    def _from_sequence_of_strings(\n        cls, strings, *, dtype: Dtype | None = None, copy: bool = False\n    ):\n        \"\"\"\n        Construct a new ExtensionArray from a sequence of strings.\n\n        Parameters\n        ----------\n        strings : Sequence\n            Each element will be an instance of the scalar type for this\n            array, ``cls.dtype.type``.\n        dtype : dtype, optional\n            Construct for this particular dtype. This should be a Dtype\n            compatible with the ExtensionArray.\n        copy : bool, default False\n            If True, copy the underlying data.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> pd.arrays.IntegerArray._from_sequence_of_strings([\"1\", \"2\", \"3\"])\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        \"\"\"\n        Reconstruct an ExtensionArray after factorization.\n\n        Parameters\n        ----------\n        values : ndarray\n            An integer ndarray with the factorized values.\n        original : ExtensionArray\n            The original ExtensionArray that factorize was called on.\n\n        See Also\n        --------\n        factorize : Top-level factorize method that dispatches here.\n        ExtensionArray.factorize : Encode the extension array as an enumerated type.\n\n        Examples\n        --------\n        >>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1),\n        ...                                      pd.Interval(1, 5), pd.Interval(1, 5)])\n        >>> codes, uniques = pd.factorize(interv_arr)\n        >>> pd.arrays.IntervalArray._from_factorized(uniques, interv_arr)\n        <IntervalArray>\n        [(0, 1], (1, 5]]\n        Length: 2, dtype: interval[int64, right]\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    # ------------------------------------------------------------------------\n    # Must be a Sequence\n    # ------------------------------------------------------------------------\n    @overload\n    def __getitem__(self, item: ScalarIndexer) -> Any:\n        ...\n\n    @overload\n    def __getitem__(self, item: SequenceIndexer) -> Self:\n        ...\n\n    def __getitem__(self, item: PositionalIndexer) -> Self | Any:\n        \"\"\"\n        Select a subset of self.\n\n        Parameters\n        ----------\n        item : int, slice, or ndarray\n            * int: The position in 'self' to get.\n\n            * slice: A slice object, where 'start', 'stop', and 'step' are\n              integers or None\n\n            * ndarray: A 1-d boolean NumPy ndarray the same length as 'self'\n\n            * list[int]:  A list of int\n\n        Returns\n        -------\n        item : scalar or ExtensionArray\n\n        Notes\n        -----\n        For scalar ``item``, return a scalar value suitable for the array's\n        type. This should be an instance of ``self.dtype.type``.\n\n        For slice ``key``, return an instance of ``ExtensionArray``, even\n        if the slice is length 0 or 1.\n\n        For a boolean mask, return an instance of ``ExtensionArray``, filtered\n        to the values where ``item`` is True.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def __setitem__(self, key, value) -> None:\n        \"\"\"\n        Set one or more values inplace.\n\n        This method is not required to satisfy the pandas extension array\n        interface.\n\n        Parameters\n        ----------\n        key : int, ndarray, or slice\n            When called from, e.g. ``Series.__setitem__``, ``key`` will be\n            one of\n\n            * scalar int\n            * ndarray of integers.\n            * boolean ndarray\n            * slice object\n\n        value : ExtensionDtype.type, Sequence[ExtensionDtype.type], or object\n            value or values to be set of ``key``.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        # Some notes to the ExtensionArray implementer who may have ended up\n        # here. While this method is not required for the interface, if you\n        # *do* choose to implement __setitem__, then some semantics should be\n        # observed:\n        #\n        # * Setting multiple values : ExtensionArrays should support setting\n        #   multiple values at once, 'key' will be a sequence of integers and\n        #  'value' will be a same-length sequence.\n        #\n        # * Broadcasting : For a sequence 'key' and a scalar 'value',\n        #   each position in 'key' should be set to 'value'.\n        #\n        # * Coercion : Most users will expect basic coercion to work. For\n        #   example, a string like '2018-01-01' is coerced to a datetime\n        #   when setting on a datetime64ns array. In general, if the\n        #   __init__ method coerces that value, then so should __setitem__\n        # Note, also, that Series/DataFrame.where internally use __setitem__\n        # on a copy of the data.\n        raise NotImplementedError(f\"{type(self)} does not implement __setitem__.\")\n\n    def __len__(self) -> int:\n        \"\"\"\n        Length of this array\n\n        Returns\n        -------\n        length : int\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def __iter__(self) -> Iterator[Any]:\n        \"\"\"\n        Iterate over elements of the array.\n        \"\"\"\n        # This needs to be implemented so that pandas recognizes extension\n        # arrays as list-like. The default implementation makes successive\n        # calls to ``__getitem__``, which may be slower than necessary.\n        for i in range(len(self)):\n            yield self[i]\n\n    def __contains__(self, item: object) -> bool | np.bool_:\n        \"\"\"\n        Return for `item in self`.\n        \"\"\"\n        # GH37867\n        # comparisons of any item to pd.NA always return pd.NA, so e.g. \"a\" in [pd.NA]\n        # would raise a TypeError. The implementation below works around that.\n        if is_scalar(item) and isna(item):\n            if not self._can_hold_na:\n                return False\n            elif item is self.dtype.na_value or isinstance(item, self.dtype.type):\n                return self._hasna\n            else:\n                return False\n        else:\n            # error: Item \"ExtensionArray\" of \"Union[ExtensionArray, ndarray]\" has no\n            # attribute \"any\"\n            return (item == self).any()  # type: ignore[union-attr]\n\n    # error: Signature of \"__eq__\" incompatible with supertype \"object\"\n    def __eq__(self, other: object) -> ArrayLike:  # type: ignore[override]\n        \"\"\"\n        Return for `self == other` (element-wise equality).\n        \"\"\"\n        # Implementer note: this should return a boolean numpy ndarray or\n        # a boolean ExtensionArray.\n        # When `other` is one of Series, Index, or DataFrame, this method should\n        # return NotImplemented (to ensure that those objects are responsible for\n        # first unpacking the arrays, and then dispatch the operation to the\n        # underlying arrays)\n        raise AbstractMethodError(self)\n\n    # error: Signature of \"__ne__\" incompatible with supertype \"object\"\n    def __ne__(self, other: object) -> ArrayLike:  # type: ignore[override]\n        \"\"\"\n        Return for `self != other` (element-wise in-equality).\n        \"\"\"\n        # error: Unsupported operand type for ~ (\"ExtensionArray\")\n        return ~(self == other)  # type: ignore[operator]\n\n    def to_numpy(\n        self,\n        dtype: npt.DTypeLike | None = None,\n        copy: bool = False,\n        na_value: object = lib.no_default,\n    ) -> np.ndarray:\n        \"\"\"\n        Convert to a NumPy ndarray.\n\n        This is similar to :meth:`numpy.asarray`, but may provide additional control\n        over how the conversion is done.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to pass to :meth:`numpy.asarray`.\n        copy : bool, default False\n            Whether to ensure that the returned value is a not a view on\n            another array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary.\n        na_value : Any, optional\n            The value to use for missing values. The default value depends\n            on `dtype` and the type of the array.\n\n        Returns\n        -------\n        numpy.ndarray\n        \"\"\"\n        result = np.asarray(self, dtype=dtype)\n        if copy or na_value is not lib.no_default:\n            result = result.copy()\n        if na_value is not lib.no_default:\n            result[self.isna()] = na_value\n        return result\n\n    # ------------------------------------------------------------------------\n    # Required attributes\n    # ------------------------------------------------------------------------\n\n    @property\n    def dtype(self) -> ExtensionDtype:\n        \"\"\"\n        An instance of ExtensionDtype.\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3]).dtype\n        Int64Dtype()\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def shape(self) -> Shape:\n        \"\"\"\n        Return a tuple of the array dimensions.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.shape\n        (3,)\n        \"\"\"\n        return (len(self),)\n\n    @property\n    def size(self) -> int:\n        \"\"\"\n        The number of elements in the array.\n        \"\"\"\n        # error: Incompatible return value type (got \"signedinteger[_64Bit]\",\n        # expected \"int\")  [return-value]\n        return np.prod(self.shape)  # type: ignore[return-value]\n\n    @property\n    def ndim(self) -> int:\n        \"\"\"\n        Extension Arrays are only allowed to be 1-dimensional.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.ndim\n        1\n        \"\"\"\n        return 1\n\n    @property\n    def nbytes(self) -> int:\n        \"\"\"\n        The number of bytes needed to store this object in memory.\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3]).nbytes\n        27\n        \"\"\"\n        # If this is expensive to compute, return an approximate lower bound\n        # on the number of bytes needed.\n        raise AbstractMethodError(self)\n\n    # ------------------------------------------------------------------------\n    # Additional Methods\n    # ------------------------------------------------------------------------\n\n    @overload\n    def astype(self, dtype: npt.DTypeLike, copy: bool = ...) -> np.ndarray:\n        ...\n\n    @overload\n    def astype(self, dtype: ExtensionDtype, copy: bool = ...) -> ExtensionArray:\n        ...\n\n    @overload\n    def astype(self, dtype: AstypeArg, copy: bool = ...) -> ArrayLike:\n        ...\n\n    def astype(self, dtype: AstypeArg, copy: bool = True) -> ArrayLike:\n        \"\"\"\n        Cast to a NumPy array or ExtensionArray with 'dtype'.\n\n        Parameters\n        ----------\n        dtype : str or dtype\n            Typecode or data-type to which the array is cast.\n        copy : bool, default True\n            Whether to copy the data, even if not necessary. If False,\n            a copy is made only if the old dtype does not match the\n            new dtype.\n\n        Returns\n        -------\n        np.ndarray or pandas.api.extensions.ExtensionArray\n            An ``ExtensionArray`` if ``dtype`` is ``ExtensionDtype``,\n            otherwise a Numpy ndarray with ``dtype`` for its dtype.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n\n        Casting to another ``ExtensionDtype`` returns an ``ExtensionArray``:\n\n        >>> arr1 = arr.astype('Float64')\n        >>> arr1\n        <FloatingArray>\n        [1.0, 2.0, 3.0]\n        Length: 3, dtype: Float64\n        >>> arr1.dtype\n        Float64Dtype()\n\n        Otherwise, we will get a Numpy ndarray:\n\n        >>> arr2 = arr.astype('float64')\n        >>> arr2\n        array([1., 2., 3.])\n        >>> arr2.dtype\n        dtype('float64')\n        \"\"\"\n        dtype = pandas_dtype(dtype)\n        if dtype == self.dtype:\n            if not copy:\n                return self\n            else:\n                return self.copy()\n\n        if isinstance(dtype, ExtensionDtype):\n            cls = dtype.construct_array_type()\n            return cls._from_sequence(self, dtype=dtype, copy=copy)\n\n        elif lib.is_np_dtype(dtype, \"M\"):\n            from pandas.core.arrays import DatetimeArray\n\n            return DatetimeArray._from_sequence(self, dtype=dtype, copy=copy)\n\n        elif lib.is_np_dtype(dtype, \"m\"):\n            from pandas.core.arrays import TimedeltaArray\n\n            return TimedeltaArray._from_sequence(self, dtype=dtype, copy=copy)\n\n        if not copy:\n            return np.asarray(self, dtype=dtype)\n        else:\n            return np.array(self, dtype=dtype, copy=copy)\n\n    def isna(self) -> np.ndarray | ExtensionArraySupportsAnyAll:\n        \"\"\"\n        A 1-D array indicating if each value is missing.\n\n        Returns\n        -------\n        numpy.ndarray or pandas.api.extensions.ExtensionArray\n            In most cases, this should return a NumPy ndarray. For\n            exceptional cases like ``SparseArray``, where returning\n            an ndarray would be expensive, an ExtensionArray may be\n            returned.\n\n        Notes\n        -----\n        If returning an ExtensionArray, then\n\n        * ``na_values._is_boolean`` should be True\n        * `na_values` should implement :func:`ExtensionArray._reduce`\n        * ``na_values.any`` and ``na_values.all`` should be implemented\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, np.nan, np.nan])\n        >>> arr.isna()\n        array([False, False,  True,  True])\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def _hasna(self) -> bool:\n        # GH#22680\n        \"\"\"\n        Equivalent to `self.isna().any()`.\n\n        Some ExtensionArray subclasses may be able to optimize this check.\n        \"\"\"\n        return bool(self.isna().any())\n\n    def _values_for_argsort(self) -> np.ndarray:\n        \"\"\"\n        Return values for sorting.\n\n        Returns\n        -------\n        ndarray\n            The transformed values should maintain the ordering between values\n            within the array.\n\n        See Also\n        --------\n        ExtensionArray.argsort : Return the indices that would sort this array.\n\n        Notes\n        -----\n        The caller is responsible for *not* modifying these values in-place, so\n        it is safe for implementers to give views on ``self``.\n\n        Functions that use this (e.g. ``ExtensionArray.argsort``) should ignore\n        entries with missing values in the original array (according to\n        ``self.isna()``). This means that the corresponding entries in the returned\n        array don't need to be modified to sort correctly.\n\n        Examples\n        --------\n        In most cases, this is the underlying Numpy array of the ``ExtensionArray``:\n\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr._values_for_argsort()\n        array([1, 2, 3])\n        \"\"\"\n        # Note: this is used in `ExtensionArray.argsort/argmin/argmax`.\n        return np.array(self)\n\n    def argsort(\n        self,\n        *,\n        ascending: bool = True,\n        kind: SortKind = \"quicksort\",\n        na_position: str = \"last\",\n        **kwargs,\n    ) -> np.ndarray:\n        \"\"\"\n        Return the indices that would sort this array.\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            Whether the indices should result in an ascending\n            or descending sort.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n            Sorting algorithm.\n        na_position : {'first', 'last'}, default 'last'\n            If ``'first'``, put ``NaN`` values at the beginning.\n            If ``'last'``, put ``NaN`` values at the end.\n        *args, **kwargs:\n            Passed through to :func:`numpy.argsort`.\n\n        Returns\n        -------\n        np.ndarray[np.intp]\n            Array of indices that sort ``self``. If NaN values are contained,\n            NaN values are placed at the end.\n\n        See Also\n        --------\n        numpy.argsort : Sorting implementation used internally.\n\n        Examples\n        --------\n        >>> arr = pd.array([3, 1, 2, 5, 4])\n        >>> arr.argsort()\n        array([1, 2, 0, 4, 3])\n        \"\"\"\n        # Implementer note: You have two places to override the behavior of\n        # argsort.\n        # 1. _values_for_argsort : construct the values passed to np.argsort\n        # 2. argsort : total control over sorting. In case of overriding this,\n        #    it is recommended to also override argmax/argmin\n        ascending = nv.validate_argsort_with_ascending(ascending, (), kwargs)\n\n        values = self._values_for_argsort()\n        return nargsort(\n            values,\n            kind=kind,\n            ascending=ascending,\n            na_position=na_position,\n            mask=np.asarray(self.isna()),\n        )\n\n    def argmin(self, skipna: bool = True) -> int:\n        \"\"\"\n        Return the index of minimum value.\n\n        In case of multiple occurrences of the minimum value, the index\n        corresponding to the first occurrence is returned.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n\n        Returns\n        -------\n        int\n\n        See Also\n        --------\n        ExtensionArray.argmax : Return the index of the maximum value.\n\n        Examples\n        --------\n        >>> arr = pd.array([3, 1, 2, 5, 4])\n        >>> arr.argmin()\n        1\n        \"\"\"\n        # Implementer note: You have two places to override the behavior of\n        # argmin.\n        # 1. _values_for_argsort : construct the values used in nargminmax\n        # 2. argmin itself : total control over sorting.\n        validate_bool_kwarg(skipna, \"skipna\")\n        if not skipna and self._hasna:\n            raise NotImplementedError\n        return nargminmax(self, \"argmin\")\n\n    def argmax(self, skipna: bool = True) -> int:\n        \"\"\"\n        Return the index of maximum value.\n\n        In case of multiple occurrences of the maximum value, the index\n        corresponding to the first occurrence is returned.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n\n        Returns\n        -------\n        int\n\n        See Also\n        --------\n        ExtensionArray.argmin : Return the index of the minimum value.\n\n        Examples\n        --------\n        >>> arr = pd.array([3, 1, 2, 5, 4])\n        >>> arr.argmax()\n        3\n        \"\"\"\n        # Implementer note: You have two places to override the behavior of\n        # argmax.\n        # 1. _values_for_argsort : construct the values used in nargminmax\n        # 2. argmax itself : total control over sorting.\n        validate_bool_kwarg(skipna, \"skipna\")\n        if not skipna and self._hasna:\n            raise NotImplementedError\n        return nargminmax(self, \"argmax\")\n\n    def interpolate(\n        self,\n        *,\n        method: InterpolateOptions,\n        axis: int,\n        index: Index,\n        limit,\n        limit_direction,\n        limit_area,\n        copy: bool,\n        **kwargs,\n    ) -> Self:\n        \"\"\"\n        See DataFrame.interpolate.__doc__.\n\n        Examples\n        --------\n        >>> arr = pd.arrays.NumpyExtensionArray(np.array([0, 1, np.nan, 3]))\n        >>> arr.interpolate(method=\"linear\",\n        ...                 limit=3,\n        ...                 limit_direction=\"forward\",\n        ...                 index=pd.Index([1, 2, 3, 4]),\n        ...                 fill_value=1,\n        ...                 copy=False,\n        ...                 axis=0,\n        ...                 limit_area=\"inside\"\n        ...                 )\n        <NumpyExtensionArray>\n        [0.0, 1.0, 2.0, 3.0]\n        Length: 4, dtype: float64\n        \"\"\"\n        # NB: we return type(self) even if copy=False\n        raise NotImplementedError(\n            f\"{type(self).__name__} does not implement interpolate\"\n        )\n\n    def _pad_or_backfill(\n        self,\n        *,\n        method: FillnaOptions,\n        limit: int | None = None,\n        limit_area: Literal[\"inside\", \"outside\"] | None = None,\n        copy: bool = True,\n    ) -> Self:\n        \"\"\"\n        Pad or backfill values, used by Series/DataFrame ffill and bfill.\n\n        Parameters\n        ----------\n        method : {'backfill', 'bfill', 'pad', 'ffill'}\n            Method to use for filling holes in reindexed Series:\n\n            * pad / ffill: propagate last valid observation forward to next valid.\n            * backfill / bfill: use NEXT valid observation to fill gap.\n\n        limit : int, default None\n            This is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled.\n\n        copy : bool, default True\n            Whether to make a copy of the data before filling. If False, then\n            the original should be modified and no new memory should be allocated.\n            For ExtensionArray subclasses that cannot do this, it is at the\n            author's discretion whether to ignore \"copy=False\" or to raise.\n            The base class implementation ignores the keyword if any NAs are\n            present.\n\n        Returns\n        -------\n        Same type as self\n\n        Examples\n        --------\n        >>> arr = pd.array([np.nan, np.nan, 2, 3, np.nan, np.nan])\n        >>> arr._pad_or_backfill(method=\"backfill\", limit=1)\n        <IntegerArray>\n        [<NA>, 2, 2, 3, <NA>, <NA>]\n        Length: 6, dtype: Int64\n        \"\"\"\n\n        # If a 3rd-party EA has implemented this functionality in fillna,\n        #  we warn that they need to implement _pad_or_backfill instead.\n        if (\n            type(self).fillna is not ExtensionArray.fillna\n            and type(self)._pad_or_backfill is ExtensionArray._pad_or_backfill\n        ):\n            # Check for _pad_or_backfill here allows us to call\n            #  super()._pad_or_backfill without getting this warning\n            warnings.warn(\n                \"ExtensionArray.fillna 'method' keyword is deprecated. \"\n                \"In a future version. arr._pad_or_backfill will be called \"\n                \"instead. 3rd-party ExtensionArray authors need to implement \"\n                \"_pad_or_backfill.\",\n                DeprecationWarning,\n                stacklevel=find_stack_level(),\n            )\n            if limit_area is not None:\n                raise NotImplementedError(\n                    f\"{type(self).__name__} does not implement limit_area \"\n                    \"(added in pandas 2.2). 3rd-party ExtnsionArray authors \"\n                    \"need to add this argument to _pad_or_backfill.\"\n                )\n            return self.fillna(method=method, limit=limit)\n\n        mask = self.isna()\n\n        if mask.any():\n            # NB: the base class does not respect the \"copy\" keyword\n            meth = missing.clean_fill_method(method)\n\n            npmask = np.asarray(mask)\n            if limit_area is not None and not npmask.all():\n                _fill_limit_area_1d(npmask, limit_area)\n            if meth == \"pad\":\n                indexer = libalgos.get_fill_indexer(npmask, limit=limit)\n                return self.take(indexer, allow_fill=True)\n            else:\n                # i.e. meth == \"backfill\"\n                indexer = libalgos.get_fill_indexer(npmask[::-1], limit=limit)[::-1]\n                return self[::-1].take(indexer, allow_fill=True)\n\n        else:\n            if not copy:\n                return self\n            new_values = self.copy()\n        return new_values\n\n    def fillna(\n        self,\n        value: object | ArrayLike | None = None,\n        method: FillnaOptions | None = None,\n        limit: int | None = None,\n        copy: bool = True,\n    ) -> Self:\n        \"\"\"\n        Fill NA/NaN values using the specified method.\n\n        Parameters\n        ----------\n        value : scalar, array-like\n            If a scalar value is passed it is used to fill all missing values.\n            Alternatively, an array-like \"value\" can be given. It's expected\n            that the array-like have the same length as 'self'.\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n            Method to use for filling holes in reindexed Series:\n\n            * pad / ffill: propagate last valid observation forward to next valid.\n            * backfill / bfill: use NEXT valid observation to fill gap.\n\n            .. deprecated:: 2.1.0\n\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled.\n\n            .. deprecated:: 2.1.0\n\n        copy : bool, default True\n            Whether to make a copy of the data before filling. If False, then\n            the original should be modified and no new memory should be allocated.\n            For ExtensionArray subclasses that cannot do this, it is at the\n            author's discretion whether to ignore \"copy=False\" or to raise.\n            The base class implementation ignores the keyword in pad/backfill\n            cases.\n\n        Returns\n        -------\n        ExtensionArray\n            With NA/NaN filled.\n\n        Examples\n        --------\n        >>> arr = pd.array([np.nan, np.nan, 2, 3, np.nan, np.nan])\n        >>> arr.fillna(0)\n        <IntegerArray>\n        [0, 0, 2, 3, 0, 0]\n        Length: 6, dtype: Int64\n        \"\"\"\n        if method is not None:\n            warnings.warn(\n                f\"The 'method' keyword in {type(self).__name__}.fillna is \"\n                \"deprecated and will be removed in a future version.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        value, method = validate_fillna_kwargs(value, method)\n\n        mask = self.isna()\n        # error: Argument 2 to \"check_value_size\" has incompatible type\n        # \"ExtensionArray\"; expected \"ndarray\"\n        value = missing.check_value_size(\n            value, mask, len(self)  # type: ignore[arg-type]\n        )\n\n        if mask.any():\n            if method is not None:\n                meth = missing.clean_fill_method(method)\n\n                npmask = np.asarray(mask)\n                if meth == \"pad\":\n                    indexer = libalgos.get_fill_indexer(npmask, limit=limit)\n                    return self.take(indexer, allow_fill=True)\n                else:\n                    # i.e. meth == \"backfill\"\n                    indexer = libalgos.get_fill_indexer(npmask[::-1], limit=limit)[::-1]\n                    return self[::-1].take(indexer, allow_fill=True)\n            else:\n                # fill with value\n                if not copy:\n                    new_values = self[:]\n                else:\n                    new_values = self.copy()\n                new_values[mask] = value\n        else:\n            if not copy:\n                new_values = self[:]\n            else:\n                new_values = self.copy()\n        return new_values\n\n    def dropna(self) -> Self:\n        \"\"\"\n        Return ExtensionArray without NA values.\n\n        Returns\n        -------\n\n        Examples\n        --------\n        >>> pd.array([1, 2, np.nan]).dropna()\n        <IntegerArray>\n        [1, 2]\n        Length: 2, dtype: Int64\n        \"\"\"\n        # error: Unsupported operand type for ~ (\"ExtensionArray\")\n        return self[~self.isna()]  # type: ignore[operator]\n\n    def duplicated(\n        self, keep: Literal[\"first\", \"last\", False] = \"first\"\n    ) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Return boolean ndarray denoting duplicate values.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n            - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n            - False : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        ndarray[bool]\n\n        Examples\n        --------\n        >>> pd.array([1, 1, 2, 3, 3], dtype=\"Int64\").duplicated()\n        array([False,  True, False, False,  True])\n        \"\"\"\n        mask = self.isna().astype(np.bool_, copy=False)\n        return duplicated(values=self, keep=keep, mask=mask)\n\n    def shift(self, periods: int = 1, fill_value: object = None) -> ExtensionArray:\n        \"\"\"\n        Shift values by desired number.\n\n        Newly introduced missing values are filled with\n        ``self.dtype.na_value``.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            The number of periods to shift. Negative values are allowed\n            for shifting backwards.\n\n        fill_value : object, optional\n            The scalar value to use for newly introduced missing values.\n            The default is ``self.dtype.na_value``.\n\n        Returns\n        -------\n        ExtensionArray\n            Shifted.\n\n        Notes\n        -----\n        If ``self`` is empty or ``periods`` is 0, a copy of ``self`` is\n        returned.\n\n        If ``periods > len(self)``, then an array of size\n        len(self) is returned, with all values filled with\n        ``self.dtype.na_value``.\n\n        For 2-dimensional ExtensionArrays, we are always shifting along axis=0.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.shift(2)\n        <IntegerArray>\n        [<NA>, <NA>, 1]\n        Length: 3, dtype: Int64\n        \"\"\"\n        # Note: this implementation assumes that `self.dtype.na_value` can be\n        # stored in an instance of your ExtensionArray with `self.dtype`.\n        if not len(self) or periods == 0:\n            return self.copy()\n\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n\n        empty = self._from_sequence(\n            [fill_value] * min(abs(periods), len(self)), dtype=self.dtype\n        )\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods) :]\n            b = empty\n        return self._concat_same_type([a, b])\n\n    def unique(self) -> Self:\n        \"\"\"\n        Compute the ExtensionArray of unique values.\n\n        Returns\n        -------\n        pandas.api.extensions.ExtensionArray\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3, 1, 2, 3])\n        >>> arr.unique()\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        uniques = unique(self.astype(object))\n        return self._from_sequence(uniques, dtype=self.dtype)\n\n    def searchsorted(\n        self,\n        value: NumpyValueArrayLike | ExtensionArray,\n        side: Literal[\"left\", \"right\"] = \"left\",\n        sorter: NumpySorter | None = None,\n    ) -> npt.NDArray[np.intp] | np.intp:\n        \"\"\"\n        Find indices where elements should be inserted to maintain order.\n\n        Find the indices into a sorted array `self` (a) such that, if the\n        corresponding elements in `value` were inserted before the indices,\n        the order of `self` would be preserved.\n\n        Assuming that `self` is sorted:\n\n        ======  ================================\n        `side`  returned index `i` satisfies\n        ======  ================================\n        left    ``self[i-1] < value <= self[i]``\n        right   ``self[i-1] <= value < self[i]``\n        ======  ================================\n\n        Parameters\n        ----------\n        value : array-like, list or scalar\n            Value(s) to insert into `self`.\n        side : {'left', 'right'}, optional\n            If 'left', the index of the first suitable location found is given.\n            If 'right', return the last such index.  If there is no suitable\n            index, return either 0 or N (where N is the length of `self`).\n        sorter : 1-D array-like, optional\n            Optional array of integer indices that sort array a into ascending\n            order. They are typically the result of argsort.\n\n        Returns\n        -------\n        array of ints or int\n            If value is array-like, array of insertion points.\n            If value is scalar, a single integer.\n\n        See Also\n        --------\n        numpy.searchsorted : Similar method from NumPy.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3, 5])\n        >>> arr.searchsorted([4])\n        array([3])\n        \"\"\"\n        # Note: the base tests provided by pandas only test the basics.\n        # We do not test\n        # 1. Values outside the range of the `data_for_sorting` fixture\n        # 2. Values between the values in the `data_for_sorting` fixture\n        # 3. Missing values.\n        arr = self.astype(object)\n        if isinstance(value, ExtensionArray):\n            value = value.astype(object)\n        return arr.searchsorted(value, side=side, sorter=sorter)\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Return if another array is equivalent to this array.\n\n        Equivalent means that both arrays have the same shape and dtype, and\n        all values compare equal. Missing values in the same location are\n        considered equal (in contrast with normal equality).\n\n        Parameters\n        ----------\n        other : ExtensionArray\n            Array to compare to this Array.\n\n        Returns\n        -------\n        boolean\n            Whether the arrays are equivalent.\n\n        Examples\n        --------\n        >>> arr1 = pd.array([1, 2, np.nan])\n        >>> arr2 = pd.array([1, 2, np.nan])\n        >>> arr1.equals(arr2)\n        True\n        \"\"\"\n        if type(self) != type(other):\n            return False\n        other = cast(ExtensionArray, other)\n        if self.dtype != other.dtype:\n            return False\n        elif len(self) != len(other):\n            return False\n        else:\n            equal_values = self == other\n            if isinstance(equal_values, ExtensionArray):\n                # boolean array with NA -> fill with False\n                equal_values = equal_values.fillna(False)\n            # error: Unsupported left operand type for & (\"ExtensionArray\")\n            equal_na = self.isna() & other.isna()  # type: ignore[operator]\n            return bool((equal_values | equal_na).all())\n\n    def isin(self, values: ArrayLike) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Pointwise comparison for set containment in the given values.\n\n        Roughly equivalent to `np.array([x in values for x in self])`\n\n        Parameters\n        ----------\n        values : np.ndarray or ExtensionArray\n\n        Returns\n        -------\n        np.ndarray[bool]\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.isin([1])\n        <BooleanArray>\n        [True, False, False]\n        Length: 3, dtype: boolean\n        \"\"\"\n        return isin(np.asarray(self), values)\n\n    def _values_for_factorize(self) -> tuple[np.ndarray, Any]:\n        \"\"\"\n        Return an array and missing value suitable for factorization.\n\n        Returns\n        -------\n        values : ndarray\n            An array suitable for factorization. This should maintain order\n            and be a supported dtype (Float64, Int64, UInt64, String, Object).\n            By default, the extension array is cast to object dtype.\n        na_value : object\n            The value in `values` to consider missing. This will be treated\n            as NA in the factorization routines, so it will be coded as\n            `-1` and not included in `uniques`. By default,\n            ``np.nan`` is used.\n\n        Notes\n        -----\n        The values returned by this method are also used in\n        :func:`pandas.util.hash_pandas_object`. If needed, this can be\n        overridden in the ``self._hash_pandas_object()`` method.\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3])._values_for_factorize()\n        (array([1, 2, 3], dtype=object), nan)\n        \"\"\"\n        return self.astype(object), np.nan\n\n    def factorize(\n        self,\n        use_na_sentinel: bool = True,\n    ) -> tuple[np.ndarray, ExtensionArray]:\n        \"\"\"\n        Encode the extension array as an enumerated type.\n\n        Parameters\n        ----------\n        use_na_sentinel : bool, default True\n            If True, the sentinel -1 will be used for NaN values. If False,\n            NaN values will be encoded as non-negative integers and will not drop the\n            NaN from the uniques of the values.\n\n            .. versionadded:: 1.5.0\n\n        Returns\n        -------\n        codes : ndarray\n            An integer NumPy array that's an indexer into the original\n            ExtensionArray.\n        uniques : ExtensionArray\n            An ExtensionArray containing the unique values of `self`.\n\n            .. note::\n\n               uniques will *not* contain an entry for the NA value of\n               the ExtensionArray if there are any missing values present\n               in `self`.\n\n        See Also\n        --------\n        factorize : Top-level factorize method that dispatches here.\n\n        Notes\n        -----\n        :meth:`pandas.factorize` offers a `sort` keyword as well.\n\n        Examples\n        --------\n        >>> idx1 = pd.PeriodIndex([\"2014-01\", \"2014-01\", \"2014-02\", \"2014-02\",\n        ...                       \"2014-03\", \"2014-03\"], freq=\"M\")\n        >>> arr, idx = idx1.factorize()\n        >>> arr\n        array([0, 0, 1, 1, 2, 2])\n        >>> idx\n        PeriodIndex(['2014-01', '2014-02', '2014-03'], dtype='period[M]')\n        \"\"\"\n        # Implementer note: There are two ways to override the behavior of\n        # pandas.factorize\n        # 1. _values_for_factorize and _from_factorize.\n        #    Specify the values passed to pandas' internal factorization\n        #    routines, and how to convert from those values back to the\n        #    original ExtensionArray.\n        # 2. ExtensionArray.factorize.\n        #    Complete control over factorization.\n        arr, na_value = self._values_for_factorize()\n\n        codes, uniques = factorize_array(\n            arr, use_na_sentinel=use_na_sentinel, na_value=na_value\n        )\n\n        uniques_ea = self._from_factorized(uniques, self)\n        return codes, uniques_ea\n\n    _extension_array_shared_docs[\n        \"repeat\"\n    ] = \"\"\"\n        Repeat elements of a %(klass)s.\n\n        Returns a new %(klass)s where each element of the current %(klass)s\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            %(klass)s.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        %(klass)s\n            Newly created %(klass)s with repeated elements.\n\n        See Also\n        --------\n        Series.repeat : Equivalent function for Series.\n        Index.repeat : Equivalent function for Index.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n        ExtensionArray.take : Take arbitrary positions.\n\n        Examples\n        --------\n        >>> cat = pd.Categorical(['a', 'b', 'c'])\n        >>> cat\n        ['a', 'b', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> cat.repeat(2)\n        ['a', 'a', 'b', 'b', 'c', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> cat.repeat([1, 2, 3])\n        ['a', 'b', 'b', 'c', 'c', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        \"\"\"\n\n    @Substitution(klass=\"ExtensionArray\")\n    @Appender(_extension_array_shared_docs[\"repeat\"])\n    def repeat(self, repeats: int | Sequence[int], axis: AxisInt | None = None) -> Self:\n        nv.validate_repeat((), {\"axis\": axis})\n        ind = np.arange(len(self)).repeat(repeats)\n        return self.take(ind)\n\n    # ------------------------------------------------------------------------\n    # Indexing methods\n    # ------------------------------------------------------------------------\n\n    def take(\n        self,\n        indices: TakeIndexer,\n        *,\n        allow_fill: bool = False,\n        fill_value: Any = None,\n    ) -> Self:\n        \"\"\"\n        Take elements from an array.\n\n        Parameters\n        ----------\n        indices : sequence of int or one-dimensional np.ndarray of int\n            Indices to be taken.\n        allow_fill : bool, default False\n            How to handle negative values in `indices`.\n\n            * False: negative values in `indices` indicate positional indices\n              from the right (the default). This is similar to\n              :func:`numpy.take`.\n\n            * True: negative values in `indices` indicate\n              missing values. These values are set to `fill_value`. Any other\n              other negative values raise a ``ValueError``.\n\n        fill_value : any, optional\n            Fill value to use for NA-indices when `allow_fill` is True.\n            This may be ``None``, in which case the default NA value for\n            the type, ``self.dtype.na_value``, is used.\n\n            For many ExtensionArrays, there will be two representations of\n            `fill_value`: a user-facing \"boxed\" scalar, and a low-level\n            physical NA value. `fill_value` should be the user-facing version,\n            and the implementation should handle translating that to the\n            physical version for processing the take if necessary.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Raises\n        ------\n        IndexError\n            When the indices are out of bounds for the array.\n        ValueError\n            When `indices` contains negative values other than ``-1``\n            and `allow_fill` is True.\n\n        See Also\n        --------\n        numpy.take : Take elements from an array along an axis.\n        api.extensions.take : Take elements from an array.\n\n        Notes\n        -----\n        ExtensionArray.take is called by ``Series.__getitem__``, ``.loc``,\n        ``iloc``, when `indices` is a sequence of values. Additionally,\n        it's called by :meth:`Series.reindex`, or any other method\n        that causes realignment, with a `fill_value`.\n\n        Examples\n        --------\n        Here's an example implementation, which relies on casting the\n        extension array to object dtype. This uses the helper method\n        :func:`pandas.api.extensions.take`.\n\n        .. code-block:: python\n\n           def take(self, indices, allow_fill=False, fill_value=None):\n               from pandas.core.algorithms import take\n\n               # If the ExtensionArray is backed by an ndarray, then\n               # just pass that here instead of coercing to object.\n               data = self.astype(object)\n\n               if allow_fill and fill_value is None:\n                   fill_value = self.dtype.na_value\n\n               # fill value should always be translated from the scalar\n               # type for the array, to the physical storage type for\n               # the data, before passing to take.\n\n               result = take(data, indices, fill_value=fill_value,\n                             allow_fill=allow_fill)\n               return self._from_sequence(result, dtype=self.dtype)\n        \"\"\"\n        # Implementer note: The `fill_value` parameter should be a user-facing\n        # value, an instance of self.dtype.type. When passed `fill_value=None`,\n        # the default of `self.dtype.na_value` should be used.\n        # This may differ from the physical storage type your ExtensionArray\n        # uses. In this case, your implementation is responsible for casting\n        # the user-facing type to the storage type, before using\n        # pandas.api.extensions.take\n        raise AbstractMethodError(self)\n\n    def copy(self) -> Self:\n        \"\"\"\n        Return a copy of the array.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr2 = arr.copy()\n        >>> arr[0] = 2\n        >>> arr2\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def view(self, dtype: Dtype | None = None) -> ArrayLike:\n        \"\"\"\n        Return a view on the array.\n\n        Parameters\n        ----------\n        dtype : str, np.dtype, or ExtensionDtype, optional\n            Default None.\n\n        Returns\n        -------\n        ExtensionArray or np.ndarray\n            A view on the :class:`ExtensionArray`'s data.\n\n        Examples\n        --------\n        This gives view on the underlying data of an ``ExtensionArray`` and is not a\n        copy. Modifications on either the view or the original ``ExtensionArray``\n        will be reflectd on the underlying data:\n\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr2 = arr.view()\n        >>> arr[0] = 2\n        >>> arr2\n        <IntegerArray>\n        [2, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        # NB:\n        # - This must return a *new* object referencing the same data, not self.\n        # - The only case that *must* be implemented is with dtype=None,\n        #   giving a view with the same dtype as self.\n        if dtype is not None:\n            raise NotImplementedError(dtype)\n        return self[:]\n\n    # ------------------------------------------------------------------------\n    # Printing\n    # ------------------------------------------------------------------------\n\n    def __repr__(self) -> str:\n        if self.ndim > 1:\n            return self._repr_2d()\n\n        from pandas.io.formats.printing import format_object_summary\n\n        # the short repr has no trailing newline, while the truncated\n        # repr does. So we include a newline in our template, and strip\n        # any trailing newlines from format_object_summary\n        data = format_object_summary(\n            self, self._formatter(), indent_for_name=False\n        ).rstrip(\", \\n\")\n        class_name = f\"<{type(self).__name__}>\\n\"\n        footer = self._get_repr_footer()\n        return f\"{class_name}{data}\\n{footer}\"\n\n    def _get_repr_footer(self) -> str:\n        # GH#24278\n        if self.ndim > 1:\n            return f\"Shape: {self.shape}, dtype: {self.dtype}\"\n        return f\"Length: {len(self)}, dtype: {self.dtype}\"\n\n    def _repr_2d(self) -> str:\n        from pandas.io.formats.printing import format_object_summary\n\n        # the short repr has no trailing newline, while the truncated\n        # repr does. So we include a newline in our template, and strip\n        # any trailing newlines from format_object_summary\n        lines = [\n            format_object_summary(x, self._formatter(), indent_for_name=False).rstrip(\n                \", \\n\"\n            )\n            for x in self\n        ]\n        data = \",\\n\".join(lines)\n        class_name = f\"<{type(self).__name__}>\"\n        footer = self._get_repr_footer()\n        return f\"{class_name}\\n[\\n{data}\\n]\\n{footer}\"\n\n    def _formatter(self, boxed: bool = False) -> Callable[[Any], str | None]:\n        \"\"\"\n        Formatting function for scalar values.\n\n        This is used in the default '__repr__'. The returned formatting\n        function receives instances of your scalar type.\n\n        Parameters\n        ----------\n        boxed : bool, default False\n            An indicated for whether or not your array is being printed\n            within a Series, DataFrame, or Index (True), or just by\n            itself (False). This may be useful if you want scalar values\n            to appear differently within a Series versus on its own (e.g.\n            quoted or not).\n\n        Returns\n        -------\n        Callable[[Any], str]\n            A callable that gets instances of the scalar type and\n            returns a string. By default, :func:`repr` is used\n            when ``boxed=False`` and :func:`str` is used when\n            ``boxed=True``.\n\n        Examples\n        --------\n        >>> class MyExtensionArray(pd.arrays.NumpyExtensionArray):\n        ...     def _formatter(self, boxed=False):\n        ...         return lambda x: '*' + str(x) + '*' if boxed else repr(x) + '*'\n        >>> MyExtensionArray(np.array([1, 2, 3, 4]))\n        <MyExtensionArray>\n        [1*, 2*, 3*, 4*]\n        Length: 4, dtype: int64\n        \"\"\"\n        if boxed:\n            return str\n        return repr\n\n    # ------------------------------------------------------------------------\n    # Reshaping\n    # ------------------------------------------------------------------------\n\n    def transpose(self, *axes: int) -> ExtensionArray:\n        \"\"\"\n        Return a transposed view on this array.\n\n        Because ExtensionArrays are always 1D, this is a no-op.  It is included\n        for compatibility with np.ndarray.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3]).transpose()\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        return self[:]\n\n    @property\n    def T(self) -> ExtensionArray:\n        return self.transpose()\n\n    def ravel(self, order: Literal[\"C\", \"F\", \"A\", \"K\"] | None = \"C\") -> ExtensionArray:\n        \"\"\"\n        Return a flattened view on this array.\n\n        Parameters\n        ----------\n        order : {None, 'C', 'F', 'A', 'K'}, default 'C'\n\n        Returns\n        -------\n        ExtensionArray\n\n        Notes\n        -----\n        - Because ExtensionArrays are 1D-only, this is a no-op.\n        - The \"order\" argument is ignored, is for compatibility with NumPy.\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3]).ravel()\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        return self\n\n    @classmethod\n    def _concat_same_type(cls, to_concat: Sequence[Self]) -> Self:\n        \"\"\"\n        Concatenate multiple array of this dtype.\n\n        Parameters\n        ----------\n        to_concat : sequence of this type\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> arr1 = pd.array([1, 2, 3])\n        >>> arr2 = pd.array([4, 5, 6])\n        >>> pd.arrays.IntegerArray._concat_same_type([arr1, arr2])\n        <IntegerArray>\n        [1, 2, 3, 4, 5, 6]\n        Length: 6, dtype: Int64\n        \"\"\"\n        # Implementer note: this method will only be called with a sequence of\n        # ExtensionArrays of this class and with the same dtype as self. This\n        # should allow \"easy\" concatenation (no upcasting needed), and result\n        # in a new ExtensionArray of the same dtype.\n        # Note: this strict behaviour is only guaranteed starting with pandas 1.1\n        raise AbstractMethodError(cls)\n\n    # The _can_hold_na attribute is set to True so that pandas internals\n    # will use the ExtensionDtype.na_value as the NA value in operations\n    # such as take(), reindex(), shift(), etc.  In addition, those results\n    # will then be of the ExtensionArray subclass rather than an array\n    # of objects\n    @cache_readonly\n    def _can_hold_na(self) -> bool:\n        return self.dtype._can_hold_na\n\n    def _accumulate(\n        self, name: str, *, skipna: bool = True, **kwargs\n    ) -> ExtensionArray:\n        \"\"\"\n        Return an ExtensionArray performing an accumulation operation.\n\n        The underlying data type might change.\n\n        Parameters\n        ----------\n        name : str\n            Name of the function, supported values are:\n            - cummin\n            - cummax\n            - cumsum\n            - cumprod\n        skipna : bool, default True\n            If True, skip NA values.\n        **kwargs\n            Additional keyword arguments passed to the accumulation function.\n            Currently, there is no supported kwarg.\n\n        Returns\n        -------\n        array\n\n        Raises\n        ------\n        NotImplementedError : subclass does not define accumulations\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr._accumulate(name='cumsum')\n        <IntegerArray>\n        [1, 3, 6]\n        Length: 3, dtype: Int64\n        \"\"\"\n        raise NotImplementedError(f\"cannot perform {name} with type {self.dtype}\")\n\n    def _reduce(\n        self, name: str, *, skipna: bool = True, keepdims: bool = False, **kwargs\n    ):\n        \"\"\"\n        Return a scalar result of performing the reduction operation.\n\n        Parameters\n        ----------\n        name : str\n            Name of the function, supported values are:\n            { any, all, min, max, sum, mean, median, prod,\n            std, var, sem, kurt, skew }.\n        skipna : bool, default True\n            If True, skip NaN values.\n        keepdims : bool, default False\n            If False, a scalar is returned.\n            If True, the result has dimension with size one along the reduced axis.\n\n            .. versionadded:: 2.1\n\n               This parameter is not required in the _reduce signature to keep backward\n               compatibility, but will become required in the future. If the parameter\n               is not found in the method signature, a FutureWarning will be emitted.\n        **kwargs\n            Additional keyword arguments passed to the reduction function.\n            Currently, `ddof` is the only supported kwarg.\n\n        Returns\n        -------\n        scalar\n\n        Raises\n        ------\n        TypeError : subclass does not define reductions\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3])._reduce(\"min\")\n        1\n        \"\"\"\n        meth = getattr(self, name, None)\n        if meth is None:\n            raise TypeError(\n                f\"'{type(self).__name__}' with dtype {self.dtype} \"\n                f\"does not support reduction '{name}'\"\n            )\n        result = meth(skipna=skipna, **kwargs)\n        if keepdims:\n            result = np.array([result])\n\n        return result\n\n    # https://github.com/python/typeshed/issues/2148#issuecomment-520783318\n    # Incompatible types in assignment (expression has type \"None\", base class\n    # \"object\" defined the type as \"Callable[[object], int]\")\n    __hash__: ClassVar[None]  # type: ignore[assignment]\n\n    # ------------------------------------------------------------------------\n    # Non-Optimized Default Methods; in the case of the private methods here,\n    #  these are not guaranteed to be stable across pandas versions.\n\n    def _values_for_json(self) -> np.ndarray:\n        \"\"\"\n        Specify how to render our entries in to_json.\n\n        Notes\n        -----\n        The dtype on the returned ndarray is not restricted, but for non-native\n        types that are not specifically handled in objToJSON.c, to_json is\n        liable to raise. In these cases, it may be safer to return an ndarray\n        of strings.\n        \"\"\"\n        return np.asarray(self)\n\n    def _hash_pandas_object(\n        self, *, encoding: str, hash_key: str, categorize: bool\n    ) -> npt.NDArray[np.uint64]:\n        \"\"\"\n        Hook for hash_pandas_object.\n\n        Default is to use the values returned by _values_for_factorize.\n\n        Parameters\n        ----------\n        encoding : str\n            Encoding for data & key when strings.\n        hash_key : str\n            Hash_key for string key to encode.\n        categorize : bool\n            Whether to first categorize object arrays before hashing. This is more\n            efficient when the array contains duplicate values.\n\n        Returns\n        -------\n        np.ndarray[uint64]\n\n        Examples\n        --------\n        >>> pd.array([1, 2])._hash_pandas_object(encoding='utf-8',\n        ...                                      hash_key=\"1000000000000000\",\n        ...                                      categorize=False\n        ...                                      )\n        array([ 6238072747940578789, 15839785061582574730], dtype=uint64)\n        \"\"\"\n        from pandas.core.util.hashing import hash_array\n\n        values, _ = self._values_for_factorize()\n        return hash_array(\n            values, encoding=encoding, hash_key=hash_key, categorize=categorize\n        )\n\n    def _explode(self) -> tuple[Self, npt.NDArray[np.uint64]]:\n        \"\"\"\n        Transform each element of list-like to a row.\n\n        For arrays that do not contain list-like elements the default\n        implementation of this method just returns a copy and an array\n        of ones (unchanged index).\n\n        Returns\n        -------\n        ExtensionArray\n            Array with the exploded values.\n        np.ndarray[uint64]\n            The original lengths of each list-like for determining the\n            resulting index.\n\n        See Also\n        --------\n        Series.explode : The method on the ``Series`` object that this\n            extension array method is meant to support.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> a = pd.array([[1, 2, 3], [4], [5, 6]],\n        ...              dtype=pd.ArrowDtype(pa.list_(pa.int64())))\n        >>> a._explode()\n        (<ArrowExtensionArray>\n        [1, 2, 3, 4, 5, 6]\n        Length: 6, dtype: int64[pyarrow], array([3, 1, 2], dtype=int32))\n        \"\"\"\n        values = self.copy()\n        counts = np.ones(shape=(len(self),), dtype=np.uint64)\n        return values, counts\n\n    def tolist(self) -> list:\n        \"\"\"\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a pandas scalar\n        (for Timestamp/Timedelta/Interval/Period)\n\n        Returns\n        -------\n        list\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.tolist()\n        [1, 2, 3]\n        \"\"\"\n        if self.ndim > 1:\n            return [x.tolist() for x in self]\n        return list(self)\n\n    def delete(self, loc: PositionalIndexer) -> Self:\n        indexer = np.delete(np.arange(len(self)), loc)\n        return self.take(indexer)\n\n    def insert(self, loc: int, item) -> Self:\n        \"\"\"\n        Insert an item at the given position.\n\n        Parameters\n        ----------\n        loc : int\n        item : scalar-like\n\n        Returns\n        -------\n        same type as self\n\n        Notes\n        -----\n        This method should be both type and dtype-preserving.  If the item\n        cannot be held in an array of this type/dtype, either ValueError or\n        TypeError should be raised.\n\n        The default implementation relies on _from_sequence to raise on invalid\n        items.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.insert(2, -1)\n        <IntegerArray>\n        [1, 2, -1, 3]\n        Length: 4, dtype: Int64\n        \"\"\"\n        loc = validate_insert_loc(loc, len(self))\n\n        item_arr = type(self)._from_sequence([item], dtype=self.dtype)\n\n        return type(self)._concat_same_type([self[:loc], item_arr, self[loc:]])\n\n    def _putmask(self, mask: npt.NDArray[np.bool_], value) -> None:\n        \"\"\"\n        Analogue to np.putmask(self, mask, value)\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool]\n        value : scalar or listlike\n            If listlike, must be arraylike with same length as self.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        Unlike np.putmask, we do not repeat listlike values with mismatched length.\n        'value' should either be a scalar or an arraylike with the same length\n        as self.\n        \"\"\"\n        if is_list_like(value):\n            val = value[mask]\n        else:\n            val = value\n\n        self[mask] = val\n\n    def _where(self, mask: npt.NDArray[np.bool_], value) -> Self:\n        \"\"\"\n        Analogue to np.where(mask, self, value)\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool]\n        value : scalar or listlike\n\n        Returns\n        -------\n        same type as self\n        \"\"\"\n        result = self.copy()\n\n        if is_list_like(value):\n            val = value[~mask]\n        else:\n            val = value\n\n        result[~mask] = val\n        return result\n\n    # TODO(3.0): this can be removed once GH#33302 deprecation is enforced\n    def _fill_mask_inplace(\n        self, method: str, limit: int | None, mask: npt.NDArray[np.bool_]\n    ) -> None:\n        \"\"\"\n        Replace values in locations specified by 'mask' using pad or backfill.\n\n        See also\n        --------\n        ExtensionArray.fillna\n        \"\"\"\n        func = missing.get_fill_func(method)\n        npvalues = self.astype(object)\n        # NB: if we don't copy mask here, it may be altered inplace, which\n        #  would mess up the `self[mask] = ...` below.\n        func(npvalues, limit=limit, mask=mask.copy())\n        new_values = self._from_sequence(npvalues, dtype=self.dtype)\n        self[mask] = new_values[mask]\n\n    def _rank(\n        self,\n        *,\n        axis: AxisInt = 0,\n        method: str = \"average\",\n        na_option: str = \"keep\",\n        ascending: bool = True,\n        pct: bool = False,\n    ):\n        \"\"\"\n        See Series.rank.__doc__.\n        \"\"\"\n        if axis != 0:\n            raise NotImplementedError\n\n        return rank(\n            self._values_for_argsort(),\n            axis=axis,\n            method=method,\n            na_option=na_option,\n            ascending=ascending,\n            pct=pct,\n        )\n\n    @classmethod\n    def _empty(cls, shape: Shape, dtype: ExtensionDtype):\n        \"\"\"\n        Create an ExtensionArray with the given shape and dtype.\n\n        See also\n        --------\n        ExtensionDtype.empty\n            ExtensionDtype.empty is the 'official' public version of this API.\n        \"\"\"\n        # Implementer note: while ExtensionDtype.empty is the public way to\n        # call this method, it is still required to implement this `_empty`\n        # method as well (it is called internally in pandas)\n        obj = cls._from_sequence([], dtype=dtype)\n\n        taker = np.broadcast_to(np.intp(-1), shape)\n        result = obj.take(taker, allow_fill=True)\n        if not isinstance(result, cls) or dtype != result.dtype:\n            raise NotImplementedError(\n                f\"Default 'empty' implementation is invalid for dtype='{dtype}'\"\n            )\n        return result\n\n    def _quantile(self, qs: npt.NDArray[np.float64], interpolation: str) -> Self:\n        \"\"\"\n        Compute the quantiles of self for each quantile in `qs`.\n\n        Parameters\n        ----------\n        qs : np.ndarray[float64]\n        interpolation: str\n\n        Returns\n        -------\n        same type as self\n        \"\"\"\n        mask = np.asarray(self.isna())\n        arr = np.asarray(self)\n        fill_value = np.nan\n\n        res_values = quantile_with_mask(arr, mask, fill_value, qs, interpolation)\n        return type(self)._from_sequence(res_values)\n\n    def _mode(self, dropna: bool = True) -> Self:\n        \"\"\"\n        Returns the mode(s) of the ExtensionArray.\n\n        Always returns `ExtensionArray` even if only one value.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NA values.\n\n        Returns\n        -------\n        same type as self\n            Sorted, if possible.\n        \"\"\"\n        # error: Incompatible return value type (got \"Union[ExtensionArray,\n        # ndarray[Any, Any]]\", expected \"Self\")\n        return mode(self, dropna=dropna)  # type: ignore[return-value]\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n        if any(\n            isinstance(other, (ABCSeries, ABCIndex, ABCDataFrame)) for other in inputs\n        ):\n            return NotImplemented\n\n        result = arraylike.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        if \"out\" in kwargs:\n            return arraylike.dispatch_ufunc_with_out(\n                self, ufunc, method, *inputs, **kwargs\n            )\n\n        if method == \"reduce\":\n            result = arraylike.dispatch_reduction_ufunc(\n                self, ufunc, method, *inputs, **kwargs\n            )\n            if result is not NotImplemented:\n                return result\n\n        return arraylike.default_array_ufunc(self, ufunc, method, *inputs, **kwargs)\n\n    def map(self, mapper, na_action=None):\n        \"\"\"\n        Map values using an input mapping or function.\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence. If 'ignore' is not supported, a\n            ``NotImplementedError`` should be raised.\n\n        Returns\n        -------\n        Union[ndarray, Index, ExtensionArray]\n            The output of the mapping function applied to the array.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n        return map_array(self, mapper, na_action=na_action)\n\n    # ------------------------------------------------------------------------\n    # GroupBy Methods\n\n    def _groupby_op(\n        self,\n        *,\n        how: str,\n        has_dropped_na: bool,\n        min_count: int,\n        ngroups: int,\n        ids: npt.NDArray[np.intp],\n        **kwargs,\n    ) -> ArrayLike:\n        \"\"\"\n        Dispatch GroupBy reduction or transformation operation.\n\n        This is an *experimental* API to allow ExtensionArray authors to implement\n        reductions and transformations. The API is subject to change.\n\n        Parameters\n        ----------\n        how : {'any', 'all', 'sum', 'prod', 'min', 'max', 'mean', 'median',\n               'median', 'var', 'std', 'sem', 'nth', 'last', 'ohlc',\n               'cumprod', 'cumsum', 'cummin', 'cummax', 'rank'}\n        has_dropped_na : bool\n        min_count : int\n        ngroups : int\n        ids : np.ndarray[np.intp]\n            ids[i] gives the integer label for the group that self[i] belongs to.\n        **kwargs : operation-specific\n            'any', 'all' -> ['skipna']\n            'var', 'std', 'sem' -> ['ddof']\n            'cumprod', 'cumsum', 'cummin', 'cummax' -> ['skipna']\n            'rank' -> ['ties_method', 'ascending', 'na_option', 'pct']\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n        \"\"\"\n        from pandas.core.arrays.string_ import StringDtype\n        from pandas.core.groupby.ops import WrappedCythonOp\n\n        kind = WrappedCythonOp.get_kind_from_how(how)\n        op = WrappedCythonOp(how=how, kind=kind, has_dropped_na=has_dropped_na)\n\n        # GH#43682\n        if isinstance(self.dtype, StringDtype):\n            # StringArray\n            if op.how not in [\"any\", \"all\"]:\n                # Fail early to avoid conversion to object\n                op._get_cython_function(op.kind, op.how, np.dtype(object), False)\n            npvalues = self.to_numpy(object, na_value=np.nan)\n        else:\n            raise NotImplementedError(\n                f\"function is not implemented for this dtype: {self.dtype}\"\n            )\n\n        res_values = op._cython_op_ndim_compat(\n            npvalues,\n            min_count=min_count,\n            ngroups=ngroups,\n            comp_ids=ids,\n            mask=None,\n            **kwargs,\n        )\n\n        if op.how in op.cast_blocklist:\n            # i.e. how in [\"rank\"], since other cast_blocklist methods don't go\n            #  through cython_operation\n            return res_values\n\n        if isinstance(self.dtype, StringDtype):\n            dtype = self.dtype\n            string_array_cls = dtype.construct_array_type()\n            return string_array_cls._from_sequence(res_values, dtype=dtype)\n\n        else:\n            raise NotImplementedError\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.base/ExtensionArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/boolean.py", "fn_id": "", "content": "class BooleanArray(BaseMaskedArray):\n    \"\"\"\n    Array of boolean (True/False) data with missing values.\n\n    This is a pandas Extension array for boolean data, under the hood\n    represented by 2 numpy arrays: a boolean array with the data and\n    a boolean array with the mask (True indicating missing).\n\n    BooleanArray implements Kleene logic (sometimes called three-value\n    logic) for logical operations. See :ref:`boolean.kleene` for more.\n\n    To construct an BooleanArray from generic array-like input, use\n    :func:`pandas.array` specifying ``dtype=\"boolean\"`` (see examples\n    below).\n\n    .. warning::\n\n       BooleanArray is considered experimental. The implementation and\n       parts of the API may change without warning.\n\n    Parameters\n    ----------\n    values : numpy.ndarray\n        A 1-d boolean-dtype array with the data.\n    mask : numpy.ndarray\n        A 1-d boolean-dtype array indicating missing values (True\n        indicates missing).\n    copy : bool, default False\n        Whether to copy the `values` and `mask` arrays.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Returns\n    -------\n    BooleanArray\n\n    Examples\n    --------\n    Create an BooleanArray with :func:`pandas.array`:\n\n    >>> pd.array([True, False, None], dtype=\"boolean\")\n    <BooleanArray>\n    [True, False, <NA>]\n    Length: 3, dtype: boolean\n    \"\"\"\n\n    # The value used to fill '_data' to avoid upcasting\n    _internal_fill_value = False\n    # Fill values used for any/all\n    # Incompatible types in assignment (expression has type \"bool\", base class\n    # \"BaseMaskedArray\" defined the type as \"<typing special form>\")\n    _truthy_value = True  # type: ignore[assignment]\n    _falsey_value = False  # type: ignore[assignment]\n    _TRUE_VALUES = {\"True\", \"TRUE\", \"true\", \"1\", \"1.0\"}\n    _FALSE_VALUES = {\"False\", \"FALSE\", \"false\", \"0\", \"0.0\"}\n\n    @classmethod\n    def _simple_new(cls, values: np.ndarray, mask: npt.NDArray[np.bool_]) -> Self:\n        result = super()._simple_new(values, mask)\n        result._dtype = BooleanDtype()\n        return result\n\n    def __init__(\n        self, values: np.ndarray, mask: np.ndarray, copy: bool = False\n    ) -> None:\n        if not (isinstance(values, np.ndarray) and values.dtype == np.bool_):\n            raise TypeError(\n                \"values should be boolean numpy array. Use \"\n                \"the 'pd.array' function instead\"\n            )\n        self._dtype = BooleanDtype()\n        super().__init__(values, mask, copy=copy)\n\n    @property\n    def dtype(self) -> BooleanDtype:\n        return self._dtype\n\n    @classmethod\n    def _from_sequence_of_strings(\n        cls,\n        strings: list[str],\n        *,\n        dtype: Dtype | None = None,\n        copy: bool = False,\n        true_values: list[str] | None = None,\n        false_values: list[str] | None = None,\n    ) -> BooleanArray:\n        true_values_union = cls._TRUE_VALUES.union(true_values or [])\n        false_values_union = cls._FALSE_VALUES.union(false_values or [])\n\n        def map_string(s) -> bool:\n            if s in true_values_union:\n                return True\n            elif s in false_values_union:\n                return False\n            else:\n                raise ValueError(f\"{s} cannot be cast to bool\")\n\n        scalars = np.array(strings, dtype=object)\n        mask = isna(scalars)\n        scalars[~mask] = list(map(map_string, scalars[~mask]))\n        return cls._from_sequence(scalars, dtype=dtype, copy=copy)\n\n    _HANDLED_TYPES = (np.ndarray, numbers.Number, bool, np.bool_)\n\n    @classmethod\n    def _coerce_to_array(\n        cls, value, *, dtype: DtypeObj, copy: bool = False\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if dtype:\n            assert dtype == \"boolean\"\n        return coerce_to_array(value, copy=copy)\n\n    def _logical_method(self, other, op):\n        assert op.__name__ in {\"or_\", \"ror_\", \"and_\", \"rand_\", \"xor\", \"rxor\"}\n        other_is_scalar = lib.is_scalar(other)\n        mask = None\n\n        if isinstance(other, BooleanArray):\n            other, mask = other._data, other._mask\n        elif is_list_like(other):\n            other = np.asarray(other, dtype=\"bool\")\n            if other.ndim > 1:\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\n            other, mask = coerce_to_array(other, copy=False)\n        elif isinstance(other, np.bool_):\n            other = other.item()\n\n        if other_is_scalar and other is not libmissing.NA and not lib.is_bool(other):\n            raise TypeError(\n                \"'other' should be pandas.NA or a bool. \"\n                f\"Got {type(other).__name__} instead.\"\n            )\n\n        if not other_is_scalar and len(self) != len(other):\n            raise ValueError(\"Lengths must match\")\n\n        if op.__name__ in {\"or_\", \"ror_\"}:\n            result, mask = ops.kleene_or(self._data, other, self._mask, mask)\n        elif op.__name__ in {\"and_\", \"rand_\"}:\n            result, mask = ops.kleene_and(self._data, other, self._mask, mask)\n        else:\n            # i.e. xor, rxor\n            result, mask = ops.kleene_xor(self._data, other, self._mask, mask)\n\n        # i.e. BooleanArray\n        return self._maybe_mask_result(result, mask)\n\n    def _accumulate(\n        self, name: str, *, skipna: bool = True, **kwargs\n    ) -> BaseMaskedArray:\n        data = self._data\n        mask = self._mask\n        if name in (\"cummin\", \"cummax\"):\n            op = getattr(masked_accumulations, name)\n            data, mask = op(data, mask, skipna=skipna, **kwargs)\n            return self._simple_new(data, mask)\n        else:\n            from pandas.core.arrays import IntegerArray\n\n            return IntegerArray(data.astype(int), mask)._accumulate(\n                name, skipna=skipna, **kwargs\n            )\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.boolean/BooleanArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/categorical.py", "fn_id": "", "content": "@delegate_names(\n    delegate=Categorical, accessors=[\"categories\", \"ordered\"], typ=\"property\"\n)\n@delegate_names(\n    delegate=Categorical,\n    accessors=[\n        \"rename_categories\",\n        \"reorder_categories\",\n        \"add_categories\",\n        \"remove_categories\",\n        \"remove_unused_categories\",\n        \"set_categories\",\n        \"as_ordered\",\n        \"as_unordered\",\n    ],\n    typ=\"method\",\n)\nclass CategoricalAccessor(PandasDelegate, PandasObject, NoNewAttributesMixin):\n    \"\"\"\n    Accessor object for categorical properties of the Series values.\n\n    Parameters\n    ----------\n    data : Series or CategoricalIndex\n\n    Examples\n    --------\n    >>> s = pd.Series(list(\"abbccc\")).astype(\"category\")\n    >>> s\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n\n    >>> s.cat.categories\n    Index(['a', 'b', 'c'], dtype='object')\n\n    >>> s.cat.rename_categories(list(\"cba\"))\n    0    c\n    1    b\n    2    b\n    3    a\n    4    a\n    5    a\n    dtype: category\n    Categories (3, object): ['c', 'b', 'a']\n\n    >>> s.cat.reorder_categories(list(\"cba\"))\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['c', 'b', 'a']\n\n    >>> s.cat.add_categories([\"d\", \"e\"])\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n\n    >>> s.cat.remove_categories([\"a\", \"c\"])\n    0    NaN\n    1      b\n    2      b\n    3    NaN\n    4    NaN\n    5    NaN\n    dtype: category\n    Categories (1, object): ['b']\n\n    >>> s1 = s.cat.add_categories([\"d\", \"e\"])\n    >>> s1.cat.remove_unused_categories()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n\n    >>> s.cat.set_categories(list(\"abcde\"))\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n\n    >>> s.cat.as_ordered()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a' < 'b' < 'c']\n\n    >>> s.cat.as_unordered()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n    \"\"\"\n\n    def __init__(self, data) -> None:\n        self._validate(data)\n        self._parent = data.values\n        self._index = data.index\n        self._name = data.name\n        self._freeze()\n\n    @staticmethod\n    def _validate(data):\n        if not isinstance(data.dtype, CategoricalDtype):\n            raise AttributeError(\"Can only use .cat accessor with a 'category' dtype\")\n\n    def _delegate_property_get(self, name: str):\n        return getattr(self._parent, name)\n\n    # error: Signature of \"_delegate_property_set\" incompatible with supertype\n    # \"PandasDelegate\"\n    def _delegate_property_set(self, name: str, new_values):  # type: ignore[override]\n        return setattr(self._parent, name, new_values)\n\n    @property\n    def codes(self) -> Series:\n        \"\"\"\n        Return Series of codes as well as the index.\n\n        Examples\n        --------\n        >>> raw_cate = pd.Categorical([\"a\", \"b\", \"c\", \"a\"], categories=[\"a\", \"b\"])\n        >>> ser = pd.Series(raw_cate)\n        >>> ser.cat.codes\n        0   0\n        1   1\n        2  -1\n        3   0\n        dtype: int8\n        \"\"\"\n        from pandas import Series\n\n        return Series(self._parent.codes, index=self._index)\n\n    def _delegate_method(self, name: str, *args, **kwargs):\n        from pandas import Series\n\n        method = getattr(self._parent, name)\n        res = method(*args, **kwargs)\n        if res is not None:\n            return Series(res, index=self._index, name=self._name)\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.categorical/CategoricalAccessor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/datetimelike.py", "fn_id": "", "content": "class TimelikeOps(DatetimeLikeArrayMixin):\n    \"\"\"\n    Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.\n    \"\"\"\n\n    _default_dtype: np.dtype\n\n    def __init__(\n        self, values, dtype=None, freq=lib.no_default, copy: bool = False\n    ) -> None:\n        warnings.warn(\n            # GH#55623\n            f\"{type(self).__name__}.__init__ is deprecated and will be \"\n            \"removed in a future version. Use pd.array instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n        values = extract_array(values, extract_numpy=True)\n        if isinstance(values, IntegerArray):\n            values = values.to_numpy(\"int64\", na_value=iNaT)\n\n        inferred_freq = getattr(values, \"_freq\", None)\n        explicit_none = freq is None\n        freq = freq if freq is not lib.no_default else None\n\n        if isinstance(values, type(self)):\n            if explicit_none:\n                # don't inherit from values\n                pass\n            elif freq is None:\n                freq = values.freq\n            elif freq and values.freq:\n                freq = to_offset(freq)\n                freq = _validate_inferred_freq(freq, values.freq)\n\n            if dtype is not None and dtype != values.dtype:\n                # TODO: we only have tests for this for DTA, not TDA (2022-07-01)\n                raise TypeError(\n                    f\"dtype={dtype} does not match data dtype {values.dtype}\"\n                )\n\n            dtype = values.dtype\n            values = values._ndarray\n\n        elif dtype is None:\n            if isinstance(values, np.ndarray) and values.dtype.kind in \"Mm\":\n                dtype = values.dtype\n            else:\n                dtype = self._default_dtype\n                if isinstance(values, np.ndarray) and values.dtype == \"i8\":\n                    values = values.view(dtype)\n\n        if not isinstance(values, np.ndarray):\n            raise ValueError(\n                f\"Unexpected type '{type(values).__name__}'. 'values' must be a \"\n                f\"{type(self).__name__}, ndarray, or Series or Index \"\n                \"containing one of those.\"\n            )\n        if values.ndim not in [1, 2]:\n            raise ValueError(\"Only 1-dimensional input arrays are supported.\")\n\n        if values.dtype == \"i8\":\n            # for compat with datetime/timedelta/period shared methods,\n            #  we can sometimes get here with int64 values.  These represent\n            #  nanosecond UTC (or tz-naive) unix timestamps\n            if dtype is None:\n                dtype = self._default_dtype\n                values = values.view(self._default_dtype)\n            elif lib.is_np_dtype(dtype, \"mM\"):\n                values = values.view(dtype)\n            elif isinstance(dtype, DatetimeTZDtype):\n                kind = self._default_dtype.kind\n                new_dtype = f\"{kind}8[{dtype.unit}]\"\n                values = values.view(new_dtype)\n\n        dtype = self._validate_dtype(values, dtype)\n\n        if freq == \"infer\":\n            raise ValueError(\n                f\"Frequency inference not allowed in {type(self).__name__}.__init__. \"\n                \"Use 'pd.array()' instead.\"\n            )\n\n        if copy:\n            values = values.copy()\n        if freq:\n            freq = to_offset(freq)\n            if values.dtype.kind == \"m\" and not isinstance(freq, Tick):\n                raise TypeError(\"TimedeltaArray/Index freq must be a Tick\")\n\n        NDArrayBacked.__init__(self, values=values, dtype=dtype)\n        self._freq = freq\n\n        if inferred_freq is None and freq is not None:\n            type(self)._validate_frequency(self, freq)\n\n    @classmethod\n    def _validate_dtype(cls, values, dtype):\n        raise AbstractMethodError(cls)\n\n    @property\n    def freq(self):\n        \"\"\"\n        Return the frequency object if it is set, otherwise None.\n        \"\"\"\n        return self._freq\n\n    @freq.setter\n    def freq(self, value) -> None:\n        if value is not None:\n            value = to_offset(value)\n            self._validate_frequency(self, value)\n            if self.dtype.kind == \"m\" and not isinstance(value, Tick):\n                raise TypeError(\"TimedeltaArray/Index freq must be a Tick\")\n\n            if self.ndim > 1:\n                raise ValueError(\"Cannot set freq with ndim > 1\")\n\n        self._freq = value\n\n    @final\n    def _maybe_pin_freq(self, freq, validate_kwds: dict):\n        \"\"\"\n        Constructor helper to pin the appropriate `freq` attribute.  Assumes\n        that self._freq is currently set to any freq inferred in\n        _from_sequence_not_strict.\n        \"\"\"\n        if freq is None:\n            # user explicitly passed None -> override any inferred_freq\n            self._freq = None\n        elif freq == \"infer\":\n            # if self._freq is *not* None then we already inferred a freq\n            #  and there is nothing left to do\n            if self._freq is None:\n                # Set _freq directly to bypass duplicative _validate_frequency\n                # check.\n                self._freq = to_offset(self.inferred_freq)\n        elif freq is lib.no_default:\n            # user did not specify anything, keep inferred freq if the original\n            #  data had one, otherwise do nothing\n            pass\n        elif self._freq is None:\n            # We cannot inherit a freq from the data, so we need to validate\n            #  the user-passed freq\n            freq = to_offset(freq)\n            type(self)._validate_frequency(self, freq, **validate_kwds)\n            self._freq = freq\n        else:\n            # Otherwise we just need to check that the user-passed freq\n            #  doesn't conflict with the one we already have.\n            freq = to_offset(freq)\n            _validate_inferred_freq(freq, self._freq)\n\n    @final\n    @classmethod\n    def _validate_frequency(cls, index, freq: BaseOffset, **kwargs):\n        \"\"\"\n        Validate that a frequency is compatible with the values of a given\n        Datetime Array/Index or Timedelta Array/Index\n\n        Parameters\n        ----------\n        index : DatetimeIndex or TimedeltaIndex\n            The index on which to determine if the given frequency is valid\n        freq : DateOffset\n            The frequency to validate\n        \"\"\"\n        inferred = index.inferred_freq\n        if index.size == 0 or inferred == freq.freqstr:\n            return None\n\n        try:\n            on_freq = cls._generate_range(\n                start=index[0],\n                end=None,\n                periods=len(index),\n                freq=freq,\n                unit=index.unit,\n                **kwargs,\n            )\n            if not np.array_equal(index.asi8, on_freq.asi8):\n                raise ValueError\n        except ValueError as err:\n            if \"non-fixed\" in str(err):\n                # non-fixed frequencies are not meaningful for timedelta64;\n                #  we retain that error message\n                raise err\n            # GH#11587 the main way this is reached is if the `np.array_equal`\n            #  check above is False.  This can also be reached if index[0]\n            #  is `NaT`, in which case the call to `cls._generate_range` will\n            #  raise a ValueError, which we re-raise with a more targeted\n            #  message.\n            raise ValueError(\n                f\"Inferred frequency {inferred} from passed values \"\n                f\"does not conform to passed frequency {freq.freqstr}\"\n            ) from err\n\n    @classmethod\n    def _generate_range(\n        cls, start, end, periods: int | None, freq, *args, **kwargs\n    ) -> Self:\n        raise AbstractMethodError(cls)\n\n    # --------------------------------------------------------------\n\n    @cache_readonly\n    def _creso(self) -> int:\n        return get_unit_from_dtype(self._ndarray.dtype)\n\n    @cache_readonly\n    def unit(self) -> str:\n        # e.g. \"ns\", \"us\", \"ms\"\n        # error: Argument 1 to \"dtype_to_unit\" has incompatible type\n        # \"ExtensionDtype\"; expected \"Union[DatetimeTZDtype, dtype[Any]]\"\n        return dtype_to_unit(self.dtype)  # type: ignore[arg-type]\n\n    def as_unit(self, unit: str, round_ok: bool = True) -> Self:\n        if unit not in [\"s\", \"ms\", \"us\", \"ns\"]:\n            raise ValueError(\"Supported units are 's', 'ms', 'us', 'ns'\")\n\n        dtype = np.dtype(f\"{self.dtype.kind}8[{unit}]\")\n        new_values = astype_overflowsafe(self._ndarray, dtype, round_ok=round_ok)\n\n        if isinstance(self.dtype, np.dtype):\n            new_dtype = new_values.dtype\n        else:\n            tz = cast(\"DatetimeArray\", self).tz\n            new_dtype = DatetimeTZDtype(tz=tz, unit=unit)\n\n        # error: Unexpected keyword argument \"freq\" for \"_simple_new\" of\n        # \"NDArrayBacked\"  [call-arg]\n        return type(self)._simple_new(\n            new_values, dtype=new_dtype, freq=self.freq  # type: ignore[call-arg]\n        )\n\n    # TODO: annotate other as DatetimeArray | TimedeltaArray | Timestamp | Timedelta\n    #  with the return type matching input type.  TypeVar?\n    def _ensure_matching_resos(self, other):\n        if self._creso != other._creso:\n            # Just as with Timestamp/Timedelta, we cast to the higher resolution\n            if self._creso < other._creso:\n                self = self.as_unit(other.unit)\n            else:\n                other = other.as_unit(self.unit)\n        return self, other\n\n    # --------------------------------------------------------------\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n        if (\n            ufunc in [np.isnan, np.isinf, np.isfinite]\n            and len(inputs) == 1\n            and inputs[0] is self\n        ):\n            # numpy 1.18 changed isinf and isnan to not raise on dt64/td64\n            return getattr(ufunc, method)(self._ndarray, **kwargs)\n\n        return super().__array_ufunc__(ufunc, method, *inputs, **kwargs)\n\n    def _round(self, freq, mode, ambiguous, nonexistent):\n        # round the local times\n        if isinstance(self.dtype, DatetimeTZDtype):\n            # operate on naive timestamps, then convert back to aware\n            self = cast(\"DatetimeArray\", self)\n            naive = self.tz_localize(None)\n            result = naive._round(freq, mode, ambiguous, nonexistent)\n            return result.tz_localize(\n                self.tz, ambiguous=ambiguous, nonexistent=nonexistent\n            )\n\n        values = self.view(\"i8\")\n        values = cast(np.ndarray, values)\n        nanos = get_unit_for_round(freq, self._creso)\n        if nanos == 0:\n            # GH 52761\n            return self.copy()\n        result_i8 = round_nsint64(values, mode, nanos)\n        result = self._maybe_mask_results(result_i8, fill_value=iNaT)\n        result = result.view(self._ndarray.dtype)\n        return self._simple_new(result, dtype=self.dtype)\n\n    @Appender((_round_doc + _round_example).format(op=\"round\"))\n    def round(\n        self,\n        freq,\n        ambiguous: TimeAmbiguous = \"raise\",\n        nonexistent: TimeNonexistent = \"raise\",\n    ) -> Self:\n        return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)\n\n    @Appender((_round_doc + _floor_example).format(op=\"floor\"))\n    def floor(\n        self,\n        freq,\n        ambiguous: TimeAmbiguous = \"raise\",\n        nonexistent: TimeNonexistent = \"raise\",\n    ) -> Self:\n        return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)\n\n    @Appender((_round_doc + _ceil_example).format(op=\"ceil\"))\n    def ceil(\n        self,\n        freq,\n        ambiguous: TimeAmbiguous = \"raise\",\n        nonexistent: TimeNonexistent = \"raise\",\n    ) -> Self:\n        return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)\n\n    # --------------------------------------------------------------\n    # Reductions\n\n    def any(self, *, axis: AxisInt | None = None, skipna: bool = True) -> bool:\n        # GH#34479 the nanops call will issue a FutureWarning for non-td64 dtype\n        return nanops.nanany(self._ndarray, axis=axis, skipna=skipna, mask=self.isna())\n\n    def all(self, *, axis: AxisInt | None = None, skipna: bool = True) -> bool:\n        # GH#34479 the nanops call will issue a FutureWarning for non-td64 dtype\n\n        return nanops.nanall(self._ndarray, axis=axis, skipna=skipna, mask=self.isna())\n\n    # --------------------------------------------------------------\n    # Frequency Methods\n\n    def _maybe_clear_freq(self) -> None:\n        self._freq = None\n\n    def _with_freq(self, freq) -> Self:\n        \"\"\"\n        Helper to get a view on the same data, with a new freq.\n\n        Parameters\n        ----------\n        freq : DateOffset, None, or \"infer\"\n\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        # GH#29843\n        if freq is None:\n            # Always valid\n            pass\n        elif len(self) == 0 and isinstance(freq, BaseOffset):\n            # Always valid.  In the TimedeltaArray case, we require a Tick offset\n            if self.dtype.kind == \"m\" and not isinstance(freq, Tick):\n                raise TypeError(\"TimedeltaArray/Index freq must be a Tick\")\n        else:\n            # As an internal method, we can ensure this assertion always holds\n            assert freq == \"infer\"\n            freq = to_offset(self.inferred_freq)\n\n        arr = self.view()\n        arr._freq = freq\n        return arr\n\n    # --------------------------------------------------------------\n    # ExtensionArray Interface\n\n    def _values_for_json(self) -> np.ndarray:\n        # Small performance bump vs the base class which calls np.asarray(self)\n        if isinstance(self.dtype, np.dtype):\n            return self._ndarray\n        return super()._values_for_json()\n\n    def factorize(\n        self,\n        use_na_sentinel: bool = True,\n        sort: bool = False,\n    ):\n        if self.freq is not None:\n            # We must be unique, so can short-circuit (and retain freq)\n            codes = np.arange(len(self), dtype=np.intp)\n            uniques = self.copy()  # TODO: copy or view?\n            if sort and self.freq.n < 0:\n                codes = codes[::-1]\n                uniques = uniques[::-1]\n            return codes, uniques\n\n        if sort:\n            # algorithms.factorize only passes sort=True here when freq is\n            #  not None, so this should not be reached.\n            raise NotImplementedError(\n                f\"The 'sort' keyword in {type(self).__name__}.factorize is \"\n                \"ignored unless arr.freq is not None. To factorize with sort, \"\n                \"call pd.factorize(obj, sort=True) instead.\"\n            )\n        return super().factorize(use_na_sentinel=use_na_sentinel)\n\n    @classmethod\n    def _concat_same_type(\n        cls,\n        to_concat: Sequence[Self],\n        axis: AxisInt = 0,\n    ) -> Self:\n        new_obj = super()._concat_same_type(to_concat, axis)\n\n        obj = to_concat[0]\n\n        if axis == 0:\n            # GH 3232: If the concat result is evenly spaced, we can retain the\n            # original frequency\n            to_concat = [x for x in to_concat if len(x)]\n\n            if obj.freq is not None and all(x.freq == obj.freq for x in to_concat):\n                pairs = zip(to_concat[:-1], to_concat[1:])\n                if all(pair[0][-1] + obj.freq == pair[1][0] for pair in pairs):\n                    new_freq = obj.freq\n                    new_obj._freq = new_freq\n        return new_obj\n\n    def copy(self, order: str = \"C\") -> Self:\n        new_obj = super().copy(order=order)\n        new_obj._freq = self.freq\n        return new_obj\n\n    def interpolate(\n        self,\n        *,\n        method: InterpolateOptions,\n        axis: int,\n        index: Index,\n        limit,\n        limit_direction,\n        limit_area,\n        copy: bool,\n        **kwargs,\n    ) -> Self:\n        \"\"\"\n        See NDFrame.interpolate.__doc__.\n        \"\"\"\n        # NB: we return type(self) even if copy=False\n        if method != \"linear\":\n            raise NotImplementedError\n\n        if not copy:\n            out_data = self._ndarray\n        else:\n            out_data = self._ndarray.copy()\n\n        missing.interpolate_2d_inplace(\n            out_data,\n            method=method,\n            axis=axis,\n            index=index,\n            limit=limit,\n            limit_direction=limit_direction,\n            limit_area=limit_area,\n            **kwargs,\n        )\n        if not copy:\n            return self\n        return type(self)._simple_new(out_data, dtype=self.dtype)\n\n    # --------------------------------------------------------------\n    # Unsorted\n\n    @property\n    def _is_dates_only(self) -> bool:\n        \"\"\"\n        Check if we are round times at midnight (and no timezone), which will\n        be given a more compact __repr__ than other cases. For TimedeltaArray\n        we are checking for multiples of 24H.\n        \"\"\"\n        if not lib.is_np_dtype(self.dtype):\n            # i.e. we have a timezone\n            return False\n\n        values_int = self.asi8\n        consider_values = values_int != iNaT\n        reso = get_unit_from_dtype(self.dtype)\n        ppd = periods_per_day(reso)\n\n        # TODO: can we reuse is_date_array_normalized?  would need a skipna kwd\n        #  (first attempt at this was less performant than this implementation)\n        even_days = np.logical_and(consider_values, values_int % ppd != 0).sum() == 0\n        return even_days\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.datetimelike/TimelikeOps", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/masked.py", "fn_id": "", "content": "class BaseMaskedArray(OpsMixin, ExtensionArray):\n    \"\"\"\n    Base class for masked arrays (which use _data and _mask to store the data).\n\n    numpy based\n    \"\"\"\n\n    # The value used to fill '_data' to avoid upcasting\n    _internal_fill_value: Scalar\n    # our underlying data and mask are each ndarrays\n    _data: np.ndarray\n    _mask: npt.NDArray[np.bool_]\n\n    # Fill values used for any/all\n    _truthy_value = Scalar  # bool(_truthy_value) = True\n    _falsey_value = Scalar  # bool(_falsey_value) = False\n\n    @classmethod\n    def _simple_new(cls, values: np.ndarray, mask: npt.NDArray[np.bool_]) -> Self:\n        result = BaseMaskedArray.__new__(cls)\n        result._data = values\n        result._mask = mask\n        return result\n\n    def __init__(\n        self, values: np.ndarray, mask: npt.NDArray[np.bool_], copy: bool = False\n    ) -> None:\n        # values is supposed to already be validated in the subclass\n        if not (isinstance(mask, np.ndarray) and mask.dtype == np.bool_):\n            raise TypeError(\n                \"mask should be boolean numpy array. Use \"\n                \"the 'pd.array' function instead\"\n            )\n        if values.shape != mask.shape:\n            raise ValueError(\"values.shape must match mask.shape\")\n\n        if copy:\n            values = values.copy()\n            mask = mask.copy()\n\n        self._data = values\n        self._mask = mask\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype=None, copy: bool = False) -> Self:\n        values, mask = cls._coerce_to_array(scalars, dtype=dtype, copy=copy)\n        return cls(values, mask)\n\n    @classmethod\n    @doc(ExtensionArray._empty)\n    def _empty(cls, shape: Shape, dtype: ExtensionDtype):\n        values = np.empty(shape, dtype=dtype.type)\n        values.fill(cls._internal_fill_value)\n        mask = np.ones(shape, dtype=bool)\n        result = cls(values, mask)\n        if not isinstance(result, cls) or dtype != result.dtype:\n            raise NotImplementedError(\n                f\"Default 'empty' implementation is invalid for dtype='{dtype}'\"\n            )\n        return result\n\n    def _formatter(self, boxed: bool = False) -> Callable[[Any], str | None]:\n        # NEP 51: https://github.com/numpy/numpy/pull/22449\n        return str\n\n    @property\n    def dtype(self) -> BaseMaskedDtype:\n        raise AbstractMethodError(self)\n\n    @overload\n    def __getitem__(self, item: ScalarIndexer) -> Any:\n        ...\n\n    @overload\n    def __getitem__(self, item: SequenceIndexer) -> Self:\n        ...\n\n    def __getitem__(self, item: PositionalIndexer) -> Self | Any:\n        item = check_array_indexer(self, item)\n\n        newmask = self._mask[item]\n        if is_bool(newmask):\n            # This is a scalar indexing\n            if newmask:\n                return self.dtype.na_value\n            return self._data[item]\n\n        return self._simple_new(self._data[item], newmask)\n\n    def _pad_or_backfill(\n        self,\n        *,\n        method: FillnaOptions,\n        limit: int | None = None,\n        limit_area: Literal[\"inside\", \"outside\"] | None = None,\n        copy: bool = True,\n    ) -> Self:\n        mask = self._mask\n\n        if mask.any():\n            func = missing.get_fill_func(method, ndim=self.ndim)\n\n            npvalues = self._data.T\n            new_mask = mask.T\n            if copy:\n                npvalues = npvalues.copy()\n                new_mask = new_mask.copy()\n            elif limit_area is not None:\n                mask = mask.copy()\n            func(npvalues, limit=limit, mask=new_mask)\n\n            if limit_area is not None and not mask.all():\n                mask = mask.T\n                neg_mask = ~mask\n                first = neg_mask.argmax()\n                last = len(neg_mask) - neg_mask[::-1].argmax() - 1\n                if limit_area == \"inside\":\n                    new_mask[:first] |= mask[:first]\n                    new_mask[last + 1 :] |= mask[last + 1 :]\n                elif limit_area == \"outside\":\n                    new_mask[first + 1 : last] |= mask[first + 1 : last]\n\n            if copy:\n                return self._simple_new(npvalues.T, new_mask.T)\n            else:\n                return self\n        else:\n            if copy:\n                new_values = self.copy()\n            else:\n                new_values = self\n        return new_values\n\n    @doc(ExtensionArray.fillna)\n    def fillna(\n        self, value=None, method=None, limit: int | None = None, copy: bool = True\n    ) -> Self:\n        value, method = validate_fillna_kwargs(value, method)\n\n        mask = self._mask\n\n        value = missing.check_value_size(value, mask, len(self))\n\n        if mask.any():\n            if method is not None:\n                func = missing.get_fill_func(method, ndim=self.ndim)\n                npvalues = self._data.T\n                new_mask = mask.T\n                if copy:\n                    npvalues = npvalues.copy()\n                    new_mask = new_mask.copy()\n                func(npvalues, limit=limit, mask=new_mask)\n                return self._simple_new(npvalues.T, new_mask.T)\n            else:\n                # fill with value\n                if copy:\n                    new_values = self.copy()\n                else:\n                    new_values = self[:]\n                new_values[mask] = value\n        else:\n            if copy:\n                new_values = self.copy()\n            else:\n                new_values = self[:]\n        return new_values\n\n    @classmethod\n    def _coerce_to_array(\n        cls, values, *, dtype: DtypeObj, copy: bool = False\n    ) -> tuple[np.ndarray, np.ndarray]:\n        raise AbstractMethodError(cls)\n\n    def _validate_setitem_value(self, value):\n        \"\"\"\n        Check if we have a scalar that we can cast losslessly.\n\n        Raises\n        ------\n        TypeError\n        \"\"\"\n        kind = self.dtype.kind\n        # TODO: get this all from np_can_hold_element?\n        if kind == \"b\":\n            if lib.is_bool(value):\n                return value\n\n        elif kind == \"f\":\n            if lib.is_integer(value) or lib.is_float(value):\n                return value\n\n        else:\n            if lib.is_integer(value) or (lib.is_float(value) and value.is_integer()):\n                return value\n            # TODO: unsigned checks\n\n        # Note: without the \"str\" here, the f-string rendering raises in\n        #  py38 builds.\n        raise TypeError(f\"Invalid value '{str(value)}' for dtype {self.dtype}\")\n\n    def __setitem__(self, key, value) -> None:\n        key = check_array_indexer(self, key)\n\n        if is_scalar(value):\n            if is_valid_na_for_dtype(value, self.dtype):\n                self._mask[key] = True\n            else:\n                value = self._validate_setitem_value(value)\n                self._data[key] = value\n                self._mask[key] = False\n            return\n\n        value, mask = self._coerce_to_array(value, dtype=self.dtype)\n\n        self._data[key] = value\n        self._mask[key] = mask\n\n    def __contains__(self, key) -> bool:\n        if isna(key) and key is not self.dtype.na_value:\n            # GH#52840\n            if self._data.dtype.kind == \"f\" and lib.is_float(key):\n                return bool((np.isnan(self._data) & ~self._mask).any())\n\n        return bool(super().__contains__(key))\n\n    def __iter__(self) -> Iterator:\n        if self.ndim == 1:\n            if not self._hasna:\n                for val in self._data:\n                    yield val\n            else:\n                na_value = self.dtype.na_value\n                for isna_, val in zip(self._mask, self._data):\n                    if isna_:\n                        yield na_value\n                    else:\n                        yield val\n        else:\n            for i in range(len(self)):\n                yield self[i]\n\n    def __len__(self) -> int:\n        return len(self._data)\n\n    @property\n    def shape(self) -> Shape:\n        return self._data.shape\n\n    @property\n    def ndim(self) -> int:\n        return self._data.ndim\n\n    def swapaxes(self, axis1, axis2) -> Self:\n        data = self._data.swapaxes(axis1, axis2)\n        mask = self._mask.swapaxes(axis1, axis2)\n        return self._simple_new(data, mask)\n\n    def delete(self, loc, axis: AxisInt = 0) -> Self:\n        data = np.delete(self._data, loc, axis=axis)\n        mask = np.delete(self._mask, loc, axis=axis)\n        return self._simple_new(data, mask)\n\n    def reshape(self, *args, **kwargs) -> Self:\n        data = self._data.reshape(*args, **kwargs)\n        mask = self._mask.reshape(*args, **kwargs)\n        return self._simple_new(data, mask)\n\n    def ravel(self, *args, **kwargs) -> Self:\n        # TODO: need to make sure we have the same order for data/mask\n        data = self._data.ravel(*args, **kwargs)\n        mask = self._mask.ravel(*args, **kwargs)\n        return type(self)(data, mask)\n\n    @property\n    def T(self) -> Self:\n        return self._simple_new(self._data.T, self._mask.T)\n\n    def round(self, decimals: int = 0, *args, **kwargs):\n        \"\"\"\n        Round each value in the array a to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int, default 0\n            Number of decimal places to round to. If decimals is negative,\n            it specifies the number of positions to the left of the decimal point.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        NumericArray\n            Rounded values of the NumericArray.\n\n        See Also\n        --------\n        numpy.around : Round values of an np.array.\n        DataFrame.round : Round values of a DataFrame.\n        Series.round : Round values of a Series.\n        \"\"\"\n        if self.dtype.kind == \"b\":\n            return self\n        nv.validate_round(args, kwargs)\n        values = np.round(self._data, decimals=decimals, **kwargs)\n\n        # Usually we'll get same type as self, but ndarray[bool] casts to float\n        return self._maybe_mask_result(values, self._mask.copy())\n\n    # ------------------------------------------------------------------\n    # Unary Methods\n\n    def __invert__(self) -> Self:\n        return self._simple_new(~self._data, self._mask.copy())\n\n    def __neg__(self) -> Self:\n        return self._simple_new(-self._data, self._mask.copy())\n\n    def __pos__(self) -> Self:\n        return self.copy()\n\n    def __abs__(self) -> Self:\n        return self._simple_new(abs(self._data), self._mask.copy())\n\n    # ------------------------------------------------------------------\n\n    def _values_for_json(self) -> np.ndarray:\n        return np.asarray(self, dtype=object)\n\n    def to_numpy(\n        self,\n        dtype: npt.DTypeLike | None = None,\n        copy: bool = False,\n        na_value: object = lib.no_default,\n    ) -> np.ndarray:\n        \"\"\"\n        Convert to a NumPy Array.\n\n        By default converts to an object-dtype NumPy array. Specify the `dtype` and\n        `na_value` keywords to customize the conversion.\n\n        Parameters\n        ----------\n        dtype : dtype, default object\n            The numpy dtype to convert to.\n        copy : bool, default False\n            Whether to ensure that the returned value is a not a view on\n            the array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary. This is typically\n            only possible when no missing values are present and `dtype`\n            is the equivalent numpy dtype.\n        na_value : scalar, optional\n             Scalar missing value indicator to use in numpy array. Defaults\n             to the native missing value indicator of this array (pd.NA).\n\n        Returns\n        -------\n        numpy.ndarray\n\n        Examples\n        --------\n        An object-dtype is the default result\n\n        >>> a = pd.array([True, False, pd.NA], dtype=\"boolean\")\n        >>> a.to_numpy()\n        array([True, False, <NA>], dtype=object)\n\n        When no missing values are present, an equivalent dtype can be used.\n\n        >>> pd.array([True, False], dtype=\"boolean\").to_numpy(dtype=\"bool\")\n        array([ True, False])\n        >>> pd.array([1, 2], dtype=\"Int64\").to_numpy(\"int64\")\n        array([1, 2])\n\n        However, requesting such dtype will raise a ValueError if\n        missing values are present and the default missing value :attr:`NA`\n        is used.\n\n        >>> a = pd.array([True, False, pd.NA], dtype=\"boolean\")\n        >>> a\n        <BooleanArray>\n        [True, False, <NA>]\n        Length: 3, dtype: boolean\n\n        >>> a.to_numpy(dtype=\"bool\")\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot convert to bool numpy array in presence of missing values\n\n        Specify a valid `na_value` instead\n\n        >>> a.to_numpy(dtype=\"bool\", na_value=False)\n        array([ True, False, False])\n        \"\"\"\n        hasna = self._hasna\n        dtype, na_value = to_numpy_dtype_inference(self, dtype, na_value, hasna)\n        if dtype is None:\n            dtype = object\n\n        if hasna:\n            if (\n                dtype != object\n                and not is_string_dtype(dtype)\n                and na_value is libmissing.NA\n            ):\n                raise ValueError(\n                    f\"cannot convert to '{dtype}'-dtype NumPy array \"\n                    \"with missing values. Specify an appropriate 'na_value' \"\n                    \"for this dtype.\"\n                )\n            # don't pass copy to astype -> always need a copy since we are mutating\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n                data = self._data.astype(dtype)\n            data[self._mask] = na_value\n        else:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n                data = self._data.astype(dtype, copy=copy)\n        return data\n\n    @doc(ExtensionArray.tolist)\n    def tolist(self):\n        if self.ndim > 1:\n            return [x.tolist() for x in self]\n        dtype = None if self._hasna else self._data.dtype\n        return self.to_numpy(dtype=dtype, na_value=libmissing.NA).tolist()\n\n    @overload\n    def astype(self, dtype: npt.DTypeLike, copy: bool = ...) -> np.ndarray:\n        ...\n\n    @overload\n    def astype(self, dtype: ExtensionDtype, copy: bool = ...) -> ExtensionArray:\n        ...\n\n    @overload\n    def astype(self, dtype: AstypeArg, copy: bool = ...) -> ArrayLike:\n        ...\n\n    def astype(self, dtype: AstypeArg, copy: bool = True) -> ArrayLike:\n        dtype = pandas_dtype(dtype)\n\n        if dtype == self.dtype:\n            if copy:\n                return self.copy()\n            return self\n\n        # if we are astyping to another nullable masked dtype, we can fastpath\n        if isinstance(dtype, BaseMaskedDtype):\n            # TODO deal with NaNs for FloatingArray case\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n                # TODO: Is rounding what we want long term?\n                data = self._data.astype(dtype.numpy_dtype, copy=copy)\n            # mask is copied depending on whether the data was copied, and\n            # not directly depending on the `copy` keyword\n            mask = self._mask if data is self._data else self._mask.copy()\n            cls = dtype.construct_array_type()\n            return cls(data, mask, copy=False)\n\n        if isinstance(dtype, ExtensionDtype):\n            eacls = dtype.construct_array_type()\n            return eacls._from_sequence(self, dtype=dtype, copy=copy)\n\n        na_value: float | np.datetime64 | lib.NoDefault\n\n        # coerce\n        if dtype.kind == \"f\":\n            # In astype, we consider dtype=float to also mean na_value=np.nan\n            na_value = np.nan\n        elif dtype.kind == \"M\":\n            na_value = np.datetime64(\"NaT\")\n        else:\n            na_value = lib.no_default\n\n        # to_numpy will also raise, but we get somewhat nicer exception messages here\n        if dtype.kind in \"iu\" and self._hasna:\n            raise ValueError(\"cannot convert NA to integer\")\n        if dtype.kind == \"b\" and self._hasna:\n            # careful: astype_nansafe converts np.nan to True\n            raise ValueError(\"cannot convert float NaN to bool\")\n\n        data = self.to_numpy(dtype=dtype, na_value=na_value, copy=copy)\n        return data\n\n    __array_priority__ = 1000  # higher than ndarray so ops dispatch to us\n\n    def __array__(\n        self, dtype: NpDtype | None = None, copy: bool | None = None\n    ) -> np.ndarray:\n        \"\"\"\n        the array interface, return my values\n        We return an object array here to preserve our scalar values\n        \"\"\"\n        return self.to_numpy(dtype=dtype)\n\n    _HANDLED_TYPES: tuple[type, ...]\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n        # For MaskedArray inputs, we apply the ufunc to ._data\n        # and mask the result.\n\n        out = kwargs.get(\"out\", ())\n\n        for x in inputs + out:\n            if not isinstance(x, self._HANDLED_TYPES + (BaseMaskedArray,)):\n                return NotImplemented\n\n        # for binary ops, use our custom dunder methods\n        result = arraylike.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        if \"out\" in kwargs:\n            # e.g. test_ufunc_with_out\n            return arraylike.dispatch_ufunc_with_out(\n                self, ufunc, method, *inputs, **kwargs\n            )\n\n        if method == \"reduce\":\n            result = arraylike.dispatch_reduction_ufunc(\n                self, ufunc, method, *inputs, **kwargs\n            )\n            if result is not NotImplemented:\n                return result\n\n        mask = np.zeros(len(self), dtype=bool)\n        inputs2 = []\n        for x in inputs:\n            if isinstance(x, BaseMaskedArray):\n                mask |= x._mask\n                inputs2.append(x._data)\n            else:\n                inputs2.append(x)\n\n        def reconstruct(x: np.ndarray):\n            # we don't worry about scalar `x` here, since we\n            # raise for reduce up above.\n            from pandas.core.arrays import (\n                BooleanArray,\n                FloatingArray,\n                IntegerArray,\n            )\n\n            if x.dtype.kind == \"b\":\n                m = mask.copy()\n                return BooleanArray(x, m)\n            elif x.dtype.kind in \"iu\":\n                m = mask.copy()\n                return IntegerArray(x, m)\n            elif x.dtype.kind == \"f\":\n                m = mask.copy()\n                if x.dtype == np.float16:\n                    # reached in e.g. np.sqrt on BooleanArray\n                    # we don't support float16\n                    x = x.astype(np.float32)\n                return FloatingArray(x, m)\n            else:\n                x[mask] = np.nan\n            return x\n\n        result = getattr(ufunc, method)(*inputs2, **kwargs)\n        if ufunc.nout > 1:\n            # e.g. np.divmod\n            return tuple(reconstruct(x) for x in result)\n        elif method == \"reduce\":\n            # e.g. np.add.reduce; test_ufunc_reduce_raises\n            if self._mask.any():\n                return self._na_value\n            return result\n        else:\n            return reconstruct(result)\n\n    def __arrow_array__(self, type=None):\n        \"\"\"\n        Convert myself into a pyarrow Array.\n        \"\"\"\n        import pyarrow as pa\n\n        return pa.array(self._data, mask=self._mask, type=type)\n\n    @property\n    def _hasna(self) -> bool:\n        # Note: this is expensive right now! The hope is that we can\n        # make this faster by having an optional mask, but not have to change\n        # source code using it..\n\n        # error: Incompatible return value type (got \"bool_\", expected \"bool\")\n        return self._mask.any()  # type: ignore[return-value]\n\n    def _propagate_mask(\n        self, mask: npt.NDArray[np.bool_] | None, other\n    ) -> npt.NDArray[np.bool_]:\n        if mask is None:\n            mask = self._mask.copy()  # TODO: need test for BooleanArray needing a copy\n            if other is libmissing.NA:\n                # GH#45421 don't alter inplace\n                mask = mask | True\n            elif is_list_like(other) and len(other) == len(mask):\n                mask = mask | isna(other)\n        else:\n            mask = self._mask | mask\n        # Incompatible return value type (got \"Optional[ndarray[Any, dtype[bool_]]]\",\n        # expected \"ndarray[Any, dtype[bool_]]\")\n        return mask  # type: ignore[return-value]\n\n    def _arith_method(self, other, op):\n        op_name = op.__name__\n        omask = None\n\n        if (\n            not hasattr(other, \"dtype\")\n            and is_list_like(other)\n            and len(other) == len(self)\n        ):\n            # Try inferring masked dtype instead of casting to object\n            other = pd_array(other)\n            other = extract_array(other, extract_numpy=True)\n\n        if isinstance(other, BaseMaskedArray):\n            other, omask = other._data, other._mask\n\n        elif is_list_like(other):\n            if not isinstance(other, ExtensionArray):\n                other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\n\n        # We wrap the non-masked arithmetic logic used for numpy dtypes\n        #  in Series/Index arithmetic ops.\n        other = ops.maybe_prepare_scalar_for_op(other, (len(self),))\n        pd_op = ops.get_array_op(op)\n        other = ensure_wrapped_if_datetimelike(other)\n\n        if op_name in {\"pow\", \"rpow\"} and isinstance(other, np.bool_):\n            # Avoid DeprecationWarning: In future, it will be an error\n            #  for 'np.bool_' scalars to be interpreted as an index\n            #  e.g. test_array_scalar_like_equivalence\n            other = bool(other)\n\n        mask = self._propagate_mask(omask, other)\n\n        if other is libmissing.NA:\n            result = np.ones_like(self._data)\n            if self.dtype.kind == \"b\":\n                if op_name in {\n                    \"floordiv\",\n                    \"rfloordiv\",\n                    \"pow\",\n                    \"rpow\",\n                    \"truediv\",\n                    \"rtruediv\",\n                }:\n                    # GH#41165 Try to match non-masked Series behavior\n                    #  This is still imperfect GH#46043\n                    raise NotImplementedError(\n                        f\"operator '{op_name}' not implemented for bool dtypes\"\n                    )\n                if op_name in {\"mod\", \"rmod\"}:\n                    dtype = \"int8\"\n                else:\n                    dtype = \"bool\"\n                result = result.astype(dtype)\n            elif \"truediv\" in op_name and self.dtype.kind != \"f\":\n                # The actual data here doesn't matter since the mask\n                #  will be all-True, but since this is division, we want\n                #  to end up with floating dtype.\n                result = result.astype(np.float64)\n        else:\n            # Make sure we do this before the \"pow\" mask checks\n            #  to get an expected exception message on shape mismatch.\n            if self.dtype.kind in \"iu\" and op_name in [\"floordiv\", \"mod\"]:\n                # TODO(GH#30188) ATM we don't match the behavior of non-masked\n                #  types with respect to floordiv-by-zero\n                pd_op = op\n\n            with np.errstate(all=\"ignore\"):\n                result = pd_op(self._data, other)\n\n        if op_name == \"pow\":\n            # 1 ** x is 1.\n            mask = np.where((self._data == 1) & ~self._mask, False, mask)\n            # x ** 0 is 1.\n            if omask is not None:\n                mask = np.where((other == 0) & ~omask, False, mask)\n            elif other is not libmissing.NA:\n                mask = np.where(other == 0, False, mask)\n\n        elif op_name == \"rpow\":\n            # 1 ** x is 1.\n            if omask is not None:\n                mask = np.where((other == 1) & ~omask, False, mask)\n            elif other is not libmissing.NA:\n                mask = np.where(other == 1, False, mask)\n            # x ** 0 is 1.\n            mask = np.where((self._data == 0) & ~self._mask, False, mask)\n\n        return self._maybe_mask_result(result, mask)\n\n    _logical_method = _arith_method\n\n    def _cmp_method(self, other, op) -> BooleanArray:\n        from pandas.core.arrays import BooleanArray\n\n        mask = None\n\n        if isinstance(other, BaseMaskedArray):\n            other, mask = other._data, other._mask\n\n        elif is_list_like(other):\n            other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\n            if len(self) != len(other):\n                raise ValueError(\"Lengths must match to compare\")\n\n        if other is libmissing.NA:\n            # numpy does not handle pd.NA well as \"other\" scalar (it returns\n            # a scalar False instead of an array)\n            # This may be fixed by NA.__array_ufunc__. Revisit this check\n            # once that's implemented.\n            result = np.zeros(self._data.shape, dtype=\"bool\")\n            mask = np.ones(self._data.shape, dtype=\"bool\")\n        else:\n            with warnings.catch_warnings():\n                # numpy may show a FutureWarning or DeprecationWarning:\n                #     elementwise comparison failed; returning scalar instead,\n                #     but in the future will perform elementwise comparison\n                # before returning NotImplemented. We fall back to the correct\n                # behavior today, so that should be fine to ignore.\n                warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                warnings.filterwarnings(\"ignore\", \"elementwise\", DeprecationWarning)\n                method = getattr(self._data, f\"__{op.__name__}__\")\n                result = method(other)\n\n                if result is NotImplemented:\n                    result = invalid_comparison(self._data, other, op)\n\n        mask = self._propagate_mask(mask, other)\n        return BooleanArray(result, mask, copy=False)\n\n    def _maybe_mask_result(\n        self, result: np.ndarray | tuple[np.ndarray, np.ndarray], mask: np.ndarray\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        result : array-like or tuple[array-like]\n        mask : array-like bool\n        \"\"\"\n        if isinstance(result, tuple):\n            # i.e. divmod\n            div, mod = result\n            return (\n                self._maybe_mask_result(div, mask),\n                self._maybe_mask_result(mod, mask),\n            )\n\n        if result.dtype.kind == \"f\":\n            from pandas.core.arrays import FloatingArray\n\n            return FloatingArray(result, mask, copy=False)\n\n        elif result.dtype.kind == \"b\":\n            from pandas.core.arrays import BooleanArray\n\n            return BooleanArray(result, mask, copy=False)\n\n        elif lib.is_np_dtype(result.dtype, \"m\") and is_supported_dtype(result.dtype):\n            # e.g. test_numeric_arr_mul_tdscalar_numexpr_path\n            from pandas.core.arrays import TimedeltaArray\n\n            result[mask] = result.dtype.type(\"NaT\")\n\n            if not isinstance(result, TimedeltaArray):\n                return TimedeltaArray._simple_new(result, dtype=result.dtype)\n\n            return result\n\n        elif result.dtype.kind in \"iu\":\n            from pandas.core.arrays import IntegerArray\n\n            return IntegerArray(result, mask, copy=False)\n\n        else:\n            result[mask] = np.nan\n            return result\n\n    def isna(self) -> np.ndarray:\n        return self._mask.copy()\n\n    @property\n    def _na_value(self):\n        return self.dtype.na_value\n\n    @property\n    def nbytes(self) -> int:\n        return self._data.nbytes + self._mask.nbytes\n\n    @classmethod\n    def _concat_same_type(\n        cls,\n        to_concat: Sequence[Self],\n        axis: AxisInt = 0,\n    ) -> Self:\n        data = np.concatenate([x._data for x in to_concat], axis=axis)\n        mask = np.concatenate([x._mask for x in to_concat], axis=axis)\n        return cls(data, mask)\n\n    def _hash_pandas_object(\n        self, *, encoding: str, hash_key: str, categorize: bool\n    ) -> npt.NDArray[np.uint64]:\n        hashed_array = hash_array(\n            self._data, encoding=encoding, hash_key=hash_key, categorize=categorize\n        )\n        hashed_array[self.isna()] = hash(self.dtype.na_value)\n        return hashed_array\n\n    def take(\n        self,\n        indexer,\n        *,\n        allow_fill: bool = False,\n        fill_value: Scalar | None = None,\n        axis: AxisInt = 0,\n    ) -> Self:\n        # we always fill with 1 internally\n        # to avoid upcasting\n        data_fill_value = self._internal_fill_value if isna(fill_value) else fill_value\n        result = take(\n            self._data,\n            indexer,\n            fill_value=data_fill_value,\n            allow_fill=allow_fill,\n            axis=axis,\n        )\n\n        mask = take(\n            self._mask, indexer, fill_value=True, allow_fill=allow_fill, axis=axis\n        )\n\n        # if we are filling\n        # we only fill where the indexer is null\n        # not existing missing values\n        # TODO(jreback) what if we have a non-na float as a fill value?\n        if allow_fill and notna(fill_value):\n            fill_mask = np.asarray(indexer) == -1\n            result[fill_mask] = fill_value\n            mask = mask ^ fill_mask\n\n        return self._simple_new(result, mask)\n\n    # error: Return type \"BooleanArray\" of \"isin\" incompatible with return type\n    # \"ndarray\" in supertype \"ExtensionArray\"\n    def isin(self, values: ArrayLike) -> BooleanArray:  # type: ignore[override]\n        from pandas.core.arrays import BooleanArray\n\n        # algorithms.isin will eventually convert values to an ndarray, so no extra\n        # cost to doing it here first\n        values_arr = np.asarray(values)\n        result = isin(self._data, values_arr)\n\n        if self._hasna:\n            values_have_NA = values_arr.dtype == object and any(\n                val is self.dtype.na_value for val in values_arr\n            )\n\n            # For now, NA does not propagate so set result according to presence of NA,\n            # see https://github.com/pandas-dev/pandas/pull/38379 for some discussion\n            result[self._mask] = values_have_NA\n\n        mask = np.zeros(self._data.shape, dtype=bool)\n        return BooleanArray(result, mask, copy=False)\n\n    def copy(self) -> Self:\n        data = self._data.copy()\n        mask = self._mask.copy()\n        return self._simple_new(data, mask)\n\n    @doc(ExtensionArray.duplicated)\n    def duplicated(\n        self, keep: Literal[\"first\", \"last\", False] = \"first\"\n    ) -> npt.NDArray[np.bool_]:\n        values = self._data\n        mask = self._mask\n        return algos.duplicated(values, keep=keep, mask=mask)\n\n    def unique(self) -> Self:\n        \"\"\"\n        Compute the BaseMaskedArray of unique values.\n\n        Returns\n        -------\n        uniques : BaseMaskedArray\n        \"\"\"\n        uniques, mask = algos.unique_with_mask(self._data, self._mask)\n        return self._simple_new(uniques, mask)\n\n    @doc(ExtensionArray.searchsorted)\n    def searchsorted(\n        self,\n        value: NumpyValueArrayLike | ExtensionArray,\n        side: Literal[\"left\", \"right\"] = \"left\",\n        sorter: NumpySorter | None = None,\n    ) -> npt.NDArray[np.intp] | np.intp:\n        if self._hasna:\n            raise ValueError(\n                \"searchsorted requires array to be sorted, which is impossible \"\n                \"with NAs present.\"\n            )\n        if isinstance(value, ExtensionArray):\n            value = value.astype(object)\n        # Base class searchsorted would cast to object, which is *much* slower.\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n\n    @doc(ExtensionArray.factorize)\n    def factorize(\n        self,\n        use_na_sentinel: bool = True,\n    ) -> tuple[np.ndarray, ExtensionArray]:\n        arr = self._data\n        mask = self._mask\n\n        # Use a sentinel for na; recode and add NA to uniques if necessary below\n        codes, uniques = factorize_array(arr, use_na_sentinel=True, mask=mask)\n\n        # check that factorize_array correctly preserves dtype.\n        assert uniques.dtype == self.dtype.numpy_dtype, (uniques.dtype, self.dtype)\n\n        has_na = mask.any()\n        if use_na_sentinel or not has_na:\n            size = len(uniques)\n        else:\n            # Make room for an NA value\n            size = len(uniques) + 1\n        uniques_mask = np.zeros(size, dtype=bool)\n        if not use_na_sentinel and has_na:\n            na_index = mask.argmax()\n            # Insert na with the proper code\n            if na_index == 0:\n                na_code = np.intp(0)\n            else:\n                na_code = codes[:na_index].max() + 1\n            codes[codes >= na_code] += 1\n            codes[codes == -1] = na_code\n            # dummy value for uniques; not used since uniques_mask will be True\n            uniques = np.insert(uniques, na_code, 0)\n            uniques_mask[na_code] = True\n        uniques_ea = self._simple_new(uniques, uniques_mask)\n\n        return codes, uniques_ea\n\n    @doc(ExtensionArray._values_for_argsort)\n    def _values_for_argsort(self) -> np.ndarray:\n        return self._data\n\n    def value_counts(self, dropna: bool = True) -> Series:\n        \"\"\"\n        Returns a Series containing counts of each unique value.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include counts of missing values.\n\n        Returns\n        -------\n        counts : Series\n\n        See Also\n        --------\n        Series.value_counts\n        \"\"\"\n        from pandas import (\n            Index,\n            Series,\n        )\n        from pandas.arrays import IntegerArray\n\n        keys, value_counts, na_counter = algos.value_counts_arraylike(\n            self._data, dropna=dropna, mask=self._mask\n        )\n        mask_index = np.zeros((len(value_counts),), dtype=np.bool_)\n        mask = mask_index.copy()\n\n        if na_counter > 0:\n            mask_index[-1] = True\n\n        arr = IntegerArray(value_counts, mask)\n        index = Index(\n            self.dtype.construct_array_type()(\n                keys, mask_index  # type: ignore[arg-type]\n            )\n        )\n        return Series(arr, index=index, name=\"count\", copy=False)\n\n    def _mode(self, dropna: bool = True) -> Self:\n        if dropna:\n            result = mode(self._data, dropna=dropna, mask=self._mask)\n            res_mask = np.zeros(result.shape, dtype=np.bool_)\n        else:\n            result, res_mask = mode(self._data, dropna=dropna, mask=self._mask)\n        result = type(self)(result, res_mask)  # type: ignore[arg-type]\n        return result[result.argsort()]\n\n    @doc(ExtensionArray.equals)\n    def equals(self, other) -> bool:\n        if type(self) != type(other):\n            return False\n        if other.dtype != self.dtype:\n            return False\n\n        # GH#44382 if e.g. self[1] is np.nan and other[1] is pd.NA, we are NOT\n        #  equal.\n        if not np.array_equal(self._mask, other._mask):\n            return False\n\n        left = self._data[~self._mask]\n        right = other._data[~other._mask]\n        return array_equivalent(left, right, strict_nan=True, dtype_equal=True)\n\n    def _quantile(\n        self, qs: npt.NDArray[np.float64], interpolation: str\n    ) -> BaseMaskedArray:\n        \"\"\"\n        Dispatch to quantile_with_mask, needed because we do not have\n        _from_factorized.\n\n        Notes\n        -----\n        We assume that all impacted cases are 1D-only.\n        \"\"\"\n        res = quantile_with_mask(\n            self._data,\n            mask=self._mask,\n            # TODO(GH#40932): na_value_for_dtype(self.dtype.numpy_dtype)\n            #  instead of np.nan\n            fill_value=np.nan,\n            qs=qs,\n            interpolation=interpolation,\n        )\n\n        if self._hasna:\n            # Our result mask is all-False unless we are all-NA, in which\n            #  case it is all-True.\n            if self.ndim == 2:\n                # I think this should be out_mask=self.isna().all(axis=1)\n                #  but am holding off until we have tests\n                raise NotImplementedError\n            if self.isna().all():\n                out_mask = np.ones(res.shape, dtype=bool)\n\n                if is_integer_dtype(self.dtype):\n                    # We try to maintain int dtype if possible for not all-na case\n                    # as well\n                    res = np.zeros(res.shape, dtype=self.dtype.numpy_dtype)\n            else:\n                out_mask = np.zeros(res.shape, dtype=bool)\n        else:\n            out_mask = np.zeros(res.shape, dtype=bool)\n        return self._maybe_mask_result(res, mask=out_mask)\n\n    # ------------------------------------------------------------------\n    # Reductions\n\n    def _reduce(\n        self, name: str, *, skipna: bool = True, keepdims: bool = False, **kwargs\n    ):\n        if name in {\"any\", \"all\", \"min\", \"max\", \"sum\", \"prod\", \"mean\", \"var\", \"std\"}:\n            result = getattr(self, name)(skipna=skipna, **kwargs)\n        else:\n            # median, skew, kurt, sem\n            data = self._data\n            mask = self._mask\n            op = getattr(nanops, f\"nan{name}\")\n            axis = kwargs.pop(\"axis\", None)\n            result = op(data, axis=axis, skipna=skipna, mask=mask, **kwargs)\n\n        if keepdims:\n            if isna(result):\n                return self._wrap_na_result(name=name, axis=0, mask_size=(1,))\n            else:\n                result = result.reshape(1)\n                mask = np.zeros(1, dtype=bool)\n                return self._maybe_mask_result(result, mask)\n\n        if isna(result):\n            return libmissing.NA\n        else:\n            return result\n\n    def _wrap_reduction_result(self, name: str, result, *, skipna, axis):\n        if isinstance(result, np.ndarray):\n            if skipna:\n                # we only retain mask for all-NA rows/columns\n                mask = self._mask.all(axis=axis)\n            else:\n                mask = self._mask.any(axis=axis)\n\n            return self._maybe_mask_result(result, mask)\n        return result\n\n    def _wrap_na_result(self, *, name, axis, mask_size):\n        mask = np.ones(mask_size, dtype=bool)\n\n        float_dtyp = \"float32\" if self.dtype == \"Float32\" else \"float64\"\n        if name in [\"mean\", \"median\", \"var\", \"std\", \"skew\", \"kurt\"]:\n            np_dtype = float_dtyp\n        elif name in [\"min\", \"max\"] or self.dtype.itemsize == 8:\n            np_dtype = self.dtype.numpy_dtype.name\n        else:\n            is_windows_or_32bit = is_platform_windows() or not IS64\n            int_dtyp = \"int32\" if is_windows_or_32bit else \"int64\"\n            uint_dtyp = \"uint32\" if is_windows_or_32bit else \"uint64\"\n            np_dtype = {\"b\": int_dtyp, \"i\": int_dtyp, \"u\": uint_dtyp, \"f\": float_dtyp}[\n                self.dtype.kind\n            ]\n\n        value = np.array([1], dtype=np_dtype)\n        return self._maybe_mask_result(value, mask=mask)\n\n    def _wrap_min_count_reduction_result(\n        self, name: str, result, *, skipna, min_count, axis\n    ):\n        if min_count == 0 and isinstance(result, np.ndarray):\n            return self._maybe_mask_result(result, np.zeros(result.shape, dtype=bool))\n        return self._wrap_reduction_result(name, result, skipna=skipna, axis=axis)\n\n    def sum(\n        self,\n        *,\n        skipna: bool = True,\n        min_count: int = 0,\n        axis: AxisInt | None = 0,\n        **kwargs,\n    ):\n        nv.validate_sum((), kwargs)\n\n        result = masked_reductions.sum(\n            self._data,\n            self._mask,\n            skipna=skipna,\n            min_count=min_count,\n            axis=axis,\n        )\n        return self._wrap_min_count_reduction_result(\n            \"sum\", result, skipna=skipna, min_count=min_count, axis=axis\n        )\n\n    def prod(\n        self,\n        *,\n        skipna: bool = True,\n        min_count: int = 0,\n        axis: AxisInt | None = 0,\n        **kwargs,\n    ):\n        nv.validate_prod((), kwargs)\n\n        result = masked_reductions.prod(\n            self._data,\n            self._mask,\n            skipna=skipna,\n            min_count=min_count,\n            axis=axis,\n        )\n        return self._wrap_min_count_reduction_result(\n            \"prod\", result, skipna=skipna, min_count=min_count, axis=axis\n        )\n\n    def mean(self, *, skipna: bool = True, axis: AxisInt | None = 0, **kwargs):\n        nv.validate_mean((), kwargs)\n        result = masked_reductions.mean(\n            self._data,\n            self._mask,\n            skipna=skipna,\n            axis=axis,\n        )\n        return self._wrap_reduction_result(\"mean\", result, skipna=skipna, axis=axis)\n\n    def var(\n        self, *, skipna: bool = True, axis: AxisInt | None = 0, ddof: int = 1, **kwargs\n    ):\n        nv.validate_stat_ddof_func((), kwargs, fname=\"var\")\n        result = masked_reductions.var(\n            self._data,\n            self._mask,\n            skipna=skipna,\n            axis=axis,\n            ddof=ddof,\n        )\n        return self._wrap_reduction_result(\"var\", result, skipna=skipna, axis=axis)\n\n    def std(\n        self, *, skipna: bool = True, axis: AxisInt | None = 0, ddof: int = 1, **kwargs\n    ):\n        nv.validate_stat_ddof_func((), kwargs, fname=\"std\")\n        result = masked_reductions.std(\n            self._data,\n            self._mask,\n            skipna=skipna,\n            axis=axis,\n            ddof=ddof,\n        )\n        return self._wrap_reduction_result(\"std\", result, skipna=skipna, axis=axis)\n\n    def min(self, *, skipna: bool = True, axis: AxisInt | None = 0, **kwargs):\n        nv.validate_min((), kwargs)\n        result = masked_reductions.min(\n            self._data,\n            self._mask,\n            skipna=skipna,\n            axis=axis,\n        )\n        return self._wrap_reduction_result(\"min\", result, skipna=skipna, axis=axis)\n\n    def max(self, *, skipna: bool = True, axis: AxisInt | None = 0, **kwargs):\n        nv.validate_max((), kwargs)\n        result = masked_reductions.max(\n            self._data,\n            self._mask,\n            skipna=skipna,\n            axis=axis,\n        )\n        return self._wrap_reduction_result(\"max\", result, skipna=skipna, axis=axis)\n\n    def map(self, mapper, na_action=None):\n        return map_array(self.to_numpy(), mapper, na_action=na_action)\n\n    def any(self, *, skipna: bool = True, axis: AxisInt | None = 0, **kwargs):\n        \"\"\"\n        Return whether any element is truthy.\n\n        Returns False unless there is at least one element that is truthy.\n        By default, NAs are skipped. If ``skipna=False`` is specified and\n        missing values are present, similar :ref:`Kleene logic <boolean.kleene>`\n        is used as for logical operations.\n\n        .. versionchanged:: 1.4.0\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Exclude NA values. If the entire array is NA and `skipna` is\n            True, then the result will be False, as for an empty array.\n            If `skipna` is False, the result will still be True if there is\n            at least one element that is truthy, otherwise NA will be returned\n            if there are NA's present.\n        axis : int, optional, default 0\n        **kwargs : any, default None\n            Additional keywords have no effect but might be accepted for\n            compatibility with NumPy.\n\n        Returns\n        -------\n        bool or :attr:`pandas.NA`\n\n        See Also\n        --------\n        numpy.any : Numpy version of this method.\n        BaseMaskedArray.all : Return whether all elements are truthy.\n\n        Examples\n        --------\n        The result indicates whether any element is truthy (and by default\n        skips NAs):\n\n        >>> pd.array([True, False, True]).any()\n        True\n        >>> pd.array([True, False, pd.NA]).any()\n        True\n        >>> pd.array([False, False, pd.NA]).any()\n        False\n        >>> pd.array([], dtype=\"boolean\").any()\n        False\n        >>> pd.array([pd.NA], dtype=\"boolean\").any()\n        False\n        >>> pd.array([pd.NA], dtype=\"Float64\").any()\n        False\n\n        With ``skipna=False``, the result can be NA if this is logically\n        required (whether ``pd.NA`` is True or False influences the result):\n\n        >>> pd.array([True, False, pd.NA]).any(skipna=False)\n        True\n        >>> pd.array([1, 0, pd.NA]).any(skipna=False)\n        True\n        >>> pd.array([False, False, pd.NA]).any(skipna=False)\n        <NA>\n        >>> pd.array([0, 0, pd.NA]).any(skipna=False)\n        <NA>\n        \"\"\"\n        nv.validate_any((), kwargs)\n\n        values = self._data.copy()\n        # error: Argument 3 to \"putmask\" has incompatible type \"object\";\n        # expected \"Union[_SupportsArray[dtype[Any]],\n        # _NestedSequence[_SupportsArray[dtype[Any]]],\n        # bool, int, float, complex, str, bytes,\n        # _NestedSequence[Union[bool, int, float, complex, str, bytes]]]\"\n        np.putmask(values, self._mask, self._falsey_value)  # type: ignore[arg-type]\n        result = values.any()\n        if skipna:\n            return result\n        else:\n            if result or len(self) == 0 or not self._mask.any():\n                return result\n            else:\n                return self.dtype.na_value\n\n    def all(self, *, skipna: bool = True, axis: AxisInt | None = 0, **kwargs):\n        \"\"\"\n        Return whether all elements are truthy.\n\n        Returns True unless there is at least one element that is falsey.\n        By default, NAs are skipped. If ``skipna=False`` is specified and\n        missing values are present, similar :ref:`Kleene logic <boolean.kleene>`\n        is used as for logical operations.\n\n        .. versionchanged:: 1.4.0\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Exclude NA values. If the entire array is NA and `skipna` is\n            True, then the result will be True, as for an empty array.\n            If `skipna` is False, the result will still be False if there is\n            at least one element that is falsey, otherwise NA will be returned\n            if there are NA's present.\n        axis : int, optional, default 0\n        **kwargs : any, default None\n            Additional keywords have no effect but might be accepted for\n            compatibility with NumPy.\n\n        Returns\n        -------\n        bool or :attr:`pandas.NA`\n\n        See Also\n        --------\n        numpy.all : Numpy version of this method.\n        BooleanArray.any : Return whether any element is truthy.\n\n        Examples\n        --------\n        The result indicates whether all elements are truthy (and by default\n        skips NAs):\n\n        >>> pd.array([True, True, pd.NA]).all()\n        True\n        >>> pd.array([1, 1, pd.NA]).all()\n        True\n        >>> pd.array([True, False, pd.NA]).all()\n        False\n        >>> pd.array([], dtype=\"boolean\").all()\n        True\n        >>> pd.array([pd.NA], dtype=\"boolean\").all()\n        True\n        >>> pd.array([pd.NA], dtype=\"Float64\").all()\n        True\n\n        With ``skipna=False``, the result can be NA if this is logically\n        required (whether ``pd.NA`` is True or False influences the result):\n\n        >>> pd.array([True, True, pd.NA]).all(skipna=False)\n        <NA>\n        >>> pd.array([1, 1, pd.NA]).all(skipna=False)\n        <NA>\n        >>> pd.array([True, False, pd.NA]).all(skipna=False)\n        False\n        >>> pd.array([1, 0, pd.NA]).all(skipna=False)\n        False\n        \"\"\"\n        nv.validate_all((), kwargs)\n\n        values = self._data.copy()\n        # error: Argument 3 to \"putmask\" has incompatible type \"object\";\n        # expected \"Union[_SupportsArray[dtype[Any]],\n        # _NestedSequence[_SupportsArray[dtype[Any]]],\n        # bool, int, float, complex, str, bytes,\n        # _NestedSequence[Union[bool, int, float, complex, str, bytes]]]\"\n        np.putmask(values, self._mask, self._truthy_value)  # type: ignore[arg-type]\n        result = values.all(axis=axis)\n\n        if skipna:\n            return result\n        else:\n            if not result or len(self) == 0 or not self._mask.any():\n                return result\n            else:\n                return self.dtype.na_value\n\n    def interpolate(\n        self,\n        *,\n        method: InterpolateOptions,\n        axis: int,\n        index,\n        limit,\n        limit_direction,\n        limit_area,\n        copy: bool,\n        **kwargs,\n    ) -> FloatingArray:\n        \"\"\"\n        See NDFrame.interpolate.__doc__.\n        \"\"\"\n        # NB: we return type(self) even if copy=False\n        if self.dtype.kind == \"f\":\n            if copy:\n                data = self._data.copy()\n                mask = self._mask.copy()\n            else:\n                data = self._data\n                mask = self._mask\n        elif self.dtype.kind in \"iu\":\n            copy = True\n            data = self._data.astype(\"f8\")\n            mask = self._mask.copy()\n        else:\n            raise NotImplementedError(\n                f\"interpolate is not implemented for dtype={self.dtype}\"\n            )\n\n        missing.interpolate_2d_inplace(\n            data,\n            method=method,\n            axis=0,\n            index=index,\n            limit=limit,\n            limit_direction=limit_direction,\n            limit_area=limit_area,\n            mask=mask,\n            **kwargs,\n        )\n        if not copy:\n            return self  # type: ignore[return-value]\n        if self.dtype.kind == \"f\":\n            return type(self)._simple_new(data, mask)  # type: ignore[return-value]\n        else:\n            from pandas.core.arrays import FloatingArray\n\n            return FloatingArray._simple_new(data, mask)\n\n    def _accumulate(\n        self, name: str, *, skipna: bool = True, **kwargs\n    ) -> BaseMaskedArray:\n        data = self._data\n        mask = self._mask\n\n        op = getattr(masked_accumulations, name)\n        data, mask = op(data, mask, skipna=skipna, **kwargs)\n\n        return self._simple_new(data, mask)\n\n    # ------------------------------------------------------------------\n    # GroupBy Methods\n\n    def _groupby_op(\n        self,\n        *,\n        how: str,\n        has_dropped_na: bool,\n        min_count: int,\n        ngroups: int,\n        ids: npt.NDArray[np.intp],\n        **kwargs,\n    ):\n        from pandas.core.groupby.ops import WrappedCythonOp\n\n        kind = WrappedCythonOp.get_kind_from_how(how)\n        op = WrappedCythonOp(how=how, kind=kind, has_dropped_na=has_dropped_na)\n\n        # libgroupby functions are responsible for NOT altering mask\n        mask = self._mask\n        if op.kind != \"aggregate\":\n            result_mask = mask.copy()\n        else:\n            result_mask = np.zeros(ngroups, dtype=bool)\n\n        if how == \"rank\" and kwargs.get(\"na_option\") in [\"top\", \"bottom\"]:\n            result_mask[:] = False\n\n        res_values = op._cython_op_ndim_compat(\n            self._data,\n            min_count=min_count,\n            ngroups=ngroups,\n            comp_ids=ids,\n            mask=mask,\n            result_mask=result_mask,\n            **kwargs,\n        )\n\n        if op.how == \"ohlc\":\n            arity = op._cython_arity.get(op.how, 1)\n            result_mask = np.tile(result_mask, (arity, 1)).T\n\n        if op.how in [\"idxmin\", \"idxmax\"]:\n            # Result values are indexes to take, keep as ndarray\n            return res_values\n        else:\n            # res_values should already have the correct dtype, we just need to\n            #  wrap in a MaskedArray\n            return self._maybe_mask_result(res_values, result_mask)\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.masked/BaseMaskedArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/period.py", "fn_id": "", "content": "class PeriodArray(dtl.DatelikeOps, libperiod.PeriodMixin):  # type: ignore[misc]\n    \"\"\"\n    Pandas ExtensionArray for storing Period data.\n\n    Users should use :func:`~pandas.array` to create new instances.\n\n    Parameters\n    ----------\n    values : Union[PeriodArray, Series[period], ndarray[int], PeriodIndex]\n        The data to store. These should be arrays that can be directly\n        converted to ordinals without inference or copy (PeriodArray,\n        ndarray[int64]), or a box around such an array (Series[period],\n        PeriodIndex).\n    dtype : PeriodDtype, optional\n        A PeriodDtype instance from which to extract a `freq`. If both\n        `freq` and `dtype` are specified, then the frequencies must match.\n    freq : str or DateOffset\n        The `freq` to use for the array. Mostly applicable when `values`\n        is an ndarray of integers, when `freq` is required. When `values`\n        is a PeriodArray (or box around), it's checked that ``values.freq``\n        matches `freq`.\n    copy : bool, default False\n        Whether to copy the ordinals before storing.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    See Also\n    --------\n    Period: Represents a period of time.\n    PeriodIndex : Immutable Index for period data.\n    period_range: Create a fixed-frequency PeriodArray.\n    array: Construct a pandas array.\n\n    Notes\n    -----\n    There are two components to a PeriodArray\n\n    - ordinals : integer ndarray\n    - freq : pd.tseries.offsets.Offset\n\n    The values are physically stored as a 1-D ndarray of integers. These are\n    called \"ordinals\" and represent some kind of offset from a base.\n\n    The `freq` indicates the span covered by each element of the array.\n    All elements in the PeriodArray have the same `freq`.\n\n    Examples\n    --------\n    >>> pd.arrays.PeriodArray(pd.PeriodIndex(['2023-01-01',\n    ...                                       '2023-01-02'], freq='D'))\n    <PeriodArray>\n    ['2023-01-01', '2023-01-02']\n    Length: 2, dtype: period[D]\n    \"\"\"\n\n    # array priority higher than numpy scalars\n    __array_priority__ = 1000\n    _typ = \"periodarray\"  # ABCPeriodArray\n    _internal_fill_value = np.int64(iNaT)\n    _recognized_scalars = (Period,)\n    _is_recognized_dtype = lambda x: isinstance(\n        x, PeriodDtype\n    )  # check_compatible_with checks freq match\n    _infer_matches = (\"period\",)\n\n    @property\n    def _scalar_type(self) -> type[Period]:\n        return Period\n\n    # Names others delegate to us\n    _other_ops: list[str] = []\n    _bool_ops: list[str] = [\"is_leap_year\"]\n    _object_ops: list[str] = [\"start_time\", \"end_time\", \"freq\"]\n    _field_ops: list[str] = [\n        \"year\",\n        \"month\",\n        \"day\",\n        \"hour\",\n        \"minute\",\n        \"second\",\n        \"weekofyear\",\n        \"weekday\",\n        \"week\",\n        \"dayofweek\",\n        \"day_of_week\",\n        \"dayofyear\",\n        \"day_of_year\",\n        \"quarter\",\n        \"qyear\",\n        \"days_in_month\",\n        \"daysinmonth\",\n    ]\n    _datetimelike_ops: list[str] = _field_ops + _object_ops + _bool_ops\n    _datetimelike_methods: list[str] = [\"strftime\", \"to_timestamp\", \"asfreq\"]\n\n    _dtype: PeriodDtype\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self, values, dtype: Dtype | None = None, freq=None, copy: bool = False\n    ) -> None:\n        if freq is not None:\n            # GH#52462\n            warnings.warn(\n                \"The 'freq' keyword in the PeriodArray constructor is deprecated \"\n                \"and will be removed in a future version. Pass 'dtype' instead\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            freq = validate_dtype_freq(dtype, freq)\n            dtype = PeriodDtype(freq)\n\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n            if not isinstance(dtype, PeriodDtype):\n                raise ValueError(f\"Invalid dtype {dtype} for PeriodArray\")\n\n        if isinstance(values, ABCSeries):\n            values = values._values\n            if not isinstance(values, type(self)):\n                raise TypeError(\"Incorrect dtype\")\n\n        elif isinstance(values, ABCPeriodIndex):\n            values = values._values\n\n        if isinstance(values, type(self)):\n            if dtype is not None and dtype != values.dtype:\n                raise raise_on_incompatible(values, dtype.freq)\n            values, dtype = values._ndarray, values.dtype\n\n        if not copy:\n            values = np.asarray(values, dtype=\"int64\")\n        else:\n            values = np.array(values, dtype=\"int64\", copy=copy)\n        if dtype is None:\n            raise ValueError(\"dtype is not specified and cannot be inferred\")\n        dtype = cast(PeriodDtype, dtype)\n        NDArrayBacked.__init__(self, values, dtype)\n\n    # error: Signature of \"_simple_new\" incompatible with supertype \"NDArrayBacked\"\n    @classmethod\n    def _simple_new(  # type: ignore[override]\n        cls,\n        values: npt.NDArray[np.int64],\n        dtype: PeriodDtype,\n    ) -> Self:\n        # alias for PeriodArray.__init__\n        assertion_msg = \"Should be numpy array of type i8\"\n        assert isinstance(values, np.ndarray) and values.dtype == \"i8\", assertion_msg\n        return cls(values, dtype=dtype)\n\n    @classmethod\n    def _from_sequence(\n        cls,\n        scalars,\n        *,\n        dtype: Dtype | None = None,\n        copy: bool = False,\n    ) -> Self:\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n        if dtype and isinstance(dtype, PeriodDtype):\n            freq = dtype.freq\n        else:\n            freq = None\n\n        if isinstance(scalars, cls):\n            validate_dtype_freq(scalars.dtype, freq)\n            if copy:\n                scalars = scalars.copy()\n            return scalars\n\n        periods = np.asarray(scalars, dtype=object)\n\n        freq = freq or libperiod.extract_freq(periods)\n        ordinals = libperiod.extract_ordinals(periods, freq)\n        dtype = PeriodDtype(freq)\n        return cls(ordinals, dtype=dtype)\n\n    @classmethod\n    def _from_sequence_of_strings(\n        cls, strings, *, dtype: Dtype | None = None, copy: bool = False\n    ) -> Self:\n        return cls._from_sequence(strings, dtype=dtype, copy=copy)\n\n    @classmethod\n    def _from_datetime64(cls, data, freq, tz=None) -> Self:\n        \"\"\"\n        Construct a PeriodArray from a datetime64 array\n\n        Parameters\n        ----------\n        data : ndarray[datetime64[ns], datetime64[ns, tz]]\n        freq : str or Tick\n        tz : tzinfo, optional\n\n        Returns\n        -------\n        PeriodArray[freq]\n        \"\"\"\n        if isinstance(freq, BaseOffset):\n            freq = freq_to_period_freqstr(freq.n, freq.name)\n        data, freq = dt64arr_to_periodarr(data, freq, tz)\n        dtype = PeriodDtype(freq)\n        return cls(data, dtype=dtype)\n\n    @classmethod\n    def _generate_range(cls, start, end, periods, freq):\n        periods = dtl.validate_periods(periods)\n\n        if freq is not None:\n            freq = Period._maybe_convert_freq(freq)\n\n        if start is not None or end is not None:\n            subarr, freq = _get_ordinal_range(start, end, periods, freq)\n        else:\n            raise ValueError(\"Not enough parameters to construct Period range\")\n\n        return subarr, freq\n\n    @classmethod\n    def _from_fields(cls, *, fields: dict, freq) -> Self:\n        subarr, freq = _range_from_fields(freq=freq, **fields)\n        dtype = PeriodDtype(freq)\n        return cls._simple_new(subarr, dtype=dtype)\n\n    # -----------------------------------------------------------------\n    # DatetimeLike Interface\n\n    # error: Argument 1 of \"_unbox_scalar\" is incompatible with supertype\n    # \"DatetimeLikeArrayMixin\"; supertype defines the argument type as\n    # \"Union[Union[Period, Any, Timedelta], NaTType]\"\n    def _unbox_scalar(  # type: ignore[override]\n        self,\n        value: Period | NaTType,\n    ) -> np.int64:\n        if value is NaT:\n            # error: Item \"Period\" of \"Union[Period, NaTType]\" has no attribute \"value\"\n            return np.int64(value._value)  # type: ignore[union-attr]\n        elif isinstance(value, self._scalar_type):\n            self._check_compatible_with(value)\n            return np.int64(value.ordinal)\n        else:\n            raise ValueError(f\"'value' should be a Period. Got '{value}' instead.\")\n\n    def _scalar_from_string(self, value: str) -> Period:\n        return Period(value, freq=self.freq)\n\n    # error: Argument 1 of \"_check_compatible_with\" is incompatible with\n    # supertype \"DatetimeLikeArrayMixin\"; supertype defines the argument type\n    # as \"Period | Timestamp | Timedelta | NaTType\"\n    def _check_compatible_with(self, other: Period | NaTType | PeriodArray) -> None:  # type: ignore[override]\n        if other is NaT:\n            return\n        # error: Item \"NaTType\" of \"Period | NaTType | PeriodArray\" has no\n        # attribute \"freq\"\n        self._require_matching_freq(other.freq)  # type: ignore[union-attr]\n\n    # --------------------------------------------------------------------\n    # Data / Attributes\n\n    @cache_readonly\n    def dtype(self) -> PeriodDtype:\n        return self._dtype\n\n    # error: Cannot override writeable attribute with read-only property\n    @property  # type: ignore[override]\n    def freq(self) -> BaseOffset:\n        \"\"\"\n        Return the frequency object for this PeriodArray.\n        \"\"\"\n        return self.dtype.freq\n\n    @property\n    def freqstr(self) -> str:\n        return freq_to_period_freqstr(self.freq.n, self.freq.name)\n\n    def __array__(\n        self, dtype: NpDtype | None = None, copy: bool | None = None\n    ) -> np.ndarray:\n        if dtype == \"i8\":\n            return self.asi8\n        elif dtype == bool:\n            return ~self._isnan\n\n        # This will raise TypeError for non-object dtypes\n        return np.array(list(self), dtype=object)\n\n    def __arrow_array__(self, type=None):\n        \"\"\"\n        Convert myself into a pyarrow Array.\n        \"\"\"\n        import pyarrow\n\n        from pandas.core.arrays.arrow.extension_types import ArrowPeriodType\n\n        if type is not None:\n            if pyarrow.types.is_integer(type):\n                return pyarrow.array(self._ndarray, mask=self.isna(), type=type)\n            elif isinstance(type, ArrowPeriodType):\n                # ensure we have the same freq\n                if self.freqstr != type.freq:\n                    raise TypeError(\n                        \"Not supported to convert PeriodArray to array with different \"\n                        f\"'freq' ({self.freqstr} vs {type.freq})\"\n                    )\n            else:\n                raise TypeError(\n                    f\"Not supported to convert PeriodArray to '{type}' type\"\n                )\n\n        period_type = ArrowPeriodType(self.freqstr)\n        storage_array = pyarrow.array(self._ndarray, mask=self.isna(), type=\"int64\")\n        return pyarrow.ExtensionArray.from_storage(period_type, storage_array)\n\n    # --------------------------------------------------------------------\n    # Vectorized analogues of Period properties\n\n    year = _field_accessor(\n        \"year\",\n        \"\"\"\n        The year of the period.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023\", \"2024\", \"2025\"], freq=\"Y\")\n        >>> idx.year\n        Index([2023, 2024, 2025], dtype='int64')\n        \"\"\",\n    )\n    month = _field_accessor(\n        \"month\",\n        \"\"\"\n        The month as January=1, December=12.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023-01\", \"2023-02\", \"2023-03\"], freq=\"M\")\n        >>> idx.month\n        Index([1, 2, 3], dtype='int64')\n        \"\"\",\n    )\n    day = _field_accessor(\n        \"day\",\n        \"\"\"\n        The days of the period.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex(['2020-01-31', '2020-02-28'], freq='D')\n        >>> idx.day\n        Index([31, 28], dtype='int64')\n        \"\"\",\n    )\n    hour = _field_accessor(\n        \"hour\",\n        \"\"\"\n        The hour of the period.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023-01-01 10:00\", \"2023-01-01 11:00\"], freq='h')\n        >>> idx.hour\n        Index([10, 11], dtype='int64')\n        \"\"\",\n    )\n    minute = _field_accessor(\n        \"minute\",\n        \"\"\"\n        The minute of the period.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023-01-01 10:30:00\",\n        ...                       \"2023-01-01 11:50:00\"], freq='min')\n        >>> idx.minute\n        Index([30, 50], dtype='int64')\n        \"\"\",\n    )\n    second = _field_accessor(\n        \"second\",\n        \"\"\"\n        The second of the period.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023-01-01 10:00:30\",\n        ...                       \"2023-01-01 10:00:31\"], freq='s')\n        >>> idx.second\n        Index([30, 31], dtype='int64')\n        \"\"\",\n    )\n    weekofyear = _field_accessor(\n        \"week\",\n        \"\"\"\n        The week ordinal of the year.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023-01\", \"2023-02\", \"2023-03\"], freq=\"M\")\n        >>> idx.week  # It can be written `weekofyear`\n        Index([5, 9, 13], dtype='int64')\n        \"\"\",\n    )\n    week = weekofyear\n    day_of_week = _field_accessor(\n        \"day_of_week\",\n        \"\"\"\n        The day of the week with Monday=0, Sunday=6.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"], freq=\"D\")\n        >>> idx.weekday\n        Index([6, 0, 1], dtype='int64')\n        \"\"\",\n    )\n    dayofweek = day_of_week\n    weekday = dayofweek\n    dayofyear = day_of_year = _field_accessor(\n        \"day_of_year\",\n        \"\"\"\n        The ordinal day of the year.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023-01-10\", \"2023-02-01\", \"2023-03-01\"], freq=\"D\")\n        >>> idx.dayofyear\n        Index([10, 32, 60], dtype='int64')\n\n        >>> idx = pd.PeriodIndex([\"2023\", \"2024\", \"2025\"], freq=\"Y\")\n        >>> idx\n        PeriodIndex(['2023', '2024', '2025'], dtype='period[Y-DEC]')\n        >>> idx.dayofyear\n        Index([365, 366, 365], dtype='int64')\n        \"\"\",\n    )\n    quarter = _field_accessor(\n        \"quarter\",\n        \"\"\"\n        The quarter of the date.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023-01\", \"2023-02\", \"2023-03\"], freq=\"M\")\n        >>> idx.quarter\n        Index([1, 1, 1], dtype='int64')\n        \"\"\",\n    )\n    qyear = _field_accessor(\"qyear\")\n    days_in_month = _field_accessor(\n        \"days_in_month\",\n        \"\"\"\n        The number of days in the month.\n\n        Examples\n        --------\n        For Series:\n\n        >>> period = pd.period_range('2020-1-1 00:00', '2020-3-1 00:00', freq='M')\n        >>> s = pd.Series(period)\n        >>> s\n        0   2020-01\n        1   2020-02\n        2   2020-03\n        dtype: period[M]\n        >>> s.dt.days_in_month\n        0    31\n        1    29\n        2    31\n        dtype: int64\n\n        For PeriodIndex:\n\n        >>> idx = pd.PeriodIndex([\"2023-01\", \"2023-02\", \"2023-03\"], freq=\"M\")\n        >>> idx.days_in_month   # It can be also entered as `daysinmonth`\n        Index([31, 28, 31], dtype='int64')\n        \"\"\",\n    )\n    daysinmonth = days_in_month\n\n    @property\n    def is_leap_year(self) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Logical indicating if the date belongs to a leap year.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023\", \"2024\", \"2025\"], freq=\"Y\")\n        >>> idx.is_leap_year\n        array([False,  True, False])\n        \"\"\"\n        return isleapyear_arr(np.asarray(self.year))\n\n    def to_timestamp(self, freq=None, how: str = \"start\") -> DatetimeArray:\n        \"\"\"\n        Cast to DatetimeArray/Index.\n\n        Parameters\n        ----------\n        freq : str or DateOffset, optional\n            Target frequency. The default is 'D' for week or longer,\n            's' otherwise.\n        how : {'s', 'e', 'start', 'end'}\n            Whether to use the start or end of the time period being converted.\n\n        Returns\n        -------\n        DatetimeArray/Index\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023-01\", \"2023-02\", \"2023-03\"], freq=\"M\")\n        >>> idx.to_timestamp()\n        DatetimeIndex(['2023-01-01', '2023-02-01', '2023-03-01'],\n        dtype='datetime64[ns]', freq='MS')\n        \"\"\"\n        from pandas.core.arrays import DatetimeArray\n\n        how = libperiod.validate_end_alias(how)\n\n        end = how == \"E\"\n        if end:\n            if freq == \"B\" or self.freq == \"B\":\n                # roll forward to ensure we land on B date\n                adjust = Timedelta(1, \"D\") - Timedelta(1, \"ns\")\n                return self.to_timestamp(how=\"start\") + adjust\n            else:\n                adjust = Timedelta(1, \"ns\")\n                return (self + self.freq).to_timestamp(how=\"start\") - adjust\n\n        if freq is None:\n            freq_code = self._dtype._get_to_timestamp_base()\n            dtype = PeriodDtypeBase(freq_code, 1)\n            freq = dtype._freqstr\n            base = freq_code\n        else:\n            freq = Period._maybe_convert_freq(freq)\n            base = freq._period_dtype_code\n\n        new_parr = self.asfreq(freq, how=how)\n\n        new_data = libperiod.periodarr_to_dt64arr(new_parr.asi8, base)\n        dta = DatetimeArray._from_sequence(new_data)\n\n        if self.freq.name == \"B\":\n            # See if we can retain BDay instead of Day in cases where\n            #  len(self) is too small for infer_freq to distinguish between them\n            diffs = libalgos.unique_deltas(self.asi8)\n            if len(diffs) == 1:\n                diff = diffs[0]\n                if diff == self.dtype._n:\n                    dta._freq = self.freq\n                elif diff == 1:\n                    dta._freq = self.freq.base\n                # TODO: other cases?\n            return dta\n        else:\n            return dta._with_freq(\"infer\")\n\n    # --------------------------------------------------------------------\n\n    def _box_func(self, x) -> Period | NaTType:\n        return Period._from_ordinal(ordinal=x, freq=self.freq)\n\n    @doc(**_shared_doc_kwargs, other=\"PeriodIndex\", other_name=\"PeriodIndex\")\n    def asfreq(self, freq=None, how: str = \"E\") -> Self:\n        \"\"\"\n        Convert the {klass} to the specified frequency `freq`.\n\n        Equivalent to applying :meth:`pandas.Period.asfreq` with the given arguments\n        to each :class:`~pandas.Period` in this {klass}.\n\n        Parameters\n        ----------\n        freq : str\n            A frequency.\n        how : str {{'E', 'S'}}, default 'E'\n            Whether the elements should be aligned to the end\n            or start within pa period.\n\n            * 'E', 'END', or 'FINISH' for end,\n            * 'S', 'START', or 'BEGIN' for start.\n\n            January 31st ('END') vs. January 1st ('START') for example.\n\n        Returns\n        -------\n        {klass}\n            The transformed {klass} with the new frequency.\n\n        See Also\n        --------\n        {other}.asfreq: Convert each Period in a {other_name} to the given frequency.\n        Period.asfreq : Convert a :class:`~pandas.Period` object to the given frequency.\n\n        Examples\n        --------\n        >>> pidx = pd.period_range('2010-01-01', '2015-01-01', freq='Y')\n        >>> pidx\n        PeriodIndex(['2010', '2011', '2012', '2013', '2014', '2015'],\n        dtype='period[Y-DEC]')\n\n        >>> pidx.asfreq('M')\n        PeriodIndex(['2010-12', '2011-12', '2012-12', '2013-12', '2014-12',\n        '2015-12'], dtype='period[M]')\n\n        >>> pidx.asfreq('M', how='S')\n        PeriodIndex(['2010-01', '2011-01', '2012-01', '2013-01', '2014-01',\n        '2015-01'], dtype='period[M]')\n        \"\"\"\n        how = libperiod.validate_end_alias(how)\n        if isinstance(freq, BaseOffset) and hasattr(freq, \"_period_dtype_code\"):\n            freq = PeriodDtype(freq)._freqstr\n        freq = Period._maybe_convert_freq(freq)\n\n        base1 = self._dtype._dtype_code\n        base2 = freq._period_dtype_code\n\n        asi8 = self.asi8\n        # self.freq.n can't be negative or 0\n        end = how == \"E\"\n        if end:\n            ordinal = asi8 + self.dtype._n - 1\n        else:\n            ordinal = asi8\n\n        new_data = period_asfreq_arr(ordinal, base1, base2, end)\n\n        if self._hasna:\n            new_data[self._isnan] = iNaT\n\n        dtype = PeriodDtype(freq)\n        return type(self)(new_data, dtype=dtype)\n\n    # ------------------------------------------------------------------\n    # Rendering Methods\n\n    def _formatter(self, boxed: bool = False):\n        if boxed:\n            return str\n        return \"'{}'\".format\n\n    def _format_native_types(\n        self, *, na_rep: str | float = \"NaT\", date_format=None, **kwargs\n    ) -> npt.NDArray[np.object_]:\n        \"\"\"\n        actually format my specific types\n        \"\"\"\n        return libperiod.period_array_strftime(\n            self.asi8, self.dtype._dtype_code, na_rep, date_format\n        )\n\n    # ------------------------------------------------------------------\n\n    def astype(self, dtype, copy: bool = True):\n        # We handle Period[T] -> Period[U]\n        # Our parent handles everything else.\n        dtype = pandas_dtype(dtype)\n        if dtype == self._dtype:\n            if not copy:\n                return self\n            else:\n                return self.copy()\n        if isinstance(dtype, PeriodDtype):\n            return self.asfreq(dtype.freq)\n\n        if lib.is_np_dtype(dtype, \"M\") or isinstance(dtype, DatetimeTZDtype):\n            # GH#45038 match PeriodIndex behavior.\n            tz = getattr(dtype, \"tz\", None)\n            unit = dtl.dtype_to_unit(dtype)\n            return self.to_timestamp().tz_localize(tz).as_unit(unit)\n\n        return super().astype(dtype, copy=copy)\n\n    def searchsorted(\n        self,\n        value: NumpyValueArrayLike | ExtensionArray,\n        side: Literal[\"left\", \"right\"] = \"left\",\n        sorter: NumpySorter | None = None,\n    ) -> npt.NDArray[np.intp] | np.intp:\n        npvalue = self._validate_setitem_value(value).view(\"M8[ns]\")\n\n        # Cast to M8 to get datetime-like NaT placement,\n        #  similar to dtl._period_dispatch\n        m8arr = self._ndarray.view(\"M8[ns]\")\n        return m8arr.searchsorted(npvalue, side=side, sorter=sorter)\n\n    def _pad_or_backfill(\n        self,\n        *,\n        method: FillnaOptions,\n        limit: int | None = None,\n        limit_area: Literal[\"inside\", \"outside\"] | None = None,\n        copy: bool = True,\n    ) -> Self:\n        # view as dt64 so we get treated as timelike in core.missing,\n        #  similar to dtl._period_dispatch\n        dta = self.view(\"M8[ns]\")\n        result = dta._pad_or_backfill(\n            method=method, limit=limit, limit_area=limit_area, copy=copy\n        )\n        if copy:\n            return cast(\"Self\", result.view(self.dtype))\n        else:\n            return self\n\n    def fillna(\n        self, value=None, method=None, limit: int | None = None, copy: bool = True\n    ) -> Self:\n        if method is not None:\n            # view as dt64 so we get treated as timelike in core.missing,\n            #  similar to dtl._period_dispatch\n            dta = self.view(\"M8[ns]\")\n            result = dta.fillna(value=value, method=method, limit=limit, copy=copy)\n            # error: Incompatible return value type (got \"Union[ExtensionArray,\n            # ndarray[Any, Any]]\", expected \"PeriodArray\")\n            return result.view(self.dtype)  # type: ignore[return-value]\n        return super().fillna(value=value, method=method, limit=limit, copy=copy)\n\n    # ------------------------------------------------------------------\n    # Arithmetic Methods\n\n    def _addsub_int_array_or_scalar(\n        self, other: np.ndarray | int, op: Callable[[Any, Any], Any]\n    ) -> Self:\n        \"\"\"\n        Add or subtract array of integers.\n\n        Parameters\n        ----------\n        other : np.ndarray[int64] or int\n        op : {operator.add, operator.sub}\n\n        Returns\n        -------\n        result : PeriodArray\n        \"\"\"\n        assert op in [operator.add, operator.sub]\n        if op is operator.sub:\n            other = -other\n        res_values = add_overflowsafe(self.asi8, np.asarray(other, dtype=\"i8\"))\n        return type(self)(res_values, dtype=self.dtype)\n\n    def _add_offset(self, other: BaseOffset):\n        assert not isinstance(other, Tick)\n\n        self._require_matching_freq(other, base=True)\n        return self._addsub_int_array_or_scalar(other.n, operator.add)\n\n    # TODO: can we de-duplicate with Period._add_timedeltalike_scalar?\n    def _add_timedeltalike_scalar(self, other):\n        \"\"\"\n        Parameters\n        ----------\n        other : timedelta, Tick, np.timedelta64\n\n        Returns\n        -------\n        PeriodArray\n        \"\"\"\n        if not isinstance(self.freq, Tick):\n            # We cannot add timedelta-like to non-tick PeriodArray\n            raise raise_on_incompatible(self, other)\n\n        if isna(other):\n            # i.e. np.timedelta64(\"NaT\")\n            return super()._add_timedeltalike_scalar(other)\n\n        td = np.asarray(Timedelta(other).asm8)\n        return self._add_timedelta_arraylike(td)\n\n    def _add_timedelta_arraylike(\n        self, other: TimedeltaArray | npt.NDArray[np.timedelta64]\n    ) -> Self:\n        \"\"\"\n        Parameters\n        ----------\n        other : TimedeltaArray or ndarray[timedelta64]\n\n        Returns\n        -------\n        PeriodArray\n        \"\"\"\n        if not self.dtype._is_tick_like():\n            # We cannot add timedelta-like to non-tick PeriodArray\n            raise TypeError(\n                f\"Cannot add or subtract timedelta64[ns] dtype from {self.dtype}\"\n            )\n\n        dtype = np.dtype(f\"m8[{self.dtype._td64_unit}]\")\n\n        # Similar to _check_timedeltalike_freq_compat, but we raise with a\n        #  more specific exception message if necessary.\n        try:\n            delta = astype_overflowsafe(\n                np.asarray(other), dtype=dtype, copy=False, round_ok=False\n            )\n        except ValueError as err:\n            # e.g. if we have minutes freq and try to add 30s\n            # \"Cannot losslessly convert units\"\n            raise IncompatibleFrequency(\n                \"Cannot add/subtract timedelta-like from PeriodArray that is \"\n                \"not an integer multiple of the PeriodArray's freq.\"\n            ) from err\n\n        res_values = add_overflowsafe(self.asi8, np.asarray(delta.view(\"i8\")))\n        return type(self)(res_values, dtype=self.dtype)\n\n    def _check_timedeltalike_freq_compat(self, other):\n        \"\"\"\n        Arithmetic operations with timedelta-like scalars or array `other`\n        are only valid if `other` is an integer multiple of `self.freq`.\n        If the operation is valid, find that integer multiple.  Otherwise,\n        raise because the operation is invalid.\n\n        Parameters\n        ----------\n        other : timedelta, np.timedelta64, Tick,\n                ndarray[timedelta64], TimedeltaArray, TimedeltaIndex\n\n        Returns\n        -------\n        multiple : int or ndarray[int64]\n\n        Raises\n        ------\n        IncompatibleFrequency\n        \"\"\"\n        assert self.dtype._is_tick_like()  # checked by calling function\n\n        dtype = np.dtype(f\"m8[{self.dtype._td64_unit}]\")\n\n        if isinstance(other, (timedelta, np.timedelta64, Tick)):\n            td = np.asarray(Timedelta(other).asm8)\n        else:\n            td = np.asarray(other)\n\n        try:\n            delta = astype_overflowsafe(td, dtype=dtype, copy=False, round_ok=False)\n        except ValueError as err:\n            raise raise_on_incompatible(self, other) from err\n\n        delta = delta.view(\"i8\")\n        return lib.item_from_zerodim(delta)\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.period/PeriodArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/sparse/array.py", "fn_id": "", "content": "class SparseArray(OpsMixin, PandasObject, ExtensionArray):\n    \"\"\"\n    An ExtensionArray for storing sparse data.\n\n    Parameters\n    ----------\n    data : array-like or scalar\n        A dense array of values to store in the SparseArray. This may contain\n        `fill_value`.\n    sparse_index : SparseIndex, optional\n    fill_value : scalar, optional\n        Elements in data that are ``fill_value`` are not stored in the\n        SparseArray. For memory savings, this should be the most common value\n        in `data`. By default, `fill_value` depends on the dtype of `data`:\n\n        =========== ==========\n        data.dtype  na_value\n        =========== ==========\n        float       ``np.nan``\n        int         ``0``\n        bool        False\n        datetime64  ``pd.NaT``\n        timedelta64 ``pd.NaT``\n        =========== ==========\n\n        The fill value is potentially specified in three ways. In order of\n        precedence, these are\n\n        1. The `fill_value` argument\n        2. ``dtype.fill_value`` if `fill_value` is None and `dtype` is\n           a ``SparseDtype``\n        3. ``data.dtype.fill_value`` if `fill_value` is None and `dtype`\n           is not a ``SparseDtype`` and `data` is a ``SparseArray``.\n\n    kind : str\n        Can be 'integer' or 'block', default is 'integer'.\n        The type of storage for sparse locations.\n\n        * 'block': Stores a `block` and `block_length` for each\n          contiguous *span* of sparse values. This is best when\n          sparse data tends to be clumped together, with large\n          regions of ``fill-value`` values between sparse values.\n        * 'integer': uses an integer to store the location of\n          each sparse value.\n\n    dtype : np.dtype or SparseDtype, optional\n        The dtype to use for the SparseArray. For numpy dtypes, this\n        determines the dtype of ``self.sp_values``. For SparseDtype,\n        this determines ``self.sp_values`` and ``self.fill_value``.\n    copy : bool, default False\n        Whether to explicitly copy the incoming `data` array.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Examples\n    --------\n    >>> from pandas.arrays import SparseArray\n    >>> arr = SparseArray([0, 0, 1, 2])\n    >>> arr\n    [0, 0, 1, 2]\n    Fill: 0\n    IntIndex\n    Indices: array([2, 3], dtype=int32)\n    \"\"\"\n\n    _subtyp = \"sparse_array\"  # register ABCSparseArray\n    _hidden_attrs = PandasObject._hidden_attrs | frozenset([])\n    _sparse_index: SparseIndex\n    _sparse_values: np.ndarray\n    _dtype: SparseDtype\n\n    def __init__(\n        self,\n        data,\n        sparse_index=None,\n        fill_value=None,\n        kind: SparseIndexKind = \"integer\",\n        dtype: Dtype | None = None,\n        copy: bool = False,\n    ) -> None:\n        if fill_value is None and isinstance(dtype, SparseDtype):\n            fill_value = dtype.fill_value\n\n        if isinstance(data, type(self)):\n            # disable normal inference on dtype, sparse_index, & fill_value\n            if sparse_index is None:\n                sparse_index = data.sp_index\n            if fill_value is None:\n                fill_value = data.fill_value\n            if dtype is None:\n                dtype = data.dtype\n            # TODO: make kind=None, and use data.kind?\n            data = data.sp_values\n\n        # Handle use-provided dtype\n        if isinstance(dtype, str):\n            # Two options: dtype='int', regular numpy dtype\n            # or dtype='Sparse[int]', a sparse dtype\n            try:\n                dtype = SparseDtype.construct_from_string(dtype)\n            except TypeError:\n                dtype = pandas_dtype(dtype)\n\n        if isinstance(dtype, SparseDtype):\n            if fill_value is None:\n                fill_value = dtype.fill_value\n            dtype = dtype.subtype\n\n        if is_scalar(data):\n            warnings.warn(\n                f\"Constructing {type(self).__name__} with scalar data is deprecated \"\n                \"and will raise in a future version. Pass a sequence instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            if sparse_index is None:\n                npoints = 1\n            else:\n                npoints = sparse_index.length\n\n            data = construct_1d_arraylike_from_scalar(data, npoints, dtype=None)\n            dtype = data.dtype\n\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n        # TODO: disentangle the fill_value dtype inference from\n        # dtype inference\n        if data is None:\n            # TODO: What should the empty dtype be? Object or float?\n\n            # error: Argument \"dtype\" to \"array\" has incompatible type\n            # \"Union[ExtensionDtype, dtype[Any], None]\"; expected \"Union[dtype[Any],\n            # None, type, _SupportsDType, str, Union[Tuple[Any, int], Tuple[Any,\n            # Union[int, Sequence[int]]], List[Any], _DTypeDict, Tuple[Any, Any]]]\"\n            data = np.array([], dtype=dtype)  # type: ignore[arg-type]\n\n        try:\n            data = sanitize_array(data, index=None)\n        except ValueError:\n            # NumPy may raise a ValueError on data like [1, []]\n            # we retry with object dtype here.\n            if dtype is None:\n                dtype = np.dtype(object)\n                data = np.atleast_1d(np.asarray(data, dtype=dtype))\n            else:\n                raise\n\n        if copy:\n            # TODO: avoid double copy when dtype forces cast.\n            data = data.copy()\n\n        if fill_value is None:\n            fill_value_dtype = data.dtype if dtype is None else dtype\n            if fill_value_dtype is None:\n                fill_value = np.nan\n            else:\n                fill_value = na_value_for_dtype(fill_value_dtype)\n\n        if isinstance(data, type(self)) and sparse_index is None:\n            sparse_index = data._sparse_index\n            # error: Argument \"dtype\" to \"asarray\" has incompatible type\n            # \"Union[ExtensionDtype, dtype[Any], None]\"; expected \"None\"\n            sparse_values = np.asarray(\n                data.sp_values, dtype=dtype  # type: ignore[arg-type]\n            )\n        elif sparse_index is None:\n            data = extract_array(data, extract_numpy=True)\n            if not isinstance(data, np.ndarray):\n                # EA\n                if isinstance(data.dtype, DatetimeTZDtype):\n                    warnings.warn(\n                        f\"Creating SparseArray from {data.dtype} data \"\n                        \"loses timezone information. Cast to object before \"\n                        \"sparse to retain timezone information.\",\n                        UserWarning,\n                        stacklevel=find_stack_level(),\n                    )\n                    data = np.asarray(data, dtype=\"datetime64[ns]\")\n                    if fill_value is NaT:\n                        fill_value = np.datetime64(\"NaT\", \"ns\")\n                data = np.asarray(data)\n            sparse_values, sparse_index, fill_value = _make_sparse(\n                # error: Argument \"dtype\" to \"_make_sparse\" has incompatible type\n                # \"Union[ExtensionDtype, dtype[Any], None]\"; expected\n                # \"Optional[dtype[Any]]\"\n                data,\n                kind=kind,\n                fill_value=fill_value,\n                dtype=dtype,  # type: ignore[arg-type]\n            )\n        else:\n            # error: Argument \"dtype\" to \"asarray\" has incompatible type\n            # \"Union[ExtensionDtype, dtype[Any], None]\"; expected \"None\"\n            sparse_values = np.asarray(data, dtype=dtype)  # type: ignore[arg-type]\n            if len(sparse_values) != sparse_index.npoints:\n                raise AssertionError(\n                    f\"Non array-like type {type(sparse_values)} must \"\n                    \"have the same length as the index\"\n                )\n        self._sparse_index = sparse_index\n        self._sparse_values = sparse_values\n        self._dtype = SparseDtype(sparse_values.dtype, fill_value)\n\n    @classmethod\n    def _simple_new(\n        cls,\n        sparse_array: np.ndarray,\n        sparse_index: SparseIndex,\n        dtype: SparseDtype,\n    ) -> Self:\n        new = object.__new__(cls)\n        new._sparse_index = sparse_index\n        new._sparse_values = sparse_array\n        new._dtype = dtype\n        return new\n\n    @classmethod\n    def from_spmatrix(cls, data: spmatrix) -> Self:\n        \"\"\"\n        Create a SparseArray from a scipy.sparse matrix.\n\n        Parameters\n        ----------\n        data : scipy.sparse.sp_matrix\n            This should be a SciPy sparse matrix where the size\n            of the second dimension is 1. In other words, a\n            sparse matrix with a single column.\n\n        Returns\n        -------\n        SparseArray\n\n        Examples\n        --------\n        >>> import scipy.sparse\n        >>> mat = scipy.sparse.coo_matrix((4, 1))\n        >>> pd.arrays.SparseArray.from_spmatrix(mat)\n        [0.0, 0.0, 0.0, 0.0]\n        Fill: 0.0\n        IntIndex\n        Indices: array([], dtype=int32)\n        \"\"\"\n        length, ncol = data.shape\n\n        if ncol != 1:\n            raise ValueError(f\"'data' must have a single column, not '{ncol}'\")\n\n        # our sparse index classes require that the positions be strictly\n        # increasing. So we need to sort loc, and arr accordingly.\n        data = data.tocsc()\n        data.sort_indices()\n        arr = data.data\n        idx = data.indices\n\n        zero = np.array(0, dtype=arr.dtype).item()\n        dtype = SparseDtype(arr.dtype, zero)\n        index = IntIndex(length, idx)\n\n        return cls._simple_new(arr, index, dtype)\n\n    def __array__(\n        self, dtype: NpDtype | None = None, copy: bool | None = None\n    ) -> np.ndarray:\n        fill_value = self.fill_value\n\n        if self.sp_index.ngaps == 0:\n            # Compat for na dtype and int values.\n            return self.sp_values\n        if dtype is None:\n            # Can NumPy represent this type?\n            # If not, `np.result_type` will raise. We catch that\n            # and return object.\n            if self.sp_values.dtype.kind == \"M\":\n                # However, we *do* special-case the common case of\n                # a datetime64 with pandas NaT.\n                if fill_value is NaT:\n                    # Can't put pd.NaT in a datetime64[ns]\n                    fill_value = np.datetime64(\"NaT\")\n            try:\n                dtype = np.result_type(self.sp_values.dtype, type(fill_value))\n            except TypeError:\n                dtype = object\n\n        out = np.full(self.shape, fill_value, dtype=dtype)\n        out[self.sp_index.indices] = self.sp_values\n        return out\n\n    def __setitem__(self, key, value) -> None:\n        # I suppose we could allow setting of non-fill_value elements.\n        # TODO(SparseArray.__setitem__): remove special cases in\n        # ExtensionBlock.where\n        msg = \"SparseArray does not support item assignment via setitem\"\n        raise TypeError(msg)\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype: Dtype | None = None, copy: bool = False):\n        return cls(scalars, dtype=dtype)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return cls(values, dtype=original.dtype)\n\n    # ------------------------------------------------------------------------\n    # Data\n    # ------------------------------------------------------------------------\n    @property\n    def sp_index(self) -> SparseIndex:\n        \"\"\"\n        The SparseIndex containing the location of non- ``fill_value`` points.\n        \"\"\"\n        return self._sparse_index\n\n    @property\n    def sp_values(self) -> np.ndarray:\n        \"\"\"\n        An ndarray containing the non- ``fill_value`` values.\n\n        Examples\n        --------\n        >>> from pandas.arrays import SparseArray\n        >>> s = SparseArray([0, 0, 1, 0, 2], fill_value=0)\n        >>> s.sp_values\n        array([1, 2])\n        \"\"\"\n        return self._sparse_values\n\n    @property\n    def dtype(self) -> SparseDtype:\n        return self._dtype\n\n    @property\n    def fill_value(self):\n        \"\"\"\n        Elements in `data` that are `fill_value` are not stored.\n\n        For memory savings, this should be the most common value in the array.\n\n        Examples\n        --------\n        >>> ser = pd.Series([0, 0, 2, 2, 2], dtype=\"Sparse[int]\")\n        >>> ser.sparse.fill_value\n        0\n        >>> spa_dtype = pd.SparseDtype(dtype=np.int32, fill_value=2)\n        >>> ser = pd.Series([0, 0, 2, 2, 2], dtype=spa_dtype)\n        >>> ser.sparse.fill_value\n        2\n        \"\"\"\n        return self.dtype.fill_value\n\n    @fill_value.setter\n    def fill_value(self, value) -> None:\n        self._dtype = SparseDtype(self.dtype.subtype, value)\n\n    @property\n    def kind(self) -> SparseIndexKind:\n        \"\"\"\n        The kind of sparse index for this array. One of {'integer', 'block'}.\n        \"\"\"\n        if isinstance(self.sp_index, IntIndex):\n            return \"integer\"\n        else:\n            return \"block\"\n\n    @property\n    def _valid_sp_values(self) -> np.ndarray:\n        sp_vals = self.sp_values\n        mask = notna(sp_vals)\n        return sp_vals[mask]\n\n    def __len__(self) -> int:\n        return self.sp_index.length\n\n    @property\n    def _null_fill_value(self) -> bool:\n        return self._dtype._is_na_fill_value\n\n    def _fill_value_matches(self, fill_value) -> bool:\n        if self._null_fill_value:\n            return isna(fill_value)\n        else:\n            return self.fill_value == fill_value\n\n    @property\n    def nbytes(self) -> int:\n        return self.sp_values.nbytes + self.sp_index.nbytes\n\n    @property\n    def density(self) -> float:\n        \"\"\"\n        The percent of non- ``fill_value`` points, as decimal.\n\n        Examples\n        --------\n        >>> from pandas.arrays import SparseArray\n        >>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\n        >>> s.density\n        0.6\n        \"\"\"\n        return self.sp_index.npoints / self.sp_index.length\n\n    @property\n    def npoints(self) -> int:\n        \"\"\"\n        The number of non- ``fill_value`` points.\n\n        Examples\n        --------\n        >>> from pandas.arrays import SparseArray\n        >>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\n        >>> s.npoints\n        3\n        \"\"\"\n        return self.sp_index.npoints\n\n    # error: Return type \"SparseArray\" of \"isna\" incompatible with return type\n    # \"ndarray[Any, Any] | ExtensionArraySupportsAnyAll\" in supertype \"ExtensionArray\"\n    def isna(self) -> Self:  # type: ignore[override]\n        # If null fill value, we want SparseDtype[bool, true]\n        # to preserve the same memory usage.\n        dtype = SparseDtype(bool, self._null_fill_value)\n        if self._null_fill_value:\n            return type(self)._simple_new(isna(self.sp_values), self.sp_index, dtype)\n        mask = np.full(len(self), False, dtype=np.bool_)\n        mask[self.sp_index.indices] = isna(self.sp_values)\n        return type(self)(mask, fill_value=False, dtype=dtype)\n\n    def _pad_or_backfill(  # pylint: disable=useless-parent-delegation\n        self,\n        *,\n        method: FillnaOptions,\n        limit: int | None = None,\n        limit_area: Literal[\"inside\", \"outside\"] | None = None,\n        copy: bool = True,\n    ) -> Self:\n        # TODO(3.0): We can remove this method once deprecation for fillna method\n        #  keyword is enforced.\n        return super()._pad_or_backfill(\n            method=method, limit=limit, limit_area=limit_area, copy=copy\n        )\n\n    def fillna(\n        self,\n        value=None,\n        method: FillnaOptions | None = None,\n        limit: int | None = None,\n        copy: bool = True,\n    ) -> Self:\n        \"\"\"\n        Fill missing values with `value`.\n\n        Parameters\n        ----------\n        value : scalar, optional\n        method : str, optional\n\n            .. warning::\n\n               Using 'method' will result in high memory use,\n               as all `fill_value` methods will be converted to\n               an in-memory ndarray\n\n        limit : int, optional\n\n        copy: bool, default True\n            Ignored for SparseArray.\n\n        Returns\n        -------\n        SparseArray\n\n        Notes\n        -----\n        When `value` is specified, the result's ``fill_value`` depends on\n        ``self.fill_value``. The goal is to maintain low-memory use.\n\n        If ``self.fill_value`` is NA, the result dtype will be\n        ``SparseDtype(self.dtype, fill_value=value)``. This will preserve\n        amount of memory used before and after filling.\n\n        When ``self.fill_value`` is not NA, the result dtype will be\n        ``self.dtype``. Again, this preserves the amount of memory used.\n        \"\"\"\n        if (method is None and value is None) or (\n            method is not None and value is not None\n        ):\n            raise ValueError(\"Must specify one of 'method' or 'value'.\")\n\n        if method is not None:\n            return super().fillna(method=method, limit=limit)\n\n        else:\n            new_values = np.where(isna(self.sp_values), value, self.sp_values)\n\n            if self._null_fill_value:\n                # This is essentially just updating the dtype.\n                new_dtype = SparseDtype(self.dtype.subtype, fill_value=value)\n            else:\n                new_dtype = self.dtype\n\n        return self._simple_new(new_values, self._sparse_index, new_dtype)\n\n    def shift(self, periods: int = 1, fill_value=None) -> Self:\n        if not len(self) or periods == 0:\n            return self.copy()\n\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n\n        subtype = np.result_type(fill_value, self.dtype.subtype)\n\n        if subtype != self.dtype.subtype:\n            # just coerce up front\n            arr = self.astype(SparseDtype(subtype, self.fill_value))\n        else:\n            arr = self\n\n        empty = self._from_sequence(\n            [fill_value] * min(abs(periods), len(self)), dtype=arr.dtype\n        )\n\n        if periods > 0:\n            a = empty\n            b = arr[:-periods]\n        else:\n            a = arr[abs(periods) :]\n            b = empty\n        return arr._concat_same_type([a, b])\n\n    def _first_fill_value_loc(self):\n        \"\"\"\n        Get the location of the first fill value.\n\n        Returns\n        -------\n        int\n        \"\"\"\n        if len(self) == 0 or self.sp_index.npoints == len(self):\n            return -1\n\n        indices = self.sp_index.indices\n        if not len(indices) or indices[0] > 0:\n            return 0\n\n        # a number larger than 1 should be appended to\n        # the last in case of fill value only appears\n        # in the tail of array\n        diff = np.r_[np.diff(indices), 2]\n        return indices[(diff > 1).argmax()] + 1\n\n    @doc(ExtensionArray.duplicated)\n    def duplicated(\n        self, keep: Literal[\"first\", \"last\", False] = \"first\"\n    ) -> npt.NDArray[np.bool_]:\n        values = np.asarray(self)\n        mask = np.asarray(self.isna())\n        return algos.duplicated(values, keep=keep, mask=mask)\n\n    def unique(self) -> Self:\n        uniques = algos.unique(self.sp_values)\n        if len(self.sp_values) != len(self):\n            fill_loc = self._first_fill_value_loc()\n            # Inorder to align the behavior of pd.unique or\n            # pd.Series.unique, we should keep the original\n            # order, here we use unique again to find the\n            # insertion place. Since the length of sp_values\n            # is not large, maybe minor performance hurt\n            # is worthwhile to the correctness.\n            insert_loc = len(algos.unique(self.sp_values[:fill_loc]))\n            uniques = np.insert(uniques, insert_loc, self.fill_value)\n        return type(self)._from_sequence(uniques, dtype=self.dtype)\n\n    def _values_for_factorize(self):\n        # Still override this for hash_pandas_object\n        return np.asarray(self), self.fill_value\n\n    def factorize(\n        self,\n        use_na_sentinel: bool = True,\n    ) -> tuple[np.ndarray, SparseArray]:\n        # Currently, ExtensionArray.factorize -> Tuple[ndarray, EA]\n        # The sparsity on this is backwards from what Sparse would want. Want\n        # ExtensionArray.factorize -> Tuple[EA, EA]\n        # Given that we have to return a dense array of codes, why bother\n        # implementing an efficient factorize?\n        codes, uniques = algos.factorize(\n            np.asarray(self), use_na_sentinel=use_na_sentinel\n        )\n        uniques_sp = SparseArray(uniques, dtype=self.dtype)\n        return codes, uniques_sp\n\n    def value_counts(self, dropna: bool = True) -> Series:\n        \"\"\"\n        Returns a Series containing counts of unique values.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include counts of NaN, even if NaN is in sp_values.\n\n        Returns\n        -------\n        counts : Series\n        \"\"\"\n        from pandas import (\n            Index,\n            Series,\n        )\n\n        keys, counts, _ = algos.value_counts_arraylike(self.sp_values, dropna=dropna)\n        fcounts = self.sp_index.ngaps\n        if fcounts > 0 and (not self._null_fill_value or not dropna):\n            mask = isna(keys) if self._null_fill_value else keys == self.fill_value\n            if mask.any():\n                counts[mask] += fcounts\n            else:\n                # error: Argument 1 to \"insert\" has incompatible type \"Union[\n                # ExtensionArray,ndarray[Any, Any]]\"; expected \"Union[\n                # _SupportsArray[dtype[Any]], Sequence[_SupportsArray[dtype\n                # [Any]]], Sequence[Sequence[_SupportsArray[dtype[Any]]]],\n                # Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]], Sequence\n                # [Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]]]]\"\n                keys = np.insert(keys, 0, self.fill_value)  # type: ignore[arg-type]\n                counts = np.insert(counts, 0, fcounts)\n\n        if not isinstance(keys, ABCIndex):\n            index = Index(keys)\n        else:\n            index = keys\n        return Series(counts, index=index, copy=False)\n\n    # --------\n    # Indexing\n    # --------\n    @overload\n    def __getitem__(self, key: ScalarIndexer) -> Any:\n        ...\n\n    @overload\n    def __getitem__(\n        self,\n        key: SequenceIndexer | tuple[int | ellipsis, ...],\n    ) -> Self:\n        ...\n\n    def __getitem__(\n        self,\n        key: PositionalIndexer | tuple[int | ellipsis, ...],\n    ) -> Self | Any:\n        if isinstance(key, tuple):\n            key = unpack_tuple_and_ellipses(key)\n            if key is Ellipsis:\n                raise ValueError(\"Cannot slice with Ellipsis\")\n\n        if is_integer(key):\n            return self._get_val_at(key)\n        elif isinstance(key, tuple):\n            # error: Invalid index type \"Tuple[Union[int, ellipsis], ...]\"\n            # for \"ndarray[Any, Any]\"; expected type\n            # \"Union[SupportsIndex, _SupportsArray[dtype[Union[bool_,\n            # integer[Any]]]], _NestedSequence[_SupportsArray[dtype[\n            # Union[bool_, integer[Any]]]]], _NestedSequence[Union[\n            # bool, int]], Tuple[Union[SupportsIndex, _SupportsArray[\n            # dtype[Union[bool_, integer[Any]]]], _NestedSequence[\n            # _SupportsArray[dtype[Union[bool_, integer[Any]]]]],\n            # _NestedSequence[Union[bool, int]]], ...]]\"\n            data_slice = self.to_dense()[key]  # type: ignore[index]\n        elif isinstance(key, slice):\n            # Avoid densifying when handling contiguous slices\n            if key.step is None or key.step == 1:\n                start = 0 if key.start is None else key.start\n                if start < 0:\n                    start += len(self)\n\n                end = len(self) if key.stop is None else key.stop\n                if end < 0:\n                    end += len(self)\n\n                indices = self.sp_index.indices\n                keep_inds = np.flatnonzero((indices >= start) & (indices < end))\n                sp_vals = self.sp_values[keep_inds]\n\n                sp_index = indices[keep_inds].copy()\n\n                # If we've sliced to not include the start of the array, all our indices\n                # should be shifted. NB: here we are careful to also not shift by a\n                # negative value for a case like [0, 1][-100:] where the start index\n                # should be treated like 0\n                if start > 0:\n                    sp_index -= start\n\n                # Length of our result should match applying this slice to a range\n                # of the length of our original array\n                new_len = len(range(len(self))[key])\n                new_sp_index = make_sparse_index(new_len, sp_index, self.kind)\n                return type(self)._simple_new(sp_vals, new_sp_index, self.dtype)\n            else:\n                indices = np.arange(len(self), dtype=np.int32)[key]\n                return self.take(indices)\n\n        elif not is_list_like(key):\n            # e.g. \"foo\" or 2.5\n            # exception message copied from numpy\n            raise IndexError(\n                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\n                r\"(`None`) and integer or boolean arrays are valid indices\"\n            )\n\n        else:\n            if isinstance(key, SparseArray):\n                # NOTE: If we guarantee that SparseDType(bool)\n                # has only fill_value - true, false or nan\n                # (see GH PR 44955)\n                # we can apply mask very fast:\n                if is_bool_dtype(key):\n                    if isna(key.fill_value):\n                        return self.take(key.sp_index.indices[key.sp_values])\n                    if not key.fill_value:\n                        return self.take(key.sp_index.indices)\n                    n = len(self)\n                    mask = np.full(n, True, dtype=np.bool_)\n                    mask[key.sp_index.indices] = False\n                    return self.take(np.arange(n)[mask])\n                else:\n                    key = np.asarray(key)\n\n            key = check_array_indexer(self, key)\n\n            if com.is_bool_indexer(key):\n                # mypy doesn't know we have an array here\n                key = cast(np.ndarray, key)\n                return self.take(np.arange(len(key), dtype=np.int32)[key])\n            elif hasattr(key, \"__len__\"):\n                return self.take(key)\n            else:\n                raise ValueError(f\"Cannot slice with '{key}'\")\n\n        return type(self)(data_slice, kind=self.kind)\n\n    def _get_val_at(self, loc):\n        loc = validate_insert_loc(loc, len(self))\n\n        sp_loc = self.sp_index.lookup(loc)\n        if sp_loc == -1:\n            return self.fill_value\n        else:\n            val = self.sp_values[sp_loc]\n            val = maybe_box_datetimelike(val, self.sp_values.dtype)\n            return val\n\n    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\n        if is_scalar(indices):\n            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\n        indices = np.asarray(indices, dtype=np.int32)\n\n        dtype = None\n        if indices.size == 0:\n            result = np.array([], dtype=\"object\")\n            dtype = self.dtype\n        elif allow_fill:\n            result = self._take_with_fill(indices, fill_value=fill_value)\n        else:\n            return self._take_without_fill(indices)\n\n        return type(self)(\n            result, fill_value=self.fill_value, kind=self.kind, dtype=dtype\n        )\n\n    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\n        if fill_value is None:\n            fill_value = self.dtype.na_value\n\n        if indices.min() < -1:\n            raise ValueError(\n                \"Invalid value in 'indices'. Must be between -1 \"\n                \"and the length of the array.\"\n            )\n\n        if indices.max() >= len(self):\n            raise IndexError(\"out of bounds value in 'indices'.\")\n\n        if len(self) == 0:\n            # Empty... Allow taking only if all empty\n            if (indices == -1).all():\n                dtype = np.result_type(self.sp_values, type(fill_value))\n                taken = np.empty_like(indices, dtype=dtype)\n                taken.fill(fill_value)\n                return taken\n            else:\n                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\n\n        # sp_indexer may be -1 for two reasons\n        # 1.) we took for an index of -1 (new)\n        # 2.) we took a value that was self.fill_value (old)\n        sp_indexer = self.sp_index.lookup_array(indices)\n        new_fill_indices = indices == -1\n        old_fill_indices = (sp_indexer == -1) & ~new_fill_indices\n\n        if self.sp_index.npoints == 0 and old_fill_indices.all():\n            # We've looked up all valid points on an all-sparse array.\n            taken = np.full(\n                sp_indexer.shape, fill_value=self.fill_value, dtype=self.dtype.subtype\n            )\n\n        elif self.sp_index.npoints == 0:\n            # Use the old fill_value unless we took for an index of -1\n            _dtype = np.result_type(self.dtype.subtype, type(fill_value))\n            taken = np.full(sp_indexer.shape, fill_value=fill_value, dtype=_dtype)\n            taken[old_fill_indices] = self.fill_value\n        else:\n            taken = self.sp_values.take(sp_indexer)\n\n            # Fill in two steps.\n            # Old fill values\n            # New fill values\n            # potentially coercing to a new dtype at each stage.\n\n            m0 = sp_indexer[old_fill_indices] < 0\n            m1 = sp_indexer[new_fill_indices] < 0\n\n            result_type = taken.dtype\n\n            if m0.any():\n                result_type = np.result_type(result_type, type(self.fill_value))\n                taken = taken.astype(result_type)\n                taken[old_fill_indices] = self.fill_value\n\n            if m1.any():\n                result_type = np.result_type(result_type, type(fill_value))\n                taken = taken.astype(result_type)\n                taken[new_fill_indices] = fill_value\n\n        return taken\n\n    def _take_without_fill(self, indices) -> Self:\n        to_shift = indices < 0\n\n        n = len(self)\n\n        if (indices.max() >= n) or (indices.min() < -n):\n            if n == 0:\n                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\n            raise IndexError(\"out of bounds value in 'indices'.\")\n\n        if to_shift.any():\n            indices = indices.copy()\n            indices[to_shift] += n\n\n        sp_indexer = self.sp_index.lookup_array(indices)\n        value_mask = sp_indexer != -1\n        new_sp_values = self.sp_values[sp_indexer[value_mask]]\n\n        value_indices = np.flatnonzero(value_mask).astype(np.int32, copy=False)\n\n        new_sp_index = make_sparse_index(len(indices), value_indices, kind=self.kind)\n        return type(self)._simple_new(new_sp_values, new_sp_index, dtype=self.dtype)\n\n    def searchsorted(\n        self,\n        v: ArrayLike | object,\n        side: Literal[\"left\", \"right\"] = \"left\",\n        sorter: NumpySorter | None = None,\n    ) -> npt.NDArray[np.intp] | np.intp:\n        msg = \"searchsorted requires high memory usage.\"\n        warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())\n        v = np.asarray(v)\n        return np.asarray(self, dtype=self.dtype.subtype).searchsorted(v, side, sorter)\n\n    def copy(self) -> Self:\n        values = self.sp_values.copy()\n        return self._simple_new(values, self.sp_index, self.dtype)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat: Sequence[Self]) -> Self:\n        fill_value = to_concat[0].fill_value\n\n        values = []\n        length = 0\n\n        if to_concat:\n            sp_kind = to_concat[0].kind\n        else:\n            sp_kind = \"integer\"\n\n        sp_index: SparseIndex\n        if sp_kind == \"integer\":\n            indices = []\n\n            for arr in to_concat:\n                int_idx = arr.sp_index.indices.copy()\n                int_idx += length  # TODO: wraparound\n                length += arr.sp_index.length\n\n                values.append(arr.sp_values)\n                indices.append(int_idx)\n\n            data = np.concatenate(values)\n            indices_arr = np.concatenate(indices)\n            # error: Argument 2 to \"IntIndex\" has incompatible type\n            # \"ndarray[Any, dtype[signedinteger[_32Bit]]]\";\n            # expected \"Sequence[int]\"\n            sp_index = IntIndex(length, indices_arr)  # type: ignore[arg-type]\n\n        else:\n            # when concatenating block indices, we don't claim that you'll\n            # get an identical index as concatenating the values and then\n            # creating a new index. We don't want to spend the time trying\n            # to merge blocks across arrays in `to_concat`, so the resulting\n            # BlockIndex may have more blocks.\n            blengths = []\n            blocs = []\n\n            for arr in to_concat:\n                block_idx = arr.sp_index.to_block_index()\n\n                values.append(arr.sp_values)\n                blocs.append(block_idx.blocs.copy() + length)\n                blengths.append(block_idx.blengths)\n                length += arr.sp_index.length\n\n            data = np.concatenate(values)\n            blocs_arr = np.concatenate(blocs)\n            blengths_arr = np.concatenate(blengths)\n\n            sp_index = BlockIndex(length, blocs_arr, blengths_arr)\n\n        return cls(data, sparse_index=sp_index, fill_value=fill_value)\n\n    def astype(self, dtype: AstypeArg | None = None, copy: bool = True):\n        \"\"\"\n        Change the dtype of a SparseArray.\n\n        The output will always be a SparseArray. To convert to a dense\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : np.dtype or ExtensionDtype\n            For SparseDtype, this changes the dtype of\n            ``self.sp_values`` and the ``self.fill_value``.\n\n            For other dtypes, this only changes the dtype of\n            ``self.sp_values``.\n\n        copy : bool, default True\n            Whether to ensure a copy is made, even if not necessary.\n\n        Returns\n        -------\n        SparseArray\n\n        Examples\n        --------\n        >>> arr = pd.arrays.SparseArray([0, 0, 1, 2])\n        >>> arr\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        >>> arr.astype(SparseDtype(np.dtype('int32')))\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\n        just ``self.sp_values``.\n\n        >>> arr.astype(SparseDtype(np.dtype('float64')))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [nan, nan, 1.0, 2.0]\n        Fill: nan\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Using a SparseDtype, you can also change the fill value as well.\n\n        >>> arr.astype(SparseDtype(\"float64\", fill_value=0.0))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [0.0, 0.0, 1.0, 2.0]\n        Fill: 0.0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n        \"\"\"\n        if dtype == self._dtype:\n            if not copy:\n                return self\n            else:\n                return self.copy()\n\n        future_dtype = pandas_dtype(dtype)\n        if not isinstance(future_dtype, SparseDtype):\n            # GH#34457\n            values = np.asarray(self)\n            values = ensure_wrapped_if_datetimelike(values)\n            return astype_array(values, dtype=future_dtype, copy=False)\n\n        dtype = self.dtype.update_dtype(dtype)\n        subtype = pandas_dtype(dtype._subtype_with_str)\n        subtype = cast(np.dtype, subtype)  # ensured by update_dtype\n        values = ensure_wrapped_if_datetimelike(self.sp_values)\n        sp_values = astype_array(values, subtype, copy=copy)\n        sp_values = np.asarray(sp_values)\n\n        return self._simple_new(sp_values, self.sp_index, dtype)\n\n    def map(self, mapper, na_action=None) -> Self:\n        \"\"\"\n        Map categories using an input mapping or function.\n\n        Parameters\n        ----------\n        mapper : dict, Series, callable\n            The correspondence from old values to new.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        SparseArray\n            The output array will have the same density as the input.\n            The output fill value will be the result of applying the\n            mapping to ``self.fill_value``\n\n        Examples\n        --------\n        >>> arr = pd.arrays.SparseArray([0, 1, 2])\n        >>> arr.map(lambda x: x + 10)\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)\n\n        >>> arr.map({0: 10, 1: 11, 2: 12})\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)\n\n        >>> arr.map(pd.Series([10, 11, 12], index=[0, 1, 2]))\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)\n        \"\"\"\n        is_map = isinstance(mapper, (abc.Mapping, ABCSeries))\n\n        fill_val = self.fill_value\n\n        if na_action is None or notna(fill_val):\n            fill_val = mapper.get(fill_val, fill_val) if is_map else mapper(fill_val)\n\n        def func(sp_val):\n            new_sp_val = mapper.get(sp_val, None) if is_map else mapper(sp_val)\n            # check identity and equality because nans are not equal to each other\n            if new_sp_val is fill_val or new_sp_val == fill_val:\n                msg = \"fill value in the sparse values not supported\"\n                raise ValueError(msg)\n            return new_sp_val\n\n        sp_values = [func(x) for x in self.sp_values]\n\n        return type(self)(sp_values, sparse_index=self.sp_index, fill_value=fill_val)\n\n    def to_dense(self) -> np.ndarray:\n        \"\"\"\n        Convert SparseArray to a NumPy array.\n\n        Returns\n        -------\n        arr : NumPy array\n        \"\"\"\n        return np.asarray(self, dtype=self.sp_values.dtype)\n\n    def _where(self, mask, value):\n        # NB: may not preserve dtype, e.g. result may be Sparse[float64]\n        #  while self is Sparse[int64]\n        naive_implementation = np.where(mask, self, value)\n        dtype = SparseDtype(naive_implementation.dtype, fill_value=self.fill_value)\n        result = type(self)._from_sequence(naive_implementation, dtype=dtype)\n        return result\n\n    # ------------------------------------------------------------------------\n    # IO\n    # ------------------------------------------------------------------------\n    def __setstate__(self, state) -> None:\n        \"\"\"Necessary for making this object picklable\"\"\"\n        if isinstance(state, tuple):\n            # Compat for pandas < 0.24.0\n            nd_state, (fill_value, sp_index) = state\n            sparse_values = np.array([])\n            sparse_values.__setstate__(nd_state)\n\n            self._sparse_values = sparse_values\n            self._sparse_index = sp_index\n            self._dtype = SparseDtype(sparse_values.dtype, fill_value)\n        else:\n            self.__dict__.update(state)\n\n    def nonzero(self) -> tuple[npt.NDArray[np.int32]]:\n        if self.fill_value == 0:\n            return (self.sp_index.indices,)\n        else:\n            return (self.sp_index.indices[self.sp_values != 0],)\n\n    # ------------------------------------------------------------------------\n    # Reductions\n    # ------------------------------------------------------------------------\n\n    def _reduce(\n        self, name: str, *, skipna: bool = True, keepdims: bool = False, **kwargs\n    ):\n        method = getattr(self, name, None)\n\n        if method is None:\n            raise TypeError(f\"cannot perform {name} with type {self.dtype}\")\n\n        if skipna:\n            arr = self\n        else:\n            arr = self.dropna()\n\n        result = getattr(arr, name)(**kwargs)\n\n        if keepdims:\n            return type(self)([result], dtype=self.dtype)\n        else:\n            return result\n\n    def all(self, axis=None, *args, **kwargs):\n        \"\"\"\n        Tests whether all elements evaluate True\n\n        Returns\n        -------\n        all : bool\n\n        See Also\n        --------\n        numpy.all\n        \"\"\"\n        nv.validate_all(args, kwargs)\n\n        values = self.sp_values\n\n        if len(values) != len(self) and not np.all(self.fill_value):\n            return False\n\n        return values.all()\n\n    def any(self, axis: AxisInt = 0, *args, **kwargs) -> bool:\n        \"\"\"\n        Tests whether at least one of elements evaluate True\n\n        Returns\n        -------\n        any : bool\n\n        See Also\n        --------\n        numpy.any\n        \"\"\"\n        nv.validate_any(args, kwargs)\n\n        values = self.sp_values\n\n        if len(values) != len(self) and np.any(self.fill_value):\n            return True\n\n        return values.any().item()\n\n    def sum(\n        self,\n        axis: AxisInt = 0,\n        min_count: int = 0,\n        skipna: bool = True,\n        *args,\n        **kwargs,\n    ) -> Scalar:\n        \"\"\"\n        Sum of non-NA/null values\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Not Used. NumPy compatibility.\n        min_count : int, default 0\n            The required number of valid values to perform the summation. If fewer\n            than ``min_count`` valid values are present, the result will be the missing\n            value indicator for subarray type.\n        *args, **kwargs\n            Not Used. NumPy compatibility.\n\n        Returns\n        -------\n        scalar\n        \"\"\"\n        nv.validate_sum(args, kwargs)\n        valid_vals = self._valid_sp_values\n        sp_sum = valid_vals.sum()\n        has_na = self.sp_index.ngaps > 0 and not self._null_fill_value\n\n        if has_na and not skipna:\n            return na_value_for_dtype(self.dtype.subtype, compat=False)\n\n        if self._null_fill_value:\n            if check_below_min_count(valid_vals.shape, None, min_count):\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\n            return sp_sum\n        else:\n            nsparse = self.sp_index.ngaps\n            if check_below_min_count(valid_vals.shape, None, min_count - nsparse):\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\n            return sp_sum + self.fill_value * nsparse\n\n    def cumsum(self, axis: AxisInt = 0, *args, **kwargs) -> SparseArray:\n        \"\"\"\n        Cumulative sum of non-NA/null values.\n\n        When performing the cumulative summation, any non-NA/null values will\n        be skipped. The resulting SparseArray will preserve the locations of\n        NaN values, but the fill value will be `np.nan` regardless.\n\n        Parameters\n        ----------\n        axis : int or None\n            Axis over which to perform the cumulative summation. If None,\n            perform cumulative summation over flattened array.\n\n        Returns\n        -------\n        cumsum : SparseArray\n        \"\"\"\n        nv.validate_cumsum(args, kwargs)\n\n        if axis is not None and axis >= self.ndim:  # Mimic ndarray behaviour.\n            raise ValueError(f\"axis(={axis}) out of bounds\")\n\n        if not self._null_fill_value:\n            return SparseArray(self.to_dense()).cumsum()\n\n        return SparseArray(\n            self.sp_values.cumsum(),\n            sparse_index=self.sp_index,\n            fill_value=self.fill_value,\n        )\n\n    def mean(self, axis: Axis = 0, *args, **kwargs):\n        \"\"\"\n        Mean of non-NA/null values\n\n        Returns\n        -------\n        mean : float\n        \"\"\"\n        nv.validate_mean(args, kwargs)\n        valid_vals = self._valid_sp_values\n        sp_sum = valid_vals.sum()\n        ct = len(valid_vals)\n\n        if self._null_fill_value:\n            return sp_sum / ct\n        else:\n            nsparse = self.sp_index.ngaps\n            return (sp_sum + self.fill_value * nsparse) / (ct + nsparse)\n\n    def max(self, *, axis: AxisInt | None = None, skipna: bool = True):\n        \"\"\"\n        Max of array values, ignoring NA values if specified.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Not Used. NumPy compatibility.\n        skipna : bool, default True\n            Whether to ignore NA values.\n\n        Returns\n        -------\n        scalar\n        \"\"\"\n        nv.validate_minmax_axis(axis, self.ndim)\n        return self._min_max(\"max\", skipna=skipna)\n\n    def min(self, *, axis: AxisInt | None = None, skipna: bool = True):\n        \"\"\"\n        Min of array values, ignoring NA values if specified.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Not Used. NumPy compatibility.\n        skipna : bool, default True\n            Whether to ignore NA values.\n\n        Returns\n        -------\n        scalar\n        \"\"\"\n        nv.validate_minmax_axis(axis, self.ndim)\n        return self._min_max(\"min\", skipna=skipna)\n\n    def _min_max(self, kind: Literal[\"min\", \"max\"], skipna: bool) -> Scalar:\n        \"\"\"\n        Min/max of non-NA/null values\n\n        Parameters\n        ----------\n        kind : {\"min\", \"max\"}\n        skipna : bool\n\n        Returns\n        -------\n        scalar\n        \"\"\"\n        valid_vals = self._valid_sp_values\n        has_nonnull_fill_vals = not self._null_fill_value and self.sp_index.ngaps > 0\n\n        if len(valid_vals) > 0:\n            sp_min_max = getattr(valid_vals, kind)()\n\n            # If a non-null fill value is currently present, it might be the min/max\n            if has_nonnull_fill_vals:\n                func = max if kind == \"max\" else min\n                return func(sp_min_max, self.fill_value)\n            elif skipna:\n                return sp_min_max\n            elif self.sp_index.ngaps == 0:\n                # No NAs present\n                return sp_min_max\n            else:\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\n        elif has_nonnull_fill_vals:\n            return self.fill_value\n        else:\n            return na_value_for_dtype(self.dtype.subtype, compat=False)\n\n    def _argmin_argmax(self, kind: Literal[\"argmin\", \"argmax\"]) -> int:\n        values = self._sparse_values\n        index = self._sparse_index.indices\n        mask = np.asarray(isna(values))\n        func = np.argmax if kind == \"argmax\" else np.argmin\n\n        idx = np.arange(values.shape[0])\n        non_nans = values[~mask]\n        non_nan_idx = idx[~mask]\n\n        _candidate = non_nan_idx[func(non_nans)]\n        candidate = index[_candidate]\n\n        if isna(self.fill_value):\n            return candidate\n        if kind == \"argmin\" and self[candidate] < self.fill_value:\n            return candidate\n        if kind == \"argmax\" and self[candidate] > self.fill_value:\n            return candidate\n        _loc = self._first_fill_value_loc()\n        if _loc == -1:\n            # fill_value doesn't exist\n            return candidate\n        else:\n            return _loc\n\n    def argmax(self, skipna: bool = True) -> int:\n        validate_bool_kwarg(skipna, \"skipna\")\n        if not skipna and self._hasna:\n            raise NotImplementedError\n        return self._argmin_argmax(\"argmax\")\n\n    def argmin(self, skipna: bool = True) -> int:\n        validate_bool_kwarg(skipna, \"skipna\")\n        if not skipna and self._hasna:\n            raise NotImplementedError\n        return self._argmin_argmax(\"argmin\")\n\n    # ------------------------------------------------------------------------\n    # Ufuncs\n    # ------------------------------------------------------------------------\n\n    _HANDLED_TYPES = (np.ndarray, numbers.Number)\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n        out = kwargs.get(\"out\", ())\n\n        for x in inputs + out:\n            if not isinstance(x, self._HANDLED_TYPES + (SparseArray,)):\n                return NotImplemented\n\n        # for binary ops, use our custom dunder methods\n        result = arraylike.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        if \"out\" in kwargs:\n            # e.g. tests.arrays.sparse.test_arithmetics.test_ndarray_inplace\n            res = arraylike.dispatch_ufunc_with_out(\n                self, ufunc, method, *inputs, **kwargs\n            )\n            return res\n\n        if method == \"reduce\":\n            result = arraylike.dispatch_reduction_ufunc(\n                self, ufunc, method, *inputs, **kwargs\n            )\n            if result is not NotImplemented:\n                # e.g. tests.series.test_ufunc.TestNumpyReductions\n                return result\n\n        if len(inputs) == 1:\n            # No alignment necessary.\n            sp_values = getattr(ufunc, method)(self.sp_values, **kwargs)\n            fill_value = getattr(ufunc, method)(self.fill_value, **kwargs)\n\n            if ufunc.nout > 1:\n                # multiple outputs. e.g. modf\n                arrays = tuple(\n                    self._simple_new(\n                        sp_value, self.sp_index, SparseDtype(sp_value.dtype, fv)\n                    )\n                    for sp_value, fv in zip(sp_values, fill_value)\n                )\n                return arrays\n            elif method == \"reduce\":\n                # e.g. reductions\n                return sp_values\n\n            return self._simple_new(\n                sp_values, self.sp_index, SparseDtype(sp_values.dtype, fill_value)\n            )\n\n        new_inputs = tuple(np.asarray(x) for x in inputs)\n        result = getattr(ufunc, method)(*new_inputs, **kwargs)\n        if out:\n            if len(out) == 1:\n                out = out[0]\n            return out\n\n        if ufunc.nout > 1:\n            return tuple(type(self)(x) for x in result)\n        elif method == \"at\":\n            # no return value\n            return None\n        else:\n            return type(self)(result)\n\n    # ------------------------------------------------------------------------\n    # Ops\n    # ------------------------------------------------------------------------\n\n    def _arith_method(self, other, op):\n        op_name = op.__name__\n\n        if isinstance(other, SparseArray):\n            return _sparse_array_op(self, other, op, op_name)\n\n        elif is_scalar(other):\n            with np.errstate(all=\"ignore\"):\n                fill = op(_get_fill(self), np.asarray(other))\n                result = op(self.sp_values, other)\n\n            if op_name == \"divmod\":\n                left, right = result\n                lfill, rfill = fill\n                return (\n                    _wrap_result(op_name, left, self.sp_index, lfill),\n                    _wrap_result(op_name, right, self.sp_index, rfill),\n                )\n\n            return _wrap_result(op_name, result, self.sp_index, fill)\n\n        else:\n            other = np.asarray(other)\n            with np.errstate(all=\"ignore\"):\n                if len(self) != len(other):\n                    raise AssertionError(\n                        f\"length mismatch: {len(self)} vs. {len(other)}\"\n                    )\n                if not isinstance(other, SparseArray):\n                    dtype = getattr(other, \"dtype\", None)\n                    other = SparseArray(other, fill_value=self.fill_value, dtype=dtype)\n                return _sparse_array_op(self, other, op, op_name)\n\n    def _cmp_method(self, other, op) -> SparseArray:\n        if not is_scalar(other) and not isinstance(other, type(self)):\n            # convert list-like to ndarray\n            other = np.asarray(other)\n\n        if isinstance(other, np.ndarray):\n            # TODO: make this more flexible than just ndarray...\n            other = SparseArray(other, fill_value=self.fill_value)\n\n        if isinstance(other, SparseArray):\n            if len(self) != len(other):\n                raise ValueError(\n                    f\"operands have mismatched length {len(self)} and {len(other)}\"\n                )\n\n            op_name = op.__name__.strip(\"_\")\n            return _sparse_array_op(self, other, op, op_name)\n        else:\n            # scalar\n            fill_value = op(self.fill_value, other)\n            result = np.full(len(self), fill_value, dtype=np.bool_)\n            result[self.sp_index.indices] = op(self.sp_values, other)\n\n            return type(self)(\n                result,\n                fill_value=fill_value,\n                dtype=np.bool_,\n            )\n\n    _logical_method = _cmp_method\n\n    def _unary_method(self, op) -> SparseArray:\n        fill_value = op(np.array(self.fill_value)).item()\n        dtype = SparseDtype(self.dtype.subtype, fill_value)\n        # NOTE: if fill_value doesn't change\n        # we just have to apply op to sp_values\n        if isna(self.fill_value) or fill_value == self.fill_value:\n            values = op(self.sp_values)\n            return type(self)._simple_new(values, self.sp_index, self.dtype)\n        # In the other case we have to recalc indexes\n        return type(self)(op(self.to_dense()), dtype=dtype)\n\n    def __pos__(self) -> SparseArray:\n        return self._unary_method(operator.pos)\n\n    def __neg__(self) -> SparseArray:\n        return self._unary_method(operator.neg)\n\n    def __invert__(self) -> SparseArray:\n        return self._unary_method(operator.invert)\n\n    def __abs__(self) -> SparseArray:\n        return self._unary_method(operator.abs)\n\n    # ----------\n    # Formatting\n    # -----------\n    def __repr__(self) -> str:\n        pp_str = printing.pprint_thing(self)\n        pp_fill = printing.pprint_thing(self.fill_value)\n        pp_index = printing.pprint_thing(self.sp_index)\n        return f\"{pp_str}\\nFill: {pp_fill}\\n{pp_index}\"\n\n    def _formatter(self, boxed: bool = False):\n        # Defer to the formatter from the GenericArrayFormatter calling us.\n        # This will infer the correct formatter from the dtype of the values.\n        return None\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.sparse.array/SparseArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/string_arrow.py", "fn_id": "", "content": "class ArrowStringArray(ObjectStringArrayMixin, ArrowExtensionArray, BaseStringArray):\n    \"\"\"\n    Extension array for string data in a ``pyarrow.ChunkedArray``.\n\n    .. warning::\n\n       ArrowStringArray is considered experimental. The implementation and\n       parts of the API may change without warning.\n\n    Parameters\n    ----------\n    values : pyarrow.Array or pyarrow.ChunkedArray\n        The array of data.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    See Also\n    --------\n    :func:`pandas.array`\n        The recommended function for creating a ArrowStringArray.\n    Series.str\n        The string methods are available on Series backed by\n        a ArrowStringArray.\n\n    Notes\n    -----\n    ArrowStringArray returns a BooleanArray for comparison methods.\n\n    Examples\n    --------\n    >>> pd.array(['This is', 'some text', None, 'data.'], dtype=\"string[pyarrow]\")\n    <ArrowStringArray>\n    ['This is', 'some text', <NA>, 'data.']\n    Length: 4, dtype: string\n    \"\"\"\n\n    # error: Incompatible types in assignment (expression has type \"StringDtype\",\n    # base class \"ArrowExtensionArray\" defined the type as \"ArrowDtype\")\n    _dtype: StringDtype  # type: ignore[assignment]\n    _storage = \"pyarrow\"\n\n    def __init__(self, values) -> None:\n        _chk_pyarrow_available()\n        if isinstance(values, (pa.Array, pa.ChunkedArray)) and pa.types.is_string(\n            values.type\n        ):\n            values = pc.cast(values, pa.large_string())\n\n        super().__init__(values)\n        self._dtype = StringDtype(storage=self._storage)\n\n        if not pa.types.is_large_string(self._pa_array.type) and not (\n            pa.types.is_dictionary(self._pa_array.type)\n            and pa.types.is_large_string(self._pa_array.type.value_type)\n        ):\n            raise ValueError(\n                \"ArrowStringArray requires a PyArrow (chunked) array of \"\n                \"large_string type\"\n            )\n\n    @classmethod\n    def _box_pa_scalar(cls, value, pa_type: pa.DataType | None = None) -> pa.Scalar:\n        pa_scalar = super()._box_pa_scalar(value, pa_type)\n        if pa.types.is_string(pa_scalar.type) and pa_type is None:\n            pa_scalar = pc.cast(pa_scalar, pa.large_string())\n        return pa_scalar\n\n    @classmethod\n    def _box_pa_array(\n        cls, value, pa_type: pa.DataType | None = None, copy: bool = False\n    ) -> pa.Array | pa.ChunkedArray:\n        pa_array = super()._box_pa_array(value, pa_type)\n        if pa.types.is_string(pa_array.type) and pa_type is None:\n            pa_array = pc.cast(pa_array, pa.large_string())\n        return pa_array\n\n    def __len__(self) -> int:\n        \"\"\"\n        Length of this array.\n\n        Returns\n        -------\n        length : int\n        \"\"\"\n        return len(self._pa_array)\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype: Dtype | None = None, copy: bool = False):\n        from pandas.core.arrays.masked import BaseMaskedArray\n\n        _chk_pyarrow_available()\n\n        if dtype and not (isinstance(dtype, str) and dtype == \"string\"):\n            dtype = pandas_dtype(dtype)\n            assert isinstance(dtype, StringDtype) and dtype.storage in (\n                \"pyarrow\",\n                \"pyarrow_numpy\",\n            )\n\n        if isinstance(scalars, BaseMaskedArray):\n            # avoid costly conversion to object dtype in ensure_string_array and\n            # numerical issues with Float32Dtype\n            na_values = scalars._mask\n            result = scalars._data\n            result = lib.ensure_string_array(result, copy=copy, convert_na_value=False)\n            return cls(pa.array(result, mask=na_values, type=pa.large_string()))\n        elif isinstance(scalars, (pa.Array, pa.ChunkedArray)):\n            return cls(pc.cast(scalars, pa.large_string()))\n\n        # convert non-na-likes to str\n        result = lib.ensure_string_array(scalars, copy=copy)\n        return cls(pa.array(result, type=pa.large_string(), from_pandas=True))\n\n    @classmethod\n    def _from_sequence_of_strings(\n        cls, strings, dtype: Dtype | None = None, copy: bool = False\n    ):\n        return cls._from_sequence(strings, dtype=dtype, copy=copy)\n\n    @property\n    def dtype(self) -> StringDtype:  # type: ignore[override]\n        \"\"\"\n        An instance of 'string[pyarrow]'.\n        \"\"\"\n        return self._dtype\n\n    def insert(self, loc: int, item) -> ArrowStringArray:\n        if not isinstance(item, str) and item is not libmissing.NA:\n            raise TypeError(\"Scalar must be NA or str\")\n        return super().insert(loc, item)\n\n    @classmethod\n    def _result_converter(cls, values, na=None):\n        return BooleanDtype().__from_arrow__(values)\n\n    def _maybe_convert_setitem_value(self, value):\n        \"\"\"Maybe convert value to be pyarrow compatible.\"\"\"\n        if is_scalar(value):\n            if isna(value):\n                value = None\n            elif not isinstance(value, str):\n                raise TypeError(\"Scalar must be NA or str\")\n        else:\n            value = np.array(value, dtype=object, copy=True)\n            value[isna(value)] = None\n            for v in value:\n                if not (v is None or isinstance(v, str)):\n                    raise TypeError(\"Scalar must be NA or str\")\n        return super()._maybe_convert_setitem_value(value)\n\n    def isin(self, values: ArrayLike) -> npt.NDArray[np.bool_]:\n        value_set = [\n            pa_scalar.as_py()\n            for pa_scalar in [pa.scalar(value, from_pandas=True) for value in values]\n            if pa_scalar.type in (pa.string(), pa.null(), pa.large_string())\n        ]\n\n        # short-circuit to return all False array.\n        if not len(value_set):\n            return np.zeros(len(self), dtype=bool)\n\n        result = pc.is_in(\n            self._pa_array, value_set=pa.array(value_set, type=self._pa_array.type)\n        )\n        # pyarrow 2.0.0 returned nulls, so we explicily specify dtype to convert nulls\n        # to False\n        return np.array(result, dtype=np.bool_)\n\n    def astype(self, dtype, copy: bool = True):\n        dtype = pandas_dtype(dtype)\n\n        if dtype == self.dtype:\n            if copy:\n                return self.copy()\n            return self\n        elif isinstance(dtype, NumericDtype):\n            data = self._pa_array.cast(pa.from_numpy_dtype(dtype.numpy_dtype))\n            return dtype.__from_arrow__(data)\n        elif isinstance(dtype, np.dtype) and np.issubdtype(dtype, np.floating):\n            return self.to_numpy(dtype=dtype, na_value=np.nan)\n\n        return super().astype(dtype, copy=copy)\n\n    @property\n    def _data(self):\n        # dask accesses ._data directlys\n        warnings.warn(\n            f\"{type(self).__name__}._data is a deprecated and will be removed \"\n            \"in a future version, use ._pa_array instead\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return self._pa_array\n\n    # ------------------------------------------------------------------------\n    # String methods interface\n\n    # error: Incompatible types in assignment (expression has type \"NAType\",\n    # base class \"ObjectStringArrayMixin\" defined the type as \"float\")\n    _str_na_value = libmissing.NA  # type: ignore[assignment]\n\n    def _str_map(\n        self, f, na_value=None, dtype: Dtype | None = None, convert: bool = True\n    ):\n        # TODO: de-duplicate with StringArray method. This method is moreless copy and\n        # paste.\n\n        from pandas.arrays import (\n            BooleanArray,\n            IntegerArray,\n        )\n\n        if dtype is None:\n            dtype = self.dtype\n        if na_value is None:\n            na_value = self.dtype.na_value\n\n        mask = isna(self)\n        arr = np.asarray(self)\n\n        if is_integer_dtype(dtype) or is_bool_dtype(dtype):\n            constructor: type[IntegerArray | BooleanArray]\n            if is_integer_dtype(dtype):\n                constructor = IntegerArray\n            else:\n                constructor = BooleanArray\n\n            na_value_is_na = isna(na_value)\n            if na_value_is_na:\n                na_value = 1\n            result = lib.map_infer_mask(\n                arr,\n                f,\n                mask.view(\"uint8\"),\n                convert=False,\n                na_value=na_value,\n                # error: Argument 1 to \"dtype\" has incompatible type\n                # \"Union[ExtensionDtype, str, dtype[Any], Type[object]]\"; expected\n                # \"Type[object]\"\n                dtype=np.dtype(dtype),  # type: ignore[arg-type]\n            )\n\n            if not na_value_is_na:\n                mask[:] = False\n\n            return constructor(result, mask)\n\n        elif is_string_dtype(dtype) and not is_object_dtype(dtype):\n            # i.e. StringDtype\n            result = lib.map_infer_mask(\n                arr, f, mask.view(\"uint8\"), convert=False, na_value=na_value\n            )\n            result = pa.array(\n                result, mask=mask, type=pa.large_string(), from_pandas=True\n            )\n            return type(self)(result)\n        else:\n            # This is when the result type is object. We reach this when\n            # -> We know the result type is truly object (e.g. .encode returns bytes\n            #    or .findall returns a list).\n            # -> We don't know the result type. E.g. `.get` can return anything.\n            return lib.map_infer_mask(arr, f, mask.view(\"uint8\"))\n\n    def _str_contains(\n        self, pat, case: bool = True, flags: int = 0, na=np.nan, regex: bool = True\n    ):\n        if flags:\n            fallback_performancewarning()\n            return super()._str_contains(pat, case, flags, na, regex)\n\n        if regex:\n            result = pc.match_substring_regex(self._pa_array, pat, ignore_case=not case)\n        else:\n            result = pc.match_substring(self._pa_array, pat, ignore_case=not case)\n        result = self._result_converter(result, na=na)\n        if not isna(na):\n            result[isna(result)] = bool(na)\n        return result\n\n    def _str_startswith(self, pat: str | tuple[str, ...], na: Scalar | None = None):\n        if isinstance(pat, str):\n            result = pc.starts_with(self._pa_array, pattern=pat)\n        else:\n            if len(pat) == 0:\n                # mimic existing behaviour of string extension array\n                # and python string method\n                result = pa.array(\n                    np.zeros(len(self._pa_array), dtype=bool), mask=isna(self._pa_array)\n                )\n            else:\n                result = pc.starts_with(self._pa_array, pattern=pat[0])\n\n                for p in pat[1:]:\n                    result = pc.or_(result, pc.starts_with(self._pa_array, pattern=p))\n        if not isna(na):\n            result = result.fill_null(na)\n        return self._result_converter(result)\n\n    def _str_endswith(self, pat: str | tuple[str, ...], na: Scalar | None = None):\n        if isinstance(pat, str):\n            result = pc.ends_with(self._pa_array, pattern=pat)\n        else:\n            if len(pat) == 0:\n                # mimic existing behaviour of string extension array\n                # and python string method\n                result = pa.array(\n                    np.zeros(len(self._pa_array), dtype=bool), mask=isna(self._pa_array)\n                )\n            else:\n                result = pc.ends_with(self._pa_array, pattern=pat[0])\n\n                for p in pat[1:]:\n                    result = pc.or_(result, pc.ends_with(self._pa_array, pattern=p))\n        if not isna(na):\n            result = result.fill_null(na)\n        return self._result_converter(result)\n\n    def _str_replace(\n        self,\n        pat: str | re.Pattern,\n        repl: str | Callable,\n        n: int = -1,\n        case: bool = True,\n        flags: int = 0,\n        regex: bool = True,\n    ):\n        if isinstance(pat, re.Pattern) or callable(repl) or not case or flags:\n            fallback_performancewarning()\n            return super()._str_replace(pat, repl, n, case, flags, regex)\n\n        func = pc.replace_substring_regex if regex else pc.replace_substring\n        result = func(self._pa_array, pattern=pat, replacement=repl, max_replacements=n)\n        return type(self)(result)\n\n    def _str_repeat(self, repeats: int | Sequence[int]):\n        if not isinstance(repeats, int):\n            return super()._str_repeat(repeats)\n        else:\n            return type(self)(pc.binary_repeat(self._pa_array, repeats))\n\n    def _str_match(\n        self, pat: str, case: bool = True, flags: int = 0, na: Scalar | None = None\n    ):\n        if not pat.startswith(\"^\"):\n            pat = f\"^{pat}\"\n        return self._str_contains(pat, case, flags, na, regex=True)\n\n    def _str_fullmatch(\n        self, pat, case: bool = True, flags: int = 0, na: Scalar | None = None\n    ):\n        if not pat.endswith(\"$\") or pat.endswith(\"\\\\$\"):\n            pat = f\"{pat}$\"\n        return self._str_match(pat, case, flags, na)\n\n    def _str_slice(\n        self, start: int | None = None, stop: int | None = None, step: int | None = None\n    ):\n        if stop is None:\n            return super()._str_slice(start, stop, step)\n        if start is None:\n            start = 0\n        if step is None:\n            step = 1\n        return type(self)(\n            pc.utf8_slice_codeunits(self._pa_array, start=start, stop=stop, step=step)\n        )\n\n    def _str_isalnum(self):\n        result = pc.utf8_is_alnum(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_isalpha(self):\n        result = pc.utf8_is_alpha(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_isdecimal(self):\n        result = pc.utf8_is_decimal(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_isdigit(self):\n        result = pc.utf8_is_digit(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_islower(self):\n        result = pc.utf8_is_lower(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_isnumeric(self):\n        result = pc.utf8_is_numeric(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_isspace(self):\n        result = pc.utf8_is_space(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_istitle(self):\n        result = pc.utf8_is_title(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_isupper(self):\n        result = pc.utf8_is_upper(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_len(self):\n        result = pc.utf8_length(self._pa_array)\n        return self._convert_int_dtype(result)\n\n    def _str_lower(self):\n        return type(self)(pc.utf8_lower(self._pa_array))\n\n    def _str_upper(self):\n        return type(self)(pc.utf8_upper(self._pa_array))\n\n    def _str_strip(self, to_strip=None):\n        if to_strip is None:\n            result = pc.utf8_trim_whitespace(self._pa_array)\n        else:\n            result = pc.utf8_trim(self._pa_array, characters=to_strip)\n        return type(self)(result)\n\n    def _str_lstrip(self, to_strip=None):\n        if to_strip is None:\n            result = pc.utf8_ltrim_whitespace(self._pa_array)\n        else:\n            result = pc.utf8_ltrim(self._pa_array, characters=to_strip)\n        return type(self)(result)\n\n    def _str_rstrip(self, to_strip=None):\n        if to_strip is None:\n            result = pc.utf8_rtrim_whitespace(self._pa_array)\n        else:\n            result = pc.utf8_rtrim(self._pa_array, characters=to_strip)\n        return type(self)(result)\n\n    def _str_removeprefix(self, prefix: str):\n        if not pa_version_under13p0:\n            starts_with = pc.starts_with(self._pa_array, pattern=prefix)\n            removed = pc.utf8_slice_codeunits(self._pa_array, len(prefix))\n            result = pc.if_else(starts_with, removed, self._pa_array)\n            return type(self)(result)\n        return super()._str_removeprefix(prefix)\n\n    def _str_removesuffix(self, suffix: str):\n        ends_with = pc.ends_with(self._pa_array, pattern=suffix)\n        removed = pc.utf8_slice_codeunits(self._pa_array, 0, stop=-len(suffix))\n        result = pc.if_else(ends_with, removed, self._pa_array)\n        return type(self)(result)\n\n    def _str_count(self, pat: str, flags: int = 0):\n        if flags:\n            return super()._str_count(pat, flags)\n        result = pc.count_substring_regex(self._pa_array, pat)\n        return self._convert_int_dtype(result)\n\n    def _str_find(self, sub: str, start: int = 0, end: int | None = None):\n        if start != 0 and end is not None:\n            slices = pc.utf8_slice_codeunits(self._pa_array, start, stop=end)\n            result = pc.find_substring(slices, sub)\n            not_found = pc.equal(result, -1)\n            offset_result = pc.add(result, end - start)\n            result = pc.if_else(not_found, result, offset_result)\n        elif start == 0 and end is None:\n            slices = self._pa_array\n            result = pc.find_substring(slices, sub)\n        else:\n            return super()._str_find(sub, start, end)\n        return self._convert_int_dtype(result)\n\n    def _str_get_dummies(self, sep: str = \"|\"):\n        dummies_pa, labels = ArrowExtensionArray(self._pa_array)._str_get_dummies(sep)\n        if len(labels) == 0:\n            return np.empty(shape=(0, 0), dtype=np.int64), labels\n        dummies = np.vstack(dummies_pa.to_numpy())\n        return dummies.astype(np.int64, copy=False), labels\n\n    def _convert_int_dtype(self, result):\n        return Int64Dtype().__from_arrow__(result)\n\n    def _reduce(\n        self, name: str, *, skipna: bool = True, keepdims: bool = False, **kwargs\n    ):\n        result = self._reduce_calc(name, skipna=skipna, keepdims=keepdims, **kwargs)\n        if name in (\"argmin\", \"argmax\") and isinstance(result, pa.Array):\n            return self._convert_int_dtype(result)\n        elif isinstance(result, pa.Array):\n            return type(self)(result)\n        else:\n            return result\n\n    def _rank(\n        self,\n        *,\n        axis: AxisInt = 0,\n        method: str = \"average\",\n        na_option: str = \"keep\",\n        ascending: bool = True,\n        pct: bool = False,\n    ):\n        \"\"\"\n        See Series.rank.__doc__.\n        \"\"\"\n        return self._convert_int_dtype(\n            self._rank_calc(\n                axis=axis,\n                method=method,\n                na_option=na_option,\n                ascending=ascending,\n                pct=pct,\n            )\n        )\n", "class_fn": true, "question_id": "pandas/pandas.core.arrays.string_arrow/ArrowStringArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/base.py", "fn_id": "", "content": "class IndexOpsMixin(OpsMixin):\n    \"\"\"\n    Common ops mixin to support a unified interface / docs for Series / Index\n    \"\"\"\n\n    # ndarray compatibility\n    __array_priority__ = 1000\n    _hidden_attrs: frozenset[str] = frozenset(\n        [\"tolist\"]  # tolist is not deprecated, just suppressed in the __dir__\n    )\n\n    @property\n    def dtype(self) -> DtypeObj:\n        # must be defined here as a property for mypy\n        raise AbstractMethodError(self)\n\n    @property\n    def _values(self) -> ExtensionArray | np.ndarray:\n        # must be defined here as a property for mypy\n        raise AbstractMethodError(self)\n\n    @final\n    def transpose(self, *args, **kwargs) -> Self:\n        \"\"\"\n        Return the transpose, which is by definition self.\n\n        Returns\n        -------\n        %(klass)s\n        \"\"\"\n        nv.validate_transpose(args, kwargs)\n        return self\n\n    T = property(\n        transpose,\n        doc=\"\"\"\n        Return the transpose, which is by definition self.\n\n        Examples\n        --------\n        For Series:\n\n        >>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n        >>> s\n        0     Ant\n        1    Bear\n        2     Cow\n        dtype: object\n        >>> s.T\n        0     Ant\n        1    Bear\n        2     Cow\n        dtype: object\n\n        For Index:\n\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx.T\n        Index([1, 2, 3], dtype='int64')\n        \"\"\",\n    )\n\n    @property\n    def shape(self) -> Shape:\n        \"\"\"\n        Return a tuple of the shape of the underlying data.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.shape\n        (3,)\n        \"\"\"\n        return self._values.shape\n\n    def __len__(self) -> int:\n        # We need this defined here for mypy\n        raise AbstractMethodError(self)\n\n    @property\n    def ndim(self) -> Literal[1]:\n        \"\"\"\n        Number of dimensions of the underlying data, by definition 1.\n\n        Examples\n        --------\n        >>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n        >>> s\n        0     Ant\n        1    Bear\n        2     Cow\n        dtype: object\n        >>> s.ndim\n        1\n\n        For Index:\n\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx\n        Index([1, 2, 3], dtype='int64')\n        >>> idx.ndim\n        1\n        \"\"\"\n        return 1\n\n    @final\n    def item(self):\n        \"\"\"\n        Return the first element of the underlying data as a Python scalar.\n\n        Returns\n        -------\n        scalar\n            The first element of Series or Index.\n\n        Raises\n        ------\n        ValueError\n            If the data is not length = 1.\n\n        Examples\n        --------\n        >>> s = pd.Series([1])\n        >>> s.item()\n        1\n\n        For an index:\n\n        >>> s = pd.Series([1], index=['a'])\n        >>> s.index.item()\n        'a'\n        \"\"\"\n        if len(self) == 1:\n            return next(iter(self))\n        raise ValueError(\"can only convert an array of size 1 to a Python scalar\")\n\n    @property\n    def nbytes(self) -> int:\n        \"\"\"\n        Return the number of bytes in the underlying data.\n\n        Examples\n        --------\n        For Series:\n\n        >>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n        >>> s\n        0     Ant\n        1    Bear\n        2     Cow\n        dtype: object\n        >>> s.nbytes\n        24\n\n        For Index:\n\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx\n        Index([1, 2, 3], dtype='int64')\n        >>> idx.nbytes\n        24\n        \"\"\"\n        return self._values.nbytes\n\n    @property\n    def size(self) -> int:\n        \"\"\"\n        Return the number of elements in the underlying data.\n\n        Examples\n        --------\n        For Series:\n\n        >>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n        >>> s\n        0     Ant\n        1    Bear\n        2     Cow\n        dtype: object\n        >>> s.size\n        3\n\n        For Index:\n\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx\n        Index([1, 2, 3], dtype='int64')\n        >>> idx.size\n        3\n        \"\"\"\n        return len(self._values)\n\n    @property\n    def array(self) -> ExtensionArray:\n        \"\"\"\n        The ExtensionArray of the data backing this Series or Index.\n\n        Returns\n        -------\n        ExtensionArray\n            An ExtensionArray of the values stored within. For extension\n            types, this is the actual array. For NumPy native types, this\n            is a thin (no copy) wrapper around :class:`numpy.ndarray`.\n\n            ``.array`` differs from ``.values``, which may require converting\n            the data to a different form.\n\n        See Also\n        --------\n        Index.to_numpy : Similar method that always returns a NumPy array.\n        Series.to_numpy : Similar method that always returns a NumPy array.\n\n        Notes\n        -----\n        This table lays out the different array types for each extension\n        dtype within pandas.\n\n        ================== =============================\n        dtype              array type\n        ================== =============================\n        category           Categorical\n        period             PeriodArray\n        interval           IntervalArray\n        IntegerNA          IntegerArray\n        string             StringArray\n        boolean            BooleanArray\n        datetime64[ns, tz] DatetimeArray\n        ================== =============================\n\n        For any 3rd-party extension types, the array type will be an\n        ExtensionArray.\n\n        For all remaining dtypes ``.array`` will be a\n        :class:`arrays.NumpyExtensionArray` wrapping the actual ndarray\n        stored within. If you absolutely need a NumPy array (possibly with\n        copying / coercing data), then use :meth:`Series.to_numpy` instead.\n\n        Examples\n        --------\n        For regular NumPy types like int, and float, a NumpyExtensionArray\n        is returned.\n\n        >>> pd.Series([1, 2, 3]).array\n        <NumpyExtensionArray>\n        [1, 2, 3]\n        Length: 3, dtype: int64\n\n        For extension types, like Categorical, the actual ExtensionArray\n        is returned\n\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\n        >>> ser.array\n        ['a', 'b', 'a']\n        Categories (2, object): ['a', 'b']\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @final\n    def to_numpy(\n        self,\n        dtype: npt.DTypeLike | None = None,\n        copy: bool = False,\n        na_value: object = lib.no_default,\n        **kwargs,\n    ) -> np.ndarray:\n        \"\"\"\n        A NumPy ndarray representing the values in this Series or Index.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to pass to :meth:`numpy.asarray`.\n        copy : bool, default False\n            Whether to ensure that the returned value is not a view on\n            another array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary.\n        na_value : Any, optional\n            The value to use for missing values. The default value depends\n            on `dtype` and the type of the array.\n        **kwargs\n            Additional keywords passed through to the ``to_numpy`` method\n            of the underlying array (for extension arrays).\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        Series.array : Get the actual data stored within.\n        Index.array : Get the actual data stored within.\n        DataFrame.to_numpy : Similar method for DataFrame.\n\n        Notes\n        -----\n        The returned array will be the same up to equality (values equal\n        in `self` will be equal in the returned array; likewise for values\n        that are not equal). When `self` contains an ExtensionArray, the\n        dtype may be different. For example, for a category-dtype Series,\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\n        will be lost.\n\n        For NumPy dtypes, this will be a reference to the actual data stored\n        in this Series or Index (assuming ``copy=False``). Modifying the result\n        in place will modify the data stored in the Series or Index (not that\n        we recommend doing that).\n\n        For extension types, ``to_numpy()`` *may* require copying data and\n        coercing the result to a NumPy type (possibly object), which may be\n        expensive. When you need a no-copy reference to the underlying data,\n        :attr:`Series.array` should be used instead.\n\n        This table lays out the different dtypes and default return types of\n        ``to_numpy()`` for various dtypes within pandas.\n\n        ================== ================================\n        dtype              array type\n        ================== ================================\n        category[T]        ndarray[T] (same dtype as input)\n        period             ndarray[object] (Periods)\n        interval           ndarray[object] (Intervals)\n        IntegerNA          ndarray[object]\n        datetime64[ns]     datetime64[ns]\n        datetime64[ns, tz] ndarray[object] (Timestamps)\n        ================== ================================\n\n        Examples\n        --------\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\n        >>> ser.to_numpy()\n        array(['a', 'b', 'a'], dtype=object)\n\n        Specify the `dtype` to control how datetime-aware data is represented.\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\n        objects, each with the correct ``tz``.\n\n        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n        >>> ser.to_numpy(dtype=object)\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\n              dtype=object)\n\n        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\n        datetime64 values. The values are converted to UTC and the timezone\n        info is dropped.\n\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\n        ... # doctest: +ELLIPSIS\n        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\n              dtype='datetime64[ns]')\n        \"\"\"\n        if isinstance(self.dtype, ExtensionDtype):\n            return self.array.to_numpy(dtype, copy=copy, na_value=na_value, **kwargs)\n        elif kwargs:\n            bad_keys = next(iter(kwargs.keys()))\n            raise TypeError(\n                f\"to_numpy() got an unexpected keyword argument '{bad_keys}'\"\n            )\n\n        fillna = (\n            na_value is not lib.no_default\n            # no need to fillna with np.nan if we already have a float dtype\n            and not (na_value is np.nan and np.issubdtype(self.dtype, np.floating))\n        )\n\n        values = self._values\n        if fillna:\n            if not can_hold_element(values, na_value):\n                # if we can't hold the na_value asarray either makes a copy or we\n                # error before modifying values. The asarray later on thus won't make\n                # another copy\n                values = np.asarray(values, dtype=dtype)\n            else:\n                values = values.copy()\n\n            values[np.asanyarray(isna(self))] = na_value\n\n        result = np.asarray(values, dtype=dtype)\n\n        if (copy and not fillna) or (not copy and using_copy_on_write()):\n            if np.shares_memory(self._values[:2], result[:2]):\n                # Take slices to improve performance of check\n                if using_copy_on_write() and not copy:\n                    result = result.view()\n                    result.flags.writeable = False\n                else:\n                    result = result.copy()\n\n        return result\n\n    @final\n    @property\n    def empty(self) -> bool:\n        return not self.size\n\n    @doc(op=\"max\", oppose=\"min\", value=\"largest\")\n    def argmax(\n        self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs\n    ) -> int:\n        \"\"\"\n        Return int position of the {value} value in the Series.\n\n        If the {op}imum is achieved in multiple locations,\n        the first row position is returned.\n\n        Parameters\n        ----------\n        axis : {{None}}\n            Unused. Parameter needed for compatibility with DataFrame.\n        skipna : bool, default True\n            Exclude NA/null values when showing the result.\n        *args, **kwargs\n            Additional arguments and keywords for compatibility with NumPy.\n\n        Returns\n        -------\n        int\n            Row position of the {op}imum value.\n\n        See Also\n        --------\n        Series.arg{op} : Return position of the {op}imum value.\n        Series.arg{oppose} : Return position of the {oppose}imum value.\n        numpy.ndarray.arg{op} : Equivalent method for numpy arrays.\n        Series.idxmax : Return index label of the maximum values.\n        Series.idxmin : Return index label of the minimum values.\n\n        Examples\n        --------\n        Consider dataset containing cereal calories\n\n        >>> s = pd.Series({{'Corn Flakes': 100.0, 'Almond Delight': 110.0,\n        ...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0}})\n        >>> s\n        Corn Flakes              100.0\n        Almond Delight           110.0\n        Cinnamon Toast Crunch    120.0\n        Cocoa Puff               110.0\n        dtype: float64\n\n        >>> s.argmax()\n        2\n        >>> s.argmin()\n        0\n\n        The maximum cereal calories is the third element and\n        the minimum cereal calories is the first element,\n        since series is zero-indexed.\n        \"\"\"\n        delegate = self._values\n        nv.validate_minmax_axis(axis)\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\n\n        if isinstance(delegate, ExtensionArray):\n            if not skipna and delegate.isna().any():\n                warnings.warn(\n                    f\"The behavior of {type(self).__name__}.argmax/argmin \"\n                    \"with skipna=False and NAs, or with all-NAs is deprecated. \"\n                    \"In a future version this will raise ValueError.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n                return -1\n            else:\n                return delegate.argmax()\n        else:\n            result = nanops.nanargmax(delegate, skipna=skipna)\n            if result == -1:\n                warnings.warn(\n                    f\"The behavior of {type(self).__name__}.argmax/argmin \"\n                    \"with skipna=False and NAs, or with all-NAs is deprecated. \"\n                    \"In a future version this will raise ValueError.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n            # error: Incompatible return value type (got \"Union[int, ndarray]\", expected\n            # \"int\")\n            return result  # type: ignore[return-value]\n\n    @doc(argmax, op=\"min\", oppose=\"max\", value=\"smallest\")\n    def argmin(\n        self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs\n    ) -> int:\n        delegate = self._values\n        nv.validate_minmax_axis(axis)\n        skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)\n\n        if isinstance(delegate, ExtensionArray):\n            if not skipna and delegate.isna().any():\n                warnings.warn(\n                    f\"The behavior of {type(self).__name__}.argmax/argmin \"\n                    \"with skipna=False and NAs, or with all-NAs is deprecated. \"\n                    \"In a future version this will raise ValueError.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n                return -1\n            else:\n                return delegate.argmin()\n        else:\n            result = nanops.nanargmin(delegate, skipna=skipna)\n            if result == -1:\n                warnings.warn(\n                    f\"The behavior of {type(self).__name__}.argmax/argmin \"\n                    \"with skipna=False and NAs, or with all-NAs is deprecated. \"\n                    \"In a future version this will raise ValueError.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n            # error: Incompatible return value type (got \"Union[int, ndarray]\", expected\n            # \"int\")\n            return result  # type: ignore[return-value]\n\n    def tolist(self):\n        \"\"\"\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a pandas scalar\n        (for Timestamp/Timedelta/Interval/Period)\n\n        Returns\n        -------\n        list\n\n        See Also\n        --------\n        numpy.ndarray.tolist : Return the array as an a.ndim-levels deep\n            nested list of Python scalars.\n\n        Examples\n        --------\n        For Series\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.to_list()\n        [1, 2, 3]\n\n        For Index:\n\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx\n        Index([1, 2, 3], dtype='int64')\n\n        >>> idx.to_list()\n        [1, 2, 3]\n        \"\"\"\n        return self._values.tolist()\n\n    to_list = tolist\n\n    def __iter__(self) -> Iterator:\n        \"\"\"\n        Return an iterator of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a pandas scalar\n        (for Timestamp/Timedelta/Interval/Period)\n\n        Returns\n        -------\n        iterator\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> for x in s:\n        ...     print(x)\n        1\n        2\n        3\n        \"\"\"\n        # We are explicitly making element iterators.\n        if not isinstance(self._values, np.ndarray):\n            # Check type instead of dtype to catch DTA/TDA\n            return iter(self._values)\n        else:\n            return map(self._values.item, range(self._values.size))\n\n    @cache_readonly\n    def hasnans(self) -> bool:\n        \"\"\"\n        Return True if there are any NaNs.\n\n        Enables various performance speedups.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, None])\n        >>> s\n        0    1.0\n        1    2.0\n        2    3.0\n        3    NaN\n        dtype: float64\n        >>> s.hasnans\n        True\n        \"\"\"\n        # error: Item \"bool\" of \"Union[bool, ndarray[Any, dtype[bool_]], NDFrame]\"\n        # has no attribute \"any\"\n        return bool(isna(self).any())  # type: ignore[union-attr]\n\n    @final\n    def _map_values(self, mapper, na_action=None, convert: bool = True):\n        \"\"\"\n        An internal function that maps values using the input\n        correspondence (which can be a dict, Series, or function).\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            The input correspondence object\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mapping function\n        convert : bool, default True\n            Try to find better dtype for elementwise function results. If\n            False, leave as dtype=object. Note that the dtype is always\n            preserved for some extension array dtypes, such as Categorical.\n\n        Returns\n        -------\n        Union[Index, MultiIndex], inferred\n            The output of the mapping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n        arr = self._values\n\n        if isinstance(arr, ExtensionArray):\n            return arr.map(mapper, na_action=na_action)\n\n        return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n\n    @final\n    def value_counts(\n        self,\n        normalize: bool = False,\n        sort: bool = True,\n        ascending: bool = False,\n        bins=None,\n        dropna: bool = True,\n    ) -> Series:\n        \"\"\"\n        Return a Series containing counts of unique values.\n\n        The resulting object will be in descending order so that the\n        first element is the most frequently-occurring element.\n        Excludes NA values by default.\n\n        Parameters\n        ----------\n        normalize : bool, default False\n            If True then the object returned will contain the relative\n            frequencies of the unique values.\n        sort : bool, default True\n            Sort by frequencies when True. Preserve the order of the data when False.\n        ascending : bool, default False\n            Sort in ascending order.\n        bins : int, optional\n            Rather than count values, group them into half-open bins,\n            a convenience for ``pd.cut``, only works with numeric data.\n        dropna : bool, default True\n            Don't include counts of NaN.\n\n        Returns\n        -------\n        Series\n\n        See Also\n        --------\n        Series.count: Number of non-NA elements in a Series.\n        DataFrame.count: Number of non-NA elements in a DataFrame.\n        DataFrame.value_counts: Equivalent method on DataFrames.\n\n        Examples\n        --------\n        >>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\n        >>> index.value_counts()\n        3.0    2\n        1.0    1\n        2.0    1\n        4.0    1\n        Name: count, dtype: int64\n\n        With `normalize` set to `True`, returns the relative frequency by\n        dividing all values by the sum of values.\n\n        >>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\n        >>> s.value_counts(normalize=True)\n        3.0    0.4\n        1.0    0.2\n        2.0    0.2\n        4.0    0.2\n        Name: proportion, dtype: float64\n\n        **bins**\n\n        Bins can be useful for going from a continuous variable to a\n        categorical variable; instead of counting unique\n        apparitions of values, divide the index in the specified\n        number of half-open bins.\n\n        >>> s.value_counts(bins=3)\n        (0.996, 2.0]    2\n        (2.0, 3.0]      2\n        (3.0, 4.0]      1\n        Name: count, dtype: int64\n\n        **dropna**\n\n        With `dropna` set to `False` we can also see NaN index values.\n\n        >>> s.value_counts(dropna=False)\n        3.0    2\n        1.0    1\n        2.0    1\n        4.0    1\n        NaN    1\n        Name: count, dtype: int64\n        \"\"\"\n        return algorithms.value_counts_internal(\n            self,\n            sort=sort,\n            ascending=ascending,\n            normalize=normalize,\n            bins=bins,\n            dropna=dropna,\n        )\n\n    def unique(self):\n        values = self._values\n        if not isinstance(values, np.ndarray):\n            # i.e. ExtensionArray\n            result = values.unique()\n        else:\n            result = algorithms.unique1d(values)\n        return result\n\n    @final\n    def nunique(self, dropna: bool = True) -> int:\n        \"\"\"\n        Return number of unique elements in the object.\n\n        Excludes NA values by default.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include NaN in the count.\n\n        Returns\n        -------\n        int\n\n        See Also\n        --------\n        DataFrame.nunique: Method nunique for DataFrame.\n        Series.count: Count non-NA/null observations in the Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 3, 5, 7, 7])\n        >>> s\n        0    1\n        1    3\n        2    5\n        3    7\n        4    7\n        dtype: int64\n\n        >>> s.nunique()\n        4\n        \"\"\"\n        uniqs = self.unique()\n        if dropna:\n            uniqs = remove_na_arraylike(uniqs)\n        return len(uniqs)\n\n    @property\n    def is_unique(self) -> bool:\n        \"\"\"\n        Return boolean if values in the object are unique.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.is_unique\n        True\n\n        >>> s = pd.Series([1, 2, 3, 1])\n        >>> s.is_unique\n        False\n        \"\"\"\n        return self.nunique(dropna=False) == len(self)\n\n    @property\n    def is_monotonic_increasing(self) -> bool:\n        \"\"\"\n        Return boolean if values in the object are monotonically increasing.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 2])\n        >>> s.is_monotonic_increasing\n        True\n\n        >>> s = pd.Series([3, 2, 1])\n        >>> s.is_monotonic_increasing\n        False\n        \"\"\"\n        from pandas import Index\n\n        return Index(self).is_monotonic_increasing\n\n    @property\n    def is_monotonic_decreasing(self) -> bool:\n        \"\"\"\n        Return boolean if values in the object are monotonically decreasing.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> s = pd.Series([3, 2, 2, 1])\n        >>> s.is_monotonic_decreasing\n        True\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.is_monotonic_decreasing\n        False\n        \"\"\"\n        from pandas import Index\n\n        return Index(self).is_monotonic_decreasing\n\n    @final\n    def _memory_usage(self, deep: bool = False) -> int:\n        \"\"\"\n        Memory usage of the values.\n\n        Parameters\n        ----------\n        deep : bool, default False\n            Introspect the data deeply, interrogate\n            `object` dtypes for system-level memory consumption.\n\n        Returns\n        -------\n        bytes used\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n            array.\n\n        Notes\n        -----\n        Memory usage does not include memory consumed by elements that\n        are not components of the array if deep=False or if used on PyPy\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx.memory_usage()\n        24\n        \"\"\"\n        if hasattr(self.array, \"memory_usage\"):\n            return self.array.memory_usage(  # pyright: ignore[reportGeneralTypeIssues]\n                deep=deep,\n            )\n\n        v = self.array.nbytes\n        if deep and is_object_dtype(self.dtype) and not PYPY:\n            values = cast(np.ndarray, self._values)\n            v += lib.memory_usage_of_objects(values)\n        return v\n\n    @doc(\n        algorithms.factorize,\n        values=\"\",\n        order=\"\",\n        size_hint=\"\",\n        sort=textwrap.dedent(\n            \"\"\"\\\n            sort : bool, default False\n                Sort `uniques` and shuffle `codes` to maintain the\n                relationship.\n            \"\"\"\n        ),\n    )\n    def factorize(\n        self,\n        sort: bool = False,\n        use_na_sentinel: bool = True,\n    ) -> tuple[npt.NDArray[np.intp], Index]:\n        codes, uniques = algorithms.factorize(\n            self._values, sort=sort, use_na_sentinel=use_na_sentinel\n        )\n        if uniques.dtype == np.float16:\n            uniques = uniques.astype(np.float32)\n\n        if isinstance(self, ABCIndex):\n            # preserve e.g. MultiIndex\n            uniques = self._constructor(uniques)\n        else:\n            from pandas import Index\n\n            uniques = Index(uniques)\n        return codes, uniques\n\n    _shared_docs[\n        \"searchsorted\"\n    ] = \"\"\"\n        Find indices where elements should be inserted to maintain order.\n\n        Find the indices into a sorted {klass} `self` such that, if the\n        corresponding elements in `value` were inserted before the indices,\n        the order of `self` would be preserved.\n\n        .. note::\n\n            The {klass} *must* be monotonically sorted, otherwise\n            wrong locations will likely be returned. Pandas does *not*\n            check this for you.\n\n        Parameters\n        ----------\n        value : array-like or scalar\n            Values to insert into `self`.\n        side : {{'left', 'right'}}, optional\n            If 'left', the index of the first suitable location found is given.\n            If 'right', return the last such index.  If there is no suitable\n            index, return either 0 or N (where N is the length of `self`).\n        sorter : 1-D array-like, optional\n            Optional array of integer indices that sort `self` into ascending\n            order. They are typically the result of ``np.argsort``.\n\n        Returns\n        -------\n        int or array of int\n            A scalar or array of insertion points with the\n            same shape as `value`.\n\n        See Also\n        --------\n        sort_values : Sort by the values along either axis.\n        numpy.searchsorted : Similar method from NumPy.\n\n        Notes\n        -----\n        Binary search is used to find the required insertion points.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1, 2, 3])\n        >>> ser\n        0    1\n        1    2\n        2    3\n        dtype: int64\n\n        >>> ser.searchsorted(4)\n        3\n\n        >>> ser.searchsorted([0, 4])\n        array([0, 3])\n\n        >>> ser.searchsorted([1, 3], side='left')\n        array([0, 2])\n\n        >>> ser.searchsorted([1, 3], side='right')\n        array([1, 3])\n\n        >>> ser = pd.Series(pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000']))\n        >>> ser\n        0   2000-03-11\n        1   2000-03-12\n        2   2000-03-13\n        dtype: datetime64[ns]\n\n        >>> ser.searchsorted('3/14/2000')\n        3\n\n        >>> ser = pd.Categorical(\n        ...     ['apple', 'bread', 'bread', 'cheese', 'milk'], ordered=True\n        ... )\n        >>> ser\n        ['apple', 'bread', 'bread', 'cheese', 'milk']\n        Categories (4, object): ['apple' < 'bread' < 'cheese' < 'milk']\n\n        >>> ser.searchsorted('bread')\n        1\n\n        >>> ser.searchsorted(['bread'], side='right')\n        array([3])\n\n        If the values are not monotonically sorted, wrong locations\n        may be returned:\n\n        >>> ser = pd.Series([2, 1, 3])\n        >>> ser\n        0    2\n        1    1\n        2    3\n        dtype: int64\n\n        >>> ser.searchsorted(1)  # doctest: +SKIP\n        0  # wrong result, correct would be 1\n        \"\"\"\n\n    # This overload is needed so that the call to searchsorted in\n    # pandas.core.resample.TimeGrouper._get_period_bins picks the correct result\n\n    # error: Overloaded function signatures 1 and 2 overlap with incompatible\n    # return types\n    @overload\n    def searchsorted(  # type: ignore[overload-overlap]\n        self,\n        value: ScalarLike_co,\n        side: Literal[\"left\", \"right\"] = ...,\n        sorter: NumpySorter = ...,\n    ) -> np.intp:\n        ...\n\n    @overload\n    def searchsorted(\n        self,\n        value: npt.ArrayLike | ExtensionArray,\n        side: Literal[\"left\", \"right\"] = ...,\n        sorter: NumpySorter = ...,\n    ) -> npt.NDArray[np.intp]:\n        ...\n\n    @doc(_shared_docs[\"searchsorted\"], klass=\"Index\")\n    def searchsorted(\n        self,\n        value: NumpyValueArrayLike | ExtensionArray,\n        side: Literal[\"left\", \"right\"] = \"left\",\n        sorter: NumpySorter | None = None,\n    ) -> npt.NDArray[np.intp] | np.intp:\n        if isinstance(value, ABCDataFrame):\n            msg = (\n                \"Value must be 1-D array-like or scalar, \"\n                f\"{type(value).__name__} is not supported\"\n            )\n            raise ValueError(msg)\n\n        values = self._values\n        if not isinstance(values, np.ndarray):\n            # Going through EA.searchsorted directly improves performance GH#38083\n            return values.searchsorted(value, side=side, sorter=sorter)\n\n        return algorithms.searchsorted(\n            values,\n            value,\n            side=side,\n            sorter=sorter,\n        )\n\n    def drop_duplicates(self, *, keep: DropKeep = \"first\"):\n        duplicated = self._duplicated(keep=keep)\n        # error: Value of type \"IndexOpsMixin\" is not indexable\n        return self[~duplicated]  # type: ignore[index]\n\n    @final\n    def _duplicated(self, keep: DropKeep = \"first\") -> npt.NDArray[np.bool_]:\n        arr = self._values\n        if isinstance(arr, ExtensionArray):\n            return arr.duplicated(keep=keep)\n        return algorithms.duplicated(arr, keep=keep)\n\n    def _arith_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n        rvalues = ops.maybe_prepare_scalar_for_op(rvalues, lvalues.shape)\n        rvalues = ensure_wrapped_if_datetimelike(rvalues)\n        if isinstance(rvalues, range):\n            rvalues = np.arange(rvalues.start, rvalues.stop, rvalues.step)\n\n        with np.errstate(all=\"ignore\"):\n            result = ops.arithmetic_op(lvalues, rvalues, op)\n\n        return self._construct_result(result, name=res_name)\n\n    def _construct_result(self, result, name):\n        \"\"\"\n        Construct an appropriately-wrapped result from the ArrayLike result\n        of an arithmetic-like operation.\n        \"\"\"\n        raise AbstractMethodError(self)\n", "class_fn": true, "question_id": "pandas/pandas.core.base/IndexOpsMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/ops.py", "fn_id": "", "content": "class BinOp(Op):\n    \"\"\"\n    Hold a binary operator and its operands.\n\n    Parameters\n    ----------\n    op : str\n    lhs : Term or Op\n    rhs : Term or Op\n    \"\"\"\n\n    def __init__(self, op: str, lhs, rhs) -> None:\n        super().__init__(op, (lhs, rhs))\n        self.lhs = lhs\n        self.rhs = rhs\n\n        self._disallow_scalar_only_bool_ops()\n\n        self.convert_values()\n\n        try:\n            self.func = _binary_ops_dict[op]\n        except KeyError as err:\n            # has to be made a list for python3\n            keys = list(_binary_ops_dict.keys())\n            raise ValueError(\n                f\"Invalid binary operator {repr(op)}, valid operators are {keys}\"\n            ) from err\n\n    def __call__(self, env):\n        \"\"\"\n        Recursively evaluate an expression in Python space.\n\n        Parameters\n        ----------\n        env : Scope\n\n        Returns\n        -------\n        object\n            The result of an evaluated expression.\n        \"\"\"\n        # recurse over the left/right nodes\n        left = self.lhs(env)\n        right = self.rhs(env)\n\n        return self.func(left, right)\n\n    def evaluate(self, env, engine: str, parser, term_type, eval_in_python):\n        \"\"\"\n        Evaluate a binary operation *before* being passed to the engine.\n\n        Parameters\n        ----------\n        env : Scope\n        engine : str\n        parser : str\n        term_type : type\n        eval_in_python : list\n\n        Returns\n        -------\n        term_type\n            The \"pre-evaluated\" expression as an instance of ``term_type``\n        \"\"\"\n        if engine == \"python\":\n            res = self(env)\n        else:\n            # recurse over the left/right nodes\n\n            left = self.lhs.evaluate(\n                env,\n                engine=engine,\n                parser=parser,\n                term_type=term_type,\n                eval_in_python=eval_in_python,\n            )\n\n            right = self.rhs.evaluate(\n                env,\n                engine=engine,\n                parser=parser,\n                term_type=term_type,\n                eval_in_python=eval_in_python,\n            )\n\n            # base cases\n            if self.op in eval_in_python:\n                res = self.func(left.value, right.value)\n            else:\n                from pandas.core.computation.eval import eval\n\n                res = eval(self, local_dict=env, engine=engine, parser=parser)\n\n        name = env.add_tmp(res)\n        return term_type(name, env=env)\n\n    def convert_values(self) -> None:\n        \"\"\"\n        Convert datetimes to a comparable value in an expression.\n        \"\"\"\n\n        def stringify(value):\n            encoder: Callable\n            if self.encoding is not None:\n                encoder = partial(pprint_thing_encoded, encoding=self.encoding)\n            else:\n                encoder = pprint_thing\n            return encoder(value)\n\n        lhs, rhs = self.lhs, self.rhs\n\n        if is_term(lhs) and lhs.is_datetime and is_term(rhs) and rhs.is_scalar:\n            v = rhs.value\n            if isinstance(v, (int, float)):\n                v = stringify(v)\n            v = Timestamp(ensure_decoded(v))\n            if v.tz is not None:\n                v = v.tz_convert(\"UTC\")\n            self.rhs.update(v)\n\n        if is_term(rhs) and rhs.is_datetime and is_term(lhs) and lhs.is_scalar:\n            v = lhs.value\n            if isinstance(v, (int, float)):\n                v = stringify(v)\n            v = Timestamp(ensure_decoded(v))\n            if v.tz is not None:\n                v = v.tz_convert(\"UTC\")\n            self.lhs.update(v)\n\n    def _disallow_scalar_only_bool_ops(self):\n        rhs = self.rhs\n        lhs = self.lhs\n\n        # GH#24883 unwrap dtype if necessary to ensure we have a type object\n        rhs_rt = rhs.return_type\n        rhs_rt = getattr(rhs_rt, \"type\", rhs_rt)\n        lhs_rt = lhs.return_type\n        lhs_rt = getattr(lhs_rt, \"type\", lhs_rt)\n        if (\n            (lhs.is_scalar or rhs.is_scalar)\n            and self.op in _bool_ops_dict\n            and (\n                not (\n                    issubclass(rhs_rt, (bool, np.bool_))\n                    and issubclass(lhs_rt, (bool, np.bool_))\n                )\n            )\n        ):\n            raise NotImplementedError(\"cannot evaluate scalar only bool ops\")\n", "class_fn": true, "question_id": "pandas/pandas.core.computation.ops/BinOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/pytables.py", "fn_id": "", "content": "class PyTablesExpr(expr.Expr):\n    \"\"\"\n    Hold a pytables-like expression, comprised of possibly multiple 'terms'.\n\n    Parameters\n    ----------\n    where : string term expression, PyTablesExpr, or list-like of PyTablesExprs\n    queryables : a \"kinds\" map (dict of column name -> kind), or None if column\n        is non-indexable\n    encoding : an encoding that will encode the query terms\n\n    Returns\n    -------\n    a PyTablesExpr object\n\n    Examples\n    --------\n    'index>=date'\n    \"columns=['A', 'D']\"\n    'columns=A'\n    'columns==A'\n    \"~(columns=['A','B'])\"\n    'index>df.index[3] & string=\"bar\"'\n    '(index>df.index[3] & index<=df.index[6]) | string=\"bar\"'\n    \"ts>=Timestamp('2012-02-01')\"\n    \"major_axis>=20130101\"\n    \"\"\"\n\n    _visitor: PyTablesExprVisitor | None\n    env: PyTablesScope\n    expr: str\n\n    def __init__(\n        self,\n        where,\n        queryables: dict[str, Any] | None = None,\n        encoding=None,\n        scope_level: int = 0,\n    ) -> None:\n        where = _validate_where(where)\n\n        self.encoding = encoding\n        self.condition = None\n        self.filter = None\n        self.terms = None\n        self._visitor = None\n\n        # capture the environment if needed\n        local_dict: _scope.DeepChainMap[Any, Any] | None = None\n\n        if isinstance(where, PyTablesExpr):\n            local_dict = where.env.scope\n            _where = where.expr\n\n        elif is_list_like(where):\n            where = list(where)\n            for idx, w in enumerate(where):\n                if isinstance(w, PyTablesExpr):\n                    local_dict = w.env.scope\n                else:\n                    where[idx] = _validate_where(w)\n            _where = \" & \".join([f\"({w})\" for w in com.flatten(where)])\n        else:\n            # _validate_where ensures we otherwise have a string\n            _where = where\n\n        self.expr = _where\n        self.env = PyTablesScope(scope_level + 1, local_dict=local_dict)\n\n        if queryables is not None and isinstance(self.expr, str):\n            self.env.queryables.update(queryables)\n            self._visitor = PyTablesExprVisitor(\n                self.env,\n                queryables=queryables,\n                parser=\"pytables\",\n                engine=\"pytables\",\n                encoding=encoding,\n            )\n            self.terms = self.parse()\n\n    def __repr__(self) -> str:\n        if self.terms is not None:\n            return pprint_thing(self.terms)\n        return pprint_thing(self.expr)\n\n    def evaluate(self):\n        \"\"\"create and return the numexpr condition and filter\"\"\"\n        try:\n            self.condition = self.terms.prune(ConditionBinOp)\n        except AttributeError as err:\n            raise ValueError(\n                f\"cannot process expression [{self.expr}], [{self}] \"\n                \"is not a valid condition\"\n            ) from err\n        try:\n            self.filter = self.terms.prune(FilterBinOp)\n        except AttributeError as err:\n            raise ValueError(\n                f\"cannot process expression [{self.expr}], [{self}] \"\n                \"is not a valid filter\"\n            ) from err\n\n        return self.condition, self.filter\n", "class_fn": true, "question_id": "pandas/pandas.core.computation.pytables/PyTablesExpr", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/dtypes/base.py", "fn_id": "", "content": "class ExtensionDtype:\n    \"\"\"\n    A custom data type, to be paired with an ExtensionArray.\n\n    See Also\n    --------\n    extensions.register_extension_dtype: Register an ExtensionType\n        with pandas as class decorator.\n    extensions.ExtensionArray: Abstract base class for custom 1-D array types.\n\n    Notes\n    -----\n    The interface includes the following abstract methods that must\n    be implemented by subclasses:\n\n    * type\n    * name\n    * construct_array_type\n\n    The following attributes and methods influence the behavior of the dtype in\n    pandas operations\n\n    * _is_numeric\n    * _is_boolean\n    * _get_common_dtype\n\n    The `na_value` class attribute can be used to set the default NA value\n    for this type. :attr:`numpy.nan` is used by default.\n\n    ExtensionDtypes are required to be hashable. The base class provides\n    a default implementation, which relies on the ``_metadata`` class\n    attribute. ``_metadata`` should be a tuple containing the strings\n    that define your data type. For example, with ``PeriodDtype`` that's\n    the ``freq`` attribute.\n\n    **If you have a parametrized dtype you should set the ``_metadata``\n    class property**.\n\n    Ideally, the attributes in ``_metadata`` will match the\n    parameters to your ``ExtensionDtype.__init__`` (if any). If any of\n    the attributes in ``_metadata`` don't implement the standard\n    ``__eq__`` or ``__hash__``, the default implementations here will not\n    work.\n\n    Examples\n    --------\n\n    For interaction with Apache Arrow (pyarrow), a ``__from_arrow__`` method\n    can be implemented: this method receives a pyarrow Array or ChunkedArray\n    as only argument and is expected to return the appropriate pandas\n    ExtensionArray for this dtype and the passed values:\n\n    >>> import pyarrow\n    >>> from pandas.api.extensions import ExtensionArray\n    >>> class ExtensionDtype:\n    ...     def __from_arrow__(\n    ...         self,\n    ...         array: pyarrow.Array | pyarrow.ChunkedArray\n    ...     ) -> ExtensionArray:\n    ...         ...\n\n    This class does not inherit from 'abc.ABCMeta' for performance reasons.\n    Methods and properties required by the interface raise\n    ``pandas.errors.AbstractMethodError`` and no ``register`` method is\n    provided for registering virtual subclasses.\n    \"\"\"\n\n    _metadata: tuple[str, ...] = ()\n\n    def __str__(self) -> str:\n        return self.name\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"\n        Check whether 'other' is equal to self.\n\n        By default, 'other' is considered equal if either\n\n        * it's a string matching 'self.name'.\n        * it's an instance of this type and all of the attributes\n          in ``self._metadata`` are equal between `self` and `other`.\n\n        Parameters\n        ----------\n        other : Any\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if isinstance(other, str):\n            try:\n                other = self.construct_from_string(other)\n            except TypeError:\n                return False\n        if isinstance(other, type(self)):\n            return all(\n                getattr(self, attr) == getattr(other, attr) for attr in self._metadata\n            )\n        return False\n\n    def __hash__(self) -> int:\n        # for python>=3.10, different nan objects have different hashes\n        # we need to avoid that and thus use hash function with old behavior\n        return object_hash(tuple(getattr(self, attr) for attr in self._metadata))\n\n    def __ne__(self, other: object) -> bool:\n        return not self.__eq__(other)\n\n    @property\n    def na_value(self) -> object:\n        \"\"\"\n        Default NA value to use for this type.\n\n        This is used in e.g. ExtensionArray.take. This should be the\n        user-facing \"boxed\" version of the NA value, not the physical NA value\n        for storage.  e.g. for JSONArray, this is an empty dictionary.\n        \"\"\"\n        return np.nan\n\n    @property\n    def type(self) -> type_t[Any]:\n        \"\"\"\n        The scalar type for the array, e.g. ``int``\n\n        It's expected ``ExtensionArray[item]`` returns an instance\n        of ``ExtensionDtype.type`` for scalar ``item``, assuming\n        that value is valid (not NA). NA values do not need to be\n        instances of `type`.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def kind(self) -> str:\n        \"\"\"\n        A character code (one of 'biufcmMOSUV'), default 'O'\n\n        This should match the NumPy dtype used when the array is\n        converted to an ndarray, which is probably 'O' for object if\n        the extension type cannot be represented as a built-in NumPy\n        type.\n\n        See Also\n        --------\n        numpy.dtype.kind\n        \"\"\"\n        return \"O\"\n\n    @property\n    def name(self) -> str:\n        \"\"\"\n        A string identifying the data type.\n\n        Will be used for display in, e.g. ``Series.dtype``\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def names(self) -> list[str] | None:\n        \"\"\"\n        Ordered list of field names, or None if there are no fields.\n\n        This is for compatibility with NumPy arrays, and may be removed in the\n        future.\n        \"\"\"\n        return None\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[ExtensionArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    def empty(self, shape: Shape) -> ExtensionArray:\n        \"\"\"\n        Construct an ExtensionArray of this dtype with the given shape.\n\n        Analogous to numpy.empty.\n\n        Parameters\n        ----------\n        shape : int or tuple[int]\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        cls = self.construct_array_type()\n        return cls._empty(shape, dtype=self)\n\n    @classmethod\n    def construct_from_string(cls, string: str) -> Self:\n        r\"\"\"\n        Construct this type from a string.\n\n        This is useful mainly for data types that accept parameters.\n        For example, a period dtype accepts a frequency parameter that\n        can be set as ``period[h]`` (where H means hourly frequency).\n\n        By default, in the abstract class, just the name of the type is\n        expected. But subclasses can overwrite this method to accept\n        parameters.\n\n        Parameters\n        ----------\n        string : str\n            The name of the type, for example ``category``.\n\n        Returns\n        -------\n        ExtensionDtype\n            Instance of the dtype.\n\n        Raises\n        ------\n        TypeError\n            If a class cannot be constructed from this 'string'.\n\n        Examples\n        --------\n        For extension dtypes with arguments the following may be an\n        adequate implementation.\n\n        >>> import re\n        >>> @classmethod\n        ... def construct_from_string(cls, string):\n        ...     pattern = re.compile(r\"^my_type\\[(?P<arg_name>.+)\\]$\")\n        ...     match = pattern.match(string)\n        ...     if match:\n        ...         return cls(**match.groupdict())\n        ...     else:\n        ...         raise TypeError(\n        ...             f\"Cannot construct a '{cls.__name__}' from '{string}'\"\n        ...         )\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n        # error: Non-overlapping equality check (left operand type: \"str\", right\n        #  operand type: \"Callable[[ExtensionDtype], str]\")  [comparison-overlap]\n        assert isinstance(cls.name, str), (cls, type(cls.name))\n        if string != cls.name:\n            raise TypeError(f\"Cannot construct a '{cls.__name__}' from '{string}'\")\n        return cls()\n\n    @classmethod\n    def is_dtype(cls, dtype: object) -> bool:\n        \"\"\"\n        Check if we match 'dtype'.\n\n        Parameters\n        ----------\n        dtype : object\n            The object to check.\n\n        Returns\n        -------\n        bool\n\n        Notes\n        -----\n        The default implementation is True if\n\n        1. ``cls.construct_from_string(dtype)`` is an instance\n           of ``cls``.\n        2. ``dtype`` is an object and is an instance of ``cls``\n        3. ``dtype`` has a ``dtype`` attribute, and any of the above\n           conditions is true for ``dtype.dtype``.\n        \"\"\"\n        dtype = getattr(dtype, \"dtype\", dtype)\n\n        if isinstance(dtype, (ABCSeries, ABCIndex, ABCDataFrame, np.dtype)):\n            # https://github.com/pandas-dev/pandas/issues/22960\n            # avoid passing data to `construct_from_string`. This could\n            # cause a FutureWarning from numpy about failing elementwise\n            # comparison from, e.g., comparing DataFrame == 'category'.\n            return False\n        elif dtype is None:\n            return False\n        elif isinstance(dtype, cls):\n            return True\n        if isinstance(dtype, str):\n            try:\n                return cls.construct_from_string(dtype) is not None\n            except TypeError:\n                return False\n        return False\n\n    @property\n    def _is_numeric(self) -> bool:\n        \"\"\"\n        Whether columns with this dtype should be considered numeric.\n\n        By default ExtensionDtypes are assumed to be non-numeric.\n        They'll be excluded from operations that exclude non-numeric\n        columns, like (groupby) reductions, plotting, etc.\n        \"\"\"\n        return False\n\n    @property\n    def _is_boolean(self) -> bool:\n        \"\"\"\n        Whether this dtype should be considered boolean.\n\n        By default, ExtensionDtypes are assumed to be non-numeric.\n        Setting this to True will affect the behavior of several places,\n        e.g.\n\n        * is_bool\n        * boolean indexing\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return False\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        \"\"\"\n        Return the common dtype, if one exists.\n\n        Used in `find_common_type` implementation. This is for example used\n        to determine the resulting dtype in a concat operation.\n\n        If no common dtype exists, return None (which gives the other dtypes\n        the chance to determine a common dtype). If all dtypes in the list\n        return None, then the common dtype will be \"object\" dtype (this means\n        it is never needed to return \"object\" dtype from this method itself).\n\n        Parameters\n        ----------\n        dtypes : list of dtypes\n            The dtypes for which to determine a common dtype. This is a list\n            of np.dtype or ExtensionDtype instances.\n\n        Returns\n        -------\n        Common dtype (np.dtype or ExtensionDtype) or None\n        \"\"\"\n        if len(set(dtypes)) == 1:\n            # only itself\n            return self\n        else:\n            return None\n\n    @property\n    def _can_hold_na(self) -> bool:\n        \"\"\"\n        Can arrays of this dtype hold NA values?\n        \"\"\"\n        return True\n\n    @property\n    def _is_immutable(self) -> bool:\n        \"\"\"\n        Can arrays with this dtype be modified with __setitem__? If not, return\n        True.\n\n        Immutable arrays are expected to raise TypeError on __setitem__ calls.\n        \"\"\"\n        return False\n\n    @cache_readonly\n    def index_class(self) -> type_t[Index]:\n        \"\"\"\n        The Index subclass to return from Index.__new__ when this dtype is\n        encountered.\n        \"\"\"\n        from pandas import Index\n\n        return Index\n\n    @property\n    def _supports_2d(self) -> bool:\n        \"\"\"\n        Do ExtensionArrays with this dtype support 2D arrays?\n\n        Historically ExtensionArrays were limited to 1D. By returning True here,\n        authors can indicate that their arrays support 2D instances. This can\n        improve performance in some cases, particularly operations with `axis=1`.\n\n        Arrays that support 2D values should:\n\n            - implement Array.reshape\n            - subclass the Dim2CompatTests in tests.extension.base\n            - _concat_same_type should support `axis` keyword\n            - _reduce and reductions should support `axis` keyword\n        \"\"\"\n        return False\n\n    @property\n    def _can_fast_transpose(self) -> bool:\n        \"\"\"\n        Is transposing an array with this dtype zero-copy?\n\n        Only relevant for cases where _supports_2d is True.\n        \"\"\"\n        return False\n", "class_fn": true, "question_id": "pandas/pandas.core.dtypes.base/ExtensionDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/dtypes/dtypes.py", "fn_id": "", "content": "class BaseMaskedDtype(ExtensionDtype):\n    \"\"\"\n    Base class for dtypes for BaseMaskedArray subclasses.\n    \"\"\"\n\n    base = None\n    type: type\n\n    @property\n    def na_value(self) -> libmissing.NAType:\n        return libmissing.NA\n\n    @cache_readonly\n    def numpy_dtype(self) -> np.dtype:\n        \"\"\"Return an instance of our numpy dtype\"\"\"\n        return np.dtype(self.type)\n\n    @cache_readonly\n    def kind(self) -> str:\n        return self.numpy_dtype.kind\n\n    @cache_readonly\n    def itemsize(self) -> int:\n        \"\"\"Return the number of bytes in this dtype\"\"\"\n        return self.numpy_dtype.itemsize\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[BaseMaskedArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def from_numpy_dtype(cls, dtype: np.dtype) -> BaseMaskedDtype:\n        \"\"\"\n        Construct the MaskedDtype corresponding to the given numpy dtype.\n        \"\"\"\n        if dtype.kind == \"b\":\n            from pandas.core.arrays.boolean import BooleanDtype\n\n            return BooleanDtype()\n        elif dtype.kind in \"iu\":\n            from pandas.core.arrays.integer import NUMPY_INT_TO_DTYPE\n\n            return NUMPY_INT_TO_DTYPE[dtype]\n        elif dtype.kind == \"f\":\n            from pandas.core.arrays.floating import NUMPY_FLOAT_TO_DTYPE\n\n            return NUMPY_FLOAT_TO_DTYPE[dtype]\n        else:\n            raise NotImplementedError(dtype)\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        # We unwrap any masked dtypes, find the common dtype we would use\n        #  for that, then re-mask the result.\n        from pandas.core.dtypes.cast import find_common_type\n\n        new_dtype = find_common_type(\n            [\n                dtype.numpy_dtype if isinstance(dtype, BaseMaskedDtype) else dtype\n                for dtype in dtypes\n            ]\n        )\n        if not isinstance(new_dtype, np.dtype):\n            # If we ever support e.g. Masked[DatetimeArray] then this will change\n            return None\n        try:\n            return type(self).from_numpy_dtype(new_dtype)\n        except (KeyError, NotImplementedError):\n            return None\n", "class_fn": true, "question_id": "pandas/pandas.core.dtypes.dtypes/BaseMaskedDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/dtypes/dtypes.py", "fn_id": "", "content": "@register_extension_dtype\nclass DatetimeTZDtype(PandasExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for timezone-aware datetime data.\n\n    **This is not an actual numpy dtype**, but a duck type.\n\n    Parameters\n    ----------\n    unit : str, default \"ns\"\n        The precision of the datetime data. Currently limited\n        to ``\"ns\"``.\n    tz : str, int, or datetime.tzinfo\n        The timezone.\n\n    Attributes\n    ----------\n    unit\n    tz\n\n    Methods\n    -------\n    None\n\n    Raises\n    ------\n    ZoneInfoNotFoundError\n        When the requested timezone cannot be found.\n\n    Examples\n    --------\n    >>> from zoneinfo import ZoneInfo\n    >>> pd.DatetimeTZDtype(tz=ZoneInfo('UTC'))\n    datetime64[ns, UTC]\n\n    >>> pd.DatetimeTZDtype(tz=ZoneInfo('Europe/Paris'))\n    datetime64[ns, Europe/Paris]\n    \"\"\"\n\n    type: type[Timestamp] = Timestamp\n    kind: str_type = \"M\"\n    num = 101\n    _metadata = (\"unit\", \"tz\")\n    _match = re.compile(r\"(datetime64|M8)\\[(?P<unit>.+), (?P<tz>.+)\\]\")\n    _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}\n    _supports_2d = True\n    _can_fast_transpose = True\n\n    @property\n    def na_value(self) -> NaTType:\n        return NaT\n\n    @cache_readonly\n    def base(self) -> DtypeObj:  # type: ignore[override]\n        return np.dtype(f\"M8[{self.unit}]\")\n\n    # error: Signature of \"str\" incompatible with supertype \"PandasExtensionDtype\"\n    @cache_readonly\n    def str(self) -> str:  # type: ignore[override]\n        return f\"|M8[{self.unit}]\"\n\n    def __init__(self, unit: str_type | DatetimeTZDtype = \"ns\", tz=None) -> None:\n        if isinstance(unit, DatetimeTZDtype):\n            # error: \"str\" has no attribute \"tz\"\n            unit, tz = unit.unit, unit.tz  # type: ignore[attr-defined]\n\n        if unit != \"ns\":\n            if isinstance(unit, str) and tz is None:\n                # maybe a string like datetime64[ns, tz], which we support for\n                # now.\n                result = type(self).construct_from_string(unit)\n                unit = result.unit\n                tz = result.tz\n                msg = (\n                    f\"Passing a dtype alias like 'datetime64[ns, {tz}]' \"\n                    \"to DatetimeTZDtype is no longer supported. Use \"\n                    \"'DatetimeTZDtype.construct_from_string()' instead.\"\n                )\n                raise ValueError(msg)\n            if unit not in [\"s\", \"ms\", \"us\", \"ns\"]:\n                raise ValueError(\"DatetimeTZDtype only supports s, ms, us, ns units\")\n\n        if tz:\n            tz = timezones.maybe_get_tz(tz)\n            tz = timezones.tz_standardize(tz)\n        elif tz is not None:\n            raise pytz.UnknownTimeZoneError(tz)\n        if tz is None:\n            raise TypeError(\"A 'tz' is required.\")\n\n        self._unit = unit\n        self._tz = tz\n\n    @cache_readonly\n    def _creso(self) -> int:\n        \"\"\"\n        The NPY_DATETIMEUNIT corresponding to this dtype's resolution.\n        \"\"\"\n        return abbrev_to_npy_unit(self.unit)\n\n    @property\n    def unit(self) -> str_type:\n        \"\"\"\n        The precision of the datetime data.\n\n        Examples\n        --------\n        >>> from zoneinfo import ZoneInfo\n        >>> dtype = pd.DatetimeTZDtype(tz=ZoneInfo('America/Los_Angeles'))\n        >>> dtype.unit\n        'ns'\n        \"\"\"\n        return self._unit\n\n    @property\n    def tz(self) -> tzinfo:\n        \"\"\"\n        The timezone.\n\n        Examples\n        --------\n        >>> from zoneinfo import ZoneInfo\n        >>> dtype = pd.DatetimeTZDtype(tz=ZoneInfo('America/Los_Angeles'))\n        >>> dtype.tz\n        zoneinfo.ZoneInfo(key='America/Los_Angeles')\n        \"\"\"\n        return self._tz\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[DatetimeArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays import DatetimeArray\n\n        return DatetimeArray\n\n    @classmethod\n    def construct_from_string(cls, string: str_type) -> DatetimeTZDtype:\n        \"\"\"\n        Construct a DatetimeTZDtype from a string.\n\n        Parameters\n        ----------\n        string : str\n            The string alias for this DatetimeTZDtype.\n            Should be formatted like ``datetime64[ns, <tz>]``,\n            where ``<tz>`` is the timezone name.\n\n        Examples\n        --------\n        >>> DatetimeTZDtype.construct_from_string('datetime64[ns, UTC]')\n        datetime64[ns, UTC]\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n\n        msg = f\"Cannot construct a 'DatetimeTZDtype' from '{string}'\"\n        match = cls._match.match(string)\n        if match:\n            d = match.groupdict()\n            try:\n                return cls(unit=d[\"unit\"], tz=d[\"tz\"])\n            except (KeyError, TypeError, ValueError) as err:\n                # KeyError if maybe_get_tz tries and fails to get a\n                #  pytz timezone (actually pytz.UnknownTimeZoneError).\n                # TypeError if we pass a nonsense tz;\n                # ValueError if we pass a unit other than \"ns\"\n                raise TypeError(msg) from err\n        raise TypeError(msg)\n\n    def __str__(self) -> str_type:\n        return f\"datetime64[{self.unit}, {self.tz}]\"\n\n    @property\n    def name(self) -> str_type:\n        \"\"\"A string representation of the dtype.\"\"\"\n        return str(self)\n\n    def __hash__(self) -> int:\n        # make myself hashable\n        # TODO: update this.\n        return hash(str(self))\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, str):\n            if other.startswith(\"M8[\"):\n                other = f\"datetime64[{other[3:]}\"\n            return other == self.name\n\n        return (\n            isinstance(other, DatetimeTZDtype)\n            and self.unit == other.unit\n            and tz_compare(self.tz, other.tz)\n        )\n\n    def __from_arrow__(self, array: pa.Array | pa.ChunkedArray) -> DatetimeArray:\n        \"\"\"\n        Construct DatetimeArray from pyarrow Array/ChunkedArray.\n\n        Note: If the units in the pyarrow Array are the same as this\n        DatetimeDtype, then values corresponding to the integer representation\n        of ``NaT`` (e.g. one nanosecond before :attr:`pandas.Timestamp.min`)\n        are converted to ``NaT``, regardless of the null indicator in the\n        pyarrow array.\n\n        Parameters\n        ----------\n        array : pyarrow.Array or pyarrow.ChunkedArray\n            The Arrow array to convert to DatetimeArray.\n\n        Returns\n        -------\n        extension array : DatetimeArray\n        \"\"\"\n        import pyarrow\n\n        from pandas.core.arrays import DatetimeArray\n\n        array = array.cast(pyarrow.timestamp(unit=self._unit), safe=True)\n\n        if isinstance(array, pyarrow.Array):\n            np_arr = array.to_numpy(zero_copy_only=False)\n        else:\n            np_arr = array.to_numpy()\n\n        return DatetimeArray._simple_new(np_arr, dtype=self)\n\n    def __setstate__(self, state) -> None:\n        # for pickle compat. __get_state__ is defined in the\n        # PandasExtensionDtype superclass and uses the public properties to\n        # pickle -> need to set the settable private ones here (see GH26067)\n        self._tz = state[\"tz\"]\n        self._unit = state[\"unit\"]\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        if all(isinstance(t, DatetimeTZDtype) and t.tz == self.tz for t in dtypes):\n            np_dtype = np.max([cast(DatetimeTZDtype, t).base for t in [self, *dtypes]])\n            unit = np.datetime_data(np_dtype)[0]\n            return type(self)(unit=unit, tz=self.tz)\n        return super()._get_common_dtype(dtypes)\n\n    @cache_readonly\n    def index_class(self) -> type_t[DatetimeIndex]:\n        from pandas import DatetimeIndex\n\n        return DatetimeIndex\n", "class_fn": true, "question_id": "pandas/pandas.core.dtypes.dtypes/DatetimeTZDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/dtypes/dtypes.py", "fn_id": "", "content": "@register_extension_dtype\nclass PeriodDtype(PeriodDtypeBase, PandasExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for Period data.\n\n    **This is not an actual numpy dtype**, but a duck type.\n\n    Parameters\n    ----------\n    freq : str or DateOffset\n        The frequency of this PeriodDtype.\n\n    Attributes\n    ----------\n    freq\n\n    Methods\n    -------\n    None\n\n    Examples\n    --------\n    >>> pd.PeriodDtype(freq='D')\n    period[D]\n\n    >>> pd.PeriodDtype(freq=pd.offsets.MonthEnd())\n    period[M]\n    \"\"\"\n\n    type: type[Period] = Period\n    kind: str_type = \"O\"\n    str = \"|O08\"\n    base = np.dtype(\"O\")\n    num = 102\n    _metadata = (\"freq\",)\n    _match = re.compile(r\"(P|p)eriod\\[(?P<freq>.+)\\]\")\n    # error: Incompatible types in assignment (expression has type\n    # \"Dict[int, PandasExtensionDtype]\", base class \"PandasExtensionDtype\"\n    # defined the type as \"Dict[str, PandasExtensionDtype]\")  [assignment]\n    _cache_dtypes: dict[BaseOffset, int] = {}  # type: ignore[assignment]\n    __hash__ = PeriodDtypeBase.__hash__\n    _freq: BaseOffset\n    _supports_2d = True\n    _can_fast_transpose = True\n\n    def __new__(cls, freq) -> PeriodDtype:  # noqa: PYI034\n        \"\"\"\n        Parameters\n        ----------\n        freq : PeriodDtype, BaseOffset, or string\n        \"\"\"\n        if isinstance(freq, PeriodDtype):\n            return freq\n\n        if not isinstance(freq, BaseOffset):\n            freq = cls._parse_dtype_strict(freq)\n\n        if isinstance(freq, BDay):\n            # GH#53446\n            # TODO(3.0): enforcing this will close GH#10575\n            warnings.warn(\n                \"PeriodDtype[B] is deprecated and will be removed in a future \"\n                \"version. Use a DatetimeIndex with freq='B' instead\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        try:\n            dtype_code = cls._cache_dtypes[freq]\n        except KeyError:\n            dtype_code = freq._period_dtype_code\n            cls._cache_dtypes[freq] = dtype_code\n        u = PeriodDtypeBase.__new__(cls, dtype_code, freq.n)\n        u._freq = freq\n        return u\n\n    def __reduce__(self) -> tuple[type_t[Self], tuple[str_type]]:\n        return type(self), (self.name,)\n\n    @property\n    def freq(self) -> BaseOffset:\n        \"\"\"\n        The frequency object of this PeriodDtype.\n\n        Examples\n        --------\n        >>> dtype = pd.PeriodDtype(freq='D')\n        >>> dtype.freq\n        <Day>\n        \"\"\"\n        return self._freq\n\n    @classmethod\n    def _parse_dtype_strict(cls, freq: str_type) -> BaseOffset:\n        if isinstance(freq, str):  # note: freq is already of type str!\n            if freq.startswith((\"Period[\", \"period[\")):\n                m = cls._match.search(freq)\n                if m is not None:\n                    freq = m.group(\"freq\")\n\n            freq_offset = to_offset(freq, is_period=True)\n            if freq_offset is not None:\n                return freq_offset\n\n        raise TypeError(\n            \"PeriodDtype argument should be string or BaseOffset, \"\n            f\"got {type(freq).__name__}\"\n        )\n\n    @classmethod\n    def construct_from_string(cls, string: str_type) -> PeriodDtype:\n        \"\"\"\n        Strict construction from a string, raise a TypeError if not\n        possible\n        \"\"\"\n        if (\n            isinstance(string, str)\n            and (string.startswith((\"period[\", \"Period[\")))\n            or isinstance(string, BaseOffset)\n        ):\n            # do not parse string like U as period[U]\n            # avoid tuple to be regarded as freq\n            try:\n                return cls(freq=string)\n            except ValueError:\n                pass\n        if isinstance(string, str):\n            msg = f\"Cannot construct a 'PeriodDtype' from '{string}'\"\n        else:\n            msg = f\"'construct_from_string' expects a string, got {type(string)}\"\n        raise TypeError(msg)\n\n    def __str__(self) -> str_type:\n        return self.name\n\n    @property\n    def name(self) -> str_type:\n        return f\"period[{self._freqstr}]\"\n\n    @property\n    def na_value(self) -> NaTType:\n        return NaT\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, str):\n            return other in [self.name, capitalize_first_letter(self.name)]\n\n        return super().__eq__(other)\n\n    def __ne__(self, other: object) -> bool:\n        return not self.__eq__(other)\n\n    @classmethod\n    def is_dtype(cls, dtype: object) -> bool:\n        \"\"\"\n        Return a boolean if we if the passed type is an actual dtype that we\n        can match (via string or type)\n        \"\"\"\n        if isinstance(dtype, str):\n            # PeriodDtype can be instantiated from freq string like \"U\",\n            # but doesn't regard freq str like \"U\" as dtype.\n            if dtype.startswith((\"period[\", \"Period[\")):\n                try:\n                    return cls._parse_dtype_strict(dtype) is not None\n                except ValueError:\n                    return False\n            else:\n                return False\n        return super().is_dtype(dtype)\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[PeriodArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays import PeriodArray\n\n        return PeriodArray\n\n    def __from_arrow__(self, array: pa.Array | pa.ChunkedArray) -> PeriodArray:\n        \"\"\"\n        Construct PeriodArray from pyarrow Array/ChunkedArray.\n        \"\"\"\n        import pyarrow\n\n        from pandas.core.arrays import PeriodArray\n        from pandas.core.arrays.arrow._arrow_utils import (\n            pyarrow_array_to_numpy_and_mask,\n        )\n\n        if isinstance(array, pyarrow.Array):\n            chunks = [array]\n        else:\n            chunks = array.chunks\n\n        results = []\n        for arr in chunks:\n            data, mask = pyarrow_array_to_numpy_and_mask(arr, dtype=np.dtype(np.int64))\n            parr = PeriodArray(data.copy(), dtype=self, copy=False)\n            # error: Invalid index type \"ndarray[Any, dtype[bool_]]\" for \"PeriodArray\";\n            # expected type \"Union[int, Sequence[int], Sequence[bool], slice]\"\n            parr[~mask] = NaT  # type: ignore[index]\n            results.append(parr)\n\n        if not results:\n            return PeriodArray(np.array([], dtype=\"int64\"), dtype=self, copy=False)\n        return PeriodArray._concat_same_type(results)\n\n    @cache_readonly\n    def index_class(self) -> type_t[PeriodIndex]:\n        from pandas import PeriodIndex\n\n        return PeriodIndex\n", "class_fn": true, "question_id": "pandas/pandas.core.dtypes.dtypes/PeriodDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/frame.py", "fn_id": "", "content": "class DataFrame(NDFrame, OpsMixin):\n    \"\"\"\n    Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).\n    Arithmetic operations align on both row and column labels. Can be\n    thought of as a dict-like container for Series objects. The primary\n    pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n        data is a dict, column order follows insertion-order. If a dict contains Series\n        which have an index defined, it is aligned by its index. This alignment also\n        occurs if data is a Series or a DataFrame itself. Alignment is done on\n        Series/DataFrame inputs.\n\n        If data is a list of dicts, column order follows insertion-order.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided.\n    columns : Index or array-like\n        Column labels to use for resulting frame when data does not have them,\n        defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n        will perform column selection instead.\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer.\n    copy : bool or None, default None\n        Copy data from inputs.\n        For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n        or 2d ndarray input, the default of None behaves like ``copy=False``.\n        If data is a dict containing one or more Series (possibly of different dtypes),\n        ``copy=False`` will ensure that these inputs are not copied.\n\n        .. versionchanged:: 1.3.0\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    read_csv : Read a comma-separated values (csv) file into DataFrame.\n    read_table : Read general delimited file into DataFrame.\n    read_clipboard : Read text from clipboard into DataFrame.\n\n    Notes\n    -----\n    Please reference the :ref:`User Guide <basics.dataframe>` for more information.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from a dictionary including Series:\n\n    >>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n    >>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n       col1  col2\n    0     0   NaN\n    1     1   NaN\n    2     2   2.0\n    3     3   3.0\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n\n    Constructing DataFrame from a numpy ndarray that has labeled columns:\n\n    >>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n    ...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n    >>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n    ...\n    >>> df3\n       c  a\n    0  3  1\n    1  6  4\n    2  9  7\n\n    Constructing DataFrame from dataclass:\n\n    >>> from dataclasses import make_dataclass\n    >>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n    >>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n       x  y\n    0  0  0\n    1  0  3\n    2  2  3\n\n    Constructing DataFrame from Series/DataFrame:\n\n    >>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n    >>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n    >>> df\n       0\n    a  1\n    c  3\n\n    >>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n    >>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n    >>> df2\n       x\n    a  1\n    c  3\n    \"\"\"\n\n    _internal_names_set = {\"columns\", \"index\"} | NDFrame._internal_names_set\n    _typ = \"dataframe\"\n    _HANDLED_TYPES = (Series, Index, ExtensionArray, np.ndarray)\n    _accessors: set[str] = {\"sparse\"}\n    _hidden_attrs: frozenset[str] = NDFrame._hidden_attrs | frozenset([])\n    _mgr: BlockManager | ArrayManager\n\n    # similar to __array_priority__, positions DataFrame before Series, Index,\n    #  and ExtensionArray.  Should NOT be overridden by subclasses.\n    __pandas_priority__ = 4000\n\n    @property\n    def _constructor(self) -> Callable[..., DataFrame]:\n        return DataFrame\n\n    def _constructor_from_mgr(self, mgr, axes) -> DataFrame:\n        df = DataFrame._from_mgr(mgr, axes=axes)\n\n        if type(self) is DataFrame:\n            # This would also work `if self._constructor is DataFrame`, but\n            #  this check is slightly faster, benefiting the most-common case.\n            return df\n\n        elif type(self).__name__ == \"GeoDataFrame\":\n            # Shim until geopandas can override their _constructor_from_mgr\n            #  bc they have different behavior for Managers than for DataFrames\n            return self._constructor(mgr)\n\n        # We assume that the subclass __init__ knows how to handle a\n        #  pd.DataFrame object.\n        return self._constructor(df)\n\n    _constructor_sliced: Callable[..., Series] = Series\n\n    def _constructor_sliced_from_mgr(self, mgr, axes) -> Series:\n        ser = Series._from_mgr(mgr, axes)\n        ser._name = None  # caller is responsible for setting real name\n\n        if type(self) is DataFrame:\n            # This would also work `if self._constructor_sliced is Series`, but\n            #  this check is slightly faster, benefiting the most-common case.\n            return ser\n\n        # We assume that the subclass __init__ knows how to handle a\n        #  pd.Series object.\n        return self._constructor_sliced(ser)\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self,\n        data=None,\n        index: Axes | None = None,\n        columns: Axes | None = None,\n        dtype: Dtype | None = None,\n        copy: bool | None = None,\n    ) -> None:\n        allow_mgr = False\n        if dtype is not None:\n            dtype = self._validate_dtype(dtype)\n\n        if isinstance(data, DataFrame):\n            data = data._mgr\n            allow_mgr = True\n            if not copy:\n                # if not copying data, ensure to still return a shallow copy\n                # to avoid the result sharing the same Manager\n                data = data.copy(deep=False)\n\n        if isinstance(data, (BlockManager, ArrayManager)):\n            if not allow_mgr:\n                # GH#52419\n                warnings.warn(\n                    f\"Passing a {type(data).__name__} to {type(self).__name__} \"\n                    \"is deprecated and will raise in a future version. \"\n                    \"Use public APIs instead.\",\n                    DeprecationWarning,\n                    stacklevel=1,  # bump to 2 once pyarrow 15.0 is released with fix\n                )\n\n            if using_copy_on_write():\n                data = data.copy(deep=False)\n            # first check if a Manager is passed without any other arguments\n            # -> use fastpath (without checking Manager type)\n            if index is None and columns is None and dtype is None and not copy:\n                # GH#33357 fastpath\n                NDFrame.__init__(self, data)\n                return\n\n        manager = _get_option(\"mode.data_manager\", silent=True)\n\n        is_pandas_object = isinstance(data, (Series, Index, ExtensionArray))\n        data_dtype = getattr(data, \"dtype\", None)\n        original_dtype = dtype\n\n        # GH47215\n        if isinstance(index, set):\n            raise ValueError(\"index cannot be a set\")\n        if isinstance(columns, set):\n            raise ValueError(\"columns cannot be a set\")\n\n        if copy is None:\n            if isinstance(data, dict):\n                # retain pre-GH#38939 default behavior\n                copy = True\n            elif (\n                manager == \"array\"\n                and isinstance(data, (np.ndarray, ExtensionArray))\n                and data.ndim == 2\n            ):\n                # INFO(ArrayManager) by default copy the 2D input array to get\n                # contiguous 1D arrays\n                copy = True\n            elif using_copy_on_write() and not isinstance(\n                data, (Index, DataFrame, Series)\n            ):\n                copy = True\n            else:\n                copy = False\n\n        if data is None:\n            index = index if index is not None else default_index(0)\n            columns = columns if columns is not None else default_index(0)\n            dtype = dtype if dtype is not None else pandas_dtype(object)\n            data = []\n\n        if isinstance(data, (BlockManager, ArrayManager)):\n            mgr = self._init_mgr(\n                data, axes={\"index\": index, \"columns\": columns}, dtype=dtype, copy=copy\n            )\n\n        elif isinstance(data, dict):\n            # GH#38939 de facto copy defaults to False only in non-dict cases\n            mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n        elif isinstance(data, ma.MaskedArray):\n            from numpy.ma import mrecords\n\n            # masked recarray\n            if isinstance(data, mrecords.MaskedRecords):\n                raise TypeError(\n                    \"MaskedRecords are not supported. Pass \"\n                    \"{name: data[name] for name in data.dtype.names} \"\n                    \"instead\"\n                )\n\n            # a masked array\n            data = sanitize_masked_array(data)\n            mgr = ndarray_to_mgr(\n                data,\n                index,\n                columns,\n                dtype=dtype,\n                copy=copy,\n                typ=manager,\n            )\n\n        elif isinstance(data, (np.ndarray, Series, Index, ExtensionArray)):\n            if data.dtype.names:\n                # i.e. numpy structured array\n                data = cast(np.ndarray, data)\n                mgr = rec_array_to_mgr(\n                    data,\n                    index,\n                    columns,\n                    dtype,\n                    copy,\n                    typ=manager,\n                )\n            elif getattr(data, \"name\", None) is not None:\n                # i.e. Series/Index with non-None name\n                _copy = copy if using_copy_on_write() else True\n                mgr = dict_to_mgr(\n                    # error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\n                    # attribute \"name\"\n                    {data.name: data},  # type: ignore[union-attr]\n                    index,\n                    columns,\n                    dtype=dtype,\n                    typ=manager,\n                    copy=_copy,\n                )\n            else:\n                mgr = ndarray_to_mgr(\n                    data,\n                    index,\n                    columns,\n                    dtype=dtype,\n                    copy=copy,\n                    typ=manager,\n                )\n\n        # For data is list-like, or Iterable (will consume into list)\n        elif is_list_like(data):\n            if not isinstance(data, abc.Sequence):\n                if hasattr(data, \"__array__\"):\n                    # GH#44616 big perf improvement for e.g. pytorch tensor\n                    data = np.asarray(data)\n                else:\n                    data = list(data)\n            if len(data) > 0:\n                if is_dataclass(data[0]):\n                    data = dataclasses_to_dicts(data)\n                if not isinstance(data, np.ndarray) and treat_as_nested(data):\n                    # exclude ndarray as we may have cast it a few lines above\n                    if columns is not None:\n                        columns = ensure_index(columns)\n                    arrays, columns, index = nested_data_to_arrays(\n                        # error: Argument 3 to \"nested_data_to_arrays\" has incompatible\n                        # type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\n                        data,\n                        columns,\n                        index,  # type: ignore[arg-type]\n                        dtype,\n                    )\n                    mgr = arrays_to_mgr(\n                        arrays,\n                        columns,\n                        index,\n                        dtype=dtype,\n                        typ=manager,\n                    )\n                else:\n                    mgr = ndarray_to_mgr(\n                        data,\n                        index,\n                        columns,\n                        dtype=dtype,\n                        copy=copy,\n                        typ=manager,\n                    )\n            else:\n                mgr = dict_to_mgr(\n                    {},\n                    index,\n                    columns if columns is not None else default_index(0),\n                    dtype=dtype,\n                    typ=manager,\n                )\n        # For data is scalar\n        else:\n            if index is None or columns is None:\n                raise ValueError(\"DataFrame constructor not properly called!\")\n\n            index = ensure_index(index)\n            columns = ensure_index(columns)\n\n            if not dtype:\n                dtype, _ = infer_dtype_from_scalar(data)\n\n            # For data is a scalar extension dtype\n            if isinstance(dtype, ExtensionDtype):\n                # TODO(EA2D): special case not needed with 2D EAs\n\n                values = [\n                    construct_1d_arraylike_from_scalar(data, len(index), dtype)\n                    for _ in range(len(columns))\n                ]\n                mgr = arrays_to_mgr(values, columns, index, dtype=None, typ=manager)\n            else:\n                arr2d = construct_2d_arraylike_from_scalar(\n                    data,\n                    len(index),\n                    len(columns),\n                    dtype,\n                    copy,\n                )\n\n                mgr = ndarray_to_mgr(\n                    arr2d,\n                    index,\n                    columns,\n                    dtype=arr2d.dtype,\n                    copy=False,\n                    typ=manager,\n                )\n\n        # ensure correct Manager type according to settings\n        mgr = mgr_to_mgr(mgr, typ=manager)\n\n        NDFrame.__init__(self, mgr)\n\n        if original_dtype is None and is_pandas_object and data_dtype == np.object_:\n            if self.dtypes.iloc[0] != data_dtype:\n                warnings.warn(\n                    \"Dtype inference on a pandas object \"\n                    \"(Series, Index, ExtensionArray) is deprecated. The DataFrame \"\n                    \"constructor will keep the original dtype in the future. \"\n                    \"Call `infer_objects` on the result to get the old \"\n                    \"behavior.\",\n                    FutureWarning,\n                    stacklevel=2,\n                )\n\n    # ----------------------------------------------------------------------\n\n    def __dataframe__(\n        self, nan_as_null: bool = False, allow_copy: bool = True\n    ) -> DataFrameXchg:\n        \"\"\"\n        Return the dataframe interchange object implementing the interchange protocol.\n\n        Parameters\n        ----------\n        nan_as_null : bool, default False\n            `nan_as_null` is DEPRECATED and has no effect. Please avoid using\n            it; it will be removed in a future release.\n        allow_copy : bool, default True\n            Whether to allow memory copying when exporting. If set to False\n            it would cause non-zero-copy exports to fail.\n\n        Returns\n        -------\n        DataFrame interchange object\n            The object which consuming library can use to ingress the dataframe.\n\n        Notes\n        -----\n        Details on the interchange protocol:\n        https://data-apis.org/dataframe-protocol/latest/index.html\n\n        Examples\n        --------\n        >>> df_not_necessarily_pandas = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n        >>> interchange_object = df_not_necessarily_pandas.__dataframe__()\n        >>> interchange_object.column_names()\n        Index(['A', 'B'], dtype='object')\n        >>> df_pandas = (pd.api.interchange.from_dataframe\n        ...              (interchange_object.select_columns_by_name(['A'])))\n        >>> df_pandas\n             A\n        0    1\n        1    2\n\n        These methods (``column_names``, ``select_columns_by_name``) should work\n        for any dataframe library which implements the interchange protocol.\n        \"\"\"\n\n        from pandas.core.interchange.dataframe import PandasDataFrameXchg\n\n        return PandasDataFrameXchg(self, allow_copy=allow_copy)\n\n    def __dataframe_consortium_standard__(\n        self, *, api_version: str | None = None\n    ) -> Any:\n        \"\"\"\n        Provide entry point to the Consortium DataFrame Standard API.\n\n        This is developed and maintained outside of pandas.\n        Please report any issues to https://github.com/data-apis/dataframe-api-compat.\n        \"\"\"\n        dataframe_api_compat = import_optional_dependency(\"dataframe_api_compat\")\n        convert_to_standard_compliant_dataframe = (\n            dataframe_api_compat.pandas_standard.convert_to_standard_compliant_dataframe\n        )\n        return convert_to_standard_compliant_dataframe(self, api_version=api_version)\n\n    def __arrow_c_stream__(self, requested_schema=None):\n        \"\"\"\n        Export the pandas DataFrame as an Arrow C stream PyCapsule.\n\n        This relies on pyarrow to convert the pandas DataFrame to the Arrow\n        format (and follows the default behaviour of ``pyarrow.Table.from_pandas``\n        in its handling of the index, i.e. store the index as a column except\n        for RangeIndex).\n        This conversion is not necessarily zero-copy.\n\n        Parameters\n        ----------\n        requested_schema : PyCapsule, default None\n            The schema to which the dataframe should be casted, passed as a\n            PyCapsule containing a C ArrowSchema representation of the\n            requested schema.\n\n        Returns\n        -------\n        PyCapsule\n        \"\"\"\n        pa = import_optional_dependency(\"pyarrow\", min_version=\"14.0.0\")\n        if requested_schema is not None:\n            requested_schema = pa.Schema._import_from_c_capsule(requested_schema)\n        table = pa.Table.from_pandas(self, schema=requested_schema)\n        return table.__arrow_c_stream__()\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def axes(self) -> list[Index]:\n        \"\"\"\n        Return a list representing the axes of the DataFrame.\n\n        It has the row axis labels and column axis labels as the only members.\n        They are returned in that order.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.axes\n        [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'],\n        dtype='object')]\n        \"\"\"\n        return [self.index, self.columns]\n\n    @property\n    def shape(self) -> tuple[int, int]:\n        \"\"\"\n        Return a tuple representing the dimensionality of the DataFrame.\n\n        See Also\n        --------\n        ndarray.shape : Tuple of array dimensions.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.shape\n        (2, 2)\n\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4],\n        ...                    'col3': [5, 6]})\n        >>> df.shape\n        (2, 3)\n        \"\"\"\n        return len(self.index), len(self.columns)\n\n    @property\n    def _is_homogeneous_type(self) -> bool:\n        \"\"\"\n        Whether all the columns in a DataFrame have the same type.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3, 4]})._is_homogeneous_type\n        True\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.0]})._is_homogeneous_type\n        False\n\n        Items with the same type but different sizes are considered\n        different types.\n\n        >>> DataFrame({\n        ...    \"A\": np.array([1, 2], dtype=np.int32),\n        ...    \"B\": np.array([1, 2], dtype=np.int64)})._is_homogeneous_type\n        False\n        \"\"\"\n        # The \"<\" part of \"<=\" here is for empty DataFrame cases\n        return len({arr.dtype for arr in self._mgr.arrays}) <= 1\n\n    @property\n    def _can_fast_transpose(self) -> bool:\n        \"\"\"\n        Can we transpose this DataFrame without creating any new array objects.\n        \"\"\"\n        if isinstance(self._mgr, ArrayManager):\n            return False\n        blocks = self._mgr.blocks\n        if len(blocks) != 1:\n            return False\n\n        dtype = blocks[0].dtype\n        # TODO(EA2D) special case would be unnecessary with 2D EAs\n        return not is_1d_only_ea_dtype(dtype)\n\n    @property\n    def _values(self) -> np.ndarray | DatetimeArray | TimedeltaArray | PeriodArray:\n        \"\"\"\n        Analogue to ._values that may return a 2D ExtensionArray.\n        \"\"\"\n        mgr = self._mgr\n\n        if isinstance(mgr, ArrayManager):\n            if len(mgr.arrays) == 1 and not is_1d_only_ea_dtype(mgr.arrays[0].dtype):\n                # error: Item \"ExtensionArray\" of \"Union[ndarray, ExtensionArray]\"\n                # has no attribute \"reshape\"\n                return mgr.arrays[0].reshape(-1, 1)  # type: ignore[union-attr]\n            return ensure_wrapped_if_datetimelike(self.values)\n\n        blocks = mgr.blocks\n        if len(blocks) != 1:\n            return ensure_wrapped_if_datetimelike(self.values)\n\n        arr = blocks[0].values\n        if arr.ndim == 1:\n            # non-2D ExtensionArray\n            return self.values\n\n        # more generally, whatever we allow in NDArrayBackedExtensionBlock\n        arr = cast(\"np.ndarray | DatetimeArray | TimedeltaArray | PeriodArray\", arr)\n        return arr.T\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def _repr_fits_vertical_(self) -> bool:\n        \"\"\"\n        Check length against max_rows.\n        \"\"\"\n        max_rows = get_option(\"display.max_rows\")\n        return len(self) <= max_rows\n\n    def _repr_fits_horizontal_(self) -> bool:\n        \"\"\"\n        Check if full repr fits in horizontal boundaries imposed by the display\n        options width and max_columns.\n        \"\"\"\n        width, height = console.get_console_size()\n        max_columns = get_option(\"display.max_columns\")\n        nb_columns = len(self.columns)\n\n        # exceed max columns\n        if (max_columns and nb_columns > max_columns) or (\n            width and nb_columns > (width // 2)\n        ):\n            return False\n\n        # used by repr_html under IPython notebook or scripts ignore terminal\n        # dims\n        if width is None or not console.in_interactive_session():\n            return True\n\n        if get_option(\"display.width\") is not None or console.in_ipython_frontend():\n            # check at least the column row for excessive width\n            max_rows = 1\n        else:\n            max_rows = get_option(\"display.max_rows\")\n\n        # when auto-detecting, so width=None and not in ipython front end\n        # check whether repr fits horizontal by actually checking\n        # the width of the rendered repr\n        buf = StringIO()\n\n        # only care about the stuff we'll actually print out\n        # and to_string on entire frame may be expensive\n        d = self\n\n        if max_rows is not None:  # unlimited rows\n            # min of two, where one may be None\n            d = d.iloc[: min(max_rows, len(d))]\n        else:\n            return True\n\n        d.to_string(buf=buf)\n        value = buf.getvalue()\n        repr_width = max(len(line) for line in value.split(\"\\n\"))\n\n        return repr_width < width\n\n    def _info_repr(self) -> bool:\n        \"\"\"\n        True if the repr should show the info view.\n        \"\"\"\n        info_repr_option = get_option(\"display.large_repr\") == \"info\"\n        return info_repr_option and not (\n            self._repr_fits_horizontal_() and self._repr_fits_vertical_()\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular DataFrame.\n        \"\"\"\n        if self._info_repr():\n            buf = StringIO()\n            self.info(buf=buf)\n            return buf.getvalue()\n\n        repr_params = fmt.get_dataframe_repr_params()\n        return self.to_string(**repr_params)\n\n    def _repr_html_(self) -> str | None:\n        \"\"\"\n        Return a html representation for a particular DataFrame.\n\n        Mainly for IPython notebook.\n        \"\"\"\n        if self._info_repr():\n            buf = StringIO()\n            self.info(buf=buf)\n            # need to escape the <class>, should be the first line.\n            val = buf.getvalue().replace(\"<\", r\"&lt;\", 1)\n            val = val.replace(\">\", r\"&gt;\", 1)\n            return f\"<pre>{val}</pre>\"\n\n        if get_option(\"display.notebook_repr_html\"):\n            max_rows = get_option(\"display.max_rows\")\n            min_rows = get_option(\"display.min_rows\")\n            max_cols = get_option(\"display.max_columns\")\n            show_dimensions = get_option(\"display.show_dimensions\")\n\n            formatter = fmt.DataFrameFormatter(\n                self,\n                columns=None,\n                col_space=None,\n                na_rep=\"NaN\",\n                formatters=None,\n                float_format=None,\n                sparsify=None,\n                justify=None,\n                index_names=True,\n                header=True,\n                index=True,\n                bold_rows=True,\n                escape=True,\n                max_rows=max_rows,\n                min_rows=min_rows,\n                max_cols=max_cols,\n                show_dimensions=show_dimensions,\n                decimal=\".\",\n            )\n            return fmt.DataFrameRenderer(formatter).to_html(notebook=True)\n        else:\n            return None\n\n    @overload\n    def to_string(\n        self,\n        buf: None = ...,\n        columns: Axes | None = ...,\n        col_space: int | list[int] | dict[Hashable, int] | None = ...,\n        header: bool | SequenceNotStr[str] = ...,\n        index: bool = ...,\n        na_rep: str = ...,\n        formatters: fmt.FormattersType | None = ...,\n        float_format: fmt.FloatFormatType | None = ...,\n        sparsify: bool | None = ...,\n        index_names: bool = ...,\n        justify: str | None = ...,\n        max_rows: int | None = ...,\n        max_cols: int | None = ...,\n        show_dimensions: bool = ...,\n        decimal: str = ...,\n        line_width: int | None = ...,\n        min_rows: int | None = ...,\n        max_colwidth: int | None = ...,\n        encoding: str | None = ...,\n    ) -> str:\n        ...\n\n    @overload\n    def to_string(\n        self,\n        buf: FilePath | WriteBuffer[str],\n        columns: Axes | None = ...,\n        col_space: int | list[int] | dict[Hashable, int] | None = ...,\n        header: bool | SequenceNotStr[str] = ...,\n        index: bool = ...,\n        na_rep: str = ...,\n        formatters: fmt.FormattersType | None = ...,\n        float_format: fmt.FloatFormatType | None = ...,\n        sparsify: bool | None = ...,\n        index_names: bool = ...,\n        justify: str | None = ...,\n        max_rows: int | None = ...,\n        max_cols: int | None = ...,\n        show_dimensions: bool = ...,\n        decimal: str = ...,\n        line_width: int | None = ...,\n        min_rows: int | None = ...,\n        max_colwidth: int | None = ...,\n        encoding: str | None = ...,\n    ) -> None:\n        ...\n\n    @deprecate_nonkeyword_arguments(\n        version=\"3.0\", allowed_args=[\"self\", \"buf\"], name=\"to_string\"\n    )\n    @Substitution(\n        header_type=\"bool or list of str\",\n        header=\"Write out the column names. If a list of columns \"\n        \"is given, it is assumed to be aliases for the \"\n        \"column names\",\n        col_space_type=\"int, list or dict of int\",\n        col_space=\"The minimum width of each column. If a list of ints is given \"\n        \"every integers corresponds with one column. If a dict is given, the key \"\n        \"references the column, while the value defines the space to use.\",\n    )\n    @Substitution(shared_params=fmt.common_docstring, returns=fmt.return_docstring)\n    def to_string(\n        self,\n        buf: FilePath | WriteBuffer[str] | None = None,\n        columns: Axes | None = None,\n        col_space: int | list[int] | dict[Hashable, int] | None = None,\n        header: bool | SequenceNotStr[str] = True,\n        index: bool = True,\n        na_rep: str = \"NaN\",\n        formatters: fmt.FormattersType | None = None,\n        float_format: fmt.FloatFormatType | None = None,\n        sparsify: bool | None = None,\n        index_names: bool = True,\n        justify: str | None = None,\n        max_rows: int | None = None,\n        max_cols: int | None = None,\n        show_dimensions: bool = False,\n        decimal: str = \".\",\n        line_width: int | None = None,\n        min_rows: int | None = None,\n        max_colwidth: int | None = None,\n        encoding: str | None = None,\n    ) -> str | None:\n        \"\"\"\n        Render a DataFrame to a console-friendly tabular output.\n        %(shared_params)s\n        line_width : int, optional\n            Width to wrap a line in characters.\n        min_rows : int, optional\n            The number of rows to display in the console in a truncated repr\n            (when number of rows is above `max_rows`).\n        max_colwidth : int, optional\n            Max width to truncate each column in characters. By default, no limit.\n        encoding : str, default \"utf-8\"\n            Set character encoding.\n        %(returns)s\n        See Also\n        --------\n        to_html : Convert DataFrame to HTML.\n\n        Examples\n        --------\n        >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n        >>> df = pd.DataFrame(d)\n        >>> print(df.to_string())\n           col1  col2\n        0     1     4\n        1     2     5\n        2     3     6\n        \"\"\"\n        from pandas import option_context\n\n        with option_context(\"display.max_colwidth\", max_colwidth):\n            formatter = fmt.DataFrameFormatter(\n                self,\n                columns=columns,\n                col_space=col_space,\n                na_rep=na_rep,\n                formatters=formatters,\n                float_format=float_format,\n                sparsify=sparsify,\n                justify=justify,\n                index_names=index_names,\n                header=header,\n                index=index,\n                min_rows=min_rows,\n                max_rows=max_rows,\n                max_cols=max_cols,\n                show_dimensions=show_dimensions,\n                decimal=decimal,\n            )\n            return fmt.DataFrameRenderer(formatter).to_string(\n                buf=buf,\n                encoding=encoding,\n                line_width=line_width,\n            )\n\n    def _get_values_for_csv(\n        self,\n        *,\n        float_format: FloatFormatType | None,\n        date_format: str | None,\n        decimal: str,\n        na_rep: str,\n        quoting,  # int csv.QUOTE_FOO from stdlib\n    ) -> Self:\n        # helper used by to_csv\n        mgr = self._mgr.get_values_for_csv(\n            float_format=float_format,\n            date_format=date_format,\n            decimal=decimal,\n            na_rep=na_rep,\n            quoting=quoting,\n        )\n        # error: Incompatible return value type (got \"DataFrame\", expected \"Self\")\n        return self._constructor_from_mgr(mgr, axes=mgr.axes)  # type: ignore[return-value]\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def style(self) -> Styler:\n        \"\"\"\n        Returns a Styler object.\n\n        Contains methods for building a styled HTML representation of the DataFrame.\n\n        See Also\n        --------\n        io.formats.style.Styler : Helps style a DataFrame or Series according to the\n            data with HTML and CSS.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3]})\n        >>> df.style  # doctest: +SKIP\n\n        Please see\n        `Table Visualization <../../user_guide/style.ipynb>`_ for more examples.\n        \"\"\"\n        from pandas.io.formats.style import Styler\n\n        return Styler(self)\n\n    _shared_docs[\n        \"items\"\n    ] = r\"\"\"\n        Iterate over (column name, Series) pairs.\n\n        Iterates over the DataFrame columns, returning a tuple with\n        the column name and the content as a Series.\n\n        Yields\n        ------\n        label : object\n            The column names for the DataFrame being iterated over.\n        content : Series\n            The column entries belonging to each label, as a Series.\n\n        See Also\n        --------\n        DataFrame.iterrows : Iterate over DataFrame rows as\n            (index, Series) pairs.\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples\n            of the values.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n        ...                   'population': [1864, 22000, 80000]},\n        ...                   index=['panda', 'polar', 'koala'])\n        >>> df\n                species   population\n        panda   bear      1864\n        polar   bear      22000\n        koala   marsupial 80000\n        >>> for label, content in df.items():\n        ...     print(f'label: {label}')\n        ...     print(f'content: {content}', sep='\\n')\n        ...\n        label: species\n        content:\n        panda         bear\n        polar         bear\n        koala    marsupial\n        Name: species, dtype: object\n        label: population\n        content:\n        panda     1864\n        polar    22000\n        koala    80000\n        Name: population, dtype: int64\n        \"\"\"\n\n    @Appender(_shared_docs[\"items\"])\n    def items(self) -> Iterable[tuple[Hashable, Series]]:\n        if self.columns.is_unique and hasattr(self, \"_item_cache\"):\n            for k in self.columns:\n                yield k, self._get_item_cache(k)\n        else:\n            for i, k in enumerate(self.columns):\n                yield k, self._ixs(i, axis=1)\n\n    def iterrows(self) -> Iterable[tuple[Hashable, Series]]:\n        \"\"\"\n        Iterate over DataFrame rows as (index, Series) pairs.\n\n        Yields\n        ------\n        index : label or tuple of label\n            The index of the row. A tuple for a `MultiIndex`.\n        data : Series\n            The data of the row as a Series.\n\n        See Also\n        --------\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.\n        DataFrame.items : Iterate over (column name, Series) pairs.\n\n        Notes\n        -----\n        1. Because ``iterrows`` returns a Series for each row,\n           it does **not** preserve dtypes across the rows (dtypes are\n           preserved across columns for DataFrames).\n\n           To preserve dtypes while iterating over the rows, it is better\n           to use :meth:`itertuples` which returns namedtuples of the values\n           and which is generally faster than ``iterrows``.\n\n        2. You should **never modify** something you are iterating over.\n           This is not guaranteed to work in all cases. Depending on the\n           data types, the iterator returns a copy and not a view, and writing\n           to it will have no effect.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])\n        >>> row = next(df.iterrows())[1]\n        >>> row\n        int      1.0\n        float    1.5\n        Name: 0, dtype: float64\n        >>> print(row['int'].dtype)\n        float64\n        >>> print(df['int'].dtype)\n        int64\n        \"\"\"\n        columns = self.columns\n        klass = self._constructor_sliced\n        using_cow = using_copy_on_write()\n        for k, v in zip(self.index, self.values):\n            s = klass(v, index=columns, name=k).__finalize__(self)\n            if using_cow and self._mgr.is_single_block:\n                s._mgr.add_references(self._mgr)  # type: ignore[arg-type]\n            yield k, s\n\n    def itertuples(\n        self, index: bool = True, name: str | None = \"Pandas\"\n    ) -> Iterable[tuple[Any, ...]]:\n        \"\"\"\n        Iterate over DataFrame rows as namedtuples.\n\n        Parameters\n        ----------\n        index : bool, default True\n            If True, return the index as the first element of the tuple.\n        name : str or None, default \"Pandas\"\n            The name of the returned namedtuples or None to return regular\n            tuples.\n\n        Returns\n        -------\n        iterator\n            An object to iterate over namedtuples for each row in the\n            DataFrame with the first field possibly being the index and\n            following fields being the column values.\n\n        See Also\n        --------\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series)\n            pairs.\n        DataFrame.items : Iterate over (column name, Series) pairs.\n\n        Notes\n        -----\n        The column names will be renamed to positional names if they are\n        invalid Python identifiers, repeated, or start with an underscore.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]},\n        ...                   index=['dog', 'hawk'])\n        >>> df\n              num_legs  num_wings\n        dog          4          0\n        hawk         2          2\n        >>> for row in df.itertuples():\n        ...     print(row)\n        ...\n        Pandas(Index='dog', num_legs=4, num_wings=0)\n        Pandas(Index='hawk', num_legs=2, num_wings=2)\n\n        By setting the `index` parameter to False we can remove the index\n        as the first element of the tuple:\n\n        >>> for row in df.itertuples(index=False):\n        ...     print(row)\n        ...\n        Pandas(num_legs=4, num_wings=0)\n        Pandas(num_legs=2, num_wings=2)\n\n        With the `name` parameter set we set a custom name for the yielded\n        namedtuples:\n\n        >>> for row in df.itertuples(name='Animal'):\n        ...     print(row)\n        ...\n        Animal(Index='dog', num_legs=4, num_wings=0)\n        Animal(Index='hawk', num_legs=2, num_wings=2)\n        \"\"\"\n        arrays = []\n        fields = list(self.columns)\n        if index:\n            arrays.append(self.index)\n            fields.insert(0, \"Index\")\n\n        # use integer indexing because of possible duplicate column names\n        arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))\n\n        if name is not None:\n            # https://github.com/python/mypy/issues/9046\n            # error: namedtuple() expects a string literal as the first argument\n            itertuple = collections.namedtuple(  # type: ignore[misc]\n                name, fields, rename=True\n            )\n            return map(itertuple._make, zip(*arrays))\n\n        # fallback to regular tuples\n        return zip(*arrays)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Returns length of info axis, but here we use the index.\n        \"\"\"\n        return len(self.index)\n\n    @overload\n    def dot(self, other: Series) -> Series:\n        ...\n\n    @overload\n    def dot(self, other: DataFrame | Index | ArrayLike) -> DataFrame:\n        ...\n\n    def dot(self, other: AnyArrayLike | DataFrame) -> DataFrame | Series:\n        \"\"\"\n        Compute the matrix multiplication between the DataFrame and other.\n\n        This method computes the matrix product between the DataFrame and the\n        values of an other Series, DataFrame or a numpy array.\n\n        It can also be called using ``self @ other``.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the matrix product with.\n\n        Returns\n        -------\n        Series or DataFrame\n            If other is a Series, return the matrix product between self and\n            other as a Series. If other is a DataFrame or a numpy.array, return\n            the matrix product of self and other in a DataFrame of a np.array.\n\n        See Also\n        --------\n        Series.dot: Similar method for Series.\n\n        Notes\n        -----\n        The dimensions of DataFrame and other must be compatible in order to\n        compute the matrix multiplication. In addition, the column names of\n        DataFrame and the index of other must contain the same values, as they\n        will be aligned prior to the multiplication.\n\n        The dot method for Series computes the inner product, instead of the\n        matrix product here.\n\n        Examples\n        --------\n        Here we multiply a DataFrame with a Series.\n\n        >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\n        >>> s = pd.Series([1, 1, 2, 1])\n        >>> df.dot(s)\n        0    -4\n        1     5\n        dtype: int64\n\n        Here we multiply a DataFrame with another DataFrame.\n\n        >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(other)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note that the dot method give the same result as @\n\n        >>> df @ other\n            0   1\n        0   1   4\n        1   2   2\n\n        The dot method works also if other is an np.array.\n\n        >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(arr)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note how shuffling of the objects does not change the result.\n\n        >>> s2 = s.reindex([1, 0, 2, 3])\n        >>> df.dot(s2)\n        0    -4\n        1     5\n        dtype: int64\n        \"\"\"\n        if isinstance(other, (Series, DataFrame)):\n            common = self.columns.union(other.index)\n            if len(common) > len(self.columns) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n\n            left = self.reindex(columns=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right._values\n        else:\n            left = self\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[1] != rvals.shape[0]:\n                raise ValueError(\n                    f\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\n                )\n\n        if isinstance(other, DataFrame):\n            common_type = find_common_type(list(self.dtypes) + list(other.dtypes))\n            return self._constructor(\n                np.dot(lvals, rvals),\n                index=left.index,\n                columns=other.columns,\n                copy=False,\n                dtype=common_type,\n            )\n        elif isinstance(other, Series):\n            common_type = find_common_type(list(self.dtypes) + [other.dtypes])\n            return self._constructor_sliced(\n                np.dot(lvals, rvals), index=left.index, copy=False, dtype=common_type\n            )\n        elif isinstance(rvals, (np.ndarray, Index)):\n            result = np.dot(lvals, rvals)\n            if result.ndim == 2:\n                return self._constructor(result, index=left.index, copy=False)\n            else:\n                return self._constructor_sliced(result, index=left.index, copy=False)\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    @overload\n    def __matmul__(self, other: Series) -> Series:\n        ...\n\n    @overload\n    def __matmul__(self, other: AnyArrayLike | DataFrame) -> DataFrame | Series:\n        ...\n\n    def __matmul__(self, other: AnyArrayLike | DataFrame) -> DataFrame | Series:\n        \"\"\"\n        Matrix multiplication using binary `@` operator.\n        \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other) -> DataFrame:\n        \"\"\"\n        Matrix multiplication using binary `@` operator.\n        \"\"\"\n        try:\n            return self.T.dot(np.transpose(other)).T\n        except ValueError as err:\n            if \"shape mismatch\" not in str(err):\n                raise\n            # GH#21581 give exception message for original shapes\n            msg = f\"shapes {np.shape(other)} and {self.shape} not aligned\"\n            raise ValueError(msg) from err\n\n    # ----------------------------------------------------------------------\n    # IO methods (to / from other formats)\n\n    @classmethod\n    def from_dict(\n        cls,\n        data: dict,\n        orient: FromDictOrient = \"columns\",\n        dtype: Dtype | None = None,\n        columns: Axes | None = None,\n    ) -> DataFrame:\n        \"\"\"\n        Construct DataFrame from dict of array-like or dicts.\n\n        Creates DataFrame object from dictionary by columns or by index\n        allowing dtype specification.\n\n        Parameters\n        ----------\n        data : dict\n            Of the form {field : array-like} or {field : dict}.\n        orient : {'columns', 'index', 'tight'}, default 'columns'\n            The \"orientation\" of the data. If the keys of the passed dict\n            should be the columns of the resulting DataFrame, pass 'columns'\n            (default). Otherwise if the keys should be rows, pass 'index'.\n            If 'tight', assume a dict with keys ['index', 'columns', 'data',\n            'index_names', 'column_names'].\n\n            .. versionadded:: 1.4.0\n               'tight' as an allowed value for the ``orient`` argument\n\n        dtype : dtype, default None\n            Data type to force after DataFrame construction, otherwise infer.\n        columns : list, default None\n            Column labels to use when ``orient='index'``. Raises a ValueError\n            if used with ``orient='columns'`` or ``orient='tight'``.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.from_records : DataFrame from structured ndarray, sequence\n            of tuples or dicts, or DataFrame.\n        DataFrame : DataFrame object creation using constructor.\n        DataFrame.to_dict : Convert the DataFrame to a dictionary.\n\n        Examples\n        --------\n        By default the keys of the dict become the DataFrame columns:\n\n        >>> data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\n        >>> pd.DataFrame.from_dict(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Specify ``orient='index'`` to create the DataFrame using dictionary\n        keys as rows:\n\n        >>> data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n        >>> pd.DataFrame.from_dict(data, orient='index')\n               0  1  2  3\n        row_1  3  2  1  0\n        row_2  a  b  c  d\n\n        When using the 'index' orientation, the column names can be\n        specified manually:\n\n        >>> pd.DataFrame.from_dict(data, orient='index',\n        ...                        columns=['A', 'B', 'C', 'D'])\n               A  B  C  D\n        row_1  3  2  1  0\n        row_2  a  b  c  d\n\n        Specify ``orient='tight'`` to create the DataFrame using a 'tight'\n        format:\n\n        >>> data = {'index': [('a', 'b'), ('a', 'c')],\n        ...         'columns': [('x', 1), ('y', 2)],\n        ...         'data': [[1, 3], [2, 4]],\n        ...         'index_names': ['n1', 'n2'],\n        ...         'column_names': ['z1', 'z2']}\n        >>> pd.DataFrame.from_dict(data, orient='tight')\n        z1     x  y\n        z2     1  2\n        n1 n2\n        a  b   1  3\n           c   2  4\n        \"\"\"\n        index = None\n        orient = orient.lower()  # type: ignore[assignment]\n        if orient == \"index\":\n            if len(data) > 0:\n                # TODO speed up Series case\n                if isinstance(next(iter(data.values())), (Series, dict)):\n                    data = _from_nested_dict(data)\n                else:\n                    index = list(data.keys())\n                    # error: Incompatible types in assignment (expression has type\n                    # \"List[Any]\", variable has type \"Dict[Any, Any]\")\n                    data = list(data.values())  # type: ignore[assignment]\n        elif orient in (\"columns\", \"tight\"):\n            if columns is not None:\n                raise ValueError(f\"cannot use columns parameter with orient='{orient}'\")\n        else:  # pragma: no cover\n            raise ValueError(\n                f\"Expected 'index', 'columns' or 'tight' for orient parameter. \"\n                f\"Got '{orient}' instead\"\n            )\n\n        if orient != \"tight\":\n            return cls(data, index=index, columns=columns, dtype=dtype)\n        else:\n            realdata = data[\"data\"]\n\n            def create_index(indexlist, namelist):\n                index: Index\n                if len(namelist) > 1:\n                    index = MultiIndex.from_tuples(indexlist, names=namelist)\n                else:\n                    index = Index(indexlist, name=namelist[0])\n                return index\n\n            index = create_index(data[\"index\"], data[\"index_names\"])\n            columns = create_index(data[\"columns\"], data[\"column_names\"])\n            return cls(realdata, index=index, columns=columns, dtype=dtype)\n\n    def to_numpy(\n        self,\n        dtype: npt.DTypeLike | None = None,\n        copy: bool = False,\n        na_value: object = lib.no_default,\n    ) -> np.ndarray:\n        \"\"\"\n        Convert the DataFrame to a NumPy array.\n\n        By default, the dtype of the returned array will be the common NumPy\n        dtype of all types in the DataFrame. For example, if the dtypes are\n        ``float16`` and ``float32``, the results dtype will be ``float32``.\n        This may require copying data and coercing values, which may be\n        expensive.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to pass to :meth:`numpy.asarray`.\n        copy : bool, default False\n            Whether to ensure that the returned value is not a view on\n            another array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary.\n        na_value : Any, optional\n            The value to use for missing values. The default value depends\n            on `dtype` and the dtypes of the DataFrame columns.\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        Series.to_numpy : Similar method for Series.\n\n        Examples\n        --------\n        >>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\n        array([[1, 3],\n               [2, 4]])\n\n        With heterogeneous data, the lowest common type will have to\n        be used.\n\n        >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\n        >>> df.to_numpy()\n        array([[1. , 3. ],\n               [2. , 4.5]])\n\n        For a mix of numeric and non-numeric types, the output array will\n        have object dtype.\n\n        >>> df['C'] = pd.date_range('2000', periods=2)\n        >>> df.to_numpy()\n        array([[1, 3.0, Timestamp('2000-01-01 00:00:00')],\n               [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)\n        \"\"\"\n        if dtype is not None:\n            dtype = np.dtype(dtype)\n        result = self._mgr.as_array(dtype=dtype, copy=copy, na_value=na_value)\n        if result.dtype is not dtype:\n            result = np.asarray(result, dtype=dtype)\n\n        return result\n\n    def _create_data_for_split_and_tight_to_dict(\n        self, are_all_object_dtype_cols: bool, object_dtype_indices: list[int]\n    ) -> list:\n        \"\"\"\n        Simple helper method to create data for to ``to_dict(orient=\"split\")`` and\n        ``to_dict(orient=\"tight\")`` to create the main output data\n        \"\"\"\n        if are_all_object_dtype_cols:\n            data = [\n                list(map(maybe_box_native, t))\n                for t in self.itertuples(index=False, name=None)\n            ]\n        else:\n            data = [list(t) for t in self.itertuples(index=False, name=None)]\n            if object_dtype_indices:\n                # If we have object_dtype_cols, apply maybe_box_naive after list\n                # comprehension for perf\n                for row in data:\n                    for i in object_dtype_indices:\n                        row[i] = maybe_box_native(row[i])\n        return data\n\n    @overload\n    def to_dict(\n        self,\n        orient: Literal[\"dict\", \"list\", \"series\", \"split\", \"tight\", \"index\"] = ...,\n        *,\n        into: type[MutableMappingT] | MutableMappingT,\n        index: bool = ...,\n    ) -> MutableMappingT:\n        ...\n\n    @overload\n    def to_dict(\n        self,\n        orient: Literal[\"records\"],\n        *,\n        into: type[MutableMappingT] | MutableMappingT,\n        index: bool = ...,\n    ) -> list[MutableMappingT]:\n        ...\n\n    @overload\n    def to_dict(\n        self,\n        orient: Literal[\"dict\", \"list\", \"series\", \"split\", \"tight\", \"index\"] = ...,\n        *,\n        into: type[dict] = ...,\n        index: bool = ...,\n    ) -> dict:\n        ...\n\n    @overload\n    def to_dict(\n        self,\n        orient: Literal[\"records\"],\n        *,\n        into: type[dict] = ...,\n        index: bool = ...,\n    ) -> list[dict]:\n        ...\n\n    # error: Incompatible default for argument \"into\" (default has type \"type\n    # [dict[Any, Any]]\", argument has type \"type[MutableMappingT] | MutableMappingT\")\n    @deprecate_nonkeyword_arguments(\n        version=\"3.0\", allowed_args=[\"self\", \"orient\"], name=\"to_dict\"\n    )\n    def to_dict(\n        self,\n        orient: Literal[\n            \"dict\", \"list\", \"series\", \"split\", \"tight\", \"records\", \"index\"\n        ] = \"dict\",\n        into: type[MutableMappingT]\n        | MutableMappingT = dict,  # type: ignore[assignment]\n        index: bool = True,\n    ) -> MutableMappingT | list[MutableMappingT]:\n        \"\"\"\n        Convert the DataFrame to a dictionary.\n\n        The type of the key-value pairs can be customized with the parameters\n        (see below).\n\n        Parameters\n        ----------\n        orient : str {'dict', 'list', 'series', 'split', 'tight', 'records', 'index'}\n            Determines the type of the values of the dictionary.\n\n            - 'dict' (default) : dict like {column -> {index -> value}}\n            - 'list' : dict like {column -> [values]}\n            - 'series' : dict like {column -> Series(values)}\n            - 'split' : dict like\n              {'index' -> [index], 'columns' -> [columns], 'data' -> [values]}\n            - 'tight' : dict like\n              {'index' -> [index], 'columns' -> [columns], 'data' -> [values],\n              'index_names' -> [index.names], 'column_names' -> [column.names]}\n            - 'records' : list like\n              [{column -> value}, ... , {column -> value}]\n            - 'index' : dict like {index -> {column -> value}}\n\n            .. versionadded:: 1.4.0\n                'tight' as an allowed value for the ``orient`` argument\n\n        into : class, default dict\n            The collections.abc.MutableMapping subclass used for all Mappings\n            in the return value.  Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        index : bool, default True\n            Whether to include the index item (and index_names item if `orient`\n            is 'tight') in the returned dictionary. Can only be ``False``\n            when `orient` is 'split' or 'tight'.\n\n            .. versionadded:: 2.0.0\n\n        Returns\n        -------\n        dict, list or collections.abc.MutableMapping\n            Return a collections.abc.MutableMapping object representing the\n            DataFrame. The resulting transformation depends on the `orient`\n            parameter.\n\n        See Also\n        --------\n        DataFrame.from_dict: Create a DataFrame from a dictionary.\n        DataFrame.to_json: Convert a DataFrame to JSON format.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2],\n        ...                    'col2': [0.5, 0.75]},\n        ...                   index=['row1', 'row2'])\n        >>> df\n              col1  col2\n        row1     1  0.50\n        row2     2  0.75\n        >>> df.to_dict()\n        {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}}\n\n        You can specify the return orientation.\n\n        >>> df.to_dict('series')\n        {'col1': row1    1\n                 row2    2\n        Name: col1, dtype: int64,\n        'col2': row1    0.50\n                row2    0.75\n        Name: col2, dtype: float64}\n\n        >>> df.to_dict('split')\n        {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n         'data': [[1, 0.5], [2, 0.75]]}\n\n        >>> df.to_dict('records')\n        [{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}]\n\n        >>> df.to_dict('index')\n        {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}\n\n        >>> df.to_dict('tight')\n        {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n         'data': [[1, 0.5], [2, 0.75]], 'index_names': [None], 'column_names': [None]}\n\n        You can also specify the mapping type.\n\n        >>> from collections import OrderedDict, defaultdict\n        >>> df.to_dict(into=OrderedDict)\n        OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])),\n                     ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])\n\n        If you want a `defaultdict`, you need to initialize it:\n\n        >>> dd = defaultdict(list)\n        >>> df.to_dict('records', into=dd)\n        [defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}),\n         defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]\n        \"\"\"\n        from pandas.core.methods.to_dict import to_dict\n\n        return to_dict(self, orient, into=into, index=index)\n\n    @deprecate_nonkeyword_arguments(\n        version=\"3.0\", allowed_args=[\"self\", \"destination_table\"], name=\"to_gbq\"\n    )\n    def to_gbq(\n        self,\n        destination_table: str,\n        project_id: str | None = None,\n        chunksize: int | None = None,\n        reauth: bool = False,\n        if_exists: ToGbqIfexist = \"fail\",\n        auth_local_webserver: bool = True,\n        table_schema: list[dict[str, str]] | None = None,\n        location: str | None = None,\n        progress_bar: bool = True,\n        credentials=None,\n    ) -> None:\n        \"\"\"\n        Write a DataFrame to a Google BigQuery table.\n\n        .. deprecated:: 2.2.0\n\n           Please use ``pandas_gbq.to_gbq`` instead.\n\n        This function requires the `pandas-gbq package\n        <https://pandas-gbq.readthedocs.io>`__.\n\n        See the `How to authenticate with Google BigQuery\n        <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__\n        guide for authentication instructions.\n\n        Parameters\n        ----------\n        destination_table : str\n            Name of table to be written, in the form ``dataset.tablename``.\n        project_id : str, optional\n            Google BigQuery Account project ID. Optional when available from\n            the environment.\n        chunksize : int, optional\n            Number of rows to be inserted in each chunk from the dataframe.\n            Set to ``None`` to load the whole dataframe at once.\n        reauth : bool, default False\n            Force Google BigQuery to re-authenticate the user. This is useful\n            if multiple accounts are used.\n        if_exists : str, default 'fail'\n            Behavior when the destination table exists. Value can be one of:\n\n            ``'fail'``\n                If table exists raise pandas_gbq.gbq.TableCreationError.\n            ``'replace'``\n                If table exists, drop it, recreate it, and insert data.\n            ``'append'``\n                If table exists, insert data. Create if does not exist.\n        auth_local_webserver : bool, default True\n            Use the `local webserver flow`_ instead of the `console flow`_\n            when getting user credentials.\n\n            .. _local webserver flow:\n                https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server\n            .. _console flow:\n                https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console\n\n            *New in version 0.2.0 of pandas-gbq*.\n\n            .. versionchanged:: 1.5.0\n               Default value is changed to ``True``. Google has deprecated the\n               ``auth_local_webserver = False`` `\"out of band\" (copy-paste)\n               flow\n               <https://developers.googleblog.com/2022/02/making-oauth-flows-safer.html?m=1#disallowed-oob>`_.\n        table_schema : list of dicts, optional\n            List of BigQuery table fields to which according DataFrame\n            columns conform to, e.g. ``[{'name': 'col1', 'type':\n            'STRING'},...]``. If schema is not provided, it will be\n            generated according to dtypes of DataFrame columns. See\n            BigQuery API documentation on available names of a field.\n\n            *New in version 0.3.1 of pandas-gbq*.\n        location : str, optional\n            Location where the load job should run. See the `BigQuery locations\n            documentation\n            <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a\n            list of available locations. The location must match that of the\n            target dataset.\n\n            *New in version 0.5.0 of pandas-gbq*.\n        progress_bar : bool, default True\n            Use the library `tqdm` to show the progress bar for the upload,\n            chunk by chunk.\n\n            *New in version 0.5.0 of pandas-gbq*.\n        credentials : google.auth.credentials.Credentials, optional\n            Credentials for accessing Google APIs. Use this parameter to\n            override default credentials, such as to use Compute Engine\n            :class:`google.auth.compute_engine.Credentials` or Service\n            Account :class:`google.oauth2.service_account.Credentials`\n            directly.\n\n            *New in version 0.8.0 of pandas-gbq*.\n\n        See Also\n        --------\n        pandas_gbq.to_gbq : This function in the pandas-gbq library.\n        read_gbq : Read a DataFrame from Google BigQuery.\n\n        Examples\n        --------\n        Example taken from `Google BigQuery documentation\n        <https://cloud.google.com/bigquery/docs/samples/bigquery-pandas-gbq-to-gbq-simple>`_\n\n        >>> project_id = \"my-project\"\n        >>> table_id = 'my_dataset.my_table'\n        >>> df = pd.DataFrame({\n        ...                   \"my_string\": [\"a\", \"b\", \"c\"],\n        ...                   \"my_int64\": [1, 2, 3],\n        ...                   \"my_float64\": [4.0, 5.0, 6.0],\n        ...                   \"my_bool1\": [True, False, True],\n        ...                   \"my_bool2\": [False, True, False],\n        ...                   \"my_dates\": pd.date_range(\"now\", periods=3),\n        ...                   }\n        ...                   )\n\n        >>> df.to_gbq(table_id, project_id=project_id)  # doctest: +SKIP\n        \"\"\"\n        from pandas.io import gbq\n\n        gbq.to_gbq(\n            self,\n            destination_table,\n            project_id=project_id,\n            chunksize=chunksize,\n            reauth=reauth,\n            if_exists=if_exists,\n            auth_local_webserver=auth_local_webserver,\n            table_schema=table_schema,\n            location=location,\n            progress_bar=progress_bar,\n            credentials=credentials,\n        )\n\n    @classmethod\n    def from_records(\n        cls,\n        data,\n        index=None,\n        exclude=None,\n        columns=None,\n        coerce_float: bool = False,\n        nrows: int | None = None,\n    ) -> DataFrame:\n        \"\"\"\n        Convert structured or record ndarray to DataFrame.\n\n        Creates a DataFrame object from a structured ndarray, sequence of\n        tuples or dicts, or DataFrame.\n\n        Parameters\n        ----------\n        data : structured ndarray, sequence of tuples or dicts, or DataFrame\n            Structured input data.\n\n            .. deprecated:: 2.1.0\n                Passing a DataFrame is deprecated.\n        index : str, list of fields, array-like\n            Field of array to use as the index, alternately a specific set of\n            input labels to use.\n        exclude : sequence, default None\n            Columns or fields to exclude.\n        columns : sequence, default None\n            Column names to use. If the passed data do not have names\n            associated with them, this argument provides names for the\n            columns. Otherwise this argument indicates the order of the columns\n            in the result (any names not found in the data will become all-NA\n            columns).\n        coerce_float : bool, default False\n            Attempt to convert values of non-string, non-numeric objects (like\n            decimal.Decimal) to floating point, useful for SQL result sets.\n        nrows : int, default None\n            Number of rows to read if data is an iterator.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.from_dict : DataFrame from dict of array-like or dicts.\n        DataFrame : DataFrame object creation using constructor.\n\n        Examples\n        --------\n        Data can be provided as a structured ndarray:\n\n        >>> data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')],\n        ...                 dtype=[('col_1', 'i4'), ('col_2', 'U1')])\n        >>> pd.DataFrame.from_records(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Data can be provided as a list of dicts:\n\n        >>> data = [{'col_1': 3, 'col_2': 'a'},\n        ...         {'col_1': 2, 'col_2': 'b'},\n        ...         {'col_1': 1, 'col_2': 'c'},\n        ...         {'col_1': 0, 'col_2': 'd'}]\n        >>> pd.DataFrame.from_records(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Data can be provided as a list of tuples with corresponding columns:\n\n        >>> data = [(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')]\n        >>> pd.DataFrame.from_records(data, columns=['col_1', 'col_2'])\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n        \"\"\"\n        if isinstance(data, DataFrame):\n            warnings.warn(\n                \"Passing a DataFrame to DataFrame.from_records is deprecated. Use \"\n                \"set_index and/or drop to modify the DataFrame instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            if columns is not None:\n                if is_scalar(columns):\n                    columns = [columns]\n                data = data[columns]\n            if index is not None:\n                data = data.set_index(index)\n            if exclude is not None:\n                data = data.drop(columns=exclude)\n            return data.copy(deep=False)\n\n        result_index = None\n\n        # Make a copy of the input columns so we can modify it\n        if columns is not None:\n            columns = ensure_index(columns)\n\n        def maybe_reorder(\n            arrays: list[ArrayLike], arr_columns: Index, columns: Index, index\n        ) -> tuple[list[ArrayLike], Index, Index | None]:\n            \"\"\"\n            If our desired 'columns' do not match the data's pre-existing 'arr_columns',\n            we re-order our arrays.  This is like a pre-emptive (cheap) reindex.\n            \"\"\"\n            if len(arrays):\n                length = len(arrays[0])\n            else:\n                length = 0\n\n            result_index = None\n            if len(arrays) == 0 and index is None and length == 0:\n                result_index = default_index(0)\n\n            arrays, arr_columns = reorder_arrays(arrays, arr_columns, columns, length)\n            return arrays, arr_columns, result_index\n\n        if is_iterator(data):\n            if nrows == 0:\n                return cls()\n\n            try:\n                first_row = next(data)\n            except StopIteration:\n                return cls(index=index, columns=columns)\n\n            dtype = None\n            if hasattr(first_row, \"dtype\") and first_row.dtype.names:\n                dtype = first_row.dtype\n\n            values = [first_row]\n\n            if nrows is None:\n                values += data\n            else:\n                values.extend(itertools.islice(data, nrows - 1))\n\n            if dtype is not None:\n                data = np.array(values, dtype=dtype)\n            else:\n                data = values\n\n        if isinstance(data, dict):\n            if columns is None:\n                columns = arr_columns = ensure_index(sorted(data))\n                arrays = [data[k] for k in columns]\n            else:\n                arrays = []\n                arr_columns_list = []\n                for k, v in data.items():\n                    if k in columns:\n                        arr_columns_list.append(k)\n                        arrays.append(v)\n\n                arr_columns = Index(arr_columns_list)\n                arrays, arr_columns, result_index = maybe_reorder(\n                    arrays, arr_columns, columns, index\n                )\n\n        elif isinstance(data, np.ndarray):\n            arrays, columns = to_arrays(data, columns)\n            arr_columns = columns\n        else:\n            arrays, arr_columns = to_arrays(data, columns)\n            if coerce_float:\n                for i, arr in enumerate(arrays):\n                    if arr.dtype == object:\n                        # error: Argument 1 to \"maybe_convert_objects\" has\n                        # incompatible type \"Union[ExtensionArray, ndarray]\";\n                        # expected \"ndarray\"\n                        arrays[i] = lib.maybe_convert_objects(\n                            arr,  # type: ignore[arg-type]\n                            try_float=True,\n                        )\n\n            arr_columns = ensure_index(arr_columns)\n            if columns is None:\n                columns = arr_columns\n            else:\n                arrays, arr_columns, result_index = maybe_reorder(\n                    arrays, arr_columns, columns, index\n                )\n\n        if exclude is None:\n            exclude = set()\n        else:\n            exclude = set(exclude)\n\n        if index is not None:\n            if isinstance(index, str) or not hasattr(index, \"__iter__\"):\n                i = columns.get_loc(index)\n                exclude.add(index)\n                if len(arrays) > 0:\n                    result_index = Index(arrays[i], name=index)\n                else:\n                    result_index = Index([], name=index)\n            else:\n                try:\n                    index_data = [arrays[arr_columns.get_loc(field)] for field in index]\n                except (KeyError, TypeError):\n                    # raised by get_loc, see GH#29258\n                    result_index = index\n                else:\n                    result_index = ensure_index_from_sequences(index_data, names=index)\n                    exclude.update(index)\n\n        if any(exclude):\n            arr_exclude = [x for x in exclude if x in arr_columns]\n            to_remove = [arr_columns.get_loc(col) for col in arr_exclude]\n            arrays = [v for i, v in enumerate(arrays) if i not in to_remove]\n\n            columns = columns.drop(exclude)\n\n        manager = _get_option(\"mode.data_manager\", silent=True)\n        mgr = arrays_to_mgr(arrays, columns, result_index, typ=manager)\n\n        return cls._from_mgr(mgr, axes=mgr.axes)\n\n    def to_records(\n        self, index: bool = True, column_dtypes=None, index_dtypes=None\n    ) -> np.rec.recarray:\n        \"\"\"\n        Convert DataFrame to a NumPy record array.\n\n        Index will be included as the first field of the record array if\n        requested.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Include index in resulting record array, stored in 'index'\n            field or using the index label, if set.\n        column_dtypes : str, type, dict, default None\n            If a string or type, the data type to store all columns. If\n            a dictionary, a mapping of column names and indices (zero-indexed)\n            to specific data types.\n        index_dtypes : str, type, dict, default None\n            If a string or type, the data type to store all index levels. If\n            a dictionary, a mapping of index level names and indices\n            (zero-indexed) to specific data types.\n\n            This mapping is applied only if `index=True`.\n\n        Returns\n        -------\n        numpy.rec.recarray\n            NumPy ndarray with the DataFrame labels as fields and each row\n            of the DataFrame as entries.\n\n        See Also\n        --------\n        DataFrame.from_records: Convert structured or record ndarray\n            to DataFrame.\n        numpy.rec.recarray: An ndarray that allows field access using\n            attributes, analogous to typed columns in a\n            spreadsheet.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n        ...                   index=['a', 'b'])\n        >>> df\n           A     B\n        a  1  0.50\n        b  2  0.75\n        >>> df.to_records()\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])\n\n        If the DataFrame index has no label then the recarray field name\n        is set to 'index'. If the index has a label then this is used as the\n        field name:\n\n        >>> df.index = df.index.rename(\"I\")\n        >>> df.to_records()\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')])\n\n        The index can be excluded from the record array:\n\n        >>> df.to_records(index=False)\n        rec.array([(1, 0.5 ), (2, 0.75)],\n                  dtype=[('A', '<i8'), ('B', '<f8')])\n\n        Data types can be specified for the columns:\n\n        >>> df.to_records(column_dtypes={\"A\": \"int32\"})\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')])\n\n        As well as for the index:\n\n        >>> df.to_records(index_dtypes=\"<S2\")\n        rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n                  dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')])\n\n        >>> index_dtypes = f\"<S{df.index.str.len().max()}\"\n        >>> df.to_records(index_dtypes=index_dtypes)\n        rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n                  dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')])\n        \"\"\"\n        if index:\n            ix_vals = [\n                np.asarray(self.index.get_level_values(i))\n                for i in range(self.index.nlevels)\n            ]\n\n            arrays = ix_vals + [\n                np.asarray(self.iloc[:, i]) for i in range(len(self.columns))\n            ]\n\n            index_names = list(self.index.names)\n\n            if isinstance(self.index, MultiIndex):\n                index_names = com.fill_missing_names(index_names)\n            elif index_names[0] is None:\n                index_names = [\"index\"]\n\n            names = [str(name) for name in itertools.chain(index_names, self.columns)]\n        else:\n            arrays = [np.asarray(self.iloc[:, i]) for i in range(len(self.columns))]\n            names = [str(c) for c in self.columns]\n            index_names = []\n\n        index_len = len(index_names)\n        formats = []\n\n        for i, v in enumerate(arrays):\n            index_int = i\n\n            # When the names and arrays are collected, we\n            # first collect those in the DataFrame's index,\n            # followed by those in its columns.\n            #\n            # Thus, the total length of the array is:\n            # len(index_names) + len(DataFrame.columns).\n            #\n            # This check allows us to see whether we are\n            # handling a name / array in the index or column.\n            if index_int < index_len:\n                dtype_mapping = index_dtypes\n                name = index_names[index_int]\n            else:\n                index_int -= index_len\n                dtype_mapping = column_dtypes\n                name = self.columns[index_int]\n\n            # We have a dictionary, so we get the data type\n            # associated with the index or column (which can\n            # be denoted by its name in the DataFrame or its\n            # position in DataFrame's array of indices or\n            # columns, whichever is applicable.\n            if is_dict_like(dtype_mapping):\n                if name in dtype_mapping:\n                    dtype_mapping = dtype_mapping[name]\n                elif index_int in dtype_mapping:\n                    dtype_mapping = dtype_mapping[index_int]\n                else:\n                    dtype_mapping = None\n\n            # If no mapping can be found, use the array's\n            # dtype attribute for formatting.\n            #\n            # A valid dtype must either be a type or\n            # string naming a type.\n            if dtype_mapping is None:\n                formats.append(v.dtype)\n            elif isinstance(dtype_mapping, (type, np.dtype, str)):\n                # error: Argument 1 to \"append\" of \"list\" has incompatible\n                # type \"Union[type, dtype[Any], str]\"; expected \"dtype[Any]\"\n                formats.append(dtype_mapping)  # type: ignore[arg-type]\n            else:\n                element = \"row\" if i < index_len else \"column\"\n                msg = f\"Invalid dtype {dtype_mapping} specified for {element} {name}\"\n                raise ValueError(msg)\n\n        return np.rec.fromarrays(arrays, dtype={\"names\": names, \"formats\": formats})\n\n    @classmethod\n    def _from_arrays(\n        cls,\n        arrays,\n        columns,\n        index,\n        dtype: Dtype | None = None,\n        verify_integrity: bool = True,\n    ) -> Self:\n        \"\"\"\n        Create DataFrame from a list of arrays corresponding to the columns.\n\n        Parameters\n        ----------\n        arrays : list-like of arrays\n            Each array in the list corresponds to one column, in order.\n        columns : list-like, Index\n            The column names for the resulting DataFrame.\n        index : list-like, Index\n            The rows labels for the resulting DataFrame.\n        dtype : dtype, optional\n            Optional dtype to enforce for all arrays.\n        verify_integrity : bool, default True\n            Validate and homogenize all input. If set to False, it is assumed\n            that all elements of `arrays` are actual arrays how they will be\n            stored in a block (numpy ndarray or ExtensionArray), have the same\n            length as and are aligned with the index, and that `columns` and\n            `index` are ensured to be an Index object.\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n        manager = _get_option(\"mode.data_manager\", silent=True)\n        columns = ensure_index(columns)\n        if len(columns) != len(arrays):\n            raise ValueError(\"len(columns) must match len(arrays)\")\n        mgr = arrays_to_mgr(\n            arrays,\n            columns,\n            index,\n            dtype=dtype,\n            verify_integrity=verify_integrity,\n            typ=manager,\n        )\n        return cls._from_mgr(mgr, axes=mgr.axes)\n\n    @doc(\n        storage_options=_shared_docs[\"storage_options\"],\n        compression_options=_shared_docs[\"compression_options\"] % \"path\",\n    )\n    def to_stata(\n        self,\n        path: FilePath | WriteBuffer[bytes],\n        *,\n        convert_dates: dict[Hashable, str] | None = None,\n        write_index: bool = True,\n        byteorder: ToStataByteorder | None = None,\n        time_stamp: datetime.datetime | None = None,\n        data_label: str | None = None,\n        variable_labels: dict[Hashable, str] | None = None,\n        version: int | None = 114,\n        convert_strl: Sequence[Hashable] | None = None,\n        compression: CompressionOptions = \"infer\",\n        storage_options: StorageOptions | None = None,\n        value_labels: dict[Hashable, dict[float, str]] | None = None,\n    ) -> None:\n        \"\"\"\n        Export DataFrame object to Stata dta format.\n\n        Writes the DataFrame to a Stata dataset file.\n        \"dta\" files contain a Stata dataset.\n\n        Parameters\n        ----------\n        path : str, path object, or buffer\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a binary ``write()`` function.\n\n        convert_dates : dict\n            Dictionary mapping columns containing datetime types to stata\n            internal format to use when writing the dates. Options are 'tc',\n            'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer\n            or a name. Datetime columns that do not have a conversion type\n            specified will be converted to 'tc'. Raises NotImplementedError if\n            a datetime column has timezone information.\n        write_index : bool\n            Write the index to Stata dataset.\n        byteorder : str\n            Can be \">\", \"<\", \"little\", or \"big\". default is `sys.byteorder`.\n        time_stamp : datetime\n            A datetime to use as file creation date.  Default is the current\n            time.\n        data_label : str, optional\n            A label for the data set.  Must be 80 characters or smaller.\n        variable_labels : dict\n            Dictionary containing columns as keys and variable labels as\n            values. Each label must be 80 characters or smaller.\n        version : {{114, 117, 118, 119, None}}, default 114\n            Version to use in the output dta file. Set to None to let pandas\n            decide between 118 or 119 formats depending on the number of\n            columns in the frame. Version 114 can be read by Stata 10 and\n            later. Version 117 can be read by Stata 13 or later. Version 118\n            is supported in Stata 14 and later. Version 119 is supported in\n            Stata 15 and later. Version 114 limits string variables to 244\n            characters or fewer while versions 117 and later allow strings\n            with lengths up to 2,000,000 characters. Versions 118 and 119\n            support Unicode characters, and version 119 supports more than\n            32,767 variables.\n\n            Version 119 should usually only be used when the number of\n            variables exceeds the capacity of dta format 118. Exporting\n            smaller datasets in format 119 may have unintended consequences,\n            and, as of November 2020, Stata SE cannot read version 119 files.\n\n        convert_strl : list, optional\n            List of column names to convert to string columns to Stata StrL\n            format. Only available if version is 117.  Storing strings in the\n            StrL format can produce smaller dta files if strings have more than\n            8 characters and values are repeated.\n        {compression_options}\n\n            .. versionchanged:: 1.4.0 Zstandard support.\n\n        {storage_options}\n\n        value_labels : dict of dicts\n            Dictionary containing columns as keys and dictionaries of column value\n            to labels as values. Labels for a single variable must be 32,000\n            characters or smaller.\n\n            .. versionadded:: 1.4.0\n\n        Raises\n        ------\n        NotImplementedError\n            * If datetimes contain timezone information\n            * Column dtype is not representable in Stata\n        ValueError\n            * Columns listed in convert_dates are neither datetime64[ns]\n              or datetime.datetime\n            * Column listed in convert_dates is not in DataFrame\n            * Categorical label contains more than 32,000 characters\n\n        See Also\n        --------\n        read_stata : Import Stata data files.\n        io.stata.StataWriter : Low-level writer for Stata data files.\n        io.stata.StataWriter117 : Low-level writer for version 117 files.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({{'animal': ['falcon', 'parrot', 'falcon',\n        ...                               'parrot'],\n        ...                    'speed': [350, 18, 361, 15]}})\n        >>> df.to_stata('animals.dta')  # doctest: +SKIP\n        \"\"\"\n        if version not in (114, 117, 118, 119, None):\n            raise ValueError(\"Only formats 114, 117, 118 and 119 are supported.\")\n        if version == 114:\n            if convert_strl is not None:\n                raise ValueError(\"strl is not supported in format 114\")\n            from pandas.io.stata import StataWriter as statawriter\n        elif version == 117:\n            # Incompatible import of \"statawriter\" (imported name has type\n            # \"Type[StataWriter117]\", local name has type \"Type[StataWriter]\")\n            from pandas.io.stata import (  # type: ignore[assignment]\n                StataWriter117 as statawriter,\n            )\n        else:  # versions 118 and 119\n            # Incompatible import of \"statawriter\" (imported name has type\n            # \"Type[StataWriter117]\", local name has type \"Type[StataWriter]\")\n            from pandas.io.stata import (  # type: ignore[assignment]\n                StataWriterUTF8 as statawriter,\n            )\n\n        kwargs: dict[str, Any] = {}\n        if version is None or version >= 117:\n            # strl conversion is only supported >= 117\n            kwargs[\"convert_strl\"] = convert_strl\n        if version is None or version >= 118:\n            # Specifying the version is only supported for UTF8 (118 or 119)\n            kwargs[\"version\"] = version\n\n        writer = statawriter(\n            path,\n            self,\n            convert_dates=convert_dates,\n            byteorder=byteorder,\n            time_stamp=time_stamp,\n            data_label=data_label,\n            write_index=write_index,\n            variable_labels=variable_labels,\n            compression=compression,\n            storage_options=storage_options,\n            value_labels=value_labels,\n            **kwargs,\n        )\n        writer.write_file()\n\n    def to_feather(self, path: FilePath | WriteBuffer[bytes], **kwargs) -> None:\n        \"\"\"\n        Write a DataFrame to the binary Feather format.\n\n        Parameters\n        ----------\n        path : str, path object, file-like object\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a binary ``write()`` function. If a string or a path,\n            it will be used as Root Directory path when writing a partitioned dataset.\n        **kwargs :\n            Additional keywords passed to :func:`pyarrow.feather.write_feather`.\n            This includes the `compression`, `compression_level`, `chunksize`\n            and `version` keywords.\n\n        Notes\n        -----\n        This function writes the dataframe as a `feather file\n        <https://arrow.apache.org/docs/python/feather.html>`_. Requires a default\n        index. For saving the DataFrame with your custom index use a method that\n        supports custom indices e.g. `to_parquet`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]])\n        >>> df.to_feather(\"file.feather\")  # doctest: +SKIP\n        \"\"\"\n        from pandas.io.feather_format import to_feather\n\n        to_feather(self, path, **kwargs)\n\n    @deprecate_nonkeyword_arguments(\n        version=\"3.0\", allowed_args=[\"self\", \"buf\"], name=\"to_markdown\"\n    )\n    @doc(\n        Series.to_markdown,\n        klass=_shared_doc_kwargs[\"klass\"],\n        storage_options=_shared_docs[\"storage_options\"],\n        examples=\"\"\"Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     data={\"animal_1\": [\"elk\", \"pig\"], \"animal_2\": [\"dog\", \"quetzal\"]}\n        ... )\n        >>> print(df.to_markdown())\n        |    | animal_1   | animal_2   |\n        |---:|:-----------|:-----------|\n        |  0 | elk        | dog        |\n        |  1 | pig        | quetzal    |\n\n        Output markdown with a tabulate option.\n\n        >>> print(df.to_markdown(tablefmt=\"grid\"))\n        +----+------------+------------+\n        |    | animal_1   | animal_2   |\n        +====+============+============+\n        |  0 | elk        | dog        |\n        +----+------------+------------+\n        |  1 | pig        | quetzal    |\n        +----+------------+------------+\"\"\",\n    )\n    def to_markdown(\n        self,\n        buf: FilePath | WriteBuffer[str] | None = None,\n        mode: str = \"wt\",\n        index: bool = True,\n        storage_options: StorageOptions | None = None,\n        **kwargs,\n    ) -> str | None:\n        if \"showindex\" in kwargs:\n            raise ValueError(\"Pass 'index' instead of 'showindex\")\n\n        kwargs.setdefault(\"headers\", \"keys\")\n        kwargs.setdefault(\"tablefmt\", \"pipe\")\n        kwargs.setdefault(\"showindex\", index)\n        tabulate = import_optional_dependency(\"tabulate\")\n        result = tabulate.tabulate(self, **kwargs)\n        if buf is None:\n            return result\n\n        with get_handle(buf, mode, storage_options=storage_options) as handles:\n            handles.handle.write(result)\n        return None\n\n    @overload\n    def to_parquet(\n        self,\n        path: None = ...,\n        engine: Literal[\"auto\", \"pyarrow\", \"fastparquet\"] = ...,\n        compression: str | None = ...,\n        index: bool | None = ...,\n        partition_cols: list[str] | None = ...,\n        storage_options: StorageOptions = ...,\n        **kwargs,\n    ) -> bytes:\n        ...\n\n    @overload\n    def to_parquet(\n        self,\n        path: FilePath | WriteBuffer[bytes],\n        engine: Literal[\"auto\", \"pyarrow\", \"fastparquet\"] = ...,\n        compression: str | None = ...,\n        index: bool | None = ...,\n        partition_cols: list[str] | None = ...,\n        storage_options: StorageOptions = ...,\n        **kwargs,\n    ) -> None:\n        ...\n\n    @deprecate_nonkeyword_arguments(\n        version=\"3.0\", allowed_args=[\"self\", \"path\"], name=\"to_parquet\"\n    )\n    @doc(storage_options=_shared_docs[\"storage_options\"])\n    def to_parquet(\n        self,\n        path: FilePath | WriteBuffer[bytes] | None = None,\n        engine: Literal[\"auto\", \"pyarrow\", \"fastparquet\"] = \"auto\",\n        compression: str | None = \"snappy\",\n        index: bool | None = None,\n        partition_cols: list[str] | None = None,\n        storage_options: StorageOptions | None = None,\n        **kwargs,\n    ) -> bytes | None:\n        \"\"\"\n        Write a DataFrame to the binary parquet format.\n\n        This function writes the dataframe as a `parquet file\n        <https://parquet.apache.org/>`_. You can choose different parquet\n        backends, and have the option of compression. See\n        :ref:`the user guide <io.parquet>` for more details.\n\n        Parameters\n        ----------\n        path : str, path object, file-like object, or None, default None\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a binary ``write()`` function. If None, the result is\n            returned as bytes. If a string or path, it will be used as Root Directory\n            path when writing a partitioned dataset.\n        engine : {{'auto', 'pyarrow', 'fastparquet'}}, default 'auto'\n            Parquet library to use. If 'auto', then the option\n            ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n            behavior is to try 'pyarrow', falling back to 'fastparquet' if\n            'pyarrow' is unavailable.\n        compression : str or None, default 'snappy'\n            Name of the compression to use. Use ``None`` for no compression.\n            Supported options: 'snappy', 'gzip', 'brotli', 'lz4', 'zstd'.\n        index : bool, default None\n            If ``True``, include the dataframe's index(es) in the file output.\n            If ``False``, they will not be written to the file.\n            If ``None``, similar to ``True`` the dataframe's index(es)\n            will be saved. However, instead of being saved as values,\n            the RangeIndex will be stored as a range in the metadata so it\n            doesn't require much space and is faster. Other indexes will\n            be included as columns in the file output.\n        partition_cols : list, optional, default None\n            Column names by which to partition the dataset.\n            Columns are partitioned in the order they are given.\n            Must be None if path is not a string.\n        {storage_options}\n\n        **kwargs\n            Additional arguments passed to the parquet library. See\n            :ref:`pandas io <io.parquet>` for more details.\n\n        Returns\n        -------\n        bytes if no path argument is provided else None\n\n        See Also\n        --------\n        read_parquet : Read a parquet file.\n        DataFrame.to_orc : Write an orc file.\n        DataFrame.to_csv : Write a csv file.\n        DataFrame.to_sql : Write to a sql table.\n        DataFrame.to_hdf : Write to hdf.\n\n        Notes\n        -----\n        This function requires either the `fastparquet\n        <https://pypi.org/project/fastparquet>`_ or `pyarrow\n        <https://arrow.apache.org/docs/python/>`_ library.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={{'col1': [1, 2], 'col2': [3, 4]}})\n        >>> df.to_parquet('df.parquet.gzip',\n        ...               compression='gzip')  # doctest: +SKIP\n        >>> pd.read_parquet('df.parquet.gzip')  # doctest: +SKIP\n           col1  col2\n        0     1     3\n        1     2     4\n\n        If you want to get a buffer to the parquet content you can use a io.BytesIO\n        object, as long as you don't use partition_cols, which creates multiple files.\n\n        >>> import io\n        >>> f = io.BytesIO()\n        >>> df.to_parquet(f)\n        >>> f.seek(0)\n        0\n        >>> content = f.read()\n        \"\"\"\n        from pandas.io.parquet import to_parquet\n\n        return to_parquet(\n            self,\n            path,\n            engine,\n            compression=compression,\n            index=index,\n            partition_cols=partition_cols,\n            storage_options=storage_options,\n            **kwargs,\n        )\n\n    def to_orc(\n        self,\n        path: FilePath | WriteBuffer[bytes] | None = None,\n        *,\n        engine: Literal[\"pyarrow\"] = \"pyarrow\",\n        index: bool | None = None,\n        engine_kwargs: dict[str, Any] | None = None,\n    ) -> bytes | None:\n        \"\"\"\n        Write a DataFrame to the ORC format.\n\n        .. versionadded:: 1.5.0\n\n        Parameters\n        ----------\n        path : str, file-like object or None, default None\n            If a string, it will be used as Root Directory path\n            when writing a partitioned dataset. By file-like object,\n            we refer to objects with a write() method, such as a file handle\n            (e.g. via builtin open function). If path is None,\n            a bytes object is returned.\n        engine : {'pyarrow'}, default 'pyarrow'\n            ORC library to use.\n        index : bool, optional\n            If ``True``, include the dataframe's index(es) in the file output.\n            If ``False``, they will not be written to the file.\n            If ``None``, similar to ``infer`` the dataframe's index(es)\n            will be saved. However, instead of being saved as values,\n            the RangeIndex will be stored as a range in the metadata so it\n            doesn't require much space and is faster. Other indexes will\n            be included as columns in the file output.\n        engine_kwargs : dict[str, Any] or None, default None\n            Additional keyword arguments passed to :func:`pyarrow.orc.write_table`.\n\n        Returns\n        -------\n        bytes if no path argument is provided else None\n\n        Raises\n        ------\n        NotImplementedError\n            Dtype of one or more columns is category, unsigned integers, interval,\n            period or sparse.\n        ValueError\n            engine is not pyarrow.\n\n        See Also\n        --------\n        read_orc : Read a ORC file.\n        DataFrame.to_parquet : Write a parquet file.\n        DataFrame.to_csv : Write a csv file.\n        DataFrame.to_sql : Write to a sql table.\n        DataFrame.to_hdf : Write to hdf.\n\n        Notes\n        -----\n        * Before using this function you should read the :ref:`user guide about\n          ORC <io.orc>` and :ref:`install optional dependencies <install.warn_orc>`.\n        * This function requires `pyarrow <https://arrow.apache.org/docs/python/>`_\n          library.\n        * For supported dtypes please refer to `supported ORC features in Arrow\n          <https://arrow.apache.org/docs/cpp/orc.html#data-types>`__.\n        * Currently timezones in datetime columns are not preserved when a\n          dataframe is converted into ORC files.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [4, 3]})\n        >>> df.to_orc('df.orc')  # doctest: +SKIP\n        >>> pd.read_orc('df.orc')  # doctest: +SKIP\n           col1  col2\n        0     1     4\n        1     2     3\n\n        If you want to get a buffer to the orc content you can write it to io.BytesIO\n\n        >>> import io\n        >>> b = io.BytesIO(df.to_orc())  # doctest: +SKIP\n        >>> b.seek(0)  # doctest: +SKIP\n        0\n        >>> content = b.read()  # doctest: +SKIP\n        \"\"\"\n        from pandas.io.orc import to_orc\n\n        return to_orc(\n            self, path, engine=engine, index=index, engine_kwargs=engine_kwargs\n        )\n\n    @overload\n    def to_html(\n        self,\n        buf: FilePath | WriteBuffer[str],\n        columns: Axes | None = ...,\n        col_space: ColspaceArgType | None = ...,\n        header: bool = ...,\n        index: bool = ...,\n        na_rep: str = ...,\n        formatters: FormattersType | None = ...,\n        float_format: FloatFormatType | None = ...,\n        sparsify: bool | None = ...,\n        index_names: bool = ...,\n        justify: str | None = ...,\n        max_rows: int | None = ...,\n        max_cols: int | None = ...,\n        show_dimensions: bool | str = ...,\n        decimal: str = ...,\n        bold_rows: bool = ...,\n        classes: str | list | tuple | None = ...,\n        escape: bool = ...,\n        notebook: bool = ...,\n        border: int | bool | None = ...,\n        table_id: str | None = ...,\n        render_links: bool = ...,\n        encoding: str | None = ...,\n    ) -> None:\n        ...\n\n    @overload\n    def to_html(\n        self,\n        buf: None = ...,\n        columns: Axes | None = ...,\n        col_space: ColspaceArgType | None = ...,\n        header: bool = ...,\n        index: bool = ...,\n        na_rep: str = ...,\n        formatters: FormattersType | None = ...,\n        float_format: FloatFormatType | None = ...,\n        sparsify: bool | None = ...,\n        index_names: bool = ...,\n        justify: str | None = ...,\n        max_rows: int | None = ...,\n        max_cols: int | None = ...,\n        show_dimensions: bool | str = ...,\n        decimal: str = ...,\n        bold_rows: bool = ...,\n        classes: str | list | tuple | None = ...,\n        escape: bool = ...,\n        notebook: bool = ...,\n        border: int | bool | None = ...,\n        table_id: str | None = ...,\n        render_links: bool = ...,\n        encoding: str | None = ...,\n    ) -> str:\n        ...\n\n    @deprecate_nonkeyword_arguments(\n        version=\"3.0\", allowed_args=[\"self\", \"buf\"], name=\"to_html\"\n    )\n    @Substitution(\n        header_type=\"bool\",\n        header=\"Whether to print column labels, default True\",\n        col_space_type=\"str or int, list or dict of int or str\",\n        col_space=\"The minimum width of each column in CSS length \"\n        \"units.  An int is assumed to be px units.\",\n    )\n    @Substitution(shared_params=fmt.common_docstring, returns=fmt.return_docstring)\n    def to_html(\n        self,\n        buf: FilePath | WriteBuffer[str] | None = None,\n        columns: Axes | None = None,\n        col_space: ColspaceArgType | None = None,\n        header: bool = True,\n        index: bool = True,\n        na_rep: str = \"NaN\",\n        formatters: FormattersType | None = None,\n        float_format: FloatFormatType | None = None,\n        sparsify: bool | None = None,\n        index_names: bool = True,\n        justify: str | None = None,\n        max_rows: int | None = None,\n        max_cols: int | None = None,\n        show_dimensions: bool | str = False,\n        decimal: str = \".\",\n        bold_rows: bool = True,\n        classes: str | list | tuple | None = None,\n        escape: bool = True,\n        notebook: bool = False,\n        border: int | bool | None = None,\n        table_id: str | None = None,\n        render_links: bool = False,\n        encoding: str | None = None,\n    ) -> str | None:\n        \"\"\"\n        Render a DataFrame as an HTML table.\n        %(shared_params)s\n        bold_rows : bool, default True\n            Make the row labels bold in the output.\n        classes : str or list or tuple, default None\n            CSS class(es) to apply to the resulting html table.\n        escape : bool, default True\n            Convert the characters <, >, and & to HTML-safe sequences.\n        notebook : {True, False}, default False\n            Whether the generated HTML is for IPython Notebook.\n        border : int\n            A ``border=border`` attribute is included in the opening\n            `<table>` tag. Default ``pd.options.display.html.border``.\n        table_id : str, optional\n            A css id is included in the opening `<table>` tag if specified.\n        render_links : bool, default False\n            Convert URLs to HTML links.\n        encoding : str, default \"utf-8\"\n            Set character encoding.\n        %(returns)s\n        See Also\n        --------\n        to_string : Convert DataFrame to a string.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [4, 3]})\n        >>> html_string = '''<table border=\"1\" class=\"dataframe\">\n        ...   <thead>\n        ...     <tr style=\"text-align: right;\">\n        ...       <th></th>\n        ...       <th>col1</th>\n        ...       <th>col2</th>\n        ...     </tr>\n        ...   </thead>\n        ...   <tbody>\n        ...     <tr>\n        ...       <th>0</th>\n        ...       <td>1</td>\n        ...       <td>4</td>\n        ...     </tr>\n        ...     <tr>\n        ...       <th>1</th>\n        ...       <td>2</td>\n        ...       <td>3</td>\n        ...     </tr>\n        ...   </tbody>\n        ... </table>'''\n        >>> assert html_string == df.to_html()\n        \"\"\"\n        if justify is not None and justify not in fmt.VALID_JUSTIFY_PARAMETERS:\n            raise ValueError(\"Invalid value for justify parameter\")\n\n        formatter = fmt.DataFrameFormatter(\n            self,\n            columns=columns,\n            col_space=col_space,\n            na_rep=na_rep,\n            header=header,\n            index=index,\n            formatters=formatters,\n            float_format=float_format,\n            bold_rows=bold_rows,\n            sparsify=sparsify,\n            justify=justify,\n            index_names=index_names,\n            escape=escape,\n            decimal=decimal,\n            max_rows=max_rows,\n            max_cols=max_cols,\n            show_dimensions=show_dimensions,\n        )\n        # TODO: a generic formatter wld b in DataFrameFormatter\n        return fmt.DataFrameRenderer(formatter).to_html(\n            buf=buf,\n            classes=classes,\n            notebook=notebook,\n            border=border,\n            encoding=encoding,\n            table_id=table_id,\n            render_links=render_links,\n        )\n\n    @overload\n    def to_xml(\n        self,\n        path_or_buffer: None = ...,\n        *,\n        index: bool = ...,\n        root_name: str | None = ...,\n        row_name: str | None = ...,\n        na_rep: str | None = ...,\n        attr_cols: list[str] | None = ...,\n        elem_cols: list[str] | None = ...,\n        namespaces: dict[str | None, str] | None = ...,\n        prefix: str | None = ...,\n        encoding: str = ...,\n        xml_declaration: bool | None = ...,\n        pretty_print: bool | None = ...,\n        parser: XMLParsers | None = ...,\n        stylesheet: FilePath | ReadBuffer[str] | ReadBuffer[bytes] | None = ...,\n        compression: CompressionOptions = ...,\n        storage_options: StorageOptions | None = ...,\n    ) -> str:\n        ...\n\n    @overload\n    def to_xml(\n        self,\n        path_or_buffer: FilePath | WriteBuffer[bytes] | WriteBuffer[str],\n        *,\n        index: bool = ...,\n        root_name: str | None = ...,\n        row_name: str | None = ...,\n        na_rep: str | None = ...,\n        attr_cols: list[str] | None = ...,\n        elem_cols: list[str] | None = ...,\n        namespaces: dict[str | None, str] | None = ...,\n        prefix: str | None = ...,\n        encoding: str = ...,\n        xml_declaration: bool | None = ...,\n        pretty_print: bool | None = ...,\n        parser: XMLParsers | None = ...,\n        stylesheet: FilePath | ReadBuffer[str] | ReadBuffer[bytes] | None = ...,\n        compression: CompressionOptions = ...,\n        storage_options: StorageOptions | None = ...,\n    ) -> None:\n        ...\n\n    @deprecate_nonkeyword_arguments(\n        version=\"3.0\", allowed_args=[\"self\", \"path_or_buffer\"], name=\"to_xml\"\n    )\n    @doc(\n        storage_options=_shared_docs[\"storage_options\"],\n        compression_options=_shared_docs[\"compression_options\"] % \"path_or_buffer\",\n    )\n    def to_xml(\n        self,\n        path_or_buffer: FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None = None,\n        index: bool = True,\n        root_name: str | None = \"data\",\n        row_name: str | None = \"row\",\n        na_rep: str | None = None,\n        attr_cols: list[str] | None = None,\n        elem_cols: list[str] | None = None,\n        namespaces: dict[str | None, str] | None = None,\n        prefix: str | None = None,\n        encoding: str = \"utf-8\",\n        xml_declaration: bool | None = True,\n        pretty_print: bool | None = True,\n        parser: XMLParsers | None = \"lxml\",\n        stylesheet: FilePath | ReadBuffer[str] | ReadBuffer[bytes] | None = None,\n        compression: CompressionOptions = \"infer\",\n        storage_options: StorageOptions | None = None,\n    ) -> str | None:\n        \"\"\"\n        Render a DataFrame to an XML document.\n\n        .. versionadded:: 1.3.0\n\n        Parameters\n        ----------\n        path_or_buffer : str, path object, file-like object, or None, default None\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a ``write()`` function. If None, the result is returned\n            as a string.\n        index : bool, default True\n            Whether to include index in XML document.\n        root_name : str, default 'data'\n            The name of root element in XML document.\n        row_name : str, default 'row'\n            The name of row element in XML document.\n        na_rep : str, optional\n            Missing data representation.\n        attr_cols : list-like, optional\n            List of columns to write as attributes in row element.\n            Hierarchical columns will be flattened with underscore\n            delimiting the different levels.\n        elem_cols : list-like, optional\n            List of columns to write as children in row element. By default,\n            all columns output as children of row element. Hierarchical\n            columns will be flattened with underscore delimiting the\n            different levels.\n        namespaces : dict, optional\n            All namespaces to be defined in root element. Keys of dict\n            should be prefix names and values of dict corresponding URIs.\n            Default namespaces should be given empty string key. For\n            example, ::\n\n                namespaces = {{\"\": \"https://example.com\"}}\n\n        prefix : str, optional\n            Namespace prefix to be used for every element and/or attribute\n            in document. This should be one of the keys in ``namespaces``\n            dict.\n        encoding : str, default 'utf-8'\n            Encoding of the resulting document.\n        xml_declaration : bool, default True\n            Whether to include the XML declaration at start of document.\n        pretty_print : bool, default True\n            Whether output should be pretty printed with indentation and\n            line breaks.\n        parser : {{'lxml','etree'}}, default 'lxml'\n            Parser module to use for building of tree. Only 'lxml' and\n            'etree' are supported. With 'lxml', the ability to use XSLT\n            stylesheet is supported.\n        stylesheet : str, path object or file-like object, optional\n            A URL, file-like object, or a raw string containing an XSLT\n            script used to transform the raw XML output. Script should use\n            layout of elements and attributes from original output. This\n            argument requires ``lxml`` to be installed. Only XSLT 1.0\n            scripts and not later versions is currently supported.\n        {compression_options}\n\n            .. versionchanged:: 1.4.0 Zstandard support.\n\n        {storage_options}\n\n        Returns\n        -------\n        None or str\n            If ``io`` is None, returns the resulting XML format as a\n            string. Otherwise returns None.\n\n        See Also\n        --------\n        to_json : Convert the pandas object to a JSON string.\n        to_html : Convert DataFrame to a html.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({{'shape': ['square', 'circle', 'triangle'],\n        ...                    'degrees': [360, 360, 180],\n        ...                    'sides': [4, np.nan, 3]}})\n\n        >>> df.to_xml()  # doctest: +SKIP\n        <?xml version='1.0' encoding='utf-8'?>\n        <data>\n          <row>\n            <index>0</index>\n            <shape>square</shape>\n            <degrees>360</degrees>\n            <sides>4.0</sides>\n          </row>\n          <row>\n            <index>1</index>\n            <shape>circle</shape>\n            <degrees>360</degrees>\n            <sides/>\n          </row>\n          <row>\n            <index>2</index>\n            <shape>triangle</shape>\n            <degrees>180</degrees>\n            <sides>3.0</sides>\n          </row>\n        </data>\n\n        >>> df.to_xml(attr_cols=[\n        ...           'index', 'shape', 'degrees', 'sides'\n        ...           ])  # doctest: +SKIP\n        <?xml version='1.0' encoding='utf-8'?>\n        <data>\n          <row index=\"0\" shape=\"square\" degrees=\"360\" sides=\"4.0\"/>\n          <row index=\"1\" shape=\"circle\" degrees=\"360\"/>\n          <row index=\"2\" shape=\"triangle\" degrees=\"180\" sides=\"3.0\"/>\n        </data>\n\n        >>> df.to_xml(namespaces={{\"doc\": \"https://example.com\"}},\n        ...           prefix=\"doc\")  # doctest: +SKIP\n        <?xml version='1.0' encoding='utf-8'?>\n        <doc:data xmlns:doc=\"https://example.com\">\n          <doc:row>\n            <doc:index>0</doc:index>\n            <doc:shape>square</doc:shape>\n            <doc:degrees>360</doc:degrees>\n            <doc:sides>4.0</doc:sides>\n          </doc:row>\n          <doc:row>\n            <doc:index>1</doc:index>\n            <doc:shape>circle</doc:shape>\n            <doc:degrees>360</doc:degrees>\n            <doc:sides/>\n          </doc:row>\n          <doc:row>\n            <doc:index>2</doc:index>\n            <doc:shape>triangle</doc:shape>\n            <doc:degrees>180</doc:degrees>\n            <doc:sides>3.0</doc:sides>\n          </doc:row>\n        </doc:data>\n        \"\"\"\n\n        from pandas.io.formats.xml import (\n            EtreeXMLFormatter,\n            LxmlXMLFormatter,\n        )\n\n        lxml = import_optional_dependency(\"lxml.etree\", errors=\"ignore\")\n\n        TreeBuilder: type[EtreeXMLFormatter | LxmlXMLFormatter]\n\n        if parser == \"lxml\":\n            if lxml is not None:\n                TreeBuilder = LxmlXMLFormatter\n            else:\n                raise ImportError(\n                    \"lxml not found, please install or use the etree parser.\"\n                )\n\n        elif parser == \"etree\":\n            TreeBuilder = EtreeXMLFormatter\n\n        else:\n            raise ValueError(\"Values for parser can only be lxml or etree.\")\n\n        xml_formatter = TreeBuilder(\n            self,\n            path_or_buffer=path_or_buffer,\n            index=index,\n            root_name=root_name,\n            row_name=row_name,\n            na_rep=na_rep,\n            attr_cols=attr_cols,\n            elem_cols=elem_cols,\n            namespaces=namespaces,\n            prefix=prefix,\n            encoding=encoding,\n            xml_declaration=xml_declaration,\n            pretty_print=pretty_print,\n            stylesheet=stylesheet,\n            compression=compression,\n            storage_options=storage_options,\n        )\n\n        return xml_formatter.write_output()\n\n    # ----------------------------------------------------------------------\n    @doc(INFO_DOCSTRING, **frame_sub_kwargs)\n    def info(\n        self,\n        verbose: bool | None = None,\n        buf: WriteBuffer[str] | None = None,\n        max_cols: int | None = None,\n        memory_usage: bool | str | None = None,\n        show_counts: bool | None = None,\n    ) -> None:\n        info = DataFrameInfo(\n            data=self,\n            memory_usage=memory_usage,\n        )\n        info.render(\n            buf=buf,\n            max_cols=max_cols,\n            verbose=verbose,\n            show_counts=show_counts,\n        )\n\n    def memory_usage(self, index: bool = True, deep: bool = False) -> Series:\n        \"\"\"\n        Return the memory usage of each column in bytes.\n\n        The memory usage can optionally include the contribution of\n        the index and elements of `object` dtype.\n\n        This value is displayed in `DataFrame.info` by default. This can be\n        suppressed by setting ``pandas.options.display.memory_usage`` to False.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the DataFrame's\n            index in returned Series. If ``index=True``, the memory usage of\n            the index is the first item in the output.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned values.\n\n        Returns\n        -------\n        Series\n            A Series whose index is the original column names and whose values\n            is the memory usage of each column in bytes.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of an\n            ndarray.\n        Series.memory_usage : Bytes consumed by a Series.\n        Categorical : Memory-efficient array for string values with\n            many repeated values.\n        DataFrame.info : Concise summary of a DataFrame.\n\n        Notes\n        -----\n        See the :ref:`Frequently Asked Questions <df-memory-usage>` for more\n        details.\n\n        Examples\n        --------\n        >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']\n        >>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t))\n        ...              for t in dtypes])\n        >>> df = pd.DataFrame(data)\n        >>> df.head()\n           int64  float64            complex128  object  bool\n        0      1      1.0              1.0+0.0j       1  True\n        1      1      1.0              1.0+0.0j       1  True\n        2      1      1.0              1.0+0.0j       1  True\n        3      1      1.0              1.0+0.0j       1  True\n        4      1      1.0              1.0+0.0j       1  True\n\n        >>> df.memory_usage()\n        Index           128\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        >>> df.memory_usage(index=False)\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        The memory footprint of `object` dtype columns is ignored by default:\n\n        >>> df.memory_usage(deep=True)\n        Index            128\n        int64          40000\n        float64        40000\n        complex128     80000\n        object        180000\n        bool            5000\n        dtype: int64\n\n        Use a Categorical for efficient storage of an object-dtype column with\n        many repeated values.\n\n        >>> df['object'].astype('category').memory_usage(deep=True)\n        5244\n        \"\"\"\n        result = self._constructor_sliced(\n            [c.memory_usage(index=False, deep=deep) for col, c in self.items()],\n            index=self.columns,\n            dtype=np.intp,\n        )\n        if index:\n            index_memory_usage = self._constructor_sliced(\n                self.index.memory_usage(deep=deep), index=[\"Index\"]\n            )\n            result = index_memory_usage._append(result)\n        return result\n\n    def transpose(self, *args, copy: bool = False) -> DataFrame:\n        \"\"\"\n        Transpose index and columns.\n\n        Reflect the DataFrame over its main diagonal by writing rows as columns\n        and vice-versa. The property :attr:`.T` is an accessor to the method\n        :meth:`transpose`.\n\n        Parameters\n        ----------\n        *args : tuple, optional\n            Accepted for compatibility with NumPy.\n        copy : bool, default False\n            Whether to copy the data after transposing, even for DataFrames\n            with a single dtype.\n\n            Note that a copy is always required for mixed dtype DataFrames,\n            or for DataFrames with any extension types.\n\n            .. note::\n                The `copy` keyword will change behavior in pandas 3.0.\n                `Copy-on-Write\n                <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n                will be enabled by default, which means that all methods with a\n                `copy` keyword will use a lazy copy mechanism to defer the copy and\n                ignore the `copy` keyword. The `copy` keyword will be removed in a\n                future version of pandas.\n\n                You can already get the future behavior and improvements through\n                enabling copy on write ``pd.options.mode.copy_on_write = True``\n\n        Returns\n        -------\n        DataFrame\n            The transposed DataFrame.\n\n        See Also\n        --------\n        numpy.transpose : Permute the dimensions of a given array.\n\n        Notes\n        -----\n        Transposing a DataFrame with mixed dtypes will result in a homogeneous\n        DataFrame with the `object` dtype. In such a case, a copy of the data\n        is always made.\n\n        Examples\n        --------\n        **Square DataFrame with homogeneous dtype**\n\n        >>> d1 = {'col1': [1, 2], 'col2': [3, 4]}\n        >>> df1 = pd.DataFrame(data=d1)\n        >>> df1\n           col1  col2\n        0     1     3\n        1     2     4\n\n        >>> df1_transposed = df1.T  # or df1.transpose()\n        >>> df1_transposed\n              0  1\n        col1  1  2\n        col2  3  4\n\n        When the dtype is homogeneous in the original DataFrame, we get a\n        transposed DataFrame with the same dtype:\n\n        >>> df1.dtypes\n        col1    int64\n        col2    int64\n        dtype: object\n        >>> df1_transposed.dtypes\n        0    int64\n        1    int64\n        dtype: object\n\n        **Non-square DataFrame with mixed dtypes**\n\n        >>> d2 = {'name': ['Alice', 'Bob'],\n        ...       'score': [9.5, 8],\n        ...       'employed': [False, True],\n        ...       'kids': [0, 0]}\n        >>> df2 = pd.DataFrame(data=d2)\n        >>> df2\n            name  score  employed  kids\n        0  Alice    9.5     False     0\n        1    Bob    8.0      True     0\n\n        >>> df2_transposed = df2.T  # or df2.transpose()\n        >>> df2_transposed\n                      0     1\n        name      Alice   Bob\n        score       9.5   8.0\n        employed  False  True\n        kids          0     0\n\n        When the DataFrame has mixed dtypes, we get a transposed DataFrame with\n        the `object` dtype:\n\n        >>> df2.dtypes\n        name         object\n        score       float64\n        employed       bool\n        kids          int64\n        dtype: object\n        >>> df2_transposed.dtypes\n        0    object\n        1    object\n        dtype: object\n        \"\"\"\n        nv.validate_transpose(args, {})\n        # construct the args\n\n        dtypes = list(self.dtypes)\n\n        if self._can_fast_transpose:\n            # Note: tests pass without this, but this improves perf quite a bit.\n            new_vals = self._values.T\n            if copy and not using_copy_on_write():\n                new_vals = new_vals.copy()\n\n            result = self._constructor(\n                new_vals,\n                index=self.columns,\n                columns=self.index,\n                copy=False,\n                dtype=new_vals.dtype,\n            )\n            if using_copy_on_write() and len(self) > 0:\n                result._mgr.add_references(self._mgr)  # type: ignore[arg-type]\n\n        elif (\n            self._is_homogeneous_type\n            and dtypes\n            and isinstance(dtypes[0], ExtensionDtype)\n        ):\n            new_values: list\n            if isinstance(dtypes[0], BaseMaskedDtype):\n                # We have masked arrays with the same dtype. We can transpose faster.\n                from pandas.core.arrays.masked import (\n                    transpose_homogeneous_masked_arrays,\n                )\n\n                new_values = transpose_homogeneous_masked_arrays(\n                    cast(Sequence[BaseMaskedArray], self._iter_column_arrays())\n                )\n            elif isinstance(dtypes[0], ArrowDtype):\n                # We have arrow EAs with the same dtype. We can transpose faster.\n                from pandas.core.arrays.arrow.array import (\n                    ArrowExtensionArray,\n                    transpose_homogeneous_pyarrow,\n                )\n\n                new_values = transpose_homogeneous_pyarrow(\n                    cast(Sequence[ArrowExtensionArray], self._iter_column_arrays())\n                )\n            else:\n                # We have other EAs with the same dtype. We preserve dtype in transpose.\n                dtyp = dtypes[0]\n                arr_typ = dtyp.construct_array_type()\n                values = self.values\n                new_values = [arr_typ._from_sequence(row, dtype=dtyp) for row in values]\n\n            result = type(self)._from_arrays(\n                new_values,\n                index=self.columns,\n                columns=self.index,\n                verify_integrity=False,\n            )\n\n        else:\n            new_arr = self.values.T\n            if copy and not using_copy_on_write():\n                new_arr = new_arr.copy()\n            result = self._constructor(\n                new_arr,\n                index=self.columns,\n                columns=self.index,\n                dtype=new_arr.dtype,\n                # We already made a copy (more than one block)\n                copy=False,\n            )\n\n        return result.__finalize__(self, method=\"transpose\")\n\n    @property\n    def T(self) -> DataFrame:\n        \"\"\"\n        The transpose of the DataFrame.\n\n        Returns\n        -------\n        DataFrame\n            The transposed DataFrame.\n\n        See Also\n        --------\n        DataFrame.transpose : Transpose index and columns.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df\n           col1  col2\n        0     1     3\n        1     2     4\n\n        >>> df.T\n              0  1\n        col1  1  2\n        col2  3  4\n        \"\"\"\n        return self.transpose()\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    def _ixs(self, i: int, axis: AxisInt = 0) -> Series:\n        \"\"\"\n        Parameters\n        ----------\n        i : int\n        axis : int\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        # irow\n        if axis == 0:\n            new_mgr = self._mgr.fast_xs(i)\n\n            # if we are a copy, mark as such\n            copy = isinstance(new_mgr.array, np.ndarray) and new_mgr.array.base is None\n            result = self._constructor_sliced_from_mgr(new_mgr, axes=new_mgr.axes)\n            result._name = self.index[i]\n            result = result.__finalize__(self)\n            result._set_is_copy(self, copy=copy)\n            return result\n\n        # icol\n        else:\n            label = self.columns[i]\n\n            col_mgr = self._mgr.iget(i)\n            result = self._box_col_values(col_mgr, i)\n\n            # this is a cached value, mark it so\n            result._set_as_cached(label, self)\n            return result\n\n    def _get_column_array(self, i: int) -> ArrayLike:\n        \"\"\"\n        Get the values of the i'th column (ndarray or ExtensionArray, as stored\n        in the Block)\n\n        Warning! The returned array is a view but doesn't handle Copy-on-Write,\n        so this should be used with caution (for read-only purposes).\n        \"\"\"\n        return self._mgr.iget_values(i)\n\n    def _iter_column_arrays(self) -> Iterator[ArrayLike]:\n        \"\"\"\n        Iterate over the arrays of all columns in order.\n        This returns the values as stored in the Block (ndarray or ExtensionArray).\n\n        Warning! The returned array is a view but doesn't handle Copy-on-Write,\n        so this should be used with caution (for read-only purposes).\n        \"\"\"\n        if isinstance(self._mgr, ArrayManager):\n            yield from self._mgr.arrays\n        else:\n            for i in range(len(self.columns)):\n                yield self._get_column_array(i)\n\n    def _getitem_nocopy(self, key: list):\n        \"\"\"\n        Behaves like __getitem__, but returns a view in cases where __getitem__\n        would make a copy.\n        \"\"\"\n        # TODO(CoW): can be removed if/when we are always Copy-on-Write\n        indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n        new_axis = self.columns[indexer]\n\n        new_mgr = self._mgr.reindex_indexer(\n            new_axis,\n            indexer,\n            axis=0,\n            allow_dups=True,\n            copy=False,\n            only_slice=True,\n        )\n        result = self._constructor_from_mgr(new_mgr, axes=new_mgr.axes)\n        result = result.__finalize__(self)\n        return result\n\n    def __getitem__(self, key):\n        check_dict_or_set_indexers(key)\n        key = lib.item_from_zerodim(key)\n        key = com.apply_if_callable(key, self)\n\n        if is_hashable(key) and not is_iterator(key):\n            # is_iterator to exclude generator e.g. test_getitem_listlike\n            # shortcut if the key is in columns\n            is_mi = isinstance(self.columns, MultiIndex)\n            # GH#45316 Return view if key is not duplicated\n            # Only use drop_duplicates with duplicates for performance\n            if not is_mi and (\n                self.columns.is_unique\n                and key in self.columns\n                or key in self.columns.drop_duplicates(keep=False)\n            ):\n                return self._get_item_cache(key)\n\n            elif is_mi and self.columns.is_unique and key in self.columns:\n                return self._getitem_multilevel(key)\n\n        # Do we have a slicer (on rows)?\n        if isinstance(key, slice):\n            return self._getitem_slice(key)\n\n        # Do we have a (boolean) DataFrame?\n        if isinstance(key, DataFrame):\n            return self.where(key)\n\n        # Do we have a (boolean) 1d indexer?\n        if com.is_bool_indexer(key):\n            return self._getitem_bool_array(key)\n\n        # We are left with two options: a single key, and a collection of keys,\n        # We interpret tuples as collections only for non-MultiIndex\n        is_single_key = isinstance(key, tuple) or not is_list_like(key)\n\n        if is_single_key:\n            if self.columns.nlevels > 1:\n                return self._getitem_multilevel(key)\n            indexer = self.columns.get_loc(key)\n            if is_integer(indexer):\n                indexer = [indexer]\n        else:\n            if is_iterator(key):\n                key = list(key)\n            indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n\n        # take() does not accept boolean indexers\n        if getattr(indexer, \"dtype\", None) == bool:\n            indexer = np.where(indexer)[0]\n\n        if isinstance(indexer, slice):\n            return self._slice(indexer, axis=1)\n\n        data = self._take_with_is_copy(indexer, axis=1)\n\n        if is_single_key:\n            # What does looking for a single key in a non-unique index return?\n            # The behavior is inconsistent. It returns a Series, except when\n            # - the key itself is repeated (test on data.shape, #9519), or\n            # - we have a MultiIndex on columns (test on self.columns, #21309)\n            if data.shape[1] == 1 and not isinstance(self.columns, MultiIndex):\n                # GH#26490 using data[key] can cause RecursionError\n                return data._get_item_cache(key)\n\n        return data\n\n    def _getitem_bool_array(self, key):\n        # also raises Exception if object array with NA values\n        # warning here just in case -- previously __setitem__ was\n        # reindexing but __getitem__ was not; it seems more reasonable to\n        # go with the __setitem__ behavior since that is more consistent\n        # with all other indexing behavior\n        if isinstance(key, Series) and not key.index.equals(self.index):\n            warnings.warn(\n                \"Boolean Series key will be reindexed to match DataFrame index.\",\n                UserWarning,\n                stacklevel=find_stack_level(),\n            )\n        elif len(key) != len(self.index):\n            raise ValueError(\n                f\"Item wrong length {len(key)} instead of {len(self.index)}.\"\n            )\n\n        # check_bool_indexer will throw exception if Series key cannot\n        # be reindexed to match DataFrame rows\n        key = check_bool_indexer(self.index, key)\n\n        if key.all():\n            return self.copy(deep=None)\n\n        indexer = key.nonzero()[0]\n        return self._take_with_is_copy(indexer, axis=0)\n\n    def _getitem_multilevel(self, key):\n        # self.columns is a MultiIndex\n        loc = self.columns.get_loc(key)\n        if isinstance(loc, (slice, np.ndarray)):\n            new_columns = self.columns[loc]\n            result_columns = maybe_droplevels(new_columns, key)\n            result = self.iloc[:, loc]\n            result.columns = result_columns\n\n            # If there is only one column being returned, and its name is\n            # either an empty string, or a tuple with an empty string as its\n            # first element, then treat the empty string as a placeholder\n            # and return the column as if the user had provided that empty\n            # string in the key. If the result is a Series, exclude the\n            # implied empty string from its name.\n            if len(result.columns) == 1:\n                # e.g. test_frame_getitem_multicolumn_empty_level,\n                #  test_frame_mixed_depth_get, test_loc_setitem_single_column_slice\n                top = result.columns[0]\n                if isinstance(top, tuple):\n                    top = top[0]\n                if top == \"\":\n                    result = result[\"\"]\n                    if isinstance(result, Series):\n                        result = self._constructor_sliced(\n                            result, index=self.index, name=key\n                        )\n\n            result._set_is_copy(self)\n            return result\n        else:\n            # loc is neither a slice nor ndarray, so must be an int\n            return self._ixs(loc, axis=1)\n\n    def _get_value(self, index, col, takeable: bool = False) -> Scalar:\n        \"\"\"\n        Quickly retrieve single value at passed column and index.\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        takeable : interpret the index/col as indexers, default False\n\n        Returns\n        -------\n        scalar\n\n        Notes\n        -----\n        Assumes that both `self.index._index_as_unique` and\n        `self.columns._index_as_unique`; Caller is responsible for checking.\n        \"\"\"\n        if takeable:\n            series = self._ixs(col, axis=1)\n            return series._values[index]\n\n        series = self._get_item_cache(col)\n        engine = self.index._engine\n\n        if not isinstance(self.index, MultiIndex):\n            # CategoricalIndex: Trying to use the engine fastpath may give incorrect\n            #  results if our categories are integers that dont match our codes\n            # IntervalIndex: IntervalTree has no get_loc\n            row = self.index.get_loc(index)\n            return series._values[row]\n\n        # For MultiIndex going through engine effectively restricts us to\n        #  same-length tuples; see test_get_set_value_no_partial_indexing\n        loc = engine.get_loc(index)\n        return series._values[loc]\n\n    def isetitem(self, loc, value) -> None:\n        \"\"\"\n        Set the given value in the column with position `loc`.\n\n        This is a positional analogue to ``__setitem__``.\n\n        Parameters\n        ----------\n        loc : int or sequence of ints\n            Index position for the column.\n        value : scalar or arraylike\n            Value(s) for the column.\n\n        Notes\n        -----\n        ``frame.isetitem(loc, value)`` is an in-place method as it will\n        modify the DataFrame in place (not returning a new object). In contrast to\n        ``frame.iloc[:, i] = value`` which will try to update the existing values in\n        place, ``frame.isetitem(loc, value)`` will not update the values of the column\n        itself in place, it will instead insert a new array.\n\n        In cases where ``frame.columns`` is unique, this is equivalent to\n        ``frame[frame.columns[i]] = value``.\n        \"\"\"\n        if isinstance(value, DataFrame):\n            if is_integer(loc):\n                loc = [loc]\n\n            if len(loc) != len(value.columns):\n                raise ValueError(\n                    f\"Got {len(loc)} positions but value has {len(value.columns)} \"\n                    f\"columns.\"\n                )\n\n            for i, idx in enumerate(loc):\n                arraylike, refs = self._sanitize_column(value.iloc[:, i])\n                self._iset_item_mgr(idx, arraylike, inplace=False, refs=refs)\n            return\n\n        arraylike, refs = self._sanitize_column(value)\n        self._iset_item_mgr(loc, arraylike, inplace=False, refs=refs)\n\n    def __setitem__(self, key, value) -> None:\n        if not PYPY and using_copy_on_write():\n            if sys.getrefcount(self) <= 3:\n                warnings.warn(\n                    _chained_assignment_msg, ChainedAssignmentError, stacklevel=2\n                )\n        elif not PYPY and not using_copy_on_write():\n            if sys.getrefcount(self) <= 3 and (\n                warn_copy_on_write()\n                or (\n                    not warn_copy_on_write()\n                    and any(b.refs.has_reference() for b in self._mgr.blocks)  # type: ignore[union-attr]\n                )\n            ):\n                warnings.warn(\n                    _chained_assignment_warning_msg, FutureWarning, stacklevel=2\n                )\n\n        key = com.apply_if_callable(key, self)\n\n        # see if we can slice the rows\n        if isinstance(key, slice):\n            slc = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._setitem_slice(slc, value)\n\n        if isinstance(key, DataFrame) or getattr(key, \"ndim\", None) == 2:\n            self._setitem_frame(key, value)\n        elif isinstance(key, (Series, np.ndarray, list, Index)):\n            self._setitem_array(key, value)\n        elif isinstance(value, DataFrame):\n            self._set_item_frame_value(key, value)\n        elif (\n            is_list_like(value)\n            and not self.columns.is_unique\n            and 1 < len(self.columns.get_indexer_for([key])) == len(value)\n        ):\n            # Column to set is duplicated\n            self._setitem_array([key], value)\n        else:\n            # set column\n            self._set_item(key, value)\n\n    def _setitem_slice(self, key: slice, value) -> None:\n        # NB: we can't just use self.loc[key] = value because that\n        #  operates on labels and we need to operate positional for\n        #  backwards-compat, xref GH#31469\n        self._check_setitem_copy()\n        self.iloc[key] = value\n\n    def _setitem_array(self, key, value):\n        # also raises Exception if object array with NA values\n        if com.is_bool_indexer(key):\n            # bool indexer is indexing along rows\n            if len(key) != len(self.index):\n                raise ValueError(\n                    f\"Item wrong length {len(key)} instead of {len(self.index)}!\"\n                )\n            key = check_bool_indexer(self.index, key)\n            indexer = key.nonzero()[0]\n            self._check_setitem_copy()\n            if isinstance(value, DataFrame):\n                # GH#39931 reindex since iloc does not align\n                value = value.reindex(self.index.take(indexer))\n            self.iloc[indexer] = value\n\n        else:\n            # Note: unlike self.iloc[:, indexer] = value, this will\n            #  never try to overwrite values inplace\n\n            if isinstance(value, DataFrame):\n                check_key_length(self.columns, key, value)\n                for k1, k2 in zip(key, value.columns):\n                    self[k1] = value[k2]\n\n            elif not is_list_like(value):\n                for col in key:\n                    self[col] = value\n\n            elif isinstance(value, np.ndarray) and value.ndim == 2:\n                self._iset_not_inplace(key, value)\n\n            elif np.ndim(value) > 1:\n                # list of lists\n                value = DataFrame(value).values\n                return self._setitem_array(key, value)\n\n            else:\n                self._iset_not_inplace(key, value)\n\n    def _iset_not_inplace(self, key, value):\n        # GH#39510 when setting with df[key] = obj with a list-like key and\n        #  list-like value, we iterate over those listlikes and set columns\n        #  one at a time.  This is different from dispatching to\n        #  `self.loc[:, key]= value`  because loc.__setitem__ may overwrite\n        #  data inplace, whereas this will insert new arrays.\n\n        def igetitem(obj, i: int):\n            # Note: we catch DataFrame obj before getting here, but\n            #  hypothetically would return obj.iloc[:, i]\n            if isinstance(obj, np.ndarray):\n                return obj[..., i]\n            else:\n                return obj[i]\n\n        if self.columns.is_unique:\n            if np.shape(value)[-1] != len(key):\n                raise ValueError(\"Columns must be same length as key\")\n\n            for i, col in enumerate(key):\n                self[col] = igetitem(value, i)\n\n        else:\n            ilocs = self.columns.get_indexer_non_unique(key)[0]\n            if (ilocs < 0).any():\n                # key entries not in self.columns\n                raise NotImplementedError\n\n            if np.shape(value)[-1] != len(ilocs):\n                raise ValueError(\"Columns must be same length as key\")\n\n            assert np.ndim(value) <= 2\n\n            orig_columns = self.columns\n\n            # Using self.iloc[:, i] = ... may set values inplace, which\n            #  by convention we do not do in __setitem__\n            try:\n                self.columns = Index(range(len(self.columns)))\n                for i, iloc in enumerate(ilocs):\n                    self[iloc] = igetitem(value, i)\n            finally:\n                self.columns = orig_columns\n\n    def _setitem_frame(self, key, value):\n        # support boolean setting with DataFrame input, e.g.\n        # df[df > df2] = 0\n        if isinstance(key, np.ndarray):\n            if key.shape != self.shape:\n                raise ValueError(\"Array conditional must be same shape as self\")\n            key = self._constructor(key, **self._construct_axes_dict(), copy=False)\n\n        if key.size and not all(is_bool_dtype(dtype) for dtype in key.dtypes):\n            raise TypeError(\n                \"Must pass DataFrame or 2-d ndarray with boolean values only\"\n            )\n\n        self._check_setitem_copy()\n        self._where(-key, value, inplace=True)\n\n    def _set_item_frame_value(self, key, value: DataFrame) -> None:\n        self._ensure_valid_index(value)\n\n        # align columns\n        if key in self.columns:\n            loc = self.columns.get_loc(key)\n            cols = self.columns[loc]\n            len_cols = 1 if is_scalar(cols) or isinstance(cols, tuple) else len(cols)\n            if len_cols != len(value.columns):\n                raise ValueError(\"Columns must be same length as key\")\n\n            # align right-hand-side columns if self.columns\n            # is multi-index and self[key] is a sub-frame\n            if isinstance(self.columns, MultiIndex) and isinstance(\n                loc, (slice, Series, np.ndarray, Index)\n            ):\n                cols_droplevel = maybe_droplevels(cols, key)\n                if len(cols_droplevel) and not cols_droplevel.equals(value.columns):\n                    value = value.reindex(cols_droplevel, axis=1)\n\n                for col, col_droplevel in zip(cols, cols_droplevel):\n                    self[col] = value[col_droplevel]\n                return\n\n            if is_scalar(cols):\n                self[cols] = value[value.columns[0]]\n                return\n\n            locs: np.ndarray | list\n            if isinstance(loc, slice):\n                locs = np.arange(loc.start, loc.stop, loc.step)\n            elif is_scalar(loc):\n                locs = [loc]\n            else:\n                locs = loc.nonzero()[0]\n\n            return self.isetitem(locs, value)\n\n        if len(value.columns) > 1:\n            raise ValueError(\n                \"Cannot set a DataFrame with multiple columns to the single \"\n                f\"column {key}\"\n            )\n        elif len(value.columns) == 0:\n            raise ValueError(\n                f\"Cannot set a DataFrame without columns to the column {key}\"\n            )\n\n        self[key] = value[value.columns[0]]\n\n    def _iset_item_mgr(\n        self,\n        loc: int | slice | np.ndarray,\n        value,\n        inplace: bool = False,\n        refs: BlockValuesRefs | None = None,\n    ) -> None:\n        # when called from _set_item_mgr loc can be anything returned from get_loc\n        self._mgr.iset(loc, value, inplace=inplace, refs=refs)\n        self._clear_item_cache()\n\n    def _set_item_mgr(\n        self, key, value: ArrayLike, refs: BlockValuesRefs | None = None\n    ) -> None:\n        try:\n            loc = self._info_axis.get_loc(key)\n        except KeyError:\n            # This item wasn't present, just insert at end\n            self._mgr.insert(len(self._info_axis), key, value, refs)\n        else:\n            self._iset_item_mgr(loc, value, refs=refs)\n\n        # check if we are modifying a copy\n        # try to set first as we want an invalid\n        # value exception to occur first\n        if len(self):\n            self._check_setitem_copy()\n\n    def _iset_item(self, loc: int, value: Series, inplace: bool = True) -> None:\n        # We are only called from _replace_columnwise which guarantees that\n        # no reindex is necessary\n        if using_copy_on_write():\n            self._iset_item_mgr(\n                loc, value._values, inplace=inplace, refs=value._references\n            )\n        else:\n            self._iset_item_mgr(loc, value._values.copy(), inplace=True)\n\n        # check if we are modifying a copy\n        # try to set first as we want an invalid\n        # value exception to occur first\n        if len(self):\n            self._check_setitem_copy()\n\n    def _set_item(self, key, value) -> None:\n        \"\"\"\n        Add series to DataFrame in specified column.\n\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\n        same length as the DataFrames index or an error will be thrown.\n\n        Series/TimeSeries will be conformed to the DataFrames index to\n        ensure homogeneity.\n        \"\"\"\n        value, refs = self._sanitize_column(value)\n\n        if (\n            key in self.columns\n            and value.ndim == 1\n            and not isinstance(value.dtype, ExtensionDtype)\n        ):\n            # broadcast across multiple columns if necessary\n            if not self.columns.is_unique or isinstance(self.columns, MultiIndex):\n                existing_piece = self[key]\n                if isinstance(existing_piece, DataFrame):\n                    value = np.tile(value, (len(existing_piece.columns), 1)).T\n                    refs = None\n\n        self._set_item_mgr(key, value, refs)\n\n    def _set_value(\n        self, index: IndexLabel, col, value: Scalar, takeable: bool = False\n    ) -> None:\n        \"\"\"\n        Put single value at passed column and index.\n\n        Parameters\n        ----------\n        index : Label\n            row label\n        col : Label\n            column label\n        value : scalar\n        takeable : bool, default False\n            Sets whether or not index/col interpreted as indexers\n        \"\"\"\n        try:\n            if takeable:\n                icol = col\n                iindex = cast(int, index)\n            else:\n                icol = self.columns.get_loc(col)\n                iindex = self.index.get_loc(index)\n            self._mgr.column_setitem(icol, iindex, value, inplace_only=True)\n            self._clear_item_cache()\n\n        except (KeyError, TypeError, ValueError, LossySetitemError):\n            # get_loc might raise a KeyError for missing labels (falling back\n            #  to (i)loc will do expansion of the index)\n            # column_setitem will do validation that may raise TypeError,\n            #  ValueError, or LossySetitemError\n            # set using a non-recursive method & reset the cache\n            if takeable:\n                self.iloc[index, col] = value\n            else:\n                self.loc[index, col] = value\n            self._item_cache.pop(col, None)\n\n        except InvalidIndexError as ii_err:\n            # GH48729: Seems like you are trying to assign a value to a\n            # row when only scalar options are permitted\n            raise InvalidIndexError(\n                f\"You can only assign a scalar value not a {type(value)}\"\n            ) from ii_err\n\n    def _ensure_valid_index(self, value) -> None:\n        \"\"\"\n        Ensure that if we don't have an index, that we can create one from the\n        passed value.\n        \"\"\"\n        # GH5632, make sure that we are a Series convertible\n        if not len(self.index) and is_list_like(value) and len(value):\n            if not isinstance(value, DataFrame):\n                try:\n                    value = Series(value)\n                except (ValueError, NotImplementedError, TypeError) as err:\n                    raise ValueError(\n                        \"Cannot set a frame with no defined index \"\n                        \"and a value that cannot be converted to a Series\"\n                    ) from err\n\n            # GH31368 preserve name of index\n            index_copy = value.index.copy()\n            if self.index.name is not None:\n                index_copy.name = self.index.name\n\n            self._mgr = self._mgr.reindex_axis(index_copy, axis=1, fill_value=np.nan)\n\n    def _box_col_values(self, values: SingleDataManager, loc: int) -> Series:\n        \"\"\"\n        Provide boxed values for a column.\n        \"\"\"\n        # Lookup in columns so that if e.g. a str datetime was passed\n        #  we attach the Timestamp object as the name.\n        name = self.columns[loc]\n        # We get index=self.index bc values is a SingleDataManager\n        obj = self._constructor_sliced_from_mgr(values, axes=values.axes)\n        obj._name = name\n        return obj.__finalize__(self)\n\n    # ----------------------------------------------------------------------\n    # Lookup Caching\n\n    def _clear_item_cache(self) -> None:\n        self._item_cache.clear()\n\n    def _get_item_cache(self, item: Hashable) -> Series:\n        \"\"\"Return the cached item, item represents a label indexer.\"\"\"\n        if using_copy_on_write() or warn_copy_on_write():\n            loc = self.columns.get_loc(item)\n            return self._ixs(loc, axis=1)\n\n        cache = self._item_cache\n        res = cache.get(item)\n        if res is None:\n            # All places that call _get_item_cache have unique columns,\n            #  pending resolution of GH#33047\n\n            loc = self.columns.get_loc(item)\n            res = self._ixs(loc, axis=1)\n\n            cache[item] = res\n\n            # for a chain\n            res._is_copy = self._is_copy\n        return res\n\n    def _reset_cacher(self) -> None:\n        # no-op for DataFrame\n        pass\n\n    def _maybe_cache_changed(self, item, value: Series, inplace: bool) -> None:\n        \"\"\"\n        The object has called back to us saying maybe it has changed.\n        \"\"\"\n        loc = self._info_axis.get_loc(item)\n        arraylike = value._values\n\n        old = self._ixs(loc, axis=1)\n        if old._values is value._values and inplace:\n            # GH#46149 avoid making unnecessary copies/block-splitting\n            return\n\n        self._mgr.iset(loc, arraylike, inplace=inplace)\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    @overload\n    def query(self, expr: str, *, inplace: Literal[False] = ..., **kwargs) -> DataFrame:\n        ...\n\n    @overload\n    def query(self, expr: str, *, inplace: Literal[True], **kwargs) -> None:\n        ...\n\n    @overload\n    def query(self, expr: str, *, inplace: bool = ..., **kwargs) -> DataFrame | None:\n        ...\n\n    def query(self, expr: str, *, inplace: bool = False, **kwargs) -> DataFrame | None:\n        \"\"\"\n        Query the columns of a DataFrame with a boolean expression.\n\n        Parameters\n        ----------\n        expr : str\n            The query string to evaluate.\n\n            You can refer to variables\n            in the environment by prefixing them with an '@' character like\n            ``@a + b``.\n\n            You can refer to column names that are not valid Python variable names\n            by surrounding them in backticks. Thus, column names containing spaces\n            or punctuations (besides underscores) or starting with digits must be\n            surrounded by backticks. (For example, a column named \"Area (cm^2)\" would\n            be referenced as ```Area (cm^2)```). Column names which are Python keywords\n            (like \"list\", \"for\", \"import\", etc) cannot be used.\n\n            For example, if one of your columns is called ``a a`` and you want\n            to sum it with ``b``, your query should be ```a a` + b``.\n\n        inplace : bool\n            Whether to modify the DataFrame rather than creating a new one.\n        **kwargs\n            See the documentation for :func:`eval` for complete details\n            on the keyword arguments accepted by :meth:`DataFrame.query`.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame resulting from the provided query expression or\n            None if ``inplace=True``.\n\n        See Also\n        --------\n        eval : Evaluate a string describing operations on\n            DataFrame columns.\n        DataFrame.eval : Evaluate a string describing operations on\n            DataFrame columns.\n\n        Notes\n        -----\n        The result of the evaluation of this expression is first passed to\n        :attr:`DataFrame.loc` and if that fails because of a\n        multidimensional key (e.g., a DataFrame) then the result will be passed\n        to :meth:`DataFrame.__getitem__`.\n\n        This method uses the top-level :func:`eval` function to\n        evaluate the passed query.\n\n        The :meth:`~pandas.DataFrame.query` method uses a slightly\n        modified Python syntax by default. For example, the ``&`` and ``|``\n        (bitwise) operators have the precedence of their boolean cousins,\n        :keyword:`and` and :keyword:`or`. This *is* syntactically valid Python,\n        however the semantics are different.\n\n        You can change the semantics of the expression by passing the keyword\n        argument ``parser='python'``. This enforces the same semantics as\n        evaluation in Python space. Likewise, you can pass ``engine='python'``\n        to evaluate an expression using Python itself as a backend. This is not\n        recommended as it is inefficient compared to using ``numexpr`` as the\n        engine.\n\n        The :attr:`DataFrame.index` and\n        :attr:`DataFrame.columns` attributes of the\n        :class:`~pandas.DataFrame` instance are placed in the query namespace\n        by default, which allows you to treat both the index and columns of the\n        frame as a column in the frame.\n        The identifier ``index`` is used for the frame index; you can also\n        use the name of the index to identify it in a query. Please note that\n        Python keywords may not be used as identifiers.\n\n        For further details and examples see the ``query`` documentation in\n        :ref:`indexing <indexing.query>`.\n\n        *Backtick quoted variables*\n\n        Backtick quoted variables are parsed as literal Python code and\n        are converted internally to a Python valid identifier.\n        This can lead to the following problems.\n\n        During parsing a number of disallowed characters inside the backtick\n        quoted string are replaced by strings that are allowed as a Python identifier.\n        These characters include all operators in Python, the space character, the\n        question mark, the exclamation mark, the dollar sign, and the euro sign.\n        For other characters that fall outside the ASCII range (U+0001..U+007F)\n        and those that are not further specified in PEP 3131,\n        the query parser will raise an error.\n        This excludes whitespace different than the space character,\n        but also the hashtag (as it is used for comments) and the backtick\n        itself (backtick can also not be escaped).\n\n        In a special case, quotes that make a pair around a backtick can\n        confuse the parser.\n        For example, ```it's` > `that's``` will raise an error,\n        as it forms a quoted string (``'s > `that'``) with a backtick inside.\n\n        See also the Python documentation about lexical analysis\n        (https://docs.python.org/3/reference/lexical_analysis.html)\n        in combination with the source code in :mod:`pandas.core.computation.parsing`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': range(1, 6),\n        ...                    'B': range(10, 0, -2),\n        ...                    'C C': range(10, 5, -1)})\n        >>> df\n           A   B  C C\n        0  1  10   10\n        1  2   8    9\n        2  3   6    8\n        3  4   4    7\n        4  5   2    6\n        >>> df.query('A > B')\n           A  B  C C\n        4  5  2    6\n\n        The previous expression is equivalent to\n\n        >>> df[df.A > df.B]\n           A  B  C C\n        4  5  2    6\n\n        For columns with spaces in their name, you can use backtick quoting.\n\n        >>> df.query('B == `C C`')\n           A   B  C C\n        0  1  10   10\n\n        The previous expression is equivalent to\n\n        >>> df[df.B == df['C C']]\n           A   B  C C\n        0  1  10   10\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if not isinstance(expr, str):\n            msg = f\"expr must be a string to be evaluated, {type(expr)} given\"\n            raise ValueError(msg)\n        kwargs[\"level\"] = kwargs.pop(\"level\", 0) + 1\n        kwargs[\"target\"] = None\n        res = self.eval(expr, **kwargs)\n\n        try:\n            result = self.loc[res]\n        except ValueError:\n            # when res is multi-dimensional loc raises, but this is sometimes a\n            # valid query\n            result = self[res]\n\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    @overload\n    def eval(self, expr: str, *, inplace: Literal[False] = ..., **kwargs) -> Any:\n        ...\n\n    @overload\n    def eval(self, expr: str, *, inplace: Literal[True], **kwargs) -> None:\n        ...\n\n    def eval(self, expr: str, *, inplace: bool = False, **kwargs) -> Any | None:\n        \"\"\"\n        Evaluate a string describing operations on DataFrame columns.\n\n        Operates on columns only, not specific rows or elements.  This allows\n        `eval` to run arbitrary code, which can make you vulnerable to code\n        injection if you pass user input to this function.\n\n        Parameters\n        ----------\n        expr : str\n            The expression string to evaluate.\n        inplace : bool, default False\n            If the expression contains an assignment, whether to perform the\n            operation inplace and mutate the existing DataFrame. Otherwise,\n            a new DataFrame is returned.\n        **kwargs\n            See the documentation for :func:`eval` for complete details\n            on the keyword arguments accepted by\n            :meth:`~pandas.DataFrame.query`.\n\n        Returns\n        -------\n        ndarray, scalar, pandas object, or None\n            The result of the evaluation or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.query : Evaluates a boolean expression to query the columns\n            of a frame.\n        DataFrame.assign : Can evaluate an expression or function to create new\n            values for a column.\n        eval : Evaluate a Python expression as a string using various\n            backends.\n\n        Notes\n        -----\n        For more details see the API documentation for :func:`~eval`.\n        For detailed examples see :ref:`enhancing performance with eval\n        <enhancingperf.eval>`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)})\n        >>> df\n           A   B\n        0  1  10\n        1  2   8\n        2  3   6\n        3  4   4\n        4  5   2\n        >>> df.eval('A + B')\n        0    11\n        1    10\n        2     9\n        3     8\n        4     7\n        dtype: int64\n\n        Assignment is allowed though by default the original DataFrame is not\n        modified.\n\n        >>> df.eval('C = A + B')\n           A   B   C\n        0  1  10  11\n        1  2   8  10\n        2  3   6   9\n        3  4   4   8\n        4  5   2   7\n        >>> df\n           A   B\n        0  1  10\n        1  2   8\n        2  3   6\n        3  4   4\n        4  5   2\n\n        Multiple columns can be assigned to using multi-line expressions:\n\n        >>> df.eval(\n        ...     '''\n        ... C = A + B\n        ... D = A - B\n        ... '''\n        ... )\n           A   B   C  D\n        0  1  10  11 -9\n        1  2   8  10 -6\n        2  3   6   9 -3\n        3  4   4   8  0\n        4  5   2   7  3\n        \"\"\"\n        from pandas.core.computation.eval import eval as _eval\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        kwargs[\"level\"] = kwargs.pop(\"level\", 0) + 1\n        index_resolvers = self._get_index_resolvers()\n        column_resolvers = self._get_cleaned_column_resolvers()\n        resolvers = column_resolvers, index_resolvers\n        if \"target\" not in kwargs:\n            kwargs[\"target\"] = self\n        kwargs[\"resolvers\"] = tuple(kwargs.get(\"resolvers\", ())) + resolvers\n\n        return _eval(expr, inplace=inplace, **kwargs)\n\n    def select_dtypes(self, include=None, exclude=None) -> Self:\n        \"\"\"\n        Return a subset of the DataFrame's columns based on the column dtypes.\n\n        Parameters\n        ----------\n        include, exclude : scalar or list-like\n            A selection of dtypes or strings to be included/excluded. At least\n            one of these parameters must be supplied.\n\n        Returns\n        -------\n        DataFrame\n            The subset of the frame including the dtypes in ``include`` and\n            excluding the dtypes in ``exclude``.\n\n        Raises\n        ------\n        ValueError\n            * If both of ``include`` and ``exclude`` are empty\n            * If ``include`` and ``exclude`` have overlapping elements\n            * If any kind of string dtype is passed in.\n\n        See Also\n        --------\n        DataFrame.dtypes: Return Series with the data type of each column.\n\n        Notes\n        -----\n        * To select all *numeric* types, use ``np.number`` or ``'number'``\n        * To select strings you must use the ``object`` dtype, but note that\n          this will return *all* object dtype columns\n        * See the `numpy dtype hierarchy\n          <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__\n        * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n          ``'datetime64'``\n        * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n          ``'timedelta64'``\n        * To select Pandas categorical dtypes, use ``'category'``\n        * To select Pandas datetimetz dtypes, use ``'datetimetz'``\n          or ``'datetime64[ns, tz]'``\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'a': [1, 2] * 3,\n        ...                    'b': [True, False] * 3,\n        ...                    'c': [1.0, 2.0] * 3})\n        >>> df\n                a      b  c\n        0       1   True  1.0\n        1       2  False  2.0\n        2       1   True  1.0\n        3       2  False  2.0\n        4       1   True  1.0\n        5       2  False  2.0\n\n        >>> df.select_dtypes(include='bool')\n           b\n        0  True\n        1  False\n        2  True\n        3  False\n        4  True\n        5  False\n\n        >>> df.select_dtypes(include=['float64'])\n           c\n        0  1.0\n        1  2.0\n        2  1.0\n        3  2.0\n        4  1.0\n        5  2.0\n\n        >>> df.select_dtypes(exclude=['int64'])\n               b    c\n        0   True  1.0\n        1  False  2.0\n        2   True  1.0\n        3  False  2.0\n        4   True  1.0\n        5  False  2.0\n        \"\"\"\n        if not is_list_like(include):\n            include = (include,) if include is not None else ()\n        if not is_list_like(exclude):\n            exclude = (exclude,) if exclude is not None else ()\n\n        selection = (frozenset(include), frozenset(exclude))\n\n        if not any(selection):\n            raise ValueError(\"at least one of include or exclude must be nonempty\")\n\n        # convert the myriad valid dtypes object to a single representation\n        def check_int_infer_dtype(dtypes):\n            converted_dtypes: list[type] = []\n            for dtype in dtypes:\n                # Numpy maps int to different types (int32, in64) on Windows and Linux\n                # see https://github.com/numpy/numpy/issues/9464\n                if (isinstance(dtype, str) and dtype == \"int\") or (dtype is int):\n                    converted_dtypes.append(np.int32)\n                    converted_dtypes.append(np.int64)\n                elif dtype == \"float\" or dtype is float:\n                    # GH#42452 : np.dtype(\"float\") coerces to np.float64 from Numpy 1.20\n                    converted_dtypes.extend([np.float64, np.float32])\n                else:\n                    converted_dtypes.append(infer_dtype_from_object(dtype))\n            return frozenset(converted_dtypes)\n\n        include = check_int_infer_dtype(include)\n        exclude = check_int_infer_dtype(exclude)\n\n        for dtypes in (include, exclude):\n            invalidate_string_dtypes(dtypes)\n\n        # can't both include AND exclude!\n        if not include.isdisjoint(exclude):\n            raise ValueError(f\"include and exclude overlap on {(include & exclude)}\")\n\n        def dtype_predicate(dtype: DtypeObj, dtypes_set) -> bool:\n            # GH 46870: BooleanDtype._is_numeric == True but should be excluded\n            dtype = dtype if not isinstance(dtype, ArrowDtype) else dtype.numpy_dtype\n            return issubclass(dtype.type, tuple(dtypes_set)) or (\n                np.number in dtypes_set\n                and getattr(dtype, \"_is_numeric\", False)\n                and not is_bool_dtype(dtype)\n            )\n\n        def predicate(arr: ArrayLike) -> bool:\n            dtype = arr.dtype\n            if include:\n                if not dtype_predicate(dtype, include):\n                    return False\n\n            if exclude:\n                if dtype_predicate(dtype, exclude):\n                    return False\n\n            return True\n\n        mgr = self._mgr._get_data_subset(predicate).copy(deep=None)\n        # error: Incompatible return value type (got \"DataFrame\", expected \"Self\")\n        return self._constructor_from_mgr(mgr, axes=mgr.axes).__finalize__(self)  # type: ignore[return-value]\n\n    def insert(\n        self,\n        loc: int,\n        column: Hashable,\n        value: Scalar | AnyArrayLike,\n        allow_duplicates: bool | lib.NoDefault = lib.no_default,\n    ) -> None:\n        \"\"\"\n        Insert column into DataFrame at specified location.\n\n        Raises a ValueError if `column` is already contained in the DataFrame,\n        unless `allow_duplicates` is set to True.\n\n        Parameters\n        ----------\n        loc : int\n            Insertion index. Must verify 0 <= loc <= len(columns).\n        column : str, number, or hashable object\n            Label of the inserted column.\n        value : Scalar, Series, or array-like\n            Content of the inserted column.\n        allow_duplicates : bool, optional, default lib.no_default\n            Allow duplicate column labels to be created.\n\n        See Also\n        --------\n        Index.insert : Insert new item by index.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df\n           col1  col2\n        0     1     3\n        1     2     4\n        >>> df.insert(1, \"newcol\", [99, 99])\n        >>> df\n           col1  newcol  col2\n        0     1      99     3\n        1     2      99     4\n        >>> df.insert(0, \"col1\", [100, 100], allow_duplicates=True)\n        >>> df\n           col1  col1  newcol  col2\n        0   100     1      99     3\n        1   100     2      99     4\n\n        Notice that pandas uses index alignment in case of `value` from type `Series`:\n\n        >>> df.insert(0, \"col0\", pd.Series([5, 6], index=[1, 2]))\n        >>> df\n           col0  col1  col1  newcol  col2\n        0   NaN   100     1      99     3\n        1   5.0   100     2      99     4\n        \"\"\"\n        if allow_duplicates is lib.no_default:\n            allow_duplicates = False\n        if allow_duplicates and not self.flags.allows_duplicate_labels:\n            raise ValueError(\n                \"Cannot specify 'allow_duplicates=True' when \"\n                \"'self.flags.allows_duplicate_labels' is False.\"\n            )\n        if not allow_duplicates and column in self.columns:\n            # Should this be a different kind of error??\n            raise ValueError(f\"cannot insert {column}, already exists\")\n        if not is_integer(loc):\n            raise TypeError(\"loc must be int\")\n        # convert non stdlib ints to satisfy typing checks\n        loc = int(loc)\n        if isinstance(value, DataFrame) and len(value.columns) > 1:\n            raise ValueError(\n                f\"Expected a one-dimensional object, got a DataFrame with \"\n                f\"{len(value.columns)} columns instead.\"\n            )\n        elif isinstance(value, DataFrame):\n            value = value.iloc[:, 0]\n\n        value, refs = self._sanitize_column(value)\n        self._mgr.insert(loc, column, value, refs=refs)\n\n    def assign(self, **kwargs) -> DataFrame:\n        r\"\"\"\n        Assign new columns to a DataFrame.\n\n        Returns a new object with all original columns in addition to new ones.\n        Existing columns that are re-assigned will be overwritten.\n\n        Parameters\n        ----------\n        **kwargs : dict of {str: callable or Series}\n            The column names are keywords. If the values are\n            callable, they are computed on the DataFrame and\n            assigned to the new columns. The callable must not\n            change input DataFrame (though pandas doesn't check it).\n            If the values are not callable, (e.g. a Series, scalar, or array),\n            they are simply assigned.\n\n        Returns\n        -------\n        DataFrame\n            A new DataFrame with the new columns in addition to\n            all the existing columns.\n\n        Notes\n        -----\n        Assigning multiple columns within the same ``assign`` is possible.\n        Later items in '\\*\\*kwargs' may refer to newly created or modified\n        columns in 'df'; items are computed and assigned into 'df' in order.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'temp_c': [17.0, 25.0]},\n        ...                   index=['Portland', 'Berkeley'])\n        >>> df\n                  temp_c\n        Portland    17.0\n        Berkeley    25.0\n\n        Where the value is a callable, evaluated on `df`:\n\n        >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        Alternatively, the same behavior can be achieved by directly\n        referencing an existing Series or sequence:\n\n        >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        You can create multiple columns within the same assign where one\n        of the columns depends on another one defined within the same assign:\n\n        >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,\n        ...           temp_k=lambda x: (x['temp_f'] + 459.67) * 5 / 9)\n                  temp_c  temp_f  temp_k\n        Portland    17.0    62.6  290.15\n        Berkeley    25.0    77.0  298.15\n        \"\"\"\n        data = self.copy(deep=None)\n\n        for k, v in kwargs.items():\n            data[k] = com.apply_if_callable(v, data)\n        return data\n\n    def _sanitize_column(self, value) -> tuple[ArrayLike, BlockValuesRefs | None]:\n        \"\"\"\n        Ensures new columns (which go into the BlockManager as new blocks) are\n        always copied (or a reference is being tracked to them under CoW)\n        and converted into an array.\n\n        Parameters\n        ----------\n        value : scalar, Series, or array-like\n\n        Returns\n        -------\n        tuple of numpy.ndarray or ExtensionArray and optional BlockValuesRefs\n        \"\"\"\n        self._ensure_valid_index(value)\n\n        # Using a DataFrame would mean coercing values to one dtype\n        assert not isinstance(value, DataFrame)\n        if is_dict_like(value):\n            if not isinstance(value, Series):\n                value = Series(value)\n            return _reindex_for_setitem(value, self.index)\n\n        if is_list_like(value):\n            com.require_length_match(value, self.index)\n        arr = sanitize_array(value, self.index, copy=True, allow_2d=True)\n        if (\n            isinstance(value, Index)\n            and value.dtype == \"object\"\n            and arr.dtype != value.dtype\n        ):  #\n            # TODO: Remove kludge in sanitize_array for string mode when enforcing\n            # this deprecation\n            warnings.warn(\n                \"Setting an Index with object dtype into a DataFrame will stop \"\n                \"inferring another dtype in a future version. Cast the Index \"\n                \"explicitly before setting it into the DataFrame.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        return arr, None\n\n    @property\n    def _series(self):\n        return {item: self._ixs(idx, axis=1) for idx, item in enumerate(self.columns)}\n\n    # ----------------------------------------------------------------------\n    # Reindexing and alignment\n\n    def _reindex_multi(\n        self, axes: dict[str, Index], copy: bool, fill_value\n    ) -> DataFrame:\n        \"\"\"\n        We are guaranteed non-Nones in the axes.\n        \"\"\"\n\n        new_index, row_indexer = self.index.reindex(axes[\"index\"])\n        new_columns, col_indexer = self.columns.reindex(axes[\"columns\"])\n\n        if row_indexer is not None and col_indexer is not None:\n            # Fastpath. By doing two 'take's at once we avoid making an\n            #  unnecessary copy.\n            # We only get here with `self._can_fast_transpose`, which (almost)\n            #  ensures that self.values is cheap. It may be worth making this\n            #  condition more specific.\n            indexer = row_indexer, col_indexer\n            new_values = take_2d_multi(self.values, indexer, fill_value=fill_value)\n            return self._constructor(\n                new_values, index=new_index, columns=new_columns, copy=False\n            )\n        else:\n            return self._reindex_with_indexers(\n                {0: [new_index, row_indexer], 1: [new_columns, col_indexer]},\n                copy=copy,\n                fill_value=fill_value,\n            )\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\n        Change the row labels.\n\n        >>> df.set_axis(['a', 'b', 'c'], axis='index')\n           A  B\n        a  1  4\n        b  2  5\n        c  3  6\n\n        Change the column labels.\n\n        >>> df.set_axis(['I', 'II'], axis='columns')\n           I  II\n        0  1   4\n        1  2   5\n        2  3   6\n        \"\"\"\n    )\n    @Substitution(\n        klass=_shared_doc_kwargs[\"klass\"],\n        axes_single_arg=_shared_doc_kwargs[\"axes_single_arg\"],\n        extended_summary_sub=\" column or\",\n        axis_description_sub=\", and 1 identifies the columns\",\n        see_also_sub=\" or columns\",\n    )\n    @Appender(NDFrame.set_axis.__doc__)\n    def set_axis(\n        self,\n        labels,\n        *,\n        axis: Axis = 0,\n        copy: bool | None = None,\n    ) -> DataFrame:\n        return super().set_axis(labels, axis=axis, copy=copy)\n\n    @doc(\n        NDFrame.reindex,\n        klass=_shared_doc_kwargs[\"klass\"],\n        optional_reindex=_shared_doc_kwargs[\"optional_reindex\"],\n    )\n    def reindex(\n        self,\n        labels=None,\n        *,\n        index=None,\n        columns=None,\n        axis: Axis | None = None,\n        method: ReindexMethod | None = None,\n        copy: bool | None = None,\n        level: Level | None = None,\n        fill_value: Scalar | None = np.nan,\n        limit: int | None = None,\n        tolerance=None,\n    ) -> DataFrame:\n        return super().reindex(\n            labels=labels,\n            index=index,\n            columns=columns,\n            axis=axis,\n            method=method,\n            copy=copy,\n            level=level,\n            fill_value=fill_value,\n            limit=limit,\n            tolerance=tolerance,\n        )\n\n    @overload\n    def drop(\n        self,\n        labels: IndexLabel = ...,\n        *,\n        axis: Axis = ...,\n        index: IndexLabel = ...,\n        columns: IndexLabel = ...,\n        level: Level = ...,\n        inplace: Literal[True],\n        errors: IgnoreRaise = ...,\n    ) -> None:\n        ...\n\n    @overload\n    def drop(\n        self,\n        labels: IndexLabel = ...,\n        *,\n        axis: Axis = ...,\n        index: IndexLabel = ...,\n        columns: IndexLabel = ...,\n        level: Level = ...,\n        inplace: Literal[False] = ...,\n        errors: IgnoreRaise = ...,\n    ) -> DataFrame:\n        ...\n\n    @overload\n    def drop(\n        self,\n        labels: IndexLabel = ...,\n        *,\n        axis: Axis = ...,\n        index: IndexLabel = ...,\n        columns: IndexLabel = ...,\n        level: Level = ...,\n        inplace: bool = ...,\n        errors: IgnoreRaise = ...,\n    ) -> DataFrame | None:\n        ...\n\n    def drop(\n        self,\n        labels: IndexLabel | None = None,\n        *,\n        axis: Axis = 0,\n        index: IndexLabel | None = None,\n        columns: IndexLabel | None = None,\n        level: Level | None = None,\n        inplace: bool = False,\n        errors: IgnoreRaise = \"raise\",\n    ) -> DataFrame | None:\n        \"\"\"\n        Drop specified labels from rows or columns.\n\n        Remove rows or columns by specifying label names and corresponding\n        axis, or by directly specifying index or column names. When using a\n        multi-index, labels on different levels can be removed by specifying\n        the level. See the :ref:`user guide <advanced.shown_levels>`\n        for more information about the now unused levels.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index or column labels to drop. A tuple will be used as a single\n            label and not treated as a list-like.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Whether to drop labels from the index (0 or 'index') or\n            columns (1 or 'columns').\n        index : single label or list-like\n            Alternative to specifying axis (``labels, axis=0``\n            is equivalent to ``index=labels``).\n        columns : single label or list-like\n            Alternative to specifying axis (``labels, axis=1``\n            is equivalent to ``columns=labels``).\n        level : int or level name, optional\n            For MultiIndex, level from which the labels will be removed.\n        inplace : bool, default False\n            If False, return a copy. Otherwise, do operation\n            in place and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are\n            dropped.\n\n        Returns\n        -------\n        DataFrame or None\n            Returns DataFrame or None DataFrame with the specified\n            index or column labels removed or None if inplace=True.\n\n        Raises\n        ------\n        KeyError\n            If any of the labels is not found in the selected axis.\n\n        See Also\n        --------\n        DataFrame.loc : Label-location based indexer for selection by label.\n        DataFrame.dropna : Return DataFrame with labels on given axis omitted\n            where (all or any) data are missing.\n        DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n            removed, optionally only considering certain columns.\n        Series.drop : Return Series with specified index labels removed.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n        ...                   columns=['A', 'B', 'C', 'D'])\n        >>> df\n           A  B   C   D\n        0  0  1   2   3\n        1  4  5   6   7\n        2  8  9  10  11\n\n        Drop columns\n\n        >>> df.drop(['B', 'C'], axis=1)\n           A   D\n        0  0   3\n        1  4   7\n        2  8  11\n\n        >>> df.drop(columns=['B', 'C'])\n           A   D\n        0  0   3\n        1  4   7\n        2  8  11\n\n        Drop a row by index\n\n        >>> df.drop([0, 1])\n           A  B   C   D\n        2  8  9  10  11\n\n        Drop columns and/or rows of MultiIndex DataFrame\n\n        >>> midx = pd.MultiIndex(levels=[['llama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n        ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n        ...                         [250, 150], [1.5, 0.8], [320, 250],\n        ...                         [1, 0.8], [0.3, 0.2]])\n        >>> df\n                        big     small\n        llama   speed   45.0    30.0\n                weight  200.0   100.0\n                length  1.5     1.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n                length  1.5     0.8\n        falcon  speed   320.0   250.0\n                weight  1.0     0.8\n                length  0.3     0.2\n\n        Drop a specific index combination from the MultiIndex\n        DataFrame, i.e., drop the combination ``'falcon'`` and\n        ``'weight'``, which deletes only the corresponding row\n\n        >>> df.drop(index=('falcon', 'weight'))\n                        big     small\n        llama   speed   45.0    30.0\n                weight  200.0   100.0\n                length  1.5     1.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n                length  1.5     0.8\n        falcon  speed   320.0   250.0\n                length  0.3     0.2\n\n        >>> df.drop(index='cow', columns='small')\n                        big\n        llama   speed   45.0\n                weight  200.0\n                length  1.5\n        falcon  speed   320.0\n                weight  1.0\n                length  0.3\n\n        >>> df.drop(index='length', level=1)\n                        big     small\n        llama   speed   45.0    30.0\n                weight  200.0   100.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n        falcon  speed   320.0   250.0\n                weight  1.0     0.8\n        \"\"\"\n        return super().drop(\n            labels=labels,\n            axis=axis,\n            index=index,\n            columns=columns,\n            level=level,\n            inplace=inplace,\n            errors=errors,\n        )\n\n    @overload\n    def rename(\n        self,\n        mapper: Renamer | None = ...,\n        *,\n        index: Renamer | None = ...,\n        columns: Renamer | None = ...,\n        axis: Axis | None = ...,\n        copy: bool | None = ...,\n        inplace: Literal[True],\n        level: Level = ...,\n        errors: IgnoreRaise = ...,\n    ) -> None:\n        ...\n\n    @overload\n    def rename(\n        self,\n        mapper: Renamer | None = ...,\n        *,\n        index: Renamer | None = ...,\n        columns: Renamer | None = ...,\n        axis: Axis | None = ...,\n        copy: bool | None = ...,\n        inplace: Literal[False] = ...,\n        level: Level = ...,\n        errors: IgnoreRaise = ...,\n    ) -> DataFrame:\n        ...\n\n    @overload\n    def rename(\n        self,\n        mapper: Renamer | None = ...,\n        *,\n        index: Renamer | None = ...,\n        columns: Renamer | None = ...,\n        axis: Axis | None = ...,\n        copy: bool | None = ...,\n        inplace: bool = ...,\n        level: Level = ...,\n        errors: IgnoreRaise = ...,\n    ) -> DataFrame | None:\n        ...\n\n    def rename(\n        self,\n        mapper: Renamer | None = None,\n        *,\n        index: Renamer | None = None,\n        columns: Renamer | None = None,\n        axis: Axis | None = None,\n        copy: bool | None = None,\n        inplace: bool = False,\n        level: Level | None = None,\n        errors: IgnoreRaise = \"ignore\",\n    ) -> DataFrame | None:\n        \"\"\"\n        Rename columns or index labels.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        mapper : dict-like or function\n            Dict-like or function transformations to apply to\n            that axis' values. Use either ``mapper`` and ``axis`` to\n            specify the axis to target with ``mapper``, or ``index`` and\n            ``columns``.\n        index : dict-like or function\n            Alternative to specifying axis (``mapper, axis=0``\n            is equivalent to ``index=mapper``).\n        columns : dict-like or function\n            Alternative to specifying axis (``mapper, axis=1``\n            is equivalent to ``columns=mapper``).\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis to target with ``mapper``. Can be either the axis name\n            ('index', 'columns') or number (0, 1). The default is 'index'.\n        copy : bool, default True\n            Also copy underlying data.\n\n            .. note::\n                The `copy` keyword will change behavior in pandas 3.0.\n                `Copy-on-Write\n                <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n                will be enabled by default, which means that all methods with a\n                `copy` keyword will use a lazy copy mechanism to defer the copy and\n                ignore the `copy` keyword. The `copy` keyword will be removed in a\n                future version of pandas.\n\n                You can already get the future behavior and improvements through\n                enabling copy on write ``pd.options.mode.copy_on_write = True``\n        inplace : bool, default False\n            Whether to modify the DataFrame rather than creating a new one.\n            If True then value of copy is ignored.\n        level : int or level name, default None\n            In case of a MultiIndex, only rename labels in the specified\n            level.\n        errors : {'ignore', 'raise'}, default 'ignore'\n            If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`,\n            or `columns` contains labels that are not present in the Index\n            being transformed.\n            If 'ignore', existing keys will be renamed and extra keys will be\n            ignored.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with the renamed axis labels or None if ``inplace=True``.\n\n        Raises\n        ------\n        KeyError\n            If any of the labels is not found in the selected axis and\n            \"errors='raise'\".\n\n        See Also\n        --------\n        DataFrame.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        ``DataFrame.rename`` supports two calling conventions\n\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\n        * ``(mapper, axis={'index', 'columns'}, ...)``\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        Rename columns using a mapping:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n        >>> df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n           a  c\n        0  1  4\n        1  2  5\n        2  3  6\n\n        Rename index using a mapping:\n\n        >>> df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"})\n           A  B\n        x  1  4\n        y  2  5\n        z  3  6\n\n        Cast index labels to a different type:\n\n        >>> df.index\n        RangeIndex(start=0, stop=3, step=1)\n        >>> df.rename(index=str).index\n        Index(['0', '1', '2'], dtype='object')\n\n        >>> df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\")\n        Traceback (most recent call last):\n        KeyError: ['C'] not found in axis\n\n        Using axis-style parameters:\n\n        >>> df.rename(str.lower, axis='columns')\n           a  b\n        0  1  4\n        1  2  5\n        2  3  6\n\n        >>> df.rename({1: 2, 2: 4}, axis='index')\n           A  B\n        0  1  4\n        2  2  5\n        4  3  6\n        \"\"\"\n        return super()._rename(\n            mapper=mapper,\n            index=index,\n            columns=columns,\n            axis=axis,\n            copy=copy,\n            inplace=inplace,\n            level=level,\n            errors=errors,\n        )\n\n    def pop(self, item: Hashable) -> Series:\n        \"\"\"\n        Return item and drop from frame. Raise KeyError if not found.\n\n        Parameters\n        ----------\n        item : label\n            Label of column to be popped.\n\n        Returns\n        -------\n        Series\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n        ...                    ('parrot', 'bird', 24.0),\n        ...                    ('lion', 'mammal', 80.5),\n        ...                    ('monkey', 'mammal', np.nan)],\n        ...                   columns=('name', 'class', 'max_speed'))\n        >>> df\n             name   class  max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        >>> df.pop('class')\n        0      bird\n        1      bird\n        2    mammal\n        3    mammal\n        Name: class, dtype: object\n\n        >>> df\n             name  max_speed\n        0  falcon      389.0\n        1  parrot       24.0\n        2    lion       80.5\n        3  monkey        NaN\n        \"\"\"\n        return super().pop(item=item)\n\n    def _replace_columnwise(\n        self, mapping: dict[Hashable, tuple[Any, Any]], inplace: bool, regex\n    ):\n        \"\"\"\n        Dispatch to Series.replace column-wise.\n\n        Parameters\n        ----------\n        mapping : dict\n            of the form {col: (target, value)}\n        inplace : bool\n        regex : bool or same types as `to_replace` in DataFrame.replace\n\n        Returns\n        -------\n        DataFrame or None\n        \"\"\"\n        # Operate column-wise\n        res = self if inplace else self.copy(deep=None)\n        ax = self.columns\n\n        for i, ax_value in enumerate(ax):\n            if ax_value in mapping:\n                ser = self.iloc[:, i]\n\n                target, value = mapping[ax_value]\n                newobj = ser.replace(target, value, regex=regex)\n\n                res._iset_item(i, newobj, inplace=inplace)\n\n        if inplace:\n            return\n        return res.__finalize__(self)\n\n    @doc(NDFrame.shift, klass=_shared_doc_kwargs[\"klass\"])\n    def shift(\n        self,\n        periods: int | Sequence[int] = 1,\n        freq: Frequency | None = None,\n        axis: Axis = 0,\n        fill_value: Hashable = lib.no_default,\n        suffix: str | None = None,\n    ) -> DataFrame:\n        if freq is not None and fill_value is not lib.no_default:\n            # GH#53832\n            warnings.warn(\n                \"Passing a 'freq' together with a 'fill_value' silently ignores \"\n                \"the fill_value and is deprecated. This will raise in a future \"\n                \"version.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            fill_value = lib.no_default\n\n        if self.empty:\n            return self.copy()\n\n        axis = self._get_axis_number(axis)\n\n        if is_list_like(periods):\n            periods = cast(Sequence, periods)\n            if axis == 1:\n                raise ValueError(\n                    \"If `periods` contains multiple shifts, `axis` cannot be 1.\"\n                )\n            if len(periods) == 0:\n                raise ValueError(\"If `periods` is an iterable, it cannot be empty.\")\n            from pandas.core.reshape.concat import concat\n\n            shifted_dataframes = []\n            for period in periods:\n                if not is_integer(period):\n                    raise TypeError(\n                        f\"Periods must be integer, but {period} is {type(period)}.\"\n                    )\n                period = cast(int, period)\n                shifted_dataframes.append(\n                    super()\n                    .shift(periods=period, freq=freq, axis=axis, fill_value=fill_value)\n                    .add_suffix(f\"{suffix}_{period}\" if suffix else f\"_{period}\")\n                )\n            return concat(shifted_dataframes, axis=1)\n        elif suffix:\n            raise ValueError(\"Cannot specify `suffix` if `periods` is an int.\")\n        periods = cast(int, periods)\n\n        ncols = len(self.columns)\n        arrays = self._mgr.arrays\n        if axis == 1 and periods != 0 and ncols > 0 and freq is None:\n            if fill_value is lib.no_default:\n                # We will infer fill_value to match the closest column\n\n                # Use a column that we know is valid for our column's dtype GH#38434\n                label = self.columns[0]\n\n                if periods > 0:\n                    result = self.iloc[:, :-periods]\n                    for col in range(min(ncols, abs(periods))):\n                        # TODO(EA2D): doing this in a loop unnecessary with 2D EAs\n                        # Define filler inside loop so we get a copy\n                        filler = self.iloc[:, 0].shift(len(self))\n                        result.insert(0, label, filler, allow_duplicates=True)\n                else:\n                    result = self.iloc[:, -periods:]\n                    for col in range(min(ncols, abs(periods))):\n                        # Define filler inside loop so we get a copy\n                        filler = self.iloc[:, -1].shift(len(self))\n                        result.insert(\n                            len(result.columns), label, filler, allow_duplicates=True\n                        )\n\n                result.columns = self.columns.copy()\n                return result\n            elif len(arrays) > 1 or (\n                # If we only have one block and we know that we can't\n                #  keep the same dtype (i.e. the _can_hold_element check)\n                #  then we can go through the reindex_indexer path\n                #  (and avoid casting logic in the Block method).\n                not can_hold_element(arrays[0], fill_value)\n            ):\n                # GH#35488 we need to watch out for multi-block cases\n                # We only get here with fill_value not-lib.no_default\n                nper = abs(periods)\n                nper = min(nper, ncols)\n                if periods > 0:\n                    indexer = np.array(\n                        [-1] * nper + list(range(ncols - periods)), dtype=np.intp\n                    )\n                else:\n                    indexer = np.array(\n                        list(range(nper, ncols)) + [-1] * nper, dtype=np.intp\n                    )\n                mgr = self._mgr.reindex_indexer(\n                    self.columns,\n                    indexer,\n                    axis=0,\n                    fill_value=fill_value,\n                    allow_dups=True,\n                )\n                res_df = self._constructor_from_mgr(mgr, axes=mgr.axes)\n                return res_df.__finalize__(self, method=\"shift\")\n            else:\n                return self.T.shift(periods=periods, fill_value=fill_value).T\n\n        return super().shift(\n            periods=periods, freq=freq, axis=axis, fill_value=fill_value\n        )\n\n    @overload\n    def set_index(\n        self,\n        keys,\n        *,\n        drop: bool = ...,\n        append: bool = ...,\n        inplace: Literal[False] = ...,\n        verify_integrity: bool = ...,\n    ) -> DataFrame:\n        ...\n\n    @overload\n    def set_index(\n        self,\n        keys,\n        *,\n        drop: bool = ...,\n        append: bool = ...,\n        inplace: Literal[True],\n        verify_integrity: bool = ...,\n    ) -> None:\n        ...\n\n    def set_index(\n        self,\n        keys,\n        *,\n        drop: bool = True,\n        append: bool = False,\n        inplace: bool = False,\n        verify_integrity: bool = False,\n    ) -> DataFrame | None:\n        \"\"\"\n        Set the DataFrame index using existing columns.\n\n        Set the DataFrame index (row labels) using one or more existing\n        columns or arrays (of the correct length). The index can replace the\n        existing index or expand on it.\n\n        Parameters\n        ----------\n        keys : label or array-like or list of labels/arrays\n            This parameter can be either a single column key, a single array of\n            the same length as the calling DataFrame, or a list containing an\n            arbitrary combination of column keys and arrays. Here, \"array\"\n            encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and\n            instances of :class:`~collections.abc.Iterator`.\n        drop : bool, default True\n            Delete columns to be used as the new index.\n        append : bool, default False\n            Whether to append columns to existing index.\n        inplace : bool, default False\n            Whether to modify the DataFrame rather than creating a new one.\n        verify_integrity : bool, default False\n            Check the new index for duplicates. Otherwise defer the check until\n            necessary. Setting to False will improve the performance of this\n            method.\n\n        Returns\n        -------\n        DataFrame or None\n            Changed row labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.reset_index : Opposite of set_index.\n        DataFrame.reindex : Change to new indices or expand indices.\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'month': [1, 4, 7, 10],\n        ...                    'year': [2012, 2014, 2013, 2014],\n        ...                    'sale': [55, 40, 84, 31]})\n        >>> df\n           month  year  sale\n        0      1  2012    55\n        1      4  2014    40\n        2      7  2013    84\n        3     10  2014    31\n\n        Set the index to become the 'month' column:\n\n        >>> df.set_index('month')\n               year  sale\n        month\n        1      2012    55\n        4      2014    40\n        7      2013    84\n        10     2014    31\n\n        Create a MultiIndex using columns 'year' and 'month':\n\n        >>> df.set_index(['year', 'month'])\n                    sale\n        year  month\n        2012  1     55\n        2014  4     40\n        2013  7     84\n        2014  10    31\n\n        Create a MultiIndex using an Index and a column:\n\n        >>> df.set_index([pd.Index([1, 2, 3, 4]), 'year'])\n                 month  sale\n           year\n        1  2012  1      55\n        2  2014  4      40\n        3  2013  7      84\n        4  2014  10     31\n\n        Create a MultiIndex using two Series:\n\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> df.set_index([s, s**2])\n              month  year  sale\n        1 1       1  2012    55\n        2 4       4  2014    40\n        3 9       7  2013    84\n        4 16     10  2014    31\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        if not isinstance(keys, list):\n            keys = [keys]\n\n        err_msg = (\n            'The parameter \"keys\" may be a column key, one-dimensional '\n            \"array, or a list containing only valid column keys and \"\n            \"one-dimensional arrays.\"\n        )\n\n        missing: list[Hashable] = []\n        for col in keys:\n            if isinstance(col, (Index, Series, np.ndarray, list, abc.Iterator)):\n                # arrays are fine as long as they are one-dimensional\n                # iterators get converted to list below\n                if getattr(col, \"ndim\", 1) != 1:\n                    raise ValueError(err_msg)\n            else:\n                # everything else gets tried as a key; see GH 24969\n                try:\n                    found = col in self.columns\n                except TypeError as err:\n                    raise TypeError(\n                        f\"{err_msg}. Received column of type {type(col)}\"\n                    ) from err\n                else:\n                    if not found:\n                        missing.append(col)\n\n        if missing:\n            raise KeyError(f\"None of {missing} are in the columns\")\n\n        if inplace:\n            frame = self\n        else:\n            # GH 49473 Use \"lazy copy\" with Copy-on-Write\n            frame = self.copy(deep=None)\n\n        arrays: list[Index] = []\n        names: list[Hashable] = []\n        if append:\n            names = list(self.index.names)\n            if isinstance(self.index, MultiIndex):\n                arrays.extend(\n                    self.index._get_level_values(i) for i in range(self.index.nlevels)\n                )\n            else:\n                arrays.append(self.index)\n\n        to_remove: list[Hashable] = []\n        for col in keys:\n            if isinstance(col, MultiIndex):\n                arrays.extend(col._get_level_values(n) for n in range(col.nlevels))\n                names.extend(col.names)\n            elif isinstance(col, (Index, Series)):\n                # if Index then not MultiIndex (treated above)\n\n                # error: Argument 1 to \"append\" of \"list\" has incompatible type\n                #  \"Union[Index, Series]\"; expected \"Index\"\n                arrays.append(col)  # type: ignore[arg-type]\n                names.append(col.name)\n            elif isinstance(col, (list, np.ndarray)):\n                # error: Argument 1 to \"append\" of \"list\" has incompatible type\n                # \"Union[List[Any], ndarray]\"; expected \"Index\"\n                arrays.append(col)  # type: ignore[arg-type]\n                names.append(None)\n            elif isinstance(col, abc.Iterator):\n                # error: Argument 1 to \"append\" of \"list\" has incompatible type\n                # \"List[Any]\"; expected \"Index\"\n                arrays.append(list(col))  # type: ignore[arg-type]\n                names.append(None)\n            # from here, col can only be a column label\n            else:\n                arrays.append(frame[col])\n                names.append(col)\n                if drop:\n                    to_remove.append(col)\n\n            if len(arrays[-1]) != len(self):\n                # check newest element against length of calling frame, since\n                # ensure_index_from_sequences would not raise for append=False.\n                raise ValueError(\n                    f\"Length mismatch: Expected {len(self)} rows, \"\n                    f\"received array of length {len(arrays[-1])}\"\n                )\n\n        index = ensure_index_from_sequences(arrays, names)\n\n        if verify_integrity and not index.is_unique:\n            duplicates = index[index.duplicated()].unique()\n            raise ValueError(f\"Index has duplicate keys: {duplicates}\")\n\n        # use set to handle duplicate column names gracefully in case of drop\n        for c in set(to_remove):\n            del frame[c]\n\n        # clear up memory usage\n        index._cleanup()\n\n        frame.index = index\n\n        if not inplace:\n            return frame\n        return None\n\n    @overload\n    def reset_index(\n        self,\n        level: IndexLabel = ...,\n        *,\n        drop: bool = ...,\n        inplace: Literal[False] = ...,\n        col_level: Hashable = ...,\n        col_fill: Hashable = ...,\n        allow_duplicates: bool | lib.NoDefault = ...,\n        names: Hashable | Sequence[Hashable] | None = None,\n    ) -> DataFrame:\n        ...\n\n    @overload\n    def reset_index(\n        self,\n        level: IndexLabel = ...,\n        *,\n        drop: bool = ...,\n        inplace: Literal[True],\n        col_level: Hashable = ...,\n        col_fill: Hashable = ...,\n        allow_duplicates: bool | lib.NoDefault = ...,\n        names: Hashable | Sequence[Hashable] | None = None,\n    ) -> None:\n        ...\n\n    @overload\n    def reset_index(\n        self,\n        level: IndexLabel = ...,\n        *,\n        drop: bool = ...,\n        inplace: bool = ...,\n        col_level: Hashable = ...,\n        col_fill: Hashable = ...,\n        allow_duplicates: bool | lib.NoDefault = ...,\n        names: Hashable | Sequence[Hashable] | None = None,\n    ) -> DataFrame | None:\n        ...\n\n    def reset_index(\n        self,\n        level: IndexLabel | None = None,\n        *,\n        drop: bool = False,\n        inplace: bool = False,\n        col_level: Hashable = 0,\n        col_fill: Hashable = \"\",\n        allow_duplicates: bool | lib.NoDefault = lib.no_default,\n        names: Hashable | Sequence[Hashable] | None = None,\n    ) -> DataFrame | None:\n        \"\"\"\n        Reset the index, or a level of it.\n\n        Reset the index of the DataFrame, and use the default one instead.\n        If the DataFrame has a MultiIndex, this method can remove one or more\n        levels.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes all levels by\n            default.\n        drop : bool, default False\n            Do not try to insert index into dataframe columns. This resets\n            the index to the default integer index.\n        inplace : bool, default False\n            Whether to modify the DataFrame rather than creating a new one.\n        col_level : int or str, default 0\n            If the columns have multiple levels, determines which level the\n            labels are inserted into. By default it is inserted into the first\n            level.\n        col_fill : object, default ''\n            If the columns have multiple levels, determines how the other\n            levels are named. If None then the index name is repeated.\n        allow_duplicates : bool, optional, default lib.no_default\n            Allow duplicate column labels to be created.\n\n            .. versionadded:: 1.5.0\n\n        names : int, str or 1-dimensional list, default None\n            Using the given string, rename the DataFrame column which contains the\n            index data. If the DataFrame has a MultiIndex, this has to be a list or\n            tuple with length equal to the number of levels.\n\n            .. versionadded:: 1.5.0\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with the new index or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.set_index : Opposite of reset_index.\n        DataFrame.reindex : Change to new indices or expand indices.\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('bird', 389.0),\n        ...                    ('bird', 24.0),\n        ...                    ('mammal', 80.5),\n        ...                    ('mammal', np.nan)],\n        ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n        ...                   columns=('class', 'max_speed'))\n        >>> df\n                 class  max_speed\n        falcon    bird      389.0\n        parrot    bird       24.0\n        lion    mammal       80.5\n        monkey  mammal        NaN\n\n        When we reset the index, the old index is added as a column, and a\n        new sequential index is used:\n\n        >>> df.reset_index()\n            index   class  max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        We can use the `drop` parameter to avoid the old index being added as\n        a column:\n\n        >>> df.reset_index(drop=True)\n            class  max_speed\n        0    bird      389.0\n        1    bird       24.0\n        2  mammal       80.5\n        3  mammal        NaN\n\n        You can also use `reset_index` with `MultiIndex`.\n\n        >>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),\n        ...                                    ('bird', 'parrot'),\n        ...                                    ('mammal', 'lion'),\n        ...                                    ('mammal', 'monkey')],\n        ...                                   names=['class', 'name'])\n        >>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),\n        ...                                      ('species', 'type')])\n        >>> df = pd.DataFrame([(389.0, 'fly'),\n        ...                    (24.0, 'fly'),\n        ...                    (80.5, 'run'),\n        ...                    (np.nan, 'jump')],\n        ...                   index=index,\n        ...                   columns=columns)\n        >>> df\n                       speed species\n                         max    type\n        class  name\n        bird   falcon  389.0     fly\n               parrot   24.0     fly\n        mammal lion     80.5     run\n               monkey    NaN    jump\n\n        Using the `names` parameter, choose a name for the index column:\n\n        >>> df.reset_index(names=['classes', 'names'])\n          classes   names  speed species\n                             max    type\n        0    bird  falcon  389.0     fly\n        1    bird  parrot   24.0     fly\n        2  mammal    lion   80.5     run\n        3  mammal  monkey    NaN    jump\n\n        If the index has multiple levels, we can reset a subset of them:\n\n        >>> df.reset_index(level='class')\n                 class  speed species\n                          max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        If we are not dropping the index, by default, it is placed in the top\n        level. We can place it in another level:\n\n        >>> df.reset_index(level='class', col_level=1)\n                        speed species\n                 class    max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        When the index is inserted under another level, we can specify under\n        which one with the parameter `col_fill`:\n\n        >>> df.reset_index(level='class', col_level=1, col_fill='species')\n                      species  speed species\n                        class    max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n\n        If we specify a nonexistent level for `col_fill`, it is created:\n\n        >>> df.reset_index(level='class', col_level=1, col_fill='genus')\n                        genus  speed species\n                        class    max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        if inplace:\n            new_obj = self\n        else:\n            new_obj = self.copy(deep=None)\n        if allow_duplicates is not lib.no_default:\n            allow_duplicates = validate_bool_kwarg(allow_duplicates, \"allow_duplicates\")\n\n        new_index = default_index(len(new_obj))\n        if level is not None:\n            if not isinstance(level, (tuple, list)):\n                level = [level]\n            level = [self.index._get_level_number(lev) for lev in level]\n            if len(level) < self.index.nlevels:\n                new_index = self.index.droplevel(level)\n\n        if not drop:\n            to_insert: Iterable[tuple[Any, Any | None]]\n\n            default = \"index\" if \"index\" not in self else \"level_0\"\n            names = self.index._get_default_index_names(names, default)\n\n            if isinstance(self.index, MultiIndex):\n                to_insert = zip(self.index.levels, self.index.codes)\n            else:\n                to_insert = ((self.index, None),)\n\n            multi_col = isinstance(self.columns, MultiIndex)\n            for i, (lev, lab) in reversed(list(enumerate(to_insert))):\n                if level is not None and i not in level:\n                    continue\n                name = names[i]\n                if multi_col:\n                    col_name = list(name) if isinstance(name, tuple) else [name]\n                    if col_fill is None:\n                        if len(col_name) not in (1, self.columns.nlevels):\n                            raise ValueError(\n                                \"col_fill=None is incompatible \"\n                                f\"with incomplete column name {name}\"\n                            )\n                        col_fill = col_name[0]\n\n                    lev_num = self.columns._get_level_number(col_level)\n                    name_lst = [col_fill] * lev_num + col_name\n                    missing = self.columns.nlevels - len(name_lst)\n                    name_lst += [col_fill] * missing\n                    name = tuple(name_lst)\n\n                # to ndarray and maybe infer different dtype\n                level_values = lev._values\n                if level_values.dtype == np.object_:\n                    level_values = lib.maybe_convert_objects(level_values)\n\n                if lab is not None:\n                    # if we have the codes, extract the values with a mask\n                    level_values = algorithms.take(\n                        level_values, lab, allow_fill=True, fill_value=lev._na_value\n                    )\n\n                new_obj.insert(\n                    0,\n                    name,\n                    level_values,\n                    allow_duplicates=allow_duplicates,\n                )\n\n        new_obj.index = new_index\n        if not inplace:\n            return new_obj\n\n        return None\n\n    # ----------------------------------------------------------------------\n    # Reindex-based selection methods\n\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])\n    def isna(self) -> DataFrame:\n        res_mgr = self._mgr.isna(func=isna)\n        result = self._constructor_from_mgr(res_mgr, axes=res_mgr.axes)\n        return result.__finalize__(self, method=\"isna\")\n\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])\n    def isnull(self) -> DataFrame:\n        \"\"\"\n        DataFrame.isnull is an alias for DataFrame.isna.\n        \"\"\"\n        return self.isna()\n\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])\n    def notna(self) -> DataFrame:\n        return ~self.isna()\n\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])\n    def notnull(self) -> DataFrame:\n        \"\"\"\n        DataFrame.notnull is an alias for DataFrame.notna.\n        \"\"\"\n        return ~self.isna()\n\n    @overload\n    def dropna(\n        self,\n        *,\n        axis: Axis = ...,\n        how: AnyAll | lib.NoDefault = ...,\n        thresh: int | lib.NoDefault = ...,\n        subset: IndexLabel = ...,\n        inplace: Literal[False] = ...,\n        ignore_index: bool = ...,\n    ) -> DataFrame:\n        ...\n\n    @overload\n    def dropna(\n        self,\n        *,\n        axis: Axis = ...,\n        how: AnyAll | lib.NoDefault = ...,\n        thresh: int | lib.NoDefault = ...,\n        subset: IndexLabel = ...,\n        inplace: Literal[True],\n        ignore_index: bool = ...,\n    ) -> None:\n        ...\n\n    def dropna(\n        self,\n        *,\n        axis: Axis = 0,\n        how: AnyAll | lib.NoDefault = lib.no_default,\n        thresh: int | lib.NoDefault = lib.no_default,\n        subset: IndexLabel | None = None,\n        inplace: bool = False,\n        ignore_index: bool = False,\n    ) -> DataFrame | None:\n        \"\"\"\n        Remove missing values.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Determine if rows or columns which contain missing values are\n            removed.\n\n            * 0, or 'index' : Drop rows which contain missing values.\n            * 1, or 'columns' : Drop columns which contain missing value.\n\n            Only a single axis is allowed.\n\n        how : {'any', 'all'}, default 'any'\n            Determine if row or column is removed from DataFrame, when we have\n            at least one NA or all NA.\n\n            * 'any' : If any NA values are present, drop that row or column.\n            * 'all' : If all values are NA, drop that row or column.\n\n        thresh : int, optional\n            Require that many non-NA values. Cannot be combined with how.\n        subset : column label or sequence of labels, optional\n            Labels along other axis to consider, e.g. if you are dropping rows\n            these would be a list of columns to include.\n        inplace : bool, default False\n            Whether to modify the DataFrame rather than creating a new one.\n        ignore_index : bool, default ``False``\n            If ``True``, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 2.0.0\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with NA entries dropped from it or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.isna: Indicate missing values.\n        DataFrame.notna : Indicate existing (non-missing) values.\n        DataFrame.fillna : Replace missing values.\n        Series.dropna : Drop missing values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n        ...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n        ...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n        ...                             pd.NaT]})\n        >>> df\n               name        toy       born\n        0    Alfred        NaN        NaT\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Drop the rows where at least one element is missing.\n\n        >>> df.dropna()\n             name        toy       born\n        1  Batman  Batmobile 1940-04-25\n\n        Drop the columns where at least one element is missing.\n\n        >>> df.dropna(axis='columns')\n               name\n        0    Alfred\n        1    Batman\n        2  Catwoman\n\n        Drop the rows where all elements are missing.\n\n        >>> df.dropna(how='all')\n               name        toy       born\n        0    Alfred        NaN        NaT\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Keep only the rows with at least 2 non-NA values.\n\n        >>> df.dropna(thresh=2)\n               name        toy       born\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Define in which columns to look for missing values.\n\n        >>> df.dropna(subset=['name', 'toy'])\n               name        toy       born\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n        \"\"\"\n        if (how is not lib.no_default) and (thresh is not lib.no_default):\n            raise TypeError(\n                \"You cannot set both the how and thresh arguments at the same time.\"\n            )\n\n        if how is lib.no_default:\n            how = \"any\"\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if isinstance(axis, (tuple, list)):\n            # GH20987\n            raise TypeError(\"supplying multiple axes to axis is no longer supported.\")\n\n        axis = self._get_axis_number(axis)\n        agg_axis = 1 - axis\n\n        agg_obj = self\n        if subset is not None:\n            # subset needs to be list\n            if not is_list_like(subset):\n                subset = [subset]\n            ax = self._get_axis(agg_axis)\n            indices = ax.get_indexer_for(subset)\n            check = indices == -1\n            if check.any():\n                raise KeyError(np.array(subset)[check].tolist())\n            agg_obj = self.take(indices, axis=agg_axis)\n\n        if thresh is not lib.no_default:\n            count = agg_obj.count(axis=agg_axis)\n            mask = count >= thresh\n        elif how == \"any\":\n            # faster equivalent to 'agg_obj.count(agg_axis) == self.shape[agg_axis]'\n            mask = notna(agg_obj).all(axis=agg_axis, bool_only=False)\n        elif how == \"all\":\n            # faster equivalent to 'agg_obj.count(agg_axis) > 0'\n            mask = notna(agg_obj).any(axis=agg_axis, bool_only=False)\n        else:\n            raise ValueError(f\"invalid how option: {how}\")\n\n        if np.all(mask):\n            result = self.copy(deep=None)\n        else:\n            result = self.loc(axis=axis)[mask]\n\n        if ignore_index:\n            result.index = default_index(len(result))\n\n        if not inplace:\n            return result\n        self._update_inplace(result)\n        return None\n\n    @overload\n    def drop_duplicates(\n        self,\n        subset: Hashable | Sequence[Hashable] | None = ...,\n        *,\n        keep: DropKeep = ...,\n        inplace: Literal[True],\n        ignore_index: bool = ...,\n    ) -> None:\n        ...\n\n    @overload\n    def drop_duplicates(\n        self,\n        subset: Hashable | Sequence[Hashable] | None = ...,\n        *,\n        keep: DropKeep = ...,\n        inplace: Literal[False] = ...,\n        ignore_index: bool = ...,\n    ) -> DataFrame:\n        ...\n\n    @overload\n    def drop_duplicates(\n        self,\n        subset: Hashable | Sequence[Hashable] | None = ...,\n        *,\n        keep: DropKeep = ...,\n        inplace: bool = ...,\n        ignore_index: bool = ...,\n    ) -> DataFrame | None:\n        ...\n\n    def drop_duplicates(\n        self,\n        subset: Hashable | Sequence[Hashable] | None = None,\n        *,\n        keep: DropKeep = \"first\",\n        inplace: bool = False,\n        ignore_index: bool = False,\n    ) -> DataFrame | None:\n        \"\"\"\n        Return DataFrame with duplicate rows removed.\n\n        Considering certain columns is optional. Indexes, including time indexes\n        are ignored.\n\n        Parameters\n        ----------\n        subset : column label or sequence of labels, optional\n            Only consider certain columns for identifying duplicates, by\n            default use all of the columns.\n        keep : {'first', 'last', ``False``}, default 'first'\n            Determines which duplicates (if any) to keep.\n\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        inplace : bool, default ``False``\n            Whether to modify the DataFrame rather than creating a new one.\n        ignore_index : bool, default ``False``\n            If ``True``, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with duplicates removed or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.value_counts: Count unique combinations of columns.\n\n        Examples\n        --------\n        Consider dataset containing ramen rating.\n\n        >>> df = pd.DataFrame({\n        ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n        ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n        ...     'rating': [4, 4, 3.5, 15, 5]\n        ... })\n        >>> df\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        By default, it removes duplicate rows based on all columns.\n\n        >>> df.drop_duplicates()\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        To remove duplicates on specific column(s), use ``subset``.\n\n        >>> df.drop_duplicates(subset=['brand'])\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n\n        To remove duplicates and keep last occurrences, use ``keep``.\n\n        >>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n            brand style  rating\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        4  Indomie  pack     5.0\n        \"\"\"\n        if self.empty:\n            return self.copy(deep=None)\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        ignore_index = validate_bool_kwarg(ignore_index, \"ignore_index\")\n\n        result = self[-self.duplicated(subset, keep=keep)]\n        if ignore_index:\n            result.index = default_index(len(result))\n\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def duplicated(\n        self,\n        subset: Hashable | Sequence[Hashable] | None = None,\n        keep: DropKeep = \"first\",\n    ) -> Series:\n        \"\"\"\n        Return boolean Series denoting duplicate rows.\n\n        Considering certain columns is optional.\n\n        Parameters\n        ----------\n        subset : column label or sequence of labels, optional\n            Only consider certain columns for identifying duplicates, by\n            default use all of the columns.\n        keep : {'first', 'last', False}, default 'first'\n            Determines which duplicates (if any) to mark.\n\n            - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n            - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n            - False : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series\n            Boolean series for each duplicated rows.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on index.\n        Series.duplicated : Equivalent method on Series.\n        Series.drop_duplicates : Remove duplicate values from Series.\n        DataFrame.drop_duplicates : Remove duplicate values from DataFrame.\n\n        Examples\n        --------\n        Consider dataset containing ramen rating.\n\n        >>> df = pd.DataFrame({\n        ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n        ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n        ...     'rating': [4, 4, 3.5, 15, 5]\n        ... })\n        >>> df\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        By default, for each set of duplicated values, the first occurrence\n        is set on False and all others on True.\n\n        >>> df.duplicated()\n        0    False\n        1     True\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True.\n\n        >>> df.duplicated(keep='last')\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        By setting ``keep`` on False, all duplicates are True.\n\n        >>> df.duplicated(keep=False)\n        0     True\n        1     True\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        To find duplicates on specific column(s), use ``subset``.\n\n        >>> df.duplicated(subset=['brand'])\n        0    False\n        1     True\n        2    False\n        3     True\n        4     True\n        dtype: bool\n        \"\"\"\n\n        if self.empty:\n            return self._constructor_sliced(dtype=bool)\n\n        def f(vals) -> tuple[np.ndarray, int]:\n            labels, shape = algorithms.factorize(vals, size_hint=len(self))\n            return labels.astype(\"i8\", copy=False), len(shape)\n\n        if subset is None:\n            # https://github.com/pandas-dev/pandas/issues/28770\n            # Incompatible types in assignment (expression has type \"Index\", variable\n            # has type \"Sequence[Any]\")\n            subset = self.columns  # type: ignore[assignment]\n        elif (\n            not np.iterable(subset)\n            or isinstance(subset, str)\n            or isinstance(subset, tuple)\n            and subset in self.columns\n        ):\n            subset = (subset,)\n\n        #  needed for mypy since can't narrow types using np.iterable\n        subset = cast(Sequence, subset)\n\n        # Verify all columns in subset exist in the queried dataframe\n        # Otherwise, raise a KeyError, same as if you try to __getitem__ with a\n        # key that doesn't exist.\n        diff = set(subset) - set(self.columns)\n        if diff:\n            raise KeyError(Index(diff))\n\n        if len(subset) == 1 and self.columns.is_unique:\n            # GH#45236 This is faster than get_group_index below\n            result = self[subset[0]].duplicated(keep)\n            result.name = None\n        else:\n            vals = (col.values for name, col in self.items() if name in subset)\n            labels, shape = map(list, zip(*map(f, vals)))\n\n            ids = get_group_index(labels, tuple(shape), sort=False, xnull=False)\n            result = self._constructor_sliced(duplicated(ids, keep), index=self.index)\n        return result.__finalize__(self, method=\"duplicated\")\n\n    # ----------------------------------------------------------------------\n    # Sorting\n    # error: Signature of \"sort_values\" incompatible with supertype \"NDFrame\"\n    @overload  # type: ignore[override]\n    def sort_values(\n        self,\n        by: IndexLabel,\n        *,\n        axis: Axis = ...,\n        ascending=...,\n        inplace: Literal[False] = ...,\n        kind: SortKind = ...,\n        na_position: NaPosition = ...,\n        ignore_index: bool = ...,\n        key: ValueKeyFunc = ...,\n    ) -> DataFrame:\n        ...\n\n    @overload\n    def sort_values(\n        self,\n        by: IndexLabel,\n        *,\n        axis: Axis = ...,\n        ascending=...,\n        inplace: Literal[True],\n        kind: SortKind = ...,\n        na_position: str = ...,\n        ignore_index: bool = ...,\n        key: ValueKeyFunc = ...,\n    ) -> None:\n        ...\n\n    def sort_values(\n        self,\n        by: IndexLabel,\n        *,\n        axis: Axis = 0,\n        ascending: bool | list[bool] | tuple[bool, ...] = True,\n        inplace: bool = False,\n        kind: SortKind = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool = False,\n        key: ValueKeyFunc | None = None,\n    ) -> DataFrame | None:\n        \"\"\"\n        Sort by the values along either axis.\n\n        Parameters\n        ----------\n        by : str or list of str\n            Name or list of names to sort by.\n\n            - if `axis` is 0 or `'index'` then `by` may contain index\n              levels and/or column labels.\n            - if `axis` is 1 or `'columns'` then `by` may contain column\n              levels and/or index labels.\n        axis : \"{0 or 'index', 1 or 'columns'}\", default 0\n             Axis to be sorted.\n        ascending : bool or list of bool, default True\n             Sort ascending vs. descending. Specify list for multiple sort\n             orders.  If this is a list of bools, must match the length of\n             the by.\n        inplace : bool, default False\n             If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n             Choice of sorting algorithm. See also :func:`numpy.sort` for more\n             information. `mergesort` and `stable` are the only stable algorithms. For\n             DataFrames, this option is only applied when sorting on a single\n             column or label.\n        na_position : {'first', 'last'}, default 'last'\n             Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n             end.\n        ignore_index : bool, default False\n             If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n        key : callable, optional\n            Apply the key function to the values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect a\n            ``Series`` and return a Series with the same shape as the input.\n            It will be applied to each column in `by` independently.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with sorted values or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.sort_index : Sort a DataFrame by the index.\n        Series.sort_values : Similar method for a Series.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\n        ...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n        ...     'col2': [2, 1, 9, 8, 7, 4],\n        ...     'col3': [0, 1, 9, 4, 2, 3],\n        ...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n        ... })\n        >>> df\n          col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n\n        Sort by col1\n\n        >>> df.sort_values(by=['col1'])\n          col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        5    C     4     3    F\n        4    D     7     2    e\n        3  NaN     8     4    D\n\n        Sort by multiple columns\n\n        >>> df.sort_values(by=['col1', 'col2'])\n          col1  col2  col3 col4\n        1    A     1     1    B\n        0    A     2     0    a\n        2    B     9     9    c\n        5    C     4     3    F\n        4    D     7     2    e\n        3  NaN     8     4    D\n\n        Sort Descending\n\n        >>> df.sort_values(by='col1', ascending=False)\n          col1  col2  col3 col4\n        4    D     7     2    e\n        5    C     4     3    F\n        2    B     9     9    c\n        0    A     2     0    a\n        1    A     1     1    B\n        3  NaN     8     4    D\n\n        Putting NAs first\n\n        >>> df.sort_values(by='col1', ascending=False, na_position='first')\n          col1  col2  col3 col4\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n        2    B     9     9    c\n        0    A     2     0    a\n        1    A     1     1    B\n\n        Sorting with a key function\n\n        >>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n           col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n\n        Natural sort with the key argument,\n        using the `natsort <https://github.com/SethMMorton/natsort>` package.\n\n        >>> df = pd.DataFrame({\n        ...    \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n        ...    \"value\": [10, 20, 30, 40, 50]\n        ... })\n        >>> df\n            time  value\n        0    0hr     10\n        1  128hr     20\n        2   72hr     30\n        3   48hr     40\n        4   96hr     50\n        >>> from natsort import index_natsorted\n        >>> df.sort_values(\n        ...     by=\"time\",\n        ...     key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n        ... )\n            time  value\n        0    0hr     10\n        3   48hr     40\n        2   72hr     30\n        4   96hr     50\n        1  128hr     20\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n        ascending = validate_ascending(ascending)\n        if not isinstance(by, list):\n            by = [by]\n        # error: Argument 1 to \"len\" has incompatible type \"Union[bool, List[bool]]\";\n        # expected \"Sized\"\n        if is_sequence(ascending) and (\n            len(by) != len(ascending)  # type: ignore[arg-type]\n        ):\n            # error: Argument 1 to \"len\" has incompatible type \"Union[bool,\n            # List[bool]]\"; expected \"Sized\"\n            raise ValueError(\n                f\"Length of ascending ({len(ascending)})\"  # type: ignore[arg-type]\n                f\" != length of by ({len(by)})\"\n            )\n        if len(by) > 1:\n            keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n\n            # need to rewrap columns in Series to apply key function\n            if key is not None:\n                # error: List comprehension has incompatible type List[Series];\n                # expected List[ndarray]\n                keys = [\n                    Series(k, name=name)  # type: ignore[misc]\n                    for (k, name) in zip(keys, by)\n                ]\n\n            indexer = lexsort_indexer(\n                keys, orders=ascending, na_position=na_position, key=key\n            )\n        elif len(by):\n            # len(by) == 1\n\n            k = self._get_label_or_level_values(by[0], axis=axis)\n\n            # need to rewrap column in Series to apply key function\n            if key is not None:\n                # error: Incompatible types in assignment (expression has type\n                # \"Series\", variable has type \"ndarray\")\n                k = Series(k, name=by[0])  # type: ignore[assignment]\n\n            if isinstance(ascending, (tuple, list)):\n                ascending = ascending[0]\n\n            indexer = nargsort(\n                k, kind=kind, ascending=ascending, na_position=na_position, key=key\n            )\n        else:\n            if inplace:\n                return self._update_inplace(self)\n            else:\n                return self.copy(deep=None)\n\n        if is_range_indexer(indexer, len(indexer)):\n            result = self.copy(deep=(not inplace and not using_copy_on_write()))\n            if ignore_index:\n                result.index = default_index(len(result))\n\n            if inplace:\n                return self._update_inplace(result)\n            else:\n                return result\n\n        new_data = self._mgr.take(\n            indexer, axis=self._get_block_manager_axis(axis), verify=False\n        )\n\n        if ignore_index:\n            new_data.set_axis(\n                self._get_block_manager_axis(axis), default_index(len(indexer))\n            )\n\n        result = self._constructor_from_mgr(new_data, axes=new_data.axes)\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_values\")\n\n    @overload\n    def sort_index(\n        self,\n        *,\n        axis: Axis = ...,\n        level: IndexLabel = ...,\n        ascending: bool | Sequence[bool] = ...,\n        inplace: Literal[True],\n        kind: SortKind = ...,\n        na_position: NaPosition = ...,\n        sort_remaining: bool = ...,\n        ignore_index: bool = ...,\n        key: IndexKeyFunc = ...,\n    ) -> None:\n        ...\n\n    @overload\n    def sort_index(\n        self,\n        *,\n        axis: Axis = ...,\n        level: IndexLabel = ...,\n        ascending: bool | Sequence[bool] = ...,\n        inplace: Literal[False] = ...,\n        kind: SortKind = ...,\n        na_position: NaPosition = ...,\n        sort_remaining: bool = ...,\n        ignore_index: bool = ...,\n        key: IndexKeyFunc = ...,\n    ) -> DataFrame:\n        ...\n\n    @overload\n    def sort_index(\n        self,\n        *,\n        axis: Axis = ...,\n        level: IndexLabel = ...,\n        ascending: bool | Sequence[bool] = ...,\n        inplace: bool = ...,\n        kind: SortKind = ...,\n        na_position: NaPosition = ...,\n        sort_remaining: bool = ...,\n        ignore_index: bool = ...,\n        key: IndexKeyFunc = ...,\n    ) -> DataFrame | None:\n        ...\n\n    def sort_index(\n        self,\n        *,\n        axis: Axis = 0,\n        level: IndexLabel | None = None,\n        ascending: bool | Sequence[bool] = True,\n        inplace: bool = False,\n        kind: SortKind = \"quicksort\",\n        na_position: NaPosition = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n        key: IndexKeyFunc | None = None,\n    ) -> DataFrame | None:\n        \"\"\"\n        Sort object by labels (along an axis).\n\n        Returns a new DataFrame sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original DataFrame and returns None.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis along which to sort.  The value 0 identifies the rows,\n            and 1 identifies the columns.\n        level : int or level name or list of ints or list of level names\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list-like of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            Whether to modify the DataFrame rather than creating a new one.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. `mergesort` and `stable` are the only stable algorithms. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            Puts NaNs at the beginning if `first`; `last` puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape. For MultiIndex\n            inputs, the key is applied *per level*.\n\n        Returns\n        -------\n        DataFrame or None\n            The original DataFrame sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.sort_index : Sort Series by the index.\n        DataFrame.sort_values : Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n        ...                   columns=['A'])\n        >>> df.sort_index()\n             A\n        1    4\n        29   2\n        100  1\n        150  5\n        234  3\n\n        By default, it sorts in ascending order, to sort in descending order,\n        use ``ascending=False``\n\n        >>> df.sort_index(ascending=False)\n             A\n        234  3\n        150  5\n        100  1\n        29   2\n        1    4\n\n        A key function can be specified which is applied to the index before\n        sorting. For a ``MultiIndex`` this is applied to each level separately.\n\n        >>> df = pd.DataFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n        >>> df.sort_index(key=lambda x: x.str.lower())\n           a\n        A  1\n        b  2\n        C  3\n        d  4\n        \"\"\"\n        return super().sort_index(\n            axis=axis,\n            level=level,\n            ascending=ascending,\n            inplace=inplace,\n            kind=kind,\n            na_position=na_position,\n            sort_remaining=sort_remaining,\n            ignore_index=ignore_index,\n            key=key,\n        )\n\n    def value_counts(\n        self,\n        subset: IndexLabel | None = None,\n        normalize: bool = False,\n        sort: bool = True,\n        ascending: bool = False,\n        dropna: bool = True,\n    ) -> Series:\n        \"\"\"\n        Return a Series containing the frequency of each distinct row in the Dataframe.\n\n        Parameters\n        ----------\n        subset : label or list of labels, optional\n            Columns to use when counting unique combinations.\n        normalize : bool, default False\n            Return proportions rather than frequencies.\n        sort : bool, default True\n            Sort by frequencies when True. Sort by DataFrame column values when False.\n        ascending : bool, default False\n            Sort in ascending order.\n        dropna : bool, default True\n            Don't include counts of rows that contain NA values.\n\n            .. versionadded:: 1.3.0\n\n        Returns\n        -------\n        Series\n\n        See Also\n        --------\n        Series.value_counts: Equivalent method on Series.\n\n        Notes\n        -----\n        The returned Series will have a MultiIndex with one level per input\n        column but an Index (non-multi) for a single label. By default, rows\n        that contain any NA values are omitted from the result. By default,\n        the resulting Series will be in descending order so that the first\n        element is the most frequently-occurring row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6],\n        ...                    'num_wings': [2, 0, 0, 0]},\n        ...                   index=['falcon', 'dog', 'cat', 'ant'])\n        >>> df\n                num_legs  num_wings\n        falcon         2          2\n        dog            4          0\n        cat            4          0\n        ant            6          0\n\n        >>> df.value_counts()\n        num_legs  num_wings\n        4         0            2\n        2         2            1\n        6         0            1\n        Name: count, dtype: int64\n\n        >>> df.value_counts(sort=False)\n        num_legs  num_wings\n        2         2            1\n        4         0            2\n        6         0            1\n        Name: count, dtype: int64\n\n        >>> df.value_counts(ascending=True)\n        num_legs  num_wings\n        2         2            1\n        6         0            1\n        4         0            2\n        Name: count, dtype: int64\n\n        >>> df.value_counts(normalize=True)\n        num_legs  num_wings\n        4         0            0.50\n        2         2            0.25\n        6         0            0.25\n        Name: proportion, dtype: float64\n\n        With `dropna` set to `False` we can also count rows with NA values.\n\n        >>> df = pd.DataFrame({'first_name': ['John', 'Anne', 'John', 'Beth'],\n        ...                    'middle_name': ['Smith', pd.NA, pd.NA, 'Louise']})\n        >>> df\n          first_name middle_name\n        0       John       Smith\n        1       Anne        <NA>\n        2       John        <NA>\n        3       Beth      Louise\n\n        >>> df.value_counts()\n        first_name  middle_name\n        Beth        Louise         1\n        John        Smith          1\n        Name: count, dtype: int64\n\n        >>> df.value_counts(dropna=False)\n        first_name  middle_name\n        Anne        NaN            1\n        Beth        Louise         1\n        John        Smith          1\n                    NaN            1\n        Name: count, dtype: int64\n\n        >>> df.value_counts(\"first_name\")\n        first_name\n        John    2\n        Anne    1\n        Beth    1\n        Name: count, dtype: int64\n        \"\"\"\n        if subset is None:\n            subset = self.columns.tolist()\n\n        name = \"proportion\" if normalize else \"count\"\n        counts = self.groupby(subset, dropna=dropna, observed=False)._grouper.size()\n        counts.name = name\n\n        if sort:\n            counts = counts.sort_values(ascending=ascending)\n        if normalize:\n            counts /= counts.sum()\n\n        # Force MultiIndex for a list_like subset with a single column\n        if is_list_like(subset) and len(subset) == 1:  # type: ignore[arg-type]\n            counts.index = MultiIndex.from_arrays(\n                [counts.index], names=[counts.index.name]\n            )\n\n        return counts\n\n    def nlargest(\n        self, n: int, columns: IndexLabel, keep: NsmallestNlargestKeep = \"first\"\n    ) -> DataFrame:\n        \"\"\"\n        Return the first `n` rows ordered by `columns` in descending order.\n\n        Return the first `n` rows with the largest values in `columns`, in\n        descending order. The columns that are not specified are returned as\n        well, but not used for ordering.\n\n        This method is equivalent to\n        ``df.sort_values(columns, ascending=False).head(n)``, but more\n        performant.\n\n        Parameters\n        ----------\n        n : int\n            Number of rows to return.\n        columns : label or list of labels\n            Column label(s) to order by.\n        keep : {'first', 'last', 'all'}, default 'first'\n            Where there are duplicate values:\n\n            - ``first`` : prioritize the first occurrence(s)\n            - ``last`` : prioritize the last occurrence(s)\n            - ``all`` : keep all the ties of the smallest item even if it means\n              selecting more than ``n`` items.\n\n        Returns\n        -------\n        DataFrame\n            The first `n` rows ordered by the given columns in descending\n            order.\n\n        See Also\n        --------\n        DataFrame.nsmallest : Return the first `n` rows ordered by `columns` in\n            ascending order.\n        DataFrame.sort_values : Sort DataFrame by the values.\n        DataFrame.head : Return the first `n` rows without re-ordering.\n\n        Notes\n        -----\n        This function cannot be used with all column types. For example, when\n        specifying columns with `object` or `category` dtypes, ``TypeError`` is\n        raised.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n        ...                                   434000, 434000, 337000, 11300,\n        ...                                   11300, 11300],\n        ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n        ...                            17036, 182, 38, 311],\n        ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n        >>> df\n                  population      GDP alpha-2\n        Italy       59000000  1937894      IT\n        France      65000000  2583560      FR\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n        Iceland       337000    17036      IS\n        Nauru          11300      182      NR\n        Tuvalu         11300       38      TV\n        Anguilla       11300      311      AI\n\n        In the following example, we will use ``nlargest`` to select the three\n        rows having the largest values in column \"population\".\n\n        >>> df.nlargest(3, 'population')\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Malta       434000    12011      MT\n\n        When using ``keep='last'``, ties are resolved in reverse order:\n\n        >>> df.nlargest(3, 'population', keep='last')\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Brunei      434000    12128      BN\n\n        When using ``keep='all'``, the number of element kept can go beyond ``n``\n        if there are duplicate values for the smallest element, all the\n        ties are kept:\n\n        >>> df.nlargest(3, 'population', keep='all')\n                  population      GDP alpha-2\n        France      65000000  2583560      FR\n        Italy       59000000  1937894      IT\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n\n        However, ``nlargest`` does not keep ``n`` distinct largest elements:\n\n        >>> df.nlargest(5, 'population', keep='all')\n                  population      GDP alpha-2\n        France      65000000  2583560      FR\n        Italy       59000000  1937894      IT\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n\n        To order by the largest values in column \"population\" and then \"GDP\",\n        we can specify multiple columns like in the next example.\n\n        >>> df.nlargest(3, ['population', 'GDP'])\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Brunei      434000    12128      BN\n        \"\"\"\n        return selectn.SelectNFrame(self, n=n, keep=keep, columns=columns).nlargest()\n\n    def nsmallest(\n        self, n: int, columns: IndexLabel, keep: NsmallestNlargestKeep = \"first\"\n    ) -> DataFrame:\n        \"\"\"\n        Return the first `n` rows ordered by `columns` in ascending order.\n\n        Return the first `n` rows with the smallest values in `columns`, in\n        ascending order. The columns that are not specified are returned as\n        well, but not used for ordering.\n\n        This method is equivalent to\n        ``df.sort_values(columns, ascending=True).head(n)``, but more\n        performant.\n\n        Parameters\n        ----------\n        n : int\n            Number of items to retrieve.\n        columns : list or str\n            Column name or names to order by.\n        keep : {'first', 'last', 'all'}, default 'first'\n            Where there are duplicate values:\n\n            - ``first`` : take the first occurrence.\n            - ``last`` : take the last occurrence.\n            - ``all`` : keep all the ties of the largest item even if it means\n              selecting more than ``n`` items.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.nlargest : Return the first `n` rows ordered by `columns` in\n            descending order.\n        DataFrame.sort_values : Sort DataFrame by the values.\n        DataFrame.head : Return the first `n` rows without re-ordering.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n        ...                                   434000, 434000, 337000, 337000,\n        ...                                   11300, 11300],\n        ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n        ...                            17036, 182, 38, 311],\n        ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n        >>> df\n                  population      GDP alpha-2\n        Italy       59000000  1937894      IT\n        France      65000000  2583560      FR\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n        Iceland       337000    17036      IS\n        Nauru         337000      182      NR\n        Tuvalu         11300       38      TV\n        Anguilla       11300      311      AI\n\n        In the following example, we will use ``nsmallest`` to select the\n        three rows having the smallest values in column \"population\".\n\n        >>> df.nsmallest(3, 'population')\n                  population    GDP alpha-2\n        Tuvalu         11300     38      TV\n        Anguilla       11300    311      AI\n        Iceland       337000  17036      IS\n\n        When using ``keep='last'``, ties are resolved in reverse order:\n\n        >>> df.nsmallest(3, 'population', keep='last')\n                  population  GDP alpha-2\n        Anguilla       11300  311      AI\n        Tuvalu         11300   38      TV\n        Nauru         337000  182      NR\n\n        When using ``keep='all'``, the number of element kept can go beyond ``n``\n        if there are duplicate values for the largest element, all the\n        ties are kept.\n\n        >>> df.nsmallest(3, 'population', keep='all')\n                  population    GDP alpha-2\n        Tuvalu         11300     38      TV\n        Anguilla       11300    311      AI\n        Iceland       337000  17036      IS\n        Nauru         337000    182      NR\n\n        However, ``nsmallest`` does not keep ``n`` distinct\n        smallest elements:\n\n        >>> df.nsmallest(4, 'population', keep='all')\n                  population    GDP alpha-2\n        Tuvalu         11300     38      TV\n        Anguilla       11300    311      AI\n        Iceland       337000  17036      IS\n        Nauru         337000    182      NR\n\n        To order by the smallest values in column \"population\" and then \"GDP\", we can\n        specify multiple columns like in the next example.\n\n        >>> df.nsmallest(3, ['population', 'GDP'])\n                  population  GDP alpha-2\n        Tuvalu         11300   38      TV\n        Anguilla       11300  311      AI\n        Nauru         337000  182      NR\n        \"\"\"\n        return selectn.SelectNFrame(self, n=n, keep=keep, columns=columns).nsmallest()\n\n    @doc(\n        Series.swaplevel,\n        klass=_shared_doc_kwargs[\"klass\"],\n        extra_params=dedent(\n            \"\"\"axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to swap levels on. 0 or 'index' for row-wise, 1 or\n            'columns' for column-wise.\"\"\"\n        ),\n        examples=dedent(\n            \"\"\"\\\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"Grade\": [\"A\", \"B\", \"A\", \"C\"]},\n        ...     index=[\n        ...         [\"Final exam\", \"Final exam\", \"Coursework\", \"Coursework\"],\n        ...         [\"History\", \"Geography\", \"History\", \"Geography\"],\n        ...         [\"January\", \"February\", \"March\", \"April\"],\n        ...     ],\n        ... )\n        >>> df\n                                            Grade\n        Final exam  History     January      A\n                    Geography   February     B\n        Coursework  History     March        A\n                    Geography   April        C\n\n        In the following example, we will swap the levels of the indices.\n        Here, we will swap the levels column-wise, but levels can be swapped row-wise\n        in a similar manner. Note that column-wise is the default behaviour.\n        By not supplying any arguments for i and j, we swap the last and second to\n        last indices.\n\n        >>> df.swaplevel()\n                                            Grade\n        Final exam  January     History         A\n                    February    Geography       B\n        Coursework  March       History         A\n                    April       Geography       C\n\n        By supplying one argument, we can choose which index to swap the last\n        index with. We can for example swap the first index with the last one as\n        follows.\n\n        >>> df.swaplevel(0)\n                                            Grade\n        January     History     Final exam      A\n        February    Geography   Final exam      B\n        March       History     Coursework      A\n        April       Geography   Coursework      C\n\n        We can also define explicitly which indices we want to swap by supplying values\n        for both i and j. Here, we for example swap the first and second indices.\n\n        >>> df.swaplevel(0, 1)\n                                            Grade\n        History     Final exam  January         A\n        Geography   Final exam  February        B\n        History     Coursework  March           A\n        Geography   Coursework  April           C\"\"\"\n        ),\n    )\n    def swaplevel(self, i: Axis = -2, j: Axis = -1, axis: Axis = 0) -> DataFrame:\n        result = self.copy(deep=None)\n\n        axis = self._get_axis_number(axis)\n\n        if not isinstance(result._get_axis(axis), MultiIndex):  # pragma: no cover\n            raise TypeError(\"Can only swap levels on a hierarchical axis.\")\n\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.swaplevel(i, j)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.swaplevel(i, j)\n        return result\n\n    def reorder_levels(self, order: Sequence[int | str], axis: Axis = 0) -> DataFrame:\n        \"\"\"\n        Rearrange index levels using input order. May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int or list of str\n            List representing new level order. Reference level by number\n            (position) or by key (label).\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Where to reorder levels.\n\n        Returns\n        -------\n        DataFrame\n\n        Examples\n        --------\n        >>> data = {\n        ...     \"class\": [\"Mammals\", \"Mammals\", \"Reptiles\"],\n        ...     \"diet\": [\"Omnivore\", \"Carnivore\", \"Carnivore\"],\n        ...     \"species\": [\"Humans\", \"Dogs\", \"Snakes\"],\n        ... }\n        >>> df = pd.DataFrame(data, columns=[\"class\", \"diet\", \"species\"])\n        >>> df = df.set_index([\"class\", \"diet\"])\n        >>> df\n                                          species\n        class      diet\n        Mammals    Omnivore                Humans\n                   Carnivore                 Dogs\n        Reptiles   Carnivore               Snakes\n\n        Let's reorder the levels of the index:\n\n        >>> df.reorder_levels([\"diet\", \"class\"])\n                                          species\n        diet      class\n        Omnivore  Mammals                  Humans\n        Carnivore Mammals                    Dogs\n                  Reptiles                 Snakes\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        if not isinstance(self._get_axis(axis), MultiIndex):  # pragma: no cover\n            raise TypeError(\"Can only reorder levels on a hierarchical axis.\")\n\n        result = self.copy(deep=None)\n\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.reorder_levels(order)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.reorder_levels(order)\n        return result\n\n    # ----------------------------------------------------------------------\n    # Arithmetic Methods\n\n    def _cmp_method(self, other, op):\n        axis: Literal[1] = 1  # only relevant for Series other case\n\n        self, other = self._align_for_op(other, axis, flex=False, level=None)\n\n        # See GH#4537 for discussion of scalar op behavior\n        new_data = self._dispatch_frame_op(other, op, axis=axis)\n        return self._construct_result(new_data)\n\n    def _arith_method(self, other, op):\n        if self._should_reindex_frame_op(other, op, 1, None, None):\n            return self._arith_method_with_reindex(other, op)\n\n        axis: Literal[1] = 1  # only relevant for Series other case\n        other = ops.maybe_prepare_scalar_for_op(other, (self.shape[axis],))\n\n        self, other = self._align_for_op(other, axis, flex=True, level=None)\n\n        with np.errstate(all=\"ignore\"):\n            new_data = self._dispatch_frame_op(other, op, axis=axis)\n        return self._construct_result(new_data)\n\n    _logical_method = _arith_method\n\n    def _dispatch_frame_op(\n        self, right, func: Callable, axis: AxisInt | None = None\n    ) -> DataFrame:\n        \"\"\"\n        Evaluate the frame operation func(left, right) by evaluating\n        column-by-column, dispatching to the Series implementation.\n\n        Parameters\n        ----------\n        right : scalar, Series, or DataFrame\n        func : arithmetic or comparison operator\n        axis : {None, 0, 1}\n\n        Returns\n        -------\n        DataFrame\n\n        Notes\n        -----\n        Caller is responsible for setting np.errstate where relevant.\n        \"\"\"\n        # Get the appropriate array-op to apply to each column/block's values.\n        array_op = ops.get_array_op(func)\n\n        right = lib.item_from_zerodim(right)\n        if not is_list_like(right):\n            # i.e. scalar, faster than checking np.ndim(right) == 0\n            bm = self._mgr.apply(array_op, right=right)\n            return self._constructor_from_mgr(bm, axes=bm.axes)\n\n        elif isinstance(right, DataFrame):\n            assert self.index.equals(right.index)\n            assert self.columns.equals(right.columns)\n            # TODO: The previous assertion `assert right._indexed_same(self)`\n            #  fails in cases with empty columns reached via\n            #  _frame_arith_method_with_reindex\n\n            # TODO operate_blockwise expects a manager of the same type\n            bm = self._mgr.operate_blockwise(\n                # error: Argument 1 to \"operate_blockwise\" of \"ArrayManager\" has\n                # incompatible type \"Union[ArrayManager, BlockManager]\"; expected\n                # \"ArrayManager\"\n                # error: Argument 1 to \"operate_blockwise\" of \"BlockManager\" has\n                # incompatible type \"Union[ArrayManager, BlockManager]\"; expected\n                # \"BlockManager\"\n                right._mgr,  # type: ignore[arg-type]\n                array_op,\n            )\n            return self._constructor_from_mgr(bm, axes=bm.axes)\n\n        elif isinstance(right, Series) and axis == 1:\n            # axis=1 means we want to operate row-by-row\n            assert right.index.equals(self.columns)\n\n            right = right._values\n            # maybe_align_as_frame ensures we do not have an ndarray here\n            assert not isinstance(right, np.ndarray)\n\n            arrays = [\n                array_op(_left, _right)\n                for _left, _right in zip(self._iter_column_arrays(), right)\n            ]\n\n        elif isinstance(right, Series):\n            assert right.index.equals(self.index)\n            right = right._values\n\n            arrays = [array_op(left, right) for left in self._iter_column_arrays()]\n\n        else:\n            raise NotImplementedError(right)\n\n        return type(self)._from_arrays(\n            arrays, self.columns, self.index, verify_integrity=False\n        )\n\n    def _combine_frame(self, other: DataFrame, func, fill_value=None):\n        # at this point we have `self._indexed_same(other)`\n\n        if fill_value is None:\n            # since _arith_op may be called in a loop, avoid function call\n            #  overhead if possible by doing this check once\n            _arith_op = func\n\n        else:\n\n            def _arith_op(left, right):\n                # for the mixed_type case where we iterate over columns,\n                # _arith_op(left, right) is equivalent to\n                # left._binop(right, func, fill_value=fill_value)\n                left, right = ops.fill_binop(left, right, fill_value)\n                return func(left, right)\n\n        new_data = self._dispatch_frame_op(other, _arith_op)\n        return new_data\n\n    def _arith_method_with_reindex(self, right: DataFrame, op) -> DataFrame:\n        \"\"\"\n        For DataFrame-with-DataFrame operations that require reindexing,\n        operate only on shared columns, then reindex.\n\n        Parameters\n        ----------\n        right : DataFrame\n        op : binary operator\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        left = self\n\n        # GH#31623, only operate on shared columns\n        cols, lcols, rcols = left.columns.join(\n            right.columns, how=\"inner\", level=None, return_indexers=True\n        )\n\n        new_left = left.iloc[:, lcols]\n        new_right = right.iloc[:, rcols]\n        result = op(new_left, new_right)\n\n        # Do the join on the columns instead of using left._align_for_op\n        #  to avoid constructing two potentially large/sparse DataFrames\n        join_columns, _, _ = left.columns.join(\n            right.columns, how=\"outer\", level=None, return_indexers=True\n        )\n\n        if result.columns.has_duplicates:\n            # Avoid reindexing with a duplicate axis.\n            # https://github.com/pandas-dev/pandas/issues/35194\n            indexer, _ = result.columns.get_indexer_non_unique(join_columns)\n            indexer = algorithms.unique1d(indexer)\n            result = result._reindex_with_indexers(\n                {1: [join_columns, indexer]}, allow_dups=True\n            )\n        else:\n            result = result.reindex(join_columns, axis=1)\n\n        return result\n\n    def _should_reindex_frame_op(self, right, op, axis: int, fill_value, level) -> bool:\n        \"\"\"\n        Check if this is an operation between DataFrames that will need to reindex.\n        \"\"\"\n        if op is operator.pow or op is roperator.rpow:\n            # GH#32685 pow has special semantics for operating with null values\n            return False\n\n        if not isinstance(right, DataFrame):\n            return False\n\n        if fill_value is None and level is None and axis == 1:\n            # TODO: any other cases we should handle here?\n\n            # Intersection is always unique so we have to check the unique columns\n            left_uniques = self.columns.unique()\n            right_uniques = right.columns.unique()\n            cols = left_uniques.intersection(right_uniques)\n            if len(cols) and not (\n                len(cols) == len(left_uniques) and len(cols) == len(right_uniques)\n            ):\n                # TODO: is there a shortcut available when len(cols) == 0?\n                return True\n\n        return False\n\n    def _align_for_op(\n        self,\n        other,\n        axis: AxisInt,\n        flex: bool | None = False,\n        level: Level | None = None,\n    ):\n        \"\"\"\n        Convert rhs to meet lhs dims if input is list, tuple or np.ndarray.\n\n        Parameters\n        ----------\n        left : DataFrame\n        right : Any\n        axis : int\n        flex : bool or None, default False\n            Whether this is a flex op, in which case we reindex.\n            None indicates not to check for alignment.\n        level : int or level name, default None\n\n        Returns\n        -------\n        left : DataFrame\n        right : Any\n        \"\"\"\n        left, right = self, other\n\n        def to_series(right):\n            msg = (\n                \"Unable to coerce to Series, \"\n                \"length must be {req_len}: given {given_len}\"\n            )\n\n            # pass dtype to avoid doing inference, which would break consistency\n            #  with Index/Series ops\n            dtype = None\n            if getattr(right, \"dtype\", None) == object:\n                # can't pass right.dtype unconditionally as that would break on e.g.\n                #  datetime64[h] ndarray\n                dtype = object\n\n            if axis == 0:\n                if len(left.index) != len(right):\n                    raise ValueError(\n                        msg.format(req_len=len(left.index), given_len=len(right))\n                    )\n                right = left._constructor_sliced(right, index=left.index, dtype=dtype)\n            else:\n                if len(left.columns) != len(right):\n                    raise ValueError(\n                        msg.format(req_len=len(left.columns), given_len=len(right))\n                    )\n                right = left._constructor_sliced(right, index=left.columns, dtype=dtype)\n            return right\n\n        if isinstance(right, np.ndarray):\n            if right.ndim == 1:\n                right = to_series(right)\n\n            elif right.ndim == 2:\n                # We need to pass dtype=right.dtype to retain object dtype\n                #  otherwise we lose consistency with Index and array ops\n                dtype = None\n                if right.dtype == object:\n                    # can't pass right.dtype unconditionally as that would break on e.g.\n                    #  datetime64[h] ndarray\n                    dtype = object\n\n                if right.shape == left.shape:\n                    right = left._constructor(\n                        right, index=left.index, columns=left.columns, dtype=dtype\n                    )\n\n                elif right.shape[0] == left.shape[0] and right.shape[1] == 1:\n                    # Broadcast across columns\n                    right = np.broadcast_to(right, left.shape)\n                    right = left._constructor(\n                        right, index=left.index, columns=left.columns, dtype=dtype\n                    )\n\n                elif right.shape[1] == left.shape[1] and right.shape[0] == 1:\n                    # Broadcast along rows\n                    right = to_series(right[0, :])\n\n                else:\n                    raise ValueError(\n                        \"Unable to coerce to DataFrame, shape \"\n                        f\"must be {left.shape}: given {right.shape}\"\n                    )\n\n            elif right.ndim > 2:\n                raise ValueError(\n                    \"Unable to coerce to Series/DataFrame, \"\n                    f\"dimension must be <= 2: {right.shape}\"\n                )\n\n        elif is_list_like(right) and not isinstance(right, (Series, DataFrame)):\n            # GH#36702. Raise when attempting arithmetic with list of array-like.\n            if any(is_array_like(el) for el in right):\n                raise ValueError(\n                    f\"Unable to coerce list of {type(right[0])} to Series/DataFrame\"\n                )\n            # GH#17901\n            right = to_series(right)\n\n        if flex is not None and isinstance(right, DataFrame):\n            if not left._indexed_same(right):\n                if flex:\n                    left, right = left.align(\n                        right, join=\"outer\", level=level, copy=False\n                    )\n                else:\n                    raise ValueError(\n                        \"Can only compare identically-labeled (both index and columns) \"\n                        \"DataFrame objects\"\n                    )\n        elif isinstance(right, Series):\n            # axis=1 is default for DataFrame-with-Series op\n            axis = axis if axis is not None else 1\n            if not flex:\n                if not left.axes[axis].equals(right.index):\n                    raise ValueError(\n                        \"Operands are not aligned. Do \"\n                        \"`left, right = left.align(right, axis=1, copy=False)` \"\n                        \"before operating.\"\n                    )\n\n            left, right = left.align(\n                right,\n                join=\"outer\",\n                axis=axis,\n                level=level,\n                copy=False,\n            )\n            right = left._maybe_align_series_as_frame(right, axis)\n\n        return left, right\n\n    def _maybe_align_series_as_frame(self, series: Series, axis: AxisInt):\n        \"\"\"\n        If the Series operand is not EA-dtype, we can broadcast to 2D and operate\n        blockwise.\n        \"\"\"\n        rvalues = series._values\n        if not isinstance(rvalues, np.ndarray):\n            # TODO(EA2D): no need to special-case with 2D EAs\n            if rvalues.dtype in (\"datetime64[ns]\", \"timedelta64[ns]\"):\n                # We can losslessly+cheaply cast to ndarray\n                rvalues = np.asarray(rvalues)\n            else:\n                return series\n\n        if axis == 0:\n            rvalues = rvalues.reshape(-1, 1)\n        else:\n            rvalues = rvalues.reshape(1, -1)\n\n        rvalues = np.broadcast_to(rvalues, self.shape)\n        # pass dtype to avoid doing inference\n        return self._constructor(\n            rvalues,\n            index=self.index,\n            columns=self.columns,\n            dtype=rvalues.dtype,\n        )\n\n    def _flex_arith_method(\n        self, other, op, *, axis: Axis = \"columns\", level=None, fill_value=None\n    ):\n        axis = self._get_axis_number(axis) if axis is not None else 1\n\n        if self._should_reindex_frame_op(other, op, axis, fill_value, level):\n            return self._arith_method_with_reindex(other, op)\n\n        if isinstance(other, Series) and fill_value is not None:\n            # TODO: We could allow this in cases where we end up going\n            #  through the DataFrame path\n            raise NotImplementedError(f\"fill_value {fill_value} not supported.\")\n\n        other = ops.maybe_prepare_scalar_for_op(other, self.shape)\n        self, other = self._align_for_op(other, axis, flex=True, level=level)\n\n        with np.errstate(all=\"ignore\"):\n            if isinstance(other, DataFrame):\n                # Another DataFrame\n                new_data = self._combine_frame(other, op, fill_value)\n\n            elif isinstance(other, Series):\n                new_data = self._dispatch_frame_op(other, op, axis=axis)\n            else:\n                # in this case we always have `np.ndim(other) == 0`\n                if fill_value is not None:\n                    self = self.fillna(fill_value)\n\n                new_data = self._dispatch_frame_op(other, op)\n\n        return self._construct_result(new_data)\n\n    def _construct_result(self, result) -> DataFrame:\n        \"\"\"\n        Wrap the result of an arithmetic, comparison, or logical operation.\n\n        Parameters\n        ----------\n        result : DataFrame\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        out = self._constructor(result, copy=False).__finalize__(self)\n        # Pin columns instead of passing to constructor for compat with\n        #  non-unique columns case\n        out.columns = self.columns\n        out.index = self.index\n        return out\n\n    def __divmod__(self, other) -> tuple[DataFrame, DataFrame]:\n        # Naive implementation, room for optimization\n        div = self // other\n        mod = self - div * other\n        return div, mod\n\n    def __rdivmod__(self, other) -> tuple[DataFrame, DataFrame]:\n        # Naive implementation, room for optimization\n        div = other // self\n        mod = other - div * self\n        return div, mod\n\n    def _flex_cmp_method(self, other, op, *, axis: Axis = \"columns\", level=None):\n        axis = self._get_axis_number(axis) if axis is not None else 1\n\n        self, other = self._align_for_op(other, axis, flex=True, level=level)\n\n        new_data = self._dispatch_frame_op(other, op, axis=axis)\n        return self._construct_result(new_data)\n\n    @Appender(ops.make_flex_doc(\"eq\", \"dataframe\"))\n    def eq(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.eq, axis=axis, level=level)\n\n    @Appender(ops.make_flex_doc(\"ne\", \"dataframe\"))\n    def ne(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.ne, axis=axis, level=level)\n\n    @Appender(ops.make_flex_doc(\"le\", \"dataframe\"))\n    def le(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.le, axis=axis, level=level)\n\n    @Appender(ops.make_flex_doc(\"lt\", \"dataframe\"))\n    def lt(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.lt, axis=axis, level=level)\n\n    @Appender(ops.make_flex_doc(\"ge\", \"dataframe\"))\n    def ge(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.ge, axis=axis, level=level)\n\n    @Appender(ops.make_flex_doc(\"gt\", \"dataframe\"))\n    def gt(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.gt, axis=axis, level=level)\n\n    @Appender(ops.make_flex_doc(\"add\", \"dataframe\"))\n    def add(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, operator.add, level=level, fill_value=fill_value, axis=axis\n        )\n\n    @Appender(ops.make_flex_doc(\"radd\", \"dataframe\"))\n    def radd(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, roperator.radd, level=level, fill_value=fill_value, axis=axis\n        )\n\n    @Appender(ops.make_flex_doc(\"sub\", \"dataframe\"))\n    def sub(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, operator.sub, level=level, fill_value=fill_value, axis=axis\n        )\n\n    subtract = sub\n\n    @Appender(ops.make_flex_doc(\"rsub\", \"dataframe\"))\n    def rsub(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, roperator.rsub, level=level, fill_value=fill_value, axis=axis\n        )\n\n    @Appender(ops.make_flex_doc(\"mul\", \"dataframe\"))\n    def mul(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, operator.mul, level=level, fill_value=fill_value, axis=axis\n        )\n\n    multiply = mul\n\n    @Appender(ops.make_flex_doc(\"rmul\", \"dataframe\"))\n    def rmul(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, roperator.rmul, level=level, fill_value=fill_value, axis=axis\n        )\n\n    @Appender(ops.make_flex_doc(\"truediv\", \"dataframe\"))\n    def truediv(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, operator.truediv, level=level, fill_value=fill_value, axis=axis\n        )\n\n    div = truediv\n    divide = truediv\n\n    @Appender(ops.make_flex_doc(\"rtruediv\", \"dataframe\"))\n    def rtruediv(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, roperator.rtruediv, level=level, fill_value=fill_value, axis=axis\n        )\n\n    rdiv = rtruediv\n\n    @Appender(ops.make_flex_doc(\"floordiv\", \"dataframe\"))\n    def floordiv(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, operator.floordiv, level=level, fill_value=fill_value, axis=axis\n        )\n\n    @Appender(ops.make_flex_doc(\"rfloordiv\", \"dataframe\"))\n    def rfloordiv(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, roperator.rfloordiv, level=level, fill_value=fill_value, axis=axis\n        )\n\n    @Appender(ops.make_flex_doc(\"mod\", \"dataframe\"))\n    def mod(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, operator.mod, level=level, fill_value=fill_value, axis=axis\n        )\n\n    @Appender(ops.make_flex_doc(\"rmod\", \"dataframe\"))\n    def rmod(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, roperator.rmod, level=level, fill_value=fill_value, axis=axis\n        )\n\n    @Appender(ops.make_flex_doc(\"pow\", \"dataframe\"))\n    def pow(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, operator.pow, level=level, fill_value=fill_value, axis=axis\n        )\n\n    @Appender(ops.make_flex_doc(\"rpow\", \"dataframe\"))\n    def rpow(\n        self, other, axis: Axis = \"columns\", level=None, fill_value=None\n    ) -> DataFrame:\n        return self._flex_arith_method(\n            other, roperator.rpow, level=level, fill_value=fill_value, axis=axis\n        )\n\n    # ----------------------------------------------------------------------\n    # Combination-Related\n\n    @doc(\n        _shared_docs[\"compare\"],\n        dedent(\n            \"\"\"\n        Returns\n        -------\n        DataFrame\n            DataFrame that shows the differences stacked side by side.\n\n            The resulting index will be a MultiIndex with 'self' and 'other'\n            stacked alternately at the inner level.\n\n        Raises\n        ------\n        ValueError\n            When the two DataFrames don't have identical labels or shape.\n\n        See Also\n        --------\n        Series.compare : Compare with another Series and show differences.\n        DataFrame.equals : Test whether two objects contain the same elements.\n\n        Notes\n        -----\n        Matching NaNs will not appear as a difference.\n\n        Can only compare identically-labeled\n        (i.e. same shape, identical row and column labels) DataFrames\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {{\n        ...         \"col1\": [\"a\", \"a\", \"b\", \"b\", \"a\"],\n        ...         \"col2\": [1.0, 2.0, 3.0, np.nan, 5.0],\n        ...         \"col3\": [1.0, 2.0, 3.0, 4.0, 5.0]\n        ...     }},\n        ...     columns=[\"col1\", \"col2\", \"col3\"],\n        ... )\n        >>> df\n          col1  col2  col3\n        0    a   1.0   1.0\n        1    a   2.0   2.0\n        2    b   3.0   3.0\n        3    b   NaN   4.0\n        4    a   5.0   5.0\n\n        >>> df2 = df.copy()\n        >>> df2.loc[0, 'col1'] = 'c'\n        >>> df2.loc[2, 'col3'] = 4.0\n        >>> df2\n          col1  col2  col3\n        0    c   1.0   1.0\n        1    a   2.0   2.0\n        2    b   3.0   4.0\n        3    b   NaN   4.0\n        4    a   5.0   5.0\n\n        Align the differences on columns\n\n        >>> df.compare(df2)\n          col1       col3\n          self other self other\n        0    a     c  NaN   NaN\n        2  NaN   NaN  3.0   4.0\n\n        Assign result_names\n\n        >>> df.compare(df2, result_names=(\"left\", \"right\"))\n          col1       col3\n          left right left right\n        0    a     c  NaN   NaN\n        2  NaN   NaN  3.0   4.0\n\n        Stack the differences on rows\n\n        >>> df.compare(df2, align_axis=0)\n                col1  col3\n        0 self     a   NaN\n          other    c   NaN\n        2 self   NaN   3.0\n          other  NaN   4.0\n\n        Keep the equal values\n\n        >>> df.compare(df2, keep_equal=True)\n          col1       col3\n          self other self other\n        0    a     c  1.0   1.0\n        2    b     b  3.0   4.0\n\n        Keep all original rows and columns\n\n        >>> df.compare(df2, keep_shape=True)\n          col1       col2       col3\n          self other self other self other\n        0    a     c  NaN   NaN  NaN   NaN\n        1  NaN   NaN  NaN   NaN  NaN   NaN\n        2  NaN   NaN  NaN   NaN  3.0   4.0\n        3  NaN   NaN  NaN   NaN  NaN   NaN\n        4  NaN   NaN  NaN   NaN  NaN   NaN\n\n        Keep all original rows and columns and also all original values\n\n        >>> df.compare(df2, keep_shape=True, keep_equal=True)\n          col1       col2       col3\n          self other self other self other\n        0    a     c  1.0   1.0  1.0   1.0\n        1    a     a  2.0   2.0  2.0   2.0\n        2    b     b  3.0   3.0  3.0   4.0\n        3    b     b  NaN   NaN  4.0   4.0\n        4    a     a  5.0   5.0  5.0   5.0\n        \"\"\"\n        ),\n        klass=_shared_doc_kwargs[\"klass\"],\n    )\n    def compare(\n        self,\n        other: DataFrame,\n        align_axis: Axis = 1,\n        keep_shape: bool = False,\n        keep_equal: bool = False,\n        result_names: Suffixes = (\"self\", \"other\"),\n    ) -> DataFrame:\n        return super().compare(\n            other=other,\n            align_axis=align_axis,\n            keep_shape=keep_shape,\n            keep_equal=keep_equal,\n            result_names=result_names,\n        )\n\n    def combine(\n        self,\n        other: DataFrame,\n        func: Callable[[Series, Series], Series | Hashable],\n        fill_value=None,\n        overwrite: bool = True,\n    ) -> DataFrame:\n        \"\"\"\n        Perform column-wise combine with another DataFrame.\n\n        Combines a DataFrame with `other` DataFrame using `func`\n        to element-wise combine columns. The row and column indexes of the\n        resulting DataFrame will be the union of the two.\n\n        Parameters\n        ----------\n        other : DataFrame\n            The DataFrame to merge column-wise.\n        func : function\n            Function that takes two series as inputs and return a Series or a\n            scalar. Used to merge the two dataframes column by columns.\n        fill_value : scalar value, default None\n            The value to fill NaNs with prior to passing any column to the\n            merge func.\n        overwrite : bool, default True\n            If True, columns in `self` that do not exist in `other` will be\n            overwritten with NaNs.\n\n        Returns\n        -------\n        DataFrame\n            Combination of the provided DataFrames.\n\n        See Also\n        --------\n        DataFrame.combine_first : Combine two DataFrame objects and default to\n            non-null values in frame calling the method.\n\n        Examples\n        --------\n        Combine using a simple function that chooses the smaller column.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2\n        >>> df1.combine(df2, take_smaller)\n           A  B\n        0  0  3\n        1  0  3\n\n        Example using a true element-wise combine function.\n\n        >>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine(df2, np.minimum)\n           A  B\n        0  1  2\n        1  0  3\n\n        Using `fill_value` fills Nones prior to passing the column to the\n        merge function.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\n           A    B\n        0  0 -5.0\n        1  0  4.0\n\n        However, if the same element in both dataframes is None, that None\n        is preserved\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\n            A    B\n        0  0 -5.0\n        1  0  3.0\n\n        Example that demonstrates the use of `overwrite` and behavior when\n        the axis differ between the dataframes.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2])\n        >>> df1.combine(df2, take_smaller)\n             A    B     C\n        0  NaN  NaN   NaN\n        1  NaN  3.0 -10.0\n        2  NaN  3.0   1.0\n\n        >>> df1.combine(df2, take_smaller, overwrite=False)\n             A    B     C\n        0  0.0  NaN   NaN\n        1  0.0  3.0 -10.0\n        2  NaN  3.0   1.0\n\n        Demonstrating the preference of the passed in dataframe.\n\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2])\n        >>> df2.combine(df1, take_smaller)\n           A    B   C\n        0  0.0  NaN NaN\n        1  0.0  3.0 NaN\n        2  NaN  3.0 NaN\n\n        >>> df2.combine(df1, take_smaller, overwrite=False)\n             A    B   C\n        0  0.0  NaN NaN\n        1  0.0  3.0 1.0\n        2  NaN  3.0 1.0\n        \"\"\"\n        other_idxlen = len(other.index)  # save for compare\n\n        this, other = self.align(other, copy=False)\n        new_index = this.index\n\n        if other.empty and len(new_index) == len(self.index):\n            return self.copy()\n\n        if self.empty and len(other) == other_idxlen:\n            return other.copy()\n\n        # sorts if possible; otherwise align above ensures that these are set-equal\n        new_columns = this.columns.union(other.columns)\n        do_fill = fill_value is not None\n        result = {}\n        for col in new_columns:\n            series = this[col]\n            other_series = other[col]\n\n            this_dtype = series.dtype\n            other_dtype = other_series.dtype\n\n            this_mask = isna(series)\n            other_mask = isna(other_series)\n\n            # don't overwrite columns unnecessarily\n            # DO propagate if this column is not in the intersection\n            if not overwrite and other_mask.all():\n                result[col] = this[col].copy()\n                continue\n\n            if do_fill:\n                series = series.copy()\n                other_series = other_series.copy()\n                series[this_mask] = fill_value\n                other_series[other_mask] = fill_value\n\n            if col not in self.columns:\n                # If self DataFrame does not have col in other DataFrame,\n                # try to promote series, which is all NaN, as other_dtype.\n                new_dtype = other_dtype\n                try:\n                    series = series.astype(new_dtype, copy=False)\n                except ValueError:\n                    # e.g. new_dtype is integer types\n                    pass\n            else:\n                # if we have different dtypes, possibly promote\n                new_dtype = find_common_type([this_dtype, other_dtype])\n                series = series.astype(new_dtype, copy=False)\n                other_series = other_series.astype(new_dtype, copy=False)\n\n            arr = func(series, other_series)\n            if isinstance(new_dtype, np.dtype):\n                # if new_dtype is an EA Dtype, then `func` is expected to return\n                # the correct dtype without any additional casting\n                # error: No overload variant of \"maybe_downcast_to_dtype\" matches\n                # argument types \"Union[Series, Hashable]\", \"dtype[Any]\"\n                arr = maybe_downcast_to_dtype(  # type: ignore[call-overload]\n                    arr, new_dtype\n                )\n\n            result[col] = arr\n\n        # convert_objects just in case\n        frame_result = self._constructor(result, index=new_index, columns=new_columns)\n        return frame_result.__finalize__(self, method=\"combine\")\n\n    def combine_first(self, other: DataFrame) -> DataFrame:\n        \"\"\"\n        Update null elements with value in the same location in `other`.\n\n        Combine two DataFrame objects by filling null values in one DataFrame\n        with non-null values from other DataFrame. The row and column indexes\n        of the resulting DataFrame will be the union of the two. The resulting\n        dataframe contains the 'first' dataframe values and overrides the\n        second one values where both first.loc[index, col] and\n        second.loc[index, col] are not missing values, upon calling\n        first.combine_first(second).\n\n        Parameters\n        ----------\n        other : DataFrame\n            Provided DataFrame to use to fill null values.\n\n        Returns\n        -------\n        DataFrame\n            The result of combining the provided DataFrame with the other object.\n\n        See Also\n        --------\n        DataFrame.combine : Perform series-wise operation on two DataFrames\n            using a given function.\n\n        Examples\n        --------\n        >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine_first(df2)\n             A    B\n        0  1.0  3.0\n        1  0.0  4.0\n\n        Null values still persist if the location of that null value\n        does not exist in `other`\n\n        >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])\n        >>> df1.combine_first(df2)\n             A    B    C\n        0  NaN  4.0  NaN\n        1  0.0  3.0  1.0\n        2  NaN  3.0  1.0\n        \"\"\"\n        from pandas.core.computation import expressions\n\n        def combiner(x: Series, y: Series):\n            mask = x.isna()._values\n\n            x_values = x._values\n            y_values = y._values\n\n            # If the column y in other DataFrame is not in first DataFrame,\n            # just return y_values.\n            if y.name not in self.columns:\n                return y_values\n\n            return expressions.where(mask, y_values, x_values)\n\n        if len(other) == 0:\n            combined = self.reindex(\n                self.columns.append(other.columns.difference(self.columns)), axis=1\n            )\n            combined = combined.astype(other.dtypes)\n        else:\n            combined = self.combine(other, combiner, overwrite=False)\n\n        dtypes = {\n            col: find_common_type([self.dtypes[col], other.dtypes[col]])\n            for col in self.columns.intersection(other.columns)\n            if combined.dtypes[col] != self.dtypes[col]\n        }\n\n        if dtypes:\n            combined = combined.astype(dtypes)\n\n        return combined.__finalize__(self, method=\"combine_first\")\n\n    def update(\n        self,\n        other,\n        join: UpdateJoin = \"left\",\n        overwrite: bool = True,\n        filter_func=None,\n        errors: IgnoreRaise = \"ignore\",\n    ) -> None:\n        \"\"\"\n        Modify in place using non-NA values from another DataFrame.\n\n        Aligns on indices. There is no return value.\n\n        Parameters\n        ----------\n        other : DataFrame, or object coercible into a DataFrame\n            Should have at least one matching index/column label\n            with the original DataFrame. If a Series is passed,\n            its name attribute must be set, and that will be\n            used as the column name to align with the original DataFrame.\n        join : {'left'}, default 'left'\n            Only left join is implemented, keeping the index and columns of the\n            original object.\n        overwrite : bool, default True\n            How to handle non-NA values for overlapping keys:\n\n            * True: overwrite original DataFrame's values\n              with values from `other`.\n            * False: only update values that are NA in\n              the original DataFrame.\n\n        filter_func : callable(1d-array) -> bool 1d-array, optional\n            Can choose to replace values other than NA. Return True for values\n            that should be updated.\n        errors : {'raise', 'ignore'}, default 'ignore'\n            If 'raise', will raise a ValueError if the DataFrame and `other`\n            both contain non-NA data in the same place.\n\n        Returns\n        -------\n        None\n            This method directly changes calling object.\n\n        Raises\n        ------\n        ValueError\n            * When `errors='raise'` and there's overlapping non-NA data.\n            * When `errors` is not either `'ignore'` or `'raise'`\n        NotImplementedError\n            * If `join != 'left'`\n\n        See Also\n        --------\n        dict.update : Similar method for dictionaries.\n        DataFrame.merge : For column(s)-on-column(s) operations.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3],\n        ...                    'B': [400, 500, 600]})\n        >>> new_df = pd.DataFrame({'B': [4, 5, 6],\n        ...                        'C': [7, 8, 9]})\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  1  4\n        1  2  5\n        2  3  6\n\n        The DataFrame's length does not increase as a result of the update,\n        only values at matching index/column labels are updated.\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_df = pd.DataFrame({'B': ['d', 'e', 'f', 'g', 'h', 'i']})\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  a  d\n        1  b  e\n        2  c  f\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_df = pd.DataFrame({'B': ['d', 'f']}, index=[0, 2])\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  a  d\n        1  b  y\n        2  c  f\n\n        For Series, its name attribute must be set.\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_column = pd.Series(['d', 'e', 'f'], name='B')\n        >>> df.update(new_column)\n        >>> df\n           A  B\n        0  a  d\n        1  b  e\n        2  c  f\n\n        If `other` contains NaNs the corresponding values are not updated\n        in the original dataframe.\n\n        >>> df = pd.DataFrame({'A': [1, 2, 3],\n        ...                    'B': [400., 500., 600.]})\n        >>> new_df = pd.DataFrame({'B': [4, np.nan, 6]})\n        >>> df.update(new_df)\n        >>> df\n           A      B\n        0  1    4.0\n        1  2  500.0\n        2  3    6.0\n        \"\"\"\n\n        if not PYPY and using_copy_on_write():\n            if sys.getrefcount(self) <= REF_COUNT:\n                warnings.warn(\n                    _chained_assignment_method_msg,\n                    ChainedAssignmentError,\n                    stacklevel=2,\n                )\n        elif not PYPY and not using_copy_on_write() and self._is_view_after_cow_rules():\n            if sys.getrefcount(self) <= REF_COUNT:\n                warnings.warn(\n                    _chained_assignment_warning_method_msg,\n                    FutureWarning,\n                    stacklevel=2,\n                )\n\n        # TODO: Support other joins\n        if join != \"left\":  # pragma: no cover\n            raise NotImplementedError(\"Only left join is supported\")\n        if errors not in [\"ignore\", \"raise\"]:\n            raise ValueError(\"The parameter errors must be either 'ignore' or 'raise'\")\n\n        if not isinstance(other, DataFrame):\n            other = DataFrame(other)\n\n        other = other.reindex(self.index)\n\n        for col in self.columns.intersection(other.columns):\n            this = self[col]._values\n            that = other[col]._values\n\n            if filter_func is not None:\n                mask = ~filter_func(this) | isna(that)\n            else:\n                if errors == \"raise\":\n                    mask_this = notna(that)\n                    mask_that = notna(this)\n                    if any(mask_this & mask_that):\n                        raise ValueError(\"Data overlaps.\")\n\n                if overwrite:\n                    mask = isna(that)\n                else:\n                    mask = notna(this)\n\n            # don't overwrite columns unnecessarily\n            if mask.all():\n                continue\n\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\",\n                    message=\"Downcasting behavior\",\n                    category=FutureWarning,\n                )\n                # GH#57124 - `that` might get upcasted because of NA values, and then\n                # downcasted in where because of the mask. Ignoring the warning\n                # is a stopgap, will replace with a new implementation of update\n                # in 3.0.\n                self.loc[:, col] = self[col].where(mask, that)\n\n    # ----------------------------------------------------------------------\n    # Data reshaping\n    @Appender(\n        dedent(\n            \"\"\"\n        Examples\n        --------\n        >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n        ...                               'Parrot', 'Parrot'],\n        ...                    'Max Speed': [380., 370., 24., 26.]})\n        >>> df\n           Animal  Max Speed\n        0  Falcon      380.0\n        1  Falcon      370.0\n        2  Parrot       24.0\n        3  Parrot       26.0\n        >>> df.groupby(['Animal']).mean()\n                Max Speed\n        Animal\n        Falcon      375.0\n        Parrot       25.0\n\n        **Hierarchical Indexes**\n\n        We can groupby different levels of a hierarchical index\n        using the `level` parameter:\n\n        >>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n        ...           ['Captive', 'Wild', 'Captive', 'Wild']]\n        >>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n        >>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]},\n        ...                   index=index)\n        >>> df\n                        Max Speed\n        Animal Type\n        Falcon Captive      390.0\n               Wild         350.0\n        Parrot Captive       30.0\n               Wild          20.0\n        >>> df.groupby(level=0).mean()\n                Max Speed\n        Animal\n        Falcon      370.0\n        Parrot       25.0\n        >>> df.groupby(level=\"Type\").mean()\n                 Max Speed\n        Type\n        Captive      210.0\n        Wild         185.0\n\n        We can also choose to include NA in group keys or not by setting\n        `dropna` parameter, the default setting is `True`.\n\n        >>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n        >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n        >>> df.groupby(by=[\"b\"]).sum()\n            a   c\n        b\n        1.0 2   3\n        2.0 2   5\n\n        >>> df.groupby(by=[\"b\"], dropna=False).sum()\n            a   c\n        b\n        1.0 2   3\n        2.0 2   5\n        NaN 1   4\n\n        >>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n        >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n        >>> df.groupby(by=\"a\").sum()\n            b     c\n        a\n        a   13.0   13.0\n        b   12.3  123.0\n\n        >>> df.groupby(by=\"a\", dropna=False).sum()\n            b     c\n        a\n        a   13.0   13.0\n        b   12.3  123.0\n        NaN 12.3   33.0\n\n        When using ``.apply()``, use ``group_keys`` to include or exclude the\n        group keys. The ``group_keys`` argument defaults to ``True`` (include).\n\n        >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n        ...                               'Parrot', 'Parrot'],\n        ...                    'Max Speed': [380., 370., 24., 26.]})\n        >>> df.groupby(\"Animal\", group_keys=True)[['Max Speed']].apply(lambda x: x)\n                  Max Speed\n        Animal\n        Falcon 0      380.0\n               1      370.0\n        Parrot 2       24.0\n               3       26.0\n\n        >>> df.groupby(\"Animal\", group_keys=False)[['Max Speed']].apply(lambda x: x)\n           Max Speed\n        0      380.0\n        1      370.0\n        2       24.0\n        3       26.0\n        \"\"\"\n        )\n    )\n    @Appender(_shared_docs[\"groupby\"] % _shared_doc_kwargs)\n    def groupby(\n        self,\n        by=None,\n        axis: Axis | lib.NoDefault = lib.no_default,\n        level: IndexLabel | None = None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        observed: bool | lib.NoDefault = lib.no_default,\n        dropna: bool = True,\n    ) -> DataFrameGroupBy:\n        if axis is not lib.no_default:\n            axis = self._get_axis_number(axis)\n            if axis == 1:\n                warnings.warn(\n                    \"DataFrame.groupby with axis=1 is deprecated. Do \"\n                    \"`frame.T.groupby(...)` without axis instead.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n            else:\n                warnings.warn(\n                    \"The 'axis' keyword in DataFrame.groupby is deprecated and \"\n                    \"will be removed in a future version.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n        else:\n            axis = 0\n\n        from pandas.core.groupby.generic import DataFrameGroupBy\n\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n\n        return DataFrameGroupBy(\n            obj=self,\n            keys=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            observed=observed,\n            dropna=dropna,\n        )\n\n    _shared_docs[\n        \"pivot\"\n    ] = \"\"\"\n        Return reshaped DataFrame organized by given index / column values.\n\n        Reshape data (produce a \"pivot\" table) based on column values. Uses\n        unique values from specified `index` / `columns` to form axes of the\n        resulting DataFrame. This function does not support data\n        aggregation, multiple values will result in a MultiIndex in the\n        columns. See the :ref:`User Guide <reshaping>` for more on reshaping.\n\n        Parameters\n        ----------%s\n        columns : str or object or a list of str\n            Column to use to make new frame's columns.\n        index : str or object or a list of str, optional\n            Column to use to make new frame's index. If not given, uses existing index.\n        values : str, object or a list of the previous, optional\n            Column(s) to use for populating new frame's values. If not\n            specified, all remaining columns will be used and the result will\n            have hierarchically indexed columns.\n\n        Returns\n        -------\n        DataFrame\n            Returns reshaped DataFrame.\n\n        Raises\n        ------\n        ValueError:\n            When there are any `index`, `columns` combinations with multiple\n            values. `DataFrame.pivot_table` when you need to aggregate.\n\n        See Also\n        --------\n        DataFrame.pivot_table : Generalization of pivot that can handle\n            duplicate values for one index/column pair.\n        DataFrame.unstack : Pivot based on the index values instead of a\n            column.\n        wide_to_long : Wide panel to long format. Less flexible but more\n            user-friendly than melt.\n\n        Notes\n        -----\n        For finer-tuned control, see hierarchical indexing documentation along\n        with the related stack/unstack methods.\n\n        Reference :ref:`the user guide <reshaping.pivot>` for more examples.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n        ...                            'two'],\n        ...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n        ...                    'baz': [1, 2, 3, 4, 5, 6],\n        ...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n        >>> df\n            foo   bar  baz  zoo\n        0   one   A    1    x\n        1   one   B    2    y\n        2   one   C    3    z\n        3   two   A    4    q\n        4   two   B    5    w\n        5   two   C    6    t\n\n        >>> df.pivot(index='foo', columns='bar', values='baz')\n        bar  A   B   C\n        foo\n        one  1   2   3\n        two  4   5   6\n\n        >>> df.pivot(index='foo', columns='bar')['baz']\n        bar  A   B   C\n        foo\n        one  1   2   3\n        two  4   5   6\n\n        >>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n              baz       zoo\n        bar   A  B  C   A  B  C\n        foo\n        one   1  2  3   x  y  z\n        two   4  5  6   q  w  t\n\n        You could also assign a list of column names or a list of index names.\n\n        >>> df = pd.DataFrame({\n        ...        \"lev1\": [1, 1, 1, 2, 2, 2],\n        ...        \"lev2\": [1, 1, 2, 1, 1, 2],\n        ...        \"lev3\": [1, 2, 1, 2, 1, 2],\n        ...        \"lev4\": [1, 2, 3, 4, 5, 6],\n        ...        \"values\": [0, 1, 2, 3, 4, 5]})\n        >>> df\n            lev1 lev2 lev3 lev4 values\n        0   1    1    1    1    0\n        1   1    1    2    2    1\n        2   1    2    1    3    2\n        3   2    1    2    4    3\n        4   2    1    1    5    4\n        5   2    2    2    6    5\n\n        >>> df.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"], values=\"values\")\n        lev2    1         2\n        lev3    1    2    1    2\n        lev1\n        1     0.0  1.0  2.0  NaN\n        2     4.0  3.0  NaN  5.0\n\n        >>> df.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"], values=\"values\")\n              lev3    1    2\n        lev1  lev2\n           1     1  0.0  1.0\n                 2  2.0  NaN\n           2     1  4.0  3.0\n                 2  NaN  5.0\n\n        A ValueError is raised if there are any duplicates.\n\n        >>> df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'],\n        ...                    \"bar\": ['A', 'A', 'B', 'C'],\n        ...                    \"baz\": [1, 2, 3, 4]})\n        >>> df\n           foo bar  baz\n        0  one   A    1\n        1  one   A    2\n        2  two   B    3\n        3  two   C    4\n\n        Notice that the first two rows are the same for our `index`\n        and `columns` arguments.\n\n        >>> df.pivot(index='foo', columns='bar', values='baz')\n        Traceback (most recent call last):\n           ...\n        ValueError: Index contains duplicate entries, cannot reshape\n        \"\"\"\n\n    @Substitution(\"\")\n    @Appender(_shared_docs[\"pivot\"])\n    def pivot(\n        self, *, columns, index=lib.no_default, values=lib.no_default\n    ) -> DataFrame:\n        from pandas.core.reshape.pivot import pivot\n\n        return pivot(self, index=index, columns=columns, values=values)\n\n    _shared_docs[\n        \"pivot_table\"\n    ] = \"\"\"\n        Create a spreadsheet-style pivot table as a DataFrame.\n\n        The levels in the pivot table will be stored in MultiIndex objects\n        (hierarchical indexes) on the index and columns of the result DataFrame.\n\n        Parameters\n        ----------%s\n        values : list-like or scalar, optional\n            Column or columns to aggregate.\n        index : column, Grouper, array, or list of the previous\n            Keys to group by on the pivot table index. If a list is passed,\n            it can contain any of the other types (except list). If an array is\n            passed, it must be the same length as the data and will be used in\n            the same manner as column values.\n        columns : column, Grouper, array, or list of the previous\n            Keys to group by on the pivot table column. If a list is passed,\n            it can contain any of the other types (except list). If an array is\n            passed, it must be the same length as the data and will be used in\n            the same manner as column values.\n        aggfunc : function, list of functions, dict, default \"mean\"\n            If a list of functions is passed, the resulting pivot table will have\n            hierarchical columns whose top level are the function names\n            (inferred from the function objects themselves).\n            If a dict is passed, the key is column to aggregate and the value is\n            function or list of functions. If ``margin=True``, aggfunc will be\n            used to calculate the partial aggregates.\n        fill_value : scalar, default None\n            Value to replace missing values with (in the resulting pivot table,\n            after aggregation).\n        margins : bool, default False\n            If ``margins=True``, special ``All`` columns and rows\n            will be added with partial group aggregates across the categories\n            on the rows and columns.\n        dropna : bool, default True\n            Do not include columns whose entries are all NaN. If True,\n            rows with a NaN value in any column will be omitted before\n            computing margins.\n        margins_name : str, default 'All'\n            Name of the row / column that will contain the totals\n            when margins is True.\n        observed : bool, default False\n            This only applies if any of the groupers are Categoricals.\n            If True: only show observed values for categorical groupers.\n            If False: show all values for categorical groupers.\n\n            .. deprecated:: 2.2.0\n\n                The default value of ``False`` is deprecated and will change to\n                ``True`` in a future version of pandas.\n\n        sort : bool, default True\n            Specifies if the result should be sorted.\n\n            .. versionadded:: 1.3.0\n\n        Returns\n        -------\n        DataFrame\n            An Excel style pivot table.\n\n        See Also\n        --------\n        DataFrame.pivot : Pivot without aggregation that can handle\n            non-numeric data.\n        DataFrame.melt: Unpivot a DataFrame from wide to long format,\n            optionally leaving identifiers set.\n        wide_to_long : Wide panel to long format. Less flexible but more\n            user-friendly than melt.\n\n        Notes\n        -----\n        Reference :ref:`the user guide <reshaping.pivot>` for more examples.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\n        ...                          \"bar\", \"bar\", \"bar\", \"bar\"],\n        ...                    \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n        ...                          \"one\", \"one\", \"two\", \"two\"],\n        ...                    \"C\": [\"small\", \"large\", \"large\", \"small\",\n        ...                          \"small\", \"large\", \"small\", \"small\",\n        ...                          \"large\"],\n        ...                    \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n        ...                    \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\n        >>> df\n             A    B      C  D  E\n        0  foo  one  small  1  2\n        1  foo  one  large  2  4\n        2  foo  one  large  2  5\n        3  foo  two  small  3  5\n        4  foo  two  small  3  6\n        5  bar  one  large  4  6\n        6  bar  one  small  5  8\n        7  bar  two  small  6  9\n        8  bar  two  large  7  9\n\n        This first example aggregates values by taking the sum.\n\n        >>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n        ...                        columns=['C'], aggfunc=\"sum\")\n        >>> table\n        C        large  small\n        A   B\n        bar one    4.0    5.0\n            two    7.0    6.0\n        foo one    4.0    1.0\n            two    NaN    6.0\n\n        We can also fill missing values using the `fill_value` parameter.\n\n        >>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n        ...                        columns=['C'], aggfunc=\"sum\", fill_value=0)\n        >>> table\n        C        large  small\n        A   B\n        bar one      4      5\n            two      7      6\n        foo one      4      1\n            two      0      6\n\n        The next example aggregates by taking the mean across multiple columns.\n\n        >>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n        ...                        aggfunc={'D': \"mean\", 'E': \"mean\"})\n        >>> table\n                        D         E\n        A   C\n        bar large  5.500000  7.500000\n            small  5.500000  8.500000\n        foo large  2.000000  4.500000\n            small  2.333333  4.333333\n\n        We can also calculate multiple types of aggregations for any given\n        value column.\n\n        >>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n        ...                        aggfunc={'D': \"mean\",\n        ...                                 'E': [\"min\", \"max\", \"mean\"]})\n        >>> table\n                          D   E\n                       mean max      mean  min\n        A   C\n        bar large  5.500000   9  7.500000    6\n            small  5.500000   9  8.500000    8\n        foo large  2.000000   5  4.500000    4\n            small  2.333333   6  4.333333    2\n        \"\"\"\n\n    @Substitution(\"\")\n    @Appender(_shared_docs[\"pivot_table\"])\n    def pivot_table(\n        self,\n        values=None,\n        index=None,\n        columns=None,\n        aggfunc: AggFuncType = \"mean\",\n        fill_value=None,\n        margins: bool = False,\n        dropna: bool = True,\n        margins_name: Level = \"All\",\n        observed: bool | lib.NoDefault = lib.no_default,\n        sort: bool = True,\n    ) -> DataFrame:\n        from pandas.core.reshape.pivot import pivot_table\n\n        return pivot_table(\n            self,\n            values=values,\n            index=index,\n            columns=columns,\n            aggfunc=aggfunc,\n            fill_value=fill_value,\n            margins=margins,\n            dropna=dropna,\n            margins_name=margins_name,\n            observed=observed,\n            sort=sort,\n        )\n\n    def stack(\n        self,\n        level: IndexLabel = -1,\n        dropna: bool | lib.NoDefault = lib.no_default,\n        sort: bool | lib.NoDefault = lib.no_default,\n        future_stack: bool = False,\n    ):\n        \"\"\"\n        Stack the prescribed level(s) from columns to index.\n\n        Return a reshaped DataFrame or Series having a multi-level\n        index with one or more new inner-most levels compared to the current\n        DataFrame. The new inner-most levels are created by pivoting the\n        columns of the current dataframe:\n\n          - if the columns have a single level, the output is a Series;\n          - if the columns have multiple levels, the new index\n            level(s) is (are) taken from the prescribed level(s) and\n            the output is a DataFrame.\n\n        Parameters\n        ----------\n        level : int, str, list, default -1\n            Level(s) to stack from the column axis onto the index\n            axis, defined as one index or label, or a list of indices\n            or labels.\n        dropna : bool, default True\n            Whether to drop rows in the resulting Frame/Series with\n            missing values. Stacking a column level onto the index\n            axis can create combinations of index and column values\n            that are missing from the original dataframe. See Examples\n            section.\n        sort : bool, default True\n            Whether to sort the levels of the resulting MultiIndex.\n        future_stack : bool, default False\n            Whether to use the new implementation that will replace the current\n            implementation in pandas 3.0. When True, dropna and sort have no impact\n            on the result and must remain unspecified. See :ref:`pandas 2.1.0 Release\n            notes <whatsnew_210.enhancements.new_stack>` for more details.\n\n        Returns\n        -------\n        DataFrame or Series\n            Stacked dataframe or series.\n\n        See Also\n        --------\n        DataFrame.unstack : Unstack prescribed level(s) from index axis\n             onto column axis.\n        DataFrame.pivot : Reshape dataframe from long format to wide\n             format.\n        DataFrame.pivot_table : Create a spreadsheet-style pivot table\n             as a DataFrame.\n\n        Notes\n        -----\n        The function is named by analogy with a collection of books\n        being reorganized from being side by side on a horizontal\n        position (the columns of the dataframe) to being stacked\n        vertically on top of each other (in the index of the\n        dataframe).\n\n        Reference :ref:`the user guide <reshaping.stacking>` for more examples.\n\n        Examples\n        --------\n        **Single level columns**\n\n        >>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=['weight', 'height'])\n\n        Stacking a dataframe with a single level column axis returns a Series:\n\n        >>> df_single_level_cols\n             weight height\n        cat       0      1\n        dog       2      3\n        >>> df_single_level_cols.stack(future_stack=True)\n        cat  weight    0\n             height    1\n        dog  weight    2\n             height    3\n        dtype: int64\n\n        **Multi level columns: simple case**\n\n        >>> multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n        ...                                        ('weight', 'pounds')])\n        >>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol1)\n\n        Stacking a dataframe with a multi-level column axis:\n\n        >>> df_multi_level_cols1\n             weight\n                 kg    pounds\n        cat       1        2\n        dog       2        4\n        >>> df_multi_level_cols1.stack(future_stack=True)\n                    weight\n        cat kg           1\n            pounds       2\n        dog kg           2\n            pounds       4\n\n        **Missing values**\n\n        >>> multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n        ...                                        ('height', 'm')])\n        >>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol2)\n\n        It is common to have missing values when stacking a dataframe\n        with multi-level columns, as the stacked dataframe typically\n        has more values than the original dataframe. Missing values\n        are filled with NaNs:\n\n        >>> df_multi_level_cols2\n            weight height\n                kg      m\n        cat    1.0    2.0\n        dog    3.0    4.0\n        >>> df_multi_level_cols2.stack(future_stack=True)\n                weight  height\n        cat kg     1.0     NaN\n            m      NaN     2.0\n        dog kg     3.0     NaN\n            m      NaN     4.0\n\n        **Prescribing the level(s) to be stacked**\n\n        The first parameter controls which level or levels are stacked:\n\n        >>> df_multi_level_cols2.stack(0, future_stack=True)\n                     kg    m\n        cat weight  1.0  NaN\n            height  NaN  2.0\n        dog weight  3.0  NaN\n            height  NaN  4.0\n        >>> df_multi_level_cols2.stack([0, 1], future_stack=True)\n        cat  weight  kg    1.0\n             height  m     2.0\n        dog  weight  kg    3.0\n             height  m     4.0\n        dtype: float64\n        \"\"\"\n        if not future_stack:\n            from pandas.core.reshape.reshape import (\n                stack,\n                stack_multiple,\n            )\n\n            if (\n                dropna is not lib.no_default\n                or sort is not lib.no_default\n                or self.columns.nlevels > 1\n            ):\n                warnings.warn(\n                    \"The previous implementation of stack is deprecated and will be \"\n                    \"removed in a future version of pandas. See the What's New notes \"\n                    \"for pandas 2.1.0 for details. Specify future_stack=True to adopt \"\n                    \"the new implementation and silence this warning.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n\n            if dropna is lib.no_default:\n                dropna = True\n            if sort is lib.no_default:\n                sort = True\n\n            if isinstance(level, (tuple, list)):\n                result = stack_multiple(self, level, dropna=dropna, sort=sort)\n            else:\n                result = stack(self, level, dropna=dropna, sort=sort)\n        else:\n            from pandas.core.reshape.reshape import stack_v3\n\n            if dropna is not lib.no_default:\n                raise ValueError(\n                    \"dropna must be unspecified with future_stack=True as the new \"\n                    \"implementation does not introduce rows of NA values. This \"\n                    \"argument will be removed in a future version of pandas.\"\n                )\n\n            if sort is not lib.no_default:\n                raise ValueError(\n                    \"Cannot specify sort with future_stack=True, this argument will be \"\n                    \"removed in a future version of pandas. Sort the result using \"\n                    \".sort_index instead.\"\n                )\n\n            if (\n                isinstance(level, (tuple, list))\n                and not all(lev in self.columns.names for lev in level)\n                and not all(isinstance(lev, int) for lev in level)\n            ):\n                raise ValueError(\n                    \"level should contain all level names or all level \"\n                    \"numbers, not a mixture of the two.\"\n                )\n\n            if not isinstance(level, (tuple, list)):\n                level = [level]\n            level = [self.columns._get_level_number(lev) for lev in level]\n            result = stack_v3(self, level)\n\n        return result.__finalize__(self, method=\"stack\")\n\n    def explode(\n        self,\n        column: IndexLabel,\n        ignore_index: bool = False,\n    ) -> DataFrame:\n        \"\"\"\n        Transform each element of a list-like to a row, replicating index values.\n\n        Parameters\n        ----------\n        column : IndexLabel\n            Column(s) to explode.\n            For multiple columns, specify a non-empty list with each element\n            be str or tuple, and all specified columns their list-like data\n            on same row of the frame must have matching length.\n\n            .. versionadded:: 1.3.0\n                Multi-column explode\n\n        ignore_index : bool, default False\n            If True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\n        Returns\n        -------\n        DataFrame\n            Exploded lists to rows of the subset columns;\n            index will be duplicated for these rows.\n\n        Raises\n        ------\n        ValueError :\n            * If columns of the frame are not unique.\n            * If specified columns to explode is empty list.\n            * If specified columns to explode have not matching count of\n              elements rowwise in the frame.\n\n        See Also\n        --------\n        DataFrame.unstack : Pivot a level of the (necessarily hierarchical)\n            index labels.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        Series.explode : Explode a DataFrame from list-like columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples, sets,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged, and empty list-likes will\n        result in a np.nan for that row. In addition, the ordering of rows in the\n        output will be non-deterministic when exploding sets.\n\n        Reference :ref:`the user guide <reshaping.explode>` for more examples.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [[0, 1, 2], 'foo', [], [3, 4]],\n        ...                    'B': 1,\n        ...                    'C': [['a', 'b', 'c'], np.nan, [], ['d', 'e']]})\n        >>> df\n                   A  B          C\n        0  [0, 1, 2]  1  [a, b, c]\n        1        foo  1        NaN\n        2         []  1         []\n        3     [3, 4]  1     [d, e]\n\n        Single-column explode.\n\n        >>> df.explode('A')\n             A  B          C\n        0    0  1  [a, b, c]\n        0    1  1  [a, b, c]\n        0    2  1  [a, b, c]\n        1  foo  1        NaN\n        2  NaN  1         []\n        3    3  1     [d, e]\n        3    4  1     [d, e]\n\n        Multi-column explode.\n\n        >>> df.explode(list('AC'))\n             A  B    C\n        0    0  1    a\n        0    1  1    b\n        0    2  1    c\n        1  foo  1  NaN\n        2  NaN  1  NaN\n        3    3  1    d\n        3    4  1    e\n        \"\"\"\n        if not self.columns.is_unique:\n            duplicate_cols = self.columns[self.columns.duplicated()].tolist()\n            raise ValueError(\n                f\"DataFrame columns must be unique. Duplicate columns: {duplicate_cols}\"\n            )\n\n        columns: list[Hashable]\n        if is_scalar(column) or isinstance(column, tuple):\n            columns = [column]\n        elif isinstance(column, list) and all(\n            is_scalar(c) or isinstance(c, tuple) for c in column\n        ):\n            if not column:\n                raise ValueError(\"column must be nonempty\")\n            if len(column) > len(set(column)):\n                raise ValueError(\"column must be unique\")\n            columns = column\n        else:\n            raise ValueError(\"column must be a scalar, tuple, or list thereof\")\n\n        df = self.reset_index(drop=True)\n        if len(columns) == 1:\n            result = df[columns[0]].explode()\n        else:\n            mylen = lambda x: len(x) if (is_list_like(x) and len(x) > 0) else 1\n            counts0 = self[columns[0]].apply(mylen)\n            for c in columns[1:]:\n                if not all(counts0 == self[c].apply(mylen)):\n                    raise ValueError(\"columns must have matching element counts\")\n            result = DataFrame({c: df[c].explode() for c in columns})\n        result = df.drop(columns, axis=1).join(result)\n        if ignore_index:\n            result.index = default_index(len(result))\n        else:\n            result.index = self.index.take(result.index)\n        result = result.reindex(columns=self.columns, copy=False)\n\n        return result.__finalize__(self, method=\"explode\")\n\n    def unstack(self, level: IndexLabel = -1, fill_value=None, sort: bool = True):\n        \"\"\"\n        Pivot a level of the (necessarily hierarchical) index labels.\n\n        Returns a DataFrame having a new level of column labels whose inner-most level\n        consists of the pivoted index labels.\n\n        If the index is not a MultiIndex, the output will be a Series\n        (the analogue of stack when the columns are not a MultiIndex).\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default -1 (last level)\n            Level(s) of index to unstack, can pass level name.\n        fill_value : int, str or dict\n            Replace NaN with this value if the unstack produces missing values.\n        sort : bool, default True\n            Sort the level(s) in the resulting MultiIndex columns.\n\n        Returns\n        -------\n        Series or DataFrame\n\n        See Also\n        --------\n        DataFrame.pivot : Pivot a table based on column values.\n        DataFrame.stack : Pivot a level of the column labels (inverse operation\n            from `unstack`).\n\n        Notes\n        -----\n        Reference :ref:`the user guide <reshaping.stacking>` for more examples.\n\n        Examples\n        --------\n        >>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\n        ...                                    ('two', 'a'), ('two', 'b')])\n        >>> s = pd.Series(np.arange(1.0, 5.0), index=index)\n        >>> s\n        one  a   1.0\n             b   2.0\n        two  a   3.0\n             b   4.0\n        dtype: float64\n\n        >>> s.unstack(level=-1)\n             a   b\n        one  1.0  2.0\n        two  3.0  4.0\n\n        >>> s.unstack(level=0)\n           one  two\n        a  1.0   3.0\n        b  2.0   4.0\n\n        >>> df = s.unstack(level=0)\n        >>> df.unstack()\n        one  a  1.0\n             b  2.0\n        two  a  3.0\n             b  4.0\n        dtype: float64\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n\n        result = unstack(self, level, fill_value, sort)\n\n        return result.__finalize__(self, method=\"unstack\")\n\n    @Appender(_shared_docs[\"melt\"] % {\"caller\": \"df.melt(\", \"other\": \"melt\"})\n    def melt(\n        self,\n        id_vars=None,\n        value_vars=None,\n        var_name=None,\n        value_name: Hashable = \"value\",\n        col_level: Level | None = None,\n        ignore_index: bool = True,\n    ) -> DataFrame:\n        return melt(\n            self,\n            id_vars=id_vars,\n            value_vars=value_vars,\n            var_name=var_name,\n            value_name=value_name,\n            col_level=col_level,\n            ignore_index=ignore_index,\n        ).__finalize__(self, method=\"melt\")\n\n    # ----------------------------------------------------------------------\n    # Time series-related\n\n    @doc(\n        Series.diff,\n        klass=\"DataFrame\",\n        extra_params=\"axis : {0 or 'index', 1 or 'columns'}, default 0\\n    \"\n        \"Take difference over rows (0) or columns (1).\\n\",\n        other_klass=\"Series\",\n        examples=dedent(\n            \"\"\"\n        Difference with previous row\n\n        >>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6],\n        ...                    'b': [1, 1, 2, 3, 5, 8],\n        ...                    'c': [1, 4, 9, 16, 25, 36]})\n        >>> df\n           a  b   c\n        0  1  1   1\n        1  2  1   4\n        2  3  2   9\n        3  4  3  16\n        4  5  5  25\n        5  6  8  36\n\n        >>> df.diff()\n             a    b     c\n        0  NaN  NaN   NaN\n        1  1.0  0.0   3.0\n        2  1.0  1.0   5.0\n        3  1.0  1.0   7.0\n        4  1.0  2.0   9.0\n        5  1.0  3.0  11.0\n\n        Difference with previous column\n\n        >>> df.diff(axis=1)\n            a  b   c\n        0 NaN  0   0\n        1 NaN -1   3\n        2 NaN -1   7\n        3 NaN -1  13\n        4 NaN  0  20\n        5 NaN  2  28\n\n        Difference with 3rd previous row\n\n        >>> df.diff(periods=3)\n             a    b     c\n        0  NaN  NaN   NaN\n        1  NaN  NaN   NaN\n        2  NaN  NaN   NaN\n        3  3.0  2.0  15.0\n        4  3.0  4.0  21.0\n        5  3.0  6.0  27.0\n\n        Difference with following row\n\n        >>> df.diff(periods=-1)\n             a    b     c\n        0 -1.0  0.0  -3.0\n        1 -1.0 -1.0  -5.0\n        2 -1.0 -1.0  -7.0\n        3 -1.0 -2.0  -9.0\n        4 -1.0 -3.0 -11.0\n        5  NaN  NaN   NaN\n\n        Overflow in input dtype\n\n        >>> df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8)\n        >>> df.diff()\n               a\n        0    NaN\n        1  255.0\"\"\"\n        ),\n    )\n    def diff(self, periods: int = 1, axis: Axis = 0) -> DataFrame:\n        if not lib.is_integer(periods):\n            if not (is_float(periods) and periods.is_integer()):\n                raise ValueError(\"periods must be an integer\")\n            periods = int(periods)\n\n        axis = self._get_axis_number(axis)\n        if axis == 1:\n            if periods != 0:\n                # in the periods == 0 case, this is equivalent diff of 0 periods\n                #  along axis=0, and the Manager method may be somewhat more\n                #  performant, so we dispatch in that case.\n                return self - self.shift(periods, axis=axis)\n            # With periods=0 this is equivalent to a diff with axis=0\n            axis = 0\n\n        new_data = self._mgr.diff(n=periods)\n        res_df = self._constructor_from_mgr(new_data, axes=new_data.axes)\n        return res_df.__finalize__(self, \"diff\")\n\n    # ----------------------------------------------------------------------\n    # Function application\n\n    def _gotitem(\n        self,\n        key: IndexLabel,\n        ndim: int,\n        subset: DataFrame | Series | None = None,\n    ) -> DataFrame | Series:\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        if subset is None:\n            subset = self\n        elif subset.ndim == 1:  # is Series\n            return subset\n\n        # TODO: _shallow_copy(subset)?\n        return subset[key]\n\n    _agg_see_also_doc = dedent(\n        \"\"\"\n    See Also\n    --------\n    DataFrame.apply : Perform any type of operations.\n    DataFrame.transform : Perform transformation type operations.\n    pandas.DataFrame.groupby : Perform operations over groups.\n    pandas.DataFrame.resample : Perform operations over resampled bins.\n    pandas.DataFrame.rolling : Perform operations over rolling window.\n    pandas.DataFrame.expanding : Perform operations over expanding window.\n    pandas.core.window.ewm.ExponentialMovingWindow : Perform operation over exponential\n        weighted window.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> df = pd.DataFrame([[1, 2, 3],\n    ...                    [4, 5, 6],\n    ...                    [7, 8, 9],\n    ...                    [np.nan, np.nan, np.nan]],\n    ...                   columns=['A', 'B', 'C'])\n\n    Aggregate these functions over the rows.\n\n    >>> df.agg(['sum', 'min'])\n            A     B     C\n    sum  12.0  15.0  18.0\n    min   1.0   2.0   3.0\n\n    Different aggregations per column.\n\n    >>> df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n            A    B\n    sum  12.0  NaN\n    min   1.0  2.0\n    max   NaN  8.0\n\n    Aggregate different functions over the columns and rename the index of the resulting\n    DataFrame.\n\n    >>> df.agg(x=('A', 'max'), y=('B', 'min'), z=('C', 'mean'))\n         A    B    C\n    x  7.0  NaN  NaN\n    y  NaN  2.0  NaN\n    z  NaN  NaN  6.0\n\n    Aggregate over the columns.\n\n    >>> df.agg(\"mean\", axis=\"columns\")\n    0    2.0\n    1    5.0\n    2    8.0\n    3    NaN\n    dtype: float64\n    \"\"\"\n    )\n\n    @doc(\n        _shared_docs[\"aggregate\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n        see_also=_agg_see_also_doc,\n        examples=_agg_examples_doc,\n    )\n    def aggregate(self, func=None, axis: Axis = 0, *args, **kwargs):\n        from pandas.core.apply import frame_apply\n\n        axis = self._get_axis_number(axis)\n\n        op = frame_apply(self, func=func, axis=axis, args=args, kwargs=kwargs)\n        result = op.agg()\n        result = reconstruct_and_relabel_result(result, func, **kwargs)\n        return result\n\n    agg = aggregate\n\n    @doc(\n        _shared_docs[\"transform\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n    )\n    def transform(\n        self, func: AggFuncType, axis: Axis = 0, *args, **kwargs\n    ) -> DataFrame:\n        from pandas.core.apply import frame_apply\n\n        op = frame_apply(self, func=func, axis=axis, args=args, kwargs=kwargs)\n        result = op.transform()\n        assert isinstance(result, DataFrame)\n        return result\n\n    def apply(\n        self,\n        func: AggFuncType,\n        axis: Axis = 0,\n        raw: bool = False,\n        result_type: Literal[\"expand\", \"reduce\", \"broadcast\"] | None = None,\n        args=(),\n        by_row: Literal[False, \"compat\"] = \"compat\",\n        engine: Literal[\"python\", \"numba\"] = \"python\",\n        engine_kwargs: dict[str, bool] | None = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Apply a function along an axis of the DataFrame.\n\n        Objects passed to the function are Series objects whose index is\n        either the DataFrame's index (``axis=0``) or the DataFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to apply to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': apply function to each column.\n            * 1 or 'columns': apply function to each row.\n\n        raw : bool, default False\n            Determines if row or column is passed as a Series or ndarray object:\n\n            * ``False`` : passes each row or column as a Series to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just applying a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Series if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the DataFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Series\n            of those. However if the apply function returns a Series these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/series.\n        by_row : False or \"compat\", default \"compat\"\n            Only has an effect when ``func`` is a listlike or dictlike of funcs\n            and the func isn't a string.\n            If \"compat\", will if possible first translate the func into pandas\n            methods (e.g. ``Series().apply(np.sum)`` will be translated to\n            ``Series().sum()``). If that doesn't work, will try call to apply again with\n            ``by_row=True`` and if that fails, will call apply again with\n            ``by_row=False`` (backward compatible).\n            If False, the funcs will be passed the whole Series at once.\n\n            .. versionadded:: 2.1.0\n\n        engine : {'python', 'numba'}, default 'python'\n            Choose between the python (default) engine or the numba engine in apply.\n\n            The numba engine will attempt to JIT compile the passed function,\n            which may result in speedups for large DataFrames.\n            It also supports the following engine_kwargs :\n\n            - nopython (compile the function in nopython mode)\n            - nogil (release the GIL inside the JIT compiled function)\n            - parallel (try to apply the function in parallel over the DataFrame)\n\n              Note: Due to limitations within numba/how pandas interfaces with numba,\n              you should only use this if raw=True\n\n            Note: The numba compiler only supports a subset of\n            valid Python/numpy operations.\n\n            Please read more about the `supported python features\n            <https://numba.pydata.org/numba-doc/dev/reference/pysupported.html>`_\n            and `supported numpy features\n            <https://numba.pydata.org/numba-doc/dev/reference/numpysupported.html>`_\n            in numba to learn what you can or cannot use in the passed function.\n\n            .. versionadded:: 2.2.0\n\n        engine_kwargs : dict\n            Pass keyword arguments to the engine.\n            This is currently only used by the numba engine,\n            see the documentation for the engine argument for more information.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Series or DataFrame\n            Result of applying ``func`` along the given axis of the\n            DataFrame.\n\n        See Also\n        --------\n        DataFrame.map: For elementwise operations.\n        DataFrame.aggregate: Only perform aggregating type operations.\n        DataFrame.transform: Only perform transforming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n        for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> df\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(df)``):\n\n        >>> df.apply(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> df.apply(np.sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> df.apply(np.sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Series\n\n        >>> df.apply(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Series inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Series index.\n\n        >>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        \"\"\"\n        from pandas.core.apply import frame_apply\n\n        op = frame_apply(\n            self,\n            func=func,\n            axis=axis,\n            raw=raw,\n            result_type=result_type,\n            by_row=by_row,\n            engine=engine,\n            engine_kwargs=engine_kwargs,\n            args=args,\n            kwargs=kwargs,\n        )\n        return op.apply().__finalize__(self, method=\"apply\")\n\n    def map(\n        self, func: PythonFuncType, na_action: str | None = None, **kwargs\n    ) -> DataFrame:\n        \"\"\"\n        Apply a function to a Dataframe elementwise.\n\n        .. versionadded:: 2.1.0\n\n           DataFrame.applymap was deprecated and renamed to DataFrame.map.\n\n        This method applies a function that accepts and returns a scalar\n        to every element of a DataFrame.\n\n        Parameters\n        ----------\n        func : callable\n            Python function, returns a single value from a single value.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to func.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        DataFrame\n            Transformed DataFrame.\n\n        See Also\n        --------\n        DataFrame.apply : Apply a function along input axis of DataFrame.\n        DataFrame.replace: Replace values given in `to_replace` with `value`.\n        Series.map : Apply a function elementwise on a Series.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n        >>> df\n               0      1\n        0  1.000  2.120\n        1  3.356  4.567\n\n        >>> df.map(lambda x: len(str(x)))\n           0  1\n        0  3  4\n        1  5  5\n\n        Like Series.map, NA values can be ignored:\n\n        >>> df_copy = df.copy()\n        >>> df_copy.iloc[0, 0] = pd.NA\n        >>> df_copy.map(lambda x: len(str(x)), na_action='ignore')\n             0  1\n        0  NaN  4\n        1  5.0  5\n\n        It is also possible to use `map` with functions that are not\n        `lambda` functions:\n\n        >>> df.map(round, ndigits=1)\n             0    1\n        0  1.0  2.1\n        1  3.4  4.6\n\n        Note that a vectorized version of `func` often exists, which will\n        be much faster. You could square each number elementwise.\n\n        >>> df.map(lambda x: x**2)\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n\n        But it's better to avoid map in that case.\n\n        >>> df ** 2\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n        \"\"\"\n        if na_action not in {\"ignore\", None}:\n            raise ValueError(\n                f\"na_action must be 'ignore' or None. Got {repr(na_action)}\"\n            )\n\n        if self.empty:\n            return self.copy()\n\n        func = functools.partial(func, **kwargs)\n\n        def infer(x):\n            return x._map_values(func, na_action=na_action)\n\n        return self.apply(infer).__finalize__(self, \"map\")\n\n    def applymap(\n        self, func: PythonFuncType, na_action: NaAction | None = None, **kwargs\n    ) -> DataFrame:\n        \"\"\"\n        Apply a function to a Dataframe elementwise.\n\n        .. deprecated:: 2.1.0\n\n           DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n        This method applies a function that accepts and returns a scalar\n        to every element of a DataFrame.\n\n        Parameters\n        ----------\n        func : callable\n            Python function, returns a single value from a single value.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to func.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        DataFrame\n            Transformed DataFrame.\n\n        See Also\n        --------\n        DataFrame.apply : Apply a function along input axis of DataFrame.\n        DataFrame.map : Apply a function along input axis of DataFrame.\n        DataFrame.replace: Replace values given in `to_replace` with `value`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n        >>> df\n               0      1\n        0  1.000  2.120\n        1  3.356  4.567\n\n        >>> df.map(lambda x: len(str(x)))\n           0  1\n        0  3  4\n        1  5  5\n        \"\"\"\n        warnings.warn(\n            \"DataFrame.applymap has been deprecated. Use DataFrame.map instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return self.map(func, na_action=na_action, **kwargs)\n\n    # ----------------------------------------------------------------------\n    # Merging / joining methods\n\n    def _append(\n        self,\n        other,\n        ignore_index: bool = False,\n        verify_integrity: bool = False,\n        sort: bool = False,\n    ) -> DataFrame:\n        if isinstance(other, (Series, dict)):\n            if isinstance(other, dict):\n                if not ignore_index:\n                    raise TypeError(\"Can only append a dict if ignore_index=True\")\n                other = Series(other)\n            if other.name is None and not ignore_index:\n                raise TypeError(\n                    \"Can only append a Series if ignore_index=True \"\n                    \"or if the Series has a name\"\n                )\n\n            index = Index(\n                [other.name],\n                name=self.index.names\n                if isinstance(self.index, MultiIndex)\n                else self.index.name,\n            )\n            row_df = other.to_frame().T\n            # infer_objects is needed for\n            #  test_append_empty_frame_to_series_with_dateutil_tz\n            other = row_df.infer_objects(copy=False).rename_axis(\n                index.names, copy=False\n            )\n        elif isinstance(other, list):\n            if not other:\n                pass\n            elif not isinstance(other[0], DataFrame):\n                other = DataFrame(other)\n                if self.index.name is not None and not ignore_index:\n                    other.index.name = self.index.name\n\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(other, (list, tuple)):\n            to_concat = [self, *other]\n        else:\n            to_concat = [self, other]\n\n        result = concat(\n            to_concat,\n            ignore_index=ignore_index,\n            verify_integrity=verify_integrity,\n            sort=sort,\n        )\n        return result.__finalize__(self, method=\"append\")\n\n    def join(\n        self,\n        other: DataFrame | Series | Iterable[DataFrame | Series],\n        on: IndexLabel | None = None,\n        how: MergeHow = \"left\",\n        lsuffix: str = \"\",\n        rsuffix: str = \"\",\n        sort: bool = False,\n        validate: JoinValidate | None = None,\n    ) -> DataFrame:\n        \"\"\"\n        Join columns of another DataFrame.\n\n        Join columns with `other` DataFrame either on index or on a key\n        column. Efficiently join multiple DataFrame objects by index at once by\n        passing a list.\n\n        Parameters\n        ----------\n        other : DataFrame, Series, or a list containing any combination of them\n            Index should be similar to one of the columns in this one. If a\n            Series is passed, its name attribute must be set, and that will be\n            used as the column name in the resulting joined DataFrame.\n        on : str, list of str, or array-like, optional\n            Column or index level name(s) in the caller to join on the index\n            in `other`, otherwise joins index-on-index. If multiple\n            values given, the `other` DataFrame must have a MultiIndex. Can\n            pass an array as the join key if it is not already contained in\n            the calling DataFrame. Like an Excel VLOOKUP operation.\n        how : {'left', 'right', 'outer', 'inner', 'cross'}, default 'left'\n            How to handle the operation of the two objects.\n\n            * left: use calling frame's index (or column if on is specified)\n            * right: use `other`'s index.\n            * outer: form union of calling frame's index (or column if on is\n              specified) with `other`'s index, and sort it lexicographically.\n            * inner: form intersection of calling frame's index (or column if\n              on is specified) with `other`'s index, preserving the order\n              of the calling's one.\n            * cross: creates the cartesian product from both frames, preserves the order\n              of the left keys.\n        lsuffix : str, default ''\n            Suffix to use from left frame's overlapping columns.\n        rsuffix : str, default ''\n            Suffix to use from right frame's overlapping columns.\n        sort : bool, default False\n            Order result DataFrame lexicographically by the join key. If False,\n            the order of the join key depends on the join type (how keyword).\n        validate : str, optional\n            If specified, checks if join is of specified type.\n\n            * \"one_to_one\" or \"1:1\": check if join keys are unique in both left\n              and right datasets.\n            * \"one_to_many\" or \"1:m\": check if join keys are unique in left dataset.\n            * \"many_to_one\" or \"m:1\": check if join keys are unique in right dataset.\n            * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n\n            .. versionadded:: 1.5.0\n\n        Returns\n        -------\n        DataFrame\n            A dataframe containing columns from both the caller and `other`.\n\n        See Also\n        --------\n        DataFrame.merge : For column(s)-on-column(s) operations.\n\n        Notes\n        -----\n        Parameters `on`, `lsuffix`, and `rsuffix` are not supported when\n        passing a list of `DataFrame` objects.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n        ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n\n        >>> df\n          key   A\n        0  K0  A0\n        1  K1  A1\n        2  K2  A2\n        3  K3  A3\n        4  K4  A4\n        5  K5  A5\n\n        >>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n        ...                       'B': ['B0', 'B1', 'B2']})\n\n        >>> other\n          key   B\n        0  K0  B0\n        1  K1  B1\n        2  K2  B2\n\n        Join DataFrames using their indexes.\n\n        >>> df.join(other, lsuffix='_caller', rsuffix='_other')\n          key_caller   A key_other    B\n        0         K0  A0        K0   B0\n        1         K1  A1        K1   B1\n        2         K2  A2        K2   B2\n        3         K3  A3       NaN  NaN\n        4         K4  A4       NaN  NaN\n        5         K5  A5       NaN  NaN\n\n        If we want to join using the key columns, we need to set key to be\n        the index in both `df` and `other`. The joined DataFrame will have\n        key as its index.\n\n        >>> df.set_index('key').join(other.set_index('key'))\n              A    B\n        key\n        K0   A0   B0\n        K1   A1   B1\n        K2   A2   B2\n        K3   A3  NaN\n        K4   A4  NaN\n        K5   A5  NaN\n\n        Another option to join using the key columns is to use the `on`\n        parameter. DataFrame.join always uses `other`'s index but we can use\n        any column in `df`. This method preserves the original DataFrame's\n        index in the result.\n\n        >>> df.join(other.set_index('key'), on='key')\n          key   A    B\n        0  K0  A0   B0\n        1  K1  A1   B1\n        2  K2  A2   B2\n        3  K3  A3  NaN\n        4  K4  A4  NaN\n        5  K5  A5  NaN\n\n        Using non-unique key values shows how they are matched.\n\n        >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K0', 'K1'],\n        ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n\n        >>> df\n          key   A\n        0  K0  A0\n        1  K1  A1\n        2  K1  A2\n        3  K3  A3\n        4  K0  A4\n        5  K1  A5\n\n        >>> df.join(other.set_index('key'), on='key', validate='m:1')\n          key   A    B\n        0  K0  A0   B0\n        1  K1  A1   B1\n        2  K1  A2   B1\n        3  K3  A3  NaN\n        4  K0  A4   B0\n        5  K1  A5   B1\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n        from pandas.core.reshape.merge import merge\n\n        if isinstance(other, Series):\n            if other.name is None:\n                raise ValueError(\"Other Series must have a name\")\n            other = DataFrame({other.name: other})\n\n        if isinstance(other, DataFrame):\n            if how == \"cross\":\n                return merge(\n                    self,\n                    other,\n                    how=how,\n                    on=on,\n                    suffixes=(lsuffix, rsuffix),\n                    sort=sort,\n                    validate=validate,\n                )\n            return merge(\n                self,\n                other,\n                left_on=on,\n                how=how,\n                left_index=on is None,\n                right_index=True,\n                suffixes=(lsuffix, rsuffix),\n                sort=sort,\n                validate=validate,\n            )\n        else:\n            if on is not None:\n                raise ValueError(\n                    \"Joining multiple DataFrames only supported for joining on index\"\n                )\n\n            if rsuffix or lsuffix:\n                raise ValueError(\n                    \"Suffixes not supported when joining multiple DataFrames\"\n                )\n\n            # Mypy thinks the RHS is a\n            # \"Union[DataFrame, Series, Iterable[Union[DataFrame, Series]]]\" whereas\n            # the LHS is an \"Iterable[DataFrame]\", but in reality both types are\n            # \"Iterable[Union[DataFrame, Series]]\" due to the if statements\n            frames = [cast(\"DataFrame | Series\", self)] + list(other)\n\n            can_concat = all(df.index.is_unique for df in frames)\n\n            # join indexes only using concat\n            if can_concat:\n                if how == \"left\":\n                    res = concat(\n                        frames, axis=1, join=\"outer\", verify_integrity=True, sort=sort\n                    )\n                    return res.reindex(self.index, copy=False)\n                else:\n                    return concat(\n                        frames, axis=1, join=how, verify_integrity=True, sort=sort\n                    )\n\n            joined = frames[0]\n\n            for frame in frames[1:]:\n                joined = merge(\n                    joined,\n                    frame,\n                    how=how,\n                    left_index=True,\n                    right_index=True,\n                    validate=validate,\n                )\n\n            return joined\n\n    @Substitution(\"\")\n    @Appender(_merge_doc, indents=2)\n    def merge(\n        self,\n        right: DataFrame | Series,\n        how: MergeHow = \"inner\",\n        on: IndexLabel | AnyArrayLike | None = None,\n        left_on: IndexLabel | AnyArrayLike | None = None,\n        right_on: IndexLabel | AnyArrayLike | None = None,\n        left_index: bool = False,\n        right_index: bool = False,\n        sort: bool = False,\n        suffixes: Suffixes = (\"_x\", \"_y\"),\n        copy: bool | None = None,\n        indicator: str | bool = False,\n        validate: MergeValidate | None = None,\n    ) -> DataFrame:\n        from pandas.core.reshape.merge import merge\n\n        return merge(\n            self,\n            right,\n            how=how,\n            on=on,\n            left_on=left_on,\n            right_on=right_on,\n            left_index=left_index,\n            right_index=right_index,\n            sort=sort,\n            suffixes=suffixes,\n            copy=copy,\n            indicator=indicator,\n            validate=validate,\n        )\n\n    def round(\n        self, decimals: int | dict[IndexLabel, int] | Series = 0, *args, **kwargs\n    ) -> DataFrame:\n        \"\"\"\n        Round a DataFrame to a variable number of decimal places.\n\n        Parameters\n        ----------\n        decimals : int, dict, Series\n            Number of decimal places to round each column to. If an int is\n            given, round each column to the same number of places.\n            Otherwise dict and Series round to variable numbers of places.\n            Column names should be in the keys if `decimals` is a\n            dict-like, or in the index if `decimals` is a Series. Any\n            columns not included in `decimals` will be left as is. Elements\n            of `decimals` which are not columns of the input will be\n            ignored.\n        *args\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n        **kwargs\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n\n        Returns\n        -------\n        DataFrame\n            A DataFrame with the affected columns rounded to the specified\n            number of decimal places.\n\n        See Also\n        --------\n        numpy.around : Round a numpy array to the given number of decimals.\n        Series.round : Round a Series to the given number of decimals.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df\n            dogs  cats\n        0  0.21  0.32\n        1  0.01  0.67\n        2  0.66  0.03\n        3  0.21  0.18\n\n        By providing an integer each column is rounded to the same number\n        of decimal places\n\n        >>> df.round(1)\n            dogs  cats\n        0   0.2   0.3\n        1   0.0   0.7\n        2   0.7   0.0\n        3   0.2   0.2\n\n        With a dict, the number of places for specific columns can be\n        specified with the column names as key and the number of decimal\n        places as value\n\n        >>> df.round({'dogs': 1, 'cats': 0})\n            dogs  cats\n        0   0.2   0.0\n        1   0.0   1.0\n        2   0.7   0.0\n        3   0.2   0.0\n\n        Using a Series, the number of places for specific columns can be\n        specified with the column names as index and the number of\n        decimal places as value\n\n        >>> decimals = pd.Series([0, 1], index=['cats', 'dogs'])\n        >>> df.round(decimals)\n            dogs  cats\n        0   0.2   0.0\n        1   0.0   1.0\n        2   0.7   0.0\n        3   0.2   0.0\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        def _dict_round(df: DataFrame, decimals):\n            for col, vals in df.items():\n                try:\n                    yield _series_round(vals, decimals[col])\n                except KeyError:\n                    yield vals\n\n        def _series_round(ser: Series, decimals: int) -> Series:\n            if is_integer_dtype(ser.dtype) or is_float_dtype(ser.dtype):\n                return ser.round(decimals)\n            return ser\n\n        nv.validate_round(args, kwargs)\n\n        if isinstance(decimals, (dict, Series)):\n            if isinstance(decimals, Series) and not decimals.index.is_unique:\n                raise ValueError(\"Index of decimals must be unique\")\n            if is_dict_like(decimals) and not all(\n                is_integer(value) for _, value in decimals.items()\n            ):\n                raise TypeError(\"Values in decimals must be integers\")\n            new_cols = list(_dict_round(self, decimals))\n        elif is_integer(decimals):\n            # Dispatch to Block.round\n            # Argument \"decimals\" to \"round\" of \"BaseBlockManager\" has incompatible\n            # type \"Union[int, integer[Any]]\"; expected \"int\"\n            new_mgr = self._mgr.round(\n                decimals=decimals,  # type: ignore[arg-type]\n                using_cow=using_copy_on_write(),\n            )\n            return self._constructor_from_mgr(new_mgr, axes=new_mgr.axes).__finalize__(\n                self, method=\"round\"\n            )\n        else:\n            raise TypeError(\"decimals must be an integer, a dict-like or a Series\")\n\n        if new_cols is not None and len(new_cols) > 0:\n            return self._constructor(\n                concat(new_cols, axis=1), index=self.index, columns=self.columns\n            ).__finalize__(self, method=\"round\")\n        else:\n            return self.copy(deep=False)\n\n    # ----------------------------------------------------------------------\n    # Statistical methods, etc.\n\n    def corr(\n        self,\n        method: CorrelationMethod = \"pearson\",\n        min_periods: int = 1,\n        numeric_only: bool = False,\n    ) -> DataFrame:\n        \"\"\"\n        Compute pairwise correlation of columns, excluding NA/null values.\n\n        Parameters\n        ----------\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method of correlation:\n\n            * pearson : standard correlation coefficient\n            * kendall : Kendall Tau correlation coefficient\n            * spearman : Spearman rank correlation\n            * callable: callable with input two 1d ndarrays\n                and returning a float. Note that the returned matrix from corr\n                will have 1 along the diagonals and will be symmetric\n                regardless of the callable's behavior.\n        min_periods : int, optional\n            Minimum number of observations required per pair of columns\n            to have a valid result. Currently only available for Pearson\n            and Spearman correlation.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n                The default value of ``numeric_only`` is now ``False``.\n\n        Returns\n        -------\n        DataFrame\n            Correlation matrix.\n\n        See Also\n        --------\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n        Series.corr : Compute the correlation between two Series.\n\n        Notes\n        -----\n        Pearson, Kendall and Spearman correlation are currently computed using pairwise complete observations.\n\n        * `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`_\n        * `Kendall rank correlation coefficient <https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient>`_\n        * `Spearman's rank correlation coefficient <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>`_\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.corr(method=histogram_intersection)\n              dogs  cats\n        dogs   1.0   0.3\n        cats   0.3   1.0\n\n        >>> df = pd.DataFrame([(1, 1), (2, np.nan), (np.nan, 3), (4, 4)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.corr(min_periods=3)\n              dogs  cats\n        dogs   1.0   NaN\n        cats   NaN   1.0\n        \"\"\"  # noqa: E501\n        data = self._get_numeric_data() if numeric_only else self\n        cols = data.columns\n        idx = cols.copy()\n        mat = data.to_numpy(dtype=float, na_value=np.nan, copy=False)\n\n        if method == \"pearson\":\n            correl = libalgos.nancorr(mat, minp=min_periods)\n        elif method == \"spearman\":\n            correl = libalgos.nancorr_spearman(mat, minp=min_periods)\n        elif method == \"kendall\" or callable(method):\n            if min_periods is None:\n                min_periods = 1\n            mat = mat.T\n            corrf = nanops.get_corr_func(method)\n            K = len(cols)\n            correl = np.empty((K, K), dtype=float)\n            mask = np.isfinite(mat)\n            for i, ac in enumerate(mat):\n                for j, bc in enumerate(mat):\n                    if i > j:\n                        continue\n\n                    valid = mask[i] & mask[j]\n                    if valid.sum() < min_periods:\n                        c = np.nan\n                    elif i == j:\n                        c = 1.0\n                    elif not valid.all():\n                        c = corrf(ac[valid], bc[valid])\n                    else:\n                        c = corrf(ac, bc)\n                    correl[i, j] = c\n                    correl[j, i] = c\n        else:\n            raise ValueError(\n                \"method must be either 'pearson', \"\n                \"'spearman', 'kendall', or a callable, \"\n                f\"'{method}' was supplied\"\n            )\n\n        result = self._constructor(correl, index=idx, columns=cols, copy=False)\n        return result.__finalize__(self, method=\"corr\")\n\n    def cov(\n        self,\n        min_periods: int | None = None,\n        ddof: int | None = 1,\n        numeric_only: bool = False,\n    ) -> DataFrame:\n        \"\"\"\n        Compute pairwise covariance of columns, excluding NA/null values.\n\n        Compute the pairwise covariance among the series of a DataFrame.\n        The returned data frame is the `covariance matrix\n        <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns\n        of the DataFrame.\n\n        Both NA and null values are automatically excluded from the\n        calculation. (See the note below about bias from missing values.)\n        A threshold can be set for the minimum number of\n        observations for each value created. Comparisons with observations\n        below this threshold will be returned as ``NaN``.\n\n        This method is generally used for the analysis of time series data to\n        understand the relationship between different measures\n        across time.\n\n        Parameters\n        ----------\n        min_periods : int, optional\n            Minimum number of observations required per pair of columns\n            to have a valid result.\n\n        ddof : int, default 1\n            Delta degrees of freedom.  The divisor used in calculations\n            is ``N - ddof``, where ``N`` represents the number of elements.\n            This argument is applicable only when no ``nan`` is in the dataframe.\n\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n                The default value of ``numeric_only`` is now ``False``.\n\n        Returns\n        -------\n        DataFrame\n            The covariance matrix of the series of the DataFrame.\n\n        See Also\n        --------\n        Series.cov : Compute covariance with another Series.\n        core.window.ewm.ExponentialMovingWindow.cov : Exponential weighted sample\n            covariance.\n        core.window.expanding.Expanding.cov : Expanding sample covariance.\n        core.window.rolling.Rolling.cov : Rolling sample covariance.\n\n        Notes\n        -----\n        Returns the covariance matrix of the DataFrame's time series.\n        The covariance is normalized by N-ddof.\n\n        For DataFrames that have Series that are missing data (assuming that\n        data is `missing at random\n        <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__)\n        the returned covariance matrix will be an unbiased estimate\n        of the variance and covariance between the member Series.\n\n        However, for many applications this estimate may not be acceptable\n        because the estimate covariance matrix is not guaranteed to be positive\n        semi-definite. This could lead to estimate correlations having\n        absolute values which are greater than one, and/or a non-invertible\n        covariance matrix. See `Estimation of covariance matrices\n        <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_\n        matrices>`__ for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.cov()\n                  dogs      cats\n        dogs  0.666667 -1.000000\n        cats -1.000000  1.666667\n\n        >>> np.random.seed(42)\n        >>> df = pd.DataFrame(np.random.randn(1000, 5),\n        ...                   columns=['a', 'b', 'c', 'd', 'e'])\n        >>> df.cov()\n                  a         b         c         d         e\n        a  0.998438 -0.020161  0.059277 -0.008943  0.014144\n        b -0.020161  1.059352 -0.008543 -0.024738  0.009826\n        c  0.059277 -0.008543  1.010670 -0.001486 -0.000271\n        d -0.008943 -0.024738 -0.001486  0.921297 -0.013692\n        e  0.014144  0.009826 -0.000271 -0.013692  0.977795\n\n        **Minimum number of periods**\n\n        This method also supports an optional ``min_periods`` keyword\n        that specifies the required minimum number of non-NA observations for\n        each column pair in order to have a valid result:\n\n        >>> np.random.seed(42)\n        >>> df = pd.DataFrame(np.random.randn(20, 3),\n        ...                   columns=['a', 'b', 'c'])\n        >>> df.loc[df.index[:5], 'a'] = np.nan\n        >>> df.loc[df.index[5:10], 'b'] = np.nan\n        >>> df.cov(min_periods=12)\n                  a         b         c\n        a  0.316741       NaN -0.150812\n        b       NaN  1.248003  0.191417\n        c -0.150812  0.191417  0.895202\n        \"\"\"\n        data = self._get_numeric_data() if numeric_only else self\n        cols = data.columns\n        idx = cols.copy()\n        mat = data.to_numpy(dtype=float, na_value=np.nan, copy=False)\n\n        if notna(mat).all():\n            if min_periods is not None and min_periods > len(mat):\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n            else:\n                base_cov = np.cov(mat.T, ddof=ddof)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n        else:\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n\n        result = self._constructor(base_cov, index=idx, columns=cols, copy=False)\n        return result.__finalize__(self, method=\"cov\")\n\n    def corrwith(\n        self,\n        other: DataFrame | Series,\n        axis: Axis = 0,\n        drop: bool = False,\n        method: CorrelationMethod = \"pearson\",\n        numeric_only: bool = False,\n    ) -> Series:\n        \"\"\"\n        Compute pairwise correlation.\n\n        Pairwise correlation is computed between rows or columns of\n        DataFrame with rows or columns of Series or DataFrame. DataFrames\n        are first aligned along both axes before computing the\n        correlations.\n\n        Parameters\n        ----------\n        other : DataFrame, Series\n            Object with which to compute correlations.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' to compute row-wise, 1 or 'columns' for\n            column-wise.\n        drop : bool, default False\n            Drop missing indices from result.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method of correlation:\n\n            * pearson : standard correlation coefficient\n            * kendall : Kendall Tau correlation coefficient\n            * spearman : Spearman rank correlation\n            * callable: callable with input two 1d ndarrays\n                and returning a float.\n\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n                The default value of ``numeric_only`` is now ``False``.\n\n        Returns\n        -------\n        Series\n            Pairwise correlations.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation of columns.\n\n        Examples\n        --------\n        >>> index = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n        >>> columns = [\"one\", \"two\", \"three\", \"four\"]\n        >>> df1 = pd.DataFrame(np.arange(20).reshape(5, 4), index=index, columns=columns)\n        >>> df2 = pd.DataFrame(np.arange(16).reshape(4, 4), index=index[:4], columns=columns)\n        >>> df1.corrwith(df2)\n        one      1.0\n        two      1.0\n        three    1.0\n        four     1.0\n        dtype: float64\n\n        >>> df2.corrwith(df1, axis=1)\n        a    1.0\n        b    1.0\n        c    1.0\n        d    1.0\n        e    NaN\n        dtype: float64\n        \"\"\"  # noqa: E501\n        axis = self._get_axis_number(axis)\n        this = self._get_numeric_data() if numeric_only else self\n\n        if isinstance(other, Series):\n            return this.apply(lambda x: other.corr(x, method=method), axis=axis)\n\n        if numeric_only:\n            other = other._get_numeric_data()\n        left, right = this.align(other, join=\"inner\", copy=False)\n\n        if axis == 1:\n            left = left.T\n            right = right.T\n\n        if method == \"pearson\":\n            # mask missing values\n            left = left + right * 0\n            right = right + left * 0\n\n            # demeaned data\n            ldem = left - left.mean(numeric_only=numeric_only)\n            rdem = right - right.mean(numeric_only=numeric_only)\n\n            num = (ldem * rdem).sum()\n            dom = (\n                (left.count() - 1)\n                * left.std(numeric_only=numeric_only)\n                * right.std(numeric_only=numeric_only)\n            )\n\n            correl = num / dom\n\n        elif method in [\"kendall\", \"spearman\"] or callable(method):\n\n            def c(x):\n                return nanops.nancorr(x[0], x[1], method=method)\n\n            correl = self._constructor_sliced(\n                map(c, zip(left.values.T, right.values.T)),\n                index=left.columns,\n                copy=False,\n            )\n\n        else:\n            raise ValueError(\n                f\"Invalid method {method} was passed, \"\n                \"valid methods are: 'pearson', 'kendall', \"\n                \"'spearman', or callable\"\n            )\n\n        if not drop:\n            # Find non-matching labels along the given axis\n            # and append missing correlations (GH 22375)\n            raxis: AxisInt = 1 if axis == 0 else 0\n            result_index = this._get_axis(raxis).union(other._get_axis(raxis))\n            idx_diff = result_index.difference(correl.index)\n\n            if len(idx_diff) > 0:\n                correl = correl._append(\n                    Series([np.nan] * len(idx_diff), index=idx_diff)\n                )\n\n        return correl\n\n    # ----------------------------------------------------------------------\n    # ndarray-like stats methods\n\n    def count(self, axis: Axis = 0, numeric_only: bool = False):\n        \"\"\"\n        Count non-NA cells for each column or row.\n\n        The values `None`, `NaN`, `NaT`, ``pandas.NA`` are considered NA.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            If 0 or 'index' counts are generated for each column.\n            If 1 or 'columns' counts are generated for each row.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n        Returns\n        -------\n        Series\n            For each column/row the number of non-NA/null entries.\n\n        See Also\n        --------\n        Series.count: Number of non-NA elements in a Series.\n        DataFrame.value_counts: Count unique combinations of columns.\n        DataFrame.shape: Number of DataFrame rows and columns (including NA\n            elements).\n        DataFrame.isna: Boolean same-sized DataFrame showing places of NA\n            elements.\n\n        Examples\n        --------\n        Constructing DataFrame from a dictionary:\n\n        >>> df = pd.DataFrame({\"Person\":\n        ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n        ...                    \"Age\": [24., np.nan, 21., 33, 26],\n        ...                    \"Single\": [False, True, True, True, False]})\n        >>> df\n           Person   Age  Single\n        0    John  24.0   False\n        1    Myla   NaN    True\n        2   Lewis  21.0    True\n        3    John  33.0    True\n        4    Myla  26.0   False\n\n        Notice the uncounted NA values:\n\n        >>> df.count()\n        Person    5\n        Age       4\n        Single    5\n        dtype: int64\n\n        Counts for each **row**:\n\n        >>> df.count(axis='columns')\n        0    3\n        1    2\n        2    3\n        3    3\n        4    3\n        dtype: int64\n        \"\"\"\n        axis = self._get_axis_number(axis)\n\n        if numeric_only:\n            frame = self._get_numeric_data()\n        else:\n            frame = self\n\n        # GH #423\n        if len(frame._get_axis(axis)) == 0:\n            result = self._constructor_sliced(0, index=frame._get_agg_axis(axis))\n        else:\n            result = notna(frame).sum(axis=axis)\n\n        return result.astype(\"int64\", copy=False).__finalize__(self, method=\"count\")\n\n    def _reduce(\n        self,\n        op,\n        name: str,\n        *,\n        axis: Axis = 0,\n        skipna: bool = True,\n        numeric_only: bool = False,\n        filter_type=None,\n        **kwds,\n    ):\n        assert filter_type is None or filter_type == \"bool\", filter_type\n        out_dtype = \"bool\" if filter_type == \"bool\" else None\n\n        if axis is not None:\n            axis = self._get_axis_number(axis)\n\n        def func(values: np.ndarray):\n            # We only use this in the case that operates on self.values\n            return op(values, axis=axis, skipna=skipna, **kwds)\n\n        dtype_has_keepdims: dict[ExtensionDtype, bool] = {}\n\n        def blk_func(values, axis: Axis = 1):\n            if isinstance(values, ExtensionArray):\n                if not is_1d_only_ea_dtype(values.dtype) and not isinstance(\n                    self._mgr, ArrayManager\n                ):\n                    return values._reduce(name, axis=1, skipna=skipna, **kwds)\n                has_keepdims = dtype_has_keepdims.get(values.dtype)\n                if has_keepdims is None:\n                    sign = signature(values._reduce)\n                    has_keepdims = \"keepdims\" in sign.parameters\n                    dtype_has_keepdims[values.dtype] = has_keepdims\n                if has_keepdims:\n                    return values._reduce(name, skipna=skipna, keepdims=True, **kwds)\n                else:\n                    warnings.warn(\n                        f\"{type(values)}._reduce will require a `keepdims` parameter \"\n                        \"in the future\",\n                        FutureWarning,\n                        stacklevel=find_stack_level(),\n                    )\n                    result = values._reduce(name, skipna=skipna, **kwds)\n                    return np.array([result])\n            else:\n                return op(values, axis=axis, skipna=skipna, **kwds)\n\n        def _get_data() -> DataFrame:\n            if filter_type is None:\n                data = self._get_numeric_data()\n            else:\n                # GH#25101, GH#24434\n                assert filter_type == \"bool\"\n                data = self._get_bool_data()\n            return data\n\n        # Case with EAs see GH#35881\n        df = self\n        if numeric_only:\n            df = _get_data()\n        if axis is None:\n            dtype = find_common_type([arr.dtype for arr in df._mgr.arrays])\n            if isinstance(dtype, ExtensionDtype):\n                df = df.astype(dtype, copy=False)\n                arr = concat_compat(list(df._iter_column_arrays()))\n                return arr._reduce(name, skipna=skipna, keepdims=False, **kwds)\n            return func(df.values)\n        elif axis == 1:\n            if len(df.index) == 0:\n                # Taking a transpose would result in no columns, losing the dtype.\n                # In the empty case, reducing along axis 0 or 1 gives the same\n                # result dtype, so reduce with axis=0 and ignore values\n                result = df._reduce(\n                    op,\n                    name,\n                    axis=0,\n                    skipna=skipna,\n                    numeric_only=False,\n                    filter_type=filter_type,\n                    **kwds,\n                ).iloc[:0]\n                result.index = df.index\n                return result\n\n            # kurtosis excluded since groupby does not implement it\n            if df.shape[1] and name != \"kurt\":\n                dtype = find_common_type([arr.dtype for arr in df._mgr.arrays])\n                if isinstance(dtype, ExtensionDtype):\n                    # GH 54341: fastpath for EA-backed axis=1 reductions\n                    # This flattens the frame into a single 1D array while keeping\n                    # track of the row and column indices of the original frame. Once\n                    # flattened, grouping by the row indices and aggregating should\n                    # be equivalent to transposing the original frame and aggregating\n                    # with axis=0.\n                    name = {\"argmax\": \"idxmax\", \"argmin\": \"idxmin\"}.get(name, name)\n                    df = df.astype(dtype, copy=False)\n                    arr = concat_compat(list(df._iter_column_arrays()))\n                    nrows, ncols = df.shape\n                    row_index = np.tile(np.arange(nrows), ncols)\n                    col_index = np.repeat(np.arange(ncols), nrows)\n                    ser = Series(arr, index=col_index, copy=False)\n                    # GroupBy will raise a warning with SeriesGroupBy as the object,\n                    # likely confusing users\n                    with rewrite_warning(\n                        target_message=(\n                            f\"The behavior of SeriesGroupBy.{name} with all-NA values\"\n                        ),\n                        target_category=FutureWarning,\n                        new_message=(\n                            f\"The behavior of {type(self).__name__}.{name} with all-NA \"\n                            \"values, or any-NA and skipna=False, is deprecated. In \"\n                            \"a future version this will raise ValueError\"\n                        ),\n                    ):\n                        result = ser.groupby(row_index).agg(name, **kwds)\n                    result.index = df.index\n                    if not skipna and name not in (\"any\", \"all\"):\n                        mask = df.isna().to_numpy(dtype=np.bool_).any(axis=1)\n                        other = -1 if name in (\"idxmax\", \"idxmin\") else lib.no_default\n                        result = result.mask(mask, other)\n                    return result\n\n            df = df.T\n\n        # After possibly _get_data and transposing, we are now in the\n        #  simple case where we can use BlockManager.reduce\n        res = df._mgr.reduce(blk_func)\n        out = df._constructor_from_mgr(res, axes=res.axes).iloc[0]\n        if out_dtype is not None and out.dtype != \"boolean\":\n            out = out.astype(out_dtype)\n        elif (df._mgr.get_dtypes() == object).any() and name not in [\"any\", \"all\"]:\n            out = out.astype(object)\n        elif len(self) == 0 and out.dtype == object and name in (\"sum\", \"prod\"):\n            # Even if we are object dtype, follow numpy and return\n            #  float64, see test_apply_funcs_over_empty\n            out = out.astype(np.float64)\n\n        return out\n\n    def _reduce_axis1(self, name: str, func, skipna: bool) -> Series:\n        \"\"\"\n        Special case for _reduce to try to avoid a potentially-expensive transpose.\n\n        Apply the reduction block-wise along axis=1 and then reduce the resulting\n        1D arrays.\n        \"\"\"\n        if name == \"all\":\n            result = np.ones(len(self), dtype=bool)\n            ufunc = np.logical_and\n        elif name == \"any\":\n            result = np.zeros(len(self), dtype=bool)\n            # error: Incompatible types in assignment\n            # (expression has type \"_UFunc_Nin2_Nout1[Literal['logical_or'],\n            # Literal[20], Literal[False]]\", variable has type\n            # \"_UFunc_Nin2_Nout1[Literal['logical_and'], Literal[20],\n            # Literal[True]]\")\n            ufunc = np.logical_or  # type: ignore[assignment]\n        else:\n            raise NotImplementedError(name)\n\n        for arr in self._mgr.arrays:\n            middle = func(arr, axis=0, skipna=skipna)\n            result = ufunc(result, middle)\n\n        res_ser = self._constructor_sliced(result, index=self.index, copy=False)\n        return res_ser\n\n    @doc(make_doc(\"any\", ndim=2))\n    # error: Signature of \"any\" incompatible with supertype \"NDFrame\"\n    def any(  # type: ignore[override]\n        self,\n        *,\n        axis: Axis | None = 0,\n        bool_only: bool = False,\n        skipna: bool = True,\n        **kwargs,\n    ) -> Series | bool:\n        result = self._logical_func(\n            \"any\", nanops.nanany, axis, bool_only, skipna, **kwargs\n        )\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method=\"any\")\n        return result\n\n    @doc(make_doc(\"all\", ndim=2))\n    def all(\n        self,\n        axis: Axis | None = 0,\n        bool_only: bool = False,\n        skipna: bool = True,\n        **kwargs,\n    ) -> Series | bool:\n        result = self._logical_func(\n            \"all\", nanops.nanall, axis, bool_only, skipna, **kwargs\n        )\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method=\"all\")\n        return result\n\n    @doc(make_doc(\"min\", ndim=2))\n    def min(\n        self,\n        axis: Axis | None = 0,\n        skipna: bool = True,\n        numeric_only: bool = False,\n        **kwargs,\n    ):\n        result = super().min(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method=\"min\")\n        return result\n\n    @doc(make_doc(\"max\", ndim=2))\n    def max(\n        self,\n        axis: Axis | None = 0,\n        skipna: bool = True,\n        numeric_only: bool = False,\n        **kwargs,\n    ):\n        result = super().max(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method=\"max\")\n        return result\n\n    @doc(make_doc(\"sum\", ndim=2))\n    def sum(\n        self,\n        axis: Axis | None = 0,\n        skipna: bool = True,\n        numeric_only: bool = False,\n        min_count: int = 0,\n        **kwargs,\n    ):\n        result = super().sum(axis, skipna, numeric_only, min_count, **kwargs)\n        return result.__finalize__(self, method=\"sum\")\n\n    @doc(make_doc(\"prod\", ndim=2))\n    def prod(\n        self,\n        axis: Axis | None = 0,\n        skipna: bool = True,\n        numeric_only: bool = False,\n        min_count: int = 0,\n        **kwargs,\n    ):\n        result = super().prod(axis, skipna, numeric_only, min_count, **kwargs)\n        return result.__finalize__(self, method=\"prod\")\n\n    @doc(make_doc(\"mean\", ndim=2))\n    def mean(\n        self,\n        axis: Axis | None = 0,\n        skipna: bool = True,\n        numeric_only: bool = False,\n        **kwargs,\n    ):\n        result = super().mean(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method=\"mean\")\n        return result\n\n    @doc(make_doc(\"median\", ndim=2))\n    def median(\n        self,\n        axis: Axis | None = 0,\n        skipna: bool = True,\n        numeric_only: bool = False,\n        **kwargs,\n    ):\n        result = super().median(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method=\"median\")\n        return result\n\n    @doc(make_doc(\"sem\", ndim=2))\n    def sem(\n        self,\n        axis: Axis | None = 0,\n        skipna: bool = True,\n        ddof: int = 1,\n        numeric_only: bool = False,\n        **kwargs,\n    ):\n        result = super().sem(axis, skipna, ddof, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method=\"sem\")\n        return result\n\n    @doc(make_doc(\"var\", ndim=2))\n    def var(\n        self,\n        axis: Axis | None = 0,\n        skipna: bool = True,\n        ddof: int = 1,\n        numeric_only: bool = False,\n        **kwargs,\n    ):\n        result = super().var(axis, skipna, ddof, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method=\"var\")\n        return result\n\n    @doc(make_doc(\"std\", ndim=2))\n    def std(\n        self,\n        axis: Axis | None = 0,\n        skipna: bool = True,\n        ddof: int = 1,\n        numeric_only: bool = False,\n        **kwargs,\n    ):\n        result = super().std(axis, skipna, ddof, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method=\"std\")\n        return result\n\n    @doc(make_doc(\"skew\", ndim=2))\n    def skew(\n        self,\n        axis: Axis | None = 0,\n        skipna: bool = True,\n        numeric_only: bool = False,\n        **kwargs,\n    ):\n        result = super().skew(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method=\"skew\")\n        return result\n\n    @doc(make_doc(\"kurt\", ndim=2))\n    def kurt(\n        self,\n        axis: Axis | None = 0,\n        skipna: bool = True,\n        numeric_only: bool = False,\n        **kwargs,\n    ):\n        result = super().kurt(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method=\"kurt\")\n        return result\n\n    kurtosis = kurt\n    product = prod\n\n    @doc(make_doc(\"cummin\", ndim=2))\n    def cummin(self, axis: Axis | None = None, skipna: bool = True, *args, **kwargs):\n        return NDFrame.cummin(self, axis, skipna, *args, **kwargs)\n\n    @doc(make_doc(\"cummax\", ndim=2))\n    def cummax(self, axis: Axis | None = None, skipna: bool = True, *args, **kwargs):\n        return NDFrame.cummax(self, axis, skipna, *args, **kwargs)\n\n    @doc(make_doc(\"cumsum\", ndim=2))\n    def cumsum(self, axis: Axis | None = None, skipna: bool = True, *args, **kwargs):\n        return NDFrame.cumsum(self, axis, skipna, *args, **kwargs)\n\n    @doc(make_doc(\"cumprod\", 2))\n    def cumprod(self, axis: Axis | None = None, skipna: bool = True, *args, **kwargs):\n        return NDFrame.cumprod(self, axis, skipna, *args, **kwargs)\n\n    def nunique(self, axis: Axis = 0, dropna: bool = True) -> Series:\n        \"\"\"\n        Count number of distinct elements in specified axis.\n\n        Return Series with number of distinct elements. Can ignore NaN\n        values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for\n            column-wise.\n        dropna : bool, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        Series\n\n        See Also\n        --------\n        Series.nunique: Method nunique for Series.\n        DataFrame.count: Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [4, 5, 6], 'B': [4, 1, 1]})\n        >>> df.nunique()\n        A    3\n        B    2\n        dtype: int64\n\n        >>> df.nunique(axis=1)\n        0    1\n        1    2\n        2    2\n        dtype: int64\n        \"\"\"\n        return self.apply(Series.nunique, axis=axis, dropna=dropna)\n\n    @doc(_shared_docs[\"idxmin\"], numeric_only_default=\"False\")\n    def idxmin(\n        self, axis: Axis = 0, skipna: bool = True, numeric_only: bool = False\n    ) -> Series:\n        axis = self._get_axis_number(axis)\n\n        if self.empty and len(self.axes[axis]):\n            axis_dtype = self.axes[axis].dtype\n            return self._constructor_sliced(dtype=axis_dtype)\n\n        if numeric_only:\n            data = self._get_numeric_data()\n        else:\n            data = self\n\n        res = data._reduce(\n            nanops.nanargmin, \"argmin\", axis=axis, skipna=skipna, numeric_only=False\n        )\n        indices = res._values\n        # indices will always be np.ndarray since axis is not N\n\n        if (indices == -1).any():\n            warnings.warn(\n                f\"The behavior of {type(self).__name__}.idxmin with all-NA \"\n                \"values, or any-NA and skipna=False, is deprecated. In a future \"\n                \"version this will raise ValueError\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        index = data._get_axis(axis)\n        result = algorithms.take(\n            index._values, indices, allow_fill=True, fill_value=index._na_value\n        )\n        final_result = data._constructor_sliced(result, index=data._get_agg_axis(axis))\n        return final_result.__finalize__(self, method=\"idxmin\")\n\n    @doc(_shared_docs[\"idxmax\"], numeric_only_default=\"False\")\n    def idxmax(\n        self, axis: Axis = 0, skipna: bool = True, numeric_only: bool = False\n    ) -> Series:\n        axis = self._get_axis_number(axis)\n\n        if self.empty and len(self.axes[axis]):\n            axis_dtype = self.axes[axis].dtype\n            return self._constructor_sliced(dtype=axis_dtype)\n\n        if numeric_only:\n            data = self._get_numeric_data()\n        else:\n            data = self\n\n        res = data._reduce(\n            nanops.nanargmax, \"argmax\", axis=axis, skipna=skipna, numeric_only=False\n        )\n        indices = res._values\n        # indices will always be 1d array since axis is not None\n\n        if (indices == -1).any():\n            warnings.warn(\n                f\"The behavior of {type(self).__name__}.idxmax with all-NA \"\n                \"values, or any-NA and skipna=False, is deprecated. In a future \"\n                \"version this will raise ValueError\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        index = data._get_axis(axis)\n        result = algorithms.take(\n            index._values, indices, allow_fill=True, fill_value=index._na_value\n        )\n        final_result = data._constructor_sliced(result, index=data._get_agg_axis(axis))\n        return final_result.__finalize__(self, method=\"idxmax\")\n\n    def _get_agg_axis(self, axis_num: int) -> Index:\n        \"\"\"\n        Let's be explicit about this.\n        \"\"\"\n        if axis_num == 0:\n            return self.columns\n        elif axis_num == 1:\n            return self.index\n        else:\n            raise ValueError(f\"Axis must be 0 or 1 (got {repr(axis_num)})\")\n\n    def mode(\n        self, axis: Axis = 0, numeric_only: bool = False, dropna: bool = True\n    ) -> DataFrame:\n        \"\"\"\n        Get the mode(s) of each element along the selected axis.\n\n        The mode of a set of values is the value that appears most often.\n        It can be multiple values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to iterate over while searching for the mode:\n\n            * 0 or 'index' : get mode of each column\n            * 1 or 'columns' : get mode of each row.\n\n        numeric_only : bool, default False\n            If True, only apply to numeric columns.\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n        Returns\n        -------\n        DataFrame\n            The modes of each column or row.\n\n        See Also\n        --------\n        Series.mode : Return the highest frequency value in a Series.\n        Series.value_counts : Return the counts of values in a Series.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('bird', 2, 2),\n        ...                    ('mammal', 4, np.nan),\n        ...                    ('arthropod', 8, 0),\n        ...                    ('bird', 2, np.nan)],\n        ...                   index=('falcon', 'horse', 'spider', 'ostrich'),\n        ...                   columns=('species', 'legs', 'wings'))\n        >>> df\n                   species  legs  wings\n        falcon        bird     2    2.0\n        horse       mammal     4    NaN\n        spider   arthropod     8    0.0\n        ostrich       bird     2    NaN\n\n        By default, missing values are not considered, and the mode of wings\n        are both 0 and 2. Because the resulting DataFrame has two rows,\n        the second row of ``species`` and ``legs`` contains ``NaN``.\n\n        >>> df.mode()\n          species  legs  wings\n        0    bird   2.0    0.0\n        1     NaN   NaN    2.0\n\n        Setting ``dropna=False`` ``NaN`` values are considered and they can be\n        the mode (like for wings).\n\n        >>> df.mode(dropna=False)\n          species  legs  wings\n        0    bird     2    NaN\n\n        Setting ``numeric_only=True``, only the mode of numeric columns is\n        computed, and columns of other types are ignored.\n\n        >>> df.mode(numeric_only=True)\n           legs  wings\n        0   2.0    0.0\n        1   NaN    2.0\n\n        To compute the mode over columns and not rows, use the axis parameter:\n\n        >>> df.mode(axis='columns', numeric_only=True)\n                   0    1\n        falcon   2.0  NaN\n        horse    4.0  NaN\n        spider   0.0  8.0\n        ostrich  2.0  NaN\n        \"\"\"\n        data = self if not numeric_only else self._get_numeric_data()\n\n        def f(s):\n            return s.mode(dropna=dropna)\n\n        data = data.apply(f, axis=axis)\n        # Ensure index is type stable (should always use int index)\n        if data.empty:\n            data.index = default_index(0)\n\n        return data\n\n    @overload\n    def quantile(\n        self,\n        q: float = ...,\n        axis: Axis = ...,\n        numeric_only: bool = ...,\n        interpolation: QuantileInterpolation = ...,\n        method: Literal[\"single\", \"table\"] = ...,\n    ) -> Series:\n        ...\n\n    @overload\n    def quantile(\n        self,\n        q: AnyArrayLike | Sequence[float],\n        axis: Axis = ...,\n        numeric_only: bool = ...,\n        interpolation: QuantileInterpolation = ...,\n        method: Literal[\"single\", \"table\"] = ...,\n    ) -> Series | DataFrame:\n        ...\n\n    @overload\n    def quantile(\n        self,\n        q: float | AnyArrayLike | Sequence[float] = ...,\n        axis: Axis = ...,\n        numeric_only: bool = ...,\n        interpolation: QuantileInterpolation = ...,\n        method: Literal[\"single\", \"table\"] = ...,\n    ) -> Series | DataFrame:\n        ...\n\n    def quantile(\n        self,\n        q: float | AnyArrayLike | Sequence[float] = 0.5,\n        axis: Axis = 0,\n        numeric_only: bool = False,\n        interpolation: QuantileInterpolation = \"linear\",\n        method: Literal[\"single\", \"table\"] = \"single\",\n    ) -> Series | DataFrame:\n        \"\"\"\n        Return values at the given quantile over requested axis.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            Value between 0 <= q <= 1, the quantile(s) to compute.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionchanged:: 2.0.0\n                The default value of ``numeric_only`` is now ``False``.\n\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n            * linear: `i + (j - i) * fraction`, where `fraction` is the\n              fractional part of the index surrounded by `i` and `j`.\n            * lower: `i`.\n            * higher: `j`.\n            * nearest: `i` or `j` whichever is nearest.\n            * midpoint: (`i` + `j`) / 2.\n        method : {'single', 'table'}, default 'single'\n            Whether to compute quantiles per-column ('single') or over all columns\n            ('table'). When 'table', the only allowed interpolation methods are\n            'nearest', 'lower', and 'higher'.\n\n        Returns\n        -------\n        Series or DataFrame\n\n            If ``q`` is an array, a DataFrame will be returned where the\n              index is ``q``, the columns are the columns of self, and the\n              values are the quantiles.\n            If ``q`` is a float, a Series will be returned where the\n              index is the columns of self and the values are the quantiles.\n\n        See Also\n        --------\n        core.window.rolling.Rolling.quantile: Rolling quantile.\n        numpy.percentile: Numpy function to compute the percentile.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),\n        ...                   columns=['a', 'b'])\n        >>> df.quantile(.1)\n        a    1.3\n        b    3.7\n        Name: 0.1, dtype: float64\n        >>> df.quantile([.1, .5])\n               a     b\n        0.1  1.3   3.7\n        0.5  2.5  55.0\n\n        Specifying `method='table'` will compute the quantile over all columns.\n\n        >>> df.quantile(.1, method=\"table\", interpolation=\"nearest\")\n        a    1\n        b    1\n        Name: 0.1, dtype: int64\n        >>> df.quantile([.1, .5], method=\"table\", interpolation=\"nearest\")\n             a    b\n        0.1  1    1\n        0.5  3  100\n\n        Specifying `numeric_only=False` will also compute the quantile of\n        datetime and timedelta data.\n\n        >>> df = pd.DataFrame({'A': [1, 2],\n        ...                    'B': [pd.Timestamp('2010'),\n        ...                          pd.Timestamp('2011')],\n        ...                    'C': [pd.Timedelta('1 days'),\n        ...                          pd.Timedelta('2 days')]})\n        >>> df.quantile(0.5, numeric_only=False)\n        A                    1.5\n        B    2010-07-02 12:00:00\n        C        1 days 12:00:00\n        Name: 0.5, dtype: object\n        \"\"\"\n        validate_percentile(q)\n        axis = self._get_axis_number(axis)\n\n        if not is_list_like(q):\n            # BlockManager.quantile expects listlike, so we wrap and unwrap here\n            # error: List item 0 has incompatible type \"float | ExtensionArray |\n            # ndarray[Any, Any] | Index | Series | Sequence[float]\"; expected \"float\"\n            res_df = self.quantile(\n                [q],  # type: ignore[list-item]\n                axis=axis,\n                numeric_only=numeric_only,\n                interpolation=interpolation,\n                method=method,\n            )\n            if method == \"single\":\n                res = res_df.iloc[0]\n            else:\n                # cannot directly iloc over sparse arrays\n                res = res_df.T.iloc[:, 0]\n            if axis == 1 and len(self) == 0:\n                # GH#41544 try to get an appropriate dtype\n                dtype = find_common_type(list(self.dtypes))\n                if needs_i8_conversion(dtype):\n                    return res.astype(dtype)\n            return res\n\n        q = Index(q, dtype=np.float64)\n        data = self._get_numeric_data() if numeric_only else self\n\n        if axis == 1:\n            data = data.T\n\n        if len(data.columns) == 0:\n            # GH#23925 _get_numeric_data may have dropped all columns\n            cols = Index([], name=self.columns.name)\n\n            dtype = np.float64\n            if axis == 1:\n                # GH#41544 try to get an appropriate dtype\n                cdtype = find_common_type(list(self.dtypes))\n                if needs_i8_conversion(cdtype):\n                    dtype = cdtype\n\n            res = self._constructor([], index=q, columns=cols, dtype=dtype)\n            return res.__finalize__(self, method=\"quantile\")\n\n        valid_method = {\"single\", \"table\"}\n        if method not in valid_method:\n            raise ValueError(\n                f\"Invalid method: {method}. Method must be in {valid_method}.\"\n            )\n        if method == \"single\":\n            res = data._mgr.quantile(qs=q, interpolation=interpolation)\n        elif method == \"table\":\n            valid_interpolation = {\"nearest\", \"lower\", \"higher\"}\n            if interpolation not in valid_interpolation:\n                raise ValueError(\n                    f\"Invalid interpolation: {interpolation}. \"\n                    f\"Interpolation must be in {valid_interpolation}\"\n                )\n            # handle degenerate case\n            if len(data) == 0:\n                if data.ndim == 2:\n                    dtype = find_common_type(list(self.dtypes))\n                else:\n                    dtype = self.dtype\n                return self._constructor([], index=q, columns=data.columns, dtype=dtype)\n\n            q_idx = np.quantile(np.arange(len(data)), q, method=interpolation)\n\n            by = data.columns\n            if len(by) > 1:\n                keys = [data._get_label_or_level_values(x) for x in by]\n                indexer = lexsort_indexer(keys)\n            else:\n                k = data._get_label_or_level_values(by[0])\n                indexer = nargsort(k)\n\n            res = data._mgr.take(indexer[q_idx], verify=False)\n            res.axes[1] = q\n\n        result = self._constructor_from_mgr(res, axes=res.axes)\n        return result.__finalize__(self, method=\"quantile\")\n\n    def to_timestamp(\n        self,\n        freq: Frequency | None = None,\n        how: ToTimestampHow = \"start\",\n        axis: Axis = 0,\n        copy: bool | None = None,\n    ) -> DataFrame:\n        \"\"\"\n        Cast to DatetimeIndex of timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to convert (the index by default).\n        copy : bool, default True\n            If False then underlying input data is not copied.\n\n            .. note::\n                The `copy` keyword will change behavior in pandas 3.0.\n                `Copy-on-Write\n                <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n                will be enabled by default, which means that all methods with a\n                `copy` keyword will use a lazy copy mechanism to defer the copy and\n                ignore the `copy` keyword. The `copy` keyword will be removed in a\n                future version of pandas.\n\n                You can already get the future behavior and improvements through\n                enabling copy on write ``pd.options.mode.copy_on_write = True``\n\n        Returns\n        -------\n        DataFrame\n            The DataFrame has a DatetimeIndex.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex(['2023', '2024'], freq='Y')\n        >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n        >>> df1 = pd.DataFrame(data=d, index=idx)\n        >>> df1\n              col1   col2\n        2023     1      3\n        2024\t 2      4\n\n        The resulting timestamps will be at the beginning of the year in this case\n\n        >>> df1 = df1.to_timestamp()\n        >>> df1\n                    col1   col2\n        2023-01-01     1      3\n        2024-01-01     2      4\n        >>> df1.index\n        DatetimeIndex(['2023-01-01', '2024-01-01'], dtype='datetime64[ns]', freq=None)\n\n        Using `freq` which is the offset that the Timestamps will have\n\n        >>> df2 = pd.DataFrame(data=d, index=idx)\n        >>> df2 = df2.to_timestamp(freq='M')\n        >>> df2\n                    col1   col2\n        2023-01-31     1      3\n        2024-01-31     2      4\n        >>> df2.index\n        DatetimeIndex(['2023-01-31', '2024-01-31'], dtype='datetime64[ns]', freq=None)\n        \"\"\"\n        new_obj = self.copy(deep=copy and not using_copy_on_write())\n\n        axis_name = self._get_axis_name(axis)\n        old_ax = getattr(self, axis_name)\n        if not isinstance(old_ax, PeriodIndex):\n            raise TypeError(f\"unsupported Type {type(old_ax).__name__}\")\n\n        new_ax = old_ax.to_timestamp(freq=freq, how=how)\n\n        setattr(new_obj, axis_name, new_ax)\n        return new_obj\n\n    def to_period(\n        self, freq: Frequency | None = None, axis: Axis = 0, copy: bool | None = None\n    ) -> DataFrame:\n        \"\"\"\n        Convert DataFrame from DatetimeIndex to PeriodIndex.\n\n        Convert DataFrame from DatetimeIndex to PeriodIndex with desired\n        frequency (inferred from index if not passed).\n\n        Parameters\n        ----------\n        freq : str, default\n            Frequency of the PeriodIndex.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to convert (the index by default).\n        copy : bool, default True\n            If False then underlying input data is not copied.\n\n            .. note::\n                The `copy` keyword will change behavior in pandas 3.0.\n                `Copy-on-Write\n                <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n                will be enabled by default, which means that all methods with a\n                `copy` keyword will use a lazy copy mechanism to defer the copy and\n                ignore the `copy` keyword. The `copy` keyword will be removed in a\n                future version of pandas.\n\n                You can already get the future behavior and improvements through\n                enabling copy on write ``pd.options.mode.copy_on_write = True``\n\n        Returns\n        -------\n        DataFrame\n            The DataFrame has a PeriodIndex.\n\n        Examples\n        --------\n        >>> idx = pd.to_datetime(\n        ...     [\n        ...         \"2001-03-31 00:00:00\",\n        ...         \"2002-05-31 00:00:00\",\n        ...         \"2003-08-31 00:00:00\",\n        ...     ]\n        ... )\n\n        >>> idx\n        DatetimeIndex(['2001-03-31', '2002-05-31', '2003-08-31'],\n        dtype='datetime64[ns]', freq=None)\n\n        >>> idx.to_period(\"M\")\n        PeriodIndex(['2001-03', '2002-05', '2003-08'], dtype='period[M]')\n\n        For the yearly frequency\n\n        >>> idx.to_period(\"Y\")\n        PeriodIndex(['2001', '2002', '2003'], dtype='period[Y-DEC]')\n        \"\"\"\n        new_obj = self.copy(deep=copy and not using_copy_on_write())\n\n        axis_name = self._get_axis_name(axis)\n        old_ax = getattr(self, axis_name)\n        if not isinstance(old_ax, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(old_ax).__name__}\")\n\n        new_ax = old_ax.to_period(freq=freq)\n\n        setattr(new_obj, axis_name, new_ax)\n        return new_obj\n\n    def isin(self, values: Series | DataFrame | Sequence | Mapping) -> DataFrame:\n        \"\"\"\n        Whether each element in the DataFrame is contained in values.\n\n        Parameters\n        ----------\n        values : iterable, Series, DataFrame or dict\n            The result will only be true at a location if all the\n            labels match. If `values` is a Series, that's the index. If\n            `values` is a dict, the keys must be the column names,\n            which must match. If `values` is a DataFrame,\n            then both the index and column labels must match.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame of booleans showing whether each element in the DataFrame\n            is contained in values.\n\n        See Also\n        --------\n        DataFrame.eq: Equality test for DataFrame.\n        Series.isin: Equivalent method on Series.\n        Series.str.contains: Test if pattern or regex is contained within a\n            string of a Series or Index.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n        ...                   index=['falcon', 'dog'])\n        >>> df\n                num_legs  num_wings\n        falcon         2          2\n        dog            4          0\n\n        When ``values`` is a list check whether every value in the DataFrame\n        is present in the list (which animals have 0 or 2 legs or wings)\n\n        >>> df.isin([0, 2])\n                num_legs  num_wings\n        falcon      True       True\n        dog        False       True\n\n        To check if ``values`` is *not* in the DataFrame, use the ``~`` operator:\n\n        >>> ~df.isin([0, 2])\n                num_legs  num_wings\n        falcon     False      False\n        dog         True      False\n\n        When ``values`` is a dict, we can pass values to check for each\n        column separately:\n\n        >>> df.isin({'num_wings': [0, 3]})\n                num_legs  num_wings\n        falcon     False      False\n        dog        False       True\n\n        When ``values`` is a Series or DataFrame the index and column must\n        match. Note that 'falcon' does not match based on the number of legs\n        in other.\n\n        >>> other = pd.DataFrame({'num_legs': [8, 3], 'num_wings': [0, 2]},\n        ...                      index=['spider', 'falcon'])\n        >>> df.isin(other)\n                num_legs  num_wings\n        falcon     False       True\n        dog        False      False\n        \"\"\"\n        if isinstance(values, dict):\n            from pandas.core.reshape.concat import concat\n\n            values = collections.defaultdict(list, values)\n            result = concat(\n                (\n                    self.iloc[:, [i]].isin(values[col])\n                    for i, col in enumerate(self.columns)\n                ),\n                axis=1,\n            )\n        elif isinstance(values, Series):\n            if not values.index.is_unique:\n                raise ValueError(\"cannot compute isin with a duplicate axis.\")\n            result = self.eq(values.reindex_like(self), axis=\"index\")\n        elif isinstance(values, DataFrame):\n            if not (values.columns.is_unique and values.index.is_unique):\n                raise ValueError(\"cannot compute isin with a duplicate axis.\")\n            result = self.eq(values.reindex_like(self))\n        else:\n            if not is_list_like(values):\n                raise TypeError(\n                    \"only list-like or dict-like objects are allowed \"\n                    \"to be passed to DataFrame.isin(), \"\n                    f\"you passed a '{type(values).__name__}'\"\n                )\n\n            def isin_(x):\n                # error: Argument 2 to \"isin\" has incompatible type \"Union[Series,\n                # DataFrame, Sequence[Any], Mapping[Any, Any]]\"; expected\n                # \"Union[Union[Union[ExtensionArray, ndarray[Any, Any]], Index,\n                # Series], List[Any], range]\"\n                result = algorithms.isin(\n                    x.ravel(),\n                    values,  # type: ignore[arg-type]\n                )\n                return result.reshape(x.shape)\n\n            res_mgr = self._mgr.apply(isin_)\n            result = self._constructor_from_mgr(\n                res_mgr,\n                axes=res_mgr.axes,\n            )\n        return result.__finalize__(self, method=\"isin\")\n\n    # ----------------------------------------------------------------------\n    # Add index and columns\n    _AXIS_ORDERS: list[Literal[\"index\", \"columns\"]] = [\"index\", \"columns\"]\n    _AXIS_TO_AXIS_NUMBER: dict[Axis, int] = {\n        **NDFrame._AXIS_TO_AXIS_NUMBER,\n        1: 1,\n        \"columns\": 1,\n    }\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number: Literal[1] = 1\n    _info_axis_name: Literal[\"columns\"] = \"columns\"\n\n    index = properties.AxisProperty(\n        axis=1,\n        doc=\"\"\"\n        The index (row labels) of the DataFrame.\n\n        The index of a DataFrame is a series of labels that identify each row.\n        The labels can be integers, strings, or any other hashable type. The index\n        is used for label-based access and alignment, and can be accessed or\n        modified using this attribute.\n\n        Returns\n        -------\n        pandas.Index\n            The index labels of the DataFrame.\n\n        See Also\n        --------\n        DataFrame.columns : The column labels of the DataFrame.\n        DataFrame.to_numpy : Convert the DataFrame to a NumPy array.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'Name': ['Alice', 'Bob', 'Aritra'],\n        ...                    'Age': [25, 30, 35],\n        ...                    'Location': ['Seattle', 'New York', 'Kona']},\n        ...                   index=([10, 20, 30]))\n        >>> df.index\n        Index([10, 20, 30], dtype='int64')\n\n        In this example, we create a DataFrame with 3 rows and 3 columns,\n        including Name, Age, and Location information. We set the index labels to\n        be the integers 10, 20, and 30. We then access the `index` attribute of the\n        DataFrame, which returns an `Index` object containing the index labels.\n\n        >>> df.index = [100, 200, 300]\n        >>> df\n            Name  Age Location\n        100  Alice   25  Seattle\n        200    Bob   30 New York\n        300  Aritra  35    Kona\n\n        In this example, we modify the index labels of the DataFrame by assigning\n        a new list of labels to the `index` attribute. The DataFrame is then\n        updated with the new labels, and the output shows the modified DataFrame.\n        \"\"\",\n    )\n    columns = properties.AxisProperty(\n        axis=0,\n        doc=dedent(\n            \"\"\"\n                The column labels of the DataFrame.\n\n                Examples\n                --------\n                >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n                >>> df\n                     A  B\n                0    1  3\n                1    2  4\n                >>> df.columns\n                Index(['A', 'B'], dtype='object')\n                \"\"\"\n        ),\n    )\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to DataFrame\n    plot = CachedAccessor(\"plot\", pandas.plotting.PlotAccessor)\n    hist = pandas.plotting.hist_frame\n    boxplot = pandas.plotting.boxplot_frame\n    sparse = CachedAccessor(\"sparse\", SparseFrameAccessor)\n\n    # ----------------------------------------------------------------------\n    # Internal Interface Methods\n\n    def _to_dict_of_blocks(self):\n        \"\"\"\n        Return a dict of dtype -> Constructor Types that\n        each is a homogeneous dtype.\n\n        Internal ONLY - only works for BlockManager\n        \"\"\"\n        mgr = self._mgr\n        # convert to BlockManager if needed -> this way support ArrayManager as well\n        mgr = cast(BlockManager, mgr_to_mgr(mgr, \"block\"))\n        return {\n            k: self._constructor_from_mgr(v, axes=v.axes).__finalize__(self)\n            for k, v, in mgr.to_dict().items()\n        }\n\n    @property\n    def values(self) -> np.ndarray:\n        \"\"\"\n        Return a Numpy representation of the DataFrame.\n\n        .. warning::\n\n           We recommend using :meth:`DataFrame.to_numpy` instead.\n\n        Only the values in the DataFrame will be returned, the axes labels\n        will be removed.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values of the DataFrame.\n\n        See Also\n        --------\n        DataFrame.to_numpy : Recommended alternative to this method.\n        DataFrame.index : Retrieve the index labels.\n        DataFrame.columns : Retrieving the column names.\n\n        Notes\n        -----\n        The dtype will be a lower-common-denominator dtype (implicit\n        upcasting); that is to say if the dtypes (even of numeric types)\n        are mixed, the one that accommodates all will be chosen. Use this\n        with care if you are not dealing with the blocks.\n\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\n        float32.  If dtypes are int32 and uint8, dtype will be upcast to\n        int32. By :func:`numpy.find_common_type` convention, mixing int64\n        and uint64 will result in a float64 dtype.\n\n        Examples\n        --------\n        A DataFrame where all columns are the same type (e.g., int64) results\n        in an array of the same type.\n\n        >>> df = pd.DataFrame({'age':    [ 3,  29],\n        ...                    'height': [94, 170],\n        ...                    'weight': [31, 115]})\n        >>> df\n           age  height  weight\n        0    3      94      31\n        1   29     170     115\n        >>> df.dtypes\n        age       int64\n        height    int64\n        weight    int64\n        dtype: object\n        >>> df.values\n        array([[  3,  94,  31],\n               [ 29, 170, 115]])\n\n        A DataFrame with mixed type columns(e.g., str/object, int64, float32)\n        results in an ndarray of the broadest type that accommodates these\n        mixed types (e.g., object).\n\n        >>> df2 = pd.DataFrame([('parrot',   24.0, 'second'),\n        ...                     ('lion',     80.5, 1),\n        ...                     ('monkey', np.nan, None)],\n        ...                   columns=('name', 'max_speed', 'rank'))\n        >>> df2.dtypes\n        name          object\n        max_speed    float64\n        rank          object\n        dtype: object\n        >>> df2.values\n        array([['parrot', 24.0, 'second'],\n               ['lion', 80.5, 1],\n               ['monkey', nan, None]], dtype=object)\n        \"\"\"\n        return self._mgr.as_array()\n", "class_fn": true, "question_id": "pandas/pandas.core.frame/DataFrame", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/generic.py", "fn_id": "", "content": "class DataFrameGroupBy(GroupBy[DataFrame]):\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> data = {\"A\": [1, 1, 2, 2],\n    ...         \"B\": [1, 2, 3, 4],\n    ...         \"C\": [0.362838, 0.227877, 1.267767, -0.562860]}\n    >>> df = pd.DataFrame(data)\n    >>> df\n       A  B         C\n    0  1  1  0.362838\n    1  1  2  0.227877\n    2  2  3  1.267767\n    3  2  4 -0.562860\n\n    The aggregation is for each column.\n\n    >>> df.groupby('A').agg('min')\n       B         C\n    A\n    1  1  0.227877\n    2  3 -0.562860\n\n    Multiple aggregations\n\n    >>> df.groupby('A').agg(['min', 'max'])\n        B             C\n      min max       min       max\n    A\n    1   1   2  0.227877  0.362838\n    2   3   4 -0.562860  1.267767\n\n    Select a column for aggregation\n\n    >>> df.groupby('A').B.agg(['min', 'max'])\n       min  max\n    A\n    1    1    2\n    2    3    4\n\n    User-defined function for aggregation\n\n    >>> df.groupby('A').agg(lambda x: sum(x) + 2)\n        B\t       C\n    A\n    1\t5\t2.590715\n    2\t9\t2.704907\n\n    Different aggregations per column\n\n    >>> df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})\n        B             C\n      min max       sum\n    A\n    1   1   2  0.590715\n    2   3   4  0.704907\n\n    To control the output names with different aggregations per column,\n    pandas supports \"named aggregation\"\n\n    >>> df.groupby(\"A\").agg(\n    ...     b_min=pd.NamedAgg(column=\"B\", aggfunc=\"min\"),\n    ...     c_sum=pd.NamedAgg(column=\"C\", aggfunc=\"sum\")\n    ... )\n       b_min     c_sum\n    A\n    1      1  0.590715\n    2      3  0.704907\n\n    - The keywords are the *output* column names\n    - The values are tuples whose first element is the column to select\n      and the second element is the aggregation to apply to that column.\n      Pandas provides the ``pandas.NamedAgg`` namedtuple with the fields\n      ``['column', 'aggfunc']`` to make it clearer what the arguments are.\n      As usual, the aggregation can be a callable or a string alias.\n\n    See :ref:`groupby.aggregate.named` for more.\n\n    .. versionchanged:: 1.3.0\n\n        The resulting dtype will reflect the return value of the aggregating function.\n\n    >>> df.groupby(\"A\")[[\"B\"]].agg(lambda x: x.astype(float).min())\n          B\n    A\n    1   1.0\n    2   3.0\n    \"\"\"\n    )\n\n    @doc(_agg_template_frame, examples=_agg_examples_doc, klass=\"DataFrame\")\n    def aggregate(self, func=None, *args, engine=None, engine_kwargs=None, **kwargs):\n        relabeling, func, columns, order = reconstruct_func(func, **kwargs)\n        func = maybe_mangle_lambdas(func)\n\n        if maybe_use_numba(engine):\n            # Not all agg functions support numba, only propagate numba kwargs\n            # if user asks for numba\n            kwargs[\"engine\"] = engine\n            kwargs[\"engine_kwargs\"] = engine_kwargs\n\n        op = GroupByApply(self, func, args=args, kwargs=kwargs)\n        result = op.agg()\n        if not is_dict_like(func) and result is not None:\n            # GH #52849\n            if not self.as_index and is_list_like(func):\n                return result.reset_index()\n            else:\n                return result\n        elif relabeling:\n            # this should be the only (non-raising) case with relabeling\n            # used reordered index of columns\n            result = cast(DataFrame, result)\n            result = result.iloc[:, order]\n            result = cast(DataFrame, result)\n            # error: Incompatible types in assignment (expression has type\n            # \"Optional[List[str]]\", variable has type\n            # \"Union[Union[Union[ExtensionArray, ndarray[Any, Any]],\n            # Index, Series], Sequence[Any]]\")\n            result.columns = columns  # type: ignore[assignment]\n\n        if result is None:\n            # Remove the kwargs we inserted\n            # (already stored in engine, engine_kwargs arguments)\n            if \"engine\" in kwargs:\n                del kwargs[\"engine\"]\n                del kwargs[\"engine_kwargs\"]\n            # at this point func is not a str, list-like, dict-like,\n            # or a known callable(e.g. sum)\n            if maybe_use_numba(engine):\n                return self._aggregate_with_numba(\n                    func, *args, engine_kwargs=engine_kwargs, **kwargs\n                )\n            # grouper specific aggregations\n            if self._grouper.nkeys > 1:\n                # test_groupby_as_index_series_scalar gets here with 'not self.as_index'\n                return self._python_agg_general(func, *args, **kwargs)\n            elif args or kwargs:\n                # test_pass_args_kwargs gets here (with and without as_index)\n                # can't return early\n                result = self._aggregate_frame(func, *args, **kwargs)\n\n            elif self.axis == 1:\n                # _aggregate_multiple_funcs does not allow self.axis == 1\n                # Note: axis == 1 precludes 'not self.as_index', see __init__\n                result = self._aggregate_frame(func)\n                return result\n\n            else:\n                # try to treat as if we are passing a list\n                gba = GroupByApply(self, [func], args=(), kwargs={})\n                try:\n                    result = gba.agg()\n\n                except ValueError as err:\n                    if \"No objects to concatenate\" not in str(err):\n                        raise\n                    # _aggregate_frame can fail with e.g. func=Series.mode,\n                    # where it expects 1D values but would be getting 2D values\n                    # In other tests, using aggregate_frame instead of GroupByApply\n                    #  would give correct values but incorrect dtypes\n                    #  object vs float64 in test_cython_agg_empty_buckets\n                    #  float64 vs int64 in test_category_order_apply\n                    result = self._aggregate_frame(func)\n\n                else:\n                    # GH#32040, GH#35246\n                    # e.g. test_groupby_as_index_select_column_sum_empty_df\n                    result = cast(DataFrame, result)\n                    result.columns = self._obj_with_exclusions.columns.copy()\n\n        if not self.as_index:\n            result = self._insert_inaxis_grouper(result)\n            result.index = default_index(len(result))\n\n        return result\n\n    agg = aggregate\n\n    def _python_agg_general(self, func, *args, **kwargs):\n        orig_func = func\n        func = com.is_builtin_func(func)\n        if orig_func != func:\n            alias = com._builtin_table_alias[func]\n            warn_alias_replacement(self, orig_func, alias)\n        f = lambda x: func(x, *args, **kwargs)\n\n        if self.ngroups == 0:\n            # e.g. test_evaluate_with_empty_groups different path gets different\n            #  result dtype in empty case.\n            return self._python_apply_general(f, self._selected_obj, is_agg=True)\n\n        obj = self._obj_with_exclusions\n        if self.axis == 1:\n            obj = obj.T\n\n        if not len(obj.columns):\n            # e.g. test_margins_no_values_no_cols\n            return self._python_apply_general(f, self._selected_obj)\n\n        output: dict[int, ArrayLike] = {}\n        for idx, (name, ser) in enumerate(obj.items()):\n            result = self._grouper.agg_series(ser, f)\n            output[idx] = result\n\n        res = self.obj._constructor(output)\n        res.columns = obj.columns.copy(deep=False)\n        return self._wrap_aggregated_output(res)\n\n    def _aggregate_frame(self, func, *args, **kwargs) -> DataFrame:\n        if self._grouper.nkeys != 1:\n            raise AssertionError(\"Number of keys must be 1\")\n\n        obj = self._obj_with_exclusions\n\n        result: dict[Hashable, NDFrame | np.ndarray] = {}\n        for name, grp_df in self._grouper.get_iterator(obj, self.axis):\n            fres = func(grp_df, *args, **kwargs)\n            result[name] = fres\n\n        result_index = self._grouper.result_index\n        other_ax = obj.axes[1 - self.axis]\n        out = self.obj._constructor(result, index=other_ax, columns=result_index)\n        if self.axis == 0:\n            out = out.T\n\n        return out\n\n    def _wrap_applied_output(\n        self,\n        data: DataFrame,\n        values: list,\n        not_indexed_same: bool = False,\n        is_transform: bool = False,\n    ):\n        if len(values) == 0:\n            if is_transform:\n                # GH#47787 see test_group_on_empty_multiindex\n                res_index = data.index\n            else:\n                res_index = self._grouper.result_index\n\n            result = self.obj._constructor(index=res_index, columns=data.columns)\n            result = result.astype(data.dtypes, copy=False)\n            return result\n\n        # GH12824\n        # using values[0] here breaks test_groupby_apply_none_first\n        first_not_none = next(com.not_none(*values), None)\n\n        if first_not_none is None:\n            # GH9684 - All values are None, return an empty frame.\n            return self.obj._constructor()\n        elif isinstance(first_not_none, DataFrame):\n            return self._concat_objects(\n                values,\n                not_indexed_same=not_indexed_same,\n                is_transform=is_transform,\n            )\n\n        key_index = self._grouper.result_index if self.as_index else None\n\n        if isinstance(first_not_none, (np.ndarray, Index)):\n            # GH#1738: values is list of arrays of unequal lengths\n            #  fall through to the outer else clause\n            # TODO: sure this is right?  we used to do this\n            #  after raising AttributeError above\n            # GH 18930\n            if not is_hashable(self._selection):\n                # error: Need type annotation for \"name\"\n                name = tuple(self._selection)  # type: ignore[var-annotated, arg-type]\n            else:\n                # error: Incompatible types in assignment\n                # (expression has type \"Hashable\", variable\n                # has type \"Tuple[Any, ...]\")\n                name = self._selection  # type: ignore[assignment]\n            return self.obj._constructor_sliced(values, index=key_index, name=name)\n        elif not isinstance(first_not_none, Series):\n            # values are not series or array-like but scalars\n            # self._selection not passed through to Series as the\n            # result should not take the name of original selection\n            # of columns\n            if self.as_index:\n                return self.obj._constructor_sliced(values, index=key_index)\n            else:\n                result = self.obj._constructor(values, columns=[self._selection])\n                result = self._insert_inaxis_grouper(result)\n                return result\n        else:\n            # values are Series\n            return self._wrap_applied_output_series(\n                values,\n                not_indexed_same,\n                first_not_none,\n                key_index,\n                is_transform,\n            )\n\n    def _wrap_applied_output_series(\n        self,\n        values: list[Series],\n        not_indexed_same: bool,\n        first_not_none,\n        key_index: Index | None,\n        is_transform: bool,\n    ) -> DataFrame | Series:\n        kwargs = first_not_none._construct_axes_dict()\n        backup = Series(**kwargs)\n        values = [x if (x is not None) else backup for x in values]\n\n        all_indexed_same = all_indexes_same(x.index for x in values)\n\n        if not all_indexed_same:\n            # GH 8467\n            return self._concat_objects(\n                values,\n                not_indexed_same=True,\n                is_transform=is_transform,\n            )\n\n        # Combine values\n        # vstack+constructor is faster than concat and handles MI-columns\n        stacked_values = np.vstack([np.asarray(v) for v in values])\n\n        if self.axis == 0:\n            index = key_index\n            columns = first_not_none.index.copy()\n            if columns.name is None:\n                # GH6124 - propagate name of Series when it's consistent\n                names = {v.name for v in values}\n                if len(names) == 1:\n                    columns.name = next(iter(names))\n        else:\n            index = first_not_none.index\n            columns = key_index\n            stacked_values = stacked_values.T\n\n        if stacked_values.dtype == object:\n            # We'll have the DataFrame constructor do inference\n            stacked_values = stacked_values.tolist()\n        result = self.obj._constructor(stacked_values, index=index, columns=columns)\n\n        if not self.as_index:\n            result = self._insert_inaxis_grouper(result)\n\n        return self._reindex_output(result)\n\n    def _cython_transform(\n        self,\n        how: str,\n        numeric_only: bool = False,\n        axis: AxisInt = 0,\n        **kwargs,\n    ) -> DataFrame:\n        assert axis == 0  # handled by caller\n\n        # With self.axis == 0, we have multi-block tests\n        #  e.g. test_rank_min_int, test_cython_transform_frame\n        #  test_transform_numeric_ret\n        # With self.axis == 1, _get_data_to_aggregate does a transpose\n        #  so we always have a single block.\n        mgr: Manager2D = self._get_data_to_aggregate(\n            numeric_only=numeric_only, name=how\n        )\n\n        def arr_func(bvalues: ArrayLike) -> ArrayLike:\n            return self._grouper._cython_operation(\n                \"transform\", bvalues, how, 1, **kwargs\n            )\n\n        # We could use `mgr.apply` here and not have to set_axis, but\n        #  we would have to do shape gymnastics for ArrayManager compat\n        res_mgr = mgr.grouped_reduce(arr_func)\n        res_mgr.set_axis(1, mgr.axes[1])\n\n        res_df = self.obj._constructor_from_mgr(res_mgr, axes=res_mgr.axes)\n        res_df = self._maybe_transpose_result(res_df)\n        return res_df\n\n    def _transform_general(self, func, engine, engine_kwargs, *args, **kwargs):\n        if maybe_use_numba(engine):\n            return self._transform_with_numba(\n                func, *args, engine_kwargs=engine_kwargs, **kwargs\n            )\n        from pandas.core.reshape.concat import concat\n\n        applied = []\n        obj = self._obj_with_exclusions\n        gen = self._grouper.get_iterator(obj, axis=self.axis)\n        fast_path, slow_path = self._define_paths(func, *args, **kwargs)\n\n        # Determine whether to use slow or fast path by evaluating on the first group.\n        # Need to handle the case of an empty generator and process the result so that\n        # it does not need to be computed again.\n        try:\n            name, group = next(gen)\n        except StopIteration:\n            pass\n        else:\n            # 2023-02-27 No tests broken by disabling this pinning\n            object.__setattr__(group, \"name\", name)\n            try:\n                path, res = self._choose_path(fast_path, slow_path, group)\n            except ValueError as err:\n                # e.g. test_transform_with_non_scalar_group\n                msg = \"transform must return a scalar value for each group\"\n                raise ValueError(msg) from err\n            if group.size > 0:\n                res = _wrap_transform_general_frame(self.obj, group, res)\n                applied.append(res)\n\n        # Compute and process with the remaining groups\n        for name, group in gen:\n            if group.size == 0:\n                continue\n            # 2023-02-27 No tests broken by disabling this pinning\n            object.__setattr__(group, \"name\", name)\n            res = path(group)\n\n            res = _wrap_transform_general_frame(self.obj, group, res)\n            applied.append(res)\n\n        concat_index = obj.columns if self.axis == 0 else obj.index\n        other_axis = 1 if self.axis == 0 else 0  # switches between 0 & 1\n        concatenated = concat(applied, axis=self.axis, verify_integrity=False)\n        concatenated = concatenated.reindex(concat_index, axis=other_axis, copy=False)\n        return self._set_result_index_ordered(concatenated)\n\n    __examples_dataframe_doc = dedent(\n        \"\"\"\n    >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n    ...                           'foo', 'bar'],\n    ...                    'B' : ['one', 'one', 'two', 'three',\n    ...                           'two', 'two'],\n    ...                    'C' : [1, 5, 5, 2, 5, 5],\n    ...                    'D' : [2.0, 5., 8., 1., 2., 9.]})\n    >>> grouped = df.groupby('A')[['C', 'D']]\n    >>> grouped.transform(lambda x: (x - x.mean()) / x.std())\n            C         D\n    0 -1.154701 -0.577350\n    1  0.577350  0.000000\n    2  0.577350  1.154701\n    3 -1.154701 -1.000000\n    4  0.577350 -0.577350\n    5  0.577350  1.000000\n\n    Broadcast result of the transformation\n\n    >>> grouped.transform(lambda x: x.max() - x.min())\n        C    D\n    0  4.0  6.0\n    1  3.0  8.0\n    2  4.0  6.0\n    3  3.0  8.0\n    4  4.0  6.0\n    5  3.0  8.0\n\n    >>> grouped.transform(\"mean\")\n        C    D\n    0  3.666667  4.0\n    1  4.000000  5.0\n    2  3.666667  4.0\n    3  4.000000  5.0\n    4  3.666667  4.0\n    5  4.000000  5.0\n\n    .. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``,\n    for example:\n\n    >>> grouped.transform(lambda x: x.astype(int).max())\n    C  D\n    0  5  8\n    1  5  9\n    2  5  8\n    3  5  9\n    4  5  8\n    5  5  9\n    \"\"\"\n    )\n\n    @Substitution(klass=\"DataFrame\", example=__examples_dataframe_doc)\n    @Appender(_transform_template)\n    def transform(self, func, *args, engine=None, engine_kwargs=None, **kwargs):\n        return self._transform(\n            func, *args, engine=engine, engine_kwargs=engine_kwargs, **kwargs\n        )\n\n    def _define_paths(self, func, *args, **kwargs):\n        if isinstance(func, str):\n            fast_path = lambda group: getattr(group, func)(*args, **kwargs)\n            slow_path = lambda group: group.apply(\n                lambda x: getattr(x, func)(*args, **kwargs), axis=self.axis\n            )\n        else:\n            fast_path = lambda group: func(group, *args, **kwargs)\n            slow_path = lambda group: group.apply(\n                lambda x: func(x, *args, **kwargs), axis=self.axis\n            )\n        return fast_path, slow_path\n\n    def _choose_path(self, fast_path: Callable, slow_path: Callable, group: DataFrame):\n        path = slow_path\n        res = slow_path(group)\n\n        if self.ngroups == 1:\n            # no need to evaluate multiple paths when only\n            # a single group exists\n            return path, res\n\n        # if we make it here, test if we can use the fast path\n        try:\n            res_fast = fast_path(group)\n        except AssertionError:\n            raise  # pragma: no cover\n        except Exception:\n            # GH#29631 For user-defined function, we can't predict what may be\n            #  raised; see test_transform.test_transform_fastpath_raises\n            return path, res\n\n        # verify fast path returns either:\n        # a DataFrame with columns equal to group.columns\n        # OR a Series with index equal to group.columns\n        if isinstance(res_fast, DataFrame):\n            if not res_fast.columns.equals(group.columns):\n                return path, res\n        elif isinstance(res_fast, Series):\n            if not res_fast.index.equals(group.columns):\n                return path, res\n        else:\n            return path, res\n\n        if res_fast.equals(res):\n            path = fast_path\n\n        return path, res\n\n    def filter(self, func, dropna: bool = True, *args, **kwargs):\n        \"\"\"\n        Filter elements from groups that don't satisfy a criterion.\n\n        Elements from groups are filtered if they do not satisfy the\n        boolean criterion specified by func.\n\n        Parameters\n        ----------\n        func : function\n            Criterion to apply to each group. Should return True or False.\n        dropna : bool\n            Drop groups that do not pass the filter. True by default; if False,\n            groups that evaluate False are filled with NaNs.\n\n        Returns\n        -------\n        DataFrame\n\n        Notes\n        -----\n        Each subframe is endowed the attribute 'name' in case you need to know\n        which group you are working on.\n\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n        for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n        ...                           'foo', 'bar'],\n        ...                    'B' : [1, 2, 3, 4, 5, 6],\n        ...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n        >>> grouped = df.groupby('A')\n        >>> grouped.filter(lambda x: x['B'].mean() > 3.)\n             A  B    C\n        1  bar  2  5.0\n        3  bar  4  1.0\n        5  bar  6  9.0\n        \"\"\"\n        indices = []\n\n        obj = self._selected_obj\n        gen = self._grouper.get_iterator(obj, axis=self.axis)\n\n        for name, group in gen:\n            # 2023-02-27 no tests are broken this pinning, but it is documented in the\n            #  docstring above.\n            object.__setattr__(group, \"name\", name)\n\n            res = func(group, *args, **kwargs)\n\n            try:\n                res = res.squeeze()\n            except AttributeError:  # allow e.g., scalars and frames to pass\n                pass\n\n            # interpret the result of the filter\n            if is_bool(res) or (is_scalar(res) and isna(res)):\n                if notna(res) and res:\n                    indices.append(self._get_index(name))\n            else:\n                # non scalars aren't allowed\n                raise TypeError(\n                    f\"filter function returned a {type(res).__name__}, \"\n                    \"but expected a scalar bool\"\n                )\n\n        return self._apply_filter(indices, dropna)\n\n    def __getitem__(self, key) -> DataFrameGroupBy | SeriesGroupBy:\n        if self.axis == 1:\n            # GH 37725\n            raise ValueError(\"Cannot subset columns when using axis=1\")\n        # per GH 23566\n        if isinstance(key, tuple) and len(key) > 1:\n            # if len == 1, then it becomes a SeriesGroupBy and this is actually\n            # valid syntax, so don't raise\n            raise ValueError(\n                \"Cannot subset columns with a tuple with more than one element. \"\n                \"Use a list instead.\"\n            )\n        return super().__getitem__(key)\n\n    def _gotitem(self, key, ndim: int, subset=None):\n        \"\"\"\n        sub-classes to define\n        return a sliced object\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        if ndim == 2:\n            if subset is None:\n                subset = self.obj\n            return DataFrameGroupBy(\n                subset,\n                self.keys,\n                axis=self.axis,\n                level=self.level,\n                grouper=self._grouper,\n                exclusions=self.exclusions,\n                selection=key,\n                as_index=self.as_index,\n                sort=self.sort,\n                group_keys=self.group_keys,\n                observed=self.observed,\n                dropna=self.dropna,\n            )\n        elif ndim == 1:\n            if subset is None:\n                subset = self.obj[key]\n            return SeriesGroupBy(\n                subset,\n                self.keys,\n                level=self.level,\n                grouper=self._grouper,\n                exclusions=self.exclusions,\n                selection=key,\n                as_index=self.as_index,\n                sort=self.sort,\n                group_keys=self.group_keys,\n                observed=self.observed,\n                dropna=self.dropna,\n            )\n\n        raise AssertionError(\"invalid ndim for _gotitem\")\n\n    def _get_data_to_aggregate(\n        self, *, numeric_only: bool = False, name: str | None = None\n    ) -> Manager2D:\n        obj = self._obj_with_exclusions\n        if self.axis == 1:\n            mgr = obj.T._mgr\n        else:\n            mgr = obj._mgr\n\n        if numeric_only:\n            mgr = mgr.get_numeric_data()\n        return mgr\n\n    def _wrap_agged_manager(self, mgr: Manager2D) -> DataFrame:\n        return self.obj._constructor_from_mgr(mgr, axes=mgr.axes)\n\n    def _apply_to_column_groupbys(self, func) -> DataFrame:\n        from pandas.core.reshape.concat import concat\n\n        obj = self._obj_with_exclusions\n        columns = obj.columns\n        sgbs = [\n            SeriesGroupBy(\n                obj.iloc[:, i],\n                selection=colname,\n                grouper=self._grouper,\n                exclusions=self.exclusions,\n                observed=self.observed,\n            )\n            for i, colname in enumerate(obj.columns)\n        ]\n        results = [func(sgb) for sgb in sgbs]\n\n        if not len(results):\n            # concat would raise\n            res_df = DataFrame([], columns=columns, index=self._grouper.result_index)\n        else:\n            res_df = concat(results, keys=columns, axis=1)\n\n        if not self.as_index:\n            res_df.index = default_index(len(res_df))\n            res_df = self._insert_inaxis_grouper(res_df)\n        return res_df\n\n    def nunique(self, dropna: bool = True) -> DataFrame:\n        \"\"\"\n        Return DataFrame with counts of unique elements in each position.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        nunique: DataFrame\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',\n        ...                           'ham', 'ham'],\n        ...                    'value1': [1, 5, 5, 2, 5, 5],\n        ...                    'value2': list('abbaxy')})\n        >>> df\n             id  value1 value2\n        0  spam       1      a\n        1   egg       5      b\n        2   egg       5      b\n        3  spam       2      a\n        4   ham       5      x\n        5   ham       5      y\n\n        >>> df.groupby('id').nunique()\n              value1  value2\n        id\n        egg        1       1\n        ham        1       2\n        spam       2       1\n\n        Check for rows with the same id but conflicting values:\n\n        >>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())\n             id  value1 value2\n        0  spam       1      a\n        3  spam       2      a\n        4   ham       5      x\n        5   ham       5      y\n        \"\"\"\n\n        if self.axis != 0:\n            # see test_groupby_crash_on_nunique\n            return self._python_apply_general(\n                lambda sgb: sgb.nunique(dropna), self._obj_with_exclusions, is_agg=True\n            )\n\n        return self._apply_to_column_groupbys(lambda sgb: sgb.nunique(dropna))\n\n    def idxmax(\n        self,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        skipna: bool = True,\n        numeric_only: bool = False,\n    ) -> DataFrame:\n        \"\"\"\n        Return index of first occurrence of maximum over requested axis.\n\n        NA/null values are excluded.\n\n        Parameters\n        ----------\n        axis : {{0 or 'index', 1 or 'columns'}}, default None\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n            If axis is not provided, grouper's axis is used.\n\n            .. versionchanged:: 2.0.0\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n        Returns\n        -------\n        Series\n            Indexes of maxima along the specified axis.\n\n        Raises\n        ------\n        ValueError\n            * If the row/column is empty\n\n        See Also\n        --------\n        Series.idxmax : Return index of the maximum element.\n\n        Notes\n        -----\n        This method is the DataFrame version of ``ndarray.argmax``.\n\n        Examples\n        --------\n        Consider a dataset containing food consumption in Argentina.\n\n        >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n        ...                    'co2_emissions': [37.2, 19.66, 1712]},\n        ...                   index=['Pork', 'Wheat Products', 'Beef'])\n\n        >>> df\n                        consumption  co2_emissions\n        Pork                  10.51         37.20\n        Wheat Products       103.11         19.66\n        Beef                  55.48       1712.00\n\n        By default, it returns the index for the maximum value in each column.\n\n        >>> df.idxmax()\n        consumption     Wheat Products\n        co2_emissions             Beef\n        dtype: object\n\n        To return the index for the maximum value in each row, use ``axis=\"columns\"``.\n\n        >>> df.idxmax(axis=\"columns\")\n        Pork              co2_emissions\n        Wheat Products     consumption\n        Beef              co2_emissions\n        dtype: object\n        \"\"\"\n        return self._idxmax_idxmin(\n            \"idxmax\", axis=axis, numeric_only=numeric_only, skipna=skipna\n        )\n\n    def idxmin(\n        self,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        skipna: bool = True,\n        numeric_only: bool = False,\n    ) -> DataFrame:\n        \"\"\"\n        Return index of first occurrence of minimum over requested axis.\n\n        NA/null values are excluded.\n\n        Parameters\n        ----------\n        axis : {{0 or 'index', 1 or 'columns'}}, default None\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n            If axis is not provided, grouper's axis is used.\n\n            .. versionchanged:: 2.0.0\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n        Returns\n        -------\n        Series\n            Indexes of minima along the specified axis.\n\n        Raises\n        ------\n        ValueError\n            * If the row/column is empty\n\n        See Also\n        --------\n        Series.idxmin : Return index of the minimum element.\n\n        Notes\n        -----\n        This method is the DataFrame version of ``ndarray.argmin``.\n\n        Examples\n        --------\n        Consider a dataset containing food consumption in Argentina.\n\n        >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n        ...                    'co2_emissions': [37.2, 19.66, 1712]},\n        ...                   index=['Pork', 'Wheat Products', 'Beef'])\n\n        >>> df\n                        consumption  co2_emissions\n        Pork                  10.51         37.20\n        Wheat Products       103.11         19.66\n        Beef                  55.48       1712.00\n\n        By default, it returns the index for the minimum value in each column.\n\n        >>> df.idxmin()\n        consumption                Pork\n        co2_emissions    Wheat Products\n        dtype: object\n\n        To return the index for the minimum value in each row, use ``axis=\"columns\"``.\n\n        >>> df.idxmin(axis=\"columns\")\n        Pork                consumption\n        Wheat Products    co2_emissions\n        Beef                consumption\n        dtype: object\n        \"\"\"\n        return self._idxmax_idxmin(\n            \"idxmin\", axis=axis, numeric_only=numeric_only, skipna=skipna\n        )\n\n    boxplot = boxplot_frame_groupby\n\n    def value_counts(\n        self,\n        subset: Sequence[Hashable] | None = None,\n        normalize: bool = False,\n        sort: bool = True,\n        ascending: bool = False,\n        dropna: bool = True,\n    ) -> DataFrame | Series:\n        \"\"\"\n        Return a Series or DataFrame containing counts of unique rows.\n\n        .. versionadded:: 1.4.0\n\n        Parameters\n        ----------\n        subset : list-like, optional\n            Columns to use when counting unique combinations.\n        normalize : bool, default False\n            Return proportions rather than frequencies.\n        sort : bool, default True\n            Sort by frequencies.\n        ascending : bool, default False\n            Sort in ascending order.\n        dropna : bool, default True\n            Don't include counts of rows that contain NA values.\n\n        Returns\n        -------\n        Series or DataFrame\n            Series if the groupby as_index is True, otherwise DataFrame.\n\n        See Also\n        --------\n        Series.value_counts: Equivalent method on Series.\n        DataFrame.value_counts: Equivalent method on DataFrame.\n        SeriesGroupBy.value_counts: Equivalent method on SeriesGroupBy.\n\n        Notes\n        -----\n        - If the groupby as_index is True then the returned Series will have a\n          MultiIndex with one level per input column.\n        - If the groupby as_index is False then the returned DataFrame will have an\n          additional column with the value_counts. The column is labelled 'count' or\n          'proportion', depending on the ``normalize`` parameter.\n\n        By default, rows that contain any NA values are omitted from\n        the result.\n\n        By default, the result will be in descending order so that the\n        first element of each group is the most frequently-occurring row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\n        ...     'gender': ['male', 'male', 'female', 'male', 'female', 'male'],\n        ...     'education': ['low', 'medium', 'high', 'low', 'high', 'low'],\n        ...     'country': ['US', 'FR', 'US', 'FR', 'FR', 'FR']\n        ... })\n\n        >>> df\n                gender  education   country\n        0       male    low         US\n        1       male    medium      FR\n        2       female  high        US\n        3       male    low         FR\n        4       female  high        FR\n        5       male    low         FR\n\n        >>> df.groupby('gender').value_counts()\n        gender  education  country\n        female  high       FR         1\n                           US         1\n        male    low        FR         2\n                           US         1\n                medium     FR         1\n        Name: count, dtype: int64\n\n        >>> df.groupby('gender').value_counts(ascending=True)\n        gender  education  country\n        female  high       FR         1\n                           US         1\n        male    low        US         1\n                medium     FR         1\n                low        FR         2\n        Name: count, dtype: int64\n\n        >>> df.groupby('gender').value_counts(normalize=True)\n        gender  education  country\n        female  high       FR         0.50\n                           US         0.50\n        male    low        FR         0.50\n                           US         0.25\n                medium     FR         0.25\n        Name: proportion, dtype: float64\n\n        >>> df.groupby('gender', as_index=False).value_counts()\n           gender education country  count\n        0  female      high      FR      1\n        1  female      high      US      1\n        2    male       low      FR      2\n        3    male       low      US      1\n        4    male    medium      FR      1\n\n        >>> df.groupby('gender', as_index=False).value_counts(normalize=True)\n           gender education country  proportion\n        0  female      high      FR        0.50\n        1  female      high      US        0.50\n        2    male       low      FR        0.50\n        3    male       low      US        0.25\n        4    male    medium      FR        0.25\n        \"\"\"\n        return self._value_counts(subset, normalize, sort, ascending, dropna)\n\n    def fillna(\n        self,\n        value: Hashable | Mapping | Series | DataFrame | None = None,\n        method: FillnaOptions | None = None,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        inplace: bool = False,\n        limit: int | None = None,\n        downcast=lib.no_default,\n    ) -> DataFrame | None:\n        \"\"\"\n        Fill NA/NaN values using the specified method within groups.\n\n        .. deprecated:: 2.2.0\n            This method is deprecated and will be removed in a future version.\n            Use the :meth:`.DataFrameGroupBy.ffill` or :meth:`.DataFrameGroupBy.bfill`\n            for forward or backward filling instead. If you want to fill with a\n            single value, use :meth:`DataFrame.fillna` instead.\n\n        Parameters\n        ----------\n        value : scalar, dict, Series, or DataFrame\n            Value to use to fill holes (e.g. 0), alternately a\n            dict/Series/DataFrame of values specifying which value to use for\n            each index (for a Series) or column (for a DataFrame).  Values not\n            in the dict/Series/DataFrame will not be filled. This value cannot\n            be a list. Users wanting to use the ``value`` argument and not ``method``\n            should prefer :meth:`.DataFrame.fillna` as this\n            will produce the same result and be more performant.\n        method : {{'bfill', 'ffill', None}}, default None\n            Method to use for filling holes. ``'ffill'`` will propagate\n            the last valid observation forward within a group.\n            ``'bfill'`` will use next valid observation to fill the gap.\n        axis : {0 or 'index', 1 or 'columns'}\n            Axis along which to fill missing values. When the :class:`DataFrameGroupBy`\n            ``axis`` argument is ``0``, using ``axis=1`` here will produce\n            the same results as :meth:`.DataFrame.fillna`. When the\n            :class:`DataFrameGroupBy` ``axis`` argument is ``1``, using ``axis=0``\n            or ``axis=1`` here will produce the same results.\n        inplace : bool, default False\n            Broken. Do not set to True.\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill within a group. In other words,\n            if there is a gap with more than this number of consecutive NaNs,\n            it will only be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled. Must be greater than 0 if not None.\n        downcast : dict, default is None\n            A dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible).\n\n        Returns\n        -------\n        DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        ffill : Forward fill values within a group.\n        bfill : Backward fill values within a group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"key\": [0, 0, 1, 1, 1],\n        ...         \"A\": [np.nan, 2, np.nan, 3, np.nan],\n        ...         \"B\": [2, 3, np.nan, np.nan, np.nan],\n        ...         \"C\": [np.nan, np.nan, 2, np.nan, np.nan],\n        ...     }\n        ... )\n        >>> df\n           key    A    B   C\n        0    0  NaN  2.0 NaN\n        1    0  2.0  3.0 NaN\n        2    1  NaN  NaN 2.0\n        3    1  3.0  NaN NaN\n        4    1  NaN  NaN NaN\n\n        Propagate non-null values forward or backward within each group along columns.\n\n        >>> df.groupby(\"key\").fillna(method=\"ffill\")\n             A    B   C\n        0  NaN  2.0 NaN\n        1  2.0  3.0 NaN\n        2  NaN  NaN 2.0\n        3  3.0  NaN 2.0\n        4  3.0  NaN 2.0\n\n        >>> df.groupby(\"key\").fillna(method=\"bfill\")\n             A    B   C\n        0  2.0  2.0 NaN\n        1  2.0  3.0 NaN\n        2  3.0  NaN 2.0\n        3  3.0  NaN NaN\n        4  NaN  NaN NaN\n\n        Propagate non-null values forward or backward within each group along rows.\n\n        >>> df.T.groupby(np.array([0, 0, 1, 1])).fillna(method=\"ffill\").T\n           key    A    B    C\n        0  0.0  0.0  2.0  2.0\n        1  0.0  2.0  3.0  3.0\n        2  1.0  1.0  NaN  2.0\n        3  1.0  3.0  NaN  NaN\n        4  1.0  1.0  NaN  NaN\n\n        >>> df.T.groupby(np.array([0, 0, 1, 1])).fillna(method=\"bfill\").T\n           key    A    B    C\n        0  0.0  NaN  2.0  NaN\n        1  0.0  2.0  3.0  NaN\n        2  1.0  NaN  2.0  2.0\n        3  1.0  3.0  NaN  NaN\n        4  1.0  NaN  NaN  NaN\n\n        Only replace the first NaN element within a group along rows.\n\n        >>> df.groupby(\"key\").fillna(method=\"ffill\", limit=1)\n             A    B    C\n        0  NaN  2.0  NaN\n        1  2.0  3.0  NaN\n        2  NaN  NaN  2.0\n        3  3.0  NaN  2.0\n        4  3.0  NaN  NaN\n        \"\"\"\n        warnings.warn(\n            f\"{type(self).__name__}.fillna is deprecated and \"\n            \"will be removed in a future version. Use obj.ffill() or obj.bfill() \"\n            \"for forward or backward filling instead. If you want to fill with a \"\n            f\"single value, use {type(self.obj).__name__}.fillna instead\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n\n        result = self._op_via_apply(\n            \"fillna\",\n            value=value,\n            method=method,\n            axis=axis,\n            inplace=inplace,\n            limit=limit,\n            downcast=downcast,\n        )\n        return result\n\n    def take(\n        self,\n        indices: TakeIndexer,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        **kwargs,\n    ) -> DataFrame:\n        \"\"\"\n        Return the elements in the given *positional* indices in each group.\n\n        This means that we are not indexing according to actual values in\n        the index attribute of the object. We are indexing according to the\n        actual position of the element in the object.\n\n        If a requested index does not exist for some group, this method will raise.\n        To get similar behavior that ignores indices that don't exist, see\n        :meth:`.DataFrameGroupBy.nth`.\n\n        Parameters\n        ----------\n        indices : array-like\n            An array of ints indicating which positions to take.\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            The axis on which to select elements. ``0`` means that we are\n            selecting rows, ``1`` means that we are selecting columns.\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        **kwargs\n            For compatibility with :meth:`numpy.take`. Has no effect on the\n            output.\n\n        Returns\n        -------\n        DataFrame\n            An DataFrame containing the elements taken from each group.\n\n        See Also\n        --------\n        DataFrame.take : Take elements from a Series along an axis.\n        DataFrame.loc : Select a subset of a DataFrame by labels.\n        DataFrame.iloc : Select a subset of a DataFrame by positions.\n        numpy.take : Take elements from an array along an axis.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n        ...                    ('parrot', 'bird', 24.0),\n        ...                    ('lion', 'mammal', 80.5),\n        ...                    ('monkey', 'mammal', np.nan),\n        ...                    ('rabbit', 'mammal', 15.0)],\n        ...                   columns=['name', 'class', 'max_speed'],\n        ...                   index=[4, 3, 2, 1, 0])\n        >>> df\n             name   class  max_speed\n        4  falcon    bird      389.0\n        3  parrot    bird       24.0\n        2    lion  mammal       80.5\n        1  monkey  mammal        NaN\n        0  rabbit  mammal       15.0\n        >>> gb = df.groupby([1, 1, 2, 2, 2])\n\n        Take elements at positions 0 and 1 along the axis 0 (default).\n\n        Note how the indices selected in the result do not correspond to\n        our input indices 0 and 1. That's because we are selecting the 0th\n        and 1st rows, not rows whose indices equal 0 and 1.\n\n        >>> gb.take([0, 1])\n               name   class  max_speed\n        1 4  falcon    bird      389.0\n          3  parrot    bird       24.0\n        2 2    lion  mammal       80.5\n          1  monkey  mammal        NaN\n\n        The order of the specified indices influences the order in the result.\n        Here, the order is swapped from the previous example.\n\n        >>> gb.take([1, 0])\n               name   class  max_speed\n        1 3  parrot    bird       24.0\n          4  falcon    bird      389.0\n        2 1  monkey  mammal        NaN\n          2    lion  mammal       80.5\n\n        Take elements at indices 1 and 2 along the axis 1 (column selection).\n\n        We may take elements using negative integers for positive indices,\n        starting from the end of the object, just like with Python lists.\n\n        >>> gb.take([-1, -2])\n               name   class  max_speed\n        1 3  parrot    bird       24.0\n          4  falcon    bird      389.0\n        2 0  rabbit  mammal       15.0\n          1  monkey  mammal        NaN\n        \"\"\"\n        result = self._op_via_apply(\"take\", indices=indices, axis=axis, **kwargs)\n        return result\n\n    def skew(\n        self,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        skipna: bool = True,\n        numeric_only: bool = False,\n        **kwargs,\n    ) -> DataFrame:\n        \"\"\"\n        Return unbiased skew within groups.\n\n        Normalized by N-1.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            Axis for the function to be applied on.\n\n            Specifying ``axis=None`` will apply the aggregation across both axes.\n\n            .. versionadded:: 2.0.0\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        skipna : bool, default True\n            Exclude NA/null values when computing the result.\n\n        numeric_only : bool, default False\n            Include only float, int, boolean columns.\n\n        **kwargs\n            Additional keyword arguments to be passed to the function.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.skew : Return unbiased skew over requested axis.\n\n        Examples\n        --------\n        >>> arrays = [['falcon', 'parrot', 'cockatoo', 'kiwi',\n        ...            'lion', 'monkey', 'rabbit'],\n        ...           ['bird', 'bird', 'bird', 'bird',\n        ...            'mammal', 'mammal', 'mammal']]\n        >>> index = pd.MultiIndex.from_arrays(arrays, names=('name', 'class'))\n        >>> df = pd.DataFrame({'max_speed': [389.0, 24.0, 70.0, np.nan,\n        ...                                  80.5, 21.5, 15.0]},\n        ...                   index=index)\n        >>> df\n                        max_speed\n        name     class\n        falcon   bird        389.0\n        parrot   bird         24.0\n        cockatoo bird         70.0\n        kiwi     bird          NaN\n        lion     mammal       80.5\n        monkey   mammal       21.5\n        rabbit   mammal       15.0\n        >>> gb = df.groupby([\"class\"])\n        >>> gb.skew()\n                max_speed\n        class\n        bird     1.628296\n        mammal   1.669046\n        >>> gb.skew(skipna=False)\n                max_speed\n        class\n        bird          NaN\n        mammal   1.669046\n        \"\"\"\n        if axis is lib.no_default:\n            axis = 0\n\n        if axis != 0:\n            result = self._op_via_apply(\n                \"skew\",\n                axis=axis,\n                skipna=skipna,\n                numeric_only=numeric_only,\n                **kwargs,\n            )\n            return result\n\n        def alt(obj):\n            # This should not be reached since the cython path should raise\n            #  TypeError and not NotImplementedError.\n            raise TypeError(f\"'skew' is not supported for dtype={obj.dtype}\")\n\n        return self._cython_agg_general(\n            \"skew\", alt=alt, skipna=skipna, numeric_only=numeric_only, **kwargs\n        )\n\n    @property\n    @doc(DataFrame.plot.__doc__)\n    def plot(self) -> GroupByPlot:\n        result = GroupByPlot(self)\n        return result\n\n    @doc(DataFrame.corr.__doc__)\n    def corr(\n        self,\n        method: str | Callable[[np.ndarray, np.ndarray], float] = \"pearson\",\n        min_periods: int = 1,\n        numeric_only: bool = False,\n    ) -> DataFrame:\n        result = self._op_via_apply(\n            \"corr\", method=method, min_periods=min_periods, numeric_only=numeric_only\n        )\n        return result\n\n    @doc(DataFrame.cov.__doc__)\n    def cov(\n        self,\n        min_periods: int | None = None,\n        ddof: int | None = 1,\n        numeric_only: bool = False,\n    ) -> DataFrame:\n        result = self._op_via_apply(\n            \"cov\", min_periods=min_periods, ddof=ddof, numeric_only=numeric_only\n        )\n        return result\n\n    @doc(DataFrame.hist.__doc__)\n    def hist(\n        self,\n        column: IndexLabel | None = None,\n        by=None,\n        grid: bool = True,\n        xlabelsize: int | None = None,\n        xrot: float | None = None,\n        ylabelsize: int | None = None,\n        yrot: float | None = None,\n        ax=None,\n        sharex: bool = False,\n        sharey: bool = False,\n        figsize: tuple[int, int] | None = None,\n        layout: tuple[int, int] | None = None,\n        bins: int | Sequence[int] = 10,\n        backend: str | None = None,\n        legend: bool = False,\n        **kwargs,\n    ):\n        result = self._op_via_apply(\n            \"hist\",\n            column=column,\n            by=by,\n            grid=grid,\n            xlabelsize=xlabelsize,\n            xrot=xrot,\n            ylabelsize=ylabelsize,\n            yrot=yrot,\n            ax=ax,\n            sharex=sharex,\n            sharey=sharey,\n            figsize=figsize,\n            layout=layout,\n            bins=bins,\n            backend=backend,\n            legend=legend,\n            **kwargs,\n        )\n        return result\n\n    @property\n    @doc(DataFrame.dtypes.__doc__)\n    def dtypes(self) -> Series:\n        # GH#51045\n        warnings.warn(\n            f\"{type(self).__name__}.dtypes is deprecated and will be removed in \"\n            \"a future version. Check the dtypes on the base object instead\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n\n        # error: Incompatible return value type (got \"DataFrame\", expected \"Series\")\n        return self._python_apply_general(  # type: ignore[return-value]\n            lambda df: df.dtypes, self._selected_obj\n        )\n\n    @doc(DataFrame.corrwith.__doc__)\n    def corrwith(\n        self,\n        other: DataFrame | Series,\n        axis: Axis | lib.NoDefault = lib.no_default,\n        drop: bool = False,\n        method: CorrelationMethod = \"pearson\",\n        numeric_only: bool = False,\n    ) -> DataFrame:\n        result = self._op_via_apply(\n            \"corrwith\",\n            other=other,\n            axis=axis,\n            drop=drop,\n            method=method,\n            numeric_only=numeric_only,\n        )\n        return result\n", "class_fn": true, "question_id": "pandas/pandas.core.groupby.generic/DataFrameGroupBy", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/groupby.py", "fn_id": "", "content": "class GroupBy(BaseGroupBy[NDFrameT]):\n    \"\"\"\n    Class for grouping and aggregating relational data.\n\n    See aggregate, transform, and apply functions on this object.\n\n    It's easiest to use obj.groupby(...) to use GroupBy, but you can also do:\n\n    ::\n\n        grouped = groupby(obj, ...)\n\n    Parameters\n    ----------\n    obj : pandas object\n    axis : int, default 0\n    level : int, default None\n        Level of MultiIndex\n    groupings : list of Grouping objects\n        Most users should ignore this\n    exclusions : array-like, optional\n        List of columns to exclude\n    name : str\n        Most users should ignore this\n\n    Returns\n    -------\n    **Attributes**\n    groups : dict\n        {group name -> group labels}\n    len(grouped) : int\n        Number of groups\n\n    Notes\n    -----\n    After grouping, see aggregate, apply, and transform functions. Here are\n    some other brief notes about usage. When grouping by multiple groups, the\n    result index will be a MultiIndex (hierarchical) by default.\n\n    Iteration produces (key, group) tuples, i.e. chunking the data by group. So\n    you can write code like:\n\n    ::\n\n        grouped = obj.groupby(keys, axis=axis)\n        for key, group in grouped:\n            # do something with the data\n\n    Function calls on GroupBy, if not specially implemented, \"dispatch\" to the\n    grouped data. So if you group a DataFrame and wish to invoke the std()\n    method on each group, you can simply do:\n\n    ::\n\n        df.groupby(mapper).std()\n\n    rather than\n\n    ::\n\n        df.groupby(mapper).aggregate(np.std)\n\n    You can pass arguments to these \"wrapped\" functions, too.\n\n    See the online documentation for full exposition on these topics and much\n    more\n    \"\"\"\n\n    _grouper: ops.BaseGrouper\n    as_index: bool\n\n    @final\n    def __init__(\n        self,\n        obj: NDFrameT,\n        keys: _KeysArgType | None = None,\n        axis: Axis = 0,\n        level: IndexLabel | None = None,\n        grouper: ops.BaseGrouper | None = None,\n        exclusions: frozenset[Hashable] | None = None,\n        selection: IndexLabel | None = None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        observed: bool | lib.NoDefault = lib.no_default,\n        dropna: bool = True,\n    ) -> None:\n        self._selection = selection\n\n        assert isinstance(obj, NDFrame), type(obj)\n\n        self.level = level\n\n        if not as_index:\n            if axis != 0:\n                raise ValueError(\"as_index=False only valid for axis=0\")\n\n        self.as_index = as_index\n        self.keys = keys\n        self.sort = sort\n        self.group_keys = group_keys\n        self.dropna = dropna\n\n        if grouper is None:\n            grouper, exclusions, obj = get_grouper(\n                obj,\n                keys,\n                axis=axis,\n                level=level,\n                sort=sort,\n                observed=False if observed is lib.no_default else observed,\n                dropna=self.dropna,\n            )\n\n        if observed is lib.no_default:\n            if any(ping._passed_categorical for ping in grouper.groupings):\n                warnings.warn(\n                    \"The default of observed=False is deprecated and will be changed \"\n                    \"to True in a future version of pandas. Pass observed=False to \"\n                    \"retain current behavior or observed=True to adopt the future \"\n                    \"default and silence this warning.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n            observed = False\n        self.observed = observed\n\n        self.obj = obj\n        self.axis = obj._get_axis_number(axis)\n        self._grouper = grouper\n        self.exclusions = frozenset(exclusions) if exclusions else frozenset()\n\n    def __getattr__(self, attr: str):\n        if attr in self._internal_names_set:\n            return object.__getattribute__(self, attr)\n        if attr in self.obj:\n            return self[attr]\n\n        raise AttributeError(\n            f\"'{type(self).__name__}' object has no attribute '{attr}'\"\n        )\n\n    @final\n    def _deprecate_axis(self, axis: int, name: str) -> None:\n        if axis == 1:\n            warnings.warn(\n                f\"{type(self).__name__}.{name} with axis=1 is deprecated and \"\n                \"will be removed in a future version. Operate on the un-grouped \"\n                \"DataFrame instead\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        else:\n            warnings.warn(\n                f\"The 'axis' keyword in {type(self).__name__}.{name} is deprecated \"\n                \"and will be removed in a future version. \"\n                \"Call without passing 'axis' instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n    @final\n    def _op_via_apply(self, name: str, *args, **kwargs):\n        \"\"\"Compute the result of an operation by using GroupBy's apply.\"\"\"\n        f = getattr(type(self._obj_with_exclusions), name)\n        sig = inspect.signature(f)\n\n        if \"axis\" in kwargs and kwargs[\"axis\"] is not lib.no_default:\n            axis = self.obj._get_axis_number(kwargs[\"axis\"])\n            self._deprecate_axis(axis, name)\n        elif \"axis\" in kwargs:\n            # exclude skew here because that was already defaulting to lib.no_default\n            #  before this deprecation was instituted\n            if name == \"skew\":\n                pass\n            elif name == \"fillna\":\n                # maintain the behavior from before the deprecation\n                kwargs[\"axis\"] = None\n            else:\n                kwargs[\"axis\"] = 0\n\n        # a little trickery for aggregation functions that need an axis\n        # argument\n        if \"axis\" in sig.parameters:\n            if kwargs.get(\"axis\", None) is None or kwargs.get(\"axis\") is lib.no_default:\n                kwargs[\"axis\"] = self.axis\n\n        def curried(x):\n            return f(x, *args, **kwargs)\n\n        # preserve the name so we can detect it when calling plot methods,\n        # to avoid duplicates\n        curried.__name__ = name\n\n        # special case otherwise extra plots are created when catching the\n        # exception below\n        if name in base.plotting_methods:\n            return self._python_apply_general(curried, self._selected_obj)\n\n        is_transform = name in base.transformation_kernels\n        result = self._python_apply_general(\n            curried,\n            self._obj_with_exclusions,\n            is_transform=is_transform,\n            not_indexed_same=not is_transform,\n        )\n\n        if self._grouper.has_dropped_na and is_transform:\n            # result will have dropped rows due to nans, fill with null\n            # and ensure index is ordered same as the input\n            result = self._set_result_index_ordered(result)\n        return result\n\n    # -----------------------------------------------------------------\n    # Dispatch/Wrapping\n\n    @final\n    def _concat_objects(\n        self,\n        values,\n        not_indexed_same: bool = False,\n        is_transform: bool = False,\n    ):\n        from pandas.core.reshape.concat import concat\n\n        if self.group_keys and not is_transform:\n            if self.as_index:\n                # possible MI return case\n                group_keys = self._grouper.result_index\n                group_levels = self._grouper.levels\n                group_names = self._grouper.names\n\n                result = concat(\n                    values,\n                    axis=self.axis,\n                    keys=group_keys,\n                    levels=group_levels,\n                    names=group_names,\n                    sort=False,\n                )\n            else:\n                # GH5610, returns a MI, with the first level being a\n                # range index\n                keys = list(range(len(values)))\n                result = concat(values, axis=self.axis, keys=keys)\n\n        elif not not_indexed_same:\n            result = concat(values, axis=self.axis)\n\n            ax = self._selected_obj._get_axis(self.axis)\n            if self.dropna:\n                labels = self._grouper.group_info[0]\n                mask = labels != -1\n                ax = ax[mask]\n\n            # this is a very unfortunate situation\n            # we can't use reindex to restore the original order\n            # when the ax has duplicates\n            # so we resort to this\n            # GH 14776, 30667\n            # TODO: can we reuse e.g. _reindex_non_unique?\n            if ax.has_duplicates and not result.axes[self.axis].equals(ax):\n                # e.g. test_category_order_transformer\n                target = algorithms.unique1d(ax._values)\n                indexer, _ = result.index.get_indexer_non_unique(target)\n                result = result.take(indexer, axis=self.axis)\n            else:\n                result = result.reindex(ax, axis=self.axis, copy=False)\n\n        else:\n            result = concat(values, axis=self.axis)\n\n        if self.obj.ndim == 1:\n            name = self.obj.name\n        elif is_hashable(self._selection):\n            name = self._selection\n        else:\n            name = None\n\n        if isinstance(result, Series) and name is not None:\n            result.name = name\n\n        return result\n\n    @final\n    def _set_result_index_ordered(\n        self, result: OutputFrameOrSeries\n    ) -> OutputFrameOrSeries:\n        # set the result index on the passed values object and\n        # return the new object, xref 8046\n\n        obj_axis = self.obj._get_axis(self.axis)\n\n        if self._grouper.is_monotonic and not self._grouper.has_dropped_na:\n            # shortcut if we have an already ordered grouper\n            result = result.set_axis(obj_axis, axis=self.axis, copy=False)\n            return result\n\n        # row order is scrambled => sort the rows by position in original index\n        original_positions = Index(self._grouper.result_ilocs())\n        result = result.set_axis(original_positions, axis=self.axis, copy=False)\n        result = result.sort_index(axis=self.axis)\n        if self._grouper.has_dropped_na:\n            # Add back in any missing rows due to dropna - index here is integral\n            # with values referring to the row of the input so can use RangeIndex\n            result = result.reindex(RangeIndex(len(obj_axis)), axis=self.axis)\n        result = result.set_axis(obj_axis, axis=self.axis, copy=False)\n\n        return result\n\n    @final\n    def _insert_inaxis_grouper(self, result: Series | DataFrame) -> DataFrame:\n        if isinstance(result, Series):\n            result = result.to_frame()\n\n        # zip in reverse so we can always insert at loc 0\n        columns = result.columns\n        for name, lev, in_axis in zip(\n            reversed(self._grouper.names),\n            reversed(self._grouper.get_group_levels()),\n            reversed([grp.in_axis for grp in self._grouper.groupings]),\n        ):\n            # GH #28549\n            # When using .apply(-), name will be in columns already\n            if name not in columns:\n                if in_axis:\n                    result.insert(0, name, lev)\n                else:\n                    msg = (\n                        \"A grouping was used that is not in the columns of the \"\n                        \"DataFrame and so was excluded from the result. This grouping \"\n                        \"will be included in a future version of pandas. Add the \"\n                        \"grouping as a column of the DataFrame to silence this warning.\"\n                    )\n                    warnings.warn(\n                        message=msg,\n                        category=FutureWarning,\n                        stacklevel=find_stack_level(),\n                    )\n\n        return result\n\n    @final\n    def _maybe_transpose_result(self, result: NDFrameT) -> NDFrameT:\n        if self.axis == 1:\n            # Only relevant for DataFrameGroupBy, no-op for SeriesGroupBy\n            result = result.T\n            if result.index.equals(self.obj.index):\n                # Retain e.g. DatetimeIndex/TimedeltaIndex freq\n                # e.g. test_groupby_crash_on_nunique\n                result.index = self.obj.index.copy()\n        return result\n\n    @final\n    def _wrap_aggregated_output(\n        self,\n        result: Series | DataFrame,\n        qs: npt.NDArray[np.float64] | None = None,\n    ):\n        \"\"\"\n        Wraps the output of GroupBy aggregations into the expected result.\n\n        Parameters\n        ----------\n        result : Series, DataFrame\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        # ATM we do not get here for SeriesGroupBy; when we do, we will\n        #  need to require that result.name already match self.obj.name\n\n        if not self.as_index:\n            # `not self.as_index` is only relevant for DataFrameGroupBy,\n            #   enforced in __init__\n            result = self._insert_inaxis_grouper(result)\n            result = result._consolidate()\n            index = Index(range(self._grouper.ngroups))\n\n        else:\n            index = self._grouper.result_index\n\n        if qs is not None:\n            # We get here with len(qs) != 1 and not self.as_index\n            #  in test_pass_args_kwargs\n            index = _insert_quantile_level(index, qs)\n\n        result.index = index\n\n        # error: Argument 1 to \"_maybe_transpose_result\" of \"GroupBy\" has\n        # incompatible type \"Union[Series, DataFrame]\"; expected \"NDFrameT\"\n        res = self._maybe_transpose_result(result)  # type: ignore[arg-type]\n        return self._reindex_output(res, qs=qs)\n\n    def _wrap_applied_output(\n        self,\n        data,\n        values: list,\n        not_indexed_same: bool = False,\n        is_transform: bool = False,\n    ):\n        raise AbstractMethodError(self)\n\n    # -----------------------------------------------------------------\n    # numba\n\n    @final\n    def _numba_prep(self, data: DataFrame):\n        ids, _, ngroups = self._grouper.group_info\n        sorted_index = self._grouper._sort_idx\n        sorted_ids = self._grouper._sorted_ids\n\n        sorted_data = data.take(sorted_index, axis=self.axis).to_numpy()\n        # GH 46867\n        index_data = data.index\n        if isinstance(index_data, MultiIndex):\n            if len(self._grouper.groupings) > 1:\n                raise NotImplementedError(\n                    \"Grouping with more than 1 grouping labels and \"\n                    \"a MultiIndex is not supported with engine='numba'\"\n                )\n            group_key = self._grouper.groupings[0].name\n            index_data = index_data.get_level_values(group_key)\n        sorted_index_data = index_data.take(sorted_index).to_numpy()\n\n        starts, ends = lib.generate_slices(sorted_ids, ngroups)\n        return (\n            starts,\n            ends,\n            sorted_index_data,\n            sorted_data,\n        )\n\n    def _numba_agg_general(\n        self,\n        func: Callable,\n        dtype_mapping: dict[np.dtype, Any],\n        engine_kwargs: dict[str, bool] | None,\n        **aggregator_kwargs,\n    ):\n        \"\"\"\n        Perform groupby with a standard numerical aggregation function (e.g. mean)\n        with Numba.\n        \"\"\"\n        if not self.as_index:\n            raise NotImplementedError(\n                \"as_index=False is not supported. Use .reset_index() instead.\"\n            )\n        if self.axis == 1:\n            raise NotImplementedError(\"axis=1 is not supported.\")\n\n        data = self._obj_with_exclusions\n        df = data if data.ndim == 2 else data.to_frame()\n\n        aggregator = executor.generate_shared_aggregator(\n            func,\n            dtype_mapping,\n            True,  # is_grouped_kernel\n            **get_jit_arguments(engine_kwargs),\n        )\n        # Pass group ids to kernel directly if it can handle it\n        # (This is faster since it doesn't require a sort)\n        ids, _, _ = self._grouper.group_info\n        ngroups = self._grouper.ngroups\n\n        res_mgr = df._mgr.apply(\n            aggregator, labels=ids, ngroups=ngroups, **aggregator_kwargs\n        )\n        res_mgr.axes[1] = self._grouper.result_index\n        result = df._constructor_from_mgr(res_mgr, axes=res_mgr.axes)\n\n        if data.ndim == 1:\n            result = result.squeeze(\"columns\")\n            result.name = data.name\n        else:\n            result.columns = data.columns\n        return result\n\n    @final\n    def _transform_with_numba(self, func, *args, engine_kwargs=None, **kwargs):\n        \"\"\"\n        Perform groupby transform routine with the numba engine.\n\n        This routine mimics the data splitting routine of the DataSplitter class\n        to generate the indices of each group in the sorted data and then passes the\n        data and indices into a Numba jitted function.\n        \"\"\"\n        data = self._obj_with_exclusions\n        df = data if data.ndim == 2 else data.to_frame()\n\n        starts, ends, sorted_index, sorted_data = self._numba_prep(df)\n        numba_.validate_udf(func)\n        numba_transform_func = numba_.generate_numba_transform_func(\n            func, **get_jit_arguments(engine_kwargs, kwargs)\n        )\n        result = numba_transform_func(\n            sorted_data,\n            sorted_index,\n            starts,\n            ends,\n            len(df.columns),\n            *args,\n        )\n        # result values needs to be resorted to their original positions since we\n        # evaluated the data sorted by group\n        result = result.take(np.argsort(sorted_index), axis=0)\n        index = data.index\n        if data.ndim == 1:\n            result_kwargs = {\"name\": data.name}\n            result = result.ravel()\n        else:\n            result_kwargs = {\"columns\": data.columns}\n        return data._constructor(result, index=index, **result_kwargs)\n\n    @final\n    def _aggregate_with_numba(self, func, *args, engine_kwargs=None, **kwargs):\n        \"\"\"\n        Perform groupby aggregation routine with the numba engine.\n\n        This routine mimics the data splitting routine of the DataSplitter class\n        to generate the indices of each group in the sorted data and then passes the\n        data and indices into a Numba jitted function.\n        \"\"\"\n        data = self._obj_with_exclusions\n        df = data if data.ndim == 2 else data.to_frame()\n\n        starts, ends, sorted_index, sorted_data = self._numba_prep(df)\n        numba_.validate_udf(func)\n        numba_agg_func = numba_.generate_numba_agg_func(\n            func, **get_jit_arguments(engine_kwargs, kwargs)\n        )\n        result = numba_agg_func(\n            sorted_data,\n            sorted_index,\n            starts,\n            ends,\n            len(df.columns),\n            *args,\n        )\n        index = self._grouper.result_index\n        if data.ndim == 1:\n            result_kwargs = {\"name\": data.name}\n            result = result.ravel()\n        else:\n            result_kwargs = {\"columns\": data.columns}\n        res = data._constructor(result, index=index, **result_kwargs)\n        if not self.as_index:\n            res = self._insert_inaxis_grouper(res)\n            res.index = default_index(len(res))\n        return res\n\n    # -----------------------------------------------------------------\n    # apply/agg/transform\n\n    @Appender(\n        _apply_docs[\"template\"].format(\n            input=\"dataframe\", examples=_apply_docs[\"dataframe_examples\"]\n        )\n    )\n    def apply(self, func, *args, include_groups: bool = True, **kwargs) -> NDFrameT:\n        orig_func = func\n        func = com.is_builtin_func(func)\n        if orig_func != func:\n            alias = com._builtin_table_alias[orig_func]\n            warn_alias_replacement(self, orig_func, alias)\n\n        if isinstance(func, str):\n            if hasattr(self, func):\n                res = getattr(self, func)\n                if callable(res):\n                    return res(*args, **kwargs)\n                elif args or kwargs:\n                    raise ValueError(f\"Cannot pass arguments to property {func}\")\n                return res\n\n            else:\n                raise TypeError(f\"apply func should be callable, not '{func}'\")\n\n        elif args or kwargs:\n            if callable(func):\n\n                @wraps(func)\n                def f(g):\n                    return func(g, *args, **kwargs)\n\n            else:\n                raise ValueError(\n                    \"func must be a callable if args or kwargs are supplied\"\n                )\n        else:\n            f = func\n\n        if not include_groups:\n            return self._python_apply_general(f, self._obj_with_exclusions)\n\n        # ignore SettingWithCopy here in case the user mutates\n        with option_context(\"mode.chained_assignment\", None):\n            try:\n                result = self._python_apply_general(f, self._selected_obj)\n                if (\n                    not isinstance(self.obj, Series)\n                    and self._selection is None\n                    and self._selected_obj.shape != self._obj_with_exclusions.shape\n                ):\n                    warnings.warn(\n                        message=_apply_groupings_depr.format(\n                            type(self).__name__, \"apply\"\n                        ),\n                        category=DeprecationWarning,\n                        stacklevel=find_stack_level(),\n                    )\n            except TypeError:\n                # gh-20949\n                # try again, with .apply acting as a filtering\n                # operation, by excluding the grouping column\n                # This would normally not be triggered\n                # except if the udf is trying an operation that\n                # fails on *some* columns, e.g. a numeric operation\n                # on a string grouper column\n\n                return self._python_apply_general(f, self._obj_with_exclusions)\n\n        return result\n\n    @final\n    def _python_apply_general(\n        self,\n        f: Callable,\n        data: DataFrame | Series,\n        not_indexed_same: bool | None = None,\n        is_transform: bool = False,\n        is_agg: bool = False,\n    ) -> NDFrameT:\n        \"\"\"\n        Apply function f in python space\n\n        Parameters\n        ----------\n        f : callable\n            Function to apply\n        data : Series or DataFrame\n            Data to apply f to\n        not_indexed_same: bool, optional\n            When specified, overrides the value of not_indexed_same. Apply behaves\n            differently when the result index is equal to the input index, but\n            this can be coincidental leading to value-dependent behavior.\n        is_transform : bool, default False\n            Indicator for whether the function is actually a transform\n            and should not have group keys prepended.\n        is_agg : bool, default False\n            Indicator for whether the function is an aggregation. When the\n            result is empty, we don't want to warn for this case.\n            See _GroupBy._python_agg_general.\n\n        Returns\n        -------\n        Series or DataFrame\n            data after applying f\n        \"\"\"\n        values, mutated = self._grouper.apply_groupwise(f, data, self.axis)\n        if not_indexed_same is None:\n            not_indexed_same = mutated\n\n        return self._wrap_applied_output(\n            data,\n            values,\n            not_indexed_same,\n            is_transform,\n        )\n\n    @final\n    def _agg_general(\n        self,\n        numeric_only: bool = False,\n        min_count: int = -1,\n        *,\n        alias: str,\n        npfunc: Callable | None = None,\n        **kwargs,\n    ):\n        result = self._cython_agg_general(\n            how=alias,\n            alt=npfunc,\n            numeric_only=numeric_only,\n            min_count=min_count,\n            **kwargs,\n        )\n        return result.__finalize__(self.obj, method=\"groupby\")\n\n    def _agg_py_fallback(\n        self, how: str, values: ArrayLike, ndim: int, alt: Callable\n    ) -> ArrayLike:\n        \"\"\"\n        Fallback to pure-python aggregation if _cython_operation raises\n        NotImplementedError.\n        \"\"\"\n        # We get here with a) EADtypes and b) object dtype\n        assert alt is not None\n\n        if values.ndim == 1:\n            # For DataFrameGroupBy we only get here with ExtensionArray\n            ser = Series(values, copy=False)\n        else:\n            # We only get here with values.dtype == object\n            df = DataFrame(values.T, dtype=values.dtype)\n            # bc we split object blocks in grouped_reduce, we have only 1 col\n            # otherwise we'd have to worry about block-splitting GH#39329\n            assert df.shape[1] == 1\n            # Avoid call to self.values that can occur in DataFrame\n            #  reductions; see GH#28949\n            ser = df.iloc[:, 0]\n\n        # We do not get here with UDFs, so we know that our dtype\n        #  should always be preserved by the implemented aggregations\n        # TODO: Is this exactly right; see WrappedCythonOp get_result_dtype?\n        try:\n            res_values = self._grouper.agg_series(ser, alt, preserve_dtype=True)\n        except Exception as err:\n            msg = f\"agg function failed [how->{how},dtype->{ser.dtype}]\"\n            # preserve the kind of exception that raised\n            raise type(err)(msg) from err\n\n        if ser.dtype == object:\n            res_values = res_values.astype(object, copy=False)\n\n        # If we are DataFrameGroupBy and went through a SeriesGroupByPath\n        # then we need to reshape\n        # GH#32223 includes case with IntegerArray values, ndarray res_values\n        # test_groupby_duplicate_columns with object dtype values\n        return ensure_block_shape(res_values, ndim=ndim)\n\n    @final\n    def _cython_agg_general(\n        self,\n        how: str,\n        alt: Callable | None = None,\n        numeric_only: bool = False,\n        min_count: int = -1,\n        **kwargs,\n    ):\n        # Note: we never get here with how=\"ohlc\" for DataFrameGroupBy;\n        #  that goes through SeriesGroupBy\n\n        data = self._get_data_to_aggregate(numeric_only=numeric_only, name=how)\n\n        def array_func(values: ArrayLike) -> ArrayLike:\n            try:\n                result = self._grouper._cython_operation(\n                    \"aggregate\",\n                    values,\n                    how,\n                    axis=data.ndim - 1,\n                    min_count=min_count,\n                    **kwargs,\n                )\n            except NotImplementedError:\n                # generally if we have numeric_only=False\n                # and non-applicable functions\n                # try to python agg\n                # TODO: shouldn't min_count matter?\n                # TODO: avoid special casing SparseArray here\n                if how in [\"any\", \"all\"] and isinstance(values, SparseArray):\n                    pass\n                elif alt is None or how in [\"any\", \"all\", \"std\", \"sem\"]:\n                    raise  # TODO: re-raise as TypeError?  should not be reached\n            else:\n                return result\n\n            assert alt is not None\n            result = self._agg_py_fallback(how, values, ndim=data.ndim, alt=alt)\n            return result\n\n        new_mgr = data.grouped_reduce(array_func)\n        res = self._wrap_agged_manager(new_mgr)\n        if how in [\"idxmin\", \"idxmax\"]:\n            res = self._wrap_idxmax_idxmin(res)\n        out = self._wrap_aggregated_output(res)\n        if self.axis == 1:\n            out = out.infer_objects(copy=False)\n        return out\n\n    def _cython_transform(\n        self, how: str, numeric_only: bool = False, axis: AxisInt = 0, **kwargs\n    ):\n        raise AbstractMethodError(self)\n\n    @final\n    def _transform(self, func, *args, engine=None, engine_kwargs=None, **kwargs):\n        # optimized transforms\n        orig_func = func\n        func = com.get_cython_func(func) or func\n        if orig_func != func:\n            warn_alias_replacement(self, orig_func, func)\n\n        if not isinstance(func, str):\n            return self._transform_general(func, engine, engine_kwargs, *args, **kwargs)\n\n        elif func not in base.transform_kernel_allowlist:\n            msg = f\"'{func}' is not a valid function name for transform(name)\"\n            raise ValueError(msg)\n        elif func in base.cythonized_kernels or func in base.transformation_kernels:\n            # cythonized transform or canned \"agg+broadcast\"\n            if engine is not None:\n                kwargs[\"engine\"] = engine\n                kwargs[\"engine_kwargs\"] = engine_kwargs\n            return getattr(self, func)(*args, **kwargs)\n\n        else:\n            # i.e. func in base.reduction_kernels\n\n            # GH#30918 Use _transform_fast only when we know func is an aggregation\n            # If func is a reduction, we need to broadcast the\n            # result to the whole group. Compute func result\n            # and deal with possible broadcasting below.\n            with com.temp_setattr(self, \"as_index\", True):\n                # GH#49834 - result needs groups in the index for\n                # _wrap_transform_fast_result\n                if func in [\"idxmin\", \"idxmax\"]:\n                    func = cast(Literal[\"idxmin\", \"idxmax\"], func)\n                    result = self._idxmax_idxmin(func, True, *args, **kwargs)\n                else:\n                    if engine is not None:\n                        kwargs[\"engine\"] = engine\n                        kwargs[\"engine_kwargs\"] = engine_kwargs\n                    result = getattr(self, func)(*args, **kwargs)\n\n            return self._wrap_transform_fast_result(result)\n\n    @final\n    def _wrap_transform_fast_result(self, result: NDFrameT) -> NDFrameT:\n        \"\"\"\n        Fast transform path for aggregations.\n        \"\"\"\n        obj = self._obj_with_exclusions\n\n        # for each col, reshape to size of original frame by take operation\n        ids, _, _ = self._grouper.group_info\n        result = result.reindex(self._grouper.result_index, axis=self.axis, copy=False)\n\n        if self.obj.ndim == 1:\n            # i.e. SeriesGroupBy\n            out = algorithms.take_nd(result._values, ids)\n            output = obj._constructor(out, index=obj.index, name=obj.name)\n        else:\n            # `.size()` gives Series output on DataFrame input, need axis 0\n            axis = 0 if result.ndim == 1 else self.axis\n            # GH#46209\n            # Don't convert indices: negative indices need to give rise\n            # to null values in the result\n            new_ax = result.axes[axis].take(ids)\n            output = result._reindex_with_indexers(\n                {axis: (new_ax, ids)}, allow_dups=True, copy=False\n            )\n            output = output.set_axis(obj._get_axis(self.axis), axis=axis)\n        return output\n\n    # -----------------------------------------------------------------\n    # Utilities\n\n    @final\n    def _apply_filter(self, indices, dropna):\n        if len(indices) == 0:\n            indices = np.array([], dtype=\"int64\")\n        else:\n            indices = np.sort(np.concatenate(indices))\n        if dropna:\n            filtered = self._selected_obj.take(indices, axis=self.axis)\n        else:\n            mask = np.empty(len(self._selected_obj.index), dtype=bool)\n            mask.fill(False)\n            mask[indices.astype(int)] = True\n            # mask fails to broadcast when passed to where; broadcast manually.\n            mask = np.tile(mask, list(self._selected_obj.shape[1:]) + [1]).T\n            filtered = self._selected_obj.where(mask)  # Fill with NaNs.\n        return filtered\n\n    @final\n    def _cumcount_array(self, ascending: bool = True) -> np.ndarray:\n        \"\"\"\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Notes\n        -----\n        this is currently implementing sort=False\n        (though the default is sort=True) for groupby in general\n        \"\"\"\n        ids, _, ngroups = self._grouper.group_info\n        sorter = get_group_index_sorter(ids, ngroups)\n        ids, count = ids[sorter], len(ids)\n\n        if count == 0:\n            return np.empty(0, dtype=np.int64)\n\n        run = np.r_[True, ids[:-1] != ids[1:]]\n        rep = np.diff(np.r_[np.nonzero(run)[0], count])\n        out = (~run).cumsum()\n\n        if ascending:\n            out -= np.repeat(out[run], rep)\n        else:\n            out = np.repeat(out[np.r_[run[1:], True]], rep) - out\n\n        if self._grouper.has_dropped_na:\n            out = np.where(ids == -1, np.nan, out.astype(np.float64, copy=False))\n        else:\n            out = out.astype(np.int64, copy=False)\n\n        rev = np.empty(count, dtype=np.intp)\n        rev[sorter] = np.arange(count, dtype=np.intp)\n        return out[rev]\n\n    # -----------------------------------------------------------------\n\n    @final\n    @property\n    def _obj_1d_constructor(self) -> Callable:\n        # GH28330 preserve subclassed Series/DataFrames\n        if isinstance(self.obj, DataFrame):\n            return self.obj._constructor_sliced\n        assert isinstance(self.obj, Series)\n        return self.obj._constructor\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def any(self, skipna: bool = True) -> NDFrameT:\n        \"\"\"\n        Return True if any value in the group is truthful, else False.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing.\n\n        Returns\n        -------\n        Series or DataFrame\n            DataFrame or Series of boolean values, where a value is True if any element\n            is True within its respective group, False otherwise.\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([1, 2, 0], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    0\n        dtype: int64\n        >>> ser.groupby(level=0).any()\n        a     True\n        b    False\n        dtype: bool\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 0, 3], [1, 0, 6], [7, 1, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"ostrich\", \"penguin\", \"parrot\"])\n        >>> df\n                 a  b  c\n        ostrich  1  0  3\n        penguin  1  0  6\n        parrot   7  1  9\n        >>> df.groupby(by=[\"a\"]).any()\n               b      c\n        a\n        1  False   True\n        7   True   True\n        \"\"\"\n        return self._cython_agg_general(\n            \"any\",\n            alt=lambda x: Series(x, copy=False).any(skipna=skipna),\n            skipna=skipna,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def all(self, skipna: bool = True) -> NDFrameT:\n        \"\"\"\n        Return True if all values in the group are truthful, else False.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing.\n\n        Returns\n        -------\n        Series or DataFrame\n            DataFrame or Series of boolean values, where a value is True if all elements\n            are True within its respective group, False otherwise.\n        %(see_also)s\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([1, 2, 0], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    0\n        dtype: int64\n        >>> ser.groupby(level=0).all()\n        a     True\n        b    False\n        dtype: bool\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 0, 3], [1, 5, 6], [7, 8, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"ostrich\", \"penguin\", \"parrot\"])\n        >>> df\n                 a  b  c\n        ostrich  1  0  3\n        penguin  1  5  6\n        parrot   7  8  9\n        >>> df.groupby(by=[\"a\"]).all()\n               b      c\n        a\n        1  False   True\n        7   True   True\n        \"\"\"\n        return self._cython_agg_general(\n            \"all\",\n            alt=lambda x: Series(x, copy=False).all(skipna=skipna),\n            skipna=skipna,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def count(self) -> NDFrameT:\n        \"\"\"\n        Compute count of group, excluding missing values.\n\n        Returns\n        -------\n        Series or DataFrame\n            Count of values within each group.\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([1, 2, np.nan], index=lst)\n        >>> ser\n        a    1.0\n        a    2.0\n        b    NaN\n        dtype: float64\n        >>> ser.groupby(level=0).count()\n        a    2\n        b    0\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, np.nan, 3], [1, np.nan, 6], [7, 8, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"cow\", \"horse\", \"bull\"])\n        >>> df\n                a\t  b\tc\n        cow     1\tNaN\t3\n        horse\t1\tNaN\t6\n        bull\t7\t8.0\t9\n        >>> df.groupby(\"a\").count()\n            b   c\n        a\n        1   0   2\n        7   1   1\n\n        For Resampler:\n\n        >>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n        ...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n        >>> ser\n        2023-01-01    1\n        2023-01-15    2\n        2023-02-01    3\n        2023-02-15    4\n        dtype: int64\n        >>> ser.resample('MS').count()\n        2023-01-01    2\n        2023-02-01    2\n        Freq: MS, dtype: int64\n        \"\"\"\n        data = self._get_data_to_aggregate()\n        ids, _, ngroups = self._grouper.group_info\n        mask = ids != -1\n\n        is_series = data.ndim == 1\n\n        def hfunc(bvalues: ArrayLike) -> ArrayLike:\n            # TODO(EA2D): reshape would not be necessary with 2D EAs\n            if bvalues.ndim == 1:\n                # EA\n                masked = mask & ~isna(bvalues).reshape(1, -1)\n            else:\n                masked = mask & ~isna(bvalues)\n\n            counted = lib.count_level_2d(masked, labels=ids, max_bin=ngroups)\n            if isinstance(bvalues, BaseMaskedArray):\n                return IntegerArray(\n                    counted[0], mask=np.zeros(counted.shape[1], dtype=np.bool_)\n                )\n            elif isinstance(bvalues, ArrowExtensionArray) and not isinstance(\n                bvalues.dtype, StringDtype\n            ):\n                dtype = pandas_dtype(\"int64[pyarrow]\")\n                return type(bvalues)._from_sequence(counted[0], dtype=dtype)\n            if is_series:\n                assert counted.ndim == 2\n                assert counted.shape[0] == 1\n                return counted[0]\n            return counted\n\n        new_mgr = data.grouped_reduce(hfunc)\n        new_obj = self._wrap_agged_manager(new_mgr)\n\n        # If we are grouping on categoricals we want unobserved categories to\n        # return zero, rather than the default of NaN which the reindexing in\n        # _wrap_aggregated_output() returns. GH 35028\n        # e.g. test_dataframe_groupby_on_2_categoricals_when_observed_is_false\n        with com.temp_setattr(self, \"observed\", True):\n            result = self._wrap_aggregated_output(new_obj)\n\n        return self._reindex_output(result, fill_value=0)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def mean(\n        self,\n        numeric_only: bool = False,\n        engine: Literal[\"cython\", \"numba\"] | None = None,\n        engine_kwargs: dict[str, bool] | None = None,\n    ):\n        \"\"\"\n        Compute mean of groups, excluding missing values.\n\n        Parameters\n        ----------\n        numeric_only : bool, default False\n            Include only float, int, boolean columns.\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only no longer accepts ``None`` and defaults to ``False``.\n\n        engine : str, default None\n            * ``'cython'`` : Runs the operation through C-extensions from cython.\n            * ``'numba'`` : Runs the operation through JIT compiled code from numba.\n            * ``None`` : Defaults to ``'cython'`` or globally setting\n              ``compute.use_numba``\n\n            .. versionadded:: 1.4.0\n\n        engine_kwargs : dict, default None\n            * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n            * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n              and ``parallel`` dictionary keys. The values must either be ``True`` or\n              ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n              ``{{'nopython': True, 'nogil': False, 'parallel': False}}``\n\n            .. versionadded:: 1.4.0\n\n        Returns\n        -------\n        pandas.Series or pandas.DataFrame\n        %(see_also)s\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5],\n        ...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])\n\n        Groupby one column and return the mean of the remaining columns in\n        each group.\n\n        >>> df.groupby('A').mean()\n             B         C\n        A\n        1  3.0  1.333333\n        2  4.0  1.500000\n\n        Groupby two columns and return the mean of the remaining column.\n\n        >>> df.groupby(['A', 'B']).mean()\n                 C\n        A B\n        1 2.0  2.0\n          4.0  1.0\n        2 3.0  1.0\n          5.0  2.0\n\n        Groupby one column and return the mean of only particular column in\n        the group.\n\n        >>> df.groupby('A')['B'].mean()\n        A\n        1    3.0\n        2    4.0\n        Name: B, dtype: float64\n        \"\"\"\n\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_mean\n\n            return self._numba_agg_general(\n                grouped_mean,\n                executor.float_dtype_mapping,\n                engine_kwargs,\n                min_periods=0,\n            )\n        else:\n            result = self._cython_agg_general(\n                \"mean\",\n                alt=lambda x: Series(x, copy=False).mean(numeric_only=numeric_only),\n                numeric_only=numeric_only,\n            )\n            return result.__finalize__(self.obj, method=\"groupby\")\n\n    @final\n    def median(self, numeric_only: bool = False) -> NDFrameT:\n        \"\"\"\n        Compute median of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Parameters\n        ----------\n        numeric_only : bool, default False\n            Include only float, int, boolean columns.\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only no longer accepts ``None`` and defaults to False.\n\n        Returns\n        -------\n        Series or DataFrame\n            Median of values within each group.\n\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n        >>> ser\n        a     7\n        a     2\n        a     8\n        b     4\n        b     3\n        b     3\n        dtype: int64\n        >>> ser.groupby(level=0).median()\n        a    7.0\n        b    3.0\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n        >>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n        ...                   'mouse', 'mouse', 'mouse', 'mouse'])\n        >>> df\n                 a  b\n          dog    1  1\n          dog    3  4\n          dog    5  8\n        mouse    7  4\n        mouse    7  4\n        mouse    8  2\n        mouse    3  1\n        >>> df.groupby(level=0).median()\n                 a    b\n        dog    3.0  4.0\n        mouse  7.0  3.0\n\n        For Resampler:\n\n        >>> ser = pd.Series([1, 2, 3, 3, 4, 5],\n        ...                 index=pd.DatetimeIndex(['2023-01-01',\n        ...                                         '2023-01-10',\n        ...                                         '2023-01-15',\n        ...                                         '2023-02-01',\n        ...                                         '2023-02-10',\n        ...                                         '2023-02-15']))\n        >>> ser.resample('MS').median()\n        2023-01-01    2.0\n        2023-02-01    4.0\n        Freq: MS, dtype: float64\n        \"\"\"\n        result = self._cython_agg_general(\n            \"median\",\n            alt=lambda x: Series(x, copy=False).median(numeric_only=numeric_only),\n            numeric_only=numeric_only,\n        )\n        return result.__finalize__(self.obj, method=\"groupby\")\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def std(\n        self,\n        ddof: int = 1,\n        engine: Literal[\"cython\", \"numba\"] | None = None,\n        engine_kwargs: dict[str, bool] | None = None,\n        numeric_only: bool = False,\n    ):\n        \"\"\"\n        Compute standard deviation of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        engine : str, default None\n            * ``'cython'`` : Runs the operation through C-extensions from cython.\n            * ``'numba'`` : Runs the operation through JIT compiled code from numba.\n            * ``None`` : Defaults to ``'cython'`` or globally setting\n              ``compute.use_numba``\n\n            .. versionadded:: 1.4.0\n\n        engine_kwargs : dict, default None\n            * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n            * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n              and ``parallel`` dictionary keys. The values must either be ``True`` or\n              ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n              ``{{'nopython': True, 'nogil': False, 'parallel': False}}``\n\n            .. versionadded:: 1.4.0\n\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only now defaults to ``False``.\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard deviation of values within each group.\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n        >>> ser\n        a     7\n        a     2\n        a     8\n        b     4\n        b     3\n        b     3\n        dtype: int64\n        >>> ser.groupby(level=0).std()\n        a    3.21455\n        b    0.57735\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n        >>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n        ...                   'mouse', 'mouse', 'mouse', 'mouse'])\n        >>> df\n                 a  b\n          dog    1  1\n          dog    3  4\n          dog    5  8\n        mouse    7  4\n        mouse    7  4\n        mouse    8  2\n        mouse    3  1\n        >>> df.groupby(level=0).std()\n                      a         b\n        dog    2.000000  3.511885\n        mouse  2.217356  1.500000\n        \"\"\"\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_var\n\n            return np.sqrt(\n                self._numba_agg_general(\n                    grouped_var,\n                    executor.float_dtype_mapping,\n                    engine_kwargs,\n                    min_periods=0,\n                    ddof=ddof,\n                )\n            )\n        else:\n            return self._cython_agg_general(\n                \"std\",\n                alt=lambda x: Series(x, copy=False).std(ddof=ddof),\n                numeric_only=numeric_only,\n                ddof=ddof,\n            )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def var(\n        self,\n        ddof: int = 1,\n        engine: Literal[\"cython\", \"numba\"] | None = None,\n        engine_kwargs: dict[str, bool] | None = None,\n        numeric_only: bool = False,\n    ):\n        \"\"\"\n        Compute variance of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        engine : str, default None\n            * ``'cython'`` : Runs the operation through C-extensions from cython.\n            * ``'numba'`` : Runs the operation through JIT compiled code from numba.\n            * ``None`` : Defaults to ``'cython'`` or globally setting\n              ``compute.use_numba``\n\n            .. versionadded:: 1.4.0\n\n        engine_kwargs : dict, default None\n            * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n            * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n              and ``parallel`` dictionary keys. The values must either be ``True`` or\n              ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n              ``{{'nopython': True, 'nogil': False, 'parallel': False}}``\n\n            .. versionadded:: 1.4.0\n\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only now defaults to ``False``.\n\n        Returns\n        -------\n        Series or DataFrame\n            Variance of values within each group.\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n        >>> ser\n        a     7\n        a     2\n        a     8\n        b     4\n        b     3\n        b     3\n        dtype: int64\n        >>> ser.groupby(level=0).var()\n        a    10.333333\n        b     0.333333\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n        >>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n        ...                   'mouse', 'mouse', 'mouse', 'mouse'])\n        >>> df\n                 a  b\n          dog    1  1\n          dog    3  4\n          dog    5  8\n        mouse    7  4\n        mouse    7  4\n        mouse    8  2\n        mouse    3  1\n        >>> df.groupby(level=0).var()\n                      a          b\n        dog    4.000000  12.333333\n        mouse  4.916667   2.250000\n        \"\"\"\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_var\n\n            return self._numba_agg_general(\n                grouped_var,\n                executor.float_dtype_mapping,\n                engine_kwargs,\n                min_periods=0,\n                ddof=ddof,\n            )\n        else:\n            return self._cython_agg_general(\n                \"var\",\n                alt=lambda x: Series(x, copy=False).var(ddof=ddof),\n                numeric_only=numeric_only,\n                ddof=ddof,\n            )\n\n    @final\n    def _value_counts(\n        self,\n        subset: Sequence[Hashable] | None = None,\n        normalize: bool = False,\n        sort: bool = True,\n        ascending: bool = False,\n        dropna: bool = True,\n    ) -> DataFrame | Series:\n        \"\"\"\n        Shared implementation of value_counts for SeriesGroupBy and DataFrameGroupBy.\n\n        SeriesGroupBy additionally supports a bins argument. See the docstring of\n        DataFrameGroupBy.value_counts for a description of arguments.\n        \"\"\"\n        if self.axis == 1:\n            raise NotImplementedError(\n                \"DataFrameGroupBy.value_counts only handles axis=0\"\n            )\n        name = \"proportion\" if normalize else \"count\"\n\n        df = self.obj\n        obj = self._obj_with_exclusions\n\n        in_axis_names = {\n            grouping.name for grouping in self._grouper.groupings if grouping.in_axis\n        }\n        if isinstance(obj, Series):\n            _name = obj.name\n            keys = [] if _name in in_axis_names else [obj]\n        else:\n            unique_cols = set(obj.columns)\n            if subset is not None:\n                subsetted = set(subset)\n                clashing = subsetted & set(in_axis_names)\n                if clashing:\n                    raise ValueError(\n                        f\"Keys {clashing} in subset cannot be in \"\n                        \"the groupby column keys.\"\n                    )\n                doesnt_exist = subsetted - unique_cols\n                if doesnt_exist:\n                    raise ValueError(\n                        f\"Keys {doesnt_exist} in subset do not \"\n                        f\"exist in the DataFrame.\"\n                    )\n            else:\n                subsetted = unique_cols\n\n            keys = [\n                # Can't use .values because the column label needs to be preserved\n                obj.iloc[:, idx]\n                for idx, _name in enumerate(obj.columns)\n                if _name not in in_axis_names and _name in subsetted\n            ]\n\n        groupings = list(self._grouper.groupings)\n        for key in keys:\n            grouper, _, _ = get_grouper(\n                df,\n                key=key,\n                axis=self.axis,\n                sort=self.sort,\n                observed=False,\n                dropna=dropna,\n            )\n            groupings += list(grouper.groupings)\n\n        # Take the size of the overall columns\n        gb = df.groupby(\n            groupings,\n            sort=self.sort,\n            observed=self.observed,\n            dropna=self.dropna,\n        )\n        result_series = cast(Series, gb.size())\n        result_series.name = name\n\n        # GH-46357 Include non-observed categories\n        # of non-grouping columns regardless of `observed`\n        if any(\n            isinstance(grouping.grouping_vector, (Categorical, CategoricalIndex))\n            and not grouping._observed\n            for grouping in groupings\n        ):\n            levels_list = [ping._result_index for ping in groupings]\n            multi_index = MultiIndex.from_product(\n                levels_list, names=[ping.name for ping in groupings]\n            )\n            result_series = result_series.reindex(multi_index, fill_value=0)\n\n        if sort:\n            # Sort by the values\n            result_series = result_series.sort_values(\n                ascending=ascending, kind=\"stable\"\n            )\n        if self.sort:\n            # Sort by the groupings\n            names = result_series.index.names\n            # GH#55951 - Temporarily replace names in case they are integers\n            result_series.index.names = range(len(names))\n            index_level = list(range(len(self._grouper.groupings)))\n            result_series = result_series.sort_index(\n                level=index_level, sort_remaining=False\n            )\n            result_series.index.names = names\n\n        if normalize:\n            # Normalize the results by dividing by the original group sizes.\n            # We are guaranteed to have the first N levels be the\n            # user-requested grouping.\n            levels = list(\n                range(len(self._grouper.groupings), result_series.index.nlevels)\n            )\n            indexed_group_size = result_series.groupby(\n                result_series.index.droplevel(levels),\n                sort=self.sort,\n                dropna=self.dropna,\n                # GH#43999 - deprecation of observed=False\n                observed=False,\n            ).transform(\"sum\")\n            result_series /= indexed_group_size\n\n            # Handle groups of non-observed categories\n            result_series = result_series.fillna(0.0)\n\n        result: Series | DataFrame\n        if self.as_index:\n            result = result_series\n        else:\n            # Convert to frame\n            index = result_series.index\n            columns = com.fill_missing_names(index.names)\n            if name in columns:\n                raise ValueError(f\"Column label '{name}' is duplicate of result column\")\n            result_series.name = name\n            result_series.index = index.set_names(range(len(columns)))\n            result_frame = result_series.reset_index()\n            orig_dtype = self._grouper.groupings[0].obj.columns.dtype  # type: ignore[union-attr]\n            cols = Index(columns, dtype=orig_dtype).insert(len(columns), name)\n            result_frame.columns = cols\n            result = result_frame\n        return result.__finalize__(self.obj, method=\"value_counts\")\n\n    @final\n    def sem(self, ddof: int = 1, numeric_only: bool = False) -> NDFrameT:\n        \"\"\"\n        Compute standard error of the mean of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only now defaults to ``False``.\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard error of the mean of values within each group.\n\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b', 'b']\n        >>> ser = pd.Series([5, 10, 8, 14], index=lst)\n        >>> ser\n        a     5\n        a    10\n        b     8\n        b    14\n        dtype: int64\n        >>> ser.groupby(level=0).sem()\n        a    2.5\n        b    3.0\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 12, 11], [1, 15, 2], [2, 5, 8], [2, 6, 12]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"tuna\", \"salmon\", \"catfish\", \"goldfish\"])\n        >>> df\n                   a   b   c\n            tuna   1  12  11\n          salmon   1  15   2\n         catfish   2   5   8\n        goldfish   2   6  12\n        >>> df.groupby(\"a\").sem()\n              b  c\n        a\n        1    1.5  4.5\n        2    0.5  2.0\n\n        For Resampler:\n\n        >>> ser = pd.Series([1, 3, 2, 4, 3, 8],\n        ...                 index=pd.DatetimeIndex(['2023-01-01',\n        ...                                         '2023-01-10',\n        ...                                         '2023-01-15',\n        ...                                         '2023-02-01',\n        ...                                         '2023-02-10',\n        ...                                         '2023-02-15']))\n        >>> ser.resample('MS').sem()\n        2023-01-01    0.577350\n        2023-02-01    1.527525\n        Freq: MS, dtype: float64\n        \"\"\"\n        if numeric_only and self.obj.ndim == 1 and not is_numeric_dtype(self.obj.dtype):\n            raise TypeError(\n                f\"{type(self).__name__}.sem called with \"\n                f\"numeric_only={numeric_only} and dtype {self.obj.dtype}\"\n            )\n        return self._cython_agg_general(\n            \"sem\",\n            alt=lambda x: Series(x, copy=False).sem(ddof=ddof),\n            numeric_only=numeric_only,\n            ddof=ddof,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def size(self) -> DataFrame | Series:\n        \"\"\"\n        Compute group sizes.\n\n        Returns\n        -------\n        DataFrame or Series\n            Number of rows in each group as a Series if as_index is True\n            or a DataFrame if as_index is False.\n        %(see_also)s\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([1, 2, 3], index=lst)\n        >>> ser\n        a     1\n        a     2\n        b     3\n        dtype: int64\n        >>> ser.groupby(level=0).size()\n        a    2\n        b    1\n        dtype: int64\n\n        >>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"owl\", \"toucan\", \"eagle\"])\n        >>> df\n                a  b  c\n        owl     1  2  3\n        toucan  1  5  6\n        eagle   7  8  9\n        >>> df.groupby(\"a\").size()\n        a\n        1    2\n        7    1\n        dtype: int64\n\n        For Resampler:\n\n        >>> ser = pd.Series([1, 2, 3], index=pd.DatetimeIndex(\n        ...                 ['2023-01-01', '2023-01-15', '2023-02-01']))\n        >>> ser\n        2023-01-01    1\n        2023-01-15    2\n        2023-02-01    3\n        dtype: int64\n        >>> ser.resample('MS').size()\n        2023-01-01    2\n        2023-02-01    1\n        Freq: MS, dtype: int64\n        \"\"\"\n        result = self._grouper.size()\n        dtype_backend: None | Literal[\"pyarrow\", \"numpy_nullable\"] = None\n        if isinstance(self.obj, Series):\n            if isinstance(self.obj.array, ArrowExtensionArray):\n                if isinstance(self.obj.array, ArrowStringArrayNumpySemantics):\n                    dtype_backend = None\n                elif isinstance(self.obj.array, ArrowStringArray):\n                    dtype_backend = \"numpy_nullable\"\n                else:\n                    dtype_backend = \"pyarrow\"\n            elif isinstance(self.obj.array, BaseMaskedArray):\n                dtype_backend = \"numpy_nullable\"\n        # TODO: For DataFrames what if columns are mixed arrow/numpy/masked?\n\n        # GH28330 preserve subclassed Series/DataFrames through calls\n        if isinstance(self.obj, Series):\n            result = self._obj_1d_constructor(result, name=self.obj.name)\n        else:\n            result = self._obj_1d_constructor(result)\n\n        if dtype_backend is not None:\n            result = result.convert_dtypes(\n                infer_objects=False,\n                convert_string=False,\n                convert_boolean=False,\n                convert_floating=False,\n                dtype_backend=dtype_backend,\n            )\n\n        with com.temp_setattr(self, \"as_index\", True):\n            # size already has the desired behavior in GH#49519, but this makes the\n            # as_index=False path of _reindex_output fail on categorical groupers.\n            result = self._reindex_output(result, fill_value=0)\n        if not self.as_index:\n            # error: Incompatible types in assignment (expression has\n            # type \"DataFrame\", variable has type \"Series\")\n            result = result.rename(\"size\").reset_index()  # type: ignore[assignment]\n        return result\n\n    @final\n    @doc(\n        _groupby_agg_method_engine_template,\n        fname=\"sum\",\n        no=False,\n        mc=0,\n        e=None,\n        ek=None,\n        example=dedent(\n            \"\"\"\\\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b', 'b']\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    3\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).sum()\n        a    3\n        b    7\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"tiger\", \"leopard\", \"cheetah\", \"lion\"])\n        >>> df\n                  a  b  c\n          tiger   1  8  2\n        leopard   1  2  5\n        cheetah   2  5  8\n           lion   2  6  9\n        >>> df.groupby(\"a\").sum()\n             b   c\n        a\n        1   10   7\n        2   11  17\"\"\"\n        ),\n    )\n    def sum(\n        self,\n        numeric_only: bool = False,\n        min_count: int = 0,\n        engine: Literal[\"cython\", \"numba\"] | None = None,\n        engine_kwargs: dict[str, bool] | None = None,\n    ):\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_sum\n\n            return self._numba_agg_general(\n                grouped_sum,\n                executor.default_dtype_mapping,\n                engine_kwargs,\n                min_periods=min_count,\n            )\n        else:\n            # If we are grouping on categoricals we want unobserved categories to\n            # return zero, rather than the default of NaN which the reindexing in\n            # _agg_general() returns. GH #31422\n            with com.temp_setattr(self, \"observed\", True):\n                result = self._agg_general(\n                    numeric_only=numeric_only,\n                    min_count=min_count,\n                    alias=\"sum\",\n                    npfunc=np.sum,\n                )\n\n            return self._reindex_output(result, fill_value=0)\n\n    @final\n    @doc(\n        _groupby_agg_method_template,\n        fname=\"prod\",\n        no=False,\n        mc=0,\n        example=dedent(\n            \"\"\"\\\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b', 'b']\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    3\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).prod()\n        a    2\n        b   12\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"tiger\", \"leopard\", \"cheetah\", \"lion\"])\n        >>> df\n                  a  b  c\n          tiger   1  8  2\n        leopard   1  2  5\n        cheetah   2  5  8\n           lion   2  6  9\n        >>> df.groupby(\"a\").prod()\n             b    c\n        a\n        1   16   10\n        2   30   72\"\"\"\n        ),\n    )\n    def prod(self, numeric_only: bool = False, min_count: int = 0) -> NDFrameT:\n        return self._agg_general(\n            numeric_only=numeric_only, min_count=min_count, alias=\"prod\", npfunc=np.prod\n        )\n\n    @final\n    @doc(\n        _groupby_agg_method_engine_template,\n        fname=\"min\",\n        no=False,\n        mc=-1,\n        e=None,\n        ek=None,\n        example=dedent(\n            \"\"\"\\\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b', 'b']\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    3\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).min()\n        a    1\n        b    3\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"tiger\", \"leopard\", \"cheetah\", \"lion\"])\n        >>> df\n                  a  b  c\n          tiger   1  8  2\n        leopard   1  2  5\n        cheetah   2  5  8\n           lion   2  6  9\n        >>> df.groupby(\"a\").min()\n            b  c\n        a\n        1   2  2\n        2   5  8\"\"\"\n        ),\n    )\n    def min(\n        self,\n        numeric_only: bool = False,\n        min_count: int = -1,\n        engine: Literal[\"cython\", \"numba\"] | None = None,\n        engine_kwargs: dict[str, bool] | None = None,\n    ):\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_min_max\n\n            return self._numba_agg_general(\n                grouped_min_max,\n                executor.identity_dtype_mapping,\n                engine_kwargs,\n                min_periods=min_count,\n                is_max=False,\n            )\n        else:\n            return self._agg_general(\n                numeric_only=numeric_only,\n                min_count=min_count,\n                alias=\"min\",\n                npfunc=np.min,\n            )\n\n    @final\n    @doc(\n        _groupby_agg_method_engine_template,\n        fname=\"max\",\n        no=False,\n        mc=-1,\n        e=None,\n        ek=None,\n        example=dedent(\n            \"\"\"\\\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b', 'b']\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    3\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).max()\n        a    2\n        b    4\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"tiger\", \"leopard\", \"cheetah\", \"lion\"])\n        >>> df\n                  a  b  c\n          tiger   1  8  2\n        leopard   1  2  5\n        cheetah   2  5  8\n           lion   2  6  9\n        >>> df.groupby(\"a\").max()\n            b  c\n        a\n        1   8  5\n        2   6  9\"\"\"\n        ),\n    )\n    def max(\n        self,\n        numeric_only: bool = False,\n        min_count: int = -1,\n        engine: Literal[\"cython\", \"numba\"] | None = None,\n        engine_kwargs: dict[str, bool] | None = None,\n    ):\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_min_max\n\n            return self._numba_agg_general(\n                grouped_min_max,\n                executor.identity_dtype_mapping,\n                engine_kwargs,\n                min_periods=min_count,\n                is_max=True,\n            )\n        else:\n            return self._agg_general(\n                numeric_only=numeric_only,\n                min_count=min_count,\n                alias=\"max\",\n                npfunc=np.max,\n            )\n\n    @final\n    def first(\n        self, numeric_only: bool = False, min_count: int = -1, skipna: bool = True\n    ) -> NDFrameT:\n        \"\"\"\n        Compute the first entry of each column within each group.\n\n        Defaults to skipping NA elements.\n\n        Parameters\n        ----------\n        numeric_only : bool, default False\n            Include only float, int, boolean columns.\n        min_count : int, default -1\n            The required number of valid values to perform the operation. If fewer\n            than ``min_count`` valid values are present the result will be NA.\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n\n            .. versionadded:: 2.2.1\n\n        Returns\n        -------\n        Series or DataFrame\n            First values within each group.\n\n        See Also\n        --------\n        DataFrame.groupby : Apply a function groupby to each row or column of a\n            DataFrame.\n        pandas.core.groupby.DataFrameGroupBy.last : Compute the last non-null entry\n            of each column.\n        pandas.core.groupby.DataFrameGroupBy.nth : Take the nth row from each group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(dict(A=[1, 1, 3], B=[None, 5, 6], C=[1, 2, 3],\n        ...                        D=['3/11/2000', '3/12/2000', '3/13/2000']))\n        >>> df['D'] = pd.to_datetime(df['D'])\n        >>> df.groupby(\"A\").first()\n             B  C          D\n        A\n        1  5.0  1 2000-03-11\n        3  6.0  3 2000-03-13\n        >>> df.groupby(\"A\").first(min_count=2)\n            B    C          D\n        A\n        1 NaN  1.0 2000-03-11\n        3 NaN  NaN        NaT\n        >>> df.groupby(\"A\").first(numeric_only=True)\n             B  C\n        A\n        1  5.0  1\n        3  6.0  3\n        \"\"\"\n\n        def first_compat(obj: NDFrameT, axis: AxisInt = 0):\n            def first(x: Series):\n                \"\"\"Helper function for first item that isn't NA.\"\"\"\n                arr = x.array[notna(x.array)]\n                if not len(arr):\n                    return x.array.dtype.na_value\n                return arr[0]\n\n            if isinstance(obj, DataFrame):\n                return obj.apply(first, axis=axis)\n            elif isinstance(obj, Series):\n                return first(obj)\n            else:  # pragma: no cover\n                raise TypeError(type(obj))\n\n        return self._agg_general(\n            numeric_only=numeric_only,\n            min_count=min_count,\n            alias=\"first\",\n            npfunc=first_compat,\n            skipna=skipna,\n        )\n\n    @final\n    def last(\n        self, numeric_only: bool = False, min_count: int = -1, skipna: bool = True\n    ) -> NDFrameT:\n        \"\"\"\n        Compute the last entry of each column within each group.\n\n        Defaults to skipping NA elements.\n\n        Parameters\n        ----------\n        numeric_only : bool, default False\n            Include only float, int, boolean columns. If None, will attempt to use\n            everything, then use only numeric data.\n        min_count : int, default -1\n            The required number of valid values to perform the operation. If fewer\n            than ``min_count`` valid values are present the result will be NA.\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n\n            .. versionadded:: 2.2.1\n\n        Returns\n        -------\n        Series or DataFrame\n            Last of values within each group.\n\n        See Also\n        --------\n        DataFrame.groupby : Apply a function groupby to each row or column of a\n            DataFrame.\n        pandas.core.groupby.DataFrameGroupBy.first : Compute the first non-null entry\n            of each column.\n        pandas.core.groupby.DataFrameGroupBy.nth : Take the nth row from each group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(dict(A=[1, 1, 3], B=[5, None, 6], C=[1, 2, 3]))\n        >>> df.groupby(\"A\").last()\n             B  C\n        A\n        1  5.0  2\n        3  6.0  3\n        \"\"\"\n\n        def last_compat(obj: NDFrameT, axis: AxisInt = 0):\n            def last(x: Series):\n                \"\"\"Helper function for last item that isn't NA.\"\"\"\n                arr = x.array[notna(x.array)]\n                if not len(arr):\n                    return x.array.dtype.na_value\n                return arr[-1]\n\n            if isinstance(obj, DataFrame):\n                return obj.apply(last, axis=axis)\n            elif isinstance(obj, Series):\n                return last(obj)\n            else:  # pragma: no cover\n                raise TypeError(type(obj))\n\n        return self._agg_general(\n            numeric_only=numeric_only,\n            min_count=min_count,\n            alias=\"last\",\n            npfunc=last_compat,\n            skipna=skipna,\n        )\n\n    @final\n    def ohlc(self) -> DataFrame:\n        \"\"\"\n        Compute open, high, low and close values of a group, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Returns\n        -------\n        DataFrame\n            Open, high, low and close values within each group.\n\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> lst = ['SPX', 'CAC', 'SPX', 'CAC', 'SPX', 'CAC', 'SPX', 'CAC',]\n        >>> ser = pd.Series([3.4, 9.0, 7.2, 5.2, 8.8, 9.4, 0.1, 0.5], index=lst)\n        >>> ser\n        SPX     3.4\n        CAC     9.0\n        SPX     7.2\n        CAC     5.2\n        SPX     8.8\n        CAC     9.4\n        SPX     0.1\n        CAC     0.5\n        dtype: float64\n        >>> ser.groupby(level=0).ohlc()\n             open  high  low  close\n        CAC   9.0   9.4  0.5    0.5\n        SPX   3.4   8.8  0.1    0.1\n\n        For DataFrameGroupBy:\n\n        >>> data = {2022: [1.2, 2.3, 8.9, 4.5, 4.4, 3, 2 , 1],\n        ...         2023: [3.4, 9.0, 7.2, 5.2, 8.8, 9.4, 8.2, 1.0]}\n        >>> df = pd.DataFrame(data, index=['SPX', 'CAC', 'SPX', 'CAC',\n        ...                   'SPX', 'CAC', 'SPX', 'CAC'])\n        >>> df\n             2022  2023\n        SPX   1.2   3.4\n        CAC   2.3   9.0\n        SPX   8.9   7.2\n        CAC   4.5   5.2\n        SPX   4.4   8.8\n        CAC   3.0   9.4\n        SPX   2.0   8.2\n        CAC   1.0   1.0\n        >>> df.groupby(level=0).ohlc()\n            2022                 2023\n            open high  low close open high  low close\n        CAC  2.3  4.5  1.0   1.0  9.0  9.4  1.0   1.0\n        SPX  1.2  8.9  1.2   2.0  3.4  8.8  3.4   8.2\n\n        For Resampler:\n\n        >>> ser = pd.Series([1, 3, 2, 4, 3, 5],\n        ...                 index=pd.DatetimeIndex(['2023-01-01',\n        ...                                         '2023-01-10',\n        ...                                         '2023-01-15',\n        ...                                         '2023-02-01',\n        ...                                         '2023-02-10',\n        ...                                         '2023-02-15']))\n        >>> ser.resample('MS').ohlc()\n                    open  high  low  close\n        2023-01-01     1     3    1      2\n        2023-02-01     4     5    3      5\n        \"\"\"\n        if self.obj.ndim == 1:\n            obj = self._selected_obj\n\n            is_numeric = is_numeric_dtype(obj.dtype)\n            if not is_numeric:\n                raise DataError(\"No numeric types to aggregate\")\n\n            res_values = self._grouper._cython_operation(\n                \"aggregate\", obj._values, \"ohlc\", axis=0, min_count=-1\n            )\n\n            agg_names = [\"open\", \"high\", \"low\", \"close\"]\n            result = self.obj._constructor_expanddim(\n                res_values, index=self._grouper.result_index, columns=agg_names\n            )\n            return self._reindex_output(result)\n\n        result = self._apply_to_column_groupbys(lambda sgb: sgb.ohlc())\n        return result\n\n    @doc(DataFrame.describe)\n    def describe(\n        self,\n        percentiles=None,\n        include=None,\n        exclude=None,\n    ) -> NDFrameT:\n        obj = self._obj_with_exclusions\n\n        if len(obj) == 0:\n            described = obj.describe(\n                percentiles=percentiles, include=include, exclude=exclude\n            )\n            if obj.ndim == 1:\n                result = described\n            else:\n                result = described.unstack()\n            return result.to_frame().T.iloc[:0]\n\n        with com.temp_setattr(self, \"as_index\", True):\n            result = self._python_apply_general(\n                lambda x: x.describe(\n                    percentiles=percentiles, include=include, exclude=exclude\n                ),\n                obj,\n                not_indexed_same=True,\n            )\n        if self.axis == 1:\n            return result.T\n\n        # GH#49256 - properly handle the grouping column(s)\n        result = result.unstack()\n        if not self.as_index:\n            result = self._insert_inaxis_grouper(result)\n            result.index = default_index(len(result))\n\n        return result\n\n    @final\n    def resample(self, rule, *args, include_groups: bool = True, **kwargs) -> Resampler:\n        \"\"\"\n        Provide resampling when using a TimeGrouper.\n\n        Given a grouper, the function resamples it according to a string\n        \"string\" -> \"frequency\".\n\n        See the :ref:`frequency aliases <timeseries.offset_aliases>`\n        documentation for more details.\n\n        Parameters\n        ----------\n        rule : str or DateOffset\n            The offset string or object representing target grouper conversion.\n        *args\n            Possible arguments are `how`, `fill_method`, `limit`, `kind` and\n            `on`, and other arguments of `TimeGrouper`.\n        include_groups : bool, default True\n            When True, will attempt to include the groupings in the operation in\n            the case that they are columns of the DataFrame. If this raises a\n            TypeError, the result will be computed with the groupings excluded.\n            When False, the groupings will be excluded when applying ``func``.\n\n            .. versionadded:: 2.2.0\n\n            .. deprecated:: 2.2.0\n\n               Setting include_groups to True is deprecated. Only the value\n               False will be allowed in a future version of pandas.\n\n        **kwargs\n            Possible arguments are `how`, `fill_method`, `limit`, `kind` and\n            `on`, and other arguments of `TimeGrouper`.\n\n        Returns\n        -------\n        pandas.api.typing.DatetimeIndexResamplerGroupby,\n        pandas.api.typing.PeriodIndexResamplerGroupby, or\n        pandas.api.typing.TimedeltaIndexResamplerGroupby\n            Return a new groupby object, with type depending on the data\n            being resampled.\n\n        See Also\n        --------\n        Grouper : Specify a frequency to resample with when\n            grouping by a key.\n        DatetimeIndex.resample : Frequency conversion and resampling of\n            time series.\n\n        Examples\n        --------\n        >>> idx = pd.date_range('1/1/2000', periods=4, freq='min')\n        >>> df = pd.DataFrame(data=4 * [range(2)],\n        ...                   index=idx,\n        ...                   columns=['a', 'b'])\n        >>> df.iloc[2, 0] = 5\n        >>> df\n                            a  b\n        2000-01-01 00:00:00  0  1\n        2000-01-01 00:01:00  0  1\n        2000-01-01 00:02:00  5  1\n        2000-01-01 00:03:00  0  1\n\n        Downsample the DataFrame into 3 minute bins and sum the values of\n        the timestamps falling into a bin.\n\n        >>> df.groupby('a').resample('3min', include_groups=False).sum()\n                                 b\n        a\n        0   2000-01-01 00:00:00  2\n            2000-01-01 00:03:00  1\n        5   2000-01-01 00:00:00  1\n\n        Upsample the series into 30 second bins.\n\n        >>> df.groupby('a').resample('30s', include_groups=False).sum()\n                            b\n        a\n        0   2000-01-01 00:00:00  1\n            2000-01-01 00:00:30  0\n            2000-01-01 00:01:00  1\n            2000-01-01 00:01:30  0\n            2000-01-01 00:02:00  0\n            2000-01-01 00:02:30  0\n            2000-01-01 00:03:00  1\n        5   2000-01-01 00:02:00  1\n\n        Resample by month. Values are assigned to the month of the period.\n\n        >>> df.groupby('a').resample('ME', include_groups=False).sum()\n                    b\n        a\n        0   2000-01-31  3\n        5   2000-01-31  1\n\n        Downsample the series into 3 minute bins as above, but close the right\n        side of the bin interval.\n\n        >>> (\n        ...     df.groupby('a')\n        ...     .resample('3min', closed='right', include_groups=False)\n        ...     .sum()\n        ... )\n                                 b\n        a\n        0   1999-12-31 23:57:00  1\n            2000-01-01 00:00:00  2\n        5   2000-01-01 00:00:00  1\n\n        Downsample the series into 3 minute bins and close the right side of\n        the bin interval, but label each bin using the right edge instead of\n        the left.\n\n        >>> (\n        ...     df.groupby('a')\n        ...     .resample('3min', closed='right', label='right', include_groups=False)\n        ...     .sum()\n        ... )\n                                 b\n        a\n        0   2000-01-01 00:00:00  1\n            2000-01-01 00:03:00  2\n        5   2000-01-01 00:03:00  1\n        \"\"\"\n        from pandas.core.resample import get_resampler_for_grouping\n\n        # mypy flags that include_groups could be specified via `*args` or `**kwargs`\n        # GH#54961 would resolve.\n        return get_resampler_for_grouping(  # type: ignore[misc]\n            self, rule, *args, include_groups=include_groups, **kwargs\n        )\n\n    @final\n    def rolling(self, *args, **kwargs) -> RollingGroupby:\n        \"\"\"\n        Return a rolling grouper, providing rolling functionality per group.\n\n        Parameters\n        ----------\n        window : int, timedelta, str, offset, or BaseIndexer subclass\n            Size of the moving window.\n\n            If an integer, the fixed number of observations used for\n            each window.\n\n            If a timedelta, str, or offset, the time period of each window. Each\n            window will be a variable sized based on the observations included in\n            the time-period. This is only valid for datetimelike indexes.\n            To learn more about the offsets & frequency strings, please see `this link\n            <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\n            If a BaseIndexer subclass, the window boundaries\n            based on the defined ``get_window_bounds`` method. Additional rolling\n            keyword arguments, namely ``min_periods``, ``center``, ``closed`` and\n            ``step`` will be passed to ``get_window_bounds``.\n\n        min_periods : int, default None\n            Minimum number of observations in window required to have a value;\n            otherwise, result is ``np.nan``.\n\n            For a window that is specified by an offset,\n            ``min_periods`` will default to 1.\n\n            For a window that is specified by an integer, ``min_periods`` will default\n            to the size of the window.\n\n        center : bool, default False\n            If False, set the window labels as the right edge of the window index.\n\n            If True, set the window labels as the center of the window index.\n\n        win_type : str, default None\n            If ``None``, all points are evenly weighted.\n\n            If a string, it must be a valid `scipy.signal window function\n            <https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows>`__.\n\n            Certain Scipy window types require additional parameters to be passed\n            in the aggregation function. The additional parameters must match\n            the keywords specified in the Scipy window type method signature.\n\n        on : str, optional\n            For a DataFrame, a column label or Index level on which\n            to calculate the rolling window, rather than the DataFrame's index.\n\n            Provided integer column is ignored and excluded from result since\n            an integer index is not used to calculate the rolling window.\n\n        axis : int or str, default 0\n            If ``0`` or ``'index'``, roll across the rows.\n\n            If ``1`` or ``'columns'``, roll across the columns.\n\n            For `Series` this parameter is unused and defaults to 0.\n\n        closed : str, default None\n            If ``'right'``, the first point in the window is excluded from calculations.\n\n            If ``'left'``, the last point in the window is excluded from calculations.\n\n            If ``'both'``, no points in the window are excluded from calculations.\n\n            If ``'neither'``, the first and last points in the window are excluded\n            from calculations.\n\n            Default ``None`` (``'right'``).\n\n        method : str {'single', 'table'}, default 'single'\n            Execute the rolling operation per single column or row (``'single'``)\n            or over the entire object (``'table'``).\n\n            This argument is only implemented when specifying ``engine='numba'``\n            in the method call.\n\n        Returns\n        -------\n        pandas.api.typing.RollingGroupby\n            Return a new grouper with our rolling appended.\n\n        See Also\n        --------\n        Series.rolling : Calling object with Series data.\n        DataFrame.rolling : Calling object with DataFrames.\n        Series.groupby : Apply a function groupby to a Series.\n        DataFrame.groupby : Apply a function groupby.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 2],\n        ...                    'B': [1, 2, 3, 4],\n        ...                    'C': [0.362, 0.227, 1.267, -0.562]})\n        >>> df\n              A  B      C\n        0     1  1  0.362\n        1     1  2  0.227\n        2     2  3  1.267\n        3     2  4 -0.562\n\n        >>> df.groupby('A').rolling(2).sum()\n            B      C\n        A\n        1 0  NaN    NaN\n          1  3.0  0.589\n        2 2  NaN    NaN\n          3  7.0  0.705\n\n        >>> df.groupby('A').rolling(2, min_periods=1).sum()\n            B      C\n        A\n        1 0  1.0  0.362\n          1  3.0  0.589\n        2 2  3.0  1.267\n          3  7.0  0.705\n\n        >>> df.groupby('A').rolling(2, on='B').sum()\n            B      C\n        A\n        1 0  1    NaN\n          1  2  0.589\n        2 2  3    NaN\n          3  4  0.705\n        \"\"\"\n        from pandas.core.window import RollingGroupby\n\n        return RollingGroupby(\n            self._selected_obj,\n            *args,\n            _grouper=self._grouper,\n            _as_index=self.as_index,\n            **kwargs,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def expanding(self, *args, **kwargs) -> ExpandingGroupby:\n        \"\"\"\n        Return an expanding grouper, providing expanding\n        functionality per group.\n\n        Returns\n        -------\n        pandas.api.typing.ExpandingGroupby\n        \"\"\"\n        from pandas.core.window import ExpandingGroupby\n\n        return ExpandingGroupby(\n            self._selected_obj,\n            *args,\n            _grouper=self._grouper,\n            **kwargs,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def ewm(self, *args, **kwargs) -> ExponentialMovingWindowGroupby:\n        \"\"\"\n        Return an ewm grouper, providing ewm functionality per group.\n\n        Returns\n        -------\n        pandas.api.typing.ExponentialMovingWindowGroupby\n        \"\"\"\n        from pandas.core.window import ExponentialMovingWindowGroupby\n\n        return ExponentialMovingWindowGroupby(\n            self._selected_obj,\n            *args,\n            _grouper=self._grouper,\n            **kwargs,\n        )\n\n    @final\n    def _fill(self, direction: Literal[\"ffill\", \"bfill\"], limit: int | None = None):\n        \"\"\"\n        Shared function for `pad` and `backfill` to call Cython method.\n\n        Parameters\n        ----------\n        direction : {'ffill', 'bfill'}\n            Direction passed to underlying Cython function. `bfill` will cause\n            values to be filled backwards. `ffill` and any other values will\n            default to a forward fill\n        limit : int, default None\n            Maximum number of consecutive values to fill. If `None`, this\n            method will convert to -1 prior to passing to Cython\n\n        Returns\n        -------\n        `Series` or `DataFrame` with filled values\n\n        See Also\n        --------\n        pad : Returns Series with minimum number of char in object.\n        backfill : Backward fill the missing values in the dataset.\n        \"\"\"\n        # Need int value for Cython\n        if limit is None:\n            limit = -1\n\n        ids, _, _ = self._grouper.group_info\n        sorted_labels = np.argsort(ids, kind=\"mergesort\").astype(np.intp, copy=False)\n        if direction == \"bfill\":\n            sorted_labels = sorted_labels[::-1]\n\n        col_func = partial(\n            libgroupby.group_fillna_indexer,\n            labels=ids,\n            sorted_labels=sorted_labels,\n            limit=limit,\n            dropna=self.dropna,\n        )\n\n        def blk_func(values: ArrayLike) -> ArrayLike:\n            mask = isna(values)\n            if values.ndim == 1:\n                indexer = np.empty(values.shape, dtype=np.intp)\n                col_func(out=indexer, mask=mask)\n                return algorithms.take_nd(values, indexer)\n\n            else:\n                # We broadcast algorithms.take_nd analogous to\n                #  np.take_along_axis\n                if isinstance(values, np.ndarray):\n                    dtype = values.dtype\n                    if self._grouper.has_dropped_na:\n                        # dropped null groups give rise to nan in the result\n                        dtype = ensure_dtype_can_hold_na(values.dtype)\n                    out = np.empty(values.shape, dtype=dtype)\n                else:\n                    # Note: we only get here with backfill/pad,\n                    #  so if we have a dtype that cannot hold NAs,\n                    #  then there will be no -1s in indexer, so we can use\n                    #  the original dtype (no need to ensure_dtype_can_hold_na)\n                    out = type(values)._empty(values.shape, dtype=values.dtype)\n\n                for i, value_element in enumerate(values):\n                    # call group_fillna_indexer column-wise\n                    indexer = np.empty(values.shape[1], dtype=np.intp)\n                    col_func(out=indexer, mask=mask[i])\n                    out[i, :] = algorithms.take_nd(value_element, indexer)\n                return out\n\n        mgr = self._get_data_to_aggregate()\n        res_mgr = mgr.apply(blk_func)\n\n        new_obj = self._wrap_agged_manager(res_mgr)\n\n        if self.axis == 1:\n            # Only relevant for DataFrameGroupBy\n            new_obj = new_obj.T\n            new_obj.columns = self.obj.columns\n\n        new_obj.index = self.obj.index\n        return new_obj\n\n    @final\n    @Substitution(name=\"groupby\")\n    def ffill(self, limit: int | None = None):\n        \"\"\"\n        Forward fill the values.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        Series.ffill: Returns Series with minimum number of char in object.\n        DataFrame.ffill: Object with missing values filled or None if inplace=True.\n        Series.fillna: Fill NaN values of a Series.\n        DataFrame.fillna: Fill NaN values of a DataFrame.\n\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> key = [0, 0, 1, 1]\n        >>> ser = pd.Series([np.nan, 2, 3, np.nan], index=key)\n        >>> ser\n        0    NaN\n        0    2.0\n        1    3.0\n        1    NaN\n        dtype: float64\n        >>> ser.groupby(level=0).ffill()\n        0    NaN\n        0    2.0\n        1    3.0\n        1    3.0\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"key\": [0, 0, 1, 1, 1],\n        ...         \"A\": [np.nan, 2, np.nan, 3, np.nan],\n        ...         \"B\": [2, 3, np.nan, np.nan, np.nan],\n        ...         \"C\": [np.nan, np.nan, 2, np.nan, np.nan],\n        ...     }\n        ... )\n        >>> df\n           key    A    B   C\n        0    0  NaN  2.0 NaN\n        1    0  2.0  3.0 NaN\n        2    1  NaN  NaN 2.0\n        3    1  3.0  NaN NaN\n        4    1  NaN  NaN NaN\n\n        Propagate non-null values forward or backward within each group along columns.\n\n        >>> df.groupby(\"key\").ffill()\n             A    B   C\n        0  NaN  2.0 NaN\n        1  2.0  3.0 NaN\n        2  NaN  NaN 2.0\n        3  3.0  NaN 2.0\n        4  3.0  NaN 2.0\n\n        Propagate non-null values forward or backward within each group along rows.\n\n        >>> df.T.groupby(np.array([0, 0, 1, 1])).ffill().T\n           key    A    B    C\n        0  0.0  0.0  2.0  2.0\n        1  0.0  2.0  3.0  3.0\n        2  1.0  1.0  NaN  2.0\n        3  1.0  3.0  NaN  NaN\n        4  1.0  1.0  NaN  NaN\n\n        Only replace the first NaN element within a group along rows.\n\n        >>> df.groupby(\"key\").ffill(limit=1)\n             A    B    C\n        0  NaN  2.0  NaN\n        1  2.0  3.0  NaN\n        2  NaN  NaN  2.0\n        3  3.0  NaN  2.0\n        4  3.0  NaN  NaN\n        \"\"\"\n        return self._fill(\"ffill\", limit=limit)\n\n    @final\n    @Substitution(name=\"groupby\")\n    def bfill(self, limit: int | None = None):\n        \"\"\"\n        Backward fill the values.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        Series.bfill :  Backward fill the missing values in the dataset.\n        DataFrame.bfill:  Backward fill the missing values in the dataset.\n        Series.fillna: Fill NaN values of a Series.\n        DataFrame.fillna: Fill NaN values of a DataFrame.\n\n        Examples\n        --------\n\n        With Series:\n\n        >>> index = ['Falcon', 'Falcon', 'Parrot', 'Parrot', 'Parrot']\n        >>> s = pd.Series([None, 1, None, None, 3], index=index)\n        >>> s\n        Falcon    NaN\n        Falcon    1.0\n        Parrot    NaN\n        Parrot    NaN\n        Parrot    3.0\n        dtype: float64\n        >>> s.groupby(level=0).bfill()\n        Falcon    1.0\n        Falcon    1.0\n        Parrot    3.0\n        Parrot    3.0\n        Parrot    3.0\n        dtype: float64\n        >>> s.groupby(level=0).bfill(limit=1)\n        Falcon    1.0\n        Falcon    1.0\n        Parrot    NaN\n        Parrot    3.0\n        Parrot    3.0\n        dtype: float64\n\n        With DataFrame:\n\n        >>> df = pd.DataFrame({'A': [1, None, None, None, 4],\n        ...                    'B': [None, None, 5, None, 7]}, index=index)\n        >>> df\n                  A\t    B\n        Falcon\t1.0\t  NaN\n        Falcon\tNaN\t  NaN\n        Parrot\tNaN\t  5.0\n        Parrot\tNaN\t  NaN\n        Parrot\t4.0\t  7.0\n        >>> df.groupby(level=0).bfill()\n                  A\t    B\n        Falcon\t1.0\t  NaN\n        Falcon\tNaN\t  NaN\n        Parrot\t4.0\t  5.0\n        Parrot\t4.0\t  7.0\n        Parrot\t4.0\t  7.0\n        >>> df.groupby(level=0).bfill(limit=1)\n                  A\t    B\n        Falcon\t1.0\t  NaN\n        Falcon\tNaN\t  NaN\n        Parrot\tNaN\t  5.0\n        Parrot\t4.0\t  7.0\n        Parrot\t4.0\t  7.0\n        \"\"\"\n        return self._fill(\"bfill\", limit=limit)\n\n    @final\n    @property\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def nth(self) -> GroupByNthSelector:\n        \"\"\"\n        Take the nth row from each group if n is an int, otherwise a subset of rows.\n\n        Can be either a call or an index. dropna is not available with index notation.\n        Index notation accepts a comma separated list of integers and slices.\n\n        If dropna, will take the nth non-null row, dropna is either\n        'all' or 'any'; this is equivalent to calling dropna(how=dropna)\n        before the groupby.\n\n        Parameters\n        ----------\n        n : int, slice or list of ints and slices\n            A single nth value for the row or a list of nth values or slices.\n\n            .. versionchanged:: 1.4.0\n                Added slice and lists containing slices.\n                Added index notation.\n\n        dropna : {'any', 'all', None}, default None\n            Apply the specified dropna operation before counting which row is\n            the nth row. Only supported if n is an int.\n\n        Returns\n        -------\n        Series or DataFrame\n            N-th value within each group.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5]}, columns=['A', 'B'])\n        >>> g = df.groupby('A')\n        >>> g.nth(0)\n           A   B\n        0  1 NaN\n        2  2 3.0\n        >>> g.nth(1)\n           A   B\n        1  1 2.0\n        4  2 5.0\n        >>> g.nth(-1)\n           A   B\n        3  1 4.0\n        4  2 5.0\n        >>> g.nth([0, 1])\n           A   B\n        0  1 NaN\n        1  1 2.0\n        2  2 3.0\n        4  2 5.0\n        >>> g.nth(slice(None, -1))\n           A   B\n        0  1 NaN\n        1  1 2.0\n        2  2 3.0\n\n        Index notation may also be used\n\n        >>> g.nth[0, 1]\n           A   B\n        0  1 NaN\n        1  1 2.0\n        2  2 3.0\n        4  2 5.0\n        >>> g.nth[:-1]\n           A   B\n        0  1 NaN\n        1  1 2.0\n        2  2 3.0\n\n        Specifying `dropna` allows ignoring ``NaN`` values\n\n        >>> g.nth(0, dropna='any')\n           A   B\n        1  1 2.0\n        2  2 3.0\n\n        When the specified ``n`` is larger than any of the groups, an\n        empty DataFrame is returned\n\n        >>> g.nth(3, dropna='any')\n        Empty DataFrame\n        Columns: [A, B]\n        Index: []\n        \"\"\"\n        return GroupByNthSelector(self)\n\n    def _nth(\n        self,\n        n: PositionalIndexer | tuple,\n        dropna: Literal[\"any\", \"all\", None] = None,\n    ) -> NDFrameT:\n        if not dropna:\n            mask = self._make_mask_from_positional_indexer(n)\n\n            ids, _, _ = self._grouper.group_info\n\n            # Drop NA values in grouping\n            mask = mask & (ids != -1)\n\n            out = self._mask_selected_obj(mask)\n            return out\n\n        # dropna is truthy\n        if not is_integer(n):\n            raise ValueError(\"dropna option only supported for an integer argument\")\n\n        if dropna not in [\"any\", \"all\"]:\n            # Note: when agg-ing picker doesn't raise this, just returns NaN\n            raise ValueError(\n                \"For a DataFrame or Series groupby.nth, dropna must be \"\n                \"either None, 'any' or 'all', \"\n                f\"(was passed {dropna}).\"\n            )\n\n        # old behaviour, but with all and any support for DataFrames.\n        # modified in GH 7559 to have better perf\n        n = cast(int, n)\n        dropped = self._selected_obj.dropna(how=dropna, axis=self.axis)\n\n        # get a new grouper for our dropped obj\n        grouper: np.ndarray | Index | ops.BaseGrouper\n        if len(dropped) == len(self._selected_obj):\n            # Nothing was dropped, can use the same grouper\n            grouper = self._grouper\n        else:\n            # we don't have the grouper info available\n            # (e.g. we have selected out\n            # a column that is not in the current object)\n            axis = self._grouper.axis\n            grouper = self._grouper.codes_info[axis.isin(dropped.index)]\n            if self._grouper.has_dropped_na:\n                # Null groups need to still be encoded as -1 when passed to groupby\n                nulls = grouper == -1\n                # error: No overload variant of \"where\" matches argument types\n                #        \"Any\", \"NAType\", \"Any\"\n                values = np.where(nulls, NA, grouper)  # type: ignore[call-overload]\n                grouper = Index(values, dtype=\"Int64\")\n\n        if self.axis == 1:\n            grb = dropped.T.groupby(grouper, as_index=self.as_index, sort=self.sort)\n        else:\n            grb = dropped.groupby(grouper, as_index=self.as_index, sort=self.sort)\n        return grb.nth(n)\n\n    @final\n    def quantile(\n        self,\n        q: float | AnyArrayLike = 0.5,\n        interpolation: str = \"linear\",\n        numeric_only: bool = False,\n    ):\n        \"\"\"\n        Return group values at the given quantile, a la numpy.percentile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            Value(s) between 0 and 1 providing the quantile(s) to compute.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            Method to use when the desired quantile falls between two points.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only now defaults to ``False``.\n\n        Returns\n        -------\n        Series or DataFrame\n            Return type determined by caller of GroupBy object.\n\n        See Also\n        --------\n        Series.quantile : Similar method for Series.\n        DataFrame.quantile : Similar method for DataFrame.\n        numpy.percentile : NumPy method to compute qth percentile.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([\n        ...     ['a', 1], ['a', 2], ['a', 3],\n        ...     ['b', 1], ['b', 3], ['b', 5]\n        ... ], columns=['key', 'val'])\n        >>> df.groupby('key').quantile()\n            val\n        key\n        a    2.0\n        b    3.0\n        \"\"\"\n        mgr = self._get_data_to_aggregate(numeric_only=numeric_only, name=\"quantile\")\n        obj = self._wrap_agged_manager(mgr)\n        if self.axis == 1:\n            splitter = self._grouper._get_splitter(obj.T, axis=self.axis)\n            sdata = splitter._sorted_data.T\n        else:\n            splitter = self._grouper._get_splitter(obj, axis=self.axis)\n            sdata = splitter._sorted_data\n\n        starts, ends = lib.generate_slices(splitter._slabels, splitter.ngroups)\n\n        def pre_processor(vals: ArrayLike) -> tuple[np.ndarray, DtypeObj | None]:\n            if is_object_dtype(vals.dtype):\n                raise TypeError(\n                    \"'quantile' cannot be performed against 'object' dtypes!\"\n                )\n\n            inference: DtypeObj | None = None\n            if isinstance(vals, BaseMaskedArray) and is_numeric_dtype(vals.dtype):\n                out = vals.to_numpy(dtype=float, na_value=np.nan)\n                inference = vals.dtype\n            elif is_integer_dtype(vals.dtype):\n                if isinstance(vals, ExtensionArray):\n                    out = vals.to_numpy(dtype=float, na_value=np.nan)\n                else:\n                    out = vals\n                inference = np.dtype(np.int64)\n            elif is_bool_dtype(vals.dtype) and isinstance(vals, ExtensionArray):\n                out = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_bool_dtype(vals.dtype):\n                # GH#51424 deprecate to match Series/DataFrame behavior\n                warnings.warn(\n                    f\"Allowing bool dtype in {type(self).__name__}.quantile is \"\n                    \"deprecated and will raise in a future version, matching \"\n                    \"the Series/DataFrame behavior. Cast to uint8 dtype before \"\n                    \"calling quantile instead.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n                out = np.asarray(vals)\n            elif needs_i8_conversion(vals.dtype):\n                inference = vals.dtype\n                # In this case we need to delay the casting until after the\n                #  np.lexsort below.\n                # error: Incompatible return value type (got\n                # \"Tuple[Union[ExtensionArray, ndarray[Any, Any]], Union[Any,\n                # ExtensionDtype]]\", expected \"Tuple[ndarray[Any, Any],\n                # Optional[Union[dtype[Any], ExtensionDtype]]]\")\n                return vals, inference  # type: ignore[return-value]\n            elif isinstance(vals, ExtensionArray) and is_float_dtype(vals.dtype):\n                inference = np.dtype(np.float64)\n                out = vals.to_numpy(dtype=float, na_value=np.nan)\n            else:\n                out = np.asarray(vals)\n\n            return out, inference\n\n        def post_processor(\n            vals: np.ndarray,\n            inference: DtypeObj | None,\n            result_mask: np.ndarray | None,\n            orig_vals: ArrayLike,\n        ) -> ArrayLike:\n            if inference:\n                # Check for edge case\n                if isinstance(orig_vals, BaseMaskedArray):\n                    assert result_mask is not None  # for mypy\n\n                    if interpolation in {\"linear\", \"midpoint\"} and not is_float_dtype(\n                        orig_vals\n                    ):\n                        return FloatingArray(vals, result_mask)\n                    else:\n                        # Item \"ExtensionDtype\" of \"Union[ExtensionDtype, str,\n                        # dtype[Any], Type[object]]\" has no attribute \"numpy_dtype\"\n                        # [union-attr]\n                        with warnings.catch_warnings():\n                            # vals.astype with nan can warn with numpy >1.24\n                            warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n                            return type(orig_vals)(\n                                vals.astype(\n                                    inference.numpy_dtype  # type: ignore[union-attr]\n                                ),\n                                result_mask,\n                            )\n\n                elif not (\n                    is_integer_dtype(inference)\n                    and interpolation in {\"linear\", \"midpoint\"}\n                ):\n                    if needs_i8_conversion(inference):\n                        # error: Item \"ExtensionArray\" of \"Union[ExtensionArray,\n                        # ndarray[Any, Any]]\" has no attribute \"_ndarray\"\n                        vals = vals.astype(\"i8\").view(\n                            orig_vals._ndarray.dtype  # type: ignore[union-attr]\n                        )\n                        # error: Item \"ExtensionArray\" of \"Union[ExtensionArray,\n                        # ndarray[Any, Any]]\" has no attribute \"_from_backing_data\"\n                        return orig_vals._from_backing_data(  # type: ignore[union-attr]\n                            vals\n                        )\n\n                    assert isinstance(inference, np.dtype)  # for mypy\n                    return vals.astype(inference)\n\n            return vals\n\n        qs = np.array(q, dtype=np.float64)\n        pass_qs: np.ndarray | None = qs\n        if is_scalar(q):\n            qs = np.array([q], dtype=np.float64)\n            pass_qs = None\n\n        ids, _, ngroups = self._grouper.group_info\n        nqs = len(qs)\n\n        func = partial(\n            libgroupby.group_quantile,\n            labels=ids,\n            qs=qs,\n            interpolation=interpolation,\n            starts=starts,\n            ends=ends,\n        )\n\n        def blk_func(values: ArrayLike) -> ArrayLike:\n            orig_vals = values\n            if isinstance(values, BaseMaskedArray):\n                mask = values._mask\n                result_mask = np.zeros((ngroups, nqs), dtype=np.bool_)\n            else:\n                mask = isna(values)\n                result_mask = None\n\n            is_datetimelike = needs_i8_conversion(values.dtype)\n\n            vals, inference = pre_processor(values)\n\n            ncols = 1\n            if vals.ndim == 2:\n                ncols = vals.shape[0]\n\n            out = np.empty((ncols, ngroups, nqs), dtype=np.float64)\n\n            if is_datetimelike:\n                vals = vals.view(\"i8\")\n\n            if vals.ndim == 1:\n                # EA is always 1d\n                func(\n                    out[0],\n                    values=vals,\n                    mask=mask,\n                    result_mask=result_mask,\n                    is_datetimelike=is_datetimelike,\n                )\n            else:\n                for i in range(ncols):\n                    func(\n                        out[i],\n                        values=vals[i],\n                        mask=mask[i],\n                        result_mask=None,\n                        is_datetimelike=is_datetimelike,\n                    )\n\n            if vals.ndim == 1:\n                out = out.ravel(\"K\")\n                if result_mask is not None:\n                    result_mask = result_mask.ravel(\"K\")\n            else:\n                out = out.reshape(ncols, ngroups * nqs)\n\n            return post_processor(out, inference, result_mask, orig_vals)\n\n        res_mgr = sdata._mgr.grouped_reduce(blk_func)\n\n        res = self._wrap_agged_manager(res_mgr)\n        return self._wrap_aggregated_output(res, qs=pass_qs)\n\n    @final\n    @Substitution(name=\"groupby\")\n    def ngroup(self, ascending: bool = True):\n        \"\"\"\n        Number each group from 0 to the number of groups - 1.\n\n        This is the enumerative complement of cumcount.  Note that the\n        numbers given to the groups match the order in which the groups\n        would be seen when iterating over the groupby object, not the\n        order they are first observed.\n\n        Groups with missing keys (where `pd.isna()` is True) will be labeled with `NaN`\n        and will be skipped from the count.\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from number of group - 1 to 0.\n\n        Returns\n        -------\n        Series\n            Unique numbers for each group.\n\n        See Also\n        --------\n        .cumcount : Number the rows in each group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"color\": [\"red\", None, \"red\", \"blue\", \"blue\", \"red\"]})\n        >>> df\n           color\n        0    red\n        1   None\n        2    red\n        3   blue\n        4   blue\n        5    red\n        >>> df.groupby(\"color\").ngroup()\n        0    1.0\n        1    NaN\n        2    1.0\n        3    0.0\n        4    0.0\n        5    1.0\n        dtype: float64\n        >>> df.groupby(\"color\", dropna=False).ngroup()\n        0    1\n        1    2\n        2    1\n        3    0\n        4    0\n        5    1\n        dtype: int64\n        >>> df.groupby(\"color\", dropna=False).ngroup(ascending=False)\n        0    1\n        1    0\n        2    1\n        3    2\n        4    2\n        5    1\n        dtype: int64\n        \"\"\"\n        obj = self._obj_with_exclusions\n        index = obj._get_axis(self.axis)\n        comp_ids = self._grouper.group_info[0]\n\n        dtype: type\n        if self._grouper.has_dropped_na:\n            comp_ids = np.where(comp_ids == -1, np.nan, comp_ids)\n            dtype = np.float64\n        else:\n            dtype = np.int64\n\n        if any(ping._passed_categorical for ping in self._grouper.groupings):\n            # comp_ids reflect non-observed groups, we need only observed\n            comp_ids = rank_1d(comp_ids, ties_method=\"dense\") - 1\n\n        result = self._obj_1d_constructor(comp_ids, index, dtype=dtype)\n        if not ascending:\n            result = self.ngroups - 1 - result\n        return result\n\n    @final\n    @Substitution(name=\"groupby\")\n    def cumcount(self, ascending: bool = True):\n        \"\"\"\n        Number each item in each group from 0 to the length of that group - 1.\n\n        Essentially this is equivalent to\n\n        .. code-block:: python\n\n            self.apply(lambda x: pd.Series(np.arange(len(x)), x.index))\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Returns\n        -------\n        Series\n            Sequence number of each element within each group.\n\n        See Also\n        --------\n        .ngroup : Number the groups themselves.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],\n        ...                   columns=['A'])\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').cumcount()\n        0    0\n        1    1\n        2    2\n        3    0\n        4    1\n        5    3\n        dtype: int64\n        >>> df.groupby('A').cumcount(ascending=False)\n        0    3\n        1    2\n        2    1\n        3    1\n        4    0\n        5    0\n        dtype: int64\n        \"\"\"\n        index = self._obj_with_exclusions._get_axis(self.axis)\n        cumcounts = self._cumcount_array(ascending=ascending)\n        return self._obj_1d_constructor(cumcounts, index)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def rank(\n        self,\n        method: str = \"average\",\n        ascending: bool = True,\n        na_option: str = \"keep\",\n        pct: bool = False,\n        axis: AxisInt | lib.NoDefault = lib.no_default,\n    ) -> NDFrameT:\n        \"\"\"\n        Provide the rank of values within each group.\n\n        Parameters\n        ----------\n        method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n            * average: average rank of group.\n            * min: lowest rank in group.\n            * max: highest rank in group.\n            * first: ranks assigned in order they appear in the array.\n            * dense: like 'min', but rank always increases by 1 between groups.\n        ascending : bool, default True\n            False for ranks by high (1) to low (N).\n        na_option : {'keep', 'top', 'bottom'}, default 'keep'\n            * keep: leave NA values where they are.\n            * top: smallest rank if ascending.\n            * bottom: smallest rank if descending.\n        pct : bool, default False\n            Compute percentage rank of data within each group.\n        axis : int, default 0\n            The axis of the object over which to compute the rank.\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        Returns\n        -------\n        DataFrame with ranking of values within each group\n        %(see_also)s\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"group\": [\"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\"],\n        ...         \"value\": [2, 4, 2, 3, 5, 1, 2, 4, 1, 5],\n        ...     }\n        ... )\n        >>> df\n          group  value\n        0     a      2\n        1     a      4\n        2     a      2\n        3     a      3\n        4     a      5\n        5     b      1\n        6     b      2\n        7     b      4\n        8     b      1\n        9     b      5\n        >>> for method in ['average', 'min', 'max', 'dense', 'first']:\n        ...     df[f'{method}_rank'] = df.groupby('group')['value'].rank(method)\n        >>> df\n          group  value  average_rank  min_rank  max_rank  dense_rank  first_rank\n        0     a      2           1.5       1.0       2.0         1.0         1.0\n        1     a      4           4.0       4.0       4.0         3.0         4.0\n        2     a      2           1.5       1.0       2.0         1.0         2.0\n        3     a      3           3.0       3.0       3.0         2.0         3.0\n        4     a      5           5.0       5.0       5.0         4.0         5.0\n        5     b      1           1.5       1.0       2.0         1.0         1.0\n        6     b      2           3.0       3.0       3.0         2.0         3.0\n        7     b      4           4.0       4.0       4.0         3.0         4.0\n        8     b      1           1.5       1.0       2.0         1.0         2.0\n        9     b      5           5.0       5.0       5.0         4.0         5.0\n        \"\"\"\n        if na_option not in {\"keep\", \"top\", \"bottom\"}:\n            msg = \"na_option must be one of 'keep', 'top', or 'bottom'\"\n            raise ValueError(msg)\n\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, \"rank\")\n        else:\n            axis = 0\n\n        kwargs = {\n            \"ties_method\": method,\n            \"ascending\": ascending,\n            \"na_option\": na_option,\n            \"pct\": pct,\n        }\n        if axis != 0:\n            # DataFrame uses different keyword name\n            kwargs[\"method\"] = kwargs.pop(\"ties_method\")\n            f = lambda x: x.rank(axis=axis, numeric_only=False, **kwargs)\n            result = self._python_apply_general(\n                f, self._selected_obj, is_transform=True\n            )\n            return result\n\n        return self._cython_transform(\n            \"rank\",\n            numeric_only=False,\n            axis=axis,\n            **kwargs,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def cumprod(\n        self, axis: Axis | lib.NoDefault = lib.no_default, *args, **kwargs\n    ) -> NDFrameT:\n        \"\"\"\n        Cumulative product for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([6, 2, 0], index=lst)\n        >>> ser\n        a    6\n        a    2\n        b    0\n        dtype: int64\n        >>> ser.groupby(level=0).cumprod()\n        a    6\n        a   12\n        b    0\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"cow\", \"horse\", \"bull\"])\n        >>> df\n                a   b   c\n        cow     1   8   2\n        horse   1   2   5\n        bull    2   6   9\n        >>> df.groupby(\"a\").groups\n        {1: ['cow', 'horse'], 2: ['bull']}\n        >>> df.groupby(\"a\").cumprod()\n                b   c\n        cow     8   2\n        horse  16  10\n        bull    6   9\n        \"\"\"\n        nv.validate_groupby_func(\"cumprod\", args, kwargs, [\"numeric_only\", \"skipna\"])\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, \"cumprod\")\n        else:\n            axis = 0\n\n        if axis != 0:\n            f = lambda x: x.cumprod(axis=axis, **kwargs)\n            return self._python_apply_general(f, self._selected_obj, is_transform=True)\n\n        return self._cython_transform(\"cumprod\", **kwargs)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def cumsum(\n        self, axis: Axis | lib.NoDefault = lib.no_default, *args, **kwargs\n    ) -> NDFrameT:\n        \"\"\"\n        Cumulative sum for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([6, 2, 0], index=lst)\n        >>> ser\n        a    6\n        a    2\n        b    0\n        dtype: int64\n        >>> ser.groupby(level=0).cumsum()\n        a    6\n        a    8\n        b    0\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"fox\", \"gorilla\", \"lion\"])\n        >>> df\n                  a   b   c\n        fox       1   8   2\n        gorilla   1   2   5\n        lion      2   6   9\n        >>> df.groupby(\"a\").groups\n        {1: ['fox', 'gorilla'], 2: ['lion']}\n        >>> df.groupby(\"a\").cumsum()\n                  b   c\n        fox       8   2\n        gorilla  10   7\n        lion      6   9\n        \"\"\"\n        nv.validate_groupby_func(\"cumsum\", args, kwargs, [\"numeric_only\", \"skipna\"])\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, \"cumsum\")\n        else:\n            axis = 0\n\n        if axis != 0:\n            f = lambda x: x.cumsum(axis=axis, **kwargs)\n            return self._python_apply_general(f, self._selected_obj, is_transform=True)\n\n        return self._cython_transform(\"cumsum\", **kwargs)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def cummin(\n        self,\n        axis: AxisInt | lib.NoDefault = lib.no_default,\n        numeric_only: bool = False,\n        **kwargs,\n    ) -> NDFrameT:\n        \"\"\"\n        Cumulative min for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([1, 6, 2, 3, 0, 4], index=lst)\n        >>> ser\n        a    1\n        a    6\n        a    2\n        b    3\n        b    0\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).cummin()\n        a    1\n        a    1\n        a    1\n        b    3\n        b    0\n        b    0\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 0, 2], [1, 1, 5], [6, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"snake\", \"rabbit\", \"turtle\"])\n        >>> df\n                a   b   c\n        snake   1   0   2\n        rabbit  1   1   5\n        turtle  6   6   9\n        >>> df.groupby(\"a\").groups\n        {1: ['snake', 'rabbit'], 6: ['turtle']}\n        >>> df.groupby(\"a\").cummin()\n                b   c\n        snake   0   2\n        rabbit  0   2\n        turtle  6   9\n        \"\"\"\n        skipna = kwargs.get(\"skipna\", True)\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, \"cummin\")\n        else:\n            axis = 0\n\n        if axis != 0:\n            f = lambda x: np.minimum.accumulate(x, axis)\n            obj = self._selected_obj\n            if numeric_only:\n                obj = obj._get_numeric_data()\n            return self._python_apply_general(f, obj, is_transform=True)\n\n        return self._cython_transform(\n            \"cummin\", numeric_only=numeric_only, skipna=skipna\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def cummax(\n        self,\n        axis: AxisInt | lib.NoDefault = lib.no_default,\n        numeric_only: bool = False,\n        **kwargs,\n    ) -> NDFrameT:\n        \"\"\"\n        Cumulative max for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([1, 6, 2, 3, 1, 4], index=lst)\n        >>> ser\n        a    1\n        a    6\n        a    2\n        b    3\n        b    1\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).cummax()\n        a    1\n        a    6\n        a    6\n        b    3\n        b    3\n        b    4\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 8, 2], [1, 1, 0], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"cow\", \"horse\", \"bull\"])\n        >>> df\n                a   b   c\n        cow     1   8   2\n        horse   1   1   0\n        bull    2   6   9\n        >>> df.groupby(\"a\").groups\n        {1: ['cow', 'horse'], 2: ['bull']}\n        >>> df.groupby(\"a\").cummax()\n                b   c\n        cow     8   2\n        horse   8   2\n        bull    6   9\n        \"\"\"\n        skipna = kwargs.get(\"skipna\", True)\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, \"cummax\")\n        else:\n            axis = 0\n\n        if axis != 0:\n            f = lambda x: np.maximum.accumulate(x, axis)\n            obj = self._selected_obj\n            if numeric_only:\n                obj = obj._get_numeric_data()\n            return self._python_apply_general(f, obj, is_transform=True)\n\n        return self._cython_transform(\n            \"cummax\", numeric_only=numeric_only, skipna=skipna\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    def shift(\n        self,\n        periods: int | Sequence[int] = 1,\n        freq=None,\n        axis: Axis | lib.NoDefault = lib.no_default,\n        fill_value=lib.no_default,\n        suffix: str | None = None,\n    ):\n        \"\"\"\n        Shift each group by periods observations.\n\n        If freq is passed, the index will be increased using the periods and the freq.\n\n        Parameters\n        ----------\n        periods : int | Sequence[int], default 1\n            Number of periods to shift. If a list of values, shift each group by\n            each period.\n        freq : str, optional\n            Frequency string.\n        axis : axis to shift, default 0\n            Shift direction.\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        fill_value : optional\n            The scalar value to use for newly introduced missing values.\n\n            .. versionchanged:: 2.1.0\n                Will raise a ``ValueError`` if ``freq`` is provided too.\n\n        suffix : str, optional\n            A string to add to each shifted column if there are multiple periods.\n            Ignored otherwise.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object shifted within each group.\n\n        See Also\n        --------\n        Index.shift : Shift values of Index.\n\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b', 'b']\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    3\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).shift(1)\n        a    NaN\n        a    1.0\n        b    NaN\n        b    3.0\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 2, 3], [1, 5, 6], [2, 5, 8], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"tuna\", \"salmon\", \"catfish\", \"goldfish\"])\n        >>> df\n                   a  b  c\n            tuna   1  2  3\n          salmon   1  5  6\n         catfish   2  5  8\n        goldfish   2  6  9\n        >>> df.groupby(\"a\").shift(1)\n                      b    c\n            tuna    NaN  NaN\n          salmon    2.0  3.0\n         catfish    NaN  NaN\n        goldfish    5.0  8.0\n        \"\"\"\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, \"shift\")\n        else:\n            axis = 0\n\n        if is_list_like(periods):\n            if axis == 1:\n                raise ValueError(\n                    \"If `periods` contains multiple shifts, `axis` cannot be 1.\"\n                )\n            periods = cast(Sequence, periods)\n            if len(periods) == 0:\n                raise ValueError(\"If `periods` is an iterable, it cannot be empty.\")\n            from pandas.core.reshape.concat import concat\n\n            add_suffix = True\n        else:\n            if not is_integer(periods):\n                raise TypeError(\n                    f\"Periods must be integer, but {periods} is {type(periods)}.\"\n                )\n            if suffix:\n                raise ValueError(\"Cannot specify `suffix` if `periods` is an int.\")\n            periods = [cast(int, periods)]\n            add_suffix = False\n\n        shifted_dataframes = []\n        for period in periods:\n            if not is_integer(period):\n                raise TypeError(\n                    f\"Periods must be integer, but {period} is {type(period)}.\"\n                )\n            period = cast(int, period)\n            if freq is not None or axis != 0:\n                f = lambda x: x.shift(\n                    period, freq, axis, fill_value  # pylint: disable=cell-var-from-loop\n                )\n                shifted = self._python_apply_general(\n                    f, self._selected_obj, is_transform=True\n                )\n            else:\n                if fill_value is lib.no_default:\n                    fill_value = None\n                ids, _, ngroups = self._grouper.group_info\n                res_indexer = np.zeros(len(ids), dtype=np.int64)\n\n                libgroupby.group_shift_indexer(res_indexer, ids, ngroups, period)\n\n                obj = self._obj_with_exclusions\n\n                shifted = obj._reindex_with_indexers(\n                    {self.axis: (obj.axes[self.axis], res_indexer)},\n                    fill_value=fill_value,\n                    allow_dups=True,\n                )\n\n            if add_suffix:\n                if isinstance(shifted, Series):\n                    shifted = cast(NDFrameT, shifted.to_frame())\n                shifted = shifted.add_suffix(\n                    f\"{suffix}_{period}\" if suffix else f\"_{period}\"\n                )\n            shifted_dataframes.append(cast(Union[Series, DataFrame], shifted))\n\n        return (\n            shifted_dataframes[0]\n            if len(shifted_dataframes) == 1\n            else concat(shifted_dataframes, axis=1)\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def diff(\n        self, periods: int = 1, axis: AxisInt | lib.NoDefault = lib.no_default\n    ) -> NDFrameT:\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of each element compared with another\n        element in the group (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative values.\n        axis : axis to shift, default 0\n            Take difference over rows (0) or columns (1).\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        Returns\n        -------\n        Series or DataFrame\n            First differences.\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n        >>> ser\n        a     7\n        a     2\n        a     8\n        b     4\n        b     3\n        b     3\n        dtype: int64\n        >>> ser.groupby(level=0).diff()\n        a    NaN\n        a   -5.0\n        a    6.0\n        b    NaN\n        b   -1.0\n        b    0.0\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n        >>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n        ...                   'mouse', 'mouse', 'mouse', 'mouse'])\n        >>> df\n                 a  b\n          dog    1  1\n          dog    3  4\n          dog    5  8\n        mouse    7  4\n        mouse    7  4\n        mouse    8  2\n        mouse    3  1\n        >>> df.groupby(level=0).diff()\n                 a    b\n          dog  NaN  NaN\n          dog  2.0  3.0\n          dog  2.0  4.0\n        mouse  NaN  NaN\n        mouse  0.0  0.0\n        mouse  1.0 -2.0\n        mouse -5.0 -1.0\n        \"\"\"\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, \"diff\")\n        else:\n            axis = 0\n\n        if axis != 0:\n            return self.apply(lambda x: x.diff(periods=periods, axis=axis))\n\n        obj = self._obj_with_exclusions\n        shifted = self.shift(periods=periods)\n\n        # GH45562 - to retain existing behavior and match behavior of Series.diff(),\n        # int8 and int16 are coerced to float32 rather than float64.\n        dtypes_to_f32 = [\"int8\", \"int16\"]\n        if obj.ndim == 1:\n            if obj.dtype in dtypes_to_f32:\n                shifted = shifted.astype(\"float32\")\n        else:\n            to_coerce = [c for c, dtype in obj.dtypes.items() if dtype in dtypes_to_f32]\n            if len(to_coerce):\n                shifted = shifted.astype({c: \"float32\" for c in to_coerce})\n\n        return obj - shifted\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def pct_change(\n        self,\n        periods: int = 1,\n        fill_method: FillnaOptions | None | lib.NoDefault = lib.no_default,\n        limit: int | None | lib.NoDefault = lib.no_default,\n        freq=None,\n        axis: Axis | lib.NoDefault = lib.no_default,\n    ):\n        \"\"\"\n        Calculate pct_change of each value to previous entry in group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Percentage changes within each group.\n        %(see_also)s\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b', 'b']\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    3\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).pct_change()\n        a         NaN\n        a    1.000000\n        b         NaN\n        b    0.333333\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 2, 3], [1, 5, 6], [2, 5, 8], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"tuna\", \"salmon\", \"catfish\", \"goldfish\"])\n        >>> df\n                   a  b  c\n            tuna   1  2  3\n          salmon   1  5  6\n         catfish   2  5  8\n        goldfish   2  6  9\n        >>> df.groupby(\"a\").pct_change()\n                    b  c\n            tuna    NaN    NaN\n          salmon    1.5  1.000\n         catfish    NaN    NaN\n        goldfish    0.2  0.125\n        \"\"\"\n        # GH#53491\n        if fill_method not in (lib.no_default, None) or limit is not lib.no_default:\n            warnings.warn(\n                \"The 'fill_method' keyword being not None and the 'limit' keyword in \"\n                f\"{type(self).__name__}.pct_change are deprecated and will be removed \"\n                \"in a future version. Either fill in any non-leading NA values prior \"\n                \"to calling pct_change or specify 'fill_method=None' to not fill NA \"\n                \"values.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        if fill_method is lib.no_default:\n            if limit is lib.no_default and any(\n                grp.isna().values.any() for _, grp in self\n            ):\n                warnings.warn(\n                    \"The default fill_method='ffill' in \"\n                    f\"{type(self).__name__}.pct_change is deprecated and will \"\n                    \"be removed in a future version. Either fill in any \"\n                    \"non-leading NA values prior to calling pct_change or \"\n                    \"specify 'fill_method=None' to not fill NA values.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n            fill_method = \"ffill\"\n        if limit is lib.no_default:\n            limit = None\n\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, \"pct_change\")\n        else:\n            axis = 0\n\n        # TODO(GH#23918): Remove this conditional for SeriesGroupBy when\n        #  GH#23918 is fixed\n        if freq is not None or axis != 0:\n            f = lambda x: x.pct_change(\n                periods=periods,\n                fill_method=fill_method,\n                limit=limit,\n                freq=freq,\n                axis=axis,\n            )\n            return self._python_apply_general(f, self._selected_obj, is_transform=True)\n\n        if fill_method is None:  # GH30463\n            fill_method = \"ffill\"\n            limit = 0\n        filled = getattr(self, fill_method)(limit=limit)\n        if self.axis == 0:\n            fill_grp = filled.groupby(self._grouper.codes, group_keys=self.group_keys)\n        else:\n            fill_grp = filled.T.groupby(self._grouper.codes, group_keys=self.group_keys)\n        shifted = fill_grp.shift(periods=periods, freq=freq)\n        if self.axis == 1:\n            shifted = shifted.T\n        return (filled / shifted) - 1\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def head(self, n: int = 5) -> NDFrameT:\n        \"\"\"\n        Return first n rows of each group.\n\n        Similar to ``.apply(lambda x: x.head(n))``, but it returns a subset of rows\n        from the original DataFrame with original index and order preserved\n        (``as_index`` flag is ignored).\n\n        Parameters\n        ----------\n        n : int\n            If positive: number of entries to include from start of each group.\n            If negative: number of entries to exclude from end of each group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Subset of original Series or DataFrame as determined by n.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A').head(1)\n           A  B\n        0  1  2\n        2  5  6\n        >>> df.groupby('A').head(-1)\n           A  B\n        0  1  2\n        \"\"\"\n        mask = self._make_mask_from_positional_indexer(slice(None, n))\n        return self._mask_selected_obj(mask)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def tail(self, n: int = 5) -> NDFrameT:\n        \"\"\"\n        Return last n rows of each group.\n\n        Similar to ``.apply(lambda x: x.tail(n))``, but it returns a subset of rows\n        from the original DataFrame with original index and order preserved\n        (``as_index`` flag is ignored).\n\n        Parameters\n        ----------\n        n : int\n            If positive: number of entries to include from end of each group.\n            If negative: number of entries to exclude from start of each group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Subset of original Series or DataFrame as determined by n.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A').tail(1)\n           A  B\n        1  a  2\n        3  b  2\n        >>> df.groupby('A').tail(-1)\n           A  B\n        1  a  2\n        3  b  2\n        \"\"\"\n        if n:\n            mask = self._make_mask_from_positional_indexer(slice(-n, None))\n        else:\n            mask = self._make_mask_from_positional_indexer([])\n\n        return self._mask_selected_obj(mask)\n\n    @final\n    def _mask_selected_obj(self, mask: npt.NDArray[np.bool_]) -> NDFrameT:\n        \"\"\"\n        Return _selected_obj with mask applied to the correct axis.\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool]\n            Boolean mask to apply.\n\n        Returns\n        -------\n        Series or DataFrame\n            Filtered _selected_obj.\n        \"\"\"\n        ids = self._grouper.group_info[0]\n        mask = mask & (ids != -1)\n\n        if self.axis == 0:\n            return self._selected_obj[mask]\n        else:\n            return self._selected_obj.iloc[:, mask]\n\n    @final\n    def _reindex_output(\n        self,\n        output: OutputFrameOrSeries,\n        fill_value: Scalar = np.nan,\n        qs: npt.NDArray[np.float64] | None = None,\n    ) -> OutputFrameOrSeries:\n        \"\"\"\n        If we have categorical groupers, then we might want to make sure that\n        we have a fully re-indexed output to the levels. This means expanding\n        the output space to accommodate all values in the cartesian product of\n        our groups, regardless of whether they were observed in the data or\n        not. This will expand the output space if there are missing groups.\n\n        The method returns early without modifying the input if the number of\n        groupings is less than 2, self.observed == True or none of the groupers\n        are categorical.\n\n        Parameters\n        ----------\n        output : Series or DataFrame\n            Object resulting from grouping and applying an operation.\n        fill_value : scalar, default np.nan\n            Value to use for unobserved categories if self.observed is False.\n        qs : np.ndarray[float64] or None, default None\n            quantile values, only relevant for quantile.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object (potentially) re-indexed to include all possible groups.\n        \"\"\"\n        groupings = self._grouper.groupings\n        if len(groupings) == 1:\n            return output\n\n        # if we only care about the observed values\n        # we are done\n        elif self.observed:\n            return output\n\n        # reindexing only applies to a Categorical grouper\n        elif not any(\n            isinstance(ping.grouping_vector, (Categorical, CategoricalIndex))\n            for ping in groupings\n        ):\n            return output\n\n        levels_list = [ping._group_index for ping in groupings]\n        names = self._grouper.names\n        if qs is not None:\n            # error: Argument 1 to \"append\" of \"list\" has incompatible type\n            # \"ndarray[Any, dtype[floating[_64Bit]]]\"; expected \"Index\"\n            levels_list.append(qs)  # type: ignore[arg-type]\n            names = names + [None]\n        index = MultiIndex.from_product(levels_list, names=names)\n        if self.sort:\n            index = index.sort_values()\n\n        if self.as_index:\n            # Always holds for SeriesGroupBy unless GH#36507 is implemented\n            d = {\n                self.obj._get_axis_name(self.axis): index,\n                \"copy\": False,\n                \"fill_value\": fill_value,\n            }\n            return output.reindex(**d)  # type: ignore[arg-type]\n\n        # GH 13204\n        # Here, the categorical in-axis groupers, which need to be fully\n        # expanded, are columns in `output`. An idea is to do:\n        # output = output.set_index(self._grouper.names)\n        #                .reindex(index).reset_index()\n        # but special care has to be taken because of possible not-in-axis\n        # groupers.\n        # So, we manually select and drop the in-axis grouper columns,\n        # reindex `output`, and then reset the in-axis grouper columns.\n\n        # Select in-axis groupers\n        in_axis_grps = [\n            (i, ping.name) for (i, ping) in enumerate(groupings) if ping.in_axis\n        ]\n        if len(in_axis_grps) > 0:\n            g_nums, g_names = zip(*in_axis_grps)\n            output = output.drop(labels=list(g_names), axis=1)\n\n        # Set a temp index and reindex (possibly expanding)\n        output = output.set_index(self._grouper.result_index).reindex(\n            index, copy=False, fill_value=fill_value\n        )\n\n        # Reset in-axis grouper columns\n        # (using level numbers `g_nums` because level names may not be unique)\n        if len(in_axis_grps) > 0:\n            output = output.reset_index(level=g_nums)\n\n        return output.reset_index(drop=True)\n\n    @final\n    def sample(\n        self,\n        n: int | None = None,\n        frac: float | None = None,\n        replace: bool = False,\n        weights: Sequence | Series | None = None,\n        random_state: RandomState | None = None,\n    ):\n        \"\"\"\n        Return a random sample of items from each group.\n\n        You can use `random_state` for reproducibility.\n\n        Parameters\n        ----------\n        n : int, optional\n            Number of items to return for each group. Cannot be used with\n            `frac` and must be no larger than the smallest group unless\n            `replace` is True. Default is one if `frac` is None.\n        frac : float, optional\n            Fraction of items to return. Cannot be used with `n`.\n        replace : bool, default False\n            Allow or disallow sampling of the same row more than once.\n        weights : list-like, optional\n            Default None results in equal probability weighting.\n            If passed a list-like then values must have the same length as\n            the underlying DataFrame or Series object and will be used as\n            sampling probabilities after normalization within each group.\n            Values must be non-negative with at least one positive element\n            within each group.\n        random_state : int, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optional\n            If int, array-like, or BitGenerator, seed for random number generator.\n            If np.random.RandomState or np.random.Generator, use as given.\n\n            .. versionchanged:: 1.4.0\n\n                np.random.Generator objects now accepted\n\n        Returns\n        -------\n        Series or DataFrame\n            A new object of same type as caller containing items randomly\n            sampled within each group from the caller object.\n\n        See Also\n        --------\n        DataFrame.sample: Generate random samples from a DataFrame object.\n        numpy.random.choice: Generate a random sample from a given 1-D numpy\n            array.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"a\": [\"red\"] * 2 + [\"blue\"] * 2 + [\"black\"] * 2, \"b\": range(6)}\n        ... )\n        >>> df\n               a  b\n        0    red  0\n        1    red  1\n        2   blue  2\n        3   blue  3\n        4  black  4\n        5  black  5\n\n        Select one row at random for each distinct value in column a. The\n        `random_state` argument can be used to guarantee reproducibility:\n\n        >>> df.groupby(\"a\").sample(n=1, random_state=1)\n               a  b\n        4  black  4\n        2   blue  2\n        1    red  1\n\n        Set `frac` to sample fixed proportions rather than counts:\n\n        >>> df.groupby(\"a\")[\"b\"].sample(frac=0.5, random_state=2)\n        5    5\n        2    2\n        0    0\n        Name: b, dtype: int64\n\n        Control sample probabilities within groups by setting weights:\n\n        >>> df.groupby(\"a\").sample(\n        ...     n=1,\n        ...     weights=[1, 1, 1, 0, 0, 1],\n        ...     random_state=1,\n        ... )\n               a  b\n        5  black  5\n        2   blue  2\n        0    red  0\n        \"\"\"  # noqa: E501\n        if self._selected_obj.empty:\n            # GH48459 prevent ValueError when object is empty\n            return self._selected_obj\n        size = sample.process_sampling_size(n, frac, replace)\n        if weights is not None:\n            weights_arr = sample.preprocess_weights(\n                self._selected_obj, weights, axis=self.axis\n            )\n\n        random_state = com.random_state(random_state)\n\n        group_iterator = self._grouper.get_iterator(self._selected_obj, self.axis)\n\n        sampled_indices = []\n        for labels, obj in group_iterator:\n            grp_indices = self.indices[labels]\n            group_size = len(grp_indices)\n            if size is not None:\n                sample_size = size\n            else:\n                assert frac is not None\n                sample_size = round(frac * group_size)\n\n            grp_sample = sample.sample(\n                group_size,\n                size=sample_size,\n                replace=replace,\n                weights=None if weights is None else weights_arr[grp_indices],\n                random_state=random_state,\n            )\n            sampled_indices.append(grp_indices[grp_sample])\n\n        sampled_indices = np.concatenate(sampled_indices)\n        return self._selected_obj.take(sampled_indices, axis=self.axis)\n\n    def _idxmax_idxmin(\n        self,\n        how: Literal[\"idxmax\", \"idxmin\"],\n        ignore_unobserved: bool = False,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        skipna: bool = True,\n        numeric_only: bool = False,\n    ) -> NDFrameT:\n        \"\"\"Compute idxmax/idxmin.\n\n        Parameters\n        ----------\n        how : {'idxmin', 'idxmax'}\n            Whether to compute idxmin or idxmax.\n        axis : {{0 or 'index', 1 or 'columns'}}, default None\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n            If axis is not provided, grouper's axis is used.\n        numeric_only : bool, default False\n            Include only float, int, boolean columns.\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n        ignore_unobserved : bool, default False\n            When True and an unobserved group is encountered, do not raise. This used\n            for transform where unobserved groups do not play an impact on the result.\n\n        Returns\n        -------\n        Series or DataFrame\n            idxmax or idxmin for the groupby operation.\n        \"\"\"\n        if axis is not lib.no_default:\n            if axis is None:\n                axis = self.axis\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, how)\n        else:\n            axis = self.axis\n\n        if not self.observed and any(\n            ping._passed_categorical for ping in self._grouper.groupings\n        ):\n            expected_len = np.prod(\n                [len(ping._group_index) for ping in self._grouper.groupings]\n            )\n            if len(self._grouper.groupings) == 1:\n                result_len = len(self._grouper.groupings[0].grouping_vector.unique())\n            else:\n                # result_index only contains observed groups in this case\n                result_len = len(self._grouper.result_index)\n            assert result_len <= expected_len\n            has_unobserved = result_len < expected_len\n\n            raise_err: bool | np.bool_ = not ignore_unobserved and has_unobserved\n            # Only raise an error if there are columns to compute; otherwise we return\n            # an empty DataFrame with an index (possibly including unobserved) but no\n            # columns\n            data = self._obj_with_exclusions\n            if raise_err and isinstance(data, DataFrame):\n                if numeric_only:\n                    data = data._get_numeric_data()\n                raise_err = len(data.columns) > 0\n\n            if raise_err:\n                raise ValueError(\n                    f\"Can't get {how} of an empty group due to unobserved categories. \"\n                    \"Specify observed=True in groupby instead.\"\n                )\n        elif not skipna:\n            if self._obj_with_exclusions.isna().any(axis=None):\n                warnings.warn(\n                    f\"The behavior of {type(self).__name__}.{how} with all-NA \"\n                    \"values, or any-NA and skipna=False, is deprecated. In a future \"\n                    \"version this will raise ValueError\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n\n        if axis == 1:\n            try:\n\n                def func(df):\n                    method = getattr(df, how)\n                    return method(axis=axis, skipna=skipna, numeric_only=numeric_only)\n\n                func.__name__ = how\n                result = self._python_apply_general(\n                    func, self._obj_with_exclusions, not_indexed_same=True\n                )\n            except ValueError as err:\n                name = \"argmax\" if how == \"idxmax\" else \"argmin\"\n                if f\"attempt to get {name} of an empty sequence\" in str(err):\n                    raise ValueError(\n                        f\"Can't get {how} of an empty group due to unobserved \"\n                        \"categories. Specify observed=True in groupby instead.\"\n                    ) from None\n                raise\n            return result\n\n        result = self._agg_general(\n            numeric_only=numeric_only,\n            min_count=1,\n            alias=how,\n            skipna=skipna,\n        )\n        return result\n\n    def _wrap_idxmax_idxmin(self, res: NDFrameT) -> NDFrameT:\n        index = self.obj._get_axis(self.axis)\n        if res.size == 0:\n            result = res.astype(index.dtype)\n        else:\n            if isinstance(index, MultiIndex):\n                index = index.to_flat_index()\n            values = res._values\n            assert isinstance(values, np.ndarray)\n            na_value = na_value_for_dtype(index.dtype, compat=False)\n            if isinstance(res, Series):\n                # mypy: expression has type \"Series\", variable has type \"NDFrameT\"\n                result = res._constructor(  # type: ignore[assignment]\n                    index.array.take(values, allow_fill=True, fill_value=na_value),\n                    index=res.index,\n                    name=res.name,\n                )\n            else:\n                data = {}\n                for k, column_values in enumerate(values.T):\n                    data[k] = index.array.take(\n                        column_values, allow_fill=True, fill_value=na_value\n                    )\n                result = self.obj._constructor(data, index=res.index)\n                result.columns = res.columns\n        return result\n", "class_fn": true, "question_id": "pandas/pandas.core.groupby.groupby/GroupBy", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/indexing.py", "fn_id": "", "content": "class GroupByIndexingMixin:\n    \"\"\"\n    Mixin for adding ._positional_selector to GroupBy.\n    \"\"\"\n\n    @cache_readonly\n    def _positional_selector(self) -> GroupByPositionalSelector:\n        \"\"\"\n        Return positional selection for each group.\n\n        ``groupby._positional_selector[i:j]`` is similar to\n        ``groupby.apply(lambda x: x.iloc[i:j])``\n        but much faster and preserves the original index and order.\n\n        ``_positional_selector[]`` is compatible with and extends :meth:`~GroupBy.head`\n        and :meth:`~GroupBy.tail`. For example:\n\n        - ``head(5)``\n        - ``_positional_selector[5:-5]``\n        - ``tail(5)``\n\n        together return all the rows.\n\n        Allowed inputs for the index are:\n\n        - An integer valued iterable, e.g. ``range(2, 4)``.\n        - A comma separated list of integers and slices, e.g. ``5``, ``2, 4``, ``2:4``.\n\n        The output format is the same as :meth:`~GroupBy.head` and\n        :meth:`~GroupBy.tail`, namely\n        a subset of the ``DataFrame`` or ``Series`` with the index and order preserved.\n\n        Returns\n        -------\n        Series\n            The filtered subset of the original Series.\n        DataFrame\n            The filtered subset of the original DataFrame.\n\n        See Also\n        --------\n        DataFrame.iloc : Purely integer-location based indexing for selection by\n            position.\n        GroupBy.head : Return first n rows of each group.\n        GroupBy.tail : Return last n rows of each group.\n        GroupBy.nth : Take the nth row from each group if n is an int, or a\n            subset of rows, if n is a list of ints.\n\n        Notes\n        -----\n        - The slice step cannot be negative.\n        - If the index specification results in overlaps, the item is not duplicated.\n        - If the index specification changes the order of items, then\n          they are returned in their original order.\n          By contrast, ``DataFrame.iloc`` can change the row order.\n        - ``groupby()`` parameters such as as_index and dropna are ignored.\n\n        The differences between ``_positional_selector[]`` and :meth:`~GroupBy.nth`\n        with ``as_index=False`` are:\n\n        - Input to ``_positional_selector`` can include\n          one or more slices whereas ``nth``\n          just handles an integer or a list of integers.\n        - ``_positional_selector`` can  accept a slice relative to the\n          last row of each group.\n        - ``_positional_selector`` does not have an equivalent to the\n          ``nth()`` ``dropna`` parameter.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[\"a\", 1], [\"a\", 2], [\"a\", 3], [\"b\", 4], [\"b\", 5]],\n        ...                   columns=[\"A\", \"B\"])\n        >>> df.groupby(\"A\")._positional_selector[1:2]\n           A  B\n        1  a  2\n        4  b  5\n\n        >>> df.groupby(\"A\")._positional_selector[1, -1]\n           A  B\n        1  a  2\n        2  a  3\n        4  b  5\n        \"\"\"\n        if TYPE_CHECKING:\n            # pylint: disable-next=used-before-assignment\n            groupby_self = cast(groupby.GroupBy, self)\n        else:\n            groupby_self = self\n\n        return GroupByPositionalSelector(groupby_self)\n\n    def _make_mask_from_positional_indexer(\n        self,\n        arg: PositionalIndexer | tuple,\n    ) -> np.ndarray:\n        if is_list_like(arg):\n            if all(is_integer(i) for i in cast(Iterable, arg)):\n                mask = self._make_mask_from_list(cast(Iterable[int], arg))\n            else:\n                mask = self._make_mask_from_tuple(cast(tuple, arg))\n\n        elif isinstance(arg, slice):\n            mask = self._make_mask_from_slice(arg)\n        elif is_integer(arg):\n            mask = self._make_mask_from_int(cast(int, arg))\n        else:\n            raise TypeError(\n                f\"Invalid index {type(arg)}. \"\n                \"Must be integer, list-like, slice or a tuple of \"\n                \"integers and slices\"\n            )\n\n        if isinstance(mask, bool):\n            if mask:\n                mask = self._ascending_count >= 0\n            else:\n                mask = self._ascending_count < 0\n\n        return cast(np.ndarray, mask)\n\n    def _make_mask_from_int(self, arg: int) -> np.ndarray:\n        if arg >= 0:\n            return self._ascending_count == arg\n        else:\n            return self._descending_count == (-arg - 1)\n\n    def _make_mask_from_list(self, args: Iterable[int]) -> bool | np.ndarray:\n        positive = [arg for arg in args if arg >= 0]\n        negative = [-arg - 1 for arg in args if arg < 0]\n\n        mask: bool | np.ndarray = False\n\n        if positive:\n            mask |= np.isin(self._ascending_count, positive)\n\n        if negative:\n            mask |= np.isin(self._descending_count, negative)\n\n        return mask\n\n    def _make_mask_from_tuple(self, args: tuple) -> bool | np.ndarray:\n        mask: bool | np.ndarray = False\n\n        for arg in args:\n            if is_integer(arg):\n                mask |= self._make_mask_from_int(cast(int, arg))\n            elif isinstance(arg, slice):\n                mask |= self._make_mask_from_slice(arg)\n            else:\n                raise ValueError(\n                    f\"Invalid argument {type(arg)}. Should be int or slice.\"\n                )\n\n        return mask\n\n    def _make_mask_from_slice(self, arg: slice) -> bool | np.ndarray:\n        start = arg.start\n        stop = arg.stop\n        step = arg.step\n\n        if step is not None and step < 0:\n            raise ValueError(f\"Invalid step {step}. Must be non-negative\")\n\n        mask: bool | np.ndarray = True\n\n        if step is None:\n            step = 1\n\n        if start is None:\n            if step > 1:\n                mask &= self._ascending_count % step == 0\n\n        elif start >= 0:\n            mask &= self._ascending_count >= start\n\n            if step > 1:\n                mask &= (self._ascending_count - start) % step == 0\n\n        else:\n            mask &= self._descending_count < -start\n\n            offset_array = self._descending_count + start + 1\n            limit_array = (\n                self._ascending_count + self._descending_count + (start + 1)\n            ) < 0\n            offset_array = np.where(limit_array, self._ascending_count, offset_array)\n\n            mask &= offset_array % step == 0\n\n        if stop is not None:\n            if stop >= 0:\n                mask &= self._ascending_count < stop\n            else:\n                mask &= self._descending_count >= -stop\n\n        return mask\n\n    @cache_readonly\n    def _ascending_count(self) -> np.ndarray:\n        if TYPE_CHECKING:\n            groupby_self = cast(groupby.GroupBy, self)\n        else:\n            groupby_self = self\n\n        return groupby_self._cumcount_array()\n\n    @cache_readonly\n    def _descending_count(self) -> np.ndarray:\n        if TYPE_CHECKING:\n            groupby_self = cast(groupby.GroupBy, self)\n        else:\n            groupby_self = self\n\n        return groupby_self._cumcount_array(ascending=False)\n", "class_fn": true, "question_id": "pandas/pandas.core.groupby.indexing/GroupByIndexingMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/ops.py", "fn_id": "", "content": "class WrappedCythonOp:\n    \"\"\"\n    Dispatch logic for functions defined in _libs.groupby\n\n    Parameters\n    ----------\n    kind: str\n        Whether the operation is an aggregate or transform.\n    how: str\n        Operation name, e.g. \"mean\".\n    has_dropped_na: bool\n        True precisely when dropna=True and the grouper contains a null value.\n    \"\"\"\n\n    # Functions for which we do _not_ attempt to cast the cython result\n    #  back to the original dtype.\n    cast_blocklist = frozenset(\n        [\"any\", \"all\", \"rank\", \"count\", \"size\", \"idxmin\", \"idxmax\"]\n    )\n\n    def __init__(self, kind: str, how: str, has_dropped_na: bool) -> None:\n        self.kind = kind\n        self.how = how\n        self.has_dropped_na = has_dropped_na\n\n    _CYTHON_FUNCTIONS: dict[str, dict] = {\n        \"aggregate\": {\n            \"any\": functools.partial(libgroupby.group_any_all, val_test=\"any\"),\n            \"all\": functools.partial(libgroupby.group_any_all, val_test=\"all\"),\n            \"sum\": \"group_sum\",\n            \"prod\": \"group_prod\",\n            \"idxmin\": functools.partial(libgroupby.group_idxmin_idxmax, name=\"idxmin\"),\n            \"idxmax\": functools.partial(libgroupby.group_idxmin_idxmax, name=\"idxmax\"),\n            \"min\": \"group_min\",\n            \"max\": \"group_max\",\n            \"mean\": \"group_mean\",\n            \"median\": \"group_median_float64\",\n            \"var\": \"group_var\",\n            \"std\": functools.partial(libgroupby.group_var, name=\"std\"),\n            \"sem\": functools.partial(libgroupby.group_var, name=\"sem\"),\n            \"skew\": \"group_skew\",\n            \"first\": \"group_nth\",\n            \"last\": \"group_last\",\n            \"ohlc\": \"group_ohlc\",\n        },\n        \"transform\": {\n            \"cumprod\": \"group_cumprod\",\n            \"cumsum\": \"group_cumsum\",\n            \"cummin\": \"group_cummin\",\n            \"cummax\": \"group_cummax\",\n            \"rank\": \"group_rank\",\n        },\n    }\n\n    _cython_arity = {\"ohlc\": 4}  # OHLC\n\n    @classmethod\n    def get_kind_from_how(cls, how: str) -> str:\n        if how in cls._CYTHON_FUNCTIONS[\"aggregate\"]:\n            return \"aggregate\"\n        return \"transform\"\n\n    # Note: we make this a classmethod and pass kind+how so that caching\n    #  works at the class level and not the instance level\n    @classmethod\n    @functools.cache\n    def _get_cython_function(\n        cls, kind: str, how: str, dtype: np.dtype, is_numeric: bool\n    ):\n        dtype_str = dtype.name\n        ftype = cls._CYTHON_FUNCTIONS[kind][how]\n\n        # see if there is a fused-type version of function\n        # only valid for numeric\n        if callable(ftype):\n            f = ftype\n        else:\n            f = getattr(libgroupby, ftype)\n        if is_numeric:\n            return f\n        elif dtype == np.dtype(object):\n            if how in [\"median\", \"cumprod\"]:\n                # no fused types -> no __signatures__\n                raise NotImplementedError(\n                    f\"function is not implemented for this dtype: \"\n                    f\"[how->{how},dtype->{dtype_str}]\"\n                )\n            elif how in [\"std\", \"sem\", \"idxmin\", \"idxmax\"]:\n                # We have a partial object that does not have __signatures__\n                return f\n            elif how == \"skew\":\n                # _get_cython_vals will convert to float64\n                pass\n            elif \"object\" not in f.__signatures__:\n                # raise NotImplementedError here rather than TypeError later\n                raise NotImplementedError(\n                    f\"function is not implemented for this dtype: \"\n                    f\"[how->{how},dtype->{dtype_str}]\"\n                )\n            return f\n        else:\n            raise NotImplementedError(\n                \"This should not be reached. Please report a bug at \"\n                \"github.com/pandas-dev/pandas/\",\n                dtype,\n            )\n\n    def _get_cython_vals(self, values: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Cast numeric dtypes to float64 for functions that only support that.\n\n        Parameters\n        ----------\n        values : np.ndarray\n\n        Returns\n        -------\n        values : np.ndarray\n        \"\"\"\n        how = self.how\n\n        if how in [\"median\", \"std\", \"sem\", \"skew\"]:\n            # median only has a float64 implementation\n            # We should only get here with is_numeric, as non-numeric cases\n            #  should raise in _get_cython_function\n            values = ensure_float64(values)\n\n        elif values.dtype.kind in \"iu\":\n            if how in [\"var\", \"mean\"] or (\n                self.kind == \"transform\" and self.has_dropped_na\n            ):\n                # has_dropped_na check need for test_null_group_str_transformer\n                # result may still include NaN, so we have to cast\n                values = ensure_float64(values)\n\n            elif how in [\"sum\", \"ohlc\", \"prod\", \"cumsum\", \"cumprod\"]:\n                # Avoid overflow during group op\n                if values.dtype.kind == \"i\":\n                    values = ensure_int64(values)\n                else:\n                    values = ensure_uint64(values)\n\n        return values\n\n    def _get_output_shape(self, ngroups: int, values: np.ndarray) -> Shape:\n        how = self.how\n        kind = self.kind\n\n        arity = self._cython_arity.get(how, 1)\n\n        out_shape: Shape\n        if how == \"ohlc\":\n            out_shape = (ngroups, arity)\n        elif arity > 1:\n            raise NotImplementedError(\n                \"arity of more than 1 is not supported for the 'how' argument\"\n            )\n        elif kind == \"transform\":\n            out_shape = values.shape\n        else:\n            out_shape = (ngroups,) + values.shape[1:]\n        return out_shape\n\n    def _get_out_dtype(self, dtype: np.dtype) -> np.dtype:\n        how = self.how\n\n        if how == \"rank\":\n            out_dtype = \"float64\"\n        elif how in [\"idxmin\", \"idxmax\"]:\n            # The Cython implementation only produces the row number; we'll take\n            # from the index using this in post processing\n            out_dtype = \"intp\"\n        else:\n            if dtype.kind in \"iufcb\":\n                out_dtype = f\"{dtype.kind}{dtype.itemsize}\"\n            else:\n                out_dtype = \"object\"\n        return np.dtype(out_dtype)\n\n    def _get_result_dtype(self, dtype: np.dtype) -> np.dtype:\n        \"\"\"\n        Get the desired dtype of a result based on the\n        input dtype and how it was computed.\n\n        Parameters\n        ----------\n        dtype : np.dtype\n\n        Returns\n        -------\n        np.dtype\n            The desired dtype of the result.\n        \"\"\"\n        how = self.how\n\n        if how in [\"sum\", \"cumsum\", \"sum\", \"prod\", \"cumprod\"]:\n            if dtype == np.dtype(bool):\n                return np.dtype(np.int64)\n        elif how in [\"mean\", \"median\", \"var\", \"std\", \"sem\"]:\n            if dtype.kind in \"fc\":\n                return dtype\n            elif dtype.kind in \"iub\":\n                return np.dtype(np.float64)\n        return dtype\n\n    @final\n    def _cython_op_ndim_compat(\n        self,\n        values: np.ndarray,\n        *,\n        min_count: int,\n        ngroups: int,\n        comp_ids: np.ndarray,\n        mask: npt.NDArray[np.bool_] | None = None,\n        result_mask: npt.NDArray[np.bool_] | None = None,\n        **kwargs,\n    ) -> np.ndarray:\n        if values.ndim == 1:\n            # expand to 2d, dispatch, then squeeze if appropriate\n            values2d = values[None, :]\n            if mask is not None:\n                mask = mask[None, :]\n            if result_mask is not None:\n                result_mask = result_mask[None, :]\n            res = self._call_cython_op(\n                values2d,\n                min_count=min_count,\n                ngroups=ngroups,\n                comp_ids=comp_ids,\n                mask=mask,\n                result_mask=result_mask,\n                **kwargs,\n            )\n            if res.shape[0] == 1:\n                return res[0]\n\n            # otherwise we have OHLC\n            return res.T\n\n        return self._call_cython_op(\n            values,\n            min_count=min_count,\n            ngroups=ngroups,\n            comp_ids=comp_ids,\n            mask=mask,\n            result_mask=result_mask,\n            **kwargs,\n        )\n\n    @final\n    def _call_cython_op(\n        self,\n        values: np.ndarray,  # np.ndarray[ndim=2]\n        *,\n        min_count: int,\n        ngroups: int,\n        comp_ids: np.ndarray,\n        mask: npt.NDArray[np.bool_] | None,\n        result_mask: npt.NDArray[np.bool_] | None,\n        **kwargs,\n    ) -> np.ndarray:  # np.ndarray[ndim=2]\n        orig_values = values\n\n        dtype = values.dtype\n        is_numeric = dtype.kind in \"iufcb\"\n\n        is_datetimelike = dtype.kind in \"mM\"\n\n        if is_datetimelike:\n            values = values.view(\"int64\")\n            is_numeric = True\n        elif dtype.kind == \"b\":\n            values = values.view(\"uint8\")\n        if values.dtype == \"float16\":\n            values = values.astype(np.float32)\n\n        if self.how in [\"any\", \"all\"]:\n            if mask is None:\n                mask = isna(values)\n            if dtype == object:\n                if kwargs[\"skipna\"]:\n                    # GH#37501: don't raise on pd.NA when skipna=True\n                    if mask.any():\n                        # mask on original values computed separately\n                        values = values.copy()\n                        values[mask] = True\n            values = values.astype(bool, copy=False).view(np.int8)\n            is_numeric = True\n\n        values = values.T\n        if mask is not None:\n            mask = mask.T\n            if result_mask is not None:\n                result_mask = result_mask.T\n\n        out_shape = self._get_output_shape(ngroups, values)\n        func = self._get_cython_function(self.kind, self.how, values.dtype, is_numeric)\n        values = self._get_cython_vals(values)\n        out_dtype = self._get_out_dtype(values.dtype)\n\n        result = maybe_fill(np.empty(out_shape, dtype=out_dtype))\n        if self.kind == \"aggregate\":\n            counts = np.zeros(ngroups, dtype=np.int64)\n            if self.how in [\n                \"idxmin\",\n                \"idxmax\",\n                \"min\",\n                \"max\",\n                \"mean\",\n                \"last\",\n                \"first\",\n                \"sum\",\n            ]:\n                func(\n                    out=result,\n                    counts=counts,\n                    values=values,\n                    labels=comp_ids,\n                    min_count=min_count,\n                    mask=mask,\n                    result_mask=result_mask,\n                    is_datetimelike=is_datetimelike,\n                    **kwargs,\n                )\n            elif self.how in [\"sem\", \"std\", \"var\", \"ohlc\", \"prod\", \"median\"]:\n                if self.how in [\"std\", \"sem\"]:\n                    kwargs[\"is_datetimelike\"] = is_datetimelike\n                func(\n                    result,\n                    counts,\n                    values,\n                    comp_ids,\n                    min_count=min_count,\n                    mask=mask,\n                    result_mask=result_mask,\n                    **kwargs,\n                )\n            elif self.how in [\"any\", \"all\"]:\n                func(\n                    out=result,\n                    values=values,\n                    labels=comp_ids,\n                    mask=mask,\n                    result_mask=result_mask,\n                    **kwargs,\n                )\n                result = result.astype(bool, copy=False)\n            elif self.how in [\"skew\"]:\n                func(\n                    out=result,\n                    counts=counts,\n                    values=values,\n                    labels=comp_ids,\n                    mask=mask,\n                    result_mask=result_mask,\n                    **kwargs,\n                )\n                if dtype == object:\n                    result = result.astype(object)\n\n            else:\n                raise NotImplementedError(f\"{self.how} is not implemented\")\n        else:\n            # TODO: min_count\n            if self.how != \"rank\":\n                # TODO: should rank take result_mask?\n                kwargs[\"result_mask\"] = result_mask\n            func(\n                out=result,\n                values=values,\n                labels=comp_ids,\n                ngroups=ngroups,\n                is_datetimelike=is_datetimelike,\n                mask=mask,\n                **kwargs,\n            )\n\n        if self.kind == \"aggregate\" and self.how not in [\"idxmin\", \"idxmax\"]:\n            # i.e. counts is defined.  Locations where count<min_count\n            # need to have the result set to np.nan, which may require casting,\n            # see GH#40767. For idxmin/idxmax is handled specially via post-processing\n            if result.dtype.kind in \"iu\" and not is_datetimelike:\n                # if the op keeps the int dtypes, we have to use 0\n                cutoff = max(0 if self.how in [\"sum\", \"prod\"] else 1, min_count)\n                empty_groups = counts < cutoff\n                if empty_groups.any():\n                    if result_mask is not None:\n                        assert result_mask[empty_groups].all()\n                    else:\n                        # Note: this conversion could be lossy, see GH#40767\n                        result = result.astype(\"float64\")\n                        result[empty_groups] = np.nan\n\n        result = result.T\n\n        if self.how not in self.cast_blocklist:\n            # e.g. if we are int64 and need to restore to datetime64/timedelta64\n            # \"rank\" is the only member of cast_blocklist we get here\n            # Casting only needed for float16, bool, datetimelike,\n            #  and self.how in [\"sum\", \"prod\", \"ohlc\", \"cumprod\"]\n            res_dtype = self._get_result_dtype(orig_values.dtype)\n            op_result = maybe_downcast_to_dtype(result, res_dtype)\n        else:\n            op_result = result\n\n        return op_result\n\n    @final\n    def _validate_axis(self, axis: AxisInt, values: ArrayLike) -> None:\n        if values.ndim > 2:\n            raise NotImplementedError(\"number of dimensions is currently limited to 2\")\n        if values.ndim == 2:\n            assert axis == 1, axis\n        elif not is_1d_only_ea_dtype(values.dtype):\n            # Note: it is *not* the case that axis is always 0 for 1-dim values,\n            #  as we can have 1D ExtensionArrays that we need to treat as 2D\n            assert axis == 0\n\n    @final\n    def cython_operation(\n        self,\n        *,\n        values: ArrayLike,\n        axis: AxisInt,\n        min_count: int = -1,\n        comp_ids: np.ndarray,\n        ngroups: int,\n        **kwargs,\n    ) -> ArrayLike:\n        \"\"\"\n        Call our cython function, with appropriate pre- and post- processing.\n        \"\"\"\n        self._validate_axis(axis, values)\n\n        if not isinstance(values, np.ndarray):\n            # i.e. ExtensionArray\n            return values._groupby_op(\n                how=self.how,\n                has_dropped_na=self.has_dropped_na,\n                min_count=min_count,\n                ngroups=ngroups,\n                ids=comp_ids,\n                **kwargs,\n            )\n\n        return self._cython_op_ndim_compat(\n            values,\n            min_count=min_count,\n            ngroups=ngroups,\n            comp_ids=comp_ids,\n            mask=None,\n            **kwargs,\n        )\n", "class_fn": true, "question_id": "pandas/pandas.core.groupby.ops/WrappedCythonOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/accessors.py", "fn_id": "", "content": "@delegate_names(\n    delegate=ArrowExtensionArray,\n    accessors=TimedeltaArray._datetimelike_ops,\n    typ=\"property\",\n    accessor_mapping=lambda x: f\"_dt_{x}\",\n    raise_on_missing=False,\n)\n@delegate_names(\n    delegate=ArrowExtensionArray,\n    accessors=TimedeltaArray._datetimelike_methods,\n    typ=\"method\",\n    accessor_mapping=lambda x: f\"_dt_{x}\",\n    raise_on_missing=False,\n)\n@delegate_names(\n    delegate=ArrowExtensionArray,\n    accessors=DatetimeArray._datetimelike_ops,\n    typ=\"property\",\n    accessor_mapping=lambda x: f\"_dt_{x}\",\n    raise_on_missing=False,\n)\n@delegate_names(\n    delegate=ArrowExtensionArray,\n    accessors=DatetimeArray._datetimelike_methods,\n    typ=\"method\",\n    accessor_mapping=lambda x: f\"_dt_{x}\",\n    raise_on_missing=False,\n)\nclass ArrowTemporalProperties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n    def __init__(self, data: Series, orig) -> None:\n        if not isinstance(data, ABCSeries):\n            raise TypeError(\n                f\"cannot convert an object of type {type(data)} to a datetimelike index\"\n            )\n\n        self._parent = data\n        self._orig = orig\n        self._freeze()\n\n    def _delegate_property_get(self, name: str):\n        if not hasattr(self._parent.array, f\"_dt_{name}\"):\n            raise NotImplementedError(\n                f\"dt.{name} is not supported for {self._parent.dtype}\"\n            )\n        result = getattr(self._parent.array, f\"_dt_{name}\")\n\n        if not is_list_like(result):\n            return result\n\n        if self._orig is not None:\n            index = self._orig.index\n        else:\n            index = self._parent.index\n        # return the result as a Series, which is by definition a copy\n        result = type(self._parent)(\n            result, index=index, name=self._parent.name\n        ).__finalize__(self._parent)\n\n        return result\n\n    def _delegate_method(self, name: str, *args, **kwargs):\n        if not hasattr(self._parent.array, f\"_dt_{name}\"):\n            raise NotImplementedError(\n                f\"dt.{name} is not supported for {self._parent.dtype}\"\n            )\n\n        result = getattr(self._parent.array, f\"_dt_{name}\")(*args, **kwargs)\n\n        if self._orig is not None:\n            index = self._orig.index\n        else:\n            index = self._parent.index\n        # return the result as a Series, which is by definition a copy\n        result = type(self._parent)(\n            result, index=index, name=self._parent.name\n        ).__finalize__(self._parent)\n\n        return result\n\n    def to_pytimedelta(self):\n        return cast(ArrowExtensionArray, self._parent.array)._dt_to_pytimedelta()\n\n    def to_pydatetime(self):\n        # GH#20306\n        warnings.warn(\n            f\"The behavior of {type(self).__name__}.to_pydatetime is deprecated, \"\n            \"in a future version this will return a Series containing python \"\n            \"datetime objects instead of an ndarray. To retain the old behavior, \"\n            \"call `np.array` on the result\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return cast(ArrowExtensionArray, self._parent.array)._dt_to_pydatetime()\n\n    def isocalendar(self) -> DataFrame:\n        from pandas import DataFrame\n\n        result = (\n            cast(ArrowExtensionArray, self._parent.array)\n            ._dt_isocalendar()\n            ._pa_array.combine_chunks()\n        )\n        iso_calendar_df = DataFrame(\n            {\n                col: type(self._parent.array)(result.field(i))  # type: ignore[call-arg]\n                for i, col in enumerate([\"year\", \"week\", \"day\"])\n            }\n        )\n        return iso_calendar_df\n\n    @property\n    def components(self) -> DataFrame:\n        from pandas import DataFrame\n\n        components_df = DataFrame(\n            {\n                col: getattr(self._parent.array, f\"_dt_{col}\")\n                for col in [\n                    \"days\",\n                    \"hours\",\n                    \"minutes\",\n                    \"seconds\",\n                    \"milliseconds\",\n                    \"microseconds\",\n                    \"nanoseconds\",\n                ]\n            }\n        )\n        return components_df\n", "class_fn": true, "question_id": "pandas/pandas.core.indexes.accessors/ArrowTemporalProperties", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/accessors.py", "fn_id": "", "content": "@delegate_names(\n    delegate=TimedeltaArray, accessors=TimedeltaArray._datetimelike_ops, typ=\"property\"\n)\n@delegate_names(\n    delegate=TimedeltaArray,\n    accessors=TimedeltaArray._datetimelike_methods,\n    typ=\"method\",\n)\nclass TimedeltaProperties(Properties):\n    \"\"\"\n    Accessor object for datetimelike properties of the Series values.\n\n    Returns a Series indexed like the original Series.\n    Raises TypeError if the Series does not contain datetimelike values.\n\n    Examples\n    --------\n    >>> seconds_series = pd.Series(\n    ...     pd.timedelta_range(start=\"1 second\", periods=3, freq=\"s\")\n    ... )\n    >>> seconds_series\n    0   0 days 00:00:01\n    1   0 days 00:00:02\n    2   0 days 00:00:03\n    dtype: timedelta64[ns]\n    >>> seconds_series.dt.seconds\n    0    1\n    1    2\n    2    3\n    dtype: int32\n    \"\"\"\n\n    def to_pytimedelta(self) -> np.ndarray:\n        \"\"\"\n        Return an array of native :class:`datetime.timedelta` objects.\n\n        Python's standard `datetime` library uses a different representation\n        timedelta's. This method converts a Series of pandas Timedeltas\n        to `datetime.timedelta` format with the same length as the original\n        Series.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of 1D containing data with `datetime.timedelta` type.\n\n        See Also\n        --------\n        datetime.timedelta : A duration expressing the difference\n            between two date, time, or datetime.\n\n        Examples\n        --------\n        >>> s = pd.Series(pd.to_timedelta(np.arange(5), unit=\"d\"))\n        >>> s\n        0   0 days\n        1   1 days\n        2   2 days\n        3   3 days\n        4   4 days\n        dtype: timedelta64[ns]\n\n        >>> s.dt.to_pytimedelta()\n        array([datetime.timedelta(0), datetime.timedelta(days=1),\n        datetime.timedelta(days=2), datetime.timedelta(days=3),\n        datetime.timedelta(days=4)], dtype=object)\n        \"\"\"\n        return self._get_values().to_pytimedelta()\n\n    @property\n    def components(self):\n        \"\"\"\n        Return a Dataframe of the components of the Timedeltas.\n\n        Returns\n        -------\n        DataFrame\n\n        Examples\n        --------\n        >>> s = pd.Series(pd.to_timedelta(np.arange(5), unit='s'))\n        >>> s\n        0   0 days 00:00:00\n        1   0 days 00:00:01\n        2   0 days 00:00:02\n        3   0 days 00:00:03\n        4   0 days 00:00:04\n        dtype: timedelta64[ns]\n        >>> s.dt.components\n           days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\n        0     0      0        0        0             0             0            0\n        1     0      0        0        1             0             0            0\n        2     0      0        0        2             0             0            0\n        3     0      0        0        3             0             0            0\n        4     0      0        0        4             0             0            0\n        \"\"\"\n        return (\n            self._get_values()\n            .components.set_index(self._parent.index)\n            .__finalize__(self._parent)\n        )\n\n    @property\n    def freq(self):\n        return self._get_values().inferred_freq\n", "class_fn": true, "question_id": "pandas/pandas.core.indexes.accessors/TimedeltaProperties", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/datetimelike.py", "fn_id": "", "content": "class DatetimeIndexOpsMixin(NDArrayBackedExtensionIndex, ABC):\n    \"\"\"\n    Common ops mixin to support a unified interface datetimelike Index.\n    \"\"\"\n\n    _can_hold_strings = False\n    _data: DatetimeArray | TimedeltaArray | PeriodArray\n\n    @doc(DatetimeLikeArrayMixin.mean)\n    def mean(self, *, skipna: bool = True, axis: int | None = 0):\n        return self._data.mean(skipna=skipna, axis=axis)\n\n    @property\n    def freq(self) -> BaseOffset | None:\n        return self._data.freq\n\n    @freq.setter\n    def freq(self, value) -> None:\n        # error: Property \"freq\" defined in \"PeriodArray\" is read-only  [misc]\n        self._data.freq = value  # type: ignore[misc]\n\n    @property\n    def asi8(self) -> npt.NDArray[np.int64]:\n        return self._data.asi8\n\n    @property\n    @doc(DatetimeLikeArrayMixin.freqstr)\n    def freqstr(self) -> str:\n        from pandas import PeriodIndex\n\n        if self._data.freqstr is not None and isinstance(\n            self._data, (PeriodArray, PeriodIndex)\n        ):\n            freq = freq_to_period_freqstr(self._data.freq.n, self._data.freq.name)\n            return freq\n        else:\n            return self._data.freqstr  # type: ignore[return-value]\n\n    @cache_readonly\n    @abstractmethod\n    def _resolution_obj(self) -> Resolution:\n        ...\n\n    @cache_readonly\n    @doc(DatetimeLikeArrayMixin.resolution)\n    def resolution(self) -> str:\n        return self._data.resolution\n\n    # ------------------------------------------------------------------------\n\n    @cache_readonly\n    def hasnans(self) -> bool:\n        return self._data._hasna\n\n    def equals(self, other: Any) -> bool:\n        \"\"\"\n        Determines if two Index objects contain the same elements.\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n        elif other.dtype.kind in \"iufc\":\n            return False\n        elif not isinstance(other, type(self)):\n            should_try = False\n            inferable = self._data._infer_matches\n            if other.dtype == object:\n                should_try = other.inferred_type in inferable\n            elif isinstance(other.dtype, CategoricalDtype):\n                other = cast(\"CategoricalIndex\", other)\n                should_try = other.categories.inferred_type in inferable\n\n            if should_try:\n                try:\n                    other = type(self)(other)\n                except (ValueError, TypeError, OverflowError):\n                    # e.g.\n                    #  ValueError -> cannot parse str entry, or OutOfBoundsDatetime\n                    #  TypeError  -> trying to convert IntervalIndex to DatetimeIndex\n                    #  OverflowError -> Index([very_large_timedeltas])\n                    return False\n\n        if self.dtype != other.dtype:\n            # have different timezone\n            return False\n\n        return np.array_equal(self.asi8, other.asi8)\n\n    @Appender(Index.__contains__.__doc__)\n    def __contains__(self, key: Any) -> bool:\n        hash(key)\n        try:\n            self.get_loc(key)\n        except (KeyError, TypeError, ValueError, InvalidIndexError):\n            return False\n        return True\n\n    def _convert_tolerance(self, tolerance, target):\n        tolerance = np.asarray(to_timedelta(tolerance).to_numpy())\n        return super()._convert_tolerance(tolerance, target)\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n    _default_na_rep = \"NaT\"\n\n    def format(\n        self,\n        name: bool = False,\n        formatter: Callable | None = None,\n        na_rep: str = \"NaT\",\n        date_format: str | None = None,\n    ) -> list[str]:\n        \"\"\"\n        Render a string representation of the Index.\n        \"\"\"\n        warnings.warn(\n            # GH#55413\n            f\"{type(self).__name__}.format is deprecated and will be removed \"\n            \"in a future version. Convert using index.astype(str) or \"\n            \"index.map(formatter) instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        header = []\n        if name:\n            header.append(\n                ibase.pprint_thing(self.name, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n                if self.name is not None\n                else \"\"\n            )\n\n        if formatter is not None:\n            return header + list(self.map(formatter))\n\n        return self._format_with_header(\n            header=header, na_rep=na_rep, date_format=date_format\n        )\n\n    def _format_with_header(\n        self, *, header: list[str], na_rep: str, date_format: str | None = None\n    ) -> list[str]:\n        # TODO: not reached in tests 2023-10-11\n        # matches base class except for whitespace padding and date_format\n        return header + list(\n            self._get_values_for_csv(na_rep=na_rep, date_format=date_format)\n        )\n\n    @property\n    def _formatter_func(self):\n        return self._data._formatter()\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value).\n        \"\"\"\n        attrs = super()._format_attrs()\n        for attrib in self._attributes:\n            # iterating over _attributes prevents us from doing this for PeriodIndex\n            if attrib == \"freq\":\n                freq = self.freqstr\n                if freq is not None:\n                    freq = repr(freq)  # e.g. D -> 'D'\n                attrs.append((\"freq\", freq))\n        return attrs\n\n    @Appender(Index._summary.__doc__)\n    def _summary(self, name=None) -> str:\n        result = super()._summary(name=name)\n        if self.freq:\n            result += f\"\\nFreq: {self.freqstr}\"\n\n        return result\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    @final\n    def _can_partial_date_slice(self, reso: Resolution) -> bool:\n        # e.g. test_getitem_setitem_periodindex\n        # History of conversation GH#3452, GH#3931, GH#2369, GH#14826\n        return reso > self._resolution_obj\n        # NB: for DTI/PI, not TDI\n\n    def _parsed_string_to_bounds(self, reso: Resolution, parsed):\n        raise NotImplementedError\n\n    def _parse_with_reso(self, label: str):\n        # overridden by TimedeltaIndex\n        try:\n            if self.freq is None or hasattr(self.freq, \"rule_code\"):\n                freq = self.freq\n        except NotImplementedError:\n            freq = getattr(self, \"freqstr\", getattr(self, \"inferred_freq\", None))\n\n        freqstr: str | None\n        if freq is not None and not isinstance(freq, str):\n            freqstr = freq.rule_code\n        else:\n            freqstr = freq\n\n        if isinstance(label, np.str_):\n            # GH#45580\n            label = str(label)\n\n        parsed, reso_str = parsing.parse_datetime_string_with_reso(label, freqstr)\n        reso = Resolution.from_attrname(reso_str)\n        return parsed, reso\n\n    def _get_string_slice(self, key: str):\n        # overridden by TimedeltaIndex\n        parsed, reso = self._parse_with_reso(key)\n        try:\n            return self._partial_date_slice(reso, parsed)\n        except KeyError as err:\n            raise KeyError(key) from err\n\n    @final\n    def _partial_date_slice(\n        self,\n        reso: Resolution,\n        parsed: datetime,\n    ) -> slice | npt.NDArray[np.intp]:\n        \"\"\"\n        Parameters\n        ----------\n        reso : Resolution\n        parsed : datetime\n\n        Returns\n        -------\n        slice or ndarray[intp]\n        \"\"\"\n        if not self._can_partial_date_slice(reso):\n            raise ValueError\n\n        t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n        vals = self._data._ndarray\n        unbox = self._data._unbox\n\n        if self.is_monotonic_increasing:\n            if len(self) and (\n                (t1 < self[0] and t2 < self[0]) or (t1 > self[-1] and t2 > self[-1])\n            ):\n                # we are out of range\n                raise KeyError\n\n            # TODO: does this depend on being monotonic _increasing_?\n\n            # a monotonic (sorted) series can be sliced\n            left = vals.searchsorted(unbox(t1), side=\"left\")\n            right = vals.searchsorted(unbox(t2), side=\"right\")\n            return slice(left, right)\n\n        else:\n            lhs_mask = vals >= unbox(t1)\n            rhs_mask = vals <= unbox(t2)\n\n            # try to find the dates\n            return (lhs_mask & rhs_mask).nonzero()[0]\n\n    def _maybe_cast_slice_bound(self, label, side: str):\n        \"\"\"\n        If label is a string, cast it to scalar type according to resolution.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n\n        Returns\n        -------\n        label : object\n\n        Notes\n        -----\n        Value of `side` parameter should be validated in caller.\n        \"\"\"\n        if isinstance(label, str):\n            try:\n                parsed, reso = self._parse_with_reso(label)\n            except ValueError as err:\n                # DTI -> parsing.DateParseError\n                # TDI -> 'unit abbreviation w/o a number'\n                # PI -> string cannot be parsed as datetime-like\n                self._raise_invalid_indexer(\"slice\", label, err)\n\n            lower, upper = self._parsed_string_to_bounds(reso, parsed)\n            return lower if side == \"left\" else upper\n        elif not isinstance(label, self._data._recognized_scalars):\n            self._raise_invalid_indexer(\"slice\", label)\n\n        return label\n\n    # --------------------------------------------------------------------\n    # Arithmetic Methods\n\n    def shift(self, periods: int = 1, freq=None) -> Self:\n        \"\"\"\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.DatetimeIndex\n            Shifted index.\n\n        See Also\n        --------\n        Index.shift : Shift values of Index.\n        PeriodIndex.shift : Shift values of PeriodIndex.\n        \"\"\"\n        raise NotImplementedError\n\n    # --------------------------------------------------------------------\n\n    @doc(Index._maybe_cast_listlike_indexer)\n    def _maybe_cast_listlike_indexer(self, keyarr):\n        try:\n            res = self._data._validate_listlike(keyarr, allow_object=True)\n        except (ValueError, TypeError):\n            if not isinstance(keyarr, ExtensionArray):\n                # e.g. we don't want to cast DTA to ndarray[object]\n                res = com.asarray_tuplesafe(keyarr)\n                # TODO: com.asarray_tuplesafe shouldn't cast e.g. DatetimeArray\n            else:\n                res = keyarr\n        return Index(res, dtype=res.dtype)\n", "class_fn": true, "question_id": "pandas/pandas.core.indexes.datetimelike/DatetimeIndexOpsMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/frozen.py", "fn_id": "", "content": "class FrozenList(PandasObject, list):\n    \"\"\"\n    Container that doesn't allow setting item *but*\n    because it's technically hashable, will be used\n    for lookups, appropriately, etc.\n    \"\"\"\n\n    # Side note: This has to be of type list. Otherwise,\n    #            it messes up PyTables type checks.\n\n    def union(self, other) -> FrozenList:\n        \"\"\"\n        Returns a FrozenList with other concatenated to the end of self.\n\n        Parameters\n        ----------\n        other : array-like\n            The array-like whose elements we are concatenating.\n\n        Returns\n        -------\n        FrozenList\n            The collection difference between self and other.\n        \"\"\"\n        if isinstance(other, tuple):\n            other = list(other)\n        return type(self)(super().__add__(other))\n\n    def difference(self, other) -> FrozenList:\n        \"\"\"\n        Returns a FrozenList with elements from other removed from self.\n\n        Parameters\n        ----------\n        other : array-like\n            The array-like whose elements we are removing self.\n\n        Returns\n        -------\n        FrozenList\n            The collection difference between self and other.\n        \"\"\"\n        other = set(other)\n        temp = [x for x in self if x not in other]\n        return type(self)(temp)\n\n    # TODO: Consider deprecating these in favor of `union` (xref gh-15506)\n    # error: Incompatible types in assignment (expression has type\n    # \"Callable[[FrozenList, Any], FrozenList]\", base class \"list\" defined the\n    # type as overloaded function)\n    __add__ = __iadd__ = union  # type: ignore[assignment]\n\n    def __getitem__(self, n):\n        if isinstance(n, slice):\n            return type(self)(super().__getitem__(n))\n        return super().__getitem__(n)\n\n    def __radd__(self, other) -> Self:\n        if isinstance(other, tuple):\n            other = list(other)\n        return type(self)(other + list(self))\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, (tuple, FrozenList)):\n            other = list(other)\n        return super().__eq__(other)\n\n    __req__ = __eq__\n\n    def __mul__(self, other) -> Self:\n        return type(self)(super().__mul__(other))\n\n    __imul__ = __mul__\n\n    def __reduce__(self):\n        return type(self), (list(self),)\n\n    # error: Signature of \"__hash__\" incompatible with supertype \"list\"\n    def __hash__(self) -> int:  # type: ignore[override]\n        return hash(tuple(self))\n\n    def _disabled(self, *args, **kwargs) -> NoReturn:\n        \"\"\"\n        This method will not function because object is immutable.\n        \"\"\"\n        raise TypeError(f\"'{type(self).__name__}' does not support mutable operations.\")\n\n    def __str__(self) -> str:\n        return pprint_thing(self, quote_strings=True, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n\n    def __repr__(self) -> str:\n        return f\"{type(self).__name__}({str(self)})\"\n\n    __setitem__ = __setslice__ = _disabled  # type: ignore[assignment]\n    __delitem__ = __delslice__ = _disabled\n    pop = append = extend = _disabled\n    remove = sort = insert = _disabled  # type: ignore[assignment]\n", "class_fn": true, "question_id": "pandas/pandas.core.indexes.frozen/FrozenList", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/period.py", "fn_id": "", "content": "@inherit_names(\n    [\"strftime\", \"start_time\", \"end_time\"] + PeriodArray._field_ops,\n    PeriodArray,\n    wrap=True,\n)\n@inherit_names([\"is_leap_year\"], PeriodArray)\nclass PeriodIndex(DatetimeIndexOpsMixin):\n    \"\"\"\n    Immutable ndarray holding ordinal values indicating regular periods in time.\n\n    Index keys are boxed to Period objects which carries the metadata (eg,\n    frequency information).\n\n    Parameters\n    ----------\n    data : array-like (1d int np.ndarray or PeriodArray), optional\n        Optional period-like data to construct index with.\n    copy : bool\n        Make a copy of input ndarray.\n    freq : str or period object, optional\n        One of pandas period strings or corresponding objects.\n    year : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    month : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    quarter : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    day : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    hour : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    minute : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    second : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    dtype : str or PeriodDtype, default None\n\n    Attributes\n    ----------\n    day\n    dayofweek\n    day_of_week\n    dayofyear\n    day_of_year\n    days_in_month\n    daysinmonth\n    end_time\n    freq\n    freqstr\n    hour\n    is_leap_year\n    minute\n    month\n    quarter\n    qyear\n    second\n    start_time\n    week\n    weekday\n    weekofyear\n    year\n\n    Methods\n    -------\n    asfreq\n    strftime\n    to_timestamp\n    from_fields\n    from_ordinals\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n    Period : Represents a period of time.\n    DatetimeIndex : Index with datetime64 data.\n    TimedeltaIndex : Index of timedelta64 data.\n    period_range : Create a fixed-frequency PeriodIndex.\n\n    Examples\n    --------\n    >>> idx = pd.PeriodIndex.from_fields(year=[2000, 2002], quarter=[1, 3])\n    >>> idx\n    PeriodIndex(['2000Q1', '2002Q3'], dtype='period[Q-DEC]')\n    \"\"\"\n\n    _typ = \"periodindex\"\n\n    _data: PeriodArray\n    freq: BaseOffset\n    dtype: PeriodDtype\n\n    _data_cls = PeriodArray\n    _supports_partial_string_indexing = True\n\n    @property\n    def _engine_type(self) -> type[libindex.PeriodEngine]:\n        return libindex.PeriodEngine\n\n    @cache_readonly\n    def _resolution_obj(self) -> Resolution:\n        # for compat with DatetimeIndex\n        return self.dtype._resolution_obj\n\n    # --------------------------------------------------------------------\n    # methods that dispatch to array and wrap result in Index\n    # These are defined here instead of via inherit_names for mypy\n\n    @doc(\n        PeriodArray.asfreq,\n        other=\"pandas.arrays.PeriodArray\",\n        other_name=\"PeriodArray\",\n        **_shared_doc_kwargs,\n    )\n    def asfreq(self, freq=None, how: str = \"E\") -> Self:\n        arr = self._data.asfreq(freq, how)\n        return type(self)._simple_new(arr, name=self.name)\n\n    @doc(PeriodArray.to_timestamp)\n    def to_timestamp(self, freq=None, how: str = \"start\") -> DatetimeIndex:\n        arr = self._data.to_timestamp(freq, how)\n        return DatetimeIndex._simple_new(arr, name=self.name)\n\n    @property\n    @doc(PeriodArray.hour.fget)\n    def hour(self) -> Index:\n        return Index(self._data.hour, name=self.name)\n\n    @property\n    @doc(PeriodArray.minute.fget)\n    def minute(self) -> Index:\n        return Index(self._data.minute, name=self.name)\n\n    @property\n    @doc(PeriodArray.second.fget)\n    def second(self) -> Index:\n        return Index(self._data.second, name=self.name)\n\n    # ------------------------------------------------------------------------\n    # Index Constructors\n\n    def __new__(\n        cls,\n        data=None,\n        ordinal=None,\n        freq=None,\n        dtype: Dtype | None = None,\n        copy: bool = False,\n        name: Hashable | None = None,\n        **fields,\n    ) -> Self:\n        valid_field_set = {\n            \"year\",\n            \"month\",\n            \"day\",\n            \"quarter\",\n            \"hour\",\n            \"minute\",\n            \"second\",\n        }\n\n        refs = None\n        if not copy and isinstance(data, (Index, ABCSeries)):\n            refs = data._references\n\n        if not set(fields).issubset(valid_field_set):\n            argument = next(iter(set(fields) - valid_field_set))\n            raise TypeError(f\"__new__() got an unexpected keyword argument {argument}\")\n        elif len(fields):\n            # GH#55960\n            warnings.warn(\n                \"Constructing PeriodIndex from fields is deprecated. Use \"\n                \"PeriodIndex.from_fields instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        if ordinal is not None:\n            # GH#55960\n            warnings.warn(\n                \"The 'ordinal' keyword in PeriodIndex is deprecated and will \"\n                \"be removed in a future version. Use PeriodIndex.from_ordinals \"\n                \"instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        name = maybe_extract_name(name, data, cls)\n\n        if data is None and ordinal is None:\n            # range-based.\n            if not fields:\n                # test_pickle_compat_construction\n                cls._raise_scalar_data_error(None)\n            data = cls.from_fields(**fields, freq=freq)._data\n            copy = False\n\n        elif fields:\n            if data is not None:\n                raise ValueError(\"Cannot pass both data and fields\")\n            raise ValueError(\"Cannot pass both ordinal and fields\")\n\n        else:\n            freq = validate_dtype_freq(dtype, freq)\n\n            # PeriodIndex allow PeriodIndex(period_index, freq=different)\n            # Let's not encourage that kind of behavior in PeriodArray.\n\n            if freq and isinstance(data, cls) and data.freq != freq:\n                # TODO: We can do some of these with no-copy / coercion?\n                # e.g. D -> 2D seems to be OK\n                data = data.asfreq(freq)\n\n            if data is None and ordinal is not None:\n                ordinal = np.asarray(ordinal, dtype=np.int64)\n                dtype = PeriodDtype(freq)\n                data = PeriodArray(ordinal, dtype=dtype)\n            elif data is not None and ordinal is not None:\n                raise ValueError(\"Cannot pass both data and ordinal\")\n            else:\n                # don't pass copy here, since we copy later.\n                data = period_array(data=data, freq=freq)\n\n        if copy:\n            data = data.copy()\n\n        return cls._simple_new(data, name=name, refs=refs)\n\n    @classmethod\n    def from_fields(\n        cls,\n        *,\n        year=None,\n        quarter=None,\n        month=None,\n        day=None,\n        hour=None,\n        minute=None,\n        second=None,\n        freq=None,\n    ) -> Self:\n        fields = {\n            \"year\": year,\n            \"quarter\": quarter,\n            \"month\": month,\n            \"day\": day,\n            \"hour\": hour,\n            \"minute\": minute,\n            \"second\": second,\n        }\n        fields = {key: value for key, value in fields.items() if value is not None}\n        arr = PeriodArray._from_fields(fields=fields, freq=freq)\n        return cls._simple_new(arr)\n\n    @classmethod\n    def from_ordinals(cls, ordinals, *, freq, name=None) -> Self:\n        ordinals = np.asarray(ordinals, dtype=np.int64)\n        dtype = PeriodDtype(freq)\n        data = PeriodArray._simple_new(ordinals, dtype=dtype)\n        return cls._simple_new(data, name=name)\n\n    # ------------------------------------------------------------------------\n    # Data\n\n    @property\n    def values(self) -> npt.NDArray[np.object_]:\n        return np.asarray(self, dtype=object)\n\n    def _maybe_convert_timedelta(self, other) -> int | npt.NDArray[np.int64]:\n        \"\"\"\n        Convert timedelta-like input to an integer multiple of self.freq\n\n        Parameters\n        ----------\n        other : timedelta, np.timedelta64, DateOffset, int, np.ndarray\n\n        Returns\n        -------\n        converted : int, np.ndarray[int64]\n\n        Raises\n        ------\n        IncompatibleFrequency : if the input cannot be written as a multiple\n            of self.freq.  Note IncompatibleFrequency subclasses ValueError.\n        \"\"\"\n        if isinstance(other, (timedelta, np.timedelta64, Tick, np.ndarray)):\n            if isinstance(self.freq, Tick):\n                # _check_timedeltalike_freq_compat will raise if incompatible\n                delta = self._data._check_timedeltalike_freq_compat(other)\n                return delta\n        elif isinstance(other, BaseOffset):\n            if other.base == self.freq.base:\n                return other.n\n\n            raise raise_on_incompatible(self, other)\n        elif is_integer(other):\n            assert isinstance(other, int)\n            return other\n\n        # raise when input doesn't have freq\n        raise raise_on_incompatible(self, None)\n\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        \"\"\"\n        Can we compare values of the given dtype to our own?\n        \"\"\"\n        return self.dtype == dtype\n\n    # ------------------------------------------------------------------------\n    # Index Methods\n\n    def asof_locs(self, where: Index, mask: npt.NDArray[np.bool_]) -> np.ndarray:\n        \"\"\"\n        where : array of timestamps\n        mask : np.ndarray[bool]\n            Array of booleans where data is not NA.\n        \"\"\"\n        if isinstance(where, DatetimeIndex):\n            where = PeriodIndex(where._values, freq=self.freq)\n        elif not isinstance(where, PeriodIndex):\n            raise TypeError(\"asof_locs `where` must be DatetimeIndex or PeriodIndex\")\n\n        return super().asof_locs(where, mask)\n\n    @property\n    def is_full(self) -> bool:\n        \"\"\"\n        Returns True if this PeriodIndex is range-like in that all Periods\n        between start and end are present, in order.\n        \"\"\"\n        if len(self) == 0:\n            return True\n        if not self.is_monotonic_increasing:\n            raise ValueError(\"Index is not monotonic\")\n        values = self.asi8\n        return bool(((values[1:] - values[:-1]) < 2).all())\n\n    @property\n    def inferred_type(self) -> str:\n        # b/c data is represented as ints make sure we can't have ambiguous\n        # indexing\n        return \"period\"\n\n    # ------------------------------------------------------------------------\n    # Indexing Methods\n\n    def _convert_tolerance(self, tolerance, target):\n        # Returned tolerance must be in dtype/units so that\n        #  `|self._get_engine_target() - target._engine_target()| <= tolerance`\n        #  is meaningful.  Since PeriodIndex returns int64 for engine_target,\n        #  we may need to convert timedelta64 tolerance to int64.\n        tolerance = super()._convert_tolerance(tolerance, target)\n\n        if self.dtype == target.dtype:\n            # convert tolerance to i8\n            tolerance = self._maybe_convert_timedelta(tolerance)\n\n        return tolerance\n\n    def get_loc(self, key):\n        \"\"\"\n        Get integer location for requested label.\n\n        Parameters\n        ----------\n        key : Period, NaT, str, or datetime\n            String or datetime key must be parsable as Period.\n\n        Returns\n        -------\n        loc : int or ndarray[int64]\n\n        Raises\n        ------\n        KeyError\n            Key is not present in the index.\n        TypeError\n            If key is listlike or otherwise not hashable.\n        \"\"\"\n        orig_key = key\n\n        self._check_indexing_error(key)\n\n        if is_valid_na_for_dtype(key, self.dtype):\n            key = NaT\n\n        elif isinstance(key, str):\n            try:\n                parsed, reso = self._parse_with_reso(key)\n            except ValueError as err:\n                # A string with invalid format\n                raise KeyError(f\"Cannot interpret '{key}' as period\") from err\n\n            if self._can_partial_date_slice(reso):\n                try:\n                    return self._partial_date_slice(reso, parsed)\n                except KeyError as err:\n                    raise KeyError(key) from err\n\n            if reso == self._resolution_obj:\n                # the reso < self._resolution_obj case goes\n                #  through _get_string_slice\n                key = self._cast_partial_indexing_scalar(parsed)\n            else:\n                raise KeyError(key)\n\n        elif isinstance(key, Period):\n            self._disallow_mismatched_indexing(key)\n\n        elif isinstance(key, datetime):\n            key = self._cast_partial_indexing_scalar(key)\n\n        else:\n            # in particular integer, which Period constructor would cast to string\n            raise KeyError(key)\n\n        try:\n            return Index.get_loc(self, key)\n        except KeyError as err:\n            raise KeyError(orig_key) from err\n\n    def _disallow_mismatched_indexing(self, key: Period) -> None:\n        if key._dtype != self.dtype:\n            raise KeyError(key)\n\n    def _cast_partial_indexing_scalar(self, label: datetime) -> Period:\n        try:\n            period = Period(label, freq=self.freq)\n        except ValueError as err:\n            # we cannot construct the Period\n            raise KeyError(label) from err\n        return period\n\n    @doc(DatetimeIndexOpsMixin._maybe_cast_slice_bound)\n    def _maybe_cast_slice_bound(self, label, side: str):\n        if isinstance(label, datetime):\n            label = self._cast_partial_indexing_scalar(label)\n\n        return super()._maybe_cast_slice_bound(label, side)\n\n    def _parsed_string_to_bounds(self, reso: Resolution, parsed: datetime):\n        freq = OFFSET_TO_PERIOD_FREQSTR.get(reso.attr_abbrev, reso.attr_abbrev)\n        iv = Period(parsed, freq=freq)\n        return (iv.asfreq(self.freq, how=\"start\"), iv.asfreq(self.freq, how=\"end\"))\n\n    @doc(DatetimeIndexOpsMixin.shift)\n    def shift(self, periods: int = 1, freq=None) -> Self:\n        if freq is not None:\n            raise TypeError(\n                f\"`freq` argument is not supported for {type(self).__name__}.shift\"\n            )\n        return self + periods\n", "class_fn": true, "question_id": "pandas/pandas.core.indexes.period/PeriodIndex", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexing.py", "fn_id": "", "content": "class IndexingMixin:\n    \"\"\"\n    Mixin for adding .loc/.iloc/.at/.iat to Dataframes and Series.\n    \"\"\"\n\n    @property\n    def iloc(self) -> _iLocIndexer:\n        \"\"\"\n        Purely integer-location based indexing for selection by position.\n\n        .. deprecated:: 2.2.0\n\n           Returning a tuple from a callable is deprecated.\n\n        ``.iloc[]`` is primarily integer position based (from ``0`` to\n        ``length-1`` of the axis), but may also be used with a boolean\n        array.\n\n        Allowed inputs are:\n\n        - An integer, e.g. ``5``.\n        - A list or array of integers, e.g. ``[4, 3, 0]``.\n        - A slice object with ints, e.g. ``1:7``.\n        - A boolean array.\n        - A ``callable`` function with one argument (the calling Series or\n          DataFrame) and that returns valid output for indexing (one of the above).\n          This is useful in method chains, when you don't have a reference to the\n          calling object, but would like to base your selection on\n          some value.\n        - A tuple of row and column indexes. The tuple elements consist of one of the\n          above inputs, e.g. ``(0, 1)``.\n\n        ``.iloc`` will raise ``IndexError`` if a requested indexer is\n        out-of-bounds, except *slice* indexers which allow out-of-bounds\n        indexing (this conforms with python/numpy *slice* semantics).\n\n        See more at :ref:`Selection by Position <indexing.integer>`.\n\n        See Also\n        --------\n        DataFrame.iat : Fast integer location scalar accessor.\n        DataFrame.loc : Purely label-location based indexer for selection by label.\n        Series.iloc : Purely integer-location based indexing for\n                       selection by position.\n\n        Examples\n        --------\n        >>> mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n        ...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n        ...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000}]\n        >>> df = pd.DataFrame(mydict)\n        >>> df\n              a     b     c     d\n        0     1     2     3     4\n        1   100   200   300   400\n        2  1000  2000  3000  4000\n\n        **Indexing just the rows**\n\n        With a scalar integer.\n\n        >>> type(df.iloc[0])\n        <class 'pandas.core.series.Series'>\n        >>> df.iloc[0]\n        a    1\n        b    2\n        c    3\n        d    4\n        Name: 0, dtype: int64\n\n        With a list of integers.\n\n        >>> df.iloc[[0]]\n           a  b  c  d\n        0  1  2  3  4\n        >>> type(df.iloc[[0]])\n        <class 'pandas.core.frame.DataFrame'>\n\n        >>> df.iloc[[0, 1]]\n             a    b    c    d\n        0    1    2    3    4\n        1  100  200  300  400\n\n        With a `slice` object.\n\n        >>> df.iloc[:3]\n              a     b     c     d\n        0     1     2     3     4\n        1   100   200   300   400\n        2  1000  2000  3000  4000\n\n        With a boolean mask the same length as the index.\n\n        >>> df.iloc[[True, False, True]]\n              a     b     c     d\n        0     1     2     3     4\n        2  1000  2000  3000  4000\n\n        With a callable, useful in method chains. The `x` passed\n        to the ``lambda`` is the DataFrame being sliced. This selects\n        the rows whose index label even.\n\n        >>> df.iloc[lambda x: x.index % 2 == 0]\n              a     b     c     d\n        0     1     2     3     4\n        2  1000  2000  3000  4000\n\n        **Indexing both axes**\n\n        You can mix the indexer types for the index and columns. Use ``:`` to\n        select the entire axis.\n\n        With scalar integers.\n\n        >>> df.iloc[0, 1]\n        2\n\n        With lists of integers.\n\n        >>> df.iloc[[0, 2], [1, 3]]\n              b     d\n        0     2     4\n        2  2000  4000\n\n        With `slice` objects.\n\n        >>> df.iloc[1:3, 0:3]\n              a     b     c\n        1   100   200   300\n        2  1000  2000  3000\n\n        With a boolean array whose length matches the columns.\n\n        >>> df.iloc[:, [True, False, True, False]]\n              a     c\n        0     1     3\n        1   100   300\n        2  1000  3000\n\n        With a callable function that expects the Series or DataFrame.\n\n        >>> df.iloc[:, lambda df: [0, 2]]\n              a     c\n        0     1     3\n        1   100   300\n        2  1000  3000\n        \"\"\"\n        return _iLocIndexer(\"iloc\", self)\n\n    @property\n    def loc(self) -> _LocIndexer:\n        \"\"\"\n        Access a group of rows and columns by label(s) or a boolean array.\n\n        ``.loc[]`` is primarily label based, but may also be used with a\n        boolean array.\n\n        Allowed inputs are:\n\n        - A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n          interpreted as a *label* of the index, and **never** as an\n          integer position along the index).\n        - A list or array of labels, e.g. ``['a', 'b', 'c']``.\n        - A slice object with labels, e.g. ``'a':'f'``.\n\n          .. warning:: Note that contrary to usual python slices, **both** the\n              start and the stop are included\n\n        - A boolean array of the same length as the axis being sliced,\n          e.g. ``[True, False, True]``.\n        - An alignable boolean Series. The index of the key will be aligned before\n          masking.\n        - An alignable Index. The Index of the returned selection will be the input.\n        - A ``callable`` function with one argument (the calling Series or\n          DataFrame) and that returns valid output for indexing (one of the above)\n\n        See more at :ref:`Selection by Label <indexing.label>`.\n\n        Raises\n        ------\n        KeyError\n            If any items are not found.\n        IndexingError\n            If an indexed key is passed and its index is unalignable to the frame index.\n\n        See Also\n        --------\n        DataFrame.at : Access a single value for a row/column label pair.\n        DataFrame.iloc : Access group of rows and columns by integer position(s).\n        DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n                       Series/DataFrame.\n        Series.loc : Access group of values using labels.\n\n        Examples\n        --------\n        **Getting values**\n\n        >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n        ...                   index=['cobra', 'viper', 'sidewinder'],\n        ...                   columns=['max_speed', 'shield'])\n        >>> df\n                    max_speed  shield\n        cobra               1       2\n        viper               4       5\n        sidewinder          7       8\n\n        Single label. Note this returns the row as a Series.\n\n        >>> df.loc['viper']\n        max_speed    4\n        shield       5\n        Name: viper, dtype: int64\n\n        List of labels. Note using ``[[]]`` returns a DataFrame.\n\n        >>> df.loc[['viper', 'sidewinder']]\n                    max_speed  shield\n        viper               4       5\n        sidewinder          7       8\n\n        Single label for row and column\n\n        >>> df.loc['cobra', 'shield']\n        2\n\n        Slice with labels for row and single label for column. As mentioned\n        above, note that both the start and stop of the slice are included.\n\n        >>> df.loc['cobra':'viper', 'max_speed']\n        cobra    1\n        viper    4\n        Name: max_speed, dtype: int64\n\n        Boolean list with the same length as the row axis\n\n        >>> df.loc[[False, False, True]]\n                    max_speed  shield\n        sidewinder          7       8\n\n        Alignable boolean Series:\n\n        >>> df.loc[pd.Series([False, True, False],\n        ...                  index=['viper', 'sidewinder', 'cobra'])]\n                             max_speed  shield\n        sidewinder          7       8\n\n        Index (same behavior as ``df.reindex``)\n\n        >>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n               max_speed  shield\n        foo\n        cobra          1       2\n        viper          4       5\n\n        Conditional that returns a boolean Series\n\n        >>> df.loc[df['shield'] > 6]\n                    max_speed  shield\n        sidewinder          7       8\n\n        Conditional that returns a boolean Series with column labels specified\n\n        >>> df.loc[df['shield'] > 6, ['max_speed']]\n                    max_speed\n        sidewinder          7\n\n        Multiple conditional using ``&`` that returns a boolean Series\n\n        >>> df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)]\n                    max_speed  shield\n        viper          4       5\n\n        Multiple conditional using ``|`` that returns a boolean Series\n\n        >>> df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)]\n                    max_speed  shield\n        cobra               1       2\n        sidewinder          7       8\n\n        Please ensure that each condition is wrapped in parentheses ``()``.\n        See the :ref:`user guide<indexing.boolean>`\n        for more details and explanations of Boolean indexing.\n\n        .. note::\n            If you find yourself using 3 or more conditionals in ``.loc[]``,\n            consider using :ref:`advanced indexing<advanced.advanced_hierarchical>`.\n\n            See below for using ``.loc[]`` on MultiIndex DataFrames.\n\n        Callable that returns a boolean Series\n\n        >>> df.loc[lambda df: df['shield'] == 8]\n                    max_speed  shield\n        sidewinder          7       8\n\n        **Setting values**\n\n        Set value for all items matching the list of labels\n\n        >>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n        >>> df\n                    max_speed  shield\n        cobra               1       2\n        viper               4      50\n        sidewinder          7      50\n\n        Set value for an entire row\n\n        >>> df.loc['cobra'] = 10\n        >>> df\n                    max_speed  shield\n        cobra              10      10\n        viper               4      50\n        sidewinder          7      50\n\n        Set value for an entire column\n\n        >>> df.loc[:, 'max_speed'] = 30\n        >>> df\n                    max_speed  shield\n        cobra              30      10\n        viper              30      50\n        sidewinder         30      50\n\n        Set value for rows matching callable condition\n\n        >>> df.loc[df['shield'] > 35] = 0\n        >>> df\n                    max_speed  shield\n        cobra              30      10\n        viper               0       0\n        sidewinder          0       0\n\n        Add value matching location\n\n        >>> df.loc[\"viper\", \"shield\"] += 5\n        >>> df\n                    max_speed  shield\n        cobra              30      10\n        viper               0       5\n        sidewinder          0       0\n\n        Setting using a ``Series`` or a ``DataFrame`` sets the values matching the\n        index labels, not the index positions.\n\n        >>> shuffled_df = df.loc[[\"viper\", \"cobra\", \"sidewinder\"]]\n        >>> df.loc[:] += shuffled_df\n        >>> df\n                    max_speed  shield\n        cobra              60      20\n        viper               0      10\n        sidewinder          0       0\n\n        **Getting values on a DataFrame with an index that has integer labels**\n\n        Another example using integers for the index\n\n        >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n        ...                   index=[7, 8, 9], columns=['max_speed', 'shield'])\n        >>> df\n           max_speed  shield\n        7          1       2\n        8          4       5\n        9          7       8\n\n        Slice with integer labels for rows. As mentioned above, note that both\n        the start and stop of the slice are included.\n\n        >>> df.loc[7:9]\n           max_speed  shield\n        7          1       2\n        8          4       5\n        9          7       8\n\n        **Getting values with a MultiIndex**\n\n        A number of examples using a DataFrame with a MultiIndex\n\n        >>> tuples = [\n        ...     ('cobra', 'mark i'), ('cobra', 'mark ii'),\n        ...     ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n        ...     ('viper', 'mark ii'), ('viper', 'mark iii')\n        ... ]\n        >>> index = pd.MultiIndex.from_tuples(tuples)\n        >>> values = [[12, 2], [0, 4], [10, 20],\n        ...           [1, 4], [7, 1], [16, 36]]\n        >>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n        >>> df\n                             max_speed  shield\n        cobra      mark i           12       2\n                   mark ii           0       4\n        sidewinder mark i           10      20\n                   mark ii           1       4\n        viper      mark ii           7       1\n                   mark iii         16      36\n\n        Single label. Note this returns a DataFrame with a single index.\n\n        >>> df.loc['cobra']\n                 max_speed  shield\n        mark i          12       2\n        mark ii          0       4\n\n        Single index tuple. Note this returns a Series.\n\n        >>> df.loc[('cobra', 'mark ii')]\n        max_speed    0\n        shield       4\n        Name: (cobra, mark ii), dtype: int64\n\n        Single label for row and column. Similar to passing in a tuple, this\n        returns a Series.\n\n        >>> df.loc['cobra', 'mark i']\n        max_speed    12\n        shield        2\n        Name: (cobra, mark i), dtype: int64\n\n        Single tuple. Note using ``[[]]`` returns a DataFrame.\n\n        >>> df.loc[[('cobra', 'mark ii')]]\n                       max_speed  shield\n        cobra mark ii          0       4\n\n        Single tuple for the index with a single label for the column\n\n        >>> df.loc[('cobra', 'mark i'), 'shield']\n        2\n\n        Slice from index tuple to single label\n\n        >>> df.loc[('cobra', 'mark i'):'viper']\n                             max_speed  shield\n        cobra      mark i           12       2\n                   mark ii           0       4\n        sidewinder mark i           10      20\n                   mark ii           1       4\n        viper      mark ii           7       1\n                   mark iii         16      36\n\n        Slice from index tuple to index tuple\n\n        >>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                            max_speed  shield\n        cobra      mark i          12       2\n                   mark ii          0       4\n        sidewinder mark i          10      20\n                   mark ii          1       4\n        viper      mark ii          7       1\n\n        Please see the :ref:`user guide<advanced.advanced_hierarchical>`\n        for more details and explanations of advanced indexing.\n        \"\"\"\n        return _LocIndexer(\"loc\", self)\n\n    @property\n    def at(self) -> _AtIndexer:\n        \"\"\"\n        Access a single value for a row/column label pair.\n\n        Similar to ``loc``, in that both provide label-based lookups. Use\n        ``at`` if you only need to get or set a single value in a DataFrame\n        or Series.\n\n        Raises\n        ------\n        KeyError\n            If getting a value and 'label' does not exist in a DataFrame or Series.\n\n        ValueError\n            If row/column label pair is not a tuple or if any label\n            from the pair is not a scalar for DataFrame.\n            If label is list-like (*excluding* NamedTuple) for Series.\n\n        See Also\n        --------\n        DataFrame.at : Access a single value for a row/column pair by label.\n        DataFrame.iat : Access a single value for a row/column pair by integer\n            position.\n        DataFrame.loc : Access a group of rows and columns by label(s).\n        DataFrame.iloc : Access a group of rows and columns by integer\n            position(s).\n        Series.at : Access a single value by label.\n        Series.iat : Access a single value by integer position.\n        Series.loc : Access a group of rows by label(s).\n        Series.iloc : Access a group of rows by integer position(s).\n\n        Notes\n        -----\n        See :ref:`Fast scalar value getting and setting <indexing.basics.get_value>`\n        for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n        ...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n        >>> df\n            A   B   C\n        4   0   2   3\n        5   0   4   1\n        6  10  20  30\n\n        Get value at specified row/column pair\n\n        >>> df.at[4, 'B']\n        2\n\n        Set value at specified row/column pair\n\n        >>> df.at[4, 'B'] = 10\n        >>> df.at[4, 'B']\n        10\n\n        Get value within a Series\n\n        >>> df.loc[5].at['B']\n        4\n        \"\"\"\n        return _AtIndexer(\"at\", self)\n\n    @property\n    def iat(self) -> _iAtIndexer:\n        \"\"\"\n        Access a single value for a row/column pair by integer position.\n\n        Similar to ``iloc``, in that both provide integer-based lookups. Use\n        ``iat`` if you only need to get or set a single value in a DataFrame\n        or Series.\n\n        Raises\n        ------\n        IndexError\n            When integer position is out of bounds.\n\n        See Also\n        --------\n        DataFrame.at : Access a single value for a row/column label pair.\n        DataFrame.loc : Access a group of rows and columns by label(s).\n        DataFrame.iloc : Access a group of rows and columns by integer position(s).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n        ...                   columns=['A', 'B', 'C'])\n        >>> df\n            A   B   C\n        0   0   2   3\n        1   0   4   1\n        2  10  20  30\n\n        Get value at specified row/column pair\n\n        >>> df.iat[1, 2]\n        1\n\n        Set value at specified row/column pair\n\n        >>> df.iat[1, 2] = 10\n        >>> df.iat[1, 2]\n        10\n\n        Get value within a series\n\n        >>> df.loc[0].iat[1]\n        2\n        \"\"\"\n        return _iAtIndexer(\"iat\", self)\n", "class_fn": true, "question_id": "pandas/pandas.core.indexing/IndexingMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/link.py", "fn_id": "", "content": "class HalfLogitLink(BaseLink):\n    \"\"\"Half the logit link function g(x)=1/2 * logit(x).\n\n    Used for the exponential loss.\n    \"\"\"\n\n    interval_y_pred = Interval(0, 1, False, False)\n\n    def link(self, y_pred, out=None):\n        out = logit(y_pred, out=out)\n        out *= 0.5\n        return out\n\n    def inverse(self, raw_prediction, out=None):\n        return expit(2 * raw_prediction, out)\n", "class_fn": true, "question_id": "sklearn/sklearn._loss.link/HalfLogitLink", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/link.py", "fn_id": "", "content": "class LogLink(BaseLink):\n    \"\"\"The log link function g(x)=log(x).\"\"\"\n\n    interval_y_pred = Interval(0, np.inf, False, False)\n\n    def link(self, y_pred, out=None):\n        return np.log(y_pred, out=out)\n\n    def inverse(self, raw_prediction, out=None):\n        return np.exp(raw_prediction, out=out)\n", "class_fn": true, "question_id": "sklearn/sklearn._loss.link/LogLink", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/loss.py", "fn_id": "", "content": "class HalfGammaLoss(BaseLoss):\n    \"\"\"Half Gamma deviance loss with log-link, for regression.\n\n    Domain:\n    y_true and y_pred in positive real numbers\n\n    Link:\n    y_pred = exp(raw_prediction)\n\n    For a given sample x_i, half Gamma deviance loss is defined as::\n\n        loss(x_i) = log(exp(raw_prediction_i)/y_true_i)\n                    + y_true/exp(raw_prediction_i) - 1\n\n    Half the Gamma deviance is actually proportional to the negative log-\n    likelihood up to constant terms (not involving raw_prediction) and\n    simplifies the computation of the gradients.\n    We also skip the constant term `-log(y_true_i) - 1`.\n    \"\"\"\n\n    def __init__(self, sample_weight=None):\n        super().__init__(closs=CyHalfGammaLoss(), link=LogLink())\n        self.interval_y_true = Interval(0, np.inf, False, False)\n\n    def constant_to_optimal_zero(self, y_true, sample_weight=None):\n        term = -np.log(y_true) - 1\n        if sample_weight is not None:\n            term *= sample_weight\n        return term\n", "class_fn": true, "question_id": "sklearn/sklearn._loss.loss/HalfGammaLoss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/loss.py", "fn_id": "", "content": "class HalfTweedieLossIdentity(BaseLoss):\n    \"\"\"Half Tweedie deviance loss with identity link, for regression.\n\n    Domain:\n    y_true in real numbers for power <= 0\n    y_true in non-negative real numbers for 0 < power < 2\n    y_true in positive real numbers for 2 <= power\n    y_pred in positive real numbers for power != 0\n    y_pred in real numbers for power = 0\n    power in real numbers\n\n    Link:\n    y_pred = raw_prediction\n\n    For a given sample x_i, half Tweedie deviance loss with p=power is defined\n    as::\n\n        loss(x_i) = max(y_true_i, 0)**(2-p) / (1-p) / (2-p)\n                    - y_true_i * raw_prediction_i**(1-p) / (1-p)\n                    + raw_prediction_i**(2-p) / (2-p)\n\n    Note that the minimum value of this loss is 0.\n\n    Note furthermore that although no Tweedie distribution exists for\n    0 < power < 1, it still gives a strictly consistent scoring function for\n    the expectation.\n    \"\"\"\n\n    def __init__(self, sample_weight=None, power=1.5):\n        super().__init__(\n            closs=CyHalfTweedieLossIdentity(power=float(power)),\n            link=IdentityLink(),\n        )\n        if self.closs.power <= 0:\n            self.interval_y_true = Interval(-np.inf, np.inf, False, False)\n        elif self.closs.power < 2:\n            self.interval_y_true = Interval(0, np.inf, True, False)\n        else:\n            self.interval_y_true = Interval(0, np.inf, False, False)\n\n        if self.closs.power == 0:\n            self.interval_y_pred = Interval(-np.inf, np.inf, False, False)\n        else:\n            self.interval_y_pred = Interval(0, np.inf, False, False)\n", "class_fn": true, "question_id": "sklearn/sklearn._loss.loss/HalfTweedieLossIdentity", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/base.py", "fn_id": "", "content": "class ClusterMixin:\n    \"\"\"Mixin class for all cluster estimators in scikit-learn.\n\n    - `_estimator_type` class attribute defaulting to `\"clusterer\"`;\n    - `fit_predict` method returning the cluster labels associated to each sample.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, ClusterMixin\n    >>> class MyClusterer(ClusterMixin, BaseEstimator):\n    ...     def fit(self, X, y=None):\n    ...         self.labels_ = np.ones(shape=(len(X),), dtype=np.int64)\n    ...         return self\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> MyClusterer().fit_predict(X)\n    array([1, 1, 1])\n    \"\"\"\n\n    _estimator_type = \"clusterer\"\n\n    def fit_predict(self, X, y=None, **kwargs):\n        \"\"\"\n        Perform clustering on `X` and returns cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **kwargs : dict\n            Arguments to be passed to ``fit``.\n\n            .. versionadded:: 1.4\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,), dtype=np.int64\n            Cluster labels.\n        \"\"\"\n        # non-optimized default implementation; override when a better\n        # method is possible for a given clustering algorithm\n        self.fit(X, **kwargs)\n        return self.labels_\n\n    def _more_tags(self):\n        return {\"preserves_dtype\": []}\n", "class_fn": true, "question_id": "sklearn/sklearn.base/ClusterMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/base.py", "fn_id": "", "content": "class MultiOutputMixin:\n    \"\"\"Mixin to mark estimators that support multioutput.\"\"\"\n\n    def _more_tags(self):\n        return {\"multioutput\": True}\n", "class_fn": true, "question_id": "sklearn/sklearn.base/MultiOutputMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/calibration.py", "fn_id": "", "content": "class _SigmoidCalibration(RegressorMixin, BaseEstimator):\n    \"\"\"Sigmoid regression model.\n\n    Attributes\n    ----------\n    a_ : float\n        The slope.\n\n    b_ : float\n        The intercept.\n    \"\"\"\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        X = column_or_1d(X)\n        y = column_or_1d(y)\n        X, y = indexable(X, y)\n\n        self.a_, self.b_ = _sigmoid_calibration(X, y, sample_weight)\n        return self\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,)\n            Data to predict from.\n\n        Returns\n        -------\n        T_ : ndarray of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        T = column_or_1d(T)\n        return expit(-(self.a_ * T + self.b_))\n", "class_fn": true, "question_id": "sklearn/sklearn.calibration/_SigmoidCalibration", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/datasets/_openml.py", "fn_id": "", "content": "class OpenMLError(ValueError):\n    \"\"\"HTTP 412 is a specific OpenML error code, indicating a generic error\"\"\"\n\n    pass\n", "class_fn": true, "question_id": "sklearn/sklearn.datasets._openml/OpenMLError", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class ArffException(Exception):\n    message: Optional[str] = None\n\n    def __init__(self):\n        self.line = -1\n\n    def __str__(self):\n        return self.message%self.line\n", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/ArffException", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class BadAttributeType(ArffException):\n    '''Error raised when some invalid type is provided into the attribute\n    declaration.'''\n    message = 'Bad @ATTRIBUTE type, at line %d.'\n", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/BadAttributeType", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class BadNominalFormatting(ArffException):\n    '''Error raised when a nominal value with space is not properly quoted.'''\n    def __init__(self, value):\n        super().__init__()\n        self.message = (\n            ('Nominal data value \"%s\" not properly quoted in line ' % value) +\n            '%d.'\n        )\n", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/BadNominalFormatting", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class BadObject(ArffException):\n    '''Error raised when the object representing the ARFF file has something\n    wrong.'''\n    def __init__(self, msg='Invalid object.'):\n        self.msg = msg\n\n    def __str__(self):\n        return '%s' % self.msg\n", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/BadObject", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class EncodedNominalConversor:\n    def __init__(self, values):\n        self.values = {v: i for i, v in enumerate(values)}\n        self.values[0] = 0\n\n    def __call__(self, value):\n        try:\n            return self.values[value]\n        except KeyError:\n            raise BadNominalValue(value)\n", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/EncodedNominalConversor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class _DataListMixin:\n    \"\"\"Mixin to return a list from decode_rows instead of a generator\"\"\"\n    def decode_rows(self, stream, conversors):\n        return list(super().decode_rows(stream, conversors))\n", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/_DataListMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_packaging/version.py", "fn_id": "", "content": "class LegacyVersion(_BaseVersion):\n    def __init__(self, version: str) -> None:\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)\n\n        warnings.warn(\n            \"Creating a LegacyVersion has been deprecated and will be \"\n            \"removed in the next major release\",\n            DeprecationWarning,\n        )\n\n    def __str__(self) -> str:\n        return self._version\n\n    def __repr__(self) -> str:\n        return f\"<LegacyVersion('{self}')>\"\n\n    @property\n    def public(self) -> str:\n        return self._version\n\n    @property\n    def base_version(self) -> str:\n        return self._version\n\n    @property\n    def epoch(self) -> int:\n        return -1\n\n    @property\n    def release(self) -> None:\n        return None\n\n    @property\n    def pre(self) -> None:\n        return None\n\n    @property\n    def post(self) -> None:\n        return None\n\n    @property\n    def dev(self) -> None:\n        return None\n\n    @property\n    def local(self) -> None:\n        return None\n\n    @property\n    def is_prerelease(self) -> bool:\n        return False\n\n    @property\n    def is_postrelease(self) -> bool:\n        return False\n\n    @property\n    def is_devrelease(self) -> bool:\n        return False\n", "class_fn": true, "question_id": "sklearn/sklearn.externals._packaging.version/LegacyVersion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/gaussian_process/tests/_mini_sequence_kernel.py", "fn_id": "", "content": "class MiniSeqKernel(GenericKernelMixin, StationaryKernelMixin, Kernel):\n    \"\"\"\n    A minimal (but valid) convolutional kernel for sequences of variable\n    length.\n    \"\"\"\n\n    def __init__(self, baseline_similarity=0.5, baseline_similarity_bounds=(1e-5, 1)):\n        self.baseline_similarity = baseline_similarity\n        self.baseline_similarity_bounds = baseline_similarity_bounds\n\n    @property\n    def hyperparameter_baseline_similarity(self):\n        return Hyperparameter(\n            \"baseline_similarity\", \"numeric\", self.baseline_similarity_bounds\n        )\n\n    def _f(self, s1, s2):\n        return sum(\n            [1.0 if c1 == c2 else self.baseline_similarity for c1 in s1 for c2 in s2]\n        )\n\n    def _g(self, s1, s2):\n        return sum([0.0 if c1 == c2 else 1.0 for c1 in s1 for c2 in s2])\n\n    def __call__(self, X, Y=None, eval_gradient=False):\n        if Y is None:\n            Y = X\n\n        if eval_gradient:\n            return (\n                np.array([[self._f(x, y) for y in Y] for x in X]),\n                np.array([[[self._g(x, y)] for y in Y] for x in X]),\n            )\n        else:\n            return np.array([[self._f(x, y) for y in Y] for x in X])\n\n    def diag(self, X):\n        return np.array([self._f(x, x) for x in X])\n\n    def clone_with_theta(self, theta):\n        cloned = clone(self)\n        cloned.theta = theta\n        return cloned\n", "class_fn": true, "question_id": "sklearn/sklearn.gaussian_process.tests._mini_sequence_kernel/MiniSeqKernel", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/linear_model/_ridge.py", "fn_id": "", "content": "class _IdentityClassifier(LinearClassifierMixin):\n    \"\"\"Fake classifier which will directly output the prediction.\n\n    We inherit from LinearClassifierMixin to get the proper shape for the\n    output `y`.\n    \"\"\"\n\n    def __init__(self, classes):\n        self.classes_ = classes\n\n    def decision_function(self, y_predict):\n        return y_predict\n", "class_fn": true, "question_id": "sklearn/sklearn.linear_model._ridge/_IdentityClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/linear_model/_ridge.py", "fn_id": "", "content": "class _X_CenterStackOp(sparse.linalg.LinearOperator):\n    \"\"\"Behaves as centered and scaled X with an added intercept column.\n\n    This operator behaves as\n    np.hstack([X - sqrt_sw[:, None] * X_mean, sqrt_sw[:, None]])\n    \"\"\"\n\n    def __init__(self, X, X_mean, sqrt_sw):\n        n_samples, n_features = X.shape\n        super().__init__(X.dtype, (n_samples, n_features + 1))\n        self.X = X\n        self.X_mean = X_mean\n        self.sqrt_sw = sqrt_sw\n\n    def _matvec(self, v):\n        v = v.ravel()\n        return (\n            safe_sparse_dot(self.X, v[:-1], dense_output=True)\n            - self.sqrt_sw * self.X_mean.dot(v[:-1])\n            + v[-1] * self.sqrt_sw\n        )\n\n    def _matmat(self, v):\n        return (\n            safe_sparse_dot(self.X, v[:-1], dense_output=True)\n            - self.sqrt_sw[:, None] * self.X_mean.dot(v[:-1])\n            + v[-1] * self.sqrt_sw[:, None]\n        )\n\n    def _transpose(self):\n        return _XT_CenterStackOp(self.X, self.X_mean, self.sqrt_sw)\n", "class_fn": true, "question_id": "sklearn/sklearn.linear_model._ridge/_X_CenterStackOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/model_selection/_search_successive_halving.py", "fn_id": "", "content": "class _SubsampleMetaSplitter:\n    \"\"\"Splitter that subsamples a given fraction of the dataset\"\"\"\n\n    def __init__(self, *, base_cv, fraction, subsample_test, random_state):\n        self.base_cv = base_cv\n        self.fraction = fraction\n        self.subsample_test = subsample_test\n        self.random_state = random_state\n\n    def split(self, X, y, **kwargs):\n        for train_idx, test_idx in self.base_cv.split(X, y, **kwargs):\n            train_idx = resample(\n                train_idx,\n                replace=False,\n                random_state=self.random_state,\n                n_samples=int(self.fraction * len(train_idx)),\n            )\n            if self.subsample_test:\n                test_idx = resample(\n                    test_idx,\n                    replace=False,\n                    random_state=self.random_state,\n                    n_samples=int(self.fraction * len(test_idx)),\n                )\n            yield train_idx, test_idx\n", "class_fn": true, "question_id": "sklearn/sklearn.model_selection._search_successive_halving/_SubsampleMetaSplitter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/model_selection/_split.py", "fn_id": "", "content": "class _CVIterableWrapper(BaseCrossValidator):\n    \"\"\"Wrapper class for old style cv objects and iterables.\"\"\"\n\n    def __init__(self, cv):\n        self.cv = list(cv)\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator.\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return len(self.cv)\n\n    def split(self, X=None, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        for train, test in self.cv:\n            yield train, test\n", "class_fn": true, "question_id": "sklearn/sklearn.model_selection._split/_CVIterableWrapper", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/multiclass.py", "fn_id": "", "content": "class _ConstantPredictor(BaseEstimator):\n    \"\"\"Helper predictor to be used when only one class is present.\"\"\"\n\n    def fit(self, X, y):\n        check_params = dict(\n            force_all_finite=False, dtype=None, ensure_2d=False, accept_sparse=True\n        )\n        self._validate_data(\n            X, y, reset=True, validate_separately=(check_params, check_params)\n        )\n        self.y_ = y\n        return self\n\n    def predict(self, X):\n        check_is_fitted(self)\n        self._validate_data(\n            X,\n            force_all_finite=False,\n            dtype=None,\n            accept_sparse=True,\n            ensure_2d=False,\n            reset=False,\n        )\n\n        return np.repeat(self.y_, _num_samples(X))\n\n    def decision_function(self, X):\n        check_is_fitted(self)\n        self._validate_data(\n            X,\n            force_all_finite=False,\n            dtype=None,\n            accept_sparse=True,\n            ensure_2d=False,\n            reset=False,\n        )\n\n        return np.repeat(self.y_, _num_samples(X))\n\n    def predict_proba(self, X):\n        check_is_fitted(self)\n        self._validate_data(\n            X,\n            force_all_finite=False,\n            dtype=None,\n            accept_sparse=True,\n            ensure_2d=False,\n            reset=False,\n        )\n        y_ = self.y_.astype(np.float64)\n        return np.repeat([np.hstack([1 - y_, y_])], _num_samples(X), axis=0)\n", "class_fn": true, "question_id": "sklearn/sklearn.multiclass/_ConstantPredictor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/tests/metadata_routing_common.py", "fn_id": "", "content": "class ConsumingRegressor(RegressorMixin, BaseEstimator):\n    \"\"\"A regressor consuming metadata.\n\n    Parameters\n    ----------\n    registry : list, default=None\n        If a list, the estimator will append itself to the list in order to have\n        a reference to the estimator later on. Since that reference is not\n        required in all tests, registration can be skipped by leaving this value\n        as None.\n    \"\"\"\n\n    def __init__(self, registry=None):\n        self.registry = registry\n\n    def partial_fit(self, X, y, sample_weight=\"default\", metadata=\"default\"):\n        if self.registry is not None:\n            self.registry.append(self)\n\n        record_metadata_not_default(\n            self, \"partial_fit\", sample_weight=sample_weight, metadata=metadata\n        )\n        return self\n\n    def fit(self, X, y, sample_weight=\"default\", metadata=\"default\"):\n        if self.registry is not None:\n            self.registry.append(self)\n\n        record_metadata_not_default(\n            self, \"fit\", sample_weight=sample_weight, metadata=metadata\n        )\n        return self\n\n    def predict(self, X, y=None, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, \"predict\", sample_weight=sample_weight, metadata=metadata\n        )\n        return np.zeros(shape=(len(X),))\n\n    def score(self, X, y, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, \"score\", sample_weight=sample_weight, metadata=metadata\n        )\n        return 1\n", "class_fn": true, "question_id": "sklearn/sklearn.tests.metadata_routing_common/ConsumingRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/tests/metadata_routing_common.py", "fn_id": "", "content": "class ConsumingTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"A transformer which accepts metadata on fit and transform.\n\n    Parameters\n    ----------\n    registry : list, default=None\n        If a list, the estimator will append itself to the list in order to have\n        a reference to the estimator later on. Since that reference is not\n        required in all tests, registration can be skipped by leaving this value\n        as None.\n    \"\"\"\n\n    def __init__(self, registry=None):\n        self.registry = registry\n\n    def fit(self, X, y=None, sample_weight=None, metadata=None):\n        if self.registry is not None:\n            self.registry.append(self)\n\n        record_metadata_not_default(\n            self, \"fit\", sample_weight=sample_weight, metadata=metadata\n        )\n        return self\n\n    def transform(self, X, sample_weight=None, metadata=None):\n        record_metadata(\n            self, \"transform\", sample_weight=sample_weight, metadata=metadata\n        )\n        return X\n\n    def fit_transform(self, X, y, sample_weight=None, metadata=None):\n        # implementing ``fit_transform`` is necessary since\n        # ``TransformerMixin.fit_transform`` doesn't route any metadata to\n        # ``transform``, while here we want ``transform`` to receive\n        # ``sample_weight`` and ``metadata``.\n        record_metadata(\n            self, \"fit_transform\", sample_weight=sample_weight, metadata=metadata\n        )\n        return self.fit(X, y, sample_weight=sample_weight, metadata=metadata).transform(\n            X, sample_weight=sample_weight, metadata=metadata\n        )\n\n    def inverse_transform(self, X, sample_weight=None, metadata=None):\n        record_metadata(\n            self, \"inverse_transform\", sample_weight=sample_weight, metadata=metadata\n        )\n        return X\n", "class_fn": true, "question_id": "sklearn/sklearn.tests.metadata_routing_common/ConsumingTransformer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/tests/metadata_routing_common.py", "fn_id": "", "content": "class NonConsumingClassifier(ClassifierMixin, BaseEstimator):\n    \"\"\"A classifier which accepts no metadata on any method.\"\"\"\n\n    def __init__(self, alpha=0.0):\n        self.alpha = alpha\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        return self\n\n    def partial_fit(self, X, y, classes=None):\n        return self\n\n    def decision_function(self, X):\n        return self.predict(X)\n\n    def predict(self, X):\n        y_pred = np.empty(shape=(len(X),))\n        y_pred[: len(X) // 2] = 0\n        y_pred[len(X) // 2 :] = 1\n        return y_pred\n", "class_fn": true, "question_id": "sklearn/sklearn.tests.metadata_routing_common/NonConsumingClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/tests/metadata_routing_common.py", "fn_id": "", "content": "class WeightedMetaRegressor(MetaEstimatorMixin, RegressorMixin, BaseEstimator):\n    \"\"\"A meta-regressor which is also a consumer.\"\"\"\n\n    def __init__(self, estimator, registry=None):\n        self.estimator = estimator\n        self.registry = registry\n\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        if self.registry is not None:\n            self.registry.append(self)\n\n        record_metadata(self, \"fit\", sample_weight=sample_weight)\n        params = process_routing(self, \"fit\", sample_weight=sample_weight, **fit_params)\n        self.estimator_ = clone(self.estimator).fit(X, y, **params.estimator.fit)\n        return self\n\n    def predict(self, X, **predict_params):\n        params = process_routing(self, \"predict\", **predict_params)\n        return self.estimator_.predict(X, **params.estimator.predict)\n\n    def get_metadata_routing(self):\n        router = (\n            MetadataRouter(owner=self.__class__.__name__)\n            .add_self_request(self)\n            .add(\n                estimator=self.estimator,\n                method_mapping=MethodMapping()\n                .add(caller=\"fit\", callee=\"fit\")\n                .add(caller=\"predict\", callee=\"predict\"),\n            )\n        )\n        return router\n", "class_fn": true, "question_id": "sklearn/sklearn.tests.metadata_routing_common/WeightedMetaRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/tree/_reingold_tilford.py", "fn_id": "", "content": "class DrawTree:\n    def __init__(self, tree, parent=None, depth=0, number=1):\n        self.x = -1.0\n        self.y = depth\n        self.tree = tree\n        self.children = [\n            DrawTree(c, self, depth + 1, i + 1) for i, c in enumerate(tree.children)\n        ]\n        self.parent = parent\n        self.thread = None\n        self.mod = 0\n        self.ancestor = self\n        self.change = self.shift = 0\n        self._lmost_sibling = None\n        # this is the number of the node in its group of siblings 1..n\n        self.number = number\n\n    def left(self):\n        return self.thread or len(self.children) and self.children[0]\n\n    def right(self):\n        return self.thread or len(self.children) and self.children[-1]\n\n    def lbrother(self):\n        n = None\n        if self.parent:\n            for node in self.parent.children:\n                if node == self:\n                    return n\n                else:\n                    n = node\n        return n\n\n    def get_lmost_sibling(self):\n        if not self._lmost_sibling and self.parent and self != self.parent.children[0]:\n            self._lmost_sibling = self.parent.children[0]\n        return self._lmost_sibling\n\n    lmost_sibling = property(get_lmost_sibling)\n\n    def __str__(self):\n        return \"%s: x=%s mod=%s\" % (self.tree, self.x, self.mod)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def max_extents(self):\n        extents = [c.max_extents() for c in self.children]\n        extents.append((self.x, self.y))\n        return np.max(extents, axis=0)\n", "class_fn": true, "question_id": "sklearn/sklearn.tree._reingold_tilford/DrawTree", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_available_if.py", "fn_id": "", "content": "class _AvailableIfDescriptor:\n    \"\"\"Implements a conditional property using the descriptor protocol.\n\n    Using this class to create a decorator will raise an ``AttributeError``\n    if check(self) returns a falsey value. Note that if check raises an error\n    this will also result in hasattr returning false.\n\n    See https://docs.python.org/3/howto/descriptor.html for an explanation of\n    descriptors.\n    \"\"\"\n\n    def __init__(self, fn, check, attribute_name):\n        self.fn = fn\n        self.check = check\n        self.attribute_name = attribute_name\n\n        # update the docstring of the descriptor\n        update_wrapper(self, fn)\n\n    def _check(self, obj, owner):\n        attr_err_msg = (\n            f\"This {repr(owner.__name__)} has no attribute {repr(self.attribute_name)}\"\n        )\n        try:\n            check_result = self.check(obj)\n        except Exception as e:\n            raise AttributeError(attr_err_msg) from e\n\n        if not check_result:\n            raise AttributeError(attr_err_msg)\n\n    def __get__(self, obj, owner=None):\n        if obj is not None:\n            # delegate only on instances, not the classes.\n            # this is to allow access to the docstrings.\n            self._check(obj, owner=owner)\n            out = MethodType(self.fn, obj)\n\n        else:\n            # This makes it possible to use the decorated method as an unbound method,\n            # for instance when monkeypatching.\n            @wraps(self.fn)\n            def out(*args, **kwargs):\n                self._check(args[0], owner=owner)\n                return self.fn(*args, **kwargs)\n\n        return out\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._available_if/_AvailableIfDescriptor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_encode.py", "fn_id": "", "content": "class _nandict(dict):\n    \"\"\"Dictionary with support for nans.\"\"\"\n\n    def __init__(self, mapping):\n        super().__init__(mapping)\n        for key, value in mapping.items():\n            if is_scalar_nan(key):\n                self.nan_value = value\n                break\n\n    def __missing__(self, key):\n        if hasattr(self, \"nan_value\") and is_scalar_nan(key):\n            return self.nan_value\n        raise KeyError(key)\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._encode/_nandict", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_metadata_requests.py", "fn_id": "", "content": "class MethodMapping:\n    \"\"\"Stores the mapping between caller and callee methods for a router.\n\n    This class is primarily used in a ``get_metadata_routing()`` of a router\n    object when defining the mapping between the router's methods and a sub-object (a\n    sub-estimator or a scorer).\n\n    Iterating through an instance of this class yields\n    ``MethodPair(caller, callee)`` instances.\n\n    .. versionadded:: 1.3\n    \"\"\"\n\n    def __init__(self):\n        self._routes = []\n\n    def __iter__(self):\n        return iter(self._routes)\n\n    def add(self, *, caller, callee):\n        \"\"\"Add a method mapping.\n\n        Parameters\n        ----------\n\n        caller : str\n            Parent estimator's method name in which the ``callee`` is called.\n\n        callee : str\n            Child object's method name. This method is called in ``caller``.\n\n        Returns\n        -------\n        self : MethodMapping\n            Returns self.\n        \"\"\"\n        if caller not in METHODS:\n            raise ValueError(\n                f\"Given caller:{caller} is not a valid method. Valid methods are:\"\n                f\" {METHODS}\"\n            )\n        if callee not in METHODS:\n            raise ValueError(\n                f\"Given callee:{callee} is not a valid method. Valid methods are:\"\n                f\" {METHODS}\"\n            )\n        self._routes.append(MethodPair(caller=caller, callee=callee))\n        return self\n\n    def _serialize(self):\n        \"\"\"Serialize the object.\n\n        Returns\n        -------\n        obj : list\n            A serialized version of the instance in the form of a list.\n        \"\"\"\n        result = list()\n        for route in self._routes:\n            result.append({\"caller\": route.caller, \"callee\": route.callee})\n        return result\n\n    def __repr__(self):\n        return str(self._serialize())\n\n    def __str__(self):\n        return str(repr(self))\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._metadata_requests/MethodMapping", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_mocking.py", "fn_id": "", "content": "class MockDataFrame:\n    \"\"\"\n    Parameters\n    ----------\n    array\n    \"\"\"\n\n    # have shape and length but don't support indexing.\n\n    def __init__(self, array):\n        self.array = array\n        self.values = array\n        self.shape = array.shape\n        self.ndim = array.ndim\n        # ugly hack to make iloc work.\n        self.iloc = ArraySlicingWrapper(array)\n\n    def __len__(self):\n        return len(self.array)\n\n    def __array__(self, dtype=None):\n        # Pandas data frames also are array-like: we want to make sure that\n        # input validation in cross-validation does not try to call that\n        # method.\n        return self.array\n\n    def __eq__(self, other):\n        return MockDataFrame(self.array == other.array)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def take(self, indices, axis=0):\n        return MockDataFrame(self.array.take(indices, axis=axis))\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._mocking/MockDataFrame", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class HasMethods(_Constraint):\n    \"\"\"Constraint representing objects that expose specific methods.\n\n    It is useful for parameters following a protocol and where we don't want to impose\n    an affiliation to a specific module or class.\n\n    Parameters\n    ----------\n    methods : str or list of str\n        The method(s) that the object is expected to expose.\n    \"\"\"\n\n    @validate_params(\n        {\"methods\": [str, list]},\n        prefer_skip_nested_validation=True,\n    )\n    def __init__(self, methods):\n        super().__init__()\n        if isinstance(methods, str):\n            methods = [methods]\n        self.methods = methods\n\n    def is_satisfied_by(self, val):\n        return all(callable(getattr(val, method, None)) for method in self.methods)\n\n    def __str__(self):\n        if len(self.methods) == 1:\n            methods = f\"{self.methods[0]!r}\"\n        else:\n            methods = (\n                f\"{', '.join([repr(m) for m in self.methods[:-1]])} and\"\n                f\" {self.methods[-1]!r}\"\n            )\n        return f\"an object implementing {methods}\"\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/HasMethods", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class Options(_Constraint):\n    \"\"\"Constraint representing a finite set of instances of a given type.\n\n    Parameters\n    ----------\n    type : type\n\n    options : set\n        The set of valid scalars.\n\n    deprecated : set or None, default=None\n        A subset of the `options` to mark as deprecated in the string\n        representation of the constraint.\n    \"\"\"\n\n    def __init__(self, type, options, *, deprecated=None):\n        super().__init__()\n        self.type = type\n        self.options = options\n        self.deprecated = deprecated or set()\n\n        if self.deprecated - self.options:\n            raise ValueError(\"The deprecated options must be a subset of the options.\")\n\n    def is_satisfied_by(self, val):\n        return isinstance(val, self.type) and val in self.options\n\n    def _mark_if_deprecated(self, option):\n        \"\"\"Add a deprecated mark to an option if needed.\"\"\"\n        option_str = f\"{option!r}\"\n        if option in self.deprecated:\n            option_str = f\"{option_str} (deprecated)\"\n        return option_str\n\n    def __str__(self):\n        options_str = (\n            f\"{', '.join([self._mark_if_deprecated(o) for o in self.options])}\"\n        )\n        return f\"a {_type_name(self.type)} among {{{options_str}}}\"\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/Options", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class _Booleans(_Constraint):\n    \"\"\"Constraint representing boolean likes.\n\n    Convenience class for\n    [bool, np.bool_]\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._constraints = [\n            _InstancesOf(bool),\n            _InstancesOf(np.bool_),\n        ]\n\n    def is_satisfied_by(self, val):\n        return any(c.is_satisfied_by(val) for c in self._constraints)\n\n    def __str__(self):\n        return (\n            f\"{', '.join([str(c) for c in self._constraints[:-1]])} or\"\n            f\" {self._constraints[-1]}\"\n        )\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/_Booleans", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class _Constraint(ABC):\n    \"\"\"Base class for the constraint objects.\"\"\"\n\n    def __init__(self):\n        self.hidden = False\n\n    @abstractmethod\n    def is_satisfied_by(self, val):\n        \"\"\"Whether or not a value satisfies the constraint.\n\n        Parameters\n        ----------\n        val : object\n            The value to check.\n\n        Returns\n        -------\n        is_satisfied : bool\n            Whether or not the constraint is satisfied by this value.\n        \"\"\"\n\n    @abstractmethod\n    def __str__(self):\n        \"\"\"A human readable representational string of the constraint.\"\"\"\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/_Constraint", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class _NanConstraint(_Constraint):\n    \"\"\"Constraint representing the indicator `np.nan`.\"\"\"\n\n    def is_satisfied_by(self, val):\n        return (\n            not isinstance(val, Integral) and isinstance(val, Real) and math.isnan(val)\n        )\n\n    def __str__(self):\n        return \"numpy.nan\"\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/_NanConstraint", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class _RandomStates(_Constraint):\n    \"\"\"Constraint representing random states.\n\n    Convenience class for\n    [Interval(Integral, 0, 2**32 - 1, closed=\"both\"), np.random.RandomState, None]\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._constraints = [\n            Interval(Integral, 0, 2**32 - 1, closed=\"both\"),\n            _InstancesOf(np.random.RandomState),\n            _NoneConstraint(),\n        ]\n\n    def is_satisfied_by(self, val):\n        return any(c.is_satisfied_by(val) for c in self._constraints)\n\n    def __str__(self):\n        return (\n            f\"{', '.join([str(c) for c in self._constraints[:-1]])} or\"\n            f\" {self._constraints[-1]}\"\n        )\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/_RandomStates", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_plotting.py", "fn_id": "", "content": "class _BinaryClassifierCurveDisplayMixin:\n    \"\"\"Mixin class to be used in Displays requiring a binary classifier.\n\n    The aim of this class is to centralize some validations regarding the estimator and\n    the target and gather the response of the estimator.\n    \"\"\"\n\n    def _validate_plot_params(self, *, ax=None, name=None):\n        check_matplotlib_support(f\"{self.__class__.__name__}.plot\")\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            _, ax = plt.subplots()\n\n        name = self.estimator_name if name is None else name\n        return ax, ax.figure, name\n\n    @classmethod\n    def _validate_and_get_response_values(\n        cls, estimator, X, y, *, response_method=\"auto\", pos_label=None, name=None\n    ):\n        check_matplotlib_support(f\"{cls.__name__}.from_estimator\")\n\n        name = estimator.__class__.__name__ if name is None else name\n\n        y_pred, pos_label = _get_response_values_binary(\n            estimator,\n            X,\n            response_method=response_method,\n            pos_label=pos_label,\n        )\n\n        return y_pred, pos_label, name\n\n    @classmethod\n    def _validate_from_predictions_params(\n        cls, y_true, y_pred, *, sample_weight=None, pos_label=None, name=None\n    ):\n        check_matplotlib_support(f\"{cls.__name__}.from_predictions\")\n\n        if type_of_target(y_true) != \"binary\":\n            raise ValueError(\n                f\"The target y is not binary. Got {type_of_target(y_true)} type of\"\n                \" target.\"\n            )\n\n        check_consistent_length(y_true, y_pred, sample_weight)\n        pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n        name = name if name is not None else \"Classifier\"\n\n        return pos_label, name\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._plotting/_BinaryClassifierCurveDisplayMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_set_output.py", "fn_id": "", "content": "class ContainerAdaptersManager:\n    def __init__(self):\n        self.adapters = {}\n\n    @property\n    def supported_outputs(self):\n        return {\"default\"} | set(self.adapters)\n\n    def register(self, adapter):\n        self.adapters[adapter.container_lib] = adapter\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._set_output/ContainerAdaptersManager", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_testing.py", "fn_id": "", "content": "class MinimalClassifier:\n    \"\"\"Minimal classifier implementation without inheriting from BaseEstimator.\n\n    This estimator should be tested with:\n\n    * `check_estimator` in `test_estimator_checks.py`;\n    * within a `Pipeline` in `test_pipeline.py`;\n    * within a `SearchCV` in `test_search.py`.\n    \"\"\"\n\n    _estimator_type = \"classifier\"\n\n    def __init__(self, param=None):\n        self.param = param\n\n    def get_params(self, deep=True):\n        return {\"param\": self.param}\n\n    def set_params(self, **params):\n        for key, value in params.items():\n            setattr(self, key, value)\n        return self\n\n    def fit(self, X, y):\n        X, y = check_X_y(X, y)\n        check_classification_targets(y)\n        self.classes_, counts = np.unique(y, return_counts=True)\n        self._most_frequent_class_idx = counts.argmax()\n        return self\n\n    def predict_proba(self, X):\n        check_is_fitted(self)\n        X = check_array(X)\n        proba_shape = (X.shape[0], self.classes_.size)\n        y_proba = np.zeros(shape=proba_shape, dtype=np.float64)\n        y_proba[:, self._most_frequent_class_idx] = 1.0\n        return y_proba\n\n    def predict(self, X):\n        y_proba = self.predict_proba(X)\n        y_pred = y_proba.argmax(axis=1)\n        return self.classes_[y_pred]\n\n    def score(self, X, y):\n        from sklearn.metrics import accuracy_score\n\n        return accuracy_score(y, self.predict(X))\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._testing/MinimalClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_testing.py", "fn_id": "", "content": "class TempMemmap:\n    \"\"\"\n    Parameters\n    ----------\n    data\n    mmap_mode : str, default='r'\n    \"\"\"\n\n    def __init__(self, data, mmap_mode=\"r\"):\n        self.mmap_mode = mmap_mode\n        self.data = data\n\n    def __enter__(self):\n        data_read_only, self.temp_folder = create_memmap_backed_data(\n            self.data, mmap_mode=self.mmap_mode, return_folder=True\n        )\n        return data_read_only\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        _delete_folder(self.temp_folder)\n", "class_fn": true, "question_id": "sklearn/sklearn.utils._testing/TempMemmap", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/estimator_checks.py", "fn_id": "", "content": "class _NotAnArray:\n    \"\"\"An object that is convertible to an array.\n\n    Parameters\n    ----------\n    data : array-like\n        The data.\n    \"\"\"\n\n    def __init__(self, data):\n        self.data = np.asarray(data)\n\n    def __array__(self, dtype=None, copy=None):\n        return self.data\n\n    def __array_function__(self, func, types, args, kwargs):\n        if func.__name__ == \"may_share_memory\":\n            return True\n        raise TypeError(\"Don't want to call array_function {}!\".format(func.__name__))\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.estimator_checks/_NotAnArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class BadBalancedWeightsClassifier(BaseBadClassifier):\n    def __init__(self, class_weight=None):\n        self.class_weight = class_weight\n\n    def fit(self, X, y):\n        from sklearn.preprocessing import LabelEncoder\n        from sklearn.utils import compute_class_weight\n\n        label_encoder = LabelEncoder().fit(y)\n        classes = label_encoder.classes_\n        class_weight = compute_class_weight(self.class_weight, classes=classes, y=y)\n\n        # Intentionally modify the balanced class_weight\n        # to simulate a bug and raise an exception\n        if self.class_weight == \"balanced\":\n            class_weight += 1.0\n\n        # Simply assigning coef_ to the class_weight\n        self.coef_ = class_weight\n        return self\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/BadBalancedWeightsClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class BrokenArrayAPI(BaseEstimator):\n    \"\"\"Make different predictions when using Numpy and the Array API\"\"\"\n\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        enabled = get_config()[\"array_api_dispatch\"]\n        xp, _ = _array_api.get_namespace(X)\n        if enabled:\n            return xp.asarray([1, 2, 3])\n        else:\n            return np.array([3, 2, 1])\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/BrokenArrayAPI", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class ChangesWrongAttribute(BaseEstimator):\n    def __init__(self, wrong_attribute=0):\n        self.wrong_attribute = wrong_attribute\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 1\n        X, y = self._validate_data(X, y)\n        return self\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/ChangesWrongAttribute", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class EstimatorMissingDefaultTags(BaseEstimator):\n    def _get_tags(self):\n        tags = super()._get_tags().copy()\n        del tags[\"allow_nan\"]\n        return tags\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/EstimatorMissingDefaultTags", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class LargeSparseNotSupportedClassifier(BaseEstimator):\n    \"\"\"Estimator that claims to support large sparse data\n    (accept_large_sparse=True), but doesn't\"\"\"\n\n    def __init__(self, raise_for_type=None):\n        # raise_for_type : str, expects \"sparse_array\" or \"sparse_matrix\"\n        self.raise_for_type = raise_for_type\n\n    def fit(self, X, y):\n        X, y = self._validate_data(\n            X,\n            y,\n            accept_sparse=(\"csr\", \"csc\", \"coo\"),\n            accept_large_sparse=True,\n            multi_output=True,\n            y_numeric=True,\n        )\n        if self.raise_for_type == \"sparse_array\":\n            correct_type = isinstance(X, sp.sparray)\n        elif self.raise_for_type == \"sparse_matrix\":\n            correct_type = isinstance(X, sp.spmatrix)\n        if correct_type:\n            if X.format == \"coo\":\n                if X.row.dtype == \"int64\" or X.col.dtype == \"int64\":\n                    raise ValueError(\"Estimator doesn't support 64-bit indices\")\n            elif X.format in [\"csc\", \"csr\"]:\n                assert \"int64\" not in (\n                    X.indices.dtype,\n                    X.indptr.dtype,\n                ), \"Estimator doesn't support 64-bit indices\"\n\n        return self\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/LargeSparseNotSupportedClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class NoCheckinPredict(BaseBadClassifier):\n    def fit(self, X, y):\n        X, y = self._validate_data(X, y)\n        return self\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/NoCheckinPredict", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class NotInvariantPredict(BaseEstimator):\n    def fit(self, X, y):\n        # Convert data\n        X, y = self._validate_data(\n            X, y, accept_sparse=(\"csr\", \"csc\"), multi_output=True, y_numeric=True\n        )\n        return self\n\n    def predict(self, X):\n        # return 1 if X has more than one element else return 0\n        X = check_array(X)\n        if X.shape[0] > 1:\n            return np.ones(X.shape[0])\n        return np.zeros(X.shape[0])\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/NotInvariantPredict", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class PartialFitChecksName(BaseEstimator):\n    def fit(self, X, y):\n        self._validate_data(X, y)\n        return self\n\n    def partial_fit(self, X, y):\n        reset = not hasattr(self, \"_fitted\")\n        self._validate_data(X, y, reset=reset)\n        self._fitted = True\n        return self\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/PartialFitChecksName", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class RequiresPositiveXRegressor(LinearRegression):\n    def fit(self, X, y):\n        X, y = self._validate_data(X, y, multi_output=True)\n        if (X < 0).any():\n            raise ValueError(\"negative X values not supported!\")\n        return super().fit(X, y)\n\n    def _more_tags(self):\n        return {\"requires_positive_X\": True}\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/RequiresPositiveXRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class SparseTransformer(BaseEstimator):\n    def __init__(self, sparse_container=None):\n        self.sparse_container = sparse_container\n\n    def fit(self, X, y=None):\n        self.X_shape_ = self._validate_data(X).shape\n        return self\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X)\n\n    def transform(self, X):\n        X = check_array(X)\n        if X.shape[1] != self.X_shape_[1]:\n            raise ValueError(\"Bad number of features\")\n        return self.sparse_container(X)\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/SparseTransformer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class _BaseMultiLabelClassifierMock(ClassifierMixin, BaseEstimator):\n    def __init__(self, response_output):\n        self.response_output = response_output\n\n    def fit(self, X, y):\n        return self\n\n    def _more_tags(self):\n        return {\"multilabel\": True}\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/_BaseMultiLabelClassifierMock", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_pprint.py", "fn_id": "", "content": "class LogisticRegression(BaseEstimator):\n    def __init__(\n        self,\n        penalty=\"l2\",\n        dual=False,\n        tol=1e-4,\n        C=1.0,\n        fit_intercept=True,\n        intercept_scaling=1,\n        class_weight=None,\n        random_state=None,\n        solver=\"warn\",\n        max_iter=100,\n        multi_class=\"warn\",\n        verbose=0,\n        warm_start=False,\n        n_jobs=None,\n        l1_ratio=None,\n    ):\n        self.penalty = penalty\n        self.dual = dual\n        self.tol = tol\n        self.C = C\n        self.fit_intercept = fit_intercept\n        self.intercept_scaling = intercept_scaling\n        self.class_weight = class_weight\n        self.random_state = random_state\n        self.solver = solver\n        self.max_iter = max_iter\n        self.multi_class = multi_class\n        self.verbose = verbose\n        self.warm_start = warm_start\n        self.n_jobs = n_jobs\n        self.l1_ratio = l1_ratio\n\n    def fit(self, X, y):\n        return self\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_pprint/LogisticRegression", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_pprint.py", "fn_id": "", "content": "class Pipeline(BaseEstimator):\n    def __init__(self, steps, memory=None):\n        self.steps = steps\n        self.memory = memory\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_pprint/Pipeline", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_pprint.py", "fn_id": "", "content": "class SimpleImputer(BaseEstimator):\n    def __init__(\n        self,\n        missing_values=np.nan,\n        strategy=\"mean\",\n        fill_value=None,\n        verbose=0,\n        copy=True,\n    ):\n        self.missing_values = missing_values\n        self.strategy = strategy\n        self.fill_value = fill_value\n        self.verbose = verbose\n        self.copy = copy\n", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_pprint/SimpleImputer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/link.py", "fn_id": "", "content": "class MultinomialLogit(BaseLink):\n    \"\"\"The symmetric multinomial logit function.\n\n    Convention:\n        - y_pred.shape = raw_prediction.shape = (n_samples, n_classes)\n\n    Notes:\n        - The inverse link h is the softmax function.\n        - The sum is over the second axis, i.e. axis=1 (n_classes).\n\n    We have to choose additional constraints in order to make\n\n        y_pred[k] = exp(raw_pred[k]) / sum(exp(raw_pred[k]), k=0..n_classes-1)\n\n    for n_classes classes identifiable and invertible.\n    We choose the symmetric side constraint where the geometric mean response\n    is set as reference category, see [2]:\n\n    The symmetric multinomial logit link function for a single data point is\n    then defined as\n\n        raw_prediction[k] = g(y_pred[k]) = log(y_pred[k]/gmean(y_pred))\n        = log(y_pred[k]) - mean(log(y_pred)).\n\n    Note that this is equivalent to the definition in [1] and implies mean\n    centered raw predictions:\n\n        sum(raw_prediction[k], k=0..n_classes-1) = 0.\n\n    For linear models with raw_prediction = X @ coef, this corresponds to\n    sum(coef[k], k=0..n_classes-1) = 0, i.e. the sum over classes for every\n    feature is zero.\n\n    Reference\n    ---------\n    .. [1] Friedman, Jerome; Hastie, Trevor; Tibshirani, Robert. \"Additive\n        logistic regression: a statistical view of boosting\" Ann. Statist.\n        28 (2000), no. 2, 337--407. doi:10.1214/aos/1016218223.\n        https://projecteuclid.org/euclid.aos/1016218223\n\n    .. [2] Zahid, Faisal Maqbool and Gerhard Tutz. \"Ridge estimation for\n        multinomial logit models with symmetric side constraints.\"\n        Computational Statistics 28 (2013): 1017-1034.\n        http://epub.ub.uni-muenchen.de/11001/1/tr067.pdf\n    \"\"\"\n\n    is_multiclass = True\n    interval_y_pred = Interval(0, 1, False, False)\n\n    def symmetrize_raw_prediction(self, raw_prediction):\n        return raw_prediction - np.mean(raw_prediction, axis=1)[:, np.newaxis]\n\n    def link(self, y_pred, out=None):\n        # geometric mean as reference category\n        gm = gmean(y_pred, axis=1)\n        return np.log(y_pred / gm[:, np.newaxis], out=out)\n\n    def inverse(self, raw_prediction, out=None):\n        if out is None:\n            return softmax(raw_prediction, copy=True)\n        else:\n            np.copyto(out, raw_prediction)\n            softmax(out, copy=False)\n            return out\n", "class_fn": true, "question_id": "sklearn/sklearn._loss.link/MultinomialLogit", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/loss.py", "fn_id": "", "content": "class HalfBinomialLoss(BaseLoss):\n    \"\"\"Half Binomial deviance loss with logit link, for binary classification.\n\n    This is also know as binary cross entropy, log-loss and logistic loss.\n\n    Domain:\n    y_true in [0, 1], i.e. regression on the unit interval\n    y_pred in (0, 1), i.e. boundaries excluded\n\n    Link:\n    y_pred = expit(raw_prediction)\n\n    For a given sample x_i, half Binomial deviance is defined as the negative\n    log-likelihood of the Binomial/Bernoulli distribution and can be expressed\n    as::\n\n        loss(x_i) = log(1 + exp(raw_pred_i)) - y_true_i * raw_pred_i\n\n    See The Elements of Statistical Learning, by Hastie, Tibshirani, Friedman,\n    section 4.4.1 (about logistic regression).\n\n    Note that the formulation works for classification, y = {0, 1}, as well as\n    logistic regression, y = [0, 1].\n    If you add `constant_to_optimal_zero` to the loss, you get half the\n    Bernoulli/binomial deviance.\n\n    More details: Inserting the predicted probability y_pred = expit(raw_prediction)\n    in the loss gives the well known::\n\n        loss(x_i) = - y_true_i * log(y_pred_i) - (1 - y_true_i) * log(1 - y_pred_i)\n    \"\"\"\n\n    def __init__(self, sample_weight=None):\n        super().__init__(\n            closs=CyHalfBinomialLoss(),\n            link=LogitLink(),\n            n_classes=2,\n        )\n        self.interval_y_true = Interval(0, 1, True, True)\n\n    def constant_to_optimal_zero(self, y_true, sample_weight=None):\n        # This is non-zero only if y_true is neither 0 nor 1.\n        term = xlogy(y_true, y_true) + xlogy(1 - y_true, 1 - y_true)\n        if sample_weight is not None:\n            term *= sample_weight\n        return term\n\n    def predict_proba(self, raw_prediction):\n        \"\"\"Predict probabilities.\n\n        Parameters\n        ----------\n        raw_prediction : array of shape (n_samples,) or (n_samples, 1)\n            Raw prediction values (in link space).\n\n        Returns\n        -------\n        proba : array of shape (n_samples, 2)\n            Element-wise class probabilities.\n        \"\"\"\n        # Be graceful to shape (n_samples, 1) -> (n_samples,)\n        if raw_prediction.ndim == 2 and raw_prediction.shape[1] == 1:\n            raw_prediction = raw_prediction.squeeze(1)\n        proba = np.empty((raw_prediction.shape[0], 2), dtype=raw_prediction.dtype)\n        proba[:, 1] = self.link.inverse(raw_prediction)\n        proba[:, 0] = 1 - proba[:, 1]\n        return proba\n", "class_fn": true, "question_id": "sklearn/sklearn._loss.loss/HalfBinomialLoss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/loss.py", "fn_id": "", "content": "class HuberLoss(BaseLoss):\n    \"\"\"Huber loss, for regression.\n\n    Domain:\n    y_true and y_pred all real numbers\n    quantile in (0, 1)\n\n    Link:\n    y_pred = raw_prediction\n\n    For a given sample x_i, the Huber loss is defined as::\n\n        loss(x_i) = 1/2 * abserr**2            if abserr <= delta\n                    delta * (abserr - delta/2) if abserr > delta\n\n        abserr = |y_true_i - raw_prediction_i|\n        delta = quantile(abserr, self.quantile)\n\n    Note: HuberLoss(quantile=1) equals HalfSquaredError and HuberLoss(quantile=0)\n    equals delta * (AbsoluteError() - delta/2).\n\n    Additional Attributes\n    ---------------------\n    quantile : float\n        The quantile level which defines the breaking point `delta` to distinguish\n        between absolute error and squared error. Must be in range (0, 1).\n\n     Reference\n    ---------\n    .. [1] Friedman, J.H. (2001). :doi:`Greedy function approximation: A gradient\n      boosting machine <10.1214/aos/1013203451>`.\n      Annals of Statistics, 29, 1189-1232.\n    \"\"\"\n\n    differentiable = False\n    need_update_leaves_values = True\n\n    def __init__(self, sample_weight=None, quantile=0.9, delta=0.5):\n        check_scalar(\n            quantile,\n            \"quantile\",\n            target_type=numbers.Real,\n            min_val=0,\n            max_val=1,\n            include_boundaries=\"neither\",\n        )\n        self.quantile = quantile  # This is better stored outside of Cython.\n        super().__init__(\n            closs=CyHuberLoss(delta=float(delta)),\n            link=IdentityLink(),\n        )\n        self.approx_hessian = True\n        self.constant_hessian = False\n\n    def fit_intercept_only(self, y_true, sample_weight=None):\n        \"\"\"Compute raw_prediction of an intercept-only model.\n\n        This is the weighted median of the target, i.e. over the samples\n        axis=0.\n        \"\"\"\n        # See formula before algo 4 in Friedman (2001), but we apply it to y_true,\n        # not to the residual y_true - raw_prediction. An estimator like\n        # HistGradientBoostingRegressor might then call it on the residual, e.g.\n        # fit_intercept_only(y_true - raw_prediction).\n        if sample_weight is None:\n            median = np.percentile(y_true, 50, axis=0)\n        else:\n            median = _weighted_percentile(y_true, sample_weight, 50)\n        diff = y_true - median\n        term = np.sign(diff) * np.minimum(self.closs.delta, np.abs(diff))\n        return median + np.average(term, weights=sample_weight)\n", "class_fn": true, "question_id": "sklearn/sklearn._loss.loss/HuberLoss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/base.py", "fn_id": "", "content": "class ClassifierMixin:\n    \"\"\"Mixin class for all classifiers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - `_estimator_type` class attribute defaulting to `\"classifier\"`;\n    - `score` method that default to :func:`~sklearn.metrics.accuracy_score`.\n    - enforce that `fit` requires `y` to be passed through the `requires_y` tag.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, ClassifierMixin\n    >>> # Mixin classes should always be on the left-hand side for a correct MRO\n    >>> class MyEstimator(ClassifierMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=1)\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([1, 1, 1])\n    >>> estimator.score(X, y)\n    0.66...\n    \"\"\"\n\n    _estimator_type = \"classifier\"\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n        \"\"\"\n        from .metrics import accuracy_score\n\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n\n    def _more_tags(self):\n        return {\"requires_y\": True}\n", "class_fn": true, "question_id": "sklearn/sklearn.base/ClassifierMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/base.py", "fn_id": "", "content": "class TransformerMixin(_SetOutputMixin):\n    \"\"\"Mixin class for all transformers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - a `fit_transform` method that delegates to `fit` and `transform`;\n    - a `set_output` method to output `X` as a specific container type.\n\n    If :term:`get_feature_names_out` is defined, then :class:`BaseEstimator` will\n    automatically wrap `transform` and `fit_transform` to follow the `set_output`\n    API. See the :ref:`developer_api_set_output` for details.\n\n    :class:`OneToOneFeatureMixin` and\n    :class:`ClassNamePrefixFeaturesOutMixin` are helpful mixins for\n    defining :term:`get_feature_names_out`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, TransformerMixin\n    >>> class MyTransformer(TransformerMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         return self\n    ...     def transform(self, X):\n    ...         return np.full(shape=len(X), fill_value=self.param)\n    >>> transformer = MyTransformer()\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> transformer.fit_transform(X)\n    array([1, 1, 1])\n    \"\"\"\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # non-optimized default implementation; override when a better\n        # method is possible for a given clustering algorithm\n\n        # we do not route parameters here, since consumers don't route. But\n        # since it's possible for a `transform` method to also consume\n        # metadata, we check if that's the case, and we raise a warning telling\n        # users that they should implement a custom `fit_transform` method\n        # to forward metadata to `transform` as well.\n        #\n        # For that, we calculate routing and check if anything would be routed\n        # to `transform` if we were to route them.\n        if _routing_enabled():\n            transform_params = self.get_metadata_routing().consumes(\n                method=\"transform\", params=fit_params.keys()\n            )\n            if transform_params:\n                warnings.warn(\n                    (\n                        f\"This object ({self.__class__.__name__}) has a `transform`\"\n                        \" method which consumes metadata, but `fit_transform` does not\"\n                        \" forward metadata to `transform`. Please implement a custom\"\n                        \" `fit_transform` method to forward metadata to `transform` as\"\n                        \" well. Alternatively, you can explicitly do\"\n                        \" `set_transform_request`and set all values to `False` to\"\n                        \" disable metadata routed to `transform`, if that's an option.\"\n                    ),\n                    UserWarning,\n                )\n\n        if y is None:\n            # fit method of arity 1 (unsupervised transformation)\n            return self.fit(X, **fit_params).transform(X)\n        else:\n            # fit method of arity 2 (supervised transformation)\n            return self.fit(X, y, **fit_params).transform(X)\n", "class_fn": true, "question_id": "sklearn/sklearn.base/TransformerMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/calibration.py", "fn_id": "", "content": "class _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        predictions, _ = _get_response_values(\n            self.estimator,\n            X,\n            response_method=[\"decision_function\", \"predict_proba\"],\n        )\n        if predictions.ndim == 1:\n            # Reshape binary output from `(n_samples,)` to `(n_samples, 1)`\n            predictions = predictions.reshape(-1, 1)\n\n        n_classes = len(self.classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n", "class_fn": true, "question_id": "sklearn/sklearn.calibration/_CalibratedClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_agglomerative.py", "fn_id": "", "content": "class FeatureAgglomeration(\n    ClassNamePrefixFeaturesOutMixin, AgglomerativeClustering, AgglomerationTransform\n):\n    \"\"\"Agglomerate features.\n\n    Recursively merges pair of clusters of features.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int or None, default=2\n        The number of clusters to find. It must be ``None`` if\n        ``distance_threshold`` is not ``None``.\n\n    metric : str or callable, default=\"euclidean\"\n        Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n        \"manhattan\", \"cosine\", or \"precomputed\". If linkage is \"ward\", only\n        \"euclidean\" is accepted. If \"precomputed\", a distance matrix is needed\n        as input for the fit method.\n\n        .. versionadded:: 1.2\n\n        .. deprecated:: 1.4\n           `metric=None` is deprecated in 1.4 and will be removed in 1.6.\n           Let `metric` be the default value (i.e. `\"euclidean\"`) instead.\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    connectivity : array-like, sparse matrix, or callable, default=None\n        Connectivity matrix. Defines for each feature the neighboring\n        features following a given structure of the data.\n        This can be a connectivity matrix itself or a callable that transforms\n        the data into a connectivity matrix, such as derived from\n        `kneighbors_graph`. Default is `None`, i.e, the\n        hierarchical clustering algorithm is unstructured.\n\n    compute_full_tree : 'auto' or bool, default='auto'\n        Stop early the construction of the tree at `n_clusters`. This is useful\n        to decrease computation time if the number of clusters is not small\n        compared to the number of features. This option is useful only when\n        specifying a connectivity matrix. Note also that when varying the\n        number of clusters and using caching, it may be advantageous to compute\n        the full tree. It must be ``True`` if ``distance_threshold`` is not\n        ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n        to `True` when `distance_threshold` is not `None` or that `n_clusters`\n        is inferior to the maximum between 100 or `0.02 * n_samples`.\n        Otherwise, \"auto\" is equivalent to `False`.\n\n    linkage : {\"ward\", \"complete\", \"average\", \"single\"}, default=\"ward\"\n        Which linkage criterion to use. The linkage criterion determines which\n        distance to use between sets of features. The algorithm will merge\n        the pairs of cluster that minimize this criterion.\n\n        - \"ward\" minimizes the variance of the clusters being merged.\n        - \"complete\" or maximum linkage uses the maximum distances between\n          all features of the two sets.\n        - \"average\" uses the average of the distances of each feature of\n          the two sets.\n        - \"single\" uses the minimum of the distances between all features\n          of the two sets.\n\n    pooling_func : callable, default=np.mean\n        This combines the values of agglomerated features into a single\n        value, and should accept an array of shape [M, N] and the keyword\n        argument `axis=1`, and reduce it to an array of size [M].\n\n    distance_threshold : float, default=None\n        The linkage distance threshold at or above which clusters will not be\n        merged. If not ``None``, ``n_clusters`` must be ``None`` and\n        ``compute_full_tree`` must be ``True``.\n\n        .. versionadded:: 0.21\n\n    compute_distances : bool, default=False\n        Computes distances between clusters even if `distance_threshold` is not\n        used. This can be used to make dendrogram visualization, but introduces\n        a computational and memory overhead.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    n_clusters_ : int\n        The number of clusters found by the algorithm. If\n        ``distance_threshold=None``, it will be equal to the given\n        ``n_clusters``.\n\n    labels_ : array-like of (n_features,)\n        Cluster labels for each feature.\n\n    n_leaves_ : int\n        Number of leaves in the hierarchical tree.\n\n    n_connected_components_ : int\n        The estimated number of connected components in the graph.\n\n        .. versionadded:: 0.21\n            ``n_connected_components_`` was added to replace ``n_components_``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    children_ : array-like of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_features`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_features` is a non-leaf\n        node and has children `children_[i - n_features]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_features + i`.\n\n    distances_ : array-like of shape (n_nodes-1,)\n        Distances between nodes in the corresponding place in `children_`.\n        Only computed if `distance_threshold` is used or `compute_distances`\n        is set to `True`.\n\n    See Also\n    --------\n    AgglomerativeClustering : Agglomerative clustering samples instead of\n        features.\n    ward_tree : Hierarchical clustering with ward linkage.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import datasets, cluster\n    >>> digits = datasets.load_digits()\n    >>> images = digits.images\n    >>> X = np.reshape(images, (len(images), -1))\n    >>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n    >>> agglo.fit(X)\n    FeatureAgglomeration(n_clusters=32)\n    >>> X_reduced = agglo.transform(X)\n    >>> X_reduced.shape\n    (1797, 32)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_clusters\": [Interval(Integral, 1, None, closed=\"left\"), None],\n        \"metric\": [\n            StrOptions(set(_VALID_METRICS) | {\"precomputed\"}),\n            callable,\n            Hidden(None),\n        ],\n        \"memory\": [str, HasMethods(\"cache\"), None],\n        \"connectivity\": [\"array-like\", \"sparse matrix\", callable, None],\n        \"compute_full_tree\": [StrOptions({\"auto\"}), \"boolean\"],\n        \"linkage\": [StrOptions(set(_TREE_BUILDERS.keys()))],\n        \"pooling_func\": [callable],\n        \"distance_threshold\": [Interval(Real, 0, None, closed=\"left\"), None],\n        \"compute_distances\": [\"boolean\"],\n    }\n\n    def __init__(\n        self,\n        n_clusters=2,\n        *,\n        metric=\"euclidean\",\n        memory=None,\n        connectivity=None,\n        compute_full_tree=\"auto\",\n        linkage=\"ward\",\n        pooling_func=np.mean,\n        distance_threshold=None,\n        compute_distances=False,\n    ):\n        super().__init__(\n            n_clusters=n_clusters,\n            memory=memory,\n            connectivity=connectivity,\n            compute_full_tree=compute_full_tree,\n            linkage=linkage,\n            metric=metric,\n            distance_threshold=distance_threshold,\n            compute_distances=compute_distances,\n        )\n        self.pooling_func = pooling_func\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the hierarchical clustering on the data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the transformer.\n        \"\"\"\n        X = self._validate_data(X, ensure_min_features=2)\n        super()._fit(X.T)\n        self._n_features_out = self.n_clusters_\n        return self\n\n    @property\n    def fit_predict(self):\n        \"\"\"Fit and return the result of each sample's clustering assignment.\"\"\"\n        raise AttributeError\n", "class_fn": true, "question_id": "sklearn/sklearn.cluster._agglomerative/FeatureAgglomeration", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_bicluster.py", "fn_id": "", "content": "class SpectralCoclustering(BaseSpectral):\n    \"\"\"Spectral Co-Clustering algorithm (Dhillon, 2001).\n\n    Clusters rows and columns of an array `X` to solve the relaxed\n    normalized cut of the bipartite graph created from `X` as follows:\n    the edge between row vertex `i` and column vertex `j` has weight\n    `X[i, j]`.\n\n    The resulting bicluster structure is block-diagonal, since each\n    row and each column belongs to exactly one bicluster.\n\n    Supports sparse matrices, as long as they are nonnegative.\n\n    Read more in the :ref:`User Guide <spectral_coclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int, default=3\n        The number of biclusters to find.\n\n    svd_method : {'randomized', 'arpack'}, default='randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', use\n        :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', use\n        :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, default=None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, default=False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random'}, or ndarray of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization of k-means algorithm; defaults to\n        'k-means++'.\n\n    n_init : int, default=10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    random_state : int, RandomState instance, default=None\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like of shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like of shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like of shape (n_rows,)\n        The bicluster label of each row.\n\n    column_labels_ : array-like of shape (n_cols,)\n        The bicluster label of each column.\n\n    biclusters_ : tuple of two ndarrays\n        The tuple contains the `rows_` and `columns_` arrays.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    SpectralBiclustering : Partitions rows and columns under the assumption\n        that the data has an underlying checkerboard structure.\n\n    References\n    ----------\n    * :doi:`Dhillon, Inderjit S, 2001. Co-clustering documents and words using\n      bipartite spectral graph partitioning.\n      <10.1145/502512.502550>`\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralCoclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_ #doctest: +SKIP\n    array([0, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_ #doctest: +SKIP\n    array([0, 0], dtype=int32)\n    >>> clustering\n    SpectralCoclustering(n_clusters=2, random_state=0)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **BaseSpectral._parameter_constraints,\n        \"n_clusters\": [Interval(Integral, 1, None, closed=\"left\")],\n    }\n\n    def __init__(\n        self,\n        n_clusters=3,\n        *,\n        svd_method=\"randomized\",\n        n_svd_vecs=None,\n        mini_batch=False,\n        init=\"k-means++\",\n        n_init=10,\n        random_state=None,\n    ):\n        super().__init__(\n            n_clusters, svd_method, n_svd_vecs, mini_batch, init, n_init, random_state\n        )\n\n    def _check_parameters(self, n_samples):\n        if self.n_clusters > n_samples:\n            raise ValueError(\n                f\"n_clusters should be <= n_samples={n_samples}. Got\"\n                f\" {self.n_clusters} instead.\"\n            )\n\n    def _fit(self, X):\n        normalized_data, row_diag, col_diag = _scale_normalize(X)\n        n_sv = 1 + int(np.ceil(np.log2(self.n_clusters)))\n        u, v = self._svd(normalized_data, n_sv, n_discard=1)\n        z = np.vstack((row_diag[:, np.newaxis] * u, col_diag[:, np.newaxis] * v))\n\n        _, labels = self._k_means(z, self.n_clusters)\n\n        n_rows = X.shape[0]\n        self.row_labels_ = labels[:n_rows]\n        self.column_labels_ = labels[n_rows:]\n\n        self.rows_ = np.vstack([self.row_labels_ == c for c in range(self.n_clusters)])\n        self.columns_ = np.vstack(\n            [self.column_labels_ == c for c in range(self.n_clusters)]\n        )\n", "class_fn": true, "question_id": "sklearn/sklearn.cluster._bicluster/SpectralCoclustering", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_birch.py", "fn_id": "", "content": "class _CFSubcluster:\n    \"\"\"Each subcluster in a CFNode is called a CFSubcluster.\n\n    A CFSubcluster can have a CFNode has its child.\n\n    Parameters\n    ----------\n    linear_sum : ndarray of shape (n_features,), default=None\n        Sample. This is kept optional to allow initialization of empty\n        subclusters.\n\n    Attributes\n    ----------\n    n_samples_ : int\n        Number of samples that belong to each subcluster.\n\n    linear_sum_ : ndarray\n        Linear sum of all the samples in a subcluster. Prevents holding\n        all sample data in memory.\n\n    squared_sum_ : float\n        Sum of the squared l2 norms of all samples belonging to a subcluster.\n\n    centroid_ : ndarray of shape (branching_factor + 1, n_features)\n        Centroid of the subcluster. Prevent recomputing of centroids when\n        ``CFNode.centroids_`` is called.\n\n    child_ : _CFNode\n        Child Node of the subcluster. Once a given _CFNode is set as the child\n        of the _CFNode, it is set to ``self.child_``.\n\n    sq_norm_ : ndarray of shape (branching_factor + 1,)\n        Squared norm of the subcluster. Used to prevent recomputing when\n        pairwise minimum distances are computed.\n    \"\"\"\n\n    def __init__(self, *, linear_sum=None):\n        if linear_sum is None:\n            self.n_samples_ = 0\n            self.squared_sum_ = 0.0\n            self.centroid_ = self.linear_sum_ = 0\n        else:\n            self.n_samples_ = 1\n            self.centroid_ = self.linear_sum_ = linear_sum\n            self.squared_sum_ = self.sq_norm_ = np.dot(\n                self.linear_sum_, self.linear_sum_\n            )\n        self.child_ = None\n\n    def update(self, subcluster):\n        self.n_samples_ += subcluster.n_samples_\n        self.linear_sum_ += subcluster.linear_sum_\n        self.squared_sum_ += subcluster.squared_sum_\n        self.centroid_ = self.linear_sum_ / self.n_samples_\n        self.sq_norm_ = np.dot(self.centroid_, self.centroid_)\n\n    def merge_subcluster(self, nominee_cluster, threshold):\n        \"\"\"Check if a cluster is worthy enough to be merged. If\n        yes then merge.\n        \"\"\"\n        new_ss = self.squared_sum_ + nominee_cluster.squared_sum_\n        new_ls = self.linear_sum_ + nominee_cluster.linear_sum_\n        new_n = self.n_samples_ + nominee_cluster.n_samples_\n        new_centroid = (1 / new_n) * new_ls\n        new_sq_norm = np.dot(new_centroid, new_centroid)\n\n        # The squared radius of the cluster is defined:\n        #   r^2  = sum_i ||x_i - c||^2 / n\n        # with x_i the n points assigned to the cluster and c its centroid:\n        #   c = sum_i x_i / n\n        # This can be expanded to:\n        #   r^2 = sum_i ||x_i||^2 / n - 2 < sum_i x_i / n, c> + n ||c||^2 / n\n        # and therefore simplifies to:\n        #   r^2 = sum_i ||x_i||^2 / n - ||c||^2\n        sq_radius = new_ss / new_n - new_sq_norm\n\n        if sq_radius <= threshold**2:\n            (\n                self.n_samples_,\n                self.linear_sum_,\n                self.squared_sum_,\n                self.centroid_,\n                self.sq_norm_,\n            ) = (new_n, new_ls, new_ss, new_centroid, new_sq_norm)\n            return True\n        return False\n\n    @property\n    def radius(self):\n        \"\"\"Return radius of the subcluster\"\"\"\n        # Because of numerical issues, this could become negative\n        sq_radius = self.squared_sum_ / self.n_samples_ - self.sq_norm_\n        return sqrt(max(0, sq_radius))\n", "class_fn": true, "question_id": "sklearn/sklearn.cluster._birch/_CFSubcluster", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_feature_agglomeration.py", "fn_id": "", "content": "class AgglomerationTransform(TransformerMixin):\n    \"\"\"\n    A class for feature agglomeration via the transform interface.\n    \"\"\"\n\n    # This prevents ``set_split_inverse_transform`` to be generated for the\n    # non-standard ``Xt`` arg on ``inverse_transform``.\n    # TODO(1.7): remove when Xt is removed for inverse_transform.\n    __metadata_request__inverse_transform = {\"Xt\": metadata_routing.UNUSED}\n\n    def transform(self, X):\n        \"\"\"\n        Transform a new matrix using the built clustering.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            A M by N array of M observations in N dimensions or a length\n            M array of M one-dimensional observations.\n\n        Returns\n        -------\n        Y : ndarray of shape (n_samples, n_clusters) or (n_clusters,)\n            The pooled values for each feature cluster.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_data(X, reset=False)\n        if self.pooling_func == np.mean and not issparse(X):\n            size = np.bincount(self.labels_)\n            n_samples = X.shape[0]\n            # a fast way to compute the mean of grouped features\n            nX = np.array(\n                [np.bincount(self.labels_, X[i, :]) / size for i in range(n_samples)]\n            )\n        else:\n            nX = [\n                self.pooling_func(X[:, self.labels_ == l], axis=1)\n                for l in np.unique(self.labels_)\n            ]\n            nX = np.array(nX).T\n        return nX\n\n    def inverse_transform(self, X=None, *, Xt=None):\n        \"\"\"\n        Inverse the transformation and return a vector of size `n_features`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_clusters) or (n_clusters,)\n            The values to be assigned to each cluster of samples.\n\n        Xt : array-like of shape (n_samples, n_clusters) or (n_clusters,)\n            The values to be assigned to each cluster of samples.\n\n            .. deprecated:: 1.5\n                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.\n\n        Returns\n        -------\n        X : ndarray of shape (n_samples, n_features) or (n_features,)\n            A vector of size `n_samples` with the values of `Xred` assigned to\n            each of the cluster of samples.\n        \"\"\"\n        X = _deprecate_Xt_in_inverse_transform(X, Xt)\n\n        check_is_fitted(self)\n\n        unil, inverse = np.unique(self.labels_, return_inverse=True)\n        return X[..., inverse]\n", "class_fn": true, "question_id": "sklearn/sklearn.cluster._feature_agglomeration/AgglomerationTransform", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_kmeans.py", "fn_id": "", "content": "class MiniBatchKMeans(_BaseKMeans):\n    \"\"\"\n    Mini-Batch K-Means clustering.\n\n    Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centroids using sampling based on\n        an empirical probability distribution of the points' contribution to the\n        overall inertia. This technique speeds up convergence. The algorithm\n        implemented is \"greedy k-means++\". It differs from the vanilla k-means++\n        by making several trials at each sampling step and choosing the best centroid\n        among them.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an array is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    max_iter : int, default=100\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n    batch_size : int, default=1024\n        Size of the mini batches.\n        For faster computations, you can set the ``batch_size`` greater than\n        256 * number of cores to enable parallelism on all cores.\n\n        .. versionchanged:: 1.0\n           `batch_size` default changed from 100 to 1024.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    compute_labels : bool, default=True\n        Compute label assignment and inertia for the complete dataset\n        once the minibatch optimization has converged in fit.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization and\n        random reassignment. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Control early stopping based on the relative center changes as\n        measured by a smoothed, variance-normalized of the mean center\n        squared position changes. This early stopping heuristics is\n        closer to the one used for the batch variant of the algorithms\n        but induces a slight computational and memory overhead over the\n        inertia heuristic.\n\n        To disable convergence detection based on normalized center\n        change, set tol to 0.0 (default).\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini\n        batches that does not yield an improvement on the smoothed inertia.\n\n        To disable convergence detection based on inertia, set\n        max_no_improvement to None.\n\n    init_size : int, default=None\n        Number of samples to randomly sample for speeding up the\n        initialization (sometimes at the expense of accuracy): the\n        only algorithm is initialized by running a batch KMeans on a\n        random subset of the data. This needs to be larger than n_clusters.\n\n        If `None`, the heuristic is `init_size = 3 * batch_size` if\n        `3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`.\n\n    n_init : 'auto' or int, default=\"auto\"\n        Number of random initializations that are tried.\n        In contrast to KMeans, the algorithm is only run once, using the best of\n        the `n_init` initializations as measured by inertia. Several runs are\n        recommended for sparse high-dimensional problems (see\n        :ref:`kmeans_sparse_high_dim`).\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        3 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'` in version.\n\n    reassignment_ratio : float, default=0.01\n        Control the fraction of the maximum number of counts for a center to\n        be reassigned. A higher value means that low count centers are more\n        easily reassigned, which means that the model will take longer to\n        converge, but should converge in a better clustering. However, too high\n        a value may cause convergence issues, especially with a small batch\n        size.\n\n    Attributes\n    ----------\n\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point (if compute_labels is set to True).\n\n    inertia_ : float\n        The value of the inertia criterion associated with the chosen\n        partition if compute_labels is set to True. If compute_labels is set to\n        False, it's an approximation of the inertia based on an exponentially\n        weighted average of the batch inertiae.\n        The inertia is defined as the sum of square distances of samples to\n        their cluster center, weighted by the sample weights if provided.\n\n    n_iter_ : int\n        Number of iterations over the full dataset.\n\n    n_steps_ : int\n        Number of minibatches processed.\n\n        .. versionadded:: 1.0\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    KMeans : The classic implementation of the clustering method based on the\n        Lloyd's algorithm. It consumes the whole set of input data at each\n        iteration.\n\n    Notes\n    -----\n    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n    When there are too few points in the dataset, some centers may be\n    duplicated, which means that a proper clustering in terms of the number\n    of requesting clusters and the number of returned clusters will not\n    always match. One solution is to set `reassignment_ratio=0`, which\n    prevents reassignments of clusters that are too small.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MiniBatchKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 0], [4, 4],\n    ...               [4, 5], [0, 1], [2, 2],\n    ...               [3, 2], [5, 5], [1, -1]])\n    >>> # manually fit on batches\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          n_init=\"auto\")\n    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n    >>> kmeans.cluster_centers_\n    array([[3.375, 3.  ],\n           [0.75 , 0.5 ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    >>> # fit on the whole data\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          max_iter=10,\n    ...                          n_init=\"auto\").fit(X)\n    >>> kmeans.cluster_centers_\n    array([[3.55102041, 2.48979592],\n           [1.06896552, 1.        ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BaseKMeans._parameter_constraints,\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"compute_labels\": [\"boolean\"],\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\n        \"init_size\": [Interval(Integral, 1, None, closed=\"left\"), None],\n        \"reassignment_ratio\": [Interval(Real, 0, None, closed=\"left\")],\n    }\n\n    def __init__(\n        self,\n        n_clusters=8,\n        *,\n        init=\"k-means++\",\n        max_iter=100,\n        batch_size=1024,\n        verbose=0,\n        compute_labels=True,\n        random_state=None,\n        tol=0.0,\n        max_no_improvement=10,\n        init_size=None,\n        n_init=\"auto\",\n        reassignment_ratio=0.01,\n    ):\n        super().__init__(\n            n_clusters=n_clusters,\n            init=init,\n            max_iter=max_iter,\n            verbose=verbose,\n            random_state=random_state,\n            tol=tol,\n            n_init=n_init,\n        )\n\n        self.max_no_improvement = max_no_improvement\n        self.batch_size = batch_size\n        self.compute_labels = compute_labels\n        self.init_size = init_size\n        self.reassignment_ratio = reassignment_ratio\n\n    def _check_params_vs_input(self, X):\n        super()._check_params_vs_input(X, default_n_init=3)\n\n        self._batch_size = min(self.batch_size, X.shape[0])\n\n        # init_size\n        self._init_size = self.init_size\n        if self._init_size is None:\n            self._init_size = 3 * self._batch_size\n            if self._init_size < self.n_clusters:\n                self._init_size = 3 * self.n_clusters\n        elif self._init_size < self.n_clusters:\n            warnings.warn(\n                (\n                    f\"init_size={self._init_size} should be larger than \"\n                    f\"n_clusters={self.n_clusters}. Setting it to \"\n                    \"min(3*n_clusters, n_samples)\"\n                ),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            self._init_size = 3 * self.n_clusters\n        self._init_size = min(self._init_size, X.shape[0])\n\n        # reassignment_ratio\n        if self.reassignment_ratio < 0:\n            raise ValueError(\n                \"reassignment_ratio should be >= 0, got \"\n                f\"{self.reassignment_ratio} instead.\"\n            )\n\n    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n        warnings.warn(\n            \"MiniBatchKMeans is known to have a memory leak on \"\n            \"Windows with MKL, when there are less chunks than \"\n            \"available threads. You can prevent it by setting \"\n            f\"batch_size >= {self._n_threads * CHUNK_SIZE} or by \"\n            \"setting the environment variable \"\n            f\"OMP_NUM_THREADS={n_active_threads}\"\n        )\n\n    def _mini_batch_convergence(\n        self, step, n_steps, n_samples, centers_squared_diff, batch_inertia\n    ):\n        \"\"\"Helper function to encapsulate the early stopping logic\"\"\"\n        # Normalize inertia to be able to compare values when\n        # batch_size changes\n        batch_inertia /= self._batch_size\n\n        # count steps starting from 1 for user friendly verbose mode.\n        step = step + 1\n\n        # Ignore first iteration because it's inertia from initialization.\n        if step == 1:\n            if self.verbose:\n                print(\n                    f\"Minibatch step {step}/{n_steps}: mean batch \"\n                    f\"inertia: {batch_inertia}\"\n                )\n            return False\n\n        # Compute an Exponentially Weighted Average of the inertia to\n        # monitor the convergence while discarding minibatch-local stochastic\n        # variability: https://en.wikipedia.org/wiki/Moving_average\n        if self._ewa_inertia is None:\n            self._ewa_inertia = batch_inertia\n        else:\n            alpha = self._batch_size * 2.0 / (n_samples + 1)\n            alpha = min(alpha, 1)\n            self._ewa_inertia = self._ewa_inertia * (1 - alpha) + batch_inertia * alpha\n\n        # Log progress to be able to monitor convergence\n        if self.verbose:\n            print(\n                f\"Minibatch step {step}/{n_steps}: mean batch inertia: \"\n                f\"{batch_inertia}, ewa inertia: {self._ewa_inertia}\"\n            )\n\n        # Early stopping based on absolute tolerance on squared change of\n        # centers position\n        if self._tol > 0.0 and centers_squared_diff <= self._tol:\n            if self.verbose:\n                print(f\"Converged (small centers change) at step {step}/{n_steps}\")\n            return True\n\n        # Early stopping heuristic due to lack of improvement on smoothed\n        # inertia\n        if self._ewa_inertia_min is None or self._ewa_inertia < self._ewa_inertia_min:\n            self._no_improvement = 0\n            self._ewa_inertia_min = self._ewa_inertia\n        else:\n            self._no_improvement += 1\n\n        if (\n            self.max_no_improvement is not None\n            and self._no_improvement >= self.max_no_improvement\n        ):\n            if self.verbose:\n                print(\n                    \"Converged (lack of improvement in inertia) at step \"\n                    f\"{step}/{n_steps}\"\n                )\n            return True\n\n        return False\n\n    def _random_reassign(self):\n        \"\"\"Check if a random reassignment needs to be done.\n\n        Do random reassignments each time 10 * n_clusters samples have been\n        processed.\n\n        If there are empty clusters we always want to reassign.\n        \"\"\"\n        self._n_since_last_reassign += self._batch_size\n        if (self._counts == 0).any() or self._n_since_last_reassign >= (\n            10 * self.n_clusters\n        ):\n            self._n_since_last_reassign = 0\n            return True\n        return False\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n        )\n\n        self._check_params_vs_input(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self._n_threads = _openmp_effective_n_threads()\n        n_samples, n_features = X.shape\n\n        # Validate init array\n        init = self.init\n        if _is_arraylike_not_scalar(init):\n            init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n            self._validate_center_shape(X, init)\n\n        self._check_mkl_vcomp(X, self._batch_size)\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        # Validation set for the init\n        validation_indices = random_state.randint(0, n_samples, self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n\n        # perform several inits with random subsets\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(f\"Init {init_idx + 1}/{self._n_init} with method {init}\")\n\n            # Initialize the centers using only a fraction of the data as we\n            # expect n_samples to be very large when using MiniBatchKMeans.\n            cluster_centers = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                init_size=self._init_size,\n                sample_weight=sample_weight,\n            )\n\n            # Compute inertia on a validation set.\n            _, inertia = _labels_inertia_threadpool_limit(\n                X_valid,\n                sample_weight_valid,\n                cluster_centers,\n                n_threads=self._n_threads,\n            )\n\n            if self.verbose:\n                print(f\"Inertia for init {init_idx + 1}/{self._n_init}: {inertia}\")\n            if best_inertia is None or inertia < best_inertia:\n                init_centers = cluster_centers\n                best_inertia = inertia\n\n        centers = init_centers\n        centers_new = np.empty_like(centers)\n\n        # Initialize counts\n        self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n\n        # Attributes to monitor the convergence\n        self._ewa_inertia = None\n        self._ewa_inertia_min = None\n        self._no_improvement = 0\n\n        # Initialize number of samples seen since last reassignment\n        self._n_since_last_reassign = 0\n\n        n_steps = (self.max_iter * n_samples) // self._batch_size\n\n        with _get_threadpool_controller().limit(limits=1, user_api=\"blas\"):\n            # Perform the iterative optimization until convergence\n            for i in range(n_steps):\n                # Sample a minibatch from the full dataset\n                minibatch_indices = random_state.randint(0, n_samples, self._batch_size)\n\n                # Perform the actual update step on the minibatch data\n                batch_inertia = _mini_batch_step(\n                    X=X[minibatch_indices],\n                    sample_weight=sample_weight[minibatch_indices],\n                    centers=centers,\n                    centers_new=centers_new,\n                    weight_sums=self._counts,\n                    random_state=random_state,\n                    random_reassign=self._random_reassign(),\n                    reassignment_ratio=self.reassignment_ratio,\n                    verbose=self.verbose,\n                    n_threads=self._n_threads,\n                )\n\n                if self._tol > 0.0:\n                    centers_squared_diff = np.sum((centers_new - centers) ** 2)\n                else:\n                    centers_squared_diff = 0\n\n                centers, centers_new = centers_new, centers\n\n                # Monitor convergence and do early stopping if necessary\n                if self._mini_batch_convergence(\n                    i, n_steps, n_samples, centers_squared_diff, batch_inertia\n                ):\n                    break\n\n        self.cluster_centers_ = centers\n        self._n_features_out = self.cluster_centers_.shape[0]\n\n        self.n_steps_ = i + 1\n        self.n_iter_ = int(np.ceil(((i + 1) * self._batch_size) / n_samples))\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n                X,\n                sample_weight,\n                self.cluster_centers_,\n                n_threads=self._n_threads,\n            )\n        else:\n            self.inertia_ = self._ewa_inertia * n_samples\n\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Update k means estimate on a single mini-batch X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n        Returns\n        -------\n        self : object\n            Return updated estimator.\n        \"\"\"\n        has_centers = hasattr(self, \"cluster_centers_\")\n\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n            reset=not has_centers,\n        )\n\n        self._random_state = getattr(\n            self, \"_random_state\", check_random_state(self.random_state)\n        )\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self.n_steps_ = getattr(self, \"n_steps_\", 0)\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if not has_centers:\n            # this instance has not been fitted yet (fit or partial_fit)\n            self._check_params_vs_input(X)\n            self._n_threads = _openmp_effective_n_threads()\n\n            # Validate init array\n            init = self.init\n            if _is_arraylike_not_scalar(init):\n                init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n                self._validate_center_shape(X, init)\n\n            self._check_mkl_vcomp(X, X.shape[0])\n\n            # initialize the cluster centers\n            self.cluster_centers_ = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=self._random_state,\n                init_size=self._init_size,\n                sample_weight=sample_weight,\n            )\n\n            # Initialize counts\n            self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n\n            # Initialize number of samples seen since last reassignment\n            self._n_since_last_reassign = 0\n\n        with _get_threadpool_controller().limit(limits=1, user_api=\"blas\"):\n            _mini_batch_step(\n                X,\n                sample_weight=sample_weight,\n                centers=self.cluster_centers_,\n                centers_new=self.cluster_centers_,\n                weight_sums=self._counts,\n                random_state=self._random_state,\n                random_reassign=self._random_reassign(),\n                reassignment_ratio=self.reassignment_ratio,\n                verbose=self.verbose,\n                n_threads=self._n_threads,\n            )\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n                X,\n                sample_weight,\n                self.cluster_centers_,\n                n_threads=self._n_threads,\n            )\n\n        self.n_steps_ += 1\n        self._n_features_out = self.cluster_centers_.shape[0]\n\n        return self\n", "class_fn": true, "question_id": "sklearn/sklearn.cluster._kmeans/MiniBatchKMeans", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_optics.py", "fn_id": "", "content": "class OPTICS(ClusterMixin, BaseEstimator):\n    \"\"\"Estimate clustering structure from vector array.\n\n    OPTICS (Ordering Points To Identify the Clustering Structure), closely\n    related to DBSCAN, finds core sample of high density and expands clusters\n    from them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\n    neighborhood radius. Better suited for usage on large datasets than the\n    current sklearn implementation of DBSCAN.\n\n    Clusters are then extracted using a DBSCAN-like method\n    (cluster_method = 'dbscan') or an automatic\n    technique proposed in [1]_ (cluster_method = 'xi').\n\n    This implementation deviates from the original OPTICS by first performing\n    k-nearest-neighborhood searches on all points to identify core sizes, then\n    computing only the distances to unprocessed points when constructing the\n    cluster order. Note that we do not employ a heap to manage the expansion\n    candidates, so the time complexity will be O(n^2).\n\n    Read more in the :ref:`User Guide <optics>`.\n\n    Parameters\n    ----------\n    min_samples : int > 1 or float between 0 and 1, default=5\n        The number of samples in a neighborhood for a point to be considered as\n        a core point. Also, up and down steep regions can't have more than\n        ``min_samples`` consecutive non-steep points. Expressed as an absolute\n        number or a fraction of the number of samples (rounded to be at least\n        2).\n\n    max_eps : float, default=np.inf\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. Default value of ``np.inf`` will\n        identify clusters across all scales; reducing ``max_eps`` will result\n        in shorter run times.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Any metric from scikit-learn\n        or scipy.spatial.distance can be used.\n\n        If metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays as input and return one value indicating the\n        distance between them. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string. If metric is\n        \"precomputed\", `X` is assumed to be a distance matrix and must be\n        square.\n\n        Valid values for metric are:\n\n        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n          'manhattan']\n\n        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n          'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n          'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n          'yule']\n\n        Sparse matrices are only supported by scikit-learn metrics.\n        See the documentation for scipy.spatial.distance for details on these\n        metrics.\n\n        .. note::\n           `'kulsinski'` is deprecated from SciPy 1.9 and will removed in SciPy 1.11.\n\n    p : float, default=2\n        Parameter for the Minkowski metric from\n        :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    cluster_method : str, default='xi'\n        The extraction method used to extract clusters using the calculated\n        reachability and ordering. Possible values are \"xi\" and \"dbscan\".\n\n    eps : float, default=None\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. By default it assumes the same value\n        as ``max_eps``.\n        Used only when ``cluster_method='dbscan'``.\n\n    xi : float between 0 and 1, default=0.05\n        Determines the minimum steepness on the reachability plot that\n        constitutes a cluster boundary. For example, an upwards point in the\n        reachability plot is defined by the ratio from one point to its\n        successor being at most 1-xi.\n        Used only when ``cluster_method='xi'``.\n\n    predecessor_correction : bool, default=True\n        Correct clusters according to the predecessors calculated by OPTICS\n        [2]_. This parameter has minimal effect on most datasets.\n        Used only when ``cluster_method='xi'``.\n\n    min_cluster_size : int > 1 or float between 0 and 1, default=None\n        Minimum number of samples in an OPTICS cluster, expressed as an\n        absolute number or a fraction of the number of samples (rounded to be\n        at least 2). If ``None``, the value of ``min_samples`` is used instead.\n        Used only when ``cluster_method='xi'``.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`~sklearn.neighbors.BallTree`.\n        - 'kd_tree' will use :class:`~sklearn.neighbors.KDTree`.\n        - 'brute' will use a brute-force search.\n        - 'auto' (default) will attempt to decide the most appropriate\n          algorithm based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to :class:`~sklearn.neighbors.BallTree` or\n        :class:`~sklearn.neighbors.KDTree`. This can affect the speed of the\n        construction and query, as well as the memory required to store the\n        tree. The optimal value depends on the nature of the problem.\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    labels_ : ndarray of shape (n_samples,)\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples and points which are not included in a leaf cluster\n        of ``cluster_hierarchy_`` are labeled as -1.\n\n    reachability_ : ndarray of shape (n_samples,)\n        Reachability distances per sample, indexed by object order. Use\n        ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\n    ordering_ : ndarray of shape (n_samples,)\n        The cluster ordered list of sample indices.\n\n    core_distances_ : ndarray of shape (n_samples,)\n        Distance at which each sample becomes a core point, indexed by object\n        order. Points which will never be core have a distance of inf. Use\n        ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\n    predecessor_ : ndarray of shape (n_samples,)\n        Point that a sample was reached from, indexed by object order.\n        Seed points have a predecessor of -1.\n\n    cluster_hierarchy_ : ndarray of shape (n_clusters, 2)\n        The list of clusters in the form of ``[start, end]`` in each row, with\n        all indices inclusive. The clusters are ordered according to\n        ``(end, -start)`` (ascending) so that larger clusters encompassing\n        smaller clusters come after those smaller ones. Since ``labels_`` does\n        not reflect the hierarchy, usually\n        ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also\n        note that these indices are of the ``ordering_``, i.e.\n        ``X[ordering_][start:end + 1]`` form a cluster.\n        Only available when ``cluster_method='xi'``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DBSCAN : A similar clustering for a specified neighborhood radius (eps).\n        Our implementation is optimized for runtime.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n    .. [2] Schubert, Erich, Michael Gertz.\n       \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n       the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import OPTICS\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> clustering = OPTICS(min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n\n    For a more detailed example see\n    :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"min_samples\": [\n            Interval(Integral, 2, None, closed=\"left\"),\n            Interval(RealNotInt, 0, 1, closed=\"both\"),\n        ],\n        \"max_eps\": [Interval(Real, 0, None, closed=\"both\")],\n        \"metric\": [StrOptions(set(_VALID_METRICS) | {\"precomputed\"}), callable],\n        \"p\": [Interval(Real, 1, None, closed=\"left\")],\n        \"metric_params\": [dict, None],\n        \"cluster_method\": [StrOptions({\"dbscan\", \"xi\"})],\n        \"eps\": [Interval(Real, 0, None, closed=\"both\"), None],\n        \"xi\": [Interval(Real, 0, 1, closed=\"both\")],\n        \"predecessor_correction\": [\"boolean\"],\n        \"min_cluster_size\": [\n            Interval(Integral, 2, None, closed=\"left\"),\n            Interval(RealNotInt, 0, 1, closed=\"right\"),\n            None,\n        ],\n        \"algorithm\": [StrOptions({\"auto\", \"brute\", \"ball_tree\", \"kd_tree\"})],\n        \"leaf_size\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"memory\": [str, HasMethods(\"cache\"), None],\n        \"n_jobs\": [Integral, None],\n    }\n\n    def __init__(\n        self,\n        *,\n        min_samples=5,\n        max_eps=np.inf,\n        metric=\"minkowski\",\n        p=2,\n        metric_params=None,\n        cluster_method=\"xi\",\n        eps=None,\n        xi=0.05,\n        predecessor_correction=True,\n        min_cluster_size=None,\n        algorithm=\"auto\",\n        leaf_size=30,\n        memory=None,\n        n_jobs=None,\n    ):\n        self.max_eps = max_eps\n        self.min_samples = min_samples\n        self.min_cluster_size = min_cluster_size\n        self.algorithm = algorithm\n        self.metric = metric\n        self.metric_params = metric_params\n        self.p = p\n        self.leaf_size = leaf_size\n        self.cluster_method = cluster_method\n        self.eps = eps\n        self.xi = xi\n        self.predecessor_correction = predecessor_correction\n        self.memory = memory\n        self.n_jobs = n_jobs\n\n    @_fit_context(\n        # Optics.metric is not validated yet\n        prefer_skip_nested_validation=False\n    )\n    def fit(self, X, y=None):\n        \"\"\"Perform OPTICS clustering.\n\n        Extracts an ordered list of points and reachability distances, and\n        performs initial clustering using ``max_eps`` distance specified at\n        OPTICS object instantiation.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features), or \\\n                (n_samples, n_samples) if metric='precomputed'\n            A feature array, or array of distances between samples if\n            metric='precomputed'. If a sparse matrix is provided, it will be\n            converted into CSR format.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns a fitted instance of self.\n        \"\"\"\n        dtype = bool if self.metric in PAIRWISE_BOOLEAN_FUNCTIONS else float\n        if dtype is bool and X.dtype != bool:\n            msg = (\n                \"Data will be converted to boolean for\"\n                f\" metric {self.metric}, to avoid this warning,\"\n                \" you may convert the data prior to calling fit.\"\n            )\n            warnings.warn(msg, DataConversionWarning)\n\n        X = self._validate_data(X, dtype=dtype, accept_sparse=\"csr\")\n        if self.metric == \"precomputed\" and issparse(X):\n            X = X.copy()  # copy to avoid in-place modification\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", SparseEfficiencyWarning)\n                # Set each diagonal to an explicit value so each point is its\n                # own neighbor\n                X.setdiag(X.diagonal())\n        memory = check_memory(self.memory)\n\n        (\n            self.ordering_,\n            self.core_distances_,\n            self.reachability_,\n            self.predecessor_,\n        ) = memory.cache(compute_optics_graph)(\n            X=X,\n            min_samples=self.min_samples,\n            algorithm=self.algorithm,\n            leaf_size=self.leaf_size,\n            metric=self.metric,\n            metric_params=self.metric_params,\n            p=self.p,\n            n_jobs=self.n_jobs,\n            max_eps=self.max_eps,\n        )\n\n        # Extract clusters from the calculated orders and reachability\n        if self.cluster_method == \"xi\":\n            labels_, clusters_ = cluster_optics_xi(\n                reachability=self.reachability_,\n                predecessor=self.predecessor_,\n                ordering=self.ordering_,\n                min_samples=self.min_samples,\n                min_cluster_size=self.min_cluster_size,\n                xi=self.xi,\n                predecessor_correction=self.predecessor_correction,\n            )\n            self.cluster_hierarchy_ = clusters_\n        elif self.cluster_method == \"dbscan\":\n            if self.eps is None:\n                eps = self.max_eps\n            else:\n                eps = self.eps\n\n            if eps > self.max_eps:\n                raise ValueError(\n                    \"Specify an epsilon smaller than %s. Got %s.\" % (self.max_eps, eps)\n                )\n\n            labels_ = cluster_optics_dbscan(\n                reachability=self.reachability_,\n                core_distances=self.core_distances_,\n                ordering=self.ordering_,\n                eps=eps,\n            )\n\n        self.labels_ = labels_\n        return self\n", "class_fn": true, "question_id": "sklearn/sklearn.cluster._optics/OPTICS", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/compose/_column_transformer.py", "fn_id": "", "content": "class _RemainderColsList(UserList):\n    \"\"\"A list that raises a warning whenever items are accessed.\n\n    It is used to store the columns handled by the \"remainder\" entry of\n    ``ColumnTransformer.transformers_``, ie ``transformers_[-1][-1]``.\n\n    For some values of the ``ColumnTransformer`` ``transformers`` parameter,\n    this list of indices will be replaced by either a list of column names or a\n    boolean mask; in those cases we emit a ``FutureWarning`` the first time an\n    element is accessed.\n\n    Parameters\n    ----------\n    columns : list of int\n        The remainder columns.\n\n    future_dtype : {'str', 'bool'}, default=None\n        The dtype that will be used by a ColumnTransformer with the same inputs\n        in a future release. There is a default value because providing a\n        constructor that takes a single argument is a requirement for\n        subclasses of UserList, but we do not use it in practice. It would only\n        be used if a user called methods that return a new list such are\n        copying or concatenating `_RemainderColsList`.\n\n    warning_was_emitted : bool, default=False\n       Whether the warning for that particular list was already shown, so we\n       only emit it once.\n\n    warning_enabled : bool, default=True\n        When False, the list never emits the warning nor updates\n        `warning_was_emitted``. This is used to obtain a quiet copy of the list\n        for use by the `ColumnTransformer` itself, so that the warning is only\n        shown when a user accesses it directly.\n    \"\"\"\n\n    def __init__(\n        self,\n        columns,\n        *,\n        future_dtype=None,\n        warning_was_emitted=False,\n        warning_enabled=True,\n    ):\n        super().__init__(columns)\n        self.future_dtype = future_dtype\n        self.warning_was_emitted = warning_was_emitted\n        self.warning_enabled = warning_enabled\n\n    def __getitem__(self, index):\n        self._show_remainder_cols_warning()\n        return super().__getitem__(index)\n\n    def _show_remainder_cols_warning(self):\n        if self.warning_was_emitted or not self.warning_enabled:\n            return\n        self.warning_was_emitted = True\n        future_dtype_description = {\n            \"str\": \"column names (of type str)\",\n            \"bool\": \"a mask array (of type bool)\",\n            # shouldn't happen because we always initialize it with a\n            # non-default future_dtype\n            None: \"a different type depending on the ColumnTransformer inputs\",\n        }.get(self.future_dtype, self.future_dtype)\n\n        # TODO(1.7) Update the warning to say that the old behavior will be\n        # removed in 1.9.\n        warnings.warn(\n            (\n                \"\\nThe format of the columns of the 'remainder' transformer in\"\n                \" ColumnTransformer.transformers_ will change in version 1.7 to\"\n                \" match the format of the other transformers.\\nAt the moment the\"\n                \" remainder columns are stored as indices (of type int). With the same\"\n                \" ColumnTransformer configuration, in the future they will be stored\"\n                f\" as {future_dtype_description}.\\nTo use the new behavior now and\"\n                \" suppress this warning, use\"\n                \" ColumnTransformer(force_int_remainder_cols=False).\\n\"\n            ),\n            category=FutureWarning,\n        )\n\n    def _repr_pretty_(self, printer, *_):\n        \"\"\"Override display in ipython console, otherwise the class name is shown.\"\"\"\n        printer.text(repr(self.data))\n", "class_fn": true, "question_id": "sklearn/sklearn.compose._column_transformer/_RemainderColsList", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/covariance/_elliptic_envelope.py", "fn_id": "", "content": "class EllipticEnvelope(OutlierMixin, MinCovDet):\n    \"\"\"An object for detecting outliers in a Gaussian distributed dataset.\n\n    Read more in the :ref:`User Guide <outlier_detection>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, the support of robust location and covariance estimates\n        is computed, and a covariance estimate is recomputed from it,\n        without centering the data.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, the robust location and covariance are directly computed\n        with the FastMCD algorithm without additional treatment.\n\n    support_fraction : float, default=None\n        The proportion of points to be included in the support of the raw\n        MCD estimate. If None, the minimum value of support_fraction will\n        be used within the algorithm: `(n_samples + n_features + 1) / 2 * n_samples`.\n        Range is (0, 1).\n\n    contamination : float, default=0.1\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set. Range is (0, 0.5].\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the pseudo random number generator for shuffling\n        the data. Pass an int for reproducible results across multiple function\n        calls. See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated robust location.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated robust covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute the\n        robust estimates of location and shape.\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores.\n        We have the relation: ``decision_function = score_samples - offset_``.\n        The offset depends on the contamination parameter and is defined in\n        such a way we obtain the expected number of outliers (samples with\n        decision function < 0) in training.\n\n        .. versionadded:: 0.20\n\n    raw_location_ : ndarray of shape (n_features,)\n        The raw robust estimated location before correction and re-weighting.\n\n    raw_covariance_ : ndarray of shape (n_features, n_features)\n        The raw robust estimated covariance before correction and re-weighting.\n\n    raw_support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the raw robust estimates of location and shape, before correction\n        and re-weighting.\n\n    dist_ : ndarray of shape (n_samples,)\n        Mahalanobis distances of the training set (on which :meth:`fit` is\n        called) observations.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    OAS : Oracle Approximating Shrinkage Estimator.\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Notes\n    -----\n    Outlier detection from covariance estimation may break or not\n    perform well in high-dimensional settings. In particular, one will\n    always take care to work with ``n_samples > n_features ** 2``.\n\n    References\n    ----------\n    .. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the\n       minimum covariance determinant estimator\" Technometrics 41(3), 212\n       (1999)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EllipticEnvelope\n    >>> true_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n    ...                                                  cov=true_cov,\n    ...                                                  size=500)\n    >>> cov = EllipticEnvelope(random_state=0).fit(X)\n    >>> # predict returns 1 for an inlier and -1 for an outlier\n    >>> cov.predict([[0, 0],\n    ...              [3, 3]])\n    array([ 1, -1])\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **MinCovDet._parameter_constraints,\n        \"contamination\": [Interval(Real, 0, 0.5, closed=\"right\")],\n    }\n\n    def __init__(\n        self,\n        *,\n        store_precision=True,\n        assume_centered=False,\n        support_fraction=None,\n        contamination=0.1,\n        random_state=None,\n    ):\n        super().__init__(\n            store_precision=store_precision,\n            assume_centered=assume_centered,\n            support_fraction=support_fraction,\n            random_state=random_state,\n        )\n        self.contamination = contamination\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the EllipticEnvelope model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        super().fit(X)\n        self.offset_ = np.percentile(-self.dist_, 100.0 * self.contamination)\n        return self\n\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of the given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        decision : ndarray of shape (n_samples,)\n            Decision function of the samples.\n            It is equal to the shifted Mahalanobis distances.\n            The threshold for being an outlier is 0, which ensures a\n            compatibility with other outlier detection algorithms.\n        \"\"\"\n        check_is_fitted(self)\n        negative_mahal_dist = self.score_samples(X)\n        return negative_mahal_dist - self.offset_\n\n    def score_samples(self, X):\n        \"\"\"Compute the negative Mahalanobis distances.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        negative_mahal_distances : array-like of shape (n_samples,)\n            Opposite of the Mahalanobis distances.\n        \"\"\"\n        check_is_fitted(self)\n        return -self.mahalanobis(X)\n\n    def predict(self, X):\n        \"\"\"\n        Predict labels (1 inlier, -1 outlier) of X according to fitted model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        is_inlier : ndarray of shape (n_samples,)\n            Returns -1 for anomalies/outliers and +1 for inliers.\n        \"\"\"\n        values = self.decision_function(X)\n        is_inlier = np.full(values.shape[0], -1, dtype=int)\n        is_inlier[values >= 0] = 1\n\n        return is_inlier\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) w.r.t. y.\n        \"\"\"\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n", "class_fn": true, "question_id": "sklearn/sklearn.covariance._elliptic_envelope/EllipticEnvelope", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/covariance/_graph_lasso.py", "fn_id": "", "content": "class GraphicalLassoCV(BaseGraphicalLasso):\n    \"\"\"Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    .. versionchanged:: v0.20\n        GraphLassoCV has been renamed to GraphicalLassoCV\n\n    Parameters\n    ----------\n    alphas : int or array-like of shape (n_alphas,), dtype=float, default=4\n        If an integer is given, it fixes the number of points on the\n        grids of alpha to be used. If a list is given, it gives the\n        grid to be used. See the notes in the class docstring for\n        more details. Range is [1, inf) for an integer.\n        Range is (0, inf] for an array-like of floats.\n\n    n_refinements : int, default=4\n        The number of times the grid is refined. Not used if explicit\n        values of alphas are passed. Range is [1, inf).\n\n    cv : int, cross-validation generator or iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    tol : float, default=1e-4\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped. Range is (0, inf].\n\n    enet_tol : float, default=1e-4\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'. Range is (0, inf].\n\n    max_iter : int, default=100\n        Maximum number of iterations.\n\n    mode : {'cd', 'lars'}, default='cd'\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where number of features is greater\n        than number of samples. Elsewhere prefer cd which is more numerically\n        stable.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    verbose : bool, default=False\n        If verbose is True, the objective function and duality gap are\n        printed at each iteration.\n\n    eps : float, default=eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Default is `np.finfo(np.float64).eps`.\n\n        .. versionadded:: 1.3\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated precision matrix (inverse covariance).\n\n    costs_ : list of (objective, dual_gap) pairs\n        The list of values of the objective function and the dual gap at\n        each iteration. Returned only if return_costs is True.\n\n        .. versionadded:: 1.3\n\n    alpha_ : float\n        Penalization parameter selected.\n\n    cv_results_ : dict of ndarrays\n        A dict with keys:\n\n        alphas : ndarray of shape (n_alphas,)\n            All penalization parameters explored.\n\n        split(k)_test_score : ndarray of shape (n_alphas,)\n            Log-likelihood score on left-out data across (k)th fold.\n\n            .. versionadded:: 1.0\n\n        mean_test_score : ndarray of shape (n_alphas,)\n            Mean of scores over the folds.\n\n            .. versionadded:: 1.0\n\n        std_test_score : ndarray of shape (n_alphas,)\n            Standard deviation of scores over the folds.\n\n            .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of iterations run for the optimal alpha.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    graphical_lasso : L1-penalized covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n\n    Notes\n    -----\n    The search for the optimal penalization parameter (`alpha`) is done on an\n    iteratively refined grid: first the cross-validated scores on a grid are\n    computed, then a new refined grid is centered around the maximum, and so\n    on.\n\n    One of the challenges which is faced here is that the solvers can\n    fail to converge to a well-conditioned estimate. The corresponding\n    values of `alpha` then come out as missing values, but the optimum may\n    be close to these missing values.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import GraphicalLassoCV\n    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n    ...                      [0.0, 0.4, 0.0, 0.0],\n    ...                      [0.2, 0.0, 0.3, 0.1],\n    ...                      [0.0, 0.0, 0.1, 0.7]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n    ...                                   cov=true_cov,\n    ...                                   size=200)\n    >>> cov = GraphicalLassoCV().fit(X)\n    >>> np.around(cov.covariance_, decimals=3)\n    array([[0.816, 0.051, 0.22 , 0.017],\n           [0.051, 0.364, 0.018, 0.036],\n           [0.22 , 0.018, 0.322, 0.094],\n           [0.017, 0.036, 0.094, 0.69 ]])\n    >>> np.around(cov.location_, decimals=3)\n    array([0.073, 0.04 , 0.038, 0.143])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **BaseGraphicalLasso._parameter_constraints,\n        \"alphas\": [Interval(Integral, 0, None, closed=\"left\"), \"array-like\"],\n        \"n_refinements\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"cv\": [\"cv_object\"],\n        \"n_jobs\": [Integral, None],\n    }\n\n    def __init__(\n        self,\n        *,\n        alphas=4,\n        n_refinements=4,\n        cv=None,\n        tol=1e-4,\n        enet_tol=1e-4,\n        max_iter=100,\n        mode=\"cd\",\n        n_jobs=None,\n        verbose=False,\n        eps=np.finfo(np.float64).eps,\n        assume_centered=False,\n    ):\n        super().__init__(\n            tol=tol,\n            enet_tol=enet_tol,\n            max_iter=max_iter,\n            mode=mode,\n            verbose=verbose,\n            eps=eps,\n            assume_centered=assume_centered,\n        )\n        self.alphas = alphas\n        self.n_refinements = n_refinements\n        self.cv = cv\n        self.n_jobs = n_jobs\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None, **params):\n        \"\"\"Fit the GraphicalLasso covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **params : dict, default=None\n            Parameters to be passed to the CV splitter and the\n            cross_val_score function.\n\n            .. versionadded:: 1.5\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        # Covariance does not make sense for a single feature\n        _raise_for_params(params, self, \"fit\")\n\n        X = self._validate_data(X, ensure_min_features=2)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        emp_cov = empirical_covariance(X, assume_centered=self.assume_centered)\n\n        cv = check_cv(self.cv, y, classifier=False)\n\n        # List of (alpha, scores, covs)\n        path = list()\n        n_alphas = self.alphas\n        inner_verbose = max(0, self.verbose - 1)\n\n        if _is_arraylike_not_scalar(n_alphas):\n            for alpha in self.alphas:\n                check_scalar(\n                    alpha,\n                    \"alpha\",\n                    Real,\n                    min_val=0,\n                    max_val=np.inf,\n                    include_boundaries=\"right\",\n                )\n            alphas = self.alphas\n            n_refinements = 1\n        else:\n            n_refinements = self.n_refinements\n            alpha_1 = alpha_max(emp_cov)\n            alpha_0 = 1e-2 * alpha_1\n            alphas = np.logspace(np.log10(alpha_0), np.log10(alpha_1), n_alphas)[::-1]\n\n        if _routing_enabled():\n            routed_params = process_routing(self, \"fit\", **params)\n        else:\n            routed_params = Bunch(splitter=Bunch(split={}))\n\n        t0 = time.time()\n        for i in range(n_refinements):\n            with warnings.catch_warnings():\n                # No need to see the convergence warnings on this grid:\n                # they will always be points that will not converge\n                # during the cross-validation\n                warnings.simplefilter(\"ignore\", ConvergenceWarning)\n                # Compute the cross-validated loss on the current grid\n\n                # NOTE: Warm-restarting graphical_lasso_path has been tried,\n                # and this did not allow to gain anything\n                # (same execution time with or without).\n                this_path = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n                    delayed(graphical_lasso_path)(\n                        X[train],\n                        alphas=alphas,\n                        X_test=X[test],\n                        mode=self.mode,\n                        tol=self.tol,\n                        enet_tol=self.enet_tol,\n                        max_iter=int(0.1 * self.max_iter),\n                        verbose=inner_verbose,\n                        eps=self.eps,\n                    )\n                    for train, test in cv.split(X, y, **routed_params.splitter.split)\n                )\n\n            # Little danse to transform the list in what we need\n            covs, _, scores = zip(*this_path)\n            covs = zip(*covs)\n            scores = zip(*scores)\n            path.extend(zip(alphas, scores, covs))\n            path = sorted(path, key=operator.itemgetter(0), reverse=True)\n\n            # Find the maximum (avoid using built in 'max' function to\n            # have a fully-reproducible selection of the smallest alpha\n            # in case of equality)\n            best_score = -np.inf\n            last_finite_idx = 0\n            for index, (alpha, scores, _) in enumerate(path):\n                this_score = np.mean(scores)\n                if this_score >= 0.1 / np.finfo(np.float64).eps:\n                    this_score = np.nan\n                if np.isfinite(this_score):\n                    last_finite_idx = index\n                if this_score >= best_score:\n                    best_score = this_score\n                    best_index = index\n\n            # Refine the grid\n            if best_index == 0:\n                # We do not need to go back: we have chosen\n                # the highest value of alpha for which there are\n                # non-zero coefficients\n                alpha_1 = path[0][0]\n                alpha_0 = path[1][0]\n            elif best_index == last_finite_idx and not best_index == len(path) - 1:\n                # We have non-converged models on the upper bound of the\n                # grid, we need to refine the grid there\n                alpha_1 = path[best_index][0]\n                alpha_0 = path[best_index + 1][0]\n            elif best_index == len(path) - 1:\n                alpha_1 = path[best_index][0]\n                alpha_0 = 0.01 * path[best_index][0]\n            else:\n                alpha_1 = path[best_index - 1][0]\n                alpha_0 = path[best_index + 1][0]\n\n            if not _is_arraylike_not_scalar(n_alphas):\n                alphas = np.logspace(np.log10(alpha_1), np.log10(alpha_0), n_alphas + 2)\n                alphas = alphas[1:-1]\n\n            if self.verbose and n_refinements > 1:\n                print(\n                    \"[GraphicalLassoCV] Done refinement % 2i out of %i: % 3is\"\n                    % (i + 1, n_refinements, time.time() - t0)\n                )\n\n        path = list(zip(*path))\n        grid_scores = list(path[1])\n        alphas = list(path[0])\n        # Finally, compute the score with alpha = 0\n        alphas.append(0)\n        grid_scores.append(\n            cross_val_score(\n                EmpiricalCovariance(),\n                X,\n                cv=cv,\n                n_jobs=self.n_jobs,\n                verbose=inner_verbose,\n                params=params,\n            )\n        )\n        grid_scores = np.array(grid_scores)\n\n        self.cv_results_ = {\"alphas\": np.array(alphas)}\n\n        for i in range(grid_scores.shape[1]):\n            self.cv_results_[f\"split{i}_test_score\"] = grid_scores[:, i]\n\n        self.cv_results_[\"mean_test_score\"] = np.mean(grid_scores, axis=1)\n        self.cv_results_[\"std_test_score\"] = np.std(grid_scores, axis=1)\n\n        best_alpha = alphas[best_index]\n        self.alpha_ = best_alpha\n\n        # Finally fit the model with the selected alpha\n        self.covariance_, self.precision_, self.costs_, self.n_iter_ = _graphical_lasso(\n            emp_cov,\n            alpha=best_alpha,\n            mode=self.mode,\n            tol=self.tol,\n            enet_tol=self.enet_tol,\n            max_iter=self.max_iter,\n            verbose=inner_verbose,\n            eps=self.eps,\n        )\n        return self\n\n    def get_metadata_routing(self):\n        \"\"\"Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.5\n\n        Returns\n        -------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.\n        \"\"\"\n        router = MetadataRouter(owner=self.__class__.__name__).add(\n            splitter=check_cv(self.cv),\n            method_mapping=MethodMapping().add(callee=\"split\", caller=\"fit\"),\n        )\n        return router\n", "class_fn": true, "question_id": "sklearn/sklearn.covariance._graph_lasso/GraphicalLassoCV", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/covariance/_shrunk_covariance.py", "fn_id": "", "content": "class OAS(EmpiricalCovariance):\n    \"\"\"Oracle Approximating Shrinkage Estimator.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data will be centered before computation.\n\n    Attributes\n    ----------\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    shrinkage_ : float\n      coefficient in the convex combination used for the computation\n      of the shrunk estimate. Range is [0, 1].\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with cross-validated\n        choice of the l1 penalty.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features),\n\n    where mu = trace(cov) / n_features and shrinkage is given by the OAS formula\n    (see [1]_).\n\n    The shrinkage formulation implemented here differs from Eq. 23 in [1]_. In\n    the original article, formula (23) states that 2/p (p being the number of\n    features) is multiplied by Trace(cov*cov) in both the numerator and\n    denominator, but this operation is omitted because for a large p, the value\n    of 2/p is so small that it doesn't affect the value of the estimator.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Shrinkage algorithms for MMSE covariance estimation.\",\n           Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.\n           IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\n           <0907.4698>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import OAS\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> oas = OAS().fit(X)\n    >>> oas.covariance_\n    array([[0.7533..., 0.2763...],\n           [0.2763..., 0.3964...]])\n    >>> oas.precision_\n    array([[ 1.7833..., -1.2431... ],\n           [-1.2431...,  3.3889...]])\n    >>> oas.shrinkage_\n    np.float64(0.0195...)\n    \"\"\"\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the Oracle Approximating Shrinkage covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = self._validate_data(X)\n        # Not calling the parent object to fit, to avoid computing the\n        # covariance matrix (and potentially the precision)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n\n        covariance, shrinkage = _oas(X - self.location_, assume_centered=True)\n        self.shrinkage_ = shrinkage\n        self._set_covariance(covariance)\n\n        return self\n", "class_fn": true, "question_id": "sklearn/sklearn.covariance._shrunk_covariance/OAS", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cross_decomposition/_pls.py", "fn_id": "", "content": "class PLSCanonical(_PLS):\n    \"\"\"Partial Least Squares transformer and regressor.\n\n    For a comparison between other cross decomposition algorithms, see\n    :ref:`sphx_glr_auto_examples_cross_decomposition_plot_compare_cross_decomposition.py`.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    .. versionadded:: 0.8\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Number of components to keep. Should be in `[1, min(n_samples,\n        n_features, n_targets)]`.\n\n    scale : bool, default=True\n        Whether to scale `X` and `Y`.\n\n    algorithm : {'nipals', 'svd'}, default='nipals'\n        The algorithm used to estimate the first singular vectors of the\n        cross-covariance matrix. 'nipals' uses the power method while 'svd'\n        will compute the whole SVD.\n\n    max_iter : int, default=500\n        The maximum number of iterations of the power method when\n        `algorithm='nipals'`. Ignored otherwise.\n\n    tol : float, default=1e-06\n        The tolerance used as convergence criteria in the power method: the\n        algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n        than `tol`, where `u` corresponds to the left singular vector.\n\n    copy : bool, default=True\n        Whether to copy `X` and `Y` in fit before applying centering, and\n        potentially scaling. If False, these operations will be done inplace,\n        modifying both arrays.\n\n    Attributes\n    ----------\n    x_weights_ : ndarray of shape (n_features, n_components)\n        The left singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    y_weights_ : ndarray of shape (n_targets, n_components)\n        The right singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    x_loadings_ : ndarray of shape (n_features, n_components)\n        The loadings of `X`.\n\n    y_loadings_ : ndarray of shape (n_targets, n_components)\n        The loadings of `Y`.\n\n    x_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `X`.\n\n    y_rotations_ : ndarray of shape (n_targets, n_components)\n        The projection matrix used to transform `Y`.\n\n    coef_ : ndarray of shape (n_targets, n_features)\n        The coefficients of the linear model such that `Y` is approximated as\n        `Y = X @ coef_.T + intercept_`.\n\n    intercept_ : ndarray of shape (n_targets,)\n        The intercepts of the linear model such that `Y` is approximated as\n        `Y = X @ coef_.T + intercept_`.\n\n        .. versionadded:: 1.1\n\n    n_iter_ : list of shape (n_components,)\n        Number of iterations of the power method, for each\n        component. Empty if `algorithm='svd'`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    CCA : Canonical Correlation Analysis.\n    PLSSVD : Partial Least Square SVD.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import PLSCanonical\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    >>> y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> plsca = PLSCanonical(n_components=2)\n    >>> plsca.fit(X, y)\n    PLSCanonical()\n    >>> X_c, y_c = plsca.transform(X, y)\n    \"\"\"\n\n    _parameter_constraints: dict = {**_PLS._parameter_constraints}\n    for param in (\"deflation_mode\", \"mode\"):\n        _parameter_constraints.pop(param)\n\n    # This implementation provides the same results that the \"plspm\" package\n    # provided in the R language (R-project), using the function plsca(X, Y).\n    # Results are equal or collinear with the function\n    # ``pls(..., mode = \"canonical\")`` of the \"mixOmics\" package. The\n    # difference relies in the fact that mixOmics implementation does not\n    # exactly implement the Wold algorithm since it does not normalize\n    # y_weights to one.\n\n    def __init__(\n        self,\n        n_components=2,\n        *,\n        scale=True,\n        algorithm=\"nipals\",\n        max_iter=500,\n        tol=1e-06,\n        copy=True,\n    ):\n        super().__init__(\n            n_components=n_components,\n            scale=scale,\n            deflation_mode=\"canonical\",\n            mode=\"A\",\n            algorithm=algorithm,\n            max_iter=max_iter,\n            tol=tol,\n            copy=copy,\n        )\n", "class_fn": true, "question_id": "sklearn/sklearn.cross_decomposition._pls/PLSCanonical", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cross_decomposition/_pls.py", "fn_id": "", "content": "class _PLS(\n    ClassNamePrefixFeaturesOutMixin,\n    TransformerMixin,\n    RegressorMixin,\n    MultiOutputMixin,\n    BaseEstimator,\n    metaclass=ABCMeta,\n):\n    \"\"\"Partial Least Squares (PLS)\n\n    This class implements the generic PLS algorithm.\n\n    Main ref: Wegelin, a survey of Partial Least Squares (PLS) methods,\n    with emphasis on the two-block case\n    https://stat.uw.edu/sites/default/files/files/reports/2000/tr371.pdf\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"scale\": [\"boolean\"],\n        \"deflation_mode\": [StrOptions({\"regression\", \"canonical\"})],\n        \"mode\": [StrOptions({\"A\", \"B\"})],\n        \"algorithm\": [StrOptions({\"svd\", \"nipals\"})],\n        \"max_iter\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n        \"copy\": [\"boolean\"],\n    }\n\n    @abstractmethod\n    def __init__(\n        self,\n        n_components=2,\n        *,\n        scale=True,\n        deflation_mode=\"regression\",\n        mode=\"A\",\n        algorithm=\"nipals\",\n        max_iter=500,\n        tol=1e-06,\n        copy=True,\n    ):\n        self.n_components = n_components\n        self.deflation_mode = deflation_mode\n        self.mode = mode\n        self.scale = scale\n        self.algorithm = algorithm\n        self.max_iter = max_iter\n        self.tol = tol\n        self.copy = copy\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None, Y=None):\n        \"\"\"Fit model to data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of predictors.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target vectors, where `n_samples` is the number of samples and\n            `n_targets` is the number of response variables.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target vectors, where `n_samples` is the number of samples and\n            `n_targets` is the number of response variables.\n\n            .. deprecated:: 1.5\n               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.\n\n        Returns\n        -------\n        self : object\n            Fitted model.\n        \"\"\"\n        y = _deprecate_Y_when_required(y, Y)\n\n        check_consistent_length(X, y)\n        X = self._validate_data(\n            X,\n            dtype=np.float64,\n            force_writeable=True,\n            copy=self.copy,\n            ensure_min_samples=2,\n        )\n        y = check_array(\n            y,\n            input_name=\"y\",\n            dtype=np.float64,\n            force_writeable=True,\n            copy=self.copy,\n            ensure_2d=False,\n        )\n        if y.ndim == 1:\n            self._predict_1d = True\n            y = y.reshape(-1, 1)\n        else:\n            self._predict_1d = False\n\n        n = X.shape[0]\n        p = X.shape[1]\n        q = y.shape[1]\n\n        n_components = self.n_components\n        # With PLSRegression n_components is bounded by the rank of (X.T X) see\n        # Wegelin page 25. With CCA and PLSCanonical, n_components is bounded\n        # by the rank of X and the rank of Y: see Wegelin page 12\n        rank_upper_bound = p if self.deflation_mode == \"regression\" else min(n, p, q)\n        if n_components > rank_upper_bound:\n            raise ValueError(\n                f\"`n_components` upper bound is {rank_upper_bound}. \"\n                f\"Got {n_components} instead. Reduce `n_components`.\"\n            )\n\n        self._norm_y_weights = self.deflation_mode == \"canonical\"  # 1.1\n        norm_y_weights = self._norm_y_weights\n\n        # Scale (in place)\n        Xk, yk, self._x_mean, self._y_mean, self._x_std, self._y_std = _center_scale_xy(\n            X, y, self.scale\n        )\n\n        self.x_weights_ = np.zeros((p, n_components))  # U\n        self.y_weights_ = np.zeros((q, n_components))  # V\n        self._x_scores = np.zeros((n, n_components))  # Xi\n        self._y_scores = np.zeros((n, n_components))  # Omega\n        self.x_loadings_ = np.zeros((p, n_components))  # Gamma\n        self.y_loadings_ = np.zeros((q, n_components))  # Delta\n        self.n_iter_ = []\n\n        # This whole thing corresponds to the algorithm in section 4.1 of the\n        # review from Wegelin. See above for a notation mapping from code to\n        # paper.\n        y_eps = np.finfo(yk.dtype).eps\n        for k in range(n_components):\n            # Find first left and right singular vectors of the X.T.dot(Y)\n            # cross-covariance matrix.\n            if self.algorithm == \"nipals\":\n                # Replace columns that are all close to zero with zeros\n                yk_mask = np.all(np.abs(yk) < 10 * y_eps, axis=0)\n                yk[:, yk_mask] = 0.0\n\n                try:\n                    (\n                        x_weights,\n                        y_weights,\n                        n_iter_,\n                    ) = _get_first_singular_vectors_power_method(\n                        Xk,\n                        yk,\n                        mode=self.mode,\n                        max_iter=self.max_iter,\n                        tol=self.tol,\n                        norm_y_weights=norm_y_weights,\n                    )\n                except StopIteration as e:\n                    if str(e) != \"y residual is constant\":\n                        raise\n                    warnings.warn(f\"y residual is constant at iteration {k}\")\n                    break\n\n                self.n_iter_.append(n_iter_)\n\n            elif self.algorithm == \"svd\":\n                x_weights, y_weights = _get_first_singular_vectors_svd(Xk, yk)\n\n            # inplace sign flip for consistency across solvers and archs\n            _svd_flip_1d(x_weights, y_weights)\n\n            # compute scores, i.e. the projections of X and Y\n            x_scores = np.dot(Xk, x_weights)\n            if norm_y_weights:\n                y_ss = 1\n            else:\n                y_ss = np.dot(y_weights, y_weights)\n            y_scores = np.dot(yk, y_weights) / y_ss\n\n            # Deflation: subtract rank-one approx to obtain Xk+1 and Yk+1\n            x_loadings = np.dot(x_scores, Xk) / np.dot(x_scores, x_scores)\n            Xk -= np.outer(x_scores, x_loadings)\n\n            if self.deflation_mode == \"canonical\":\n                # regress Yk on y_score\n                y_loadings = np.dot(y_scores, yk) / np.dot(y_scores, y_scores)\n                yk -= np.outer(y_scores, y_loadings)\n            if self.deflation_mode == \"regression\":\n                # regress Yk on x_score\n                y_loadings = np.dot(x_scores, yk) / np.dot(x_scores, x_scores)\n                yk -= np.outer(x_scores, y_loadings)\n\n            self.x_weights_[:, k] = x_weights\n            self.y_weights_[:, k] = y_weights\n            self._x_scores[:, k] = x_scores\n            self._y_scores[:, k] = y_scores\n            self.x_loadings_[:, k] = x_loadings\n            self.y_loadings_[:, k] = y_loadings\n\n        # X was approximated as Xi . Gamma.T + X_(R+1)\n        # Xi . Gamma.T is a sum of n_components rank-1 matrices. X_(R+1) is\n        # whatever is left to fully reconstruct X, and can be 0 if X is of rank\n        # n_components.\n        # Similarly, y was approximated as Omega . Delta.T + y_(R+1)\n\n        # Compute transformation matrices (rotations_). See User Guide.\n        self.x_rotations_ = np.dot(\n            self.x_weights_,\n            pinv2(np.dot(self.x_loadings_.T, self.x_weights_), check_finite=False),\n        )\n        self.y_rotations_ = np.dot(\n            self.y_weights_,\n            pinv2(np.dot(self.y_loadings_.T, self.y_weights_), check_finite=False),\n        )\n        self.coef_ = np.dot(self.x_rotations_, self.y_loadings_.T)\n        self.coef_ = (self.coef_ * self._y_std).T / self._x_std\n        self.intercept_ = self._y_mean\n        self._n_features_out = self.x_rotations_.shape[1]\n        return self\n\n    def transform(self, X, y=None, Y=None, copy=True):\n        \"\"\"Apply the dimension reduction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to transform.\n\n        y : array-like of shape (n_samples, n_targets), default=None\n            Target vectors.\n\n        Y : array-like of shape (n_samples, n_targets), default=None\n            Target vectors.\n\n            .. deprecated:: 1.5\n               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.\n\n        copy : bool, default=True\n            Whether to copy `X` and `Y`, or perform in-place normalization.\n\n        Returns\n        -------\n        x_scores, y_scores : array-like or tuple of array-like\n            Return `x_scores` if `Y` is not given, `(x_scores, y_scores)` otherwise.\n        \"\"\"\n        y = _deprecate_Y_when_optional(y, Y)\n\n        check_is_fitted(self)\n        X = self._validate_data(X, copy=copy, dtype=FLOAT_DTYPES, reset=False)\n        # Normalize\n        X -= self._x_mean\n        X /= self._x_std\n        # Apply rotation\n        x_scores = np.dot(X, self.x_rotations_)\n        if y is not None:\n            y = check_array(\n                y, input_name=\"y\", ensure_2d=False, copy=copy, dtype=FLOAT_DTYPES\n            )\n            if y.ndim == 1:\n                y = y.reshape(-1, 1)\n            y -= self._y_mean\n            y /= self._y_std\n            y_scores = np.dot(y, self.y_rotations_)\n            return x_scores, y_scores\n\n        return x_scores\n\n    def inverse_transform(self, X, y=None, Y=None):\n        \"\"\"Transform data back to its original space.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            New data, where `n_samples` is the number of samples\n            and `n_components` is the number of pls components.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_components)\n            New target, where `n_samples` is the number of samples\n            and `n_components` is the number of pls components.\n\n        Y : array-like of shape (n_samples, n_components)\n            New target, where `n_samples` is the number of samples\n            and `n_components` is the number of pls components.\n\n            .. deprecated:: 1.5\n               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.\n\n        Returns\n        -------\n        X_reconstructed : ndarray of shape (n_samples, n_features)\n            Return the reconstructed `X` data.\n\n        y_reconstructed : ndarray of shape (n_samples, n_targets)\n            Return the reconstructed `X` target. Only returned when `y` is given.\n\n        Notes\n        -----\n        This transformation will only be exact if `n_components=n_features`.\n        \"\"\"\n        y = _deprecate_Y_when_optional(y, Y)\n\n        check_is_fitted(self)\n        X = check_array(X, input_name=\"X\", dtype=FLOAT_DTYPES)\n        # From pls space to original space\n        X_reconstructed = np.matmul(X, self.x_loadings_.T)\n        # Denormalize\n        X_reconstructed *= self._x_std\n        X_reconstructed += self._x_mean\n\n        if y is not None:\n            y = check_array(y, input_name=\"y\", dtype=FLOAT_DTYPES)\n            # From pls space to original space\n            y_reconstructed = np.matmul(y, self.y_loadings_.T)\n            # Denormalize\n            y_reconstructed *= self._y_std\n            y_reconstructed += self._y_mean\n            return X_reconstructed, y_reconstructed\n\n        return X_reconstructed\n\n    def predict(self, X, copy=True):\n        \"\"\"Predict targets of given samples.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples.\n\n        copy : bool, default=True\n            Whether to copy `X` and `Y`, or perform in-place normalization.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n\n        Notes\n        -----\n        This call requires the estimation of a matrix of shape\n        `(n_features, n_targets)`, which may be an issue in high dimensional\n        space.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, copy=copy, dtype=FLOAT_DTYPES, reset=False)\n        # Only center X but do not scale it since the coefficients are already scaled\n        X -= self._x_mean\n        Ypred = X @ self.coef_.T + self.intercept_\n        return Ypred.ravel() if self._predict_1d else Ypred\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Learn and apply the dimension reduction on the train data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of predictors.\n\n        y : array-like of shape (n_samples, n_targets), default=None\n            Target vectors, where `n_samples` is the number of samples and\n            `n_targets` is the number of response variables.\n\n        Returns\n        -------\n        self : ndarray of shape (n_samples, n_components)\n            Return `x_scores` if `Y` is not given, `(x_scores, y_scores)` otherwise.\n        \"\"\"\n        return self.fit(X, y).transform(X, y)\n\n    def _more_tags(self):\n        return {\"poor_score\": True, \"requires_y\": False}\n", "class_fn": true, "question_id": "sklearn/sklearn.cross_decomposition._pls/_PLS", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_dict_learning.py", "fn_id": "", "content": "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\n    \"\"\"Mini-batch dictionary learning.\n\n    Finds a dictionary (a set of atoms) that performs well at sparsely\n    encoding the fitted data.\n\n    Solves the optimization problem::\n\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                    (U,V)\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\n\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\n    the entry-wise matrix norm which is the sum of the absolute values\n    of all the entries in the matrix.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of dictionary elements to extract.\n\n    alpha : float, default=1\n        Sparsity controlling parameter.\n\n    max_iter : int, default=1_000\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n        .. versionadded:: 1.1\n\n        .. deprecated:: 1.4\n           `max_iter=None` is deprecated in 1.4 and will be removed in 1.6.\n           Use the default value (i.e. `1_000`) instead.\n\n    fit_algorithm : {'lars', 'cd'}, default='lars'\n        The algorithm used:\n\n        - `'lars'`: uses the least angle regression method to solve the lasso\n          problem (`linear_model.lars_path`)\n        - `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    batch_size : int, default=256\n        Number of samples in each mini-batch.\n\n        .. versionchanged:: 1.3\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\n\n    shuffle : bool, default=True\n        Whether to shuffle the samples before forming batches.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial value of the dictionary for warm restart scenarios.\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\n            'threshold'}, default='omp'\n        Algorithm used to transform the data:\n\n        - `'lars'`: uses the least angle regression method\n          (`linear_model.lars_path`);\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\n          if the estimated components are sparse.\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution.\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\n          the projection ``dictionary * X'``.\n\n    transform_n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and\n        `algorithm='omp'`. If `None`, then\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\n\n    transform_alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `None`, defaults to `alpha`.\n\n        .. versionchanged:: 1.2\n            When None, default value changed from 1.0 to `alpha`.\n\n    verbose : bool or int, default=False\n        To control the verbosity of the procedure.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initializing the dictionary when ``dict_init`` is not\n        specified, randomly shuffling the data when ``shuffle`` is set to\n        ``True``, and updating the dictionary. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `'lasso_lars'`.\n\n        .. versionadded:: 0.22\n\n    callback : callable, default=None\n        A callable that gets invoked at the end of each iteration.\n\n        .. versionadded:: 1.1\n\n    tol : float, default=1e-3\n        Control early stopping based on the norm of the differences in the\n        dictionary between 2 steps.\n\n        To disable early stopping based on changes in the dictionary, set\n        `tol` to 0.0.\n\n        .. versionadded:: 1.1\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to None.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components extracted from the data.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of iterations over the full dataset.\n\n    n_steps_ : int\n        Number of mini-batches processed.\n\n        .. versionadded:: 1.1\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n\n    References\n    ----------\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\n    >>> X, dictionary, code = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42)\n    >>> dict_learner = MiniBatchDictionaryLearning(\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\n    ...     transform_alpha=0.1, max_iter=20, random_state=42)\n    >>> X_transformed = dict_learner.fit_transform(X)\n\n    We can check the level of sparsity of `X_transformed`:\n\n    >>> np.mean(X_transformed == 0) > 0.5\n    np.True_\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = X_transformed @ dict_learner.components_\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.052...)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), Hidden(None)],\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\n        \"n_jobs\": [None, Integral],\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"shuffle\": [\"boolean\"],\n        \"dict_init\": [None, np.ndarray],\n        \"transform_algorithm\": [\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\n        ],\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\n        \"verbose\": [\"verbose\"],\n        \"split_sign\": [\"boolean\"],\n        \"random_state\": [\"random_state\"],\n        \"positive_code\": [\"boolean\"],\n        \"positive_dict\": [\"boolean\"],\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\n        \"callback\": [None, callable],\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\n    }\n\n    def __init__(\n        self,\n        n_components=None,\n        *,\n        alpha=1,\n        max_iter=1_000,\n        fit_algorithm=\"lars\",\n        n_jobs=None,\n        batch_size=256,\n        shuffle=True,\n        dict_init=None,\n        transform_algorithm=\"omp\",\n        transform_n_nonzero_coefs=None,\n        transform_alpha=None,\n        verbose=False,\n        split_sign=False,\n        random_state=None,\n        positive_code=False,\n        positive_dict=False,\n        transform_max_iter=1000,\n        callback=None,\n        tol=1e-3,\n        max_no_improvement=10,\n    ):\n        super().__init__(\n            transform_algorithm,\n            transform_n_nonzero_coefs,\n            transform_alpha,\n            split_sign,\n            n_jobs,\n            positive_code,\n            transform_max_iter,\n        )\n        self.n_components = n_components\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.fit_algorithm = fit_algorithm\n        self.dict_init = dict_init\n        self.verbose = verbose\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.split_sign = split_sign\n        self.random_state = random_state\n        self.positive_dict = positive_dict\n        self.callback = callback\n        self.max_no_improvement = max_no_improvement\n        self.tol = tol\n\n    def _check_params(self, X):\n        # n_components\n        self._n_components = self.n_components\n        if self._n_components is None:\n            self._n_components = X.shape[1]\n\n        # fit_algorithm\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\n\n        # batch_size\n        self._batch_size = min(self.batch_size, X.shape[0])\n\n    def _initialize_dict(self, X, random_state):\n        \"\"\"Initialization of the dictionary.\"\"\"\n        if self.dict_init is not None:\n            dictionary = self.dict_init\n        else:\n            # Init V with SVD of X\n            _, S, dictionary = randomized_svd(\n                X, self._n_components, random_state=random_state\n            )\n            dictionary = S[:, np.newaxis] * dictionary\n\n        if self._n_components <= len(dictionary):\n            dictionary = dictionary[: self._n_components, :]\n        else:\n            dictionary = np.concatenate(\n                (\n                    dictionary,\n                    np.zeros(\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\n                        dtype=dictionary.dtype,\n                    ),\n                )\n            )\n\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\n        dictionary = np.require(dictionary, requirements=\"W\")\n\n        return dictionary\n\n    def _update_inner_stats(self, X, code, batch_size, step):\n        \"\"\"Update the inner stats inplace.\"\"\"\n        if step < batch_size - 1:\n            theta = (step + 1) * batch_size\n        else:\n            theta = batch_size**2 + step + 1 - batch_size\n        beta = (theta + 1 - batch_size) / (theta + 1)\n\n        self._A *= beta\n        self._A += code.T @ code / batch_size\n        self._B *= beta\n        self._B += X.T @ code / batch_size\n\n    def _minibatch_step(self, X, dictionary, random_state, step):\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\n        batch_size = X.shape[0]\n\n        # Compute code for this batch\n        code = _sparse_encode(\n            X,\n            dictionary,\n            algorithm=self._fit_algorithm,\n            alpha=self.alpha,\n            n_jobs=self.n_jobs,\n            positive=self.positive_code,\n            max_iter=self.transform_max_iter,\n            verbose=self.verbose,\n        )\n\n        batch_cost = (\n            0.5 * ((X - code @ dictionary) ** 2).sum()\n            + self.alpha * np.sum(np.abs(code))\n        ) / batch_size\n\n        # Update inner stats\n        self._update_inner_stats(X, code, batch_size, step)\n\n        # Update dictionary\n        _update_dict(\n            dictionary,\n            X,\n            code,\n            self._A,\n            self._B,\n            verbose=self.verbose,\n            random_state=random_state,\n            positive=self.positive_dict,\n        )\n\n        return batch_cost\n\n    def _check_convergence(\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\n    ):\n        \"\"\"Helper function to encapsulate the early stopping logic.\n\n        Early stopping is based on two factors:\n        - A small change of the dictionary between two minibatch updates. This is\n          controlled by the tol parameter.\n        - No more improvement on a smoothed estimate of the objective function for a\n          a certain number of consecutive minibatch updates. This is controlled by\n          the max_no_improvement parameter.\n        \"\"\"\n        batch_size = X.shape[0]\n\n        # counts steps starting from 1 for user friendly verbose mode.\n        step = step + 1\n\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\n        # too bad value\n        if step <= min(100, n_samples / batch_size):\n            if self.verbose:\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\n            return False\n\n        # Compute an Exponentially Weighted Average of the cost function to\n        # monitor the convergence while discarding minibatch-local stochastic\n        # variability: https://en.wikipedia.org/wiki/Moving_average\n        if self._ewa_cost is None:\n            self._ewa_cost = batch_cost\n        else:\n            alpha = batch_size / (n_samples + 1)\n            alpha = min(alpha, 1)\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\n\n        if self.verbose:\n            print(\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\n            )\n\n        # Early stopping based on change of dictionary\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\n        if self.tol > 0 and dict_diff <= self.tol:\n            if self.verbose:\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\n            return True\n\n        # Early stopping heuristic due to lack of improvement on smoothed\n        # cost function\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\n            self._no_improvement = 0\n            self._ewa_cost_min = self._ewa_cost\n        else:\n            self._no_improvement += 1\n\n        if (\n            self.max_no_improvement is not None\n            and self._no_improvement >= self.max_no_improvement\n        ):\n            if self.verbose:\n                print(\n                    \"Converged (lack of improvement in objective function) \"\n                    f\"at step {step}/{n_steps}\"\n                )\n            return True\n\n        return False\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = self._validate_data(\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\n        )\n\n        self._check_params(X)\n        self._random_state = check_random_state(self.random_state)\n\n        dictionary = self._initialize_dict(X, self._random_state)\n        old_dict = dictionary.copy()\n\n        if self.shuffle:\n            X_train = X.copy()\n            self._random_state.shuffle(X_train)\n        else:\n            X_train = X\n\n        n_samples, n_features = X_train.shape\n\n        if self.verbose:\n            print(\"[dict_learning]\")\n\n        # Inner stats\n        self._A = np.zeros(\n            (self._n_components, self._n_components), dtype=X_train.dtype\n        )\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\n\n        # TODO(1.6): remove in 1.6\n        if self.max_iter is None:\n            warn(\n                (\n                    \"`max_iter=None` is deprecated in version 1.4 and will be removed\"\n                    \" in version 1.6. Use the default value (i.e. `1_000`) instead.\"\n                ),\n                FutureWarning,\n            )\n            max_iter = 1_000\n        else:\n            max_iter = self.max_iter\n\n        # Attributes to monitor the convergence\n        self._ewa_cost = None\n        self._ewa_cost_min = None\n        self._no_improvement = 0\n\n        batches = gen_batches(n_samples, self._batch_size)\n        batches = itertools.cycle(batches)\n        n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\n        n_steps = max_iter * n_steps_per_iter\n\n        i = -1  # to allow max_iter = 0\n\n        for i, batch in zip(range(n_steps), batches):\n            X_batch = X_train[batch]\n\n            batch_cost = self._minibatch_step(\n                X_batch, dictionary, self._random_state, i\n            )\n\n            if self._check_convergence(\n                X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\n            ):\n                break\n\n            # XXX callback param added for backward compat in #18975 but a common\n            # unified callback API should be preferred\n            if self.callback is not None:\n                self.callback(locals())\n\n            old_dict[:] = dictionary\n\n        self.n_steps_ = i + 1\n        self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\n        self.components_ = dictionary\n\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def partial_fit(self, X, y=None):\n        \"\"\"Update the model using the data in X as a mini-batch.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Return the instance itself.\n        \"\"\"\n        has_components = hasattr(self, \"components_\")\n\n        X = self._validate_data(\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\n        )\n\n        if not has_components:\n            # This instance has not been fitted yet (fit or partial_fit)\n            self._check_params(X)\n            self._random_state = check_random_state(self.random_state)\n\n            dictionary = self._initialize_dict(X, self._random_state)\n\n            self.n_steps_ = 0\n\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\n        else:\n            dictionary = self.components_\n\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\n\n        self.components_ = dictionary\n        self.n_steps_ += 1\n\n        return self\n\n    @property\n    def _n_features_out(self):\n        \"\"\"Number of transformed output features.\"\"\"\n        return self.components_.shape[0]\n\n    def _more_tags(self):\n        return {\n            \"preserves_dtype\": [np.float64, np.float32],\n        }\n", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_factor_analysis.py", "fn_id": "", "content": "class FactorAnalysis(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Factor Analysis (FA).\n\n    A simple linear generative model with Gaussian latent variables.\n\n    The observations are assumed to be caused by a linear transformation of\n    lower dimensional latent factors and added Gaussian noise.\n    Without loss of generality the factors are distributed according to a\n    Gaussian with zero mean and unit covariance. The noise is also zero mean\n    and has an arbitrary diagonal covariance matrix.\n\n    If we would restrict the model further, by assuming that the Gaussian\n    noise is even isotropic (all diagonal entries are the same) we would obtain\n    :class:`PCA`.\n\n    FactorAnalysis performs a maximum likelihood estimate of the so-called\n    `loading` matrix, the transformation of the latent variables to the\n    observed ones, using SVD based approach.\n\n    Read more in the :ref:`User Guide <FA>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Dimensionality of latent space, the number of components\n        of ``X`` that are obtained after ``transform``.\n        If None, n_components is set to the number of features.\n\n    tol : float, default=1e-2\n        Stopping tolerance for log-likelihood increase.\n\n    copy : bool, default=True\n        Whether to make a copy of X. If ``False``, the input X gets overwritten\n        during fitting.\n\n    max_iter : int, default=1000\n        Maximum number of iterations.\n\n    noise_variance_init : array-like of shape (n_features,), default=None\n        The initial guess of the noise variance for each feature.\n        If None, it defaults to np.ones(n_features).\n\n    svd_method : {'lapack', 'randomized'}, default='randomized'\n        Which SVD method to use. If 'lapack' use standard SVD from\n        scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n        Defaults to 'randomized'. For most applications 'randomized' will\n        be sufficiently precise while providing significant speed gains.\n        Accuracy can also be improved by setting higher values for\n        `iterated_power`. If this is not sufficient, for maximum precision\n        you should choose 'lapack'.\n\n    iterated_power : int, default=3\n        Number of iterations for the power method. 3 by default. Only used\n        if ``svd_method`` equals 'randomized'.\n\n    rotation : {'varimax', 'quartimax'}, default=None\n        If not None, apply the indicated rotation. Currently, varimax and\n        quartimax are implemented. See\n        `\"The varimax criterion for analytic rotation in factor analysis\"\n        <https://link.springer.com/article/10.1007%2FBF02289233>`_\n        H. F. Kaiser, 1958.\n\n        .. versionadded:: 0.24\n\n    random_state : int or RandomState instance, default=0\n        Only used when ``svd_method`` equals 'randomized'. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components with maximum variance.\n\n    loglike_ : list of shape (n_iterations,)\n        The log likelihood at each iteration.\n\n    noise_variance_ : ndarray of shape (n_features,)\n        The estimated noise variance for each feature.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    PCA: Principal component analysis is also a latent linear variable model\n        which however assumes equal noise variance for each feature.\n        This extra assumption makes probabilistic PCA faster as it can be\n        computed in closed form.\n    FastICA: Independent component analysis, a latent variable model with\n        non-Gaussian latent variables.\n\n    References\n    ----------\n    - David Barber, Bayesian Reasoning and Machine Learning,\n      Algorithm 21.1.\n\n    - Christopher M. Bishop: Pattern Recognition and Machine Learning,\n      Chapter 12.2.4.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FactorAnalysis\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FactorAnalysis(n_components=7, random_state=0)\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_components\": [Interval(Integral, 0, None, closed=\"left\"), None],\n        \"tol\": [Interval(Real, 0.0, None, closed=\"left\")],\n        \"copy\": [\"boolean\"],\n        \"max_iter\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"noise_variance_init\": [\"array-like\", None],\n        \"svd_method\": [StrOptions({\"randomized\", \"lapack\"})],\n        \"iterated_power\": [Interval(Integral, 0, None, closed=\"left\")],\n        \"rotation\": [StrOptions({\"varimax\", \"quartimax\"}), None],\n        \"random_state\": [\"random_state\"],\n    }\n\n    def __init__(\n        self,\n        n_components=None,\n        *,\n        tol=1e-2,\n        copy=True,\n        max_iter=1000,\n        noise_variance_init=None,\n        svd_method=\"randomized\",\n        iterated_power=3,\n        rotation=None,\n        random_state=0,\n    ):\n        self.n_components = n_components\n        self.copy = copy\n        self.tol = tol\n        self.max_iter = max_iter\n        self.svd_method = svd_method\n\n        self.noise_variance_init = noise_variance_init\n        self.iterated_power = iterated_power\n        self.random_state = random_state\n        self.rotation = rotation\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the FactorAnalysis model to X using SVD based approach.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Ignored parameter.\n\n        Returns\n        -------\n        self : object\n            FactorAnalysis class instance.\n        \"\"\"\n        X = self._validate_data(\n            X, copy=self.copy, dtype=np.float64, force_writeable=True\n        )\n\n        n_samples, n_features = X.shape\n        n_components = self.n_components\n        if n_components is None:\n            n_components = n_features\n\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n\n        # some constant terms\n        nsqrt = sqrt(n_samples)\n        llconst = n_features * log(2.0 * np.pi) + n_components\n        var = np.var(X, axis=0)\n\n        if self.noise_variance_init is None:\n            psi = np.ones(n_features, dtype=X.dtype)\n        else:\n            if len(self.noise_variance_init) != n_features:\n                raise ValueError(\n                    \"noise_variance_init dimension does not \"\n                    \"with number of features : %d != %d\"\n                    % (len(self.noise_variance_init), n_features)\n                )\n            psi = np.array(self.noise_variance_init)\n\n        loglike = []\n        old_ll = -np.inf\n        SMALL = 1e-12\n\n        # we'll modify svd outputs to return unexplained variance\n        # to allow for unified computation of loglikelihood\n        if self.svd_method == \"lapack\":\n\n            def my_svd(X):\n                _, s, Vt = linalg.svd(X, full_matrices=False, check_finite=False)\n                return (\n                    s[:n_components],\n                    Vt[:n_components],\n                    squared_norm(s[n_components:]),\n                )\n\n        else:  # svd_method == \"randomized\"\n            random_state = check_random_state(self.random_state)\n\n            def my_svd(X):\n                _, s, Vt = randomized_svd(\n                    X,\n                    n_components,\n                    random_state=random_state,\n                    n_iter=self.iterated_power,\n                )\n                return s, Vt, squared_norm(X) - squared_norm(s)\n\n        for i in range(self.max_iter):\n            # SMALL helps numerics\n            sqrt_psi = np.sqrt(psi) + SMALL\n            s, Vt, unexp_var = my_svd(X / (sqrt_psi * nsqrt))\n            s **= 2\n            # Use 'maximum' here to avoid sqrt problems.\n            W = np.sqrt(np.maximum(s - 1.0, 0.0))[:, np.newaxis] * Vt\n            del Vt\n            W *= sqrt_psi\n\n            # loglikelihood\n            ll = llconst + np.sum(np.log(s))\n            ll += unexp_var + np.sum(np.log(psi))\n            ll *= -n_samples / 2.0\n            loglike.append(ll)\n            if (ll - old_ll) < self.tol:\n                break\n            old_ll = ll\n\n            psi = np.maximum(var - np.sum(W**2, axis=0), SMALL)\n        else:\n            warnings.warn(\n                \"FactorAnalysis did not converge.\"\n                + \" You might want\"\n                + \" to increase the number of iterations.\",\n                ConvergenceWarning,\n            )\n\n        self.components_ = W\n        if self.rotation is not None:\n            self.components_ = self._rotate(W)\n        self.noise_variance_ = psi\n        self.loglike_ = loglike\n        self.n_iter_ = i + 1\n        return self\n\n    def transform(self, X):\n        \"\"\"Apply dimensionality reduction to X using the model.\n\n        Compute the expected mean of the latent variables.\n        See Barber, 21.2.33 (or Bishop, 12.66).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            The latent variables of X.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_data(X, reset=False)\n        Ih = np.eye(len(self.components_))\n\n        X_transformed = X - self.mean_\n\n        Wpsi = self.components_ / self.noise_variance_\n        cov_z = linalg.inv(Ih + np.dot(Wpsi, self.components_.T))\n        tmp = np.dot(X_transformed, Wpsi.T)\n        X_transformed = np.dot(tmp, cov_z)\n\n        return X_transformed\n\n    def get_covariance(self):\n        \"\"\"Compute data covariance with the FactorAnalysis model.\n\n        ``cov = components_.T * components_ + diag(noise_variance)``\n\n        Returns\n        -------\n        cov : ndarray of shape (n_features, n_features)\n            Estimated covariance of data.\n        \"\"\"\n        check_is_fitted(self)\n\n        cov = np.dot(self.components_.T, self.components_)\n        cov.flat[:: len(cov) + 1] += self.noise_variance_  # modify diag inplace\n        return cov\n\n    def get_precision(self):\n        \"\"\"Compute data precision matrix with the FactorAnalysis model.\n\n        Returns\n        -------\n        precision : ndarray of shape (n_features, n_features)\n            Estimated precision of data.\n        \"\"\"\n        check_is_fitted(self)\n\n        n_features = self.components_.shape[1]\n\n        # handle corner cases first\n        if self.n_components == 0:\n            return np.diag(1.0 / self.noise_variance_)\n        if self.n_components == n_features:\n            return linalg.inv(self.get_covariance())\n\n        # Get precision using matrix inversion lemma\n        components_ = self.components_\n        precision = np.dot(components_ / self.noise_variance_, components_.T)\n        precision.flat[:: len(precision) + 1] += 1.0\n        precision = np.dot(components_.T, np.dot(linalg.inv(precision), components_))\n        precision /= self.noise_variance_[:, np.newaxis]\n        precision /= -self.noise_variance_[np.newaxis, :]\n        precision.flat[:: len(precision) + 1] += 1.0 / self.noise_variance_\n        return precision\n\n    def score_samples(self, X):\n        \"\"\"Compute the log-likelihood of each sample.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        ll : ndarray of shape (n_samples,)\n            Log-likelihood of each sample under the current model.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        Xr = X - self.mean_\n        precision = self.get_precision()\n        n_features = X.shape[1]\n        log_like = -0.5 * (Xr * (np.dot(Xr, precision))).sum(axis=1)\n        log_like -= 0.5 * (n_features * log(2.0 * np.pi) - fast_logdet(precision))\n        return log_like\n\n    def score(self, X, y=None):\n        \"\"\"Compute the average log-likelihood of the samples.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data.\n\n        y : Ignored\n            Ignored parameter.\n\n        Returns\n        -------\n        ll : float\n            Average log-likelihood of the samples under the current model.\n        \"\"\"\n        return np.mean(self.score_samples(X))\n\n    def _rotate(self, components, n_components=None, tol=1e-6):\n        \"Rotate the factor analysis solution.\"\n        # note that tol is not exposed\n        return _ortho_rotation(components.T, method=self.rotation, tol=tol)[\n            : self.n_components\n        ]\n\n    @property\n    def _n_features_out(self):\n        \"\"\"Number of transformed output features.\"\"\"\n        return self.components_.shape[0]\n", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._factor_analysis/FactorAnalysis", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_kernel_pca.py", "fn_id": "", "content": "class KernelPCA(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Kernel Principal component analysis (KPCA).\n\n    Non-linear dimensionality reduction through the use of kernels [1]_, see also\n    :ref:`metrics`.\n\n    It uses the :func:`scipy.linalg.eigh` LAPACK implementation of the full SVD\n    or the :func:`scipy.sparse.linalg.eigsh` ARPACK implementation of the\n    truncated SVD, depending on the shape of the input data and the number of\n    components to extract. It can also use a randomized truncated SVD by the\n    method proposed in [3]_, see `eigen_solver`.\n\n    For a usage example and comparison between\n    Principal Components Analysis (PCA) and its kernelized version (KPCA), see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_kernel_pca.py`.\n\n    For a usage example in denoising images using KPCA, see\n    :ref:`sphx_glr_auto_examples_applications_plot_digits_denoising.py`.\n\n    Read more in the :ref:`User Guide <kernel_PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components. If None, all non-zero components are kept.\n\n    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed'} \\\n            or callable, default='linear'\n        Kernel used for PCA.\n\n    gamma : float, default=None\n        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n        kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.\n\n    degree : float, default=3\n        Degree for poly kernels. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Independent term in poly and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : dict, default=None\n        Parameters (keyword arguments) and\n        values for kernel passed as callable object.\n        Ignored by other kernels.\n\n    alpha : float, default=1.0\n        Hyperparameter of the ridge regression that learns the\n        inverse transform (when fit_inverse_transform=True).\n\n    fit_inverse_transform : bool, default=False\n        Learn the inverse transform for non-precomputed kernels\n        (i.e. learn to find the pre-image of a point). This method is based\n        on [2]_.\n\n    eigen_solver : {'auto', 'dense', 'arpack', 'randomized'}, \\\n            default='auto'\n        Select eigensolver to use. If `n_components` is much\n        less than the number of training samples, randomized (or arpack to a\n        smaller extent) may be more efficient than the dense eigensolver.\n        Randomized SVD is performed according to the method of Halko et al\n        [3]_.\n\n        auto :\n            the solver is selected by a default policy based on n_samples\n            (the number of training samples) and `n_components`:\n            if the number of components to extract is less than 10 (strict) and\n            the number of samples is more than 200 (strict), the 'arpack'\n            method is enabled. Otherwise the exact full eigenvalue\n            decomposition is computed and optionally truncated afterwards\n            ('dense' method).\n        dense :\n            run exact full eigenvalue decomposition calling the standard\n            LAPACK solver via `scipy.linalg.eigh`, and select the components\n            by postprocessing\n        arpack :\n            run SVD truncated to n_components calling ARPACK solver using\n            `scipy.sparse.linalg.eigsh`. It requires strictly\n            0 < n_components < n_samples\n        randomized :\n            run randomized SVD by the method of Halko et al. [3]_. The current\n            implementation selects eigenvalues based on their module; therefore\n            using this method can lead to unexpected results if the kernel is\n            not positive semi-definite. See also [4]_.\n\n        .. versionchanged:: 1.0\n           `'randomized'` was added.\n\n    tol : float, default=0\n        Convergence tolerance for arpack.\n        If 0, optimal value will be chosen by arpack.\n\n    max_iter : int, default=None\n        Maximum number of iterations for arpack.\n        If None, optimal value will be chosen by arpack.\n\n    iterated_power : int >= 0, or 'auto', default='auto'\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'. When 'auto', it is set to 7 when\n        `n_components < 0.1 * min(X.shape)`, other it is set to 4.\n\n        .. versionadded:: 1.0\n\n    remove_zero_eig : bool, default=False\n        If True, then all components with zero eigenvalues are removed, so\n        that the number of components in the output may be < n_components\n        (and sometimes even zero due to numerical instability).\n        When n_components is None, this parameter is ignored and components\n        with zero eigenvalues are removed regardless.\n\n    random_state : int, RandomState instance or None, default=None\n        Used when ``eigen_solver`` == 'arpack' or 'randomized'. Pass an int\n        for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.18\n\n    copy_X : bool, default=True\n        If True, input X is copied and stored by the model in the `X_fit_`\n        attribute. If no further changes will be done to X, setting\n        `copy_X=False` saves memory by storing a reference.\n\n        .. versionadded:: 0.18\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    Attributes\n    ----------\n    eigenvalues_ : ndarray of shape (n_components,)\n        Eigenvalues of the centered kernel matrix in decreasing order.\n        If `n_components` and `remove_zero_eig` are not set,\n        then all values are stored.\n\n    eigenvectors_ : ndarray of shape (n_samples, n_components)\n        Eigenvectors of the centered kernel matrix. If `n_components` and\n        `remove_zero_eig` are not set, then all components are stored.\n\n    dual_coef_ : ndarray of shape (n_samples, n_features)\n        Inverse transform matrix. Only available when\n        ``fit_inverse_transform`` is True.\n\n    X_transformed_fit_ : ndarray of shape (n_samples, n_components)\n        Projection of the fitted data on the kernel principal components.\n        Only available when ``fit_inverse_transform`` is True.\n\n    X_fit_ : ndarray of shape (n_samples, n_features)\n        The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n        a reference. This attribute is used for the calls to transform.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    gamma_ : float\n        Kernel coefficient for rbf, poly and sigmoid kernels. When `gamma`\n        is explicitly provided, this is just the same as `gamma`. When `gamma`\n        is `None`, this is the actual value of kernel coefficient.\n\n        .. versionadded:: 1.3\n\n    See Also\n    --------\n    FastICA : A fast algorithm for Independent Component Analysis.\n    IncrementalPCA : Incremental Principal Component Analysis.\n    NMF : Non-Negative Matrix Factorization.\n    PCA : Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] `Sch\u00f6lkopf, Bernhard, Alexander Smola, and Klaus-Robert M\u00fcller.\n       \"Kernel principal component analysis.\"\n       International conference on artificial neural networks.\n       Springer, Berlin, Heidelberg, 1997.\n       <https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf>`_\n\n    .. [2] `Bak\u0131r, G\u00f6khan H., Jason Weston, and Bernhard Sch\u00f6lkopf.\n       \"Learning to find pre-images.\"\n       Advances in neural information processing systems 16 (2004): 449-456.\n       <https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_\n\n    .. [3] :arxiv:`Halko, Nathan, Per-Gunnar Martinsson, and Joel A. Tropp.\n       \"Finding structure with randomness: Probabilistic algorithms for\n       constructing approximate matrix decompositions.\"\n       SIAM review 53.2 (2011): 217-288. <0909.4061>`\n\n    .. [4] `Martinsson, Per-Gunnar, Vladimir Rokhlin, and Mark Tygert.\n       \"A randomized algorithm for the decomposition of matrices.\"\n       Applied and Computational Harmonic Analysis 30.1 (2011): 47-68.\n       <https://www.sciencedirect.com/science/article/pii/S1063520310000242>`_\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import KernelPCA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = KernelPCA(n_components=7, kernel='linear')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_components\": [\n            Interval(Integral, 1, None, closed=\"left\"),\n            None,\n        ],\n        \"kernel\": [\n            StrOptions({\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"cosine\", \"precomputed\"}),\n            callable,\n        ],\n        \"gamma\": [\n            Interval(Real, 0, None, closed=\"left\"),\n            None,\n        ],\n        \"degree\": [Interval(Real, 0, None, closed=\"left\")],\n        \"coef0\": [Interval(Real, None, None, closed=\"neither\")],\n        \"kernel_params\": [dict, None],\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\n        \"fit_inverse_transform\": [\"boolean\"],\n        \"eigen_solver\": [StrOptions({\"auto\", \"dense\", \"arpack\", \"randomized\"})],\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n        \"max_iter\": [\n            Interval(Integral, 1, None, closed=\"left\"),\n            None,\n        ],\n        \"iterated_power\": [\n            Interval(Integral, 0, None, closed=\"left\"),\n            StrOptions({\"auto\"}),\n        ],\n        \"remove_zero_eig\": [\"boolean\"],\n        \"random_state\": [\"random_state\"],\n        \"copy_X\": [\"boolean\"],\n        \"n_jobs\": [None, Integral],\n    }\n\n    def __init__(\n        self,\n        n_components=None,\n        *,\n        kernel=\"linear\",\n        gamma=None,\n        degree=3,\n        coef0=1,\n        kernel_params=None,\n        alpha=1.0,\n        fit_inverse_transform=False,\n        eigen_solver=\"auto\",\n        tol=0,\n        max_iter=None,\n        iterated_power=\"auto\",\n        remove_zero_eig=False,\n        random_state=None,\n        copy_X=True,\n        n_jobs=None,\n    ):\n        self.n_components = n_components\n        self.kernel = kernel\n        self.kernel_params = kernel_params\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.alpha = alpha\n        self.fit_inverse_transform = fit_inverse_transform\n        self.eigen_solver = eigen_solver\n        self.tol = tol\n        self.max_iter = max_iter\n        self.iterated_power = iterated_power\n        self.remove_zero_eig = remove_zero_eig\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n        self.copy_X = copy_X\n\n    def _get_kernel(self, X, Y=None):\n        if callable(self.kernel):\n            params = self.kernel_params or {}\n        else:\n            params = {\"gamma\": self.gamma_, \"degree\": self.degree, \"coef0\": self.coef0}\n        return pairwise_kernels(\n            X, Y, metric=self.kernel, filter_params=True, n_jobs=self.n_jobs, **params\n        )\n\n    def _fit_transform(self, K):\n        \"\"\"Fit's using kernel K\"\"\"\n        # center kernel\n        K = self._centerer.fit_transform(K)\n\n        # adjust n_components according to user inputs\n        if self.n_components is None:\n            n_components = K.shape[0]  # use all dimensions\n        else:\n            n_components = min(K.shape[0], self.n_components)\n\n        # compute eigenvectors\n        if self.eigen_solver == \"auto\":\n            if K.shape[0] > 200 and n_components < 10:\n                eigen_solver = \"arpack\"\n            else:\n                eigen_solver = \"dense\"\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == \"dense\":\n            # Note: subset_by_index specifies the indices of smallest/largest to return\n            self.eigenvalues_, self.eigenvectors_ = eigh(\n                K, subset_by_index=(K.shape[0] - n_components, K.shape[0] - 1)\n            )\n        elif eigen_solver == \"arpack\":\n            v0 = _init_arpack_v0(K.shape[0], self.random_state)\n            self.eigenvalues_, self.eigenvectors_ = eigsh(\n                K, n_components, which=\"LA\", tol=self.tol, maxiter=self.max_iter, v0=v0\n            )\n        elif eigen_solver == \"randomized\":\n            self.eigenvalues_, self.eigenvectors_ = _randomized_eigsh(\n                K,\n                n_components=n_components,\n                n_iter=self.iterated_power,\n                random_state=self.random_state,\n                selection=\"module\",\n            )\n\n        # make sure that the eigenvalues are ok and fix numerical issues\n        self.eigenvalues_ = _check_psd_eigenvalues(\n            self.eigenvalues_, enable_warnings=False\n        )\n\n        # flip eigenvectors' sign to enforce deterministic output\n        self.eigenvectors_, _ = svd_flip(u=self.eigenvectors_, v=None)\n\n        # sort eigenvectors in descending order\n        indices = self.eigenvalues_.argsort()[::-1]\n        self.eigenvalues_ = self.eigenvalues_[indices]\n        self.eigenvectors_ = self.eigenvectors_[:, indices]\n\n        # remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is None:\n            self.eigenvectors_ = self.eigenvectors_[:, self.eigenvalues_ > 0]\n            self.eigenvalues_ = self.eigenvalues_[self.eigenvalues_ > 0]\n\n        # Maintenance note on Eigenvectors normalization\n        # ----------------------------------------------\n        # there is a link between\n        # the eigenvectors of K=Phi(X)'Phi(X) and the ones of Phi(X)Phi(X)'\n        # if v is an eigenvector of K\n        #     then Phi(X)v  is an eigenvector of Phi(X)Phi(X)'\n        # if u is an eigenvector of Phi(X)Phi(X)'\n        #     then Phi(X)'u is an eigenvector of Phi(X)'Phi(X)\n        #\n        # At this stage our self.eigenvectors_ (the v) have norm 1, we need to scale\n        # them so that eigenvectors in kernel feature space (the u) have norm=1\n        # instead\n        #\n        # We COULD scale them here:\n        #       self.eigenvectors_ = self.eigenvectors_ / np.sqrt(self.eigenvalues_)\n        #\n        # But choose to perform that LATER when needed, in `fit()` and in\n        # `transform()`.\n\n        return K\n\n    def _fit_inverse_transform(self, X_transformed, X):\n        if hasattr(X, \"tocsr\"):\n            raise NotImplementedError(\n                \"Inverse transform not implemented for sparse matrices!\"\n            )\n\n        n_samples = X_transformed.shape[0]\n        K = self._get_kernel(X_transformed)\n        K.flat[:: n_samples + 1] += self.alpha\n        self.dual_coef_ = linalg.solve(K, X, assume_a=\"pos\", overwrite_a=True)\n        self.X_transformed_fit_ = X_transformed\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        if self.fit_inverse_transform and self.kernel == \"precomputed\":\n            raise ValueError(\"Cannot fit_inverse_transform with a precomputed kernel.\")\n        X = self._validate_data(X, accept_sparse=\"csr\", copy=self.copy_X)\n        self.gamma_ = 1 / X.shape[1] if self.gamma is None else self.gamma\n        self._centerer = KernelCenterer().set_output(transform=\"default\")\n        K = self._get_kernel(X)\n        self._fit_transform(K)\n\n        if self.fit_inverse_transform:\n            # no need to use the kernel to transform X, use shortcut expression\n            X_transformed = self.eigenvectors_ * np.sqrt(self.eigenvalues_)\n\n            self._fit_inverse_transform(X_transformed, X)\n\n        self.X_fit_ = X\n        return self\n\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **params : kwargs\n            Parameters (keyword arguments) and values passed to\n            the fit_transform instance.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Returns the instance itself.\n        \"\"\"\n        self.fit(X, **params)\n\n        # no need to use the kernel to transform X, use shortcut expression\n        X_transformed = self.eigenvectors_ * np.sqrt(self.eigenvalues_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n\n    def transform(self, X):\n        \"\"\"Transform X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Returns the instance itself.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n\n        # Compute centered gram matrix between X and training data X_fit_\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n\n        # scale eigenvectors (properly account for null-space for dot product)\n        non_zeros = np.flatnonzero(self.eigenvalues_)\n        scaled_alphas = np.zeros_like(self.eigenvectors_)\n        scaled_alphas[:, non_zeros] = self.eigenvectors_[:, non_zeros] / np.sqrt(\n            self.eigenvalues_[non_zeros]\n        )\n\n        # Project with a scalar product between K and the scaled eigenvectors\n        return np.dot(K, scaled_alphas)\n\n    def inverse_transform(self, X):\n        \"\"\"Transform X back to original space.\n\n        ``inverse_transform`` approximates the inverse transformation using\n        a learned pre-image. The pre-image is learned by kernel ridge\n        regression of the original data on their low-dimensional representation\n        vectors.\n\n        .. note:\n            :meth:`~sklearn.decomposition.fit` internally uses a centered\n            kernel. As the centered kernel no longer contains the information\n            of the mean of kernel features, such information is not taken into\n            account in reconstruction.\n\n        .. note::\n            When users want to compute inverse transformation for 'linear'\n            kernel, it is recommended that they use\n            :class:`~sklearn.decomposition.PCA` instead. Unlike\n            :class:`~sklearn.decomposition.PCA`,\n            :class:`~sklearn.decomposition.KernelPCA`'s ``inverse_transform``\n            does not reconstruct the mean of data when 'linear' kernel is used\n            due to the use of centered kernel.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_components)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_features)\n            Returns the instance itself.\n\n        References\n        ----------\n        `Bak\u0131r, G\u00f6khan H., Jason Weston, and Bernhard Sch\u00f6lkopf.\n        \"Learning to find pre-images.\"\n        Advances in neural information processing systems 16 (2004): 449-456.\n        <https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_\n        \"\"\"\n        if not self.fit_inverse_transform:\n            raise NotFittedError(\n                \"The fit_inverse_transform parameter was not\"\n                \" set to True when instantiating and hence \"\n                \"the inverse transform is not available.\"\n            )\n\n        K = self._get_kernel(X, self.X_transformed_fit_)\n        return np.dot(K, self.dual_coef_)\n\n    def _more_tags(self):\n        return {\n            \"preserves_dtype\": [np.float64, np.float32],\n            \"pairwise\": self.kernel == \"precomputed\",\n        }\n\n    @property\n    def _n_features_out(self):\n        \"\"\"Number of transformed output features.\"\"\"\n        return self.eigenvalues_.shape[0]\n", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._kernel_pca/KernelPCA", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_nmf.py", "fn_id": "", "content": "class NMF(_BaseNMF):\n    \"\"\"Non-Negative Matrix Factorization (NMF).\n\n    Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)\n    whose product approximates the non-negative matrix X. This factorization can be used\n    for example for dimensionality reduction, source separation or topic extraction.\n\n    The objective function is:\n\n        .. math::\n\n            L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n            &+ alpha\\\\_W * l1\\\\_ratio * n\\\\_features * ||vec(W)||_1\n\n            &+ alpha\\\\_H * l1\\\\_ratio * n\\\\_samples * ||vec(H)||_1\n\n            &+ 0.5 * alpha\\\\_W * (1 - l1\\\\_ratio) * n\\\\_features * ||W||_{Fro}^2\n\n            &+ 0.5 * alpha\\\\_H * (1 - l1\\\\_ratio) * n\\\\_samples * ||H||_{Fro}^2\n\n    Where:\n\n    :math:`||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n    :math:`||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\n    The generic norm :math:`||X - WH||_{loss}` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for\n    `H` to keep their impact balanced with respect to one another and to the data fit\n    term as independent as possible of the size `n_samples` of the training set.\n\n    The objective function is minimized with an alternating minimization of W\n    and H.\n\n    Note that the transformed data is named W and the components matrix is named H. In\n    the NMF literature, the naming convention is usually the opposite since the data\n    matrix X is transposed.\n\n    Read more in the :ref:`User Guide <NMF>`.\n\n    Parameters\n    ----------\n    n_components : int or {'auto'} or None, default=None\n        Number of components, if n_components is not set all features\n        are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from W or H shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Valid options:\n\n        - `None`: 'nndsvda' if n_components <= min(n_samples, n_features),\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness)\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired)\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired)\n\n        - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n        .. versionchanged:: 1.1\n            When `init=None` and n_components is less than n_samples and n_features\n            defaults to `nndsvda` instead of `nndsvd`.\n\n    solver : {'cd', 'mu'}, default='cd'\n        Numerical solver to use:\n\n        - 'cd' is a Coordinate Descent solver.\n        - 'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler', \\\n            'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n        .. versionadded:: 1.0\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n        .. versionadded:: 1.0\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n        .. versionadded:: 0.17\n           Regularization parameter *l1_ratio* used in the Coordinate Descent\n           solver.\n\n    verbose : int, default=0\n        Whether to be verbose.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n        .. versionadded:: 0.17\n           *shuffle* parameter used in the Coordinate Descent solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data ``X`` and the reconstructed data ``WH`` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of iterations.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    PCA : Principal component analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BaseNMF._parameter_constraints,\n        \"solver\": [StrOptions({\"mu\", \"cd\"})],\n        \"shuffle\": [\"boolean\"],\n    }\n\n    def __init__(\n        self,\n        n_components=\"warn\",\n        *,\n        init=None,\n        solver=\"cd\",\n        beta_loss=\"frobenius\",\n        tol=1e-4,\n        max_iter=200,\n        random_state=None,\n        alpha_W=0.0,\n        alpha_H=\"same\",\n        l1_ratio=0.0,\n        verbose=0,\n        shuffle=False,\n    ):\n        super().__init__(\n            n_components=n_components,\n            init=init,\n            beta_loss=beta_loss,\n            tol=tol,\n            max_iter=max_iter,\n            random_state=random_state,\n            alpha_W=alpha_W,\n            alpha_H=alpha_H,\n            l1_ratio=l1_ratio,\n            verbose=verbose,\n        )\n\n        self.solver = solver\n        self.shuffle = shuffle\n\n    def _check_params(self, X):\n        super()._check_params(X)\n\n        # solver\n        if self.solver != \"mu\" and self.beta_loss not in (2, \"frobenius\"):\n            # 'mu' is the only solver that handles other beta losses than 'frobenius'\n            raise ValueError(\n                f\"Invalid beta_loss parameter: solver {self.solver!r} does not handle \"\n                f\"beta_loss = {self.beta_loss!r}\"\n            )\n        if self.solver == \"mu\" and self.init == \"nndsvd\":\n            warnings.warn(\n                (\n                    \"The multiplicative update ('mu') solver cannot update \"\n                    \"zeros present in the initialization, and so leads to \"\n                    \"poorer results when used jointly with init='nndsvd'. \"\n                    \"You may try init='nndsvda' or init='nndsvdar' instead.\"\n                ),\n                UserWarning,\n            )\n\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit_transform(self, X, y=None, W=None, H=None):\n        \"\"\"Learn a NMF model for the data X and returns the transformed data.\n\n        This is more efficient than calling fit followed by transform.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        W : array-like of shape (n_samples, n_components), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `None`, uses the initialisation method specified in `init`.\n\n        H : array-like of shape (n_components, n_features), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `None`, uses the initialisation method specified in `init`.\n\n        Returns\n        -------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        X = self._validate_data(\n            X, accept_sparse=(\"csr\", \"csc\"), dtype=[np.float64, np.float32]\n        )\n\n        with config_context(assume_finite=True):\n            W, H, n_iter = self._fit_transform(X, W=W, H=H)\n\n        self.reconstruction_err_ = _beta_divergence(\n            X, W, H, self._beta_loss, square_root=True\n        )\n\n        self.n_components_ = H.shape[0]\n        self.components_ = H\n        self.n_iter_ = n_iter\n\n        return W\n\n    def _fit_transform(self, X, y=None, W=None, H=None, update_H=True):\n        \"\"\"Learn a NMF model for the data X and returns the transformed data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data matrix to be decomposed\n\n        y : Ignored\n\n        W : array-like of shape (n_samples, n_components), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `update_H=False`, it is initialised as an array of zeros, unless\n            `solver='mu'`, then it is filled with values calculated by\n            `np.sqrt(X.mean() / self._n_components)`.\n            If `None`, uses the initialisation method specified in `init`.\n\n        H : array-like of shape (n_components, n_features), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `update_H=False`, it is used as a constant, to solve for W only.\n            If `None`, uses the initialisation method specified in `init`.\n\n        update_H : bool, default=True\n            If True, both W and H will be estimated from initial guesses,\n            this corresponds to a call to the 'fit_transform' method.\n            If False, only W will be estimated, this corresponds to a call\n            to the 'transform' method.\n\n        Returns\n        -------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.\n\n        H : ndarray of shape (n_components, n_features)\n            Factorization matrix, sometimes called 'dictionary'.\n\n        n_iter_ : int\n            Actual number of iterations.\n        \"\"\"\n        check_non_negative(X, \"NMF (input X)\")\n\n        # check parameters\n        self._check_params(X)\n\n        if X.min() == 0 and self._beta_loss <= 0:\n            raise ValueError(\n                \"When beta_loss <= 0 and X contains zeros, \"\n                \"the solver may diverge. Please add small values \"\n                \"to X, or use a positive beta_loss.\"\n            )\n\n        # initialize or check W and H\n        W, H = self._check_w_h(X, W, H, update_H)\n\n        # scale the regularization terms\n        l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H = self._compute_regularization(X)\n\n        if self.solver == \"cd\":\n            W, H, n_iter = _fit_coordinate_descent(\n                X,\n                W,\n                H,\n                self.tol,\n                self.max_iter,\n                l1_reg_W,\n                l1_reg_H,\n                l2_reg_W,\n                l2_reg_H,\n                update_H=update_H,\n                verbose=self.verbose,\n                shuffle=self.shuffle,\n                random_state=self.random_state,\n            )\n        elif self.solver == \"mu\":\n            W, H, n_iter, *_ = _fit_multiplicative_update(\n                X,\n                W,\n                H,\n                self._beta_loss,\n                self.max_iter,\n                self.tol,\n                l1_reg_W,\n                l1_reg_H,\n                l2_reg_W,\n                l2_reg_H,\n                update_H,\n                self.verbose,\n            )\n        else:\n            raise ValueError(\"Invalid solver parameter '%s'.\" % self.solver)\n\n        if n_iter == self.max_iter and self.tol > 0:\n            warnings.warn(\n                \"Maximum number of iterations %d reached. Increase \"\n                \"it to improve convergence.\" % self.max_iter,\n                ConvergenceWarning,\n            )\n\n        return W, H, n_iter\n\n    def transform(self, X):\n        \"\"\"Transform the data X according to the fitted NMF model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        Returns\n        -------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(\n            X, accept_sparse=(\"csr\", \"csc\"), dtype=[np.float64, np.float32], reset=False\n        )\n\n        with config_context(assume_finite=True):\n            W, *_ = self._fit_transform(X, H=self.components_, update_H=False)\n\n        return W\n", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._nmf/NMF", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_sparse_pca.py", "fn_id": "", "content": "class MiniBatchSparsePCA(_BaseSparsePCA):\n    \"\"\"Mini-batch Sparse Principal Components Analysis.\n\n    Finds the set of sparse components that can optimally reconstruct\n    the data.  The amount of sparseness is controllable by the coefficient\n    of the L1 penalty, given by the parameter alpha.\n\n    For an example comparing sparse PCA to PCA, see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`\n\n    Read more in the :ref:`User Guide <SparsePCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of sparse atoms to extract. If None, then ``n_components``\n        is set to ``n_features``.\n\n    alpha : int, default=1\n        Sparsity controlling parameter. Higher values lead to sparser\n        components.\n\n    ridge_alpha : float, default=0.01\n        Amount of ridge shrinkage to apply in order to improve\n        conditioning when calling the transform method.\n\n    max_iter : int, default=1_000\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n        .. versionadded:: 1.2\n\n        .. deprecated:: 1.4\n           `max_iter=None` is deprecated in 1.4 and will be removed in 1.6.\n           Use the default value (i.e. `100`) instead.\n\n    callback : callable, default=None\n        Callable that gets invoked every five iterations.\n\n    batch_size : int, default=3\n        The number of features to take in each mini batch.\n\n    verbose : int or bool, default=False\n        Controls the verbosity; the higher, the more messages. Defaults to 0.\n\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting it in batches.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    method : {'lars', 'cd'}, default='lars'\n        Method to be used for optimization.\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for random shuffling when ``shuffle`` is set to ``True``,\n        during online dictionary learning. Pass an int for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=1e-3\n        Control early stopping based on the norm of the differences in the\n        dictionary between 2 steps.\n\n        To disable early stopping based on changes in the dictionary, set\n        `tol` to 0.0.\n\n        .. versionadded:: 1.1\n\n    max_no_improvement : int or None, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to `None`.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Sparse components extracted from the data.\n\n    n_components_ : int\n        Estimated number of components.\n\n        .. versionadded:: 0.23\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n        Equal to ``X.mean(axis=0)``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    IncrementalPCA : Incremental principal components analysis.\n    PCA : Principal component analysis.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.decomposition import MiniBatchSparsePCA\n    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n    >>> transformer = MiniBatchSparsePCA(n_components=5, batch_size=50,\n    ...                                  max_iter=10, random_state=0)\n    >>> transformer.fit(X)\n    MiniBatchSparsePCA(...)\n    >>> X_transformed = transformer.transform(X)\n    >>> X_transformed.shape\n    (200, 5)\n    >>> # most values in the components_ are zero (sparsity)\n    >>> np.mean(transformer.components_ == 0)\n    np.float64(0.9...)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BaseSparsePCA._parameter_constraints,\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), Hidden(None)],\n        \"callback\": [None, callable],\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"shuffle\": [\"boolean\"],\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\n    }\n\n    def __init__(\n        self,\n        n_components=None,\n        *,\n        alpha=1,\n        ridge_alpha=0.01,\n        max_iter=1_000,\n        callback=None,\n        batch_size=3,\n        verbose=False,\n        shuffle=True,\n        n_jobs=None,\n        method=\"lars\",\n        random_state=None,\n        tol=1e-3,\n        max_no_improvement=10,\n    ):\n        super().__init__(\n            n_components=n_components,\n            alpha=alpha,\n            ridge_alpha=ridge_alpha,\n            max_iter=max_iter,\n            tol=tol,\n            method=method,\n            n_jobs=n_jobs,\n            verbose=verbose,\n            random_state=random_state,\n        )\n        self.callback = callback\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.max_no_improvement = max_no_improvement\n\n    def _fit(self, X, n_components, random_state):\n        \"\"\"Specialized `fit` for MiniBatchSparsePCA.\"\"\"\n\n        transform_algorithm = \"lasso_\" + self.method\n        est = MiniBatchDictionaryLearning(\n            n_components=n_components,\n            alpha=self.alpha,\n            max_iter=self.max_iter,\n            dict_init=None,\n            batch_size=self.batch_size,\n            shuffle=self.shuffle,\n            n_jobs=self.n_jobs,\n            fit_algorithm=self.method,\n            random_state=random_state,\n            transform_algorithm=transform_algorithm,\n            transform_alpha=self.alpha,\n            verbose=self.verbose,\n            callback=self.callback,\n            tol=self.tol,\n            max_no_improvement=self.max_no_improvement,\n        )\n        est.set_output(transform=\"default\")\n        est.fit(X.T)\n\n        self.components_, self.n_iter_ = est.transform(X.T).T, est.n_iter_\n\n        components_norm = np.linalg.norm(self.components_, axis=1)[:, np.newaxis]\n        components_norm[components_norm == 0] = 1\n        self.components_ /= components_norm\n        self.n_components_ = len(self.components_)\n\n        return self\n", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_truncated_svd.py", "fn_id": "", "content": "class TruncatedSVD(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Dimensionality reduction using truncated SVD (aka LSA).\n\n    This transformer performs linear dimensionality reduction by means of\n    truncated singular value decomposition (SVD). Contrary to PCA, this\n    estimator does not center the data before computing the singular value\n    decomposition. This means it can work with sparse matrices\n    efficiently.\n\n    In particular, truncated SVD works on term count/tf-idf matrices as\n    returned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In\n    that context, it is known as latent semantic analysis (LSA).\n\n    This estimator supports two algorithms: a fast randomized SVD solver, and\n    a \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or\n    `X.T * X`, whichever is more efficient.\n\n    Read more in the :ref:`User Guide <LSA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Desired dimensionality of output data.\n        If algorithm='arpack', must be strictly less than the number of features.\n        If algorithm='randomized', must be less than or equal to the number of features.\n        The default value is useful for visualisation. For LSA, a value of\n        100 is recommended.\n\n    algorithm : {'arpack', 'randomized'}, default='randomized'\n        SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n        algorithm due to Halko (2009).\n\n    n_iter : int, default=5\n        Number of iterations for randomized SVD solver. Not used by ARPACK. The\n        default is larger than the default in\n        :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\n        matrices that may have large slowly decaying spectrum.\n\n    n_oversamples : int, default=10\n        Number of oversamples for randomized SVD solver. Not used by ARPACK.\n        See :func:`~sklearn.utils.extmath.randomized_svd` for a complete\n        description.\n\n        .. versionadded:: 1.1\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Power iteration normalizer for randomized SVD solver.\n        Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n        for more details.\n\n        .. versionadded:: 1.1\n\n    random_state : int, RandomState instance or None, default=None\n        Used during randomized svd. Pass an int for reproducible results across\n        multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n        SVD solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The right singular vectors of the input data.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        The variance of the training samples transformed by a projection to\n        each component.\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    FactorAnalysis : A simple linear generative model with\n        Gaussian latent variables.\n    IncrementalPCA : Incremental principal components analysis.\n    KernelPCA : Kernel Principal component analysis.\n    NMF : Non-Negative Matrix Factorization.\n    PCA : Principal component analysis.\n\n    Notes\n    -----\n    SVD suffers from a problem called \"sign indeterminacy\", which means the\n    sign of the ``components_`` and the output from transform depend on the\n    algorithm and random state. To work around this, fit instances of this\n    class to data once, then keep the instance around to do transformations.\n\n    References\n    ----------\n    :arxiv:`Halko, et al. (2009). \"Finding structure with randomness:\n    Stochastic algorithms for constructing approximate matrix decompositions\"\n    <0909.4061>`\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import TruncatedSVD\n    >>> from scipy.sparse import csr_matrix\n    >>> import numpy as np\n    >>> np.random.seed(0)\n    >>> X_dense = np.random.rand(100, 100)\n    >>> X_dense[:, 2 * np.arange(50)] = 0\n    >>> X = csr_matrix(X_dense)\n    >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> svd.fit(X)\n    TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> print(svd.explained_variance_ratio_)\n    [0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]\n    >>> print(svd.explained_variance_ratio_.sum())\n    0.2102...\n    >>> print(svd.singular_values_)\n    [35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"algorithm\": [StrOptions({\"arpack\", \"randomized\"})],\n        \"n_iter\": [Interval(Integral, 0, None, closed=\"left\")],\n        \"n_oversamples\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"power_iteration_normalizer\": [StrOptions({\"auto\", \"OR\", \"LU\", \"none\"})],\n        \"random_state\": [\"random_state\"],\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n    }\n\n    def __init__(\n        self,\n        n_components=2,\n        *,\n        algorithm=\"randomized\",\n        n_iter=5,\n        n_oversamples=10,\n        power_iteration_normalizer=\"auto\",\n        random_state=None,\n        tol=0.0,\n    ):\n        self.algorithm = algorithm\n        self.n_components = n_components\n        self.n_iter = n_iter\n        self.n_oversamples = n_oversamples\n        self.power_iteration_normalizer = power_iteration_normalizer\n        self.random_state = random_state\n        self.tol = tol\n\n    def fit(self, X, y=None):\n        \"\"\"Fit model on training data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the transformer object.\n        \"\"\"\n        self.fit_transform(X)\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit_transform(self, X, y=None):\n        \"\"\"Fit model to X and perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\"], ensure_min_features=2)\n        random_state = check_random_state(self.random_state)\n\n        if self.algorithm == \"arpack\":\n            v0 = _init_arpack_v0(min(X.shape), random_state)\n            U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol, v0=v0)\n            # svds doesn't abide by scipy.linalg.svd/randomized_svd\n            # conventions, so reverse its outputs.\n            Sigma = Sigma[::-1]\n            # u_based_decision=False is needed to be consistent with PCA.\n            U, VT = svd_flip(U[:, ::-1], VT[::-1], u_based_decision=False)\n\n        elif self.algorithm == \"randomized\":\n            if self.n_components > X.shape[1]:\n                raise ValueError(\n                    f\"n_components({self.n_components}) must be <=\"\n                    f\" n_features({X.shape[1]}).\"\n                )\n            U, Sigma, VT = randomized_svd(\n                X,\n                self.n_components,\n                n_iter=self.n_iter,\n                n_oversamples=self.n_oversamples,\n                power_iteration_normalizer=self.power_iteration_normalizer,\n                random_state=random_state,\n                flip_sign=False,\n            )\n            U, VT = svd_flip(U, VT, u_based_decision=False)\n\n        self.components_ = VT\n\n        # As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,\n        # X @ V is not the same as U @ Sigma\n        if self.algorithm == \"randomized\" or (\n            self.algorithm == \"arpack\" and self.tol > 0\n        ):\n            X_transformed = safe_sparse_dot(X, self.components_.T)\n        else:\n            X_transformed = U * Sigma\n\n        # Calculate explained variance & explained variance ratio\n        self.explained_variance_ = exp_var = np.var(X_transformed, axis=0)\n        if sp.issparse(X):\n            _, full_var = mean_variance_axis(X, axis=0)\n            full_var = full_var.sum()\n        else:\n            full_var = np.var(X, axis=0).sum()\n        self.explained_variance_ratio_ = exp_var / full_var\n        self.singular_values_ = Sigma  # Store the singular values.\n\n        return X_transformed\n\n    def transform(self, X):\n        \"\"\"Perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\"], reset=False)\n        return safe_sparse_dot(X, self.components_.T)\n\n    def inverse_transform(self, X):\n        \"\"\"Transform X back to its original space.\n\n        Returns an array X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            New data.\n\n        Returns\n        -------\n        X_original : ndarray of shape (n_samples, n_features)\n            Note that this is always a dense array.\n        \"\"\"\n        X = check_array(X)\n        return np.dot(X, self.components_)\n\n    def _more_tags(self):\n        return {\"preserves_dtype\": [np.float64, np.float32]}\n\n    @property\n    def _n_features_out(self):\n        \"\"\"Number of transformed output features.\"\"\"\n        return self.components_.shape[0]\n", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._truncated_svd/TruncatedSVD", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/dummy.py", "fn_id": "", "content": "class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):\n    \"\"\"DummyClassifier makes predictions that ignore the input features.\n\n    This classifier serves as a simple baseline to compare against other more\n    complex classifiers.\n\n    The specific behavior of the baseline is selected with the `strategy`\n    parameter.\n\n    All strategies make predictions that ignore the input feature values passed\n    as the `X` argument to `fit` and `predict`. The predictions, however,\n    typically depend on values observed in the `y` parameter passed to `fit`.\n\n    Note that the \"stratified\" and \"uniform\" strategies lead to\n    non-deterministic predictions that can be rendered deterministic by setting\n    the `random_state` parameter if needed. The other strategies are naturally\n    deterministic and, once fit, always return the same constant prediction\n    for any value of `X`.\n\n    Read more in the :ref:`User Guide <dummy_estimators>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    strategy : {\"most_frequent\", \"prior\", \"stratified\", \"uniform\", \\\n            \"constant\"}, default=\"prior\"\n        Strategy to use to generate predictions.\n\n        * \"most_frequent\": the `predict` method always returns the most\n          frequent class label in the observed `y` argument passed to `fit`.\n          The `predict_proba` method returns the matching one-hot encoded\n          vector.\n        * \"prior\": the `predict` method always returns the most frequent\n          class label in the observed `y` argument passed to `fit` (like\n          \"most_frequent\"). ``predict_proba`` always returns the empirical\n          class distribution of `y` also known as the empirical class prior\n          distribution.\n        * \"stratified\": the `predict_proba` method randomly samples one-hot\n          vectors from a multinomial distribution parametrized by the empirical\n          class prior probabilities.\n          The `predict` method returns the class label which got probability\n          one in the one-hot vector of `predict_proba`.\n          Each sampled row of both methods is therefore independent and\n          identically distributed.\n        * \"uniform\": generates predictions uniformly at random from the list\n          of unique classes observed in `y`, i.e. each class has equal\n          probability.\n        * \"constant\": always predicts a constant label that is provided by\n          the user. This is useful for metrics that evaluate a non-majority\n          class.\n\n          .. versionchanged:: 0.24\n             The default value of `strategy` has changed to \"prior\" in version\n             0.24.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the randomness to generate the predictions when\n        ``strategy='stratified'`` or ``strategy='uniform'``.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    constant : int or str or array-like of shape (n_outputs,), default=None\n        The explicit constant as predicted by the \"constant\" strategy. This\n        parameter is useful only for the \"constant\" strategy.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,) or list of such arrays\n        Unique class labels observed in `y`. For multi-output classification\n        problems, this attribute is a list of arrays as each output has an\n        independent set of possible classes.\n\n    n_classes_ : int or list of int\n        Number of label for each output.\n\n    class_prior_ : ndarray of shape (n_classes,) or list of such arrays\n        Frequency of each class observed in `y`. For multioutput classification\n        problems, this is computed independently for each output.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X` has\n        feature names that are all strings.\n\n    n_outputs_ : int\n        Number of outputs.\n\n    sparse_output_ : bool\n        True if the array returned from predict is to be in sparse CSC format.\n        Is automatically set to True if the input `y` is passed in sparse\n        format.\n\n    See Also\n    --------\n    DummyRegressor : Regressor that makes predictions using simple rules.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.dummy import DummyClassifier\n    >>> X = np.array([-1, 1, 1, 1])\n    >>> y = np.array([0, 1, 1, 1])\n    >>> dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n    >>> dummy_clf.fit(X, y)\n    DummyClassifier(strategy='most_frequent')\n    >>> dummy_clf.predict(X)\n    array([1, 1, 1, 1])\n    >>> dummy_clf.score(X, y)\n    0.75\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"strategy\": [\n            StrOptions({\"most_frequent\", \"prior\", \"stratified\", \"uniform\", \"constant\"})\n        ],\n        \"random_state\": [\"random_state\"],\n        \"constant\": [Integral, str, \"array-like\", None],\n    }\n\n    def __init__(self, *, strategy=\"prior\", random_state=None, constant=None):\n        self.strategy = strategy\n        self.random_state = random_state\n        self.constant = constant\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the baseline classifier.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self._validate_data(X, cast_to_ndarray=False)\n\n        self._strategy = self.strategy\n\n        if self._strategy == \"uniform\" and sp.issparse(y):\n            y = y.toarray()\n            warnings.warn(\n                (\n                    \"A local copy of the target data has been converted \"\n                    \"to a numpy array. Predicting on sparse target data \"\n                    \"with the uniform strategy would not save memory \"\n                    \"and would be slower.\"\n                ),\n                UserWarning,\n            )\n\n        self.sparse_output_ = sp.issparse(y)\n\n        if not self.sparse_output_:\n            y = np.asarray(y)\n            y = np.atleast_1d(y)\n\n        if y.ndim == 1:\n            y = np.reshape(y, (-1, 1))\n\n        self.n_outputs_ = y.shape[1]\n\n        check_consistent_length(X, y)\n\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X)\n\n        if self._strategy == \"constant\":\n            if self.constant is None:\n                raise ValueError(\n                    \"Constant target value has to be specified \"\n                    \"when the constant strategy is used.\"\n                )\n            else:\n                constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))\n                if constant.shape[0] != self.n_outputs_:\n                    raise ValueError(\n                        \"Constant target value should have shape (%d, 1).\"\n                        % self.n_outputs_\n                    )\n\n        (self.classes_, self.n_classes_, self.class_prior_) = class_distribution(\n            y, sample_weight\n        )\n\n        if self._strategy == \"constant\":\n            for k in range(self.n_outputs_):\n                if not any(constant[k][0] == c for c in self.classes_[k]):\n                    # Checking in case of constant strategy if the constant\n                    # provided by the user is in y.\n                    err_msg = (\n                        \"The constant target value must be present in \"\n                        \"the training data. You provided constant={}. \"\n                        \"Possible values are: {}.\".format(\n                            self.constant, self.classes_[k].tolist()\n                        )\n                    )\n                    raise ValueError(err_msg)\n\n        if self.n_outputs_ == 1:\n            self.n_classes_ = self.n_classes_[0]\n            self.classes_ = self.classes_[0]\n            self.class_prior_ = self.class_prior_[0]\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Perform classification on test vectors X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test data.\n\n        Returns\n        -------\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            Predicted target values for X.\n        \"\"\"\n        check_is_fitted(self)\n\n        # numpy random_state expects Python int and not long as size argument\n        # under Windows\n        n_samples = _num_samples(X)\n        rs = check_random_state(self.random_state)\n\n        n_classes_ = self.n_classes_\n        classes_ = self.classes_\n        class_prior_ = self.class_prior_\n        constant = self.constant\n        if self.n_outputs_ == 1:\n            # Get same type even for self.n_outputs_ == 1\n            n_classes_ = [n_classes_]\n            classes_ = [classes_]\n            class_prior_ = [class_prior_]\n            constant = [constant]\n        # Compute probability only once\n        if self._strategy == \"stratified\":\n            proba = self.predict_proba(X)\n            if self.n_outputs_ == 1:\n                proba = [proba]\n\n        if self.sparse_output_:\n            class_prob = None\n            if self._strategy in (\"most_frequent\", \"prior\"):\n                classes_ = [np.array([cp.argmax()]) for cp in class_prior_]\n\n            elif self._strategy == \"stratified\":\n                class_prob = class_prior_\n\n            elif self._strategy == \"uniform\":\n                raise ValueError(\n                    \"Sparse target prediction is not \"\n                    \"supported with the uniform strategy\"\n                )\n\n            elif self._strategy == \"constant\":\n                classes_ = [np.array([c]) for c in constant]\n\n            y = _random_choice_csc(n_samples, classes_, class_prob, self.random_state)\n        else:\n            if self._strategy in (\"most_frequent\", \"prior\"):\n                y = np.tile(\n                    [\n                        classes_[k][class_prior_[k].argmax()]\n                        for k in range(self.n_outputs_)\n                    ],\n                    [n_samples, 1],\n                )\n\n            elif self._strategy == \"stratified\":\n                y = np.vstack(\n                    [\n                        classes_[k][proba[k].argmax(axis=1)]\n                        for k in range(self.n_outputs_)\n                    ]\n                ).T\n\n            elif self._strategy == \"uniform\":\n                ret = [\n                    classes_[k][rs.randint(n_classes_[k], size=n_samples)]\n                    for k in range(self.n_outputs_)\n                ]\n                y = np.vstack(ret).T\n\n            elif self._strategy == \"constant\":\n                y = np.tile(self.constant, (n_samples, 1))\n\n            if self.n_outputs_ == 1:\n                y = np.ravel(y)\n\n        return y\n\n    def predict_proba(self, X):\n        \"\"\"\n        Return probability estimates for the test vectors X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test data.\n\n        Returns\n        -------\n        P : ndarray of shape (n_samples, n_classes) or list of such arrays\n            Returns the probability of the sample for each class in\n            the model, where classes are ordered arithmetically, for each\n            output.\n        \"\"\"\n        check_is_fitted(self)\n\n        # numpy random_state expects Python int and not long as size argument\n        # under Windows\n        n_samples = _num_samples(X)\n        rs = check_random_state(self.random_state)\n\n        n_classes_ = self.n_classes_\n        classes_ = self.classes_\n        class_prior_ = self.class_prior_\n        constant = self.constant\n        if self.n_outputs_ == 1:\n            # Get same type even for self.n_outputs_ == 1\n            n_classes_ = [n_classes_]\n            classes_ = [classes_]\n            class_prior_ = [class_prior_]\n            constant = [constant]\n\n        P = []\n        for k in range(self.n_outputs_):\n            if self._strategy == \"most_frequent\":\n                ind = class_prior_[k].argmax()\n                out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n                out[:, ind] = 1.0\n            elif self._strategy == \"prior\":\n                out = np.ones((n_samples, 1)) * class_prior_[k]\n\n            elif self._strategy == \"stratified\":\n                out = rs.multinomial(1, class_prior_[k], size=n_samples)\n                out = out.astype(np.float64)\n\n            elif self._strategy == \"uniform\":\n                out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)\n                out /= n_classes_[k]\n\n            elif self._strategy == \"constant\":\n                ind = np.where(classes_[k] == constant[k])\n                out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n                out[:, ind] = 1.0\n\n            P.append(out)\n\n        if self.n_outputs_ == 1:\n            P = P[0]\n\n        return P\n\n    def predict_log_proba(self, X):\n        \"\"\"\n        Return log probability estimates for the test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, object with finite length or shape}\n            Training data.\n\n        Returns\n        -------\n        P : ndarray of shape (n_samples, n_classes) or list of such arrays\n            Returns the log probability of the sample for each class in\n            the model, where classes are ordered arithmetically for each\n            output.\n        \"\"\"\n        proba = self.predict_proba(X)\n        if self.n_outputs_ == 1:\n            return np.log(proba)\n        else:\n            return [np.log(p) for p in proba]\n\n    def _more_tags(self):\n        return {\n            \"poor_score\": True,\n            \"no_validation\": True,\n            \"_xfail_checks\": {\n                \"check_methods_subset_invariance\": \"fails for the predict method\",\n                \"check_methods_sample_order_invariance\": \"fails for the predict method\",\n            },\n        }\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : None or array-like of shape (n_samples, n_features)\n            Test samples. Passing None as test samples gives the same result\n            as passing real test samples, since DummyClassifier\n            operates independently of the sampled observations.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) w.r.t. y.\n        \"\"\"\n        if X is None:\n            X = np.zeros(shape=(len(y), 1))\n        return super().score(X, y, sample_weight)\n", "class_fn": true, "question_id": "sklearn/sklearn.dummy/DummyClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_bagging.py", "fn_id": "", "content": "class BaggingRegressor(RegressorMixin, BaseBagging):\n    \"\"\"A Bagging regressor.\n\n    A Bagging regressor is an ensemble meta-estimator that fits base\n    regressors each on random subsets of the original dataset and then\n    aggregate their individual predictions (either by voting or by averaging)\n    to form a final prediction. Such a meta-estimator can typically be used as\n    a way to reduce the variance of a black-box estimator (e.g., a decision\n    tree), by introducing randomization into its construction procedure and\n    then making an ensemble out of it.\n\n    This algorithm encompasses several works from the literature. When random\n    subsets of the dataset are drawn as random subsets of the samples, then\n    this algorithm is known as Pasting [1]_. If samples are drawn with\n    replacement, then the method is known as Bagging [2]_. When random subsets\n    of the dataset are drawn as random subsets of the features, then the method\n    is known as Random Subspaces [3]_. Finally, when base estimators are built\n    on subsets of both samples and features, then the method is known as\n    Random Patches [4]_.\n\n    Read more in the :ref:`User Guide <bagging>`.\n\n    .. versionadded:: 0.15\n\n    Parameters\n    ----------\n    estimator : object, default=None\n        The base estimator to fit on random subsets of the dataset.\n        If None, then the base estimator is a\n        :class:`~sklearn.tree.DecisionTreeRegressor`.\n\n        .. versionadded:: 1.2\n           `base_estimator` was renamed to `estimator`.\n\n    n_estimators : int, default=10\n        The number of base estimators in the ensemble.\n\n    max_samples : int or float, default=1.0\n        The number of samples to draw from X to train each base estimator (with\n        replacement by default, see `bootstrap` for more details).\n\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n\n    max_features : int or float, default=1.0\n        The number of features to draw from X to train each base estimator (\n        without replacement by default, see `bootstrap_features` for more\n        details).\n\n        - If int, then draw `max_features` features.\n        - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n\n    bootstrap : bool, default=True\n        Whether samples are drawn with replacement. If False, sampling\n        without replacement is performed.\n\n    bootstrap_features : bool, default=False\n        Whether features are drawn with replacement.\n\n    oob_score : bool, default=False\n        Whether to use out-of-bag samples to estimate\n        the generalization error. Only available if bootstrap=True.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit\n        a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for both :meth:`fit` and\n        :meth:`predict`. ``None`` means 1 unless in a\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n        processors. See :term:`Glossary <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random resampling of the original dataset\n        (sample wise and feature wise).\n        If the base estimator accepts a `random_state` attribute, a different\n        seed is generated for each instance in the ensemble.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    Attributes\n    ----------\n    estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    estimators_ : list of estimators\n        The collection of fitted sub-estimators.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator. Each subset is defined by an array of the indices selected.\n\n    estimators_features_ : list of arrays\n        The subset of drawn features for each base estimator.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_prediction_ : ndarray of shape (n_samples,)\n        Prediction computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_prediction_` might contain NaN. This attribute exists only\n        when ``oob_score`` is True.\n\n    See Also\n    --------\n    BaggingClassifier : A Bagging classifier.\n\n    References\n    ----------\n\n    .. [1] L. Breiman, \"Pasting small votes for classification in large\n           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n           1996.\n\n    .. [3] T. Ho, \"The random subspace method for constructing decision\n           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n           1998.\n\n    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVR\n    >>> from sklearn.ensemble import BaggingRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_samples=100, n_features=4,\n    ...                        n_informative=2, n_targets=1,\n    ...                        random_state=0, shuffle=False)\n    >>> regr = BaggingRegressor(estimator=SVR(),\n    ...                         n_estimators=10, random_state=0).fit(X, y)\n    >>> regr.predict([[0, 0, 0, 0]])\n    array([-2.8720...])\n    \"\"\"\n\n    def __init__(\n        self,\n        estimator=None,\n        n_estimators=10,\n        *,\n        max_samples=1.0,\n        max_features=1.0,\n        bootstrap=True,\n        bootstrap_features=False,\n        oob_score=False,\n        warm_start=False,\n        n_jobs=None,\n        random_state=None,\n        verbose=0,\n    ):\n        super().__init__(\n            estimator=estimator,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            bootstrap=bootstrap,\n            bootstrap_features=bootstrap_features,\n            oob_score=oob_score,\n            warm_start=warm_start,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n        )\n\n    def predict(self, X):\n        \"\"\"Predict regression target for X.\n\n        The predicted regression target of an input sample is computed as the\n        mean predicted regression targets of the estimators in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        # Check data\n        X = self._validate_data(\n            X,\n            accept_sparse=[\"csr\", \"csc\"],\n            dtype=None,\n            force_all_finite=False,\n            reset=False,\n        )\n\n        # Parallel loop\n        n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n\n        all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n            delayed(_parallel_predict_regression)(\n                self.estimators_[starts[i] : starts[i + 1]],\n                self.estimators_features_[starts[i] : starts[i + 1]],\n                X,\n            )\n            for i in range(n_jobs)\n        )\n\n        # Reduce\n        y_hat = sum(all_y_hat) / self.n_estimators\n\n        return y_hat\n\n    def _set_oob_score(self, X, y):\n        n_samples = y.shape[0]\n\n        predictions = np.zeros((n_samples,))\n        n_predictions = np.zeros((n_samples,))\n\n        for estimator, samples, features in zip(\n            self.estimators_, self.estimators_samples_, self.estimators_features_\n        ):\n            # Create mask for OOB samples\n            mask = ~indices_to_mask(samples, n_samples)\n\n            predictions[mask] += estimator.predict((X[mask, :])[:, features])\n            n_predictions[mask] += 1\n\n        if (n_predictions == 0).any():\n            warn(\n                \"Some inputs do not have OOB scores. \"\n                \"This probably means too few estimators were used \"\n                \"to compute any reliable oob estimates.\"\n            )\n            n_predictions[n_predictions == 0] = 1\n\n        predictions /= n_predictions\n\n        self.oob_prediction_ = predictions\n        self.oob_score_ = r2_score(y, predictions)\n\n    def _get_estimator(self):\n        \"\"\"Resolve which estimator to return (default is DecisionTreeClassifier)\"\"\"\n        if self.estimator is None:\n            return DecisionTreeRegressor()\n        return self.estimator\n", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._bagging/BaggingRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_base.py", "fn_id": "", "content": "class _BaseHeterogeneousEnsemble(\n    MetaEstimatorMixin, _BaseComposition, metaclass=ABCMeta\n):\n    \"\"\"Base class for heterogeneous ensemble of learners.\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator) tuples\n        The ensemble of estimators to use in the ensemble. Each element of the\n        list is defined as a tuple of string (i.e. name of the estimator) and\n        an estimator instance. An estimator can be set to `'drop'` using\n        `set_params`.\n\n    Attributes\n    ----------\n    estimators_ : list of estimators\n        The elements of the estimators parameter, having been fitted on the\n        training data. If an estimator has been set to `'drop'`, it will not\n        appear in `estimators_`.\n    \"\"\"\n\n    _required_parameters = [\"estimators\"]\n\n    @property\n    def named_estimators(self):\n        \"\"\"Dictionary to access any fitted sub-estimators by name.\n\n        Returns\n        -------\n        :class:`~sklearn.utils.Bunch`\n        \"\"\"\n        return Bunch(**dict(self.estimators))\n\n    @abstractmethod\n    def __init__(self, estimators):\n        self.estimators = estimators\n\n    def _validate_estimators(self):\n        if len(self.estimators) == 0:\n            raise ValueError(\n                \"Invalid 'estimators' attribute, 'estimators' should be a \"\n                \"non-empty list of (string, estimator) tuples.\"\n            )\n        names, estimators = zip(*self.estimators)\n        # defined by MetaEstimatorMixin\n        self._validate_names(names)\n\n        has_estimator = any(est != \"drop\" for est in estimators)\n        if not has_estimator:\n            raise ValueError(\n                \"All estimators are dropped. At least one is required \"\n                \"to be an estimator.\"\n            )\n\n        is_estimator_type = is_classifier if is_classifier(self) else is_regressor\n\n        for est in estimators:\n            if est != \"drop\" and not is_estimator_type(est):\n                raise ValueError(\n                    \"The estimator {} should be a {}.\".format(\n                        est.__class__.__name__, is_estimator_type.__name__[3:]\n                    )\n                )\n\n        return names, estimators\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of an estimator from the ensemble.\n\n        Valid parameter keys can be listed with `get_params()`. Note that you\n        can directly set the parameters of the estimators contained in\n        `estimators`.\n\n        Parameters\n        ----------\n        **params : keyword arguments\n            Specific parameters using e.g.\n            `set_params(parameter_name=new_value)`. In addition, to setting the\n            parameters of the estimator, the individual estimator of the\n            estimators can also be set, or can be removed by setting them to\n            'drop'.\n\n        Returns\n        -------\n        self : object\n            Estimator instance.\n        \"\"\"\n        super()._set_params(\"estimators\", **params)\n        return self\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get the parameters of an estimator from the ensemble.\n\n        Returns the parameters given in the constructor as well as the\n        estimators contained within the `estimators` parameter.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            Setting it to True gets the various estimators and the parameters\n            of the estimators as well.\n\n        Returns\n        -------\n        params : dict\n            Parameter and estimator names mapped to their values or parameter\n            names mapped to their values.\n        \"\"\"\n        return super()._get_params(\"estimators\", deep=deep)\n\n    def _more_tags(self):\n        try:\n            allow_nan = all(\n                _safe_tags(est[1])[\"allow_nan\"] if est[1] != \"drop\" else True\n                for est in self.estimators\n            )\n        except Exception:\n            # If `estimators` does not comply with our API (list of tuples) then it will\n            # fail. In this case, we assume that `allow_nan` is False but the parameter\n            # validation will raise an error during `fit`.\n            allow_nan = False\n        return {\"preserves_dtype\": [], \"allow_nan\": allow_nan}\n", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._base/_BaseHeterogeneousEnsemble", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_forest.py", "fn_id": "", "content": "class ExtraTreesRegressor(ForestRegressor):\n    \"\"\"\n    An extra-trees regressor.\n\n    This class implements a meta estimator that fits a number of\n    randomized decision trees (a.k.a. extra-trees) on various sub-samples\n    of the dataset and uses averaging to improve the predictive accuracy\n    and control over-fitting.\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"}, \\\n            default=\"squared_error\"\n        The function to measure the quality of a split. Supported criteria\n        are \"squared_error\" for the mean squared error, which is equal to\n        variance reduction as feature selection criterion and minimizes the L2\n        loss using the mean of each terminal node, \"friedman_mse\", which uses\n        mean squared error with Friedman's improvement score for potential\n        splits, \"absolute_error\" for the mean absolute error, which minimizes\n        the L1 loss using the median of each terminal node, and \"poisson\" which\n        uses reduction in Poisson deviance to find splits.\n        Training using \"absolute_error\" is significantly slower\n        than when using \"squared_error\".\n\n        .. versionadded:: 0.18\n           Mean Absolute Error (MAE) criterion.\n\n    max_depth : int, default=None\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `max(1, int(max_features * n_features_in_))` features are considered at each\n          split.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None or 1.0, then `max_features=n_features`.\n\n        .. note::\n            The default of 1.0 is equivalent to bagged trees and more\n            randomness can be achieved by setting smaller values, e.g. 0.3.\n\n        .. versionchanged:: 1.1\n            The default of `max_features` changed from `\"auto\"` to 1.0.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    bootstrap : bool, default=False\n        Whether bootstrap samples are used when building trees. If False, the\n        whole dataset is used to build each tree.\n\n    oob_score : bool or callable, default=False\n        Whether to use out-of-bag samples to estimate the generalization score.\n        By default, :func:`~sklearn.metrics.r2_score` is used.\n        Provide a callable with signature `metric(y_true, y_pred)` to use a\n        custom metric. Only available if `bootstrap=True`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls 3 sources of randomness:\n\n        - the bootstrapping of the samples used when building trees\n          (if ``bootstrap=True``)\n        - the sampling of the features to consider when looking for the best\n          split at each node (if ``max_features < n_features``)\n        - the draw of the splits for each of the `max_features`\n\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`Glossary <warm_start>` and\n        :ref:`tree_ensemble_warm_start` for details.\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n          `max_samples` should be in the interval `(0.0, 1.0]`.\n\n        .. versionadded:: 0.22\n\n    monotonic_cst : array-like of int of shape (n_features), default=None\n        Indicates the monotonicity constraint to enforce on each feature.\n          - 1: monotonically increasing\n          - 0: no constraint\n          - -1: monotonically decreasing\n\n        If monotonic_cst is None, no constraints are applied.\n\n        Monotonicity constraints are not supported for:\n          - multioutput regressions (i.e. when `n_outputs_ > 1`),\n          - regressions trained on data with missing values.\n\n        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n        .. versionadded:: 1.4\n\n    Attributes\n    ----------\n    estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor`\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    estimators_ : list of DecisionTreeRegressor\n        The collection of fitted sub-estimators.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_outputs_ : int\n        The number of outputs.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n        Prediction computed with out-of-bag estimate on the training set.\n        This attribute exists only when ``oob_score`` is True.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator. Each subset is defined by an array of the indices selected.\n\n        .. versionadded:: 1.4\n\n    See Also\n    --------\n    ExtraTreesClassifier : An extra-trees classifier with random splits.\n    RandomForestClassifier : A random forest classifier with optimal splits.\n    RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    References\n    ----------\n    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n           Machine Learning, 63(1), 3-42, 2006.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_diabetes\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.ensemble import ExtraTreesRegressor\n    >>> X, y = load_diabetes(return_X_y=True)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=0)\n    >>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n    ...    X_train, y_train)\n    >>> reg.score(X_test, y_test)\n    0.2727...\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **ForestRegressor._parameter_constraints,\n        **DecisionTreeRegressor._parameter_constraints,\n    }\n    _parameter_constraints.pop(\"splitter\")\n\n    def __init__(\n        self,\n        n_estimators=100,\n        *,\n        criterion=\"squared_error\",\n        max_depth=None,\n        min_samples_split=2,\n        min_samples_leaf=1,\n        min_weight_fraction_leaf=0.0,\n        max_features=1.0,\n        max_leaf_nodes=None,\n        min_impurity_decrease=0.0,\n        bootstrap=False,\n        oob_score=False,\n        n_jobs=None,\n        random_state=None,\n        verbose=0,\n        warm_start=False,\n        ccp_alpha=0.0,\n        max_samples=None,\n        monotonic_cst=None,\n    ):\n        super().__init__(\n            estimator=ExtraTreeRegressor(),\n            n_estimators=n_estimators,\n            estimator_params=(\n                \"criterion\",\n                \"max_depth\",\n                \"min_samples_split\",\n                \"min_samples_leaf\",\n                \"min_weight_fraction_leaf\",\n                \"max_features\",\n                \"max_leaf_nodes\",\n                \"min_impurity_decrease\",\n                \"random_state\",\n                \"ccp_alpha\",\n                \"monotonic_cst\",\n            ),\n            bootstrap=bootstrap,\n            oob_score=oob_score,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            max_samples=max_samples,\n        )\n\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.ccp_alpha = ccp_alpha\n        self.monotonic_cst = monotonic_cst\n", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._forest/ExtraTreesRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_forest.py", "fn_id": "", "content": "class RandomForestClassifier(ForestClassifier):\n    \"\"\"\n    A random forest classifier.\n\n    A random forest is a meta estimator that fits a number of decision tree\n    classifiers on various sub-samples of the dataset and uses averaging to\n    improve the predictive accuracy and control over-fitting.\n    Trees in the forest use the best split strategy, i.e. equivalent to passing\n    `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\n    The sub-sample size is controlled with the `max_samples` parameter if\n    `bootstrap=True` (default), otherwise the whole dataset is used to build\n    each tree.\n\n    For a comparison between tree-based ensemble models see the example\n    :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n        Shannon information gain, see :ref:`tree_mathematical_formulation`.\n        Note: This parameter is tree-specific.\n\n    max_depth : int, default=None\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `max(1, int(max_features * n_features_in_))` features are considered at each\n          split.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        .. versionchanged:: 1.1\n            The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    bootstrap : bool, default=True\n        Whether bootstrap samples are used when building trees. If False, the\n        whole dataset is used to build each tree.\n\n    oob_score : bool or callable, default=False\n        Whether to use out-of-bag samples to estimate the generalization score.\n        By default, :func:`~sklearn.metrics.accuracy_score` is used.\n        Provide a callable with signature `metric(y_true, y_pred)` to use a\n        custom metric. Only available if `bootstrap=True`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls both the randomness of the bootstrapping of the samples used\n        when building trees (if ``bootstrap=True``) and the sampling of the\n        features to consider when looking for the best split at each node\n        (if ``max_features < n_features``).\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`Glossary <warm_start>` and\n        :ref:`tree_ensemble_warm_start` for details.\n\n    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\\n            default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n        weights are computed based on the bootstrap sample for every tree\n        grown.\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n          `max_samples` should be in the interval `(0.0, 1.0]`.\n\n        .. versionadded:: 0.22\n\n    monotonic_cst : array-like of int of shape (n_features), default=None\n        Indicates the monotonicity constraint to enforce on each feature.\n          - 1: monotonic increase\n          - 0: no constraint\n          - -1: monotonic decrease\n\n        If monotonic_cst is None, no constraints are applied.\n\n        Monotonicity constraints are not supported for:\n          - multiclass classifications (i.e. when `n_classes > 2`),\n          - multioutput classifications (i.e. when `n_outputs_ > 1`),\n          - classifications trained on data with missing values.\n\n        The constraints hold over the probability of the positive class.\n\n        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n        .. versionadded:: 1.4\n\n    Attributes\n    ----------\n    estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    estimators_ : list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    classes_ : ndarray of shape (n_classes,) or a list of such arrays\n        The classes labels (single output problem), or a list of arrays of\n        class labels (multi-output problem).\n\n    n_classes_ : int or list\n        The number of classes (single output problem), or a list containing the\n        number of classes for each output (multi-output problem).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \\\n            (n_samples, n_classes, n_outputs)\n        Decision function computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_decision_function_` might contain NaN. This attribute exists\n        only when ``oob_score`` is True.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator. Each subset is defined by an array of the indices selected.\n\n        .. versionadded:: 1.4\n\n    See Also\n    --------\n    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n    sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n        tree classifiers.\n    sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n        Boosting Classification Tree, very fast for big datasets (n_samples >=\n        10_000).\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data,\n    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n    of the criterion is identical for several splits enumerated during the\n    search of the best split. To obtain a deterministic behaviour during\n    fitting, ``random_state`` has to be fixed.\n\n    References\n    ----------\n    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n    >>> clf.fit(X, y)\n    RandomForestClassifier(...)\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **ForestClassifier._parameter_constraints,\n        **DecisionTreeClassifier._parameter_constraints,\n        \"class_weight\": [\n            StrOptions({\"balanced_subsample\", \"balanced\"}),\n            dict,\n            list,\n            None,\n        ],\n    }\n    _parameter_constraints.pop(\"splitter\")\n\n    def __init__(\n        self,\n        n_estimators=100,\n        *,\n        criterion=\"gini\",\n        max_depth=None,\n        min_samples_split=2,\n        min_samples_leaf=1,\n        min_weight_fraction_leaf=0.0,\n        max_features=\"sqrt\",\n        max_leaf_nodes=None,\n        min_impurity_decrease=0.0,\n        bootstrap=True,\n        oob_score=False,\n        n_jobs=None,\n        random_state=None,\n        verbose=0,\n        warm_start=False,\n        class_weight=None,\n        ccp_alpha=0.0,\n        max_samples=None,\n        monotonic_cst=None,\n    ):\n        super().__init__(\n            estimator=DecisionTreeClassifier(),\n            n_estimators=n_estimators,\n            estimator_params=(\n                \"criterion\",\n                \"max_depth\",\n                \"min_samples_split\",\n                \"min_samples_leaf\",\n                \"min_weight_fraction_leaf\",\n                \"max_features\",\n                \"max_leaf_nodes\",\n                \"min_impurity_decrease\",\n                \"random_state\",\n                \"ccp_alpha\",\n                \"monotonic_cst\",\n            ),\n            bootstrap=bootstrap,\n            oob_score=oob_score,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            class_weight=class_weight,\n            max_samples=max_samples,\n        )\n\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.monotonic_cst = monotonic_cst\n        self.ccp_alpha = ccp_alpha\n", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._forest/RandomForestClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_gb.py", "fn_id": "", "content": "class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):\n    \"\"\"Abstract base class for Gradient Boosting.\"\"\"\n\n    _parameter_constraints: dict = {\n        **DecisionTreeRegressor._parameter_constraints,\n        \"learning_rate\": [Interval(Real, 0.0, None, closed=\"left\")],\n        \"n_estimators\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"criterion\": [StrOptions({\"friedman_mse\", \"squared_error\"})],\n        \"subsample\": [Interval(Real, 0.0, 1.0, closed=\"right\")],\n        \"verbose\": [\"verbose\"],\n        \"warm_start\": [\"boolean\"],\n        \"validation_fraction\": [Interval(Real, 0.0, 1.0, closed=\"neither\")],\n        \"n_iter_no_change\": [Interval(Integral, 1, None, closed=\"left\"), None],\n        \"tol\": [Interval(Real, 0.0, None, closed=\"left\")],\n    }\n    _parameter_constraints.pop(\"splitter\")\n    _parameter_constraints.pop(\"monotonic_cst\")\n\n    @abstractmethod\n    def __init__(\n        self,\n        *,\n        loss,\n        learning_rate,\n        n_estimators,\n        criterion,\n        min_samples_split,\n        min_samples_leaf,\n        min_weight_fraction_leaf,\n        max_depth,\n        min_impurity_decrease,\n        init,\n        subsample,\n        max_features,\n        ccp_alpha,\n        random_state,\n        alpha=0.9,\n        verbose=0,\n        max_leaf_nodes=None,\n        warm_start=False,\n        validation_fraction=0.1,\n        n_iter_no_change=None,\n        tol=1e-4,\n    ):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.loss = loss\n        self.criterion = criterion\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.subsample = subsample\n        self.max_features = max_features\n        self.max_depth = max_depth\n        self.min_impurity_decrease = min_impurity_decrease\n        self.ccp_alpha = ccp_alpha\n        self.init = init\n        self.random_state = random_state\n        self.alpha = alpha\n        self.verbose = verbose\n        self.max_leaf_nodes = max_leaf_nodes\n        self.warm_start = warm_start\n        self.validation_fraction = validation_fraction\n        self.n_iter_no_change = n_iter_no_change\n        self.tol = tol\n\n    @abstractmethod\n    def _encode_y(self, y=None, sample_weight=None):\n        \"\"\"Called by fit to validate and encode y.\"\"\"\n\n    @abstractmethod\n    def _get_loss(self, sample_weight):\n        \"\"\"Get loss object from sklearn._loss.loss.\"\"\"\n\n    def _fit_stage(\n        self,\n        i,\n        X,\n        y,\n        raw_predictions,\n        sample_weight,\n        sample_mask,\n        random_state,\n        X_csc=None,\n        X_csr=None,\n    ):\n        \"\"\"Fit another stage of ``n_trees_per_iteration_`` trees.\"\"\"\n        original_y = y\n\n        if isinstance(self._loss, HuberLoss):\n            set_huber_delta(\n                loss=self._loss,\n                y_true=y,\n                raw_prediction=raw_predictions,\n                sample_weight=sample_weight,\n            )\n        # TODO: Without oob, i.e. with self.subsample = 1.0, we could call\n        # self._loss.loss_gradient and use it to set train_score_.\n        # But note that train_score_[i] is the score AFTER fitting the i-th tree.\n        # Note: We need the negative gradient!\n        neg_gradient = -self._loss.gradient(\n            y_true=y,\n            raw_prediction=raw_predictions,\n            sample_weight=None,  # We pass sample_weights to the tree directly.\n        )\n        # 2-d views of shape (n_samples, n_trees_per_iteration_) or (n_samples, 1)\n        # on neg_gradient to simplify the loop over n_trees_per_iteration_.\n        if neg_gradient.ndim == 1:\n            neg_g_view = neg_gradient.reshape((-1, 1))\n        else:\n            neg_g_view = neg_gradient\n\n        for k in range(self.n_trees_per_iteration_):\n            if self._loss.is_multiclass:\n                y = np.array(original_y == k, dtype=np.float64)\n\n            # induce regression tree on the negative gradient\n            tree = DecisionTreeRegressor(\n                criterion=self.criterion,\n                splitter=\"best\",\n                max_depth=self.max_depth,\n                min_samples_split=self.min_samples_split,\n                min_samples_leaf=self.min_samples_leaf,\n                min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n                min_impurity_decrease=self.min_impurity_decrease,\n                max_features=self.max_features,\n                max_leaf_nodes=self.max_leaf_nodes,\n                random_state=random_state,\n                ccp_alpha=self.ccp_alpha,\n            )\n\n            if self.subsample < 1.0:\n                # no inplace multiplication!\n                sample_weight = sample_weight * sample_mask.astype(np.float64)\n\n            X = X_csc if X_csc is not None else X\n            tree.fit(\n                X, neg_g_view[:, k], sample_weight=sample_weight, check_input=False\n            )\n\n            # update tree leaves\n            X_for_tree_update = X_csr if X_csr is not None else X\n            _update_terminal_regions(\n                self._loss,\n                tree.tree_,\n                X_for_tree_update,\n                y,\n                neg_g_view[:, k],\n                raw_predictions,\n                sample_weight,\n                sample_mask,\n                learning_rate=self.learning_rate,\n                k=k,\n            )\n\n            # add tree to ensemble\n            self.estimators_[i, k] = tree\n\n        return raw_predictions\n\n    def _set_max_features(self):\n        \"\"\"Set self.max_features_.\"\"\"\n        if isinstance(self.max_features, str):\n            if self.max_features == \"auto\":\n                if is_classifier(self):\n                    max_features = max(1, int(np.sqrt(self.n_features_in_)))\n                else:\n                    max_features = self.n_features_in_\n            elif self.max_features == \"sqrt\":\n                max_features = max(1, int(np.sqrt(self.n_features_in_)))\n            else:  # self.max_features == \"log2\"\n                max_features = max(1, int(np.log2(self.n_features_in_)))\n        elif self.max_features is None:\n            max_features = self.n_features_in_\n        elif isinstance(self.max_features, Integral):\n            max_features = self.max_features\n        else:  # float\n            max_features = max(1, int(self.max_features * self.n_features_in_))\n\n        self.max_features_ = max_features\n\n    def _init_state(self):\n        \"\"\"Initialize model state and allocate model state data structures.\"\"\"\n\n        self.init_ = self.init\n        if self.init_ is None:\n            if is_classifier(self):\n                self.init_ = DummyClassifier(strategy=\"prior\")\n            elif isinstance(self._loss, (AbsoluteError, HuberLoss)):\n                self.init_ = DummyRegressor(strategy=\"quantile\", quantile=0.5)\n            elif isinstance(self._loss, PinballLoss):\n                self.init_ = DummyRegressor(strategy=\"quantile\", quantile=self.alpha)\n            else:\n                self.init_ = DummyRegressor(strategy=\"mean\")\n\n        self.estimators_ = np.empty(\n            (self.n_estimators, self.n_trees_per_iteration_), dtype=object\n        )\n        self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)\n        # do oob?\n        if self.subsample < 1.0:\n            self.oob_improvement_ = np.zeros((self.n_estimators), dtype=np.float64)\n            self.oob_scores_ = np.zeros((self.n_estimators), dtype=np.float64)\n            self.oob_score_ = np.nan\n\n    def _clear_state(self):\n        \"\"\"Clear the state of the gradient boosting model.\"\"\"\n        if hasattr(self, \"estimators_\"):\n            self.estimators_ = np.empty((0, 0), dtype=object)\n        if hasattr(self, \"train_score_\"):\n            del self.train_score_\n        if hasattr(self, \"oob_improvement_\"):\n            del self.oob_improvement_\n        if hasattr(self, \"oob_scores_\"):\n            del self.oob_scores_\n        if hasattr(self, \"oob_score_\"):\n            del self.oob_score_\n        if hasattr(self, \"init_\"):\n            del self.init_\n        if hasattr(self, \"_rng\"):\n            del self._rng\n\n    def _resize_state(self):\n        \"\"\"Add additional ``n_estimators`` entries to all attributes.\"\"\"\n        # self.n_estimators is the number of additional est to fit\n        total_n_estimators = self.n_estimators\n        if total_n_estimators < self.estimators_.shape[0]:\n            raise ValueError(\n                \"resize with smaller n_estimators %d < %d\"\n                % (total_n_estimators, self.estimators_[0])\n            )\n\n        self.estimators_ = np.resize(\n            self.estimators_, (total_n_estimators, self.n_trees_per_iteration_)\n        )\n        self.train_score_ = np.resize(self.train_score_, total_n_estimators)\n        if self.subsample < 1 or hasattr(self, \"oob_improvement_\"):\n            # if do oob resize arrays or create new if not available\n            if hasattr(self, \"oob_improvement_\"):\n                self.oob_improvement_ = np.resize(\n                    self.oob_improvement_, total_n_estimators\n                )\n                self.oob_scores_ = np.resize(self.oob_scores_, total_n_estimators)\n                self.oob_score_ = np.nan\n            else:\n                self.oob_improvement_ = np.zeros(\n                    (total_n_estimators,), dtype=np.float64\n                )\n                self.oob_scores_ = np.zeros((total_n_estimators,), dtype=np.float64)\n                self.oob_score_ = np.nan\n\n    def _is_fitted(self):\n        return len(getattr(self, \"estimators_\", [])) > 0\n\n    def _check_initialized(self):\n        \"\"\"Check that the estimator is initialized, raising an error if not.\"\"\"\n        check_is_fitted(self)\n\n    @_fit_context(\n        # GradientBoosting*.init is not validated yet\n        prefer_skip_nested_validation=False\n    )\n    def fit(self, X, y, sample_weight=None, monitor=None):\n        \"\"\"Fit the gradient boosting model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        y : array-like of shape (n_samples,)\n            Target values (strings or integers in classification, real numbers\n            in regression)\n            For classification, labels must correspond to classes.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted. Splits\n            that would create child nodes with net zero or negative weight are\n            ignored while searching for a split in each node. In the case of\n            classification, splits are also ignored if they would result in any\n            single class carrying a negative weight in either child node.\n\n        monitor : callable, default=None\n            The monitor is called after each iteration with the current\n            iteration, a reference to the estimator and the local variables of\n            ``_fit_stages`` as keyword arguments ``callable(i, self,\n            locals())``. If the callable returns ``True`` the fitting procedure\n            is stopped. The monitor can be used for various things such as\n            computing held-out estimates, early stopping, model introspect, and\n            snapshotting.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        if not self.warm_start:\n            self._clear_state()\n\n        # Check input\n        # Since check_array converts both X and y to the same dtype, but the\n        # trees use different types for X and y, checking them separately.\n\n        X, y = self._validate_data(\n            X, y, accept_sparse=[\"csr\", \"csc\", \"coo\"], dtype=DTYPE, multi_output=True\n        )\n        sample_weight_is_none = sample_weight is None\n        sample_weight = _check_sample_weight(sample_weight, X)\n        if sample_weight_is_none:\n            y = self._encode_y(y=y, sample_weight=None)\n        else:\n            y = self._encode_y(y=y, sample_weight=sample_weight)\n        y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n\n        self._set_max_features()\n\n        # self.loss is guaranteed to be a string\n        self._loss = self._get_loss(sample_weight=sample_weight)\n\n        if self.n_iter_no_change is not None:\n            stratify = y if is_classifier(self) else None\n            (\n                X_train,\n                X_val,\n                y_train,\n                y_val,\n                sample_weight_train,\n                sample_weight_val,\n            ) = train_test_split(\n                X,\n                y,\n                sample_weight,\n                random_state=self.random_state,\n                test_size=self.validation_fraction,\n                stratify=stratify,\n            )\n            if is_classifier(self):\n                if self.n_classes_ != np.unique(y_train).shape[0]:\n                    # We choose to error here. The problem is that the init\n                    # estimator would be trained on y, which has some missing\n                    # classes now, so its predictions would not have the\n                    # correct shape.\n                    raise ValueError(\n                        \"The training data after the early stopping split \"\n                        \"is missing some classes. Try using another random \"\n                        \"seed.\"\n                    )\n        else:\n            X_train, y_train, sample_weight_train = X, y, sample_weight\n            X_val = y_val = sample_weight_val = None\n\n        n_samples = X_train.shape[0]\n\n        # First time calling fit.\n        if not self._is_fitted():\n            # init state\n            self._init_state()\n\n            # fit initial model and initialize raw predictions\n            if self.init_ == \"zero\":\n                raw_predictions = np.zeros(\n                    shape=(n_samples, self.n_trees_per_iteration_),\n                    dtype=np.float64,\n                )\n            else:\n                # XXX clean this once we have a support_sample_weight tag\n                if sample_weight_is_none:\n                    self.init_.fit(X_train, y_train)\n                else:\n                    msg = (\n                        \"The initial estimator {} does not support sample \"\n                        \"weights.\".format(self.init_.__class__.__name__)\n                    )\n                    try:\n                        self.init_.fit(\n                            X_train, y_train, sample_weight=sample_weight_train\n                        )\n                    except TypeError as e:\n                        if \"unexpected keyword argument 'sample_weight'\" in str(e):\n                            # regular estimator without SW support\n                            raise ValueError(msg) from e\n                        else:  # regular estimator whose input checking failed\n                            raise\n                    except ValueError as e:\n                        if (\n                            \"pass parameters to specific steps of \"\n                            \"your pipeline using the \"\n                            \"stepname__parameter\" in str(e)\n                        ):  # pipeline\n                            raise ValueError(msg) from e\n                        else:  # regular estimator whose input checking failed\n                            raise\n\n                raw_predictions = _init_raw_predictions(\n                    X_train, self.init_, self._loss, is_classifier(self)\n                )\n\n            begin_at_stage = 0\n\n            # The rng state must be preserved if warm_start is True\n            self._rng = check_random_state(self.random_state)\n\n        # warm start: this is not the first time fit was called\n        else:\n            # add more estimators to fitted model\n            # invariant: warm_start = True\n            if self.n_estimators < self.estimators_.shape[0]:\n                raise ValueError(\n                    \"n_estimators=%d must be larger or equal to \"\n                    \"estimators_.shape[0]=%d when \"\n                    \"warm_start==True\" % (self.n_estimators, self.estimators_.shape[0])\n                )\n            begin_at_stage = self.estimators_.shape[0]\n            # The requirements of _raw_predict\n            # are more constrained than fit. It accepts only CSR\n            # matrices. Finite values have already been checked in _validate_data.\n            X_train = check_array(\n                X_train,\n                dtype=DTYPE,\n                order=\"C\",\n                accept_sparse=\"csr\",\n                force_all_finite=False,\n            )\n            raw_predictions = self._raw_predict(X_train)\n            self._resize_state()\n\n        # fit the boosting stages\n        n_stages = self._fit_stages(\n            X_train,\n            y_train,\n            raw_predictions,\n            sample_weight_train,\n            self._rng,\n            X_val,\n            y_val,\n            sample_weight_val,\n            begin_at_stage,\n            monitor,\n        )\n\n        # change shape of arrays after fit (early-stopping or additional ests)\n        if n_stages != self.estimators_.shape[0]:\n            self.estimators_ = self.estimators_[:n_stages]\n            self.train_score_ = self.train_score_[:n_stages]\n            if hasattr(self, \"oob_improvement_\"):\n                # OOB scores were computed\n                self.oob_improvement_ = self.oob_improvement_[:n_stages]\n                self.oob_scores_ = self.oob_scores_[:n_stages]\n                self.oob_score_ = self.oob_scores_[-1]\n        self.n_estimators_ = n_stages\n        return self\n\n    def _fit_stages(\n        self,\n        X,\n        y,\n        raw_predictions,\n        sample_weight,\n        random_state,\n        X_val,\n        y_val,\n        sample_weight_val,\n        begin_at_stage=0,\n        monitor=None,\n    ):\n        \"\"\"Iteratively fits the stages.\n\n        For each stage it computes the progress (OOB, train score)\n        and delegates to ``_fit_stage``.\n        Returns the number of stages fit; might differ from ``n_estimators``\n        due to early stopping.\n        \"\"\"\n        n_samples = X.shape[0]\n        do_oob = self.subsample < 1.0\n        sample_mask = np.ones((n_samples,), dtype=bool)\n        n_inbag = max(1, int(self.subsample * n_samples))\n\n        if self.verbose:\n            verbose_reporter = VerboseReporter(verbose=self.verbose)\n            verbose_reporter.init(self, begin_at_stage)\n\n        X_csc = csc_matrix(X) if issparse(X) else None\n        X_csr = csr_matrix(X) if issparse(X) else None\n\n        if self.n_iter_no_change is not None:\n            loss_history = np.full(self.n_iter_no_change, np.inf)\n            # We create a generator to get the predictions for X_val after\n            # the addition of each successive stage\n            y_val_pred_iter = self._staged_raw_predict(X_val, check_input=False)\n\n        # Older versions of GBT had its own loss functions. With the new common\n        # private loss function submodule _loss, we often are a factor of 2\n        # away from the old version. Here we keep backward compatibility for\n        # oob_scores_ and oob_improvement_, even if the old way is quite\n        # inconsistent (sometimes the gradient is half the gradient, sometimes\n        # not).\n        if isinstance(\n            self._loss,\n            (\n                HalfSquaredError,\n                HalfBinomialLoss,\n            ),\n        ):\n            factor = 2\n        else:\n            factor = 1\n\n        # perform boosting iterations\n        i = begin_at_stage\n        for i in range(begin_at_stage, self.n_estimators):\n            # subsampling\n            if do_oob:\n                sample_mask = _random_sample_mask(n_samples, n_inbag, random_state)\n                y_oob_masked = y[~sample_mask]\n                sample_weight_oob_masked = sample_weight[~sample_mask]\n                if i == 0:  # store the initial loss to compute the OOB score\n                    initial_loss = factor * self._loss(\n                        y_true=y_oob_masked,\n                        raw_prediction=raw_predictions[~sample_mask],\n                        sample_weight=sample_weight_oob_masked,\n                    )\n\n            # fit next stage of trees\n            raw_predictions = self._fit_stage(\n                i,\n                X,\n                y,\n                raw_predictions,\n                sample_weight,\n                sample_mask,\n                random_state,\n                X_csc=X_csc,\n                X_csr=X_csr,\n            )\n\n            # track loss\n            if do_oob:\n                self.train_score_[i] = factor * self._loss(\n                    y_true=y[sample_mask],\n                    raw_prediction=raw_predictions[sample_mask],\n                    sample_weight=sample_weight[sample_mask],\n                )\n                self.oob_scores_[i] = factor * self._loss(\n                    y_true=y_oob_masked,\n                    raw_prediction=raw_predictions[~sample_mask],\n                    sample_weight=sample_weight_oob_masked,\n                )\n                previous_loss = initial_loss if i == 0 else self.oob_scores_[i - 1]\n                self.oob_improvement_[i] = previous_loss - self.oob_scores_[i]\n                self.oob_score_ = self.oob_scores_[-1]\n            else:\n                # no need to fancy index w/ no subsampling\n                self.train_score_[i] = factor * self._loss(\n                    y_true=y,\n                    raw_prediction=raw_predictions,\n                    sample_weight=sample_weight,\n                )\n\n            if self.verbose > 0:\n                verbose_reporter.update(i, self)\n\n            if monitor is not None:\n                early_stopping = monitor(i, self, locals())\n                if early_stopping:\n                    break\n\n            # We also provide an early stopping based on the score from\n            # validation set (X_val, y_val), if n_iter_no_change is set\n            if self.n_iter_no_change is not None:\n                # By calling next(y_val_pred_iter), we get the predictions\n                # for X_val after the addition of the current stage\n                validation_loss = factor * self._loss(\n                    y_val, next(y_val_pred_iter), sample_weight_val\n                )\n\n                # Require validation_score to be better (less) than at least\n                # one of the last n_iter_no_change evaluations\n                if np.any(validation_loss + self.tol < loss_history):\n                    loss_history[i % len(loss_history)] = validation_loss\n                else:\n                    break\n\n        return i + 1\n\n    def _make_estimator(self, append=True):\n        # we don't need _make_estimator\n        raise NotImplementedError()\n\n    def _raw_predict_init(self, X):\n        \"\"\"Check input and compute raw predictions of the init estimator.\"\"\"\n        self._check_initialized()\n        X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n        if self.init_ == \"zero\":\n            raw_predictions = np.zeros(\n                shape=(X.shape[0], self.n_trees_per_iteration_), dtype=np.float64\n            )\n        else:\n            raw_predictions = _init_raw_predictions(\n                X, self.init_, self._loss, is_classifier(self)\n            )\n        return raw_predictions\n\n    def _raw_predict(self, X):\n        \"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\n        check_is_fitted(self)\n        raw_predictions = self._raw_predict_init(X)\n        predict_stages(self.estimators_, X, self.learning_rate, raw_predictions)\n        return raw_predictions\n\n    def _staged_raw_predict(self, X, check_input=True):\n        \"\"\"Compute raw predictions of ``X`` for each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        check_input : bool, default=True\n            If False, the input arrays X will not be checked.\n\n        Returns\n        -------\n        raw_predictions : generator of ndarray of shape (n_samples, k)\n            The raw predictions of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification are special cases with\n            ``k == 1``, otherwise ``k==n_classes``.\n        \"\"\"\n        if check_input:\n            X = self._validate_data(\n                X, dtype=DTYPE, order=\"C\", accept_sparse=\"csr\", reset=False\n            )\n        raw_predictions = self._raw_predict_init(X)\n        for i in range(self.estimators_.shape[0]):\n            predict_stage(self.estimators_, i, X, self.learning_rate, raw_predictions)\n            yield raw_predictions.copy()\n\n    @property\n    def feature_importances_(self):\n        \"\"\"The impurity-based feature importances.\n\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n        Returns\n        -------\n        feature_importances_ : ndarray of shape (n_features,)\n            The values of this array sum to 1, unless all trees are single node\n            trees consisting of only the root node, in which case it will be an\n            array of zeros.\n        \"\"\"\n        self._check_initialized()\n\n        relevant_trees = [\n            tree\n            for stage in self.estimators_\n            for tree in stage\n            if tree.tree_.node_count > 1\n        ]\n        if not relevant_trees:\n            # degenerate case where all trees have only one node\n            return np.zeros(shape=self.n_features_in_, dtype=np.float64)\n\n        relevant_feature_importances = [\n            tree.tree_.compute_feature_importances(normalize=False)\n            for tree in relevant_trees\n        ]\n        avg_feature_importances = np.mean(\n            relevant_feature_importances, axis=0, dtype=np.float64\n        )\n        return avg_feature_importances / np.sum(avg_feature_importances)\n\n    def _compute_partial_dependence_recursion(self, grid, target_features):\n        \"\"\"Fast partial dependence computation.\n\n        Parameters\n        ----------\n        grid : ndarray of shape (n_samples, n_target_features), dtype=np.float32\n            The grid points on which the partial dependence should be\n            evaluated.\n        target_features : ndarray of shape (n_target_features,), dtype=np.intp\n            The set of target features for which the partial dependence\n            should be evaluated.\n\n        Returns\n        -------\n        averaged_predictions : ndarray of shape \\\n                (n_trees_per_iteration_, n_samples)\n            The value of the partial dependence function on each grid point.\n        \"\"\"\n        if self.init is not None:\n            warnings.warn(\n                \"Using recursion method with a non-constant init predictor \"\n                \"will lead to incorrect partial dependence values. \"\n                \"Got init=%s.\" % self.init,\n                UserWarning,\n            )\n        grid = np.asarray(grid, dtype=DTYPE, order=\"C\")\n        n_estimators, n_trees_per_stage = self.estimators_.shape\n        averaged_predictions = np.zeros(\n            (n_trees_per_stage, grid.shape[0]), dtype=np.float64, order=\"C\"\n        )\n        target_features = np.asarray(target_features, dtype=np.intp, order=\"C\")\n\n        for stage in range(n_estimators):\n            for k in range(n_trees_per_stage):\n                tree = self.estimators_[stage, k].tree_\n                tree.compute_partial_dependence(\n                    grid, target_features, averaged_predictions[k]\n                )\n        averaged_predictions *= self.learning_rate\n\n        return averaged_predictions\n\n    def apply(self, X):\n        \"\"\"Apply trees in the ensemble to X, return leaf indices.\n\n        .. versionadded:: 0.17\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will\n            be converted to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        X_leaves : array-like of shape (n_samples, n_estimators, n_classes)\n            For each datapoint x in X and for each tree in the ensemble,\n            return the index of the leaf x ends up in each estimator.\n            In the case of binary classification n_classes is 1.\n        \"\"\"\n\n        self._check_initialized()\n        X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n\n        # n_classes will be equal to 1 in the binary classification or the\n        # regression case.\n        n_estimators, n_classes = self.estimators_.shape\n        leaves = np.zeros((X.shape[0], n_estimators, n_classes))\n\n        for i in range(n_estimators):\n            for j in range(n_classes):\n                estimator = self.estimators_[i, j]\n                leaves[:, i, j] = estimator.apply(X, check_input=False)\n\n        return leaves\n", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._gb/BaseGradientBoosting", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_gb.py", "fn_id": "", "content": "class VerboseReporter:\n    \"\"\"Reports verbose output to stdout.\n\n    Parameters\n    ----------\n    verbose : int\n        Verbosity level. If ``verbose==1`` output is printed once in a while\n        (when iteration mod verbose_mod is zero).; if larger than 1 then output\n        is printed for each update.\n    \"\"\"\n\n    def __init__(self, verbose):\n        self.verbose = verbose\n\n    def init(self, est, begin_at_stage=0):\n        \"\"\"Initialize reporter\n\n        Parameters\n        ----------\n        est : Estimator\n            The estimator\n\n        begin_at_stage : int, default=0\n            stage at which to begin reporting\n        \"\"\"\n        # header fields and line format str\n        header_fields = [\"Iter\", \"Train Loss\"]\n        verbose_fmt = [\"{iter:>10d}\", \"{train_score:>16.4f}\"]\n        # do oob?\n        if est.subsample < 1:\n            header_fields.append(\"OOB Improve\")\n            verbose_fmt.append(\"{oob_impr:>16.4f}\")\n        header_fields.append(\"Remaining Time\")\n        verbose_fmt.append(\"{remaining_time:>16s}\")\n\n        # print the header line\n        print((\"%10s \" + \"%16s \" * (len(header_fields) - 1)) % tuple(header_fields))\n\n        self.verbose_fmt = \" \".join(verbose_fmt)\n        # plot verbose info each time i % verbose_mod == 0\n        self.verbose_mod = 1\n        self.start_time = time()\n        self.begin_at_stage = begin_at_stage\n\n    def update(self, j, est):\n        \"\"\"Update reporter with new iteration.\n\n        Parameters\n        ----------\n        j : int\n            The new iteration.\n        est : Estimator\n            The estimator.\n        \"\"\"\n        do_oob = est.subsample < 1\n        # we need to take into account if we fit additional estimators.\n        i = j - self.begin_at_stage  # iteration relative to the start iter\n        if (i + 1) % self.verbose_mod == 0:\n            oob_impr = est.oob_improvement_[j] if do_oob else 0\n            remaining_time = (\n                (est.n_estimators - (j + 1)) * (time() - self.start_time) / float(i + 1)\n            )\n            if remaining_time > 60:\n                remaining_time = \"{0:.2f}m\".format(remaining_time / 60.0)\n            else:\n                remaining_time = \"{0:.2f}s\".format(remaining_time)\n            print(\n                self.verbose_fmt.format(\n                    iter=j + 1,\n                    train_score=est.train_score_[j],\n                    oob_impr=oob_impr,\n                    remaining_time=remaining_time,\n                )\n            )\n            if self.verbose == 1 and ((i + 1) // (self.verbose_mod * 10) > 0):\n                # adjust verbose frequency (powers of 10)\n                self.verbose_mod *= 10\n", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._gb/VerboseReporter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py", "fn_id": "", "content": "class HistGradientBoostingClassifier(ClassifierMixin, BaseHistGradientBoosting):\n    \"\"\"Histogram-based Gradient Boosting Classification Tree.\n\n    This estimator is much faster than\n    :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\n    for big datasets (n_samples >= 10 000).\n\n    This estimator has native support for missing values (NaNs). During\n    training, the tree grower learns at each split point whether samples\n    with missing values should go to the left or right child, based on the\n    potential gain. When predicting, samples with missing values are\n    assigned to the left or right child consequently. If no missing values\n    were encountered for a given feature during training, then samples with\n    missing values are mapped to whichever child has the most samples.\n\n    This implementation is inspired by\n    `LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n    Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n    .. versionadded:: 0.21\n\n    Parameters\n    ----------\n    loss : {'log_loss'}, default='log_loss'\n        The loss function to use in the boosting process.\n\n        For binary classification problems, 'log_loss' is also known as logistic loss,\n        binomial deviance or binary crossentropy. Internally, the model fits one tree\n        per boosting iteration and uses the logistic sigmoid function (expit) as\n        inverse link function to compute the predicted positive class probability.\n\n        For multiclass classification problems, 'log_loss' is also known as multinomial\n        deviance or categorical crossentropy. Internally, the model fits one tree per\n        boosting iteration and per class and uses the softmax function as inverse link\n        function to compute the predicted probabilities of the classes.\n\n    learning_rate : float, default=0.1\n        The learning rate, also known as *shrinkage*. This is used as a\n        multiplicative factor for the leaves values. Use ``1`` for no\n        shrinkage.\n    max_iter : int, default=100\n        The maximum number of iterations of the boosting process, i.e. the\n        maximum number of trees for binary classification. For multiclass\n        classification, `n_classes` trees per iteration are built.\n    max_leaf_nodes : int or None, default=31\n        The maximum number of leaves for each tree. Must be strictly greater\n        than 1. If None, there is no maximum limit.\n    max_depth : int or None, default=None\n        The maximum depth of each tree. The depth of a tree is the number of\n        edges to go from the root to the deepest leaf.\n        Depth isn't constrained by default.\n    min_samples_leaf : int, default=20\n        The minimum number of samples per leaf. For small datasets with less\n        than a few hundred samples, it is recommended to lower this value\n        since only very shallow trees would be built.\n    l2_regularization : float, default=0\n        The L2 regularization parameter penalizing leaves with small hessians.\n        Use ``0`` for no regularization (default).\n    max_features : float, default=1.0\n        Proportion of randomly chosen features in each and every node split.\n        This is a form of regularization, smaller values make the trees weaker\n        learners and might prevent overfitting.\n        If interaction constraints from `interaction_cst` are present, only allowed\n        features are taken into account for the subsampling.\n\n        .. versionadded:: 1.4\n\n    max_bins : int, default=255\n        The maximum number of bins to use for non-missing values. Before\n        training, each feature of the input array `X` is binned into\n        integer-valued bins, which allows for a much faster training stage.\n        Features with a small number of unique values may use less than\n        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n        is always reserved for missing values. Must be no larger than 255.\n    categorical_features : array-like of {bool, int, str} of shape (n_features) \\\n            or shape (n_categorical_features,), default=None\n        Indicates the categorical features.\n\n        - None : no feature will be considered categorical.\n        - boolean array-like : boolean mask indicating categorical features.\n        - integer array-like : integer indices indicating categorical\n          features.\n        - str array-like: names of categorical features (assuming the training\n          data has feature names).\n        - `\"from_dtype\"`: dataframe columns with dtype \"category\" are\n          considered to be categorical features. The input must be an object\n          exposing a ``__dataframe__`` method such as pandas or polars\n          DataFrames to use this feature.\n\n        For each categorical feature, there must be at most `max_bins` unique\n        categories. Negative values for categorical features encoded as numeric\n        dtypes are treated as missing values. All categorical values are\n        converted to floating point numbers. This means that categorical values\n        of 1.0 and 1 are treated as the same category.\n\n        Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n        .. versionadded:: 0.24\n\n        .. versionchanged:: 1.2\n           Added support for feature names.\n\n        .. versionchanged:: 1.4\n           Added `\"from_dtype\"` option. The default will change to `\"from_dtype\"` in\n           v1.6.\n\n    monotonic_cst : array-like of int of shape (n_features) or dict, default=None\n        Monotonic constraint to enforce on each feature are specified using the\n        following integer values:\n\n        - 1: monotonic increase\n        - 0: no constraint\n        - -1: monotonic decrease\n\n        If a dict with str keys, map feature to monotonic constraints by name.\n        If an array, the features are mapped to constraints by position. See\n        :ref:`monotonic_cst_features_names` for a usage example.\n\n        The constraints are only valid for binary classifications and hold\n        over the probability of the positive class.\n        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n        .. versionadded:: 0.23\n\n        .. versionchanged:: 1.2\n           Accept dict of constraints with feature names as keys.\n\n    interaction_cst : {\"pairwise\", \"no_interactions\"} or sequence of lists/tuples/sets \\\n            of int, default=None\n        Specify interaction constraints, the sets of features which can\n        interact with each other in child node splits.\n\n        Each item specifies the set of feature indices that are allowed\n        to interact with each other. If there are more features than\n        specified in these constraints, they are treated as if they were\n        specified as an additional set.\n\n        The strings \"pairwise\" and \"no_interactions\" are shorthands for\n        allowing only pairwise or no interactions, respectively.\n\n        For instance, with 5 features in total, `interaction_cst=[{0, 1}]`\n        is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`,\n        and specifies that each branch of a tree will either only split\n        on features 0 and 1 or only split on features 2, 3 and 4.\n\n        .. versionadded:: 1.2\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble. For results to be valid, the\n        estimator should be re-trained on the same data only.\n        See :term:`the Glossary <warm_start>`.\n    early_stopping : 'auto' or bool, default='auto'\n        If 'auto', early stopping is enabled if the sample size is larger than\n        10000. If True, early stopping is enabled, otherwise early stopping is\n        disabled.\n\n        .. versionadded:: 0.23\n\n    scoring : str or callable or None, default='loss'\n        Scoring parameter to use for early stopping. It can be a single\n        string (see :ref:`scoring_parameter`) or a callable (see\n        :ref:`scoring`). If None, the estimator's default scorer\n        is used. If ``scoring='loss'``, early stopping is checked\n        w.r.t the loss value. Only used if early stopping is performed.\n    validation_fraction : int or float or None, default=0.1\n        Proportion (or absolute size) of training data to set aside as\n        validation data for early stopping. If None, early stopping is done on\n        the training data. Only used if early stopping is performed.\n    n_iter_no_change : int, default=10\n        Used to determine when to \"early stop\". The fitting process is\n        stopped when none of the last ``n_iter_no_change`` scores are better\n        than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n        tolerance. Only used if early stopping is performed.\n    tol : float, default=1e-7\n        The absolute tolerance to use when comparing scores. The higher the\n        tolerance, the more likely we are to early stop: higher tolerance\n        means that it will be harder for subsequent iterations to be\n        considered an improvement upon the reference score.\n    verbose : int, default=0\n        The verbosity level. If not zero, print some information about the\n        fitting process.\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the subsampling in the\n        binning process, and the train/validation data split if early stopping\n        is enabled.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form `{class_label: weight}`.\n        If not given, all classes are supposed to have weight one.\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as `n_samples / (n_classes * np.bincount(y))`.\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if `sample_weight` is specified.\n\n        .. versionadded:: 1.2\n\n    Attributes\n    ----------\n    classes_ : array, shape = (n_classes,)\n        Class labels.\n    do_early_stopping_ : bool\n        Indicates whether early stopping is used during training.\n    n_iter_ : int\n        The number of iterations as selected by early stopping, depending on\n        the `early_stopping` parameter. Otherwise it corresponds to max_iter.\n    n_trees_per_iteration_ : int\n        The number of tree that are built at each iteration. This is equal to 1\n        for binary classification, and to ``n_classes`` for multiclass\n        classification.\n    train_score_ : ndarray, shape (n_iter_+1,)\n        The scores at each iteration on the training data. The first entry\n        is the score of the ensemble before the first iteration. Scores are\n        computed according to the ``scoring`` parameter. If ``scoring`` is\n        not 'loss', scores are computed on a subset of at most 10 000\n        samples. Empty if no early stopping.\n    validation_score_ : ndarray, shape (n_iter_+1,)\n        The scores at each iteration on the held-out validation data. The\n        first entry is the score of the ensemble before the first iteration.\n        Scores are computed according to the ``scoring`` parameter. Empty if\n        no early stopping or if ``validation_fraction`` is None.\n    is_categorical_ : ndarray, shape (n_features, ) or None\n        Boolean mask for the categorical features. ``None`` if there are no\n        categorical features.\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    GradientBoostingClassifier : Exact gradient boosting method that does not\n        scale as good on datasets with a large number of samples.\n    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n    RandomForestClassifier : A meta-estimator that fits a number of decision\n        tree classifiers on various sub-samples of the dataset and uses\n        averaging to improve the predictive accuracy and control over-fitting.\n    AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n        on the original dataset and then fits additional copies of the\n        classifier on the same dataset where the weights of incorrectly\n        classified instances are adjusted such that subsequent classifiers\n        focus more on difficult cases.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import HistGradientBoostingClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = HistGradientBoostingClassifier().fit(X, y)\n    >>> clf.score(X, y)\n    1.0\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **BaseHistGradientBoosting._parameter_constraints,\n        \"loss\": [StrOptions({\"log_loss\"}), BaseLoss],\n        \"class_weight\": [dict, StrOptions({\"balanced\"}), None],\n    }\n\n    def __init__(\n        self,\n        loss=\"log_loss\",\n        *,\n        learning_rate=0.1,\n        max_iter=100,\n        max_leaf_nodes=31,\n        max_depth=None,\n        min_samples_leaf=20,\n        l2_regularization=0.0,\n        max_features=1.0,\n        max_bins=255,\n        categorical_features=\"warn\",\n        monotonic_cst=None,\n        interaction_cst=None,\n        warm_start=False,\n        early_stopping=\"auto\",\n        scoring=\"loss\",\n        validation_fraction=0.1,\n        n_iter_no_change=10,\n        tol=1e-7,\n        verbose=0,\n        random_state=None,\n        class_weight=None,\n    ):\n        super(HistGradientBoostingClassifier, self).__init__(\n            loss=loss,\n            learning_rate=learning_rate,\n            max_iter=max_iter,\n            max_leaf_nodes=max_leaf_nodes,\n            max_depth=max_depth,\n            min_samples_leaf=min_samples_leaf,\n            l2_regularization=l2_regularization,\n            max_features=max_features,\n            max_bins=max_bins,\n            categorical_features=categorical_features,\n            monotonic_cst=monotonic_cst,\n            interaction_cst=interaction_cst,\n            warm_start=warm_start,\n            early_stopping=early_stopping,\n            scoring=scoring,\n            validation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change,\n            tol=tol,\n            verbose=verbose,\n            random_state=random_state,\n        )\n        self.class_weight = class_weight\n\n    def _finalize_sample_weight(self, sample_weight, y):\n        \"\"\"Adjust sample_weights with class_weights.\"\"\"\n        if self.class_weight is None:\n            return sample_weight\n\n        expanded_class_weight = compute_sample_weight(self.class_weight, y)\n\n        if sample_weight is not None:\n            return sample_weight * expanded_class_weight\n        else:\n            return expanded_class_weight\n\n    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        y : ndarray, shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        # TODO: This could be done in parallel\n        raw_predictions = self._raw_predict(X)\n        if raw_predictions.shape[1] == 1:\n            # np.argmax([0.5, 0.5]) is 0, not 1. Therefore \"> 0\" not \">= 0\" to be\n            # consistent with the multiclass case.\n            encoded_classes = (raw_predictions.ravel() > 0).astype(int)\n        else:\n            encoded_classes = np.argmax(raw_predictions, axis=1)\n        return self.classes_[encoded_classes]\n\n    def staged_predict(self, X):\n        \"\"\"Predict classes at each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        .. versionadded:: 0.24\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted classes of the input samples, for each iteration.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            if raw_predictions.shape[1] == 1:\n                # np.argmax([0, 0]) is 0, not 1, therefor \"> 0\" not \">= 0\"\n                encoded_classes = (raw_predictions.ravel() > 0).astype(int)\n            else:\n                encoded_classes = np.argmax(raw_predictions, axis=1)\n            yield self.classes_.take(encoded_classes, axis=0)\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        p : ndarray, shape (n_samples, n_classes)\n            The class probabilities of the input samples.\n        \"\"\"\n        raw_predictions = self._raw_predict(X)\n        return self._loss.predict_proba(raw_predictions)\n\n    def staged_predict_proba(self, X):\n        \"\"\"Predict class probabilities at each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted class probabilities of the input samples,\n            for each iteration.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            yield self._loss.predict_proba(raw_predictions)\n\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        decision : ndarray, shape (n_samples,) or \\\n                (n_samples, n_trees_per_iteration)\n            The raw predicted values (i.e. the sum of the trees leaves) for\n            each sample. n_trees_per_iteration is equal to the number of\n            classes in multiclass classification.\n        \"\"\"\n        decision = self._raw_predict(X)\n        if decision.shape[1] == 1:\n            decision = decision.ravel()\n        return decision\n\n    def staged_decision_function(self, X):\n        \"\"\"Compute decision function of ``X`` for each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        ------\n        decision : generator of ndarray of shape (n_samples,) or \\\n                (n_samples, n_trees_per_iteration)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        for staged_decision in self._staged_raw_predict(X):\n            if staged_decision.shape[1] == 1:\n                staged_decision = staged_decision.ravel()\n            yield staged_decision\n\n    def _encode_y(self, y):\n        # encode classes into 0 ... n_classes - 1 and sets attributes classes_\n        # and n_trees_per_iteration_\n        check_classification_targets(y)\n\n        label_encoder = LabelEncoder()\n        encoded_y = label_encoder.fit_transform(y)\n        self.classes_ = label_encoder.classes_\n        n_classes = self.classes_.shape[0]\n        # only 1 tree for binary classification. For multiclass classification,\n        # we build 1 tree per class.\n        self.n_trees_per_iteration_ = 1 if n_classes <= 2 else n_classes\n        encoded_y = encoded_y.astype(Y_DTYPE, copy=False)\n        return encoded_y\n\n    def _get_loss(self, sample_weight):\n        # At this point self.loss == \"log_loss\"\n        if self.n_trees_per_iteration_ == 1:\n            return HalfBinomialLoss(sample_weight=sample_weight)\n        else:\n            return HalfMultinomialLoss(\n                sample_weight=sample_weight, n_classes=self.n_trees_per_iteration_\n            )\n", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_hist_gradient_boosting/grower.py", "fn_id": "", "content": "class TreeNode:\n    \"\"\"Tree Node class used in TreeGrower.\n\n    This isn't used for prediction purposes, only for training (see\n    TreePredictor).\n\n    Parameters\n    ----------\n    depth : int\n        The depth of the node, i.e. its distance from the root.\n    sample_indices : ndarray of shape (n_samples_at_node,), dtype=np.uint32\n        The indices of the samples at the node.\n    partition_start : int\n        start position of the node's sample_indices in splitter.partition.\n    partition_stop : int\n        stop position of the node's sample_indices in splitter.partition.\n    sum_gradients : float\n        The sum of the gradients of the samples at the node.\n    sum_hessians : float\n        The sum of the hessians of the samples at the node.\n\n    Attributes\n    ----------\n    depth : int\n        The depth of the node, i.e. its distance from the root.\n    sample_indices : ndarray of shape (n_samples_at_node,), dtype=np.uint32\n        The indices of the samples at the node.\n    sum_gradients : float\n        The sum of the gradients of the samples at the node.\n    sum_hessians : float\n        The sum of the hessians of the samples at the node.\n    split_info : SplitInfo or None\n        The result of the split evaluation.\n    is_leaf : bool\n        True if node is a leaf\n    left_child : TreeNode or None\n        The left child of the node. None for leaves.\n    right_child : TreeNode or None\n        The right child of the node. None for leaves.\n    value : float or None\n        The value of the leaf, as computed in finalize_leaf(). None for\n        non-leaf nodes.\n    partition_start : int\n        start position of the node's sample_indices in splitter.partition.\n    partition_stop : int\n        stop position of the node's sample_indices in splitter.partition.\n    allowed_features : None or ndarray, dtype=int\n        Indices of features allowed to split for children.\n    interaction_cst_indices : None or list of ints\n        Indices of the interaction sets that have to be applied on splits of\n        child nodes. The fewer sets the stronger the constraint as fewer sets\n        contain fewer features.\n    children_lower_bound : float\n    children_upper_bound : float\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        depth,\n        sample_indices,\n        partition_start,\n        partition_stop,\n        sum_gradients,\n        sum_hessians,\n        value=None,\n    ):\n        self.depth = depth\n        self.sample_indices = sample_indices\n        self.n_samples = sample_indices.shape[0]\n        self.sum_gradients = sum_gradients\n        self.sum_hessians = sum_hessians\n        self.value = value\n        self.is_leaf = False\n        self.allowed_features = None\n        self.interaction_cst_indices = None\n        self.set_children_bounds(float(\"-inf\"), float(\"+inf\"))\n        self.split_info = None\n        self.left_child = None\n        self.right_child = None\n        self.histograms = None\n        # start and stop indices of the node in the splitter.partition\n        # array. Concretely,\n        # self.sample_indices = view(self.splitter.partition[start:stop])\n        # Please see the comments about splitter.partition and\n        # splitter.split_indices for more info about this design.\n        # These 2 attributes are only used in _update_raw_prediction, because we\n        # need to iterate over the leaves and I don't know how to efficiently\n        # store the sample_indices views because they're all of different sizes.\n        self.partition_start = partition_start\n        self.partition_stop = partition_stop\n\n    def set_children_bounds(self, lower, upper):\n        \"\"\"Set children values bounds to respect monotonic constraints.\"\"\"\n\n        # These are bounds for the node's *children* values, not the node's\n        # value. The bounds are used in the splitter when considering potential\n        # left and right child.\n        self.children_lower_bound = lower\n        self.children_upper_bound = upper\n\n    def __lt__(self, other_node):\n        \"\"\"Comparison for priority queue.\n\n        Nodes with high gain are higher priority than nodes with low gain.\n\n        heapq.heappush only need the '<' operator.\n        heapq.heappop take the smallest item first (smaller is higher\n        priority).\n\n        Parameters\n        ----------\n        other_node : TreeNode\n            The node to compare with.\n        \"\"\"\n        return self.split_info.gain > other_node.split_info.gain\n", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._hist_gradient_boosting.grower/TreeNode", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_stacking.py", "fn_id": "", "content": "class StackingClassifier(_RoutingNotSupportedMixin, ClassifierMixin, _BaseStacking):\n    \"\"\"Stack of estimators with a final classifier.\n\n    Stacked generalization consists in stacking the output of individual\n    estimator and use a classifier to compute the final prediction. Stacking\n    allows to use the strength of each individual estimator by using their\n    output as input of a final estimator.\n\n    Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n    is trained using cross-validated predictions of the base estimators using\n    `cross_val_predict`.\n\n    Read more in the :ref:`User Guide <stacking>`.\n\n    .. versionadded:: 0.22\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator)\n        Base estimators which will be stacked together. Each element of the\n        list is defined as a tuple of string (i.e. name) and an estimator\n        instance. An estimator can be set to 'drop' using `set_params`.\n\n        The type of estimator is generally expected to be a classifier.\n        However, one can pass a regressor for some use case (e.g. ordinal\n        regression).\n\n    final_estimator : estimator, default=None\n        A classifier which will be used to combine the base estimators.\n        The default classifier is a\n        :class:`~sklearn.linear_model.LogisticRegression`.\n\n    cv : int, cross-validation generator, iterable, or \"prefit\", default=None\n        Determines the cross-validation splitting strategy used in\n        `cross_val_predict` to train `final_estimator`. Possible inputs for\n        cv are:\n\n        * None, to use the default 5-fold cross validation,\n        * integer, to specify the number of folds in a (Stratified) KFold,\n        * An object to be used as a cross-validation generator,\n        * An iterable yielding train, test splits,\n        * `\"prefit\"` to assume the `estimators` are prefit. In this case, the\n          estimators will not be refitted.\n\n        For integer/None inputs, if the estimator is a classifier and y is\n        either binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used.\n        In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n        These splitters are instantiated with `shuffle=False` so the splits\n        will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        If \"prefit\" is passed, it is assumed that all `estimators` have\n        been fitted already. The `final_estimator_` is trained on the `estimators`\n        predictions on the full training set and are **not** cross validated\n        predictions. Please note that if the models have been trained on the same\n        data to train the stacking model, there is a very high risk of overfitting.\n\n        .. versionadded:: 1.1\n            The 'prefit' option was added in 1.1\n\n        .. note::\n           A larger number of split will provide no benefits if the number\n           of training samples is large enough. Indeed, the training time\n           will increase. ``cv`` is not used for model evaluation but for\n           prediction.\n\n    stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'}, \\\n            default='auto'\n        Methods called for each base estimator. It can be:\n\n        * if 'auto', it will try to invoke, for each estimator,\n          `'predict_proba'`, `'decision_function'` or `'predict'` in that\n          order.\n        * otherwise, one of `'predict_proba'`, `'decision_function'` or\n          `'predict'`. If the method is not implemented by the estimator, it\n          will raise an error.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel all `estimators` `fit`.\n        `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n        using all processors. See Glossary for more details.\n\n    passthrough : bool, default=False\n        When False, only the predictions of estimators will be used as\n        training data for `final_estimator`. When True, the\n        `final_estimator` is trained on the predictions as well as the\n        original training data.\n\n    verbose : int, default=0\n        Verbosity level.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,) or list of ndarray if `y` \\\n        is of type `\"multilabel-indicator\"`.\n        Class labels.\n\n    estimators_ : list of estimators\n        The elements of the `estimators` parameter, having been fitted on the\n        training data. If an estimator has been set to `'drop'`, it\n        will not appear in `estimators_`. When `cv=\"prefit\"`, `estimators_`\n        is set to `estimators` and is not fitted again.\n\n    named_estimators_ : :class:`~sklearn.utils.Bunch`\n        Attribute to access any fitted sub-estimators by name.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying classifier exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimators expose such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    final_estimator_ : estimator\n        The classifier which predicts given the output of `estimators_`.\n\n    stack_method_ : list of str\n        The method used by each base estimator.\n\n    See Also\n    --------\n    StackingRegressor : Stack of estimators with a final regressor.\n\n    Notes\n    -----\n    When `predict_proba` is used by each estimator (i.e. most of the time for\n    `stack_method='auto'` or specifically for `stack_method='predict_proba'`),\n    The first column predicted by each estimator will be dropped in the case\n    of a binary classification problem. Indeed, both feature will be perfectly\n    collinear.\n\n    In some cases (e.g. ordinal regression), one can pass regressors as the\n    first layer of the :class:`StackingClassifier`. However, note that `y` will\n    be internally encoded in a numerically increasing order or lexicographic\n    order. If this ordering is not adequate, one should manually numerically\n    encode the classes in the desired order.\n\n    References\n    ----------\n    .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n       (1992): 241-259.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.svm import LinearSVC\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.ensemble import StackingClassifier\n    >>> X, y = load_iris(return_X_y=True)\n    >>> estimators = [\n    ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n    ...     ('svr', make_pipeline(StandardScaler(),\n    ...                           LinearSVC(random_state=42)))\n    ... ]\n    >>> clf = StackingClassifier(\n    ...     estimators=estimators, final_estimator=LogisticRegression()\n    ... )\n    >>> from sklearn.model_selection import train_test_split\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, stratify=y, random_state=42\n    ... )\n    >>> clf.fit(X_train, y_train).score(X_test, y_test)\n    0.9...\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BaseStacking._parameter_constraints,\n        \"stack_method\": [\n            StrOptions({\"auto\", \"predict_proba\", \"decision_function\", \"predict\"})\n        ],\n    }\n\n    def __init__(\n        self,\n        estimators,\n        final_estimator=None,\n        *,\n        cv=None,\n        stack_method=\"auto\",\n        n_jobs=None,\n        passthrough=False,\n        verbose=0,\n    ):\n        super().__init__(\n            estimators=estimators,\n            final_estimator=final_estimator,\n            cv=cv,\n            stack_method=stack_method,\n            n_jobs=n_jobs,\n            passthrough=passthrough,\n            verbose=verbose,\n        )\n\n    def _validate_final_estimator(self):\n        self._clone_final_estimator(default=LogisticRegression())\n        if not is_classifier(self.final_estimator_):\n            raise ValueError(\n                \"'final_estimator' parameter should be a classifier. Got {}\".format(\n                    self.final_estimator_\n                )\n            )\n\n    def _validate_estimators(self):\n        \"\"\"Overload the method of `_BaseHeterogeneousEnsemble` to be more\n        lenient towards the type of `estimators`.\n\n        Regressors can be accepted for some cases such as ordinal regression.\n        \"\"\"\n        if len(self.estimators) == 0:\n            raise ValueError(\n                \"Invalid 'estimators' attribute, 'estimators' should be a \"\n                \"non-empty list of (string, estimator) tuples.\"\n            )\n        names, estimators = zip(*self.estimators)\n        self._validate_names(names)\n\n        has_estimator = any(est != \"drop\" for est in estimators)\n        if not has_estimator:\n            raise ValueError(\n                \"All estimators are dropped. At least one is required \"\n                \"to be an estimator.\"\n            )\n\n        return names, estimators\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values. Note that `y` will be internally encoded in\n            numerically increasing order or lexicographic order. If the order\n            matter (e.g. for ordinal regression), one should numerically encode\n            the target `y` before calling :term:`fit`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n        Returns\n        -------\n        self : object\n            Returns a fitted instance of estimator.\n        \"\"\"\n        _raise_for_unsupported_routing(self, \"fit\", sample_weight=sample_weight)\n        check_classification_targets(y)\n        if type_of_target(y) == \"multilabel-indicator\":\n            self._label_encoder = [LabelEncoder().fit(yk) for yk in y.T]\n            self.classes_ = [le.classes_ for le in self._label_encoder]\n            y_encoded = np.array(\n                [\n                    self._label_encoder[target_idx].transform(target)\n                    for target_idx, target in enumerate(y.T)\n                ]\n            ).T\n        else:\n            self._label_encoder = LabelEncoder().fit(y)\n            self.classes_ = self._label_encoder.classes_\n            y_encoded = self._label_encoder.transform(y)\n        return super().fit(X, y_encoded, sample_weight)\n\n    @available_if(_estimator_has(\"predict\"))\n    def predict(self, X, **predict_params):\n        \"\"\"Predict target for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        **predict_params : dict of str -> obj\n            Parameters to the `predict` called by the `final_estimator`. Note\n            that this may be used to return uncertainties from some estimators\n            with `return_std` or `return_cov`. Be aware that it will only\n            accounts for uncertainty in the final estimator.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n            Predicted targets.\n        \"\"\"\n        y_pred = super().predict(X, **predict_params)\n        if isinstance(self._label_encoder, list):\n            # Handle the multilabel-indicator case\n            y_pred = np.array(\n                [\n                    self._label_encoder[target_idx].inverse_transform(target)\n                    for target_idx, target in enumerate(y_pred.T)\n                ]\n            ).T\n        else:\n            y_pred = self._label_encoder.inverse_transform(y_pred)\n        return y_pred\n\n    @available_if(_estimator_has(\"predict_proba\"))\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for `X` using the final estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        probabilities : ndarray of shape (n_samples, n_classes) or \\\n            list of ndarray of shape (n_output,)\n            The class probabilities of the input samples.\n        \"\"\"\n        check_is_fitted(self)\n        y_pred = self.final_estimator_.predict_proba(self.transform(X))\n\n        if isinstance(self._label_encoder, list):\n            # Handle the multilabel-indicator cases\n            y_pred = np.array([preds[:, 0] for preds in y_pred]).T\n        return y_pred\n\n    @available_if(_estimator_has(\"decision_function\"))\n    def decision_function(self, X):\n        \"\"\"Decision function for samples in `X` using the final estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        decisions : ndarray of shape (n_samples,), (n_samples, n_classes), \\\n            or (n_samples, n_classes * (n_classes-1) / 2)\n            The decision function computed the final estimator.\n        \"\"\"\n        check_is_fitted(self)\n        return self.final_estimator_.decision_function(self.transform(X))\n\n    def transform(self, X):\n        \"\"\"Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        y_preds : ndarray of shape (n_samples, n_estimators) or \\\n                (n_samples, n_classes * n_estimators)\n            Prediction outputs for each estimator.\n        \"\"\"\n        return self._transform(X)\n\n    def _sk_visual_block_(self):\n        # If final_estimator's default changes then this should be\n        # updated.\n        if self.final_estimator is None:\n            final_estimator = LogisticRegression()\n        else:\n            final_estimator = self.final_estimator\n        return super()._sk_visual_block_with_final_estimator(final_estimator)\n", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._stacking/StackingClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_voting.py", "fn_id": "", "content": "class VotingClassifier(ClassifierMixin, _BaseVoting):\n    \"\"\"Soft Voting/Majority Rule classifier for unfitted estimators.\n\n    Read more in the :ref:`User Guide <voting_classifier>`.\n\n    .. versionadded:: 0.17\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator) tuples\n        Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n        of those original estimators that will be stored in the class attribute\n        ``self.estimators_``. An estimator can be set to ``'drop'`` using\n        :meth:`set_params`.\n\n        .. versionchanged:: 0.21\n            ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n            support was removed in 0.24.\n\n    voting : {'hard', 'soft'}, default='hard'\n        If 'hard', uses predicted class labels for majority rule voting.\n        Else if 'soft', predicts the class label based on the argmax of\n        the sums of the predicted probabilities, which is recommended for\n        an ensemble of well-calibrated classifiers.\n\n    weights : array-like of shape (n_classifiers,), default=None\n        Sequence of weights (`float` or `int`) to weight the occurrences of\n        predicted class labels (`hard` voting) or class probabilities\n        before averaging (`soft` voting). Uses uniform weights if `None`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for ``fit``.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    flatten_transform : bool, default=True\n        Affects shape of transform output only when voting='soft'\n        If voting='soft' and flatten_transform=True, transform method returns\n        matrix with shape (n_samples, n_classifiers * n_classes). If\n        flatten_transform=False, it returns\n        (n_classifiers, n_samples, n_classes).\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting will be printed as it\n        is completed.\n\n        .. versionadded:: 0.23\n\n    Attributes\n    ----------\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators as defined in ``estimators``\n        that are not 'drop'.\n\n    named_estimators_ : :class:`~sklearn.utils.Bunch`\n        Attribute to access any fitted sub-estimators by name.\n\n        .. versionadded:: 0.20\n\n    le_ : :class:`~sklearn.preprocessing.LabelEncoder`\n        Transformer used to encode the labels during fit and decode during\n        prediction.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying classifier exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimators expose such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    VotingRegressor : Prediction voting regressor.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n    >>> clf1 = LogisticRegression(random_state=1)\n    >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    >>> clf3 = GaussianNB()\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> eclf1 = VotingClassifier(estimators=[\n    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n    >>> eclf1 = eclf1.fit(X, y)\n    >>> print(eclf1.predict(X))\n    [1 1 1 2 2 2]\n    >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n    ...                eclf1.named_estimators_['lr'].predict(X))\n    True\n    >>> eclf2 = VotingClassifier(estimators=[\n    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    ...         voting='soft')\n    >>> eclf2 = eclf2.fit(X, y)\n    >>> print(eclf2.predict(X))\n    [1 1 1 2 2 2]\n\n    To drop an estimator, :meth:`set_params` can be used to remove it. Here we\n    dropped one of the estimators, resulting in 2 fitted estimators:\n\n    >>> eclf2 = eclf2.set_params(lr='drop')\n    >>> eclf2 = eclf2.fit(X, y)\n    >>> len(eclf2.estimators_)\n    2\n\n    Setting `flatten_transform=True` with `voting='soft'` flattens output shape of\n    `transform`:\n\n    >>> eclf3 = VotingClassifier(estimators=[\n    ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    ...        voting='soft', weights=[2,1,1],\n    ...        flatten_transform=True)\n    >>> eclf3 = eclf3.fit(X, y)\n    >>> print(eclf3.predict(X))\n    [1 1 1 2 2 2]\n    >>> print(eclf3.transform(X).shape)\n    (6, 6)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BaseVoting._parameter_constraints,\n        \"voting\": [StrOptions({\"hard\", \"soft\"})],\n        \"flatten_transform\": [\"boolean\"],\n    }\n\n    def __init__(\n        self,\n        estimators,\n        *,\n        voting=\"hard\",\n        weights=None,\n        n_jobs=None,\n        flatten_transform=True,\n        verbose=False,\n    ):\n        super().__init__(estimators=estimators)\n        self.voting = voting\n        self.weights = weights\n        self.n_jobs = n_jobs\n        self.flatten_transform = flatten_transform\n        self.verbose = verbose\n\n    @_fit_context(\n        # estimators in VotingClassifier.estimators are not validated yet\n        prefer_skip_nested_validation=False\n    )\n    # TODO(1.7): remove `sample_weight` from the signature after deprecation\n    # cycle; pop it from `fit_params` before the `_raise_for_params` check and\n    # reinsert later, for backwards compatibility\n    @_deprecate_positional_args(version=\"1.7\")\n    def fit(self, X, y, *, sample_weight=None, **fit_params):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n            .. versionadded:: 0.18\n\n        **fit_params : dict\n            Parameters to pass to the underlying estimators.\n\n            .. versionadded:: 1.5\n\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        _raise_for_params(fit_params, self, \"fit\")\n        y_type = type_of_target(y, input_name=\"y\")\n        if y_type in (\"unknown\", \"continuous\"):\n            # raise a specific ValueError for non-classification tasks\n            raise ValueError(\n                f\"Unknown label type: {y_type}. Maybe you are trying to fit a \"\n                \"classifier, which expects discrete classes on a \"\n                \"regression target with continuous values.\"\n            )\n        elif y_type not in (\"binary\", \"multiclass\"):\n            # raise a NotImplementedError for backward compatibility for non-supported\n            # classification tasks\n            raise NotImplementedError(\n                f\"{self.__class__.__name__} only supports binary or multiclass \"\n                \"classification. Multilabel and multi-output classification are not \"\n                \"supported.\"\n            )\n\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        transformed_y = self.le_.transform(y)\n\n        if sample_weight is not None:\n            fit_params[\"sample_weight\"] = sample_weight\n\n        return super().fit(X, transformed_y, **fit_params)\n\n    def predict(self, X):\n        \"\"\"Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        maj : array-like of shape (n_samples,)\n            Predicted class labels.\n        \"\"\"\n        check_is_fitted(self)\n        if self.voting == \"soft\":\n            maj = np.argmax(self.predict_proba(X), axis=1)\n\n        else:  # 'hard' voting\n            predictions = self._predict(X)\n            maj = np.apply_along_axis(\n                lambda x: np.argmax(np.bincount(x, weights=self._weights_not_none)),\n                axis=1,\n                arr=predictions,\n            )\n\n        maj = self.le_.inverse_transform(maj)\n\n        return maj\n\n    def _collect_probas(self, X):\n        \"\"\"Collect results from clf.predict calls.\"\"\"\n        return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n\n    def _check_voting(self):\n        if self.voting == \"hard\":\n            raise AttributeError(\n                f\"predict_proba is not available when voting={repr(self.voting)}\"\n            )\n        return True\n\n    @available_if(_check_voting)\n    def predict_proba(self, X):\n        \"\"\"Compute probabilities of possible outcomes for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        avg : array-like of shape (n_samples, n_classes)\n            Weighted average probability for each class per sample.\n        \"\"\"\n        check_is_fitted(self)\n        avg = np.average(\n            self._collect_probas(X), axis=0, weights=self._weights_not_none\n        )\n        return avg\n\n    def transform(self, X):\n        \"\"\"Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        probabilities_or_labels\n            If `voting='soft'` and `flatten_transform=True`:\n                returns ndarray of shape (n_samples, n_classifiers * n_classes),\n                being class probabilities calculated by each classifier.\n            If `voting='soft' and `flatten_transform=False`:\n                ndarray of shape (n_classifiers, n_samples, n_classes)\n            If `voting='hard'`:\n                ndarray of shape (n_samples, n_classifiers), being\n                class labels predicted by each classifier.\n        \"\"\"\n        check_is_fitted(self)\n\n        if self.voting == \"soft\":\n            probas = self._collect_probas(X)\n            if not self.flatten_transform:\n                return probas\n            return np.hstack(probas)\n\n        else:\n            return self._predict(X)\n\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.\n        \"\"\"\n        check_is_fitted(self, \"n_features_in_\")\n        if self.voting == \"soft\" and not self.flatten_transform:\n            raise ValueError(\n                \"get_feature_names_out is not supported when `voting='soft'` and \"\n                \"`flatten_transform=False`\"\n            )\n\n        _check_feature_names_in(self, input_features, generate_names=False)\n        class_name = self.__class__.__name__.lower()\n\n        active_names = [name for name, est in self.estimators if est != \"drop\"]\n\n        if self.voting == \"hard\":\n            return np.asarray(\n                [f\"{class_name}_{name}\" for name in active_names], dtype=object\n            )\n\n        # voting == \"soft\"\n        n_classes = len(self.classes_)\n        names_out = [\n            f\"{class_name}_{name}{i}\" for name in active_names for i in range(n_classes)\n        ]\n        return np.asarray(names_out, dtype=object)\n", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._voting/VotingClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_weight_boosting.py", "fn_id": "", "content": "class AdaBoostClassifier(\n    _RoutingNotSupportedMixin, ClassifierMixin, BaseWeightBoosting\n):\n    \"\"\"An AdaBoost classifier.\n\n    An AdaBoost [1]_ classifier is a meta-estimator that begins by fitting a\n    classifier on the original dataset and then fits additional copies of the\n    classifier on the same dataset but where the weights of incorrectly\n    classified instances are adjusted such that subsequent classifiers focus\n    more on difficult cases.\n\n    This class implements the algorithm based on [2]_.\n\n    Read more in the :ref:`User Guide <adaboost>`.\n\n    .. versionadded:: 0.14\n\n    Parameters\n    ----------\n    estimator : object, default=None\n        The base estimator from which the boosted ensemble is built.\n        Support for sample weighting is required, as well as proper\n        ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n        the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n        initialized with `max_depth=1`.\n\n        .. versionadded:: 1.2\n           `base_estimator` was renamed to `estimator`.\n\n    n_estimators : int, default=50\n        The maximum number of estimators at which boosting is terminated.\n        In case of perfect fit, the learning procedure is stopped early.\n        Values must be in the range `[1, inf)`.\n\n    learning_rate : float, default=1.0\n        Weight applied to each classifier at each boosting iteration. A higher\n        learning rate increases the contribution of each classifier. There is\n        a trade-off between the `learning_rate` and `n_estimators` parameters.\n        Values must be in the range `(0.0, inf)`.\n\n    algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n        ``estimator`` must support calculation of class probabilities.\n        If 'SAMME' then use the SAMME discrete boosting algorithm.\n        The SAMME.R algorithm typically converges faster than SAMME,\n        achieving a lower test error with fewer boosting iterations.\n\n        .. deprecated:: 1.4\n            `\"SAMME.R\"` is deprecated and will be removed in version 1.6.\n            '\"SAMME\"' will become the default.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given at each `estimator` at each\n        boosting iteration.\n        Thus, it is only used when `estimator` exposes a `random_state`.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_classes_ : int\n        The number of classes.\n\n    estimator_weights_ : ndarray of floats\n        Weights for each estimator in the boosted ensemble.\n\n    estimator_errors_ : ndarray of floats\n        Classification error for each estimator in the boosted\n        ensemble.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances if supported by the\n        ``estimator`` (when based on decision trees).\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n        regressor on the original dataset and then fits additional copies of\n        the regressor on the same dataset but where the weights of instances\n        are adjusted according to the error of the current prediction.\n\n    GradientBoostingClassifier : GB builds an additive model in a forward\n        stage-wise fashion. Regression trees are fit on the negative gradient\n        of the binomial or multinomial deviance loss function. Binary\n        classification is a special case where only a single regression tree is\n        induced.\n\n    sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n        method used for classification.\n        Creates a model that predicts the value of a target variable by\n        learning simple decision rules inferred from the data features.\n\n    References\n    ----------\n    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n           on-Line Learning and an Application to Boosting\", 1995.\n\n    .. [2] :doi:`J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class adaboost.\"\n           Statistics and its Interface 2.3 (2009): 349-360.\n           <10.4310/SII.2009.v2.n3.a8>`\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import AdaBoostClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0)\n    >>> clf.fit(X, y)\n    AdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)\n    >>> clf.predict([[0, 0, 0, 0]])\n    array([1])\n    >>> clf.score(X, y)\n    0.96...\n\n    For a detailed example of using AdaBoost to fit a sequence of DecisionTrees\n    as weaklearners, please refer to\n    :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_multiclass.py`.\n\n    For a detailed example of using AdaBoost to fit a non-linearly seperable\n    classification dataset composed of two Gaussian quantiles clusters, please\n    refer to :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_twoclass.py`.\n    \"\"\"\n\n    # TODO(1.6): Modify _parameter_constraints for \"algorithm\" to only check\n    # for \"SAMME\"\n    _parameter_constraints: dict = {\n        **BaseWeightBoosting._parameter_constraints,\n        \"algorithm\": [\n            StrOptions({\"SAMME\", \"SAMME.R\"}),\n        ],\n    }\n\n    # TODO(1.6): Change default \"algorithm\" value to \"SAMME\"\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        n_estimators=50,\n        learning_rate=1.0,\n        algorithm=\"SAMME.R\",\n        random_state=None,\n    ):\n        super().__init__(\n            estimator=estimator,\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            random_state=random_state,\n        )\n\n        self.algorithm = algorithm\n\n    def _validate_estimator(self):\n        \"\"\"Check the estimator and set the estimator_ attribute.\"\"\"\n        super()._validate_estimator(default=DecisionTreeClassifier(max_depth=1))\n\n        # TODO(1.6): Remove, as \"SAMME.R\" value for \"algorithm\" param will be\n        # removed in 1.6\n        # SAMME-R requires predict_proba-enabled base estimators\n        if self.algorithm != \"SAMME\":\n            warnings.warn(\n                (\n                    \"The SAMME.R algorithm (the default) is deprecated and will be\"\n                    \" removed in 1.6. Use the SAMME algorithm to circumvent this\"\n                    \" warning.\"\n                ),\n                FutureWarning,\n            )\n            if not hasattr(self.estimator_, \"predict_proba\"):\n                raise TypeError(\n                    \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\n                    \"that the weak learner supports the calculation of class \"\n                    \"probabilities with a predict_proba method.\\n\"\n                    \"Please change the base estimator or set \"\n                    \"algorithm='SAMME' instead.\"\n                )\n\n        if not has_fit_parameter(self.estimator_, \"sample_weight\"):\n            raise ValueError(\n                f\"{self.estimator.__class__.__name__} doesn't support sample_weight.\"\n            )\n\n    # TODO(1.6): Redefine the scope of the `_boost` and `_boost_discrete`\n    # functions to be the same since SAMME will be the default value for the\n    # \"algorithm\" parameter in version 1.6. Thus, a distinguishing function is\n    # no longer needed. (Or adjust code here, if another algorithm, shall be\n    # used instead of SAMME.R.)\n    def _boost(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost.\n\n        Perform a single boost according to the real multi-class SAMME.R\n        algorithm or to the discrete SAMME algorithm and return the updated\n        sample weights.\n\n        Parameters\n        ----------\n        iboost : int\n            The index of the current boost iteration.\n\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,)\n            The target values (class labels).\n\n        sample_weight : array-like of shape (n_samples,)\n            The current sample weights.\n\n        random_state : RandomState instance\n            The RandomState instance used if the base estimator accepts a\n            `random_state` attribute.\n\n        Returns\n        -------\n        sample_weight : array-like of shape (n_samples,) or None\n            The reweighted sample weights.\n            If None then boosting has terminated early.\n\n        estimator_weight : float\n            The weight for the current boost.\n            If None then boosting has terminated early.\n\n        estimator_error : float\n            The classification error for the current boost.\n            If None then boosting has terminated early.\n        \"\"\"\n        if self.algorithm == \"SAMME.R\":\n            return self._boost_real(iboost, X, y, sample_weight, random_state)\n\n        else:  # elif self.algorithm == \"SAMME\":\n            return self._boost_discrete(iboost, X, y, sample_weight, random_state)\n\n    # TODO(1.6): Remove function. The `_boost_real` function won't be used any\n    # longer, because the SAMME.R algorithm will be deprecated in 1.6.\n    def _boost_real(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n        estimator = self._make_estimator(random_state=random_state)\n\n        estimator.fit(X, y, sample_weight=sample_weight)\n\n        y_predict_proba = estimator.predict_proba(X)\n\n        if iboost == 0:\n            self.classes_ = getattr(estimator, \"classes_\", None)\n            self.n_classes_ = len(self.classes_)\n\n        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1), axis=0)\n\n        # Instances incorrectly classified\n        incorrect = y_predict != y\n\n        # Error fraction\n        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n\n        # Stop if classification is perfect\n        if estimator_error <= 0:\n            return sample_weight, 1.0, 0.0\n\n        # Construct y coding as described in Zhu et al [2]:\n        #\n        #    y_k = 1 if c == k else -1 / (K - 1)\n        #\n        # where K == n_classes_ and c, k in [0, K) are indices along the second\n        # axis of the y coding with c being the index corresponding to the true\n        # class label.\n        n_classes = self.n_classes_\n        classes = self.classes_\n        y_codes = np.array([-1.0 / (n_classes - 1), 1.0])\n        y_coding = y_codes.take(classes == y[:, np.newaxis])\n\n        # Displace zero probabilities so the log is defined.\n        # Also fix negative elements which may occur with\n        # negative sample weights.\n        proba = y_predict_proba  # alias for readability\n        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n\n        # Boost weight using multi-class AdaBoost SAMME.R alg\n        estimator_weight = (\n            -1.0\n            * self.learning_rate\n            * ((n_classes - 1.0) / n_classes)\n            * xlogy(y_coding, y_predict_proba).sum(axis=1)\n        )\n\n        # Only boost the weights if it will fit again\n        if not iboost == self.n_estimators - 1:\n            # Only boost positive weights\n            sample_weight *= np.exp(\n                estimator_weight * ((sample_weight > 0) | (estimator_weight < 0))\n            )\n\n        return sample_weight, 1.0, estimator_error\n\n    def _boost_discrete(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\n        estimator = self._make_estimator(random_state=random_state)\n\n        estimator.fit(X, y, sample_weight=sample_weight)\n\n        y_predict = estimator.predict(X)\n\n        if iboost == 0:\n            self.classes_ = getattr(estimator, \"classes_\", None)\n            self.n_classes_ = len(self.classes_)\n\n        # Instances incorrectly classified\n        incorrect = y_predict != y\n\n        # Error fraction\n        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n\n        # Stop if classification is perfect\n        if estimator_error <= 0:\n            return sample_weight, 1.0, 0.0\n\n        n_classes = self.n_classes_\n\n        # Stop if the error is at least as bad as random guessing\n        if estimator_error >= 1.0 - (1.0 / n_classes):\n            self.estimators_.pop(-1)\n            if len(self.estimators_) == 0:\n                raise ValueError(\n                    \"BaseClassifier in AdaBoostClassifier \"\n                    \"ensemble is worse than random, ensemble \"\n                    \"can not be fit.\"\n                )\n            return None, None, None\n\n        # Boost weight using multi-class AdaBoost SAMME alg\n        estimator_weight = self.learning_rate * (\n            np.log((1.0 - estimator_error) / estimator_error) + np.log(n_classes - 1.0)\n        )\n\n        # Only boost the weights if it will fit again\n        if not iboost == self.n_estimators - 1:\n            # Only boost positive weights\n            sample_weight = np.exp(\n                np.log(sample_weight)\n                + estimator_weight * incorrect * (sample_weight > 0)\n            )\n\n        return sample_weight, estimator_weight, estimator_error\n\n    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        The predicted class of an input sample is computed as the weighted mean\n        prediction of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        pred = self.decision_function(X)\n\n        if self.n_classes_ == 2:\n            return self.classes_.take(pred > 0, axis=0)\n\n        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n\n    def staged_predict(self, X):\n        \"\"\"Return staged predictions for X.\n\n        The predicted class of an input sample is computed as the weighted mean\n        prediction of the classifiers in the ensemble.\n\n        This generator method yields the ensemble prediction after each\n        iteration of boosting and therefore allows monitoring, such as to\n        determine the prediction on a test set after each boost.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n        classes = self.classes_\n\n        if n_classes == 2:\n            for pred in self.staged_decision_function(X):\n                yield np.array(classes.take(pred > 0, axis=0))\n\n        else:\n            for pred in self.staged_decision_function(X):\n                yield np.array(classes.take(np.argmax(pred, axis=1), axis=0))\n\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        score : ndarray of shape of (n_samples, k)\n            The decision function of the input samples. The order of\n            outputs is the same as that of the :term:`classes_` attribute.\n            Binary classification is a special cases with ``k == 1``,\n            otherwise ``k==n_classes``. For binary classification,\n            values closer to -1 or 1 mean more like the first or second\n            class in ``classes_``, respectively.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n        classes = self.classes_[:, np.newaxis]\n\n        # TODO(1.6): Remove, because \"algorithm\" param will be deprecated in 1.6\n        if self.algorithm == \"SAMME.R\":\n            # The weights are all 1. for SAMME.R\n            pred = sum(\n                _samme_proba(estimator, n_classes, X) for estimator in self.estimators_\n            )\n        else:  # self.algorithm == \"SAMME\"\n            pred = sum(\n                np.where(\n                    (estimator.predict(X) == classes).T,\n                    w,\n                    -1 / (n_classes - 1) * w,\n                )\n                for estimator, w in zip(self.estimators_, self.estimator_weights_)\n            )\n\n        pred /= self.estimator_weights_.sum()\n        if n_classes == 2:\n            pred[:, 0] *= -1\n            return pred.sum(axis=1)\n        return pred\n\n    def staged_decision_function(self, X):\n        \"\"\"Compute decision function of ``X`` for each boosting iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each boosting iteration.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        ------\n        score : generator of ndarray of shape (n_samples, k)\n            The decision function of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n            Binary classification is a special cases with ``k == 1``,\n            otherwise ``k==n_classes``. For binary classification,\n            values closer to -1 or 1 mean more like the first or second\n            class in ``classes_``, respectively.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n        classes = self.classes_[:, np.newaxis]\n        pred = None\n        norm = 0.0\n\n        for weight, estimator in zip(self.estimator_weights_, self.estimators_):\n            norm += weight\n\n            # TODO(1.6): Remove, because \"algorithm\" param will be deprecated in\n            # 1.6\n            if self.algorithm == \"SAMME.R\":\n                # The weights are all 1. for SAMME.R\n                current_pred = _samme_proba(estimator, n_classes, X)\n            else:  # elif self.algorithm == \"SAMME\":\n                current_pred = np.where(\n                    (estimator.predict(X) == classes).T,\n                    weight,\n                    -1 / (n_classes - 1) * weight,\n                )\n\n            if pred is None:\n                pred = current_pred\n            else:\n                pred += current_pred\n\n            if n_classes == 2:\n                tmp_pred = np.copy(pred)\n                tmp_pred[:, 0] *= -1\n                yield (tmp_pred / norm).sum(axis=1)\n            else:\n                yield pred / norm\n\n    @staticmethod\n    def _compute_proba_from_decision(decision, n_classes):\n        \"\"\"Compute probabilities from the decision function.\n\n        This is based eq. (15) of [1] where:\n            p(y=c|X) = exp((1 / K-1) f_c(X)) / sum_k(exp((1 / K-1) f_k(X)))\n                     = softmax((1 / K-1) * f(X))\n\n        References\n        ----------\n        .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\",\n               2009.\n        \"\"\"\n        if n_classes == 2:\n            decision = np.vstack([-decision, decision]).T / 2\n        else:\n            decision /= n_classes - 1\n        return softmax(decision, copy=False)\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities of the classifiers\n        in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n        check_is_fitted(self)\n        n_classes = self.n_classes_\n\n        if n_classes == 1:\n            return np.ones((_num_samples(X), 1))\n\n        decision = self.decision_function(X)\n        return self._compute_proba_from_decision(decision, n_classes)\n\n    def staged_predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities of the classifiers\n        in the ensemble.\n\n        This generator method yields the ensemble predicted class probabilities\n        after each iteration of boosting and therefore allows monitoring, such\n        as to determine the predicted class probabilities on a test set after\n        each boost.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        ------\n        p : generator of ndarray of shape (n_samples,)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n\n        n_classes = self.n_classes_\n\n        for decision in self.staged_decision_function(X):\n            yield self._compute_proba_from_decision(decision, n_classes)\n\n    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the weighted mean predicted class log-probabilities of the classifiers\n        in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n        return np.log(self.predict_proba(X))\n", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._weight_boosting/AdaBoostClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/exceptions.py", "fn_id": "", "content": "class ConvergenceWarning(UserWarning):\n    \"\"\"Custom warning to capture convergence problems\n\n    .. versionchanged:: 0.18\n       Moved from sklearn.utils.\n    \"\"\"\n", "class_fn": true, "question_id": "sklearn/sklearn.exceptions/ConvergenceWarning", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/exceptions.py", "fn_id": "", "content": "class EfficiencyWarning(UserWarning):\n    \"\"\"Warning used to notify the user of inefficient computation.\n\n    This warning notifies the user that the efficiency may not be optimal due\n    to some reason which may be included as a part of the warning message.\n    This may be subclassed into a more specific Warning class.\n\n    .. versionadded:: 0.18\n    \"\"\"\n", "class_fn": true, "question_id": "sklearn/sklearn.exceptions/EfficiencyWarning", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/exceptions.py", "fn_id": "", "content": "class PositiveSpectrumWarning(UserWarning):\n    \"\"\"Warning raised when the eigenvalues of a PSD matrix have issues\n\n    This warning is typically raised by ``_check_psd_eigenvalues`` when the\n    eigenvalues of a positive semidefinite (PSD) matrix such as a gram matrix\n    (kernel) present significant negative eigenvalues, or bad conditioning i.e.\n    very small non-zero eigenvalues compared to the largest eigenvalue.\n\n    .. versionadded:: 0.22\n    \"\"\"\n", "class_fn": true, "question_id": "sklearn/sklearn.exceptions/PositiveSpectrumWarning", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class ArffDecoder:\n    '''An ARFF decoder.'''\n\n    def __init__(self):\n        '''Constructor.'''\n        self._conversors = []\n        self._current_line = 0\n\n    def _decode_comment(self, s):\n        '''(INTERNAL) Decodes a comment line.\n\n        Comments are single line strings starting, obligatorily, with the ``%``\n        character, and can have any symbol, including whitespaces or special\n        characters.\n\n        This method must receive a normalized string, i.e., a string without\n        padding, including the \"\\r\\n\" characters.\n\n        :param s: a normalized string.\n        :return: a string with the decoded comment.\n        '''\n        res = re.sub(r'^\\%( )?', '', s)\n        return res\n\n    def _decode_relation(self, s):\n        '''(INTERNAL) Decodes a relation line.\n\n        The relation declaration is a line with the format ``@RELATION\n        <relation-name>``, where ``relation-name`` is a string. The string must\n        start with alphabetic character and must be quoted if the name includes\n        spaces, otherwise this method will raise a `BadRelationFormat` exception.\n\n        This method must receive a normalized string, i.e., a string without\n        padding, including the \"\\r\\n\" characters.\n\n        :param s: a normalized string.\n        :return: a string with the decoded relation name.\n        '''\n        _, v = s.split(' ', 1)\n        v = v.strip()\n\n        if not _RE_RELATION.match(v):\n            raise BadRelationFormat()\n\n        res = str(v.strip('\"\\''))\n        return res\n\n    def _decode_attribute(self, s):\n        '''(INTERNAL) Decodes an attribute line.\n\n        The attribute is the most complex declaration in an arff file. All\n        attributes must follow the template::\n\n             @attribute <attribute-name> <datatype>\n\n        where ``attribute-name`` is a string, quoted if the name contains any\n        whitespace, and ``datatype`` can be:\n\n        - Numerical attributes as ``NUMERIC``, ``INTEGER`` or ``REAL``.\n        - Strings as ``STRING``.\n        - Dates (NOT IMPLEMENTED).\n        - Nominal attributes with format:\n\n            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...}\n\n        The nominal names follow the rules for the attribute names, i.e., they\n        must be quoted if the name contains whitespaces.\n\n        This method must receive a normalized string, i.e., a string without\n        padding, including the \"\\r\\n\" characters.\n\n        :param s: a normalized string.\n        :return: a tuple (ATTRIBUTE_NAME, TYPE_OR_VALUES).\n        '''\n        _, v = s.split(' ', 1)\n        v = v.strip()\n\n        # Verify the general structure of declaration\n        m = _RE_ATTRIBUTE.match(v)\n        if not m:\n            raise BadAttributeFormat()\n\n        # Extracts the raw name and type\n        name, type_ = m.groups()\n\n        # Extracts the final name\n        name = str(name.strip('\"\\''))\n\n        # Extracts the final type\n        if type_[:1] == \"{\" and type_[-1:] == \"}\":\n            try:\n                type_ = _parse_values(type_.strip('{} '))\n            except Exception:\n                raise BadAttributeType()\n            if isinstance(type_, dict):\n                raise BadAttributeType()\n\n        else:\n            # If not nominal, verify the type name\n            type_ = str(type_).upper()\n            if type_ not in ['NUMERIC', 'REAL', 'INTEGER', 'STRING']:\n                raise BadAttributeType()\n\n        return (name, type_)\n\n    def _decode(self, s, encode_nominal=False, matrix_type=DENSE):\n        '''Do the job the ``encode``.'''\n\n        # Make sure this method is idempotent\n        self._current_line = 0\n\n        # If string, convert to a list of lines\n        if isinstance(s, str):\n            s = s.strip('\\r\\n ').replace('\\r\\n', '\\n').split('\\n')\n\n        # Create the return object\n        obj: ArffContainerType = {\n            'description': '',\n            'relation': '',\n            'attributes': [],\n            'data': []\n        }\n        attribute_names = {}\n\n        # Create the data helper object\n        data = _get_data_object_for_decoding(matrix_type)\n\n        # Read all lines\n        STATE = _TK_DESCRIPTION\n        s = iter(s)\n        for row in s:\n            self._current_line += 1\n            # Ignore empty lines\n            row = row.strip(' \\r\\n')\n            if not row: continue\n\n            u_row = row.upper()\n\n            # DESCRIPTION -----------------------------------------------------\n            if u_row.startswith(_TK_DESCRIPTION) and STATE == _TK_DESCRIPTION:\n                obj['description'] += self._decode_comment(row) + '\\n'\n            # -----------------------------------------------------------------\n\n            # RELATION --------------------------------------------------------\n            elif u_row.startswith(_TK_RELATION):\n                if STATE != _TK_DESCRIPTION:\n                    raise BadLayout()\n\n                STATE = _TK_RELATION\n                obj['relation'] = self._decode_relation(row)\n            # -----------------------------------------------------------------\n\n            # ATTRIBUTE -------------------------------------------------------\n            elif u_row.startswith(_TK_ATTRIBUTE):\n                if STATE != _TK_RELATION and STATE != _TK_ATTRIBUTE:\n                    raise BadLayout()\n\n                STATE = _TK_ATTRIBUTE\n\n                attr = self._decode_attribute(row)\n                if attr[0] in attribute_names:\n                    raise BadAttributeName(attr[0], attribute_names[attr[0]])\n                else:\n                    attribute_names[attr[0]] = self._current_line\n                obj['attributes'].append(attr)\n\n                if isinstance(attr[1], (list, tuple)):\n                    if encode_nominal:\n                        conversor = EncodedNominalConversor(attr[1])\n                    else:\n                        conversor = NominalConversor(attr[1])\n                else:\n                    CONVERSOR_MAP = {'STRING': str,\n                                     'INTEGER': lambda x: int(float(x)),\n                                     'NUMERIC': float,\n                                     'REAL': float}\n                    conversor = CONVERSOR_MAP[attr[1]]\n\n                self._conversors.append(conversor)\n            # -----------------------------------------------------------------\n\n            # DATA ------------------------------------------------------------\n            elif u_row.startswith(_TK_DATA):\n                if STATE != _TK_ATTRIBUTE:\n                    raise BadLayout()\n\n                break\n            # -----------------------------------------------------------------\n\n            # COMMENT ---------------------------------------------------------\n            elif u_row.startswith(_TK_COMMENT):\n                pass\n            # -----------------------------------------------------------------\n        else:\n            # Never found @DATA\n            raise BadLayout()\n\n        def stream():\n            for row in s:\n                self._current_line += 1\n                row = row.strip()\n                # Ignore empty lines and comment lines.\n                if row and not row.startswith(_TK_COMMENT):\n                    yield row\n\n        # Alter the data object\n        obj['data'] = data.decode_rows(stream(), self._conversors)\n        if obj['description'].endswith('\\n'):\n            obj['description'] = obj['description'][:-1]\n\n        return obj\n\n    def decode(self, s, encode_nominal=False, return_type=DENSE):\n        '''Returns the Python representation of a given ARFF file.\n\n        When a file object is passed as an argument, this method reads lines\n        iteratively, avoiding to load unnecessary information to the memory.\n\n        :param s: a string or file object with the ARFF file.\n        :param encode_nominal: boolean, if True perform a label encoding\n            while reading the .arff file.\n        :param return_type: determines the data structure used to store the\n            dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,\n            `arff.DENSE_GEN` or `arff.LOD_GEN`.\n            Consult the sections on `working with sparse data`_ and `loading\n            progressively`_.\n        '''\n        try:\n            return self._decode(s, encode_nominal=encode_nominal,\n                                matrix_type=return_type)\n        except ArffException as e:\n            e.line = self._current_line\n            raise e\n", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/ArffDecoder", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class Data(_DataListMixin, DenseGeneratorData):\n    pass\n", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/Data", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_packaging/version.py", "fn_id": "", "content": "class InvalidVersion(ValueError):\n    \"\"\"\n    An invalid version was found, users should refer to PEP 440.\n    \"\"\"\n", "class_fn": true, "question_id": "sklearn/sklearn.externals._packaging.version/InvalidVersion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_extraction/_hash.py", "fn_id": "", "content": "class FeatureHasher(TransformerMixin, BaseEstimator):\n    \"\"\"Implements feature hashing, aka the hashing trick.\n\n    This class turns sequences of symbolic feature names (strings) into\n    scipy.sparse matrices, using a hash function to compute the matrix column\n    corresponding to a name. The hash function employed is the signed 32-bit\n    version of Murmurhash3.\n\n    Feature names of type byte string are used as-is. Unicode strings are\n    converted to UTF-8 first, but no Unicode normalization is done.\n    Feature values must be (finite) numbers.\n\n    This class is a low-memory alternative to DictVectorizer and\n    CountVectorizer, intended for large-scale (online) learning and situations\n    where memory is tight, e.g. when running prediction code on embedded\n    devices.\n\n    For an efficiency comparison of the different feature extractors, see\n    :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\n    Read more in the :ref:`User Guide <feature_hashing>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_features : int, default=2**20\n        The number of features (columns) in the output matrices. Small numbers\n        of features are likely to cause hash collisions, but large numbers\n        will cause larger coefficient dimensions in linear learners.\n    input_type : str, default='dict'\n        Choose a string from {'dict', 'pair', 'string'}.\n        Either \"dict\" (the default) to accept dictionaries over\n        (feature_name, value); \"pair\" to accept pairs of (feature_name, value);\n        or \"string\" to accept single strings.\n        feature_name should be a string, while value should be a number.\n        In the case of \"string\", a value of 1 is implied.\n        The feature_name is hashed to find the appropriate column for the\n        feature. The value's sign might be flipped in the output (but see\n        non_negative, below).\n    dtype : numpy dtype, default=np.float64\n        The type of feature values. Passed to scipy.sparse matrix constructors\n        as the dtype argument. Do not set this to bool, np.boolean or any\n        unsigned integer type.\n    alternate_sign : bool, default=True\n        When True, an alternating sign is added to the features as to\n        approximately conserve the inner product in the hashed space even for\n        small n_features. This approach is similar to sparse random projection.\n\n        .. versionchanged:: 0.19\n            ``alternate_sign`` replaces the now deprecated ``non_negative``\n            parameter.\n\n    See Also\n    --------\n    DictVectorizer : Vectorizes string-valued features using a hash table.\n    sklearn.preprocessing.OneHotEncoder : Handles nominal/categorical features.\n\n    Notes\n    -----\n    This estimator is :term:`stateless` and does not need to be fitted.\n    However, we recommend to call :meth:`fit_transform` instead of\n    :meth:`transform`, as parameter validation is only performed in\n    :meth:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction import FeatureHasher\n    >>> h = FeatureHasher(n_features=10)\n    >>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n    >>> f = h.transform(D)\n    >>> f.toarray()\n    array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],\n           [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])\n\n    With `input_type=\"string\"`, the input must be an iterable over iterables of\n    strings:\n\n    >>> h = FeatureHasher(n_features=8, input_type=\"string\")\n    >>> raw_X = [[\"dog\", \"cat\", \"snake\"], [\"snake\", \"dog\"], [\"cat\", \"bird\"]]\n    >>> f = h.transform(raw_X)\n    >>> f.toarray()\n    array([[ 0.,  0.,  0., -1.,  0., -1.,  0.,  1.],\n           [ 0.,  0.,  0., -1.,  0., -1.,  0.,  0.],\n           [ 0., -1.,  0.,  0.,  0.,  0.,  0.,  1.]])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_features\": [Interval(Integral, 1, np.iinfo(np.int32).max, closed=\"both\")],\n        \"input_type\": [StrOptions({\"dict\", \"pair\", \"string\"})],\n        \"dtype\": \"no_validation\",  # delegate to numpy\n        \"alternate_sign\": [\"boolean\"],\n    }\n\n    def __init__(\n        self,\n        n_features=(2**20),\n        *,\n        input_type=\"dict\",\n        dtype=np.float64,\n        alternate_sign=True,\n    ):\n        self.dtype = dtype\n        self.input_type = input_type\n        self.n_features = n_features\n        self.alternate_sign = alternate_sign\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X=None, y=None):\n        \"\"\"Only validates estimator's parameters.\n\n        This method allows to: (i) validate the estimator's parameters and\n        (ii) be consistent with the scikit-learn transformer API.\n\n        Parameters\n        ----------\n        X : Ignored\n            Not used, present here for API consistency by convention.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            FeatureHasher class instance.\n        \"\"\"\n        return self\n\n    def transform(self, raw_X):\n        \"\"\"Transform a sequence of instances to a scipy.sparse matrix.\n\n        Parameters\n        ----------\n        raw_X : iterable over iterable over raw features, length = n_samples\n            Samples. Each sample must be iterable an (e.g., a list or tuple)\n            containing/generating feature names (and optionally values, see\n            the input_type constructor argument) which will be hashed.\n            raw_X need not support the len function, so it can be the result\n            of a generator; n_samples is determined on the fly.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Feature matrix, for use with estimators or further transformers.\n        \"\"\"\n        raw_X = iter(raw_X)\n        if self.input_type == \"dict\":\n            raw_X = (_iteritems(d) for d in raw_X)\n        elif self.input_type == \"string\":\n            first_raw_X = next(raw_X)\n            if isinstance(first_raw_X, str):\n                raise ValueError(\n                    \"Samples can not be a single string. The input must be an iterable\"\n                    \" over iterables of strings.\"\n                )\n            raw_X_ = chain([first_raw_X], raw_X)\n            raw_X = (((f, 1) for f in x) for x in raw_X_)\n\n        indices, indptr, values = _hashing_transform(\n            raw_X, self.n_features, self.dtype, self.alternate_sign, seed=0\n        )\n        n_samples = indptr.shape[0] - 1\n\n        if n_samples == 0:\n            raise ValueError(\"Cannot vectorize empty sequence.\")\n\n        X = sp.csr_matrix(\n            (values, indices, indptr),\n            dtype=self.dtype,\n            shape=(n_samples, self.n_features),\n        )\n        X.sum_duplicates()  # also sorts the indices\n\n        return X\n\n    def _more_tags(self):\n        return {\"X_types\": [self.input_type]}\n", "class_fn": true, "question_id": "sklearn/sklearn.feature_extraction._hash/FeatureHasher", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_extraction/text.py", "fn_id": "", "content": "class HashingVectorizer(\n    TransformerMixin, _VectorizerMixin, BaseEstimator, auto_wrap_output_keys=None\n):\n    r\"\"\"Convert a collection of text documents to a matrix of token occurrences.\n\n    It turns a collection of text documents into a scipy.sparse matrix holding\n    token occurrence counts (or binary occurrence information), possibly\n    normalized as token frequencies if norm='l1' or projected on the euclidean\n    unit sphere if norm='l2'.\n\n    This text vectorizer implementation uses the hashing trick to find the\n    token string name to feature integer index mapping.\n\n    This strategy has several advantages:\n\n    - it is very low memory scalable to large datasets as there is no need to\n      store a vocabulary dictionary in memory.\n\n    - it is fast to pickle and un-pickle as it holds no state besides the\n      constructor parameters.\n\n    - it can be used in a streaming (partial fit) or parallel pipeline as there\n      is no state computed during fit.\n\n    There are also a couple of cons (vs using a CountVectorizer with an\n    in-memory vocabulary):\n\n    - there is no way to compute the inverse transform (from feature indices to\n      string feature names) which can be a problem when trying to introspect\n      which features are most important to a model.\n\n    - there can be collisions: distinct tokens can be mapped to the same\n      feature index. However in practice this is rarely an issue if n_features\n      is large enough (e.g. 2 ** 18 for text classification problems).\n\n    - no IDF weighting as this would render the transformer stateful.\n\n    The hash function employed is the signed 32-bit version of Murmurhash3.\n\n    For an efficiency comparison of the different feature extractors, see\n    :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\n    For an example of document clustering and comparison with\n    :class:`~sklearn.feature_extraction.text.TfidfVectorizer`, see\n    :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n    input : {'filename', 'file', 'content'}, default='content'\n        - If `'filename'`, the sequence passed as an argument to fit is\n          expected to be a list of filenames that need reading to fetch\n          the raw content to analyze.\n\n        - If `'file'`, the sequence items must have a 'read' method (file-like\n          object) that is called to fetch the bytes in memory.\n\n        - If `'content'`, the input is expected to be a sequence of items that\n          can be of type string or byte.\n\n    encoding : str, default='utf-8'\n        If bytes or files are given to analyze, this encoding is used to\n        decode.\n\n    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. By default, it is\n        'strict', meaning that a UnicodeDecodeError will be raised. Other\n        values are 'ignore' and 'replace'.\n\n    strip_accents : {'ascii', 'unicode'} or callable, default=None\n        Remove accents and perform other character normalization\n        during the preprocessing step.\n        'ascii' is a fast method that only works on characters that have\n        a direct ASCII mapping.\n        'unicode' is a slightly slower method that works on any character.\n        None (default) means no character normalization is performed.\n\n        Both 'ascii' and 'unicode' use NFKD normalization from\n        :func:`unicodedata.normalize`.\n\n    lowercase : bool, default=True\n        Convert all characters to lowercase before tokenizing.\n\n    preprocessor : callable, default=None\n        Override the preprocessing (string transformation) stage while\n        preserving the tokenizing and n-grams generation steps.\n        Only applies if ``analyzer`` is not callable.\n\n    tokenizer : callable, default=None\n        Override the string tokenization step while preserving the\n        preprocessing and n-grams generation steps.\n        Only applies if ``analyzer == 'word'``.\n\n    stop_words : {'english'}, list, default=None\n        If 'english', a built-in stop word list for English is used.\n        There are several known issues with 'english' and you should\n        consider an alternative (see :ref:`stop_words`).\n\n        If a list, that list is assumed to contain stop words, all of which\n        will be removed from the resulting tokens.\n        Only applies if ``analyzer == 'word'``.\n\n    token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n        Regular expression denoting what constitutes a \"token\", only used\n        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n        or more alphanumeric characters (punctuation is completely ignored\n        and always treated as a token separator).\n\n        If there is a capturing group in token_pattern then the\n        captured group content, not the entire match, becomes the token.\n        At most one capturing group is permitted.\n\n    ngram_range : tuple (min_n, max_n), default=(1, 1)\n        The lower and upper boundary of the range of n-values for different\n        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n        only bigrams.\n        Only applies if ``analyzer`` is not callable.\n\n    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n        Whether the feature should be made of word or character n-grams.\n        Option 'char_wb' creates character n-grams only from text inside\n        word boundaries; n-grams at the edges of words are padded with space.\n\n        If a callable is passed it is used to extract the sequence of features\n        out of the raw, unprocessed input.\n\n        .. versionchanged:: 0.21\n            Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n            is first read from the file and then passed to the given callable\n            analyzer.\n\n    n_features : int, default=(2 ** 20)\n        The number of features (columns) in the output matrices. Small numbers\n        of features are likely to cause hash collisions, but large numbers\n        will cause larger coefficient dimensions in linear learners.\n\n    binary : bool, default=False\n        If True, all non zero counts are set to 1. This is useful for discrete\n        probabilistic models that model binary events rather than integer\n        counts.\n\n    norm : {'l1', 'l2'}, default='l2'\n        Norm used to normalize term vectors. None for no normalization.\n\n    alternate_sign : bool, default=True\n        When True, an alternating sign is added to the features as to\n        approximately conserve the inner product in the hashed space even for\n        small n_features. This approach is similar to sparse random projection.\n\n        .. versionadded:: 0.19\n\n    dtype : type, default=np.float64\n        Type of the matrix returned by fit_transform() or transform().\n\n    See Also\n    --------\n    CountVectorizer : Convert a collection of text documents to a matrix of\n        token counts.\n    TfidfVectorizer : Convert a collection of raw documents to a matrix of\n        TF-IDF features.\n\n    Notes\n    -----\n    This estimator is :term:`stateless` and does not need to be fitted.\n    However, we recommend to call :meth:`fit_transform` instead of\n    :meth:`transform`, as parameter validation is only performed in\n    :meth:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction.text import HashingVectorizer\n    >>> corpus = [\n    ...     'This is the first document.',\n    ...     'This document is the second document.',\n    ...     'And this is the third one.',\n    ...     'Is this the first document?',\n    ... ]\n    >>> vectorizer = HashingVectorizer(n_features=2**4)\n    >>> X = vectorizer.fit_transform(corpus)\n    >>> print(X.shape)\n    (4, 16)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"input\": [StrOptions({\"filename\", \"file\", \"content\"})],\n        \"encoding\": [str],\n        \"decode_error\": [StrOptions({\"strict\", \"ignore\", \"replace\"})],\n        \"strip_accents\": [StrOptions({\"ascii\", \"unicode\"}), None, callable],\n        \"lowercase\": [\"boolean\"],\n        \"preprocessor\": [callable, None],\n        \"tokenizer\": [callable, None],\n        \"stop_words\": [StrOptions({\"english\"}), list, None],\n        \"token_pattern\": [str, None],\n        \"ngram_range\": [tuple],\n        \"analyzer\": [StrOptions({\"word\", \"char\", \"char_wb\"}), callable],\n        \"n_features\": [Interval(Integral, 1, np.iinfo(np.int32).max, closed=\"left\")],\n        \"binary\": [\"boolean\"],\n        \"norm\": [StrOptions({\"l1\", \"l2\"}), None],\n        \"alternate_sign\": [\"boolean\"],\n        \"dtype\": \"no_validation\",  # delegate to numpy\n    }\n\n    def __init__(\n        self,\n        *,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        stop_words=None,\n        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n        ngram_range=(1, 1),\n        analyzer=\"word\",\n        n_features=(2**20),\n        binary=False,\n        norm=\"l2\",\n        alternate_sign=True,\n        dtype=np.float64,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.n_features = n_features\n        self.ngram_range = ngram_range\n        self.binary = binary\n        self.norm = norm\n        self.alternate_sign = alternate_sign\n        self.dtype = dtype\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def partial_fit(self, X, y=None):\n        \"\"\"Only validates estimator's parameters.\n\n        This method allows to: (i) validate the estimator's parameters and\n        (ii) be consistent with the scikit-learn transformer API.\n\n        Parameters\n        ----------\n        X : ndarray of shape [n_samples, n_features]\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            HashingVectorizer instance.\n        \"\"\"\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Only validates estimator's parameters.\n\n        This method allows to: (i) validate the estimator's parameters and\n        (ii) be consistent with the scikit-learn transformer API.\n\n        Parameters\n        ----------\n        X : ndarray of shape [n_samples, n_features]\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            HashingVectorizer instance.\n        \"\"\"\n        # triggers a parameter validation\n        if isinstance(X, str):\n            raise ValueError(\n                \"Iterable over raw text documents expected, string object received.\"\n            )\n\n        self._warn_for_unused_params()\n        self._validate_ngram_range()\n\n        self._get_hasher().fit(X, y=y)\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform a sequence of documents to a document-term matrix.\n\n        Parameters\n        ----------\n        X : iterable over raw text documents, length = n_samples\n            Samples. Each sample must be a text document (either bytes or\n            unicode strings, file name or file object depending on the\n            constructor argument) which will be tokenized and hashed.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        if isinstance(X, str):\n            raise ValueError(\n                \"Iterable over raw text documents expected, string object received.\"\n            )\n\n        self._validate_ngram_range()\n\n        analyzer = self.build_analyzer()\n        X = self._get_hasher().transform(analyzer(doc) for doc in X)\n        if self.binary:\n            X.data.fill(1)\n        if self.norm is not None:\n            X = normalize(X, norm=self.norm, copy=False)\n        return X\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Transform a sequence of documents to a document-term matrix.\n\n        Parameters\n        ----------\n        X : iterable over raw text documents, length = n_samples\n            Samples. Each sample must be a text document (either bytes or\n            unicode strings, file name or file object depending on the\n            constructor argument) which will be tokenized and hashed.\n        y : any\n            Ignored. This parameter exists only for compatibility with\n            sklearn.pipeline.Pipeline.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        return self.fit(X, y).transform(X)\n\n    def _get_hasher(self):\n        return FeatureHasher(\n            n_features=self.n_features,\n            input_type=\"string\",\n            dtype=self.dtype,\n            alternate_sign=self.alternate_sign,\n        )\n\n    def _more_tags(self):\n        return {\"X_types\": [\"string\"]}\n", "class_fn": true, "question_id": "sklearn/sklearn.feature_extraction.text/HashingVectorizer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_extraction/text.py", "fn_id": "", "content": "class _VectorizerMixin:\n    \"\"\"Provides common code for text vectorizers (tokenization logic).\"\"\"\n\n    _white_spaces = re.compile(r\"\\s\\s+\")\n\n    def decode(self, doc):\n        \"\"\"Decode the input into a string of unicode symbols.\n\n        The decoding strategy depends on the vectorizer parameters.\n\n        Parameters\n        ----------\n        doc : bytes or str\n            The string to decode.\n\n        Returns\n        -------\n        doc: str\n            A string of unicode symbols.\n        \"\"\"\n        if self.input == \"filename\":\n            with open(doc, \"rb\") as fh:\n                doc = fh.read()\n\n        elif self.input == \"file\":\n            doc = doc.read()\n\n        if isinstance(doc, bytes):\n            doc = doc.decode(self.encoding, self.decode_error)\n\n        if doc is np.nan:\n            raise ValueError(\n                \"np.nan is an invalid document, expected byte or unicode string.\"\n            )\n\n        return doc\n\n    def _word_ngrams(self, tokens, stop_words=None):\n        \"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\n        # handle stop words\n        if stop_words is not None:\n            tokens = [w for w in tokens if w not in stop_words]\n\n        # handle token n-grams\n        min_n, max_n = self.ngram_range\n        if max_n != 1:\n            original_tokens = tokens\n            if min_n == 1:\n                # no need to do any slicing for unigrams\n                # just iterate through the original tokens\n                tokens = list(original_tokens)\n                min_n += 1\n            else:\n                tokens = []\n\n            n_original_tokens = len(original_tokens)\n\n            # bind method outside of loop to reduce overhead\n            tokens_append = tokens.append\n            space_join = \" \".join\n\n            for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):\n                for i in range(n_original_tokens - n + 1):\n                    tokens_append(space_join(original_tokens[i : i + n]))\n\n        return tokens\n\n    def _char_ngrams(self, text_document):\n        \"\"\"Tokenize text_document into a sequence of character n-grams\"\"\"\n        # normalize white spaces\n        text_document = self._white_spaces.sub(\" \", text_document)\n\n        text_len = len(text_document)\n        min_n, max_n = self.ngram_range\n        if min_n == 1:\n            # no need to do any slicing for unigrams\n            # iterate through the string\n            ngrams = list(text_document)\n            min_n += 1\n        else:\n            ngrams = []\n\n        # bind method outside of loop to reduce overhead\n        ngrams_append = ngrams.append\n\n        for n in range(min_n, min(max_n + 1, text_len + 1)):\n            for i in range(text_len - n + 1):\n                ngrams_append(text_document[i : i + n])\n        return ngrams\n\n    def _char_wb_ngrams(self, text_document):\n        \"\"\"Whitespace sensitive char-n-gram tokenization.\n\n        Tokenize text_document into a sequence of character n-grams\n        operating only inside word boundaries. n-grams at the edges\n        of words are padded with space.\"\"\"\n        # normalize white spaces\n        text_document = self._white_spaces.sub(\" \", text_document)\n\n        min_n, max_n = self.ngram_range\n        ngrams = []\n\n        # bind method outside of loop to reduce overhead\n        ngrams_append = ngrams.append\n\n        for w in text_document.split():\n            w = \" \" + w + \" \"\n            w_len = len(w)\n            for n in range(min_n, max_n + 1):\n                offset = 0\n                ngrams_append(w[offset : offset + n])\n                while offset + n < w_len:\n                    offset += 1\n                    ngrams_append(w[offset : offset + n])\n                if offset == 0:  # count a short word (w_len < n) only once\n                    break\n        return ngrams\n\n    def build_preprocessor(self):\n        \"\"\"Return a function to preprocess the text before tokenization.\n\n        Returns\n        -------\n        preprocessor: callable\n              A function to preprocess the text before tokenization.\n        \"\"\"\n        if self.preprocessor is not None:\n            return self.preprocessor\n\n        # accent stripping\n        if not self.strip_accents:\n            strip_accents = None\n        elif callable(self.strip_accents):\n            strip_accents = self.strip_accents\n        elif self.strip_accents == \"ascii\":\n            strip_accents = strip_accents_ascii\n        elif self.strip_accents == \"unicode\":\n            strip_accents = strip_accents_unicode\n        else:\n            raise ValueError(\n                'Invalid value for \"strip_accents\": %s' % self.strip_accents\n            )\n\n        return partial(_preprocess, accent_function=strip_accents, lower=self.lowercase)\n\n    def build_tokenizer(self):\n        \"\"\"Return a function that splits a string into a sequence of tokens.\n\n        Returns\n        -------\n        tokenizer: callable\n              A function to split a string into a sequence of tokens.\n        \"\"\"\n        if self.tokenizer is not None:\n            return self.tokenizer\n        token_pattern = re.compile(self.token_pattern)\n\n        if token_pattern.groups > 1:\n            raise ValueError(\n                \"More than 1 capturing group in token pattern. Only a single \"\n                \"group should be captured.\"\n            )\n\n        return token_pattern.findall\n\n    def get_stop_words(self):\n        \"\"\"Build or fetch the effective stop words list.\n\n        Returns\n        -------\n        stop_words: list or None\n                A list of stop words.\n        \"\"\"\n        return _check_stop_list(self.stop_words)\n\n    def _check_stop_words_consistency(self, stop_words, preprocess, tokenize):\n        \"\"\"Check if stop words are consistent\n\n        Returns\n        -------\n        is_consistent : True if stop words are consistent with the preprocessor\n                        and tokenizer, False if they are not, None if the check\n                        was previously performed, \"error\" if it could not be\n                        performed (e.g. because of the use of a custom\n                        preprocessor / tokenizer)\n        \"\"\"\n        if id(self.stop_words) == getattr(self, \"_stop_words_id\", None):\n            # Stop words are were previously validated\n            return None\n\n        # NB: stop_words is validated, unlike self.stop_words\n        try:\n            inconsistent = set()\n            for w in stop_words or ():\n                tokens = list(tokenize(preprocess(w)))\n                for token in tokens:\n                    if token not in stop_words:\n                        inconsistent.add(token)\n            self._stop_words_id = id(self.stop_words)\n\n            if inconsistent:\n                warnings.warn(\n                    \"Your stop_words may be inconsistent with \"\n                    \"your preprocessing. Tokenizing the stop \"\n                    \"words generated tokens %r not in \"\n                    \"stop_words.\" % sorted(inconsistent)\n                )\n            return not inconsistent\n        except Exception:\n            # Failed to check stop words consistency (e.g. because a custom\n            # preprocessor or tokenizer was used)\n            self._stop_words_id = id(self.stop_words)\n            return \"error\"\n\n    def build_analyzer(self):\n        \"\"\"Return a callable to process input data.\n\n        The callable handles preprocessing, tokenization, and n-grams generation.\n\n        Returns\n        -------\n        analyzer: callable\n            A function to handle preprocessing, tokenization\n            and n-grams generation.\n        \"\"\"\n\n        if callable(self.analyzer):\n            return partial(_analyze, analyzer=self.analyzer, decoder=self.decode)\n\n        preprocess = self.build_preprocessor()\n\n        if self.analyzer == \"char\":\n            return partial(\n                _analyze,\n                ngrams=self._char_ngrams,\n                preprocessor=preprocess,\n                decoder=self.decode,\n            )\n\n        elif self.analyzer == \"char_wb\":\n            return partial(\n                _analyze,\n                ngrams=self._char_wb_ngrams,\n                preprocessor=preprocess,\n                decoder=self.decode,\n            )\n\n        elif self.analyzer == \"word\":\n            stop_words = self.get_stop_words()\n            tokenize = self.build_tokenizer()\n            self._check_stop_words_consistency(stop_words, preprocess, tokenize)\n            return partial(\n                _analyze,\n                ngrams=self._word_ngrams,\n                tokenizer=tokenize,\n                preprocessor=preprocess,\n                decoder=self.decode,\n                stop_words=stop_words,\n            )\n\n        else:\n            raise ValueError(\n                \"%s is not a valid tokenization scheme/analyzer\" % self.analyzer\n            )\n\n    def _validate_vocabulary(self):\n        vocabulary = self.vocabulary\n        if vocabulary is not None:\n            if isinstance(vocabulary, set):\n                vocabulary = sorted(vocabulary)\n            if not isinstance(vocabulary, Mapping):\n                vocab = {}\n                for i, t in enumerate(vocabulary):\n                    if vocab.setdefault(t, i) != i:\n                        msg = \"Duplicate term in vocabulary: %r\" % t\n                        raise ValueError(msg)\n                vocabulary = vocab\n            else:\n                indices = set(vocabulary.values())\n                if len(indices) != len(vocabulary):\n                    raise ValueError(\"Vocabulary contains repeated indices.\")\n                for i in range(len(vocabulary)):\n                    if i not in indices:\n                        msg = \"Vocabulary of size %d doesn't contain index %d.\" % (\n                            len(vocabulary),\n                            i,\n                        )\n                        raise ValueError(msg)\n            if not vocabulary:\n                raise ValueError(\"empty vocabulary passed to fit\")\n            self.fixed_vocabulary_ = True\n            self.vocabulary_ = dict(vocabulary)\n        else:\n            self.fixed_vocabulary_ = False\n\n    def _check_vocabulary(self):\n        \"\"\"Check if vocabulary is empty or missing (not fitted)\"\"\"\n        if not hasattr(self, \"vocabulary_\"):\n            self._validate_vocabulary()\n            if not self.fixed_vocabulary_:\n                raise NotFittedError(\"Vocabulary not fitted or provided\")\n\n        if len(self.vocabulary_) == 0:\n            raise ValueError(\"Vocabulary is empty\")\n\n    def _validate_ngram_range(self):\n        \"\"\"Check validity of ngram_range parameter\"\"\"\n        min_n, max_m = self.ngram_range\n        if min_n > max_m:\n            raise ValueError(\n                \"Invalid value for ngram_range=%s \"\n                \"lower boundary larger than the upper boundary.\" % str(self.ngram_range)\n            )\n\n    def _warn_for_unused_params(self):\n        if self.tokenizer is not None and self.token_pattern is not None:\n            warnings.warn(\n                \"The parameter 'token_pattern' will not be used\"\n                \" since 'tokenizer' is not None'\"\n            )\n\n        if self.preprocessor is not None and callable(self.analyzer):\n            warnings.warn(\n                \"The parameter 'preprocessor' will not be used\"\n                \" since 'analyzer' is callable'\"\n            )\n\n        if (\n            self.ngram_range != (1, 1)\n            and self.ngram_range is not None\n            and callable(self.analyzer)\n        ):\n            warnings.warn(\n                \"The parameter 'ngram_range' will not be used\"\n                \" since 'analyzer' is callable'\"\n            )\n        if self.analyzer != \"word\" or callable(self.analyzer):\n            if self.stop_words is not None:\n                warnings.warn(\n                    \"The parameter 'stop_words' will not be used\"\n                    \" since 'analyzer' != 'word'\"\n                )\n            if (\n                self.token_pattern is not None\n                and self.token_pattern != r\"(?u)\\b\\w\\w+\\b\"\n            ):\n                warnings.warn(\n                    \"The parameter 'token_pattern' will not be used\"\n                    \" since 'analyzer' != 'word'\"\n                )\n            if self.tokenizer is not None:\n                warnings.warn(\n                    \"The parameter 'tokenizer' will not be used\"\n                    \" since 'analyzer' != 'word'\"\n                )\n", "class_fn": true, "question_id": "sklearn/sklearn.feature_extraction.text/_VectorizerMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_selection/_rfe.py", "fn_id": "", "content": "class RFE(_RoutingNotSupportedMixin, SelectorMixin, MetaEstimatorMixin, BaseEstimator):\n    \"\"\"Feature ranking with recursive feature elimination.\n\n    Given an external estimator that assigns weights to features (e.g., the\n    coefficients of a linear model), the goal of recursive feature elimination\n    (RFE) is to select features by recursively considering smaller and smaller\n    sets of features. First, the estimator is trained on the initial set of\n    features and the importance of each feature is obtained either through\n    any specific attribute or callable.\n    Then, the least important features are pruned from current set of features.\n    That procedure is recursively repeated on the pruned set until the desired\n    number of features to select is eventually reached.\n\n    Read more in the :ref:`User Guide <rfe>`.\n\n    Parameters\n    ----------\n    estimator : ``Estimator`` instance\n        A supervised learning estimator with a ``fit`` method that provides\n        information about feature importance\n        (e.g. `coef_`, `feature_importances_`).\n\n    n_features_to_select : int or float, default=None\n        The number of features to select. If `None`, half of the features are\n        selected. If integer, the parameter is the absolute number of features\n        to select. If float between 0 and 1, it is the fraction of features to\n        select.\n\n        .. versionchanged:: 0.24\n           Added float values for fractions.\n\n    step : int or float, default=1\n        If greater than or equal to 1, then ``step`` corresponds to the\n        (integer) number of features to remove at each iteration.\n        If within (0.0, 1.0), then ``step`` corresponds to the percentage\n        (rounded down) of features to remove at each iteration.\n\n    verbose : int, default=0\n        Controls verbosity of output.\n\n    importance_getter : str or callable, default='auto'\n        If 'auto', uses the feature importance either through a `coef_`\n        or `feature_importances_` attributes of estimator.\n\n        Also accepts a string that specifies an attribute name/path\n        for extracting feature importance (implemented with `attrgetter`).\n        For example, give `regressor_.coef_` in case of\n        :class:`~sklearn.compose.TransformedTargetRegressor`  or\n        `named_steps.clf.feature_importances_` in case of\n        class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n        If `callable`, overrides the default feature importance getter.\n        The callable is passed with the fitted estimator and it should\n        return importance for each feature.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels. Only available when `estimator` is a classifier.\n\n    estimator_ : ``Estimator`` instance\n        The fitted estimator used to select features.\n\n    n_features_ : int\n        The number of selected features.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    ranking_ : ndarray of shape (n_features,)\n        The feature ranking, such that ``ranking_[i]`` corresponds to the\n        ranking position of the i-th feature. Selected (i.e., estimated\n        best) features are assigned rank 1.\n\n    support_ : ndarray of shape (n_features,)\n        The mask of selected features.\n\n    See Also\n    --------\n    RFECV : Recursive feature elimination with built-in cross-validated\n        selection of the best number of features.\n    SelectFromModel : Feature selection based on thresholds of importance\n        weights.\n    SequentialFeatureSelector : Sequential cross-validation based feature\n        selection. Does not rely on importance weights.\n\n    Notes\n    -----\n    Allows NaN/Inf in the input if the underlying estimator does as well.\n\n    References\n    ----------\n\n    .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n           for cancer classification using support vector machines\",\n           Mach. Learn., 46(1-3), 389--422, 2002.\n\n    Examples\n    --------\n    The following example shows how to retrieve the 5 most informative\n    features in the Friedman #1 dataset.\n\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.feature_selection import RFE\n    >>> from sklearn.svm import SVR\n    >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n    >>> estimator = SVR(kernel=\"linear\")\n    >>> selector = RFE(estimator, n_features_to_select=5, step=1)\n    >>> selector = selector.fit(X, y)\n    >>> selector.support_\n    array([ True,  True,  True,  True,  True, False, False, False, False,\n           False])\n    >>> selector.ranking_\n    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"estimator\": [HasMethods([\"fit\"])],\n        \"n_features_to_select\": [\n            None,\n            Interval(RealNotInt, 0, 1, closed=\"right\"),\n            Interval(Integral, 0, None, closed=\"neither\"),\n        ],\n        \"step\": [\n            Interval(Integral, 0, None, closed=\"neither\"),\n            Interval(RealNotInt, 0, 1, closed=\"neither\"),\n        ],\n        \"verbose\": [\"verbose\"],\n        \"importance_getter\": [str, callable],\n    }\n\n    def __init__(\n        self,\n        estimator,\n        *,\n        n_features_to_select=None,\n        step=1,\n        verbose=0,\n        importance_getter=\"auto\",\n    ):\n        self.estimator = estimator\n        self.n_features_to_select = n_features_to_select\n        self.step = step\n        self.importance_getter = importance_getter\n        self.verbose = verbose\n\n    @property\n    def _estimator_type(self):\n        return self.estimator._estimator_type\n\n    @property\n    def classes_(self):\n        \"\"\"Classes labels available when `estimator` is a classifier.\n\n        Returns\n        -------\n        ndarray of shape (n_classes,)\n        \"\"\"\n        return self.estimator_.classes_\n\n    @_fit_context(\n        # RFE.estimator is not validated yet\n        prefer_skip_nested_validation=False\n    )\n    def fit(self, X, y, **fit_params):\n        \"\"\"Fit the RFE model and then the underlying estimator on the selected features.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        **fit_params : dict\n            Additional parameters passed to the `fit` method of the underlying\n            estimator.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        _raise_for_unsupported_routing(self, \"fit\", **fit_params)\n        return self._fit(X, y, **fit_params)\n\n    def _fit(self, X, y, step_score=None, **fit_params):\n        # Parameter step_score controls the calculation of self.step_scores_\n        # step_score is not exposed to users and is used when implementing RFECV\n        # self.step_scores_ will not be calculated when calling _fit through fit\n\n        X, y = self._validate_data(\n            X,\n            y,\n            accept_sparse=\"csc\",\n            ensure_min_features=2,\n            force_all_finite=False,\n            multi_output=True,\n        )\n\n        # Initialization\n        n_features = X.shape[1]\n        if self.n_features_to_select is None:\n            n_features_to_select = n_features // 2\n        elif isinstance(self.n_features_to_select, Integral):  # int\n            n_features_to_select = self.n_features_to_select\n            if n_features_to_select > n_features:\n                warnings.warn(\n                    (\n                        f\"Found {n_features_to_select=} > {n_features=}. There will be\"\n                        \" no feature selection and all features will be kept.\"\n                    ),\n                    UserWarning,\n                )\n        else:  # float\n            n_features_to_select = int(n_features * self.n_features_to_select)\n\n        if 0.0 < self.step < 1.0:\n            step = int(max(1, self.step * n_features))\n        else:\n            step = int(self.step)\n\n        support_ = np.ones(n_features, dtype=bool)\n        ranking_ = np.ones(n_features, dtype=int)\n\n        if step_score:\n            self.step_n_features_ = []\n            self.step_scores_ = []\n\n        # Elimination\n        while np.sum(support_) > n_features_to_select:\n            # Remaining features\n            features = np.arange(n_features)[support_]\n\n            # Rank the remaining features\n            estimator = clone(self.estimator)\n            if self.verbose > 0:\n                print(\"Fitting estimator with %d features.\" % np.sum(support_))\n\n            estimator.fit(X[:, features], y, **fit_params)\n\n            # Get importance and rank them\n            importances = _get_feature_importances(\n                estimator,\n                self.importance_getter,\n                transform_func=\"square\",\n            )\n            ranks = np.argsort(importances)\n\n            # for sparse case ranks is matrix\n            ranks = np.ravel(ranks)\n\n            # Eliminate the worse features\n            threshold = min(step, np.sum(support_) - n_features_to_select)\n\n            # Compute step score on the previous selection iteration\n            # because 'estimator' must use features\n            # that have not been eliminated yet\n            if step_score:\n                self.step_n_features_.append(len(features))\n                self.step_scores_.append(step_score(estimator, features))\n            support_[features[ranks][:threshold]] = False\n            ranking_[np.logical_not(support_)] += 1\n\n        # Set final attributes\n        features = np.arange(n_features)[support_]\n        self.estimator_ = clone(self.estimator)\n        self.estimator_.fit(X[:, features], y, **fit_params)\n\n        # Compute step score when only n_features_to_select features left\n        if step_score:\n            self.step_n_features_.append(len(features))\n            self.step_scores_.append(step_score(self.estimator_, features))\n        self.n_features_ = support_.sum()\n        self.support_ = support_\n        self.ranking_ = ranking_\n\n        return self\n\n    @available_if(_estimator_has(\"predict\"))\n    def predict(self, X):\n        \"\"\"Reduce X to the selected features and predict using the estimator.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        Returns\n        -------\n        y : array of shape [n_samples]\n            The predicted target values.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict(self.transform(X))\n\n    @available_if(_estimator_has(\"score\"))\n    def score(self, X, y, **fit_params):\n        \"\"\"Reduce X to the selected features and return the score of the estimator.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        y : array of shape [n_samples]\n            The target values.\n\n        **fit_params : dict\n            Parameters to pass to the `score` method of the underlying\n            estimator.\n\n            .. versionadded:: 1.0\n\n        Returns\n        -------\n        score : float\n            Score of the underlying base estimator computed with the selected\n            features returned by `rfe.transform(X)` and `y`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.score(self.transform(X), y, **fit_params)\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n        return self.support_\n\n    @available_if(_estimator_has(\"decision_function\"))\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        score : array, shape = [n_samples, n_classes] or [n_samples]\n            The decision function of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification produce an array of shape\n            [n_samples].\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.decision_function(self.transform(X))\n\n    @available_if(_estimator_has(\"predict_proba\"))\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : array of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict_proba(self.transform(X))\n\n    @available_if(_estimator_has(\"predict_log_proba\"))\n    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        Returns\n        -------\n        p : array of shape (n_samples, n_classes)\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict_log_proba(self.transform(X))\n\n    def _more_tags(self):\n        tags = {\n            \"poor_score\": True,\n            \"requires_y\": True,\n            \"allow_nan\": True,\n        }\n\n        # Adjust allow_nan if estimator explicitly defines `allow_nan`.\n        if hasattr(self.estimator, \"_get_tags\"):\n            tags[\"allow_nan\"] = self.estimator._get_tags()[\"allow_nan\"]\n\n        return tags\n", "class_fn": true, "question_id": "sklearn/sklearn.feature_selection._rfe/RFE", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_selection/_univariate_selection.py", "fn_id": "", "content": "class GenericUnivariateSelect(_BaseFilter):\n    \"\"\"Univariate feature selector with configurable strategy.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable, default=f_classif\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues). For modes 'percentile' or 'kbest' it can return\n        a single array scores.\n\n    mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'\n        Feature selection mode. Note that the `'percentile'` and `'kbest'`\n        modes are supporting unsupervised feature selection (when `y` is `None`).\n\n    param : \"all\", float or int, default=1e-5\n        Parameter of the corresponding mode.\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores, None if `score_func` returned scores only.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    mutual_info_classif : Mutual information for a discrete target.\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    f_regression : F-value between label/feature for regression tasks.\n    mutual_info_regression : Mutual information for a continuous target.\n    SelectPercentile : Select features based on percentile of the highest\n        scores.\n    SelectKBest : Select features based on the k highest scores.\n    SelectFpr : Select features based on a false positive rate test.\n    SelectFdr : Select features based on an estimated false discovery rate.\n    SelectFwe : Select features based on family-wise error rate.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.feature_selection import GenericUnivariateSelect, chi2\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> X.shape\n    (569, 30)\n    >>> transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n    >>> X_new = transformer.fit_transform(X, y)\n    >>> X_new.shape\n    (569, 20)\n    \"\"\"\n\n    _selection_modes: dict = {\n        \"percentile\": SelectPercentile,\n        \"k_best\": SelectKBest,\n        \"fpr\": SelectFpr,\n        \"fdr\": SelectFdr,\n        \"fwe\": SelectFwe,\n    }\n\n    _parameter_constraints: dict = {\n        **_BaseFilter._parameter_constraints,\n        \"mode\": [StrOptions(set(_selection_modes.keys()))],\n        \"param\": [Interval(Real, 0, None, closed=\"left\"), StrOptions({\"all\"})],\n    }\n\n    def __init__(self, score_func=f_classif, *, mode=\"percentile\", param=1e-5):\n        super().__init__(score_func=score_func)\n        self.mode = mode\n        self.param = param\n\n    def _make_selector(self):\n        selector = self._selection_modes[self.mode](score_func=self.score_func)\n\n        # Now perform some acrobatics to set the right named parameter in\n        # the selector\n        possible_params = selector._get_param_names()\n        possible_params.remove(\"score_func\")\n        selector.set_params(**{possible_params[0]: self.param})\n\n        return selector\n\n    def _more_tags(self):\n        return {\"preserves_dtype\": [np.float64, np.float32]}\n\n    def _check_params(self, X, y):\n        self._make_selector()._check_params(X, y)\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n\n        selector = self._make_selector()\n        selector.pvalues_ = self.pvalues_\n        selector.scores_ = self.scores_\n        return selector._get_support_mask()\n", "class_fn": true, "question_id": "sklearn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_selection/_univariate_selection.py", "fn_id": "", "content": "class SelectFwe(_BaseFilter):\n    \"\"\"Filter: Select the p-values corresponding to Family-wise error rate.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable, default=f_classif\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues).\n        Default is f_classif (see below \"See Also\"). The default function only\n        works with classification tasks.\n\n    alpha : float, default=5e-2\n        The highest uncorrected p-value for features to keep.\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    f_regression : F-value between label/feature for regression tasks.\n    SelectPercentile : Select features based on percentile of the highest\n        scores.\n    SelectKBest : Select features based on the k highest scores.\n    SelectFpr : Select features based on a false positive rate test.\n    SelectFdr : Select features based on an estimated false discovery rate.\n    GenericUnivariateSelect : Univariate feature selector with configurable\n        mode.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.feature_selection import SelectFwe, chi2\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> X.shape\n    (569, 30)\n    >>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n    >>> X_new.shape\n    (569, 15)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BaseFilter._parameter_constraints,\n        \"alpha\": [Interval(Real, 0, 1, closed=\"both\")],\n    }\n\n    def __init__(self, score_func=f_classif, *, alpha=5e-2):\n        super().__init__(score_func=score_func)\n        self.alpha = alpha\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n\n        return self.pvalues_ < self.alpha / len(self.pvalues_)\n", "class_fn": true, "question_id": "sklearn/sklearn.feature_selection._univariate_selection/SelectFwe", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_selection/_variance_threshold.py", "fn_id": "", "content": "class VarianceThreshold(SelectorMixin, BaseEstimator):\n    \"\"\"Feature selector that removes all low-variance features.\n\n    This feature selection algorithm looks only at the features (X), not the\n    desired outputs (y), and can thus be used for unsupervised learning.\n\n    Read more in the :ref:`User Guide <variance_threshold>`.\n\n    Parameters\n    ----------\n    threshold : float, default=0\n        Features with a training-set variance lower than this threshold will\n        be removed. The default is to keep all features with non-zero variance,\n        i.e. remove the features that have the same value in all samples.\n\n    Attributes\n    ----------\n    variances_ : array, shape (n_features,)\n        Variances of individual features.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    SelectFromModel: Meta-transformer for selecting features based on\n        importance weights.\n    SelectPercentile : Select features according to a percentile of the highest\n        scores.\n    SequentialFeatureSelector : Transformer that performs Sequential Feature\n        Selection.\n\n    Notes\n    -----\n    Allows NaN in the input.\n    Raises ValueError if no feature in X meets the variance threshold.\n\n    Examples\n    --------\n    The following dataset has integer features, two of which are the same\n    in every sample. These are removed with the default setting for threshold::\n\n        >>> from sklearn.feature_selection import VarianceThreshold\n        >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n        >>> selector = VarianceThreshold()\n        >>> selector.fit_transform(X)\n        array([[2, 0],\n               [1, 4],\n               [1, 1]])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"threshold\": [Interval(Real, 0, None, closed=\"left\")]\n    }\n\n    def __init__(self, threshold=0.0):\n        self.threshold = threshold\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Learn empirical variances from X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Data from which to compute variances, where `n_samples` is\n            the number of samples and `n_features` is the number of features.\n\n        y : any, default=None\n            Ignored. This parameter exists only for compatibility with\n            sklearn.pipeline.Pipeline.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = self._validate_data(\n            X,\n            accept_sparse=(\"csr\", \"csc\"),\n            dtype=np.float64,\n            force_all_finite=\"allow-nan\",\n        )\n\n        if hasattr(X, \"toarray\"):  # sparse matrix\n            _, self.variances_ = mean_variance_axis(X, axis=0)\n            if self.threshold == 0:\n                mins, maxes = min_max_axis(X, axis=0)\n                peak_to_peaks = maxes - mins\n        else:\n            self.variances_ = np.nanvar(X, axis=0)\n            if self.threshold == 0:\n                peak_to_peaks = np.ptp(X, axis=0)\n\n        if self.threshold == 0:\n            # Use peak-to-peak to avoid numeric precision issues\n            # for constant features\n            compare_arr = np.array([self.variances_, peak_to_peaks])\n            self.variances_ = np.nanmin(compare_arr, axis=0)\n\n        if np.all(~np.isfinite(self.variances_) | (self.variances_ <= self.threshold)):\n            msg = \"No feature in X meets the variance threshold {0:.5f}\"\n            if X.shape[0] == 1:\n                msg += \" (X contains only one sample)\"\n            raise ValueError(msg.format(self.threshold))\n\n        return self\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n\n        return self.variances_ > self.threshold\n\n    def _more_tags(self):\n        return {\"allow_nan\": True}\n", "class_fn": true, "question_id": "sklearn/sklearn.feature_selection._variance_threshold/VarianceThreshold", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/gaussian_process/_gpr.py", "fn_id": "", "content": "class GaussianProcessRegressor(MultiOutputMixin, RegressorMixin, BaseEstimator):\n    \"\"\"Gaussian process regression (GPR).\n\n    The implementation is based on Algorithm 2.1 of [RW2006]_.\n\n    In addition to standard scikit-learn estimator API,\n    :class:`GaussianProcessRegressor`:\n\n       * allows prediction without prior fitting (based on the GP prior)\n       * provides an additional method `sample_y(X)`, which evaluates samples\n         drawn from the GPR (prior or posterior) at given inputs\n       * exposes a method `log_marginal_likelihood(theta)`, which can be used\n         externally for other ways of selecting hyperparameters, e.g., via\n         Markov chain Monte Carlo.\n\n    To learn the difference between a point-estimate approach vs. a more\n    Bayesian modelling approach, refer to the example entitled\n    :ref:`sphx_glr_auto_examples_gaussian_process_plot_compare_gpr_krr.py`.\n\n    Read more in the :ref:`User Guide <gaussian_process>`.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    kernel : kernel instance, default=None\n        The kernel specifying the covariance function of the GP. If None is\n        passed, the kernel ``ConstantKernel(1.0, constant_value_bounds=\"fixed\")\n        * RBF(1.0, length_scale_bounds=\"fixed\")`` is used as default. Note that\n        the kernel hyperparameters are optimized during fitting unless the\n        bounds are marked as \"fixed\".\n\n    alpha : float or ndarray of shape (n_samples,), default=1e-10\n        Value added to the diagonal of the kernel matrix during fitting.\n        This can prevent a potential numerical issue during fitting, by\n        ensuring that the calculated values form a positive definite matrix.\n        It can also be interpreted as the variance of additional Gaussian\n        measurement noise on the training observations. Note that this is\n        different from using a `WhiteKernel`. If an array is passed, it must\n        have the same number of entries as the data used for fitting and is\n        used as datapoint-dependent noise level. Allowing to specify the\n        noise level directly as a parameter is mainly for convenience and\n        for consistency with :class:`~sklearn.linear_model.Ridge`.\n\n    optimizer : \"fmin_l_bfgs_b\", callable or None, default=\"fmin_l_bfgs_b\"\n        Can either be one of the internally supported optimizers for optimizing\n        the kernel's parameters, specified by a string, or an externally\n        defined optimizer passed as a callable. If a callable is passed, it\n        must have the signature::\n\n            def optimizer(obj_func, initial_theta, bounds):\n                # * 'obj_func': the objective function to be minimized, which\n                #   takes the hyperparameters theta as a parameter and an\n                #   optional flag eval_gradient, which determines if the\n                #   gradient is returned additionally to the function value\n                # * 'initial_theta': the initial value for theta, which can be\n                #   used by local optimizers\n                # * 'bounds': the bounds on the values of theta\n                ....\n                # Returned are the best found hyperparameters theta and\n                # the corresponding value of the target function.\n                return theta_opt, func_min\n\n        Per default, the L-BFGS-B algorithm from `scipy.optimize.minimize`\n        is used. If None is passed, the kernel's parameters are kept fixed.\n        Available internal optimizers are: `{'fmin_l_bfgs_b'}`.\n\n    n_restarts_optimizer : int, default=0\n        The number of restarts of the optimizer for finding the kernel's\n        parameters which maximize the log-marginal likelihood. The first run\n        of the optimizer is performed from the kernel's initial parameters,\n        the remaining ones (if any) from thetas sampled log-uniform randomly\n        from the space of allowed theta-values. If greater than 0, all bounds\n        must be finite. Note that `n_restarts_optimizer == 0` implies that one\n        run is performed.\n\n    normalize_y : bool, default=False\n        Whether or not to normalize the target values `y` by removing the mean\n        and scaling to unit-variance. This is recommended for cases where\n        zero-mean, unit-variance priors are used. Note that, in this\n        implementation, the normalisation is reversed before the GP predictions\n        are reported.\n\n        .. versionchanged:: 0.23\n\n    copy_X_train : bool, default=True\n        If True, a persistent copy of the training data is stored in the\n        object. Otherwise, just a reference to the training data is stored,\n        which might cause predictions to change if the data is modified\n        externally.\n\n    n_targets : int, default=None\n        The number of dimensions of the target values. Used to decide the number\n        of outputs when sampling from the prior distributions (i.e. calling\n        :meth:`sample_y` before :meth:`fit`). This parameter is ignored once\n        :meth:`fit` has been called.\n\n        .. versionadded:: 1.3\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation used to initialize the centers.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    X_train_ : array-like of shape (n_samples, n_features) or list of object\n        Feature vectors or other representations of training data (also\n        required for prediction).\n\n    y_train_ : array-like of shape (n_samples,) or (n_samples, n_targets)\n        Target values in training data (also required for prediction).\n\n    kernel_ : kernel instance\n        The kernel used for prediction. The structure of the kernel is the\n        same as the one passed as parameter but with optimized hyperparameters.\n\n    L_ : array-like of shape (n_samples, n_samples)\n        Lower-triangular Cholesky decomposition of the kernel in ``X_train_``.\n\n    alpha_ : array-like of shape (n_samples,)\n        Dual coefficients of training data points in kernel space.\n\n    log_marginal_likelihood_value_ : float\n        The log-marginal-likelihood of ``self.kernel_.theta``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    GaussianProcessClassifier : Gaussian process classification (GPC)\n        based on Laplace approximation.\n\n    References\n    ----------\n    .. [RW2006] `Carl E. Rasmussen and Christopher K.I. Williams,\n       \"Gaussian Processes for Machine Learning\",\n       MIT Press 2006 <https://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_friedman2\n    >>> from sklearn.gaussian_process import GaussianProcessRegressor\n    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n    >>> kernel = DotProduct() + WhiteKernel()\n    >>> gpr = GaussianProcessRegressor(kernel=kernel,\n    ...         random_state=0).fit(X, y)\n    >>> gpr.score(X, y)\n    0.3680...\n    >>> gpr.predict(X[:2,:], return_std=True)\n    (array([653.0..., 592.1...]), array([316.6..., 316.6...]))\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"kernel\": [None, Kernel],\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), np.ndarray],\n        \"optimizer\": [StrOptions({\"fmin_l_bfgs_b\"}), callable, None],\n        \"n_restarts_optimizer\": [Interval(Integral, 0, None, closed=\"left\")],\n        \"normalize_y\": [\"boolean\"],\n        \"copy_X_train\": [\"boolean\"],\n        \"n_targets\": [Interval(Integral, 1, None, closed=\"left\"), None],\n        \"random_state\": [\"random_state\"],\n    }\n\n    def __init__(\n        self,\n        kernel=None,\n        *,\n        alpha=1e-10,\n        optimizer=\"fmin_l_bfgs_b\",\n        n_restarts_optimizer=0,\n        normalize_y=False,\n        copy_X_train=True,\n        n_targets=None,\n        random_state=None,\n    ):\n        self.kernel = kernel\n        self.alpha = alpha\n        self.optimizer = optimizer\n        self.n_restarts_optimizer = n_restarts_optimizer\n        self.normalize_y = normalize_y\n        self.copy_X_train = copy_X_train\n        self.n_targets = n_targets\n        self.random_state = random_state\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y):\n        \"\"\"Fit Gaussian process regression model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or list of object\n            Feature vectors or other representations of training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            GaussianProcessRegressor class instance.\n        \"\"\"\n        if self.kernel is None:  # Use an RBF kernel as default\n            self.kernel_ = C(1.0, constant_value_bounds=\"fixed\") * RBF(\n                1.0, length_scale_bounds=\"fixed\"\n            )\n        else:\n            self.kernel_ = clone(self.kernel)\n\n        self._rng = check_random_state(self.random_state)\n\n        if self.kernel_.requires_vector_input:\n            dtype, ensure_2d = \"numeric\", True\n        else:\n            dtype, ensure_2d = None, False\n        X, y = self._validate_data(\n            X,\n            y,\n            multi_output=True,\n            y_numeric=True,\n            ensure_2d=ensure_2d,\n            dtype=dtype,\n        )\n\n        n_targets_seen = y.shape[1] if y.ndim > 1 else 1\n        if self.n_targets is not None and n_targets_seen != self.n_targets:\n            raise ValueError(\n                \"The number of targets seen in `y` is different from the parameter \"\n                f\"`n_targets`. Got {n_targets_seen} != {self.n_targets}.\"\n            )\n\n        # Normalize target value\n        if self.normalize_y:\n            self._y_train_mean = np.mean(y, axis=0)\n            self._y_train_std = _handle_zeros_in_scale(np.std(y, axis=0), copy=False)\n\n            # Remove mean and make unit variance\n            y = (y - self._y_train_mean) / self._y_train_std\n\n        else:\n            shape_y_stats = (y.shape[1],) if y.ndim == 2 else 1\n            self._y_train_mean = np.zeros(shape=shape_y_stats)\n            self._y_train_std = np.ones(shape=shape_y_stats)\n\n        if np.iterable(self.alpha) and self.alpha.shape[0] != y.shape[0]:\n            if self.alpha.shape[0] == 1:\n                self.alpha = self.alpha[0]\n            else:\n                raise ValueError(\n                    \"alpha must be a scalar or an array with same number of \"\n                    f\"entries as y. ({self.alpha.shape[0]} != {y.shape[0]})\"\n                )\n\n        self.X_train_ = np.copy(X) if self.copy_X_train else X\n        self.y_train_ = np.copy(y) if self.copy_X_train else y\n\n        if self.optimizer is not None and self.kernel_.n_dims > 0:\n            # Choose hyperparameters based on maximizing the log-marginal\n            # likelihood (potentially starting from several initial values)\n            def obj_func(theta, eval_gradient=True):\n                if eval_gradient:\n                    lml, grad = self.log_marginal_likelihood(\n                        theta, eval_gradient=True, clone_kernel=False\n                    )\n                    return -lml, -grad\n                else:\n                    return -self.log_marginal_likelihood(theta, clone_kernel=False)\n\n            # First optimize starting from theta specified in kernel\n            optima = [\n                (\n                    self._constrained_optimization(\n                        obj_func, self.kernel_.theta, self.kernel_.bounds\n                    )\n                )\n            ]\n\n            # Additional runs are performed from log-uniform chosen initial\n            # theta\n            if self.n_restarts_optimizer > 0:\n                if not np.isfinite(self.kernel_.bounds).all():\n                    raise ValueError(\n                        \"Multiple optimizer restarts (n_restarts_optimizer>0) \"\n                        \"requires that all bounds are finite.\"\n                    )\n                bounds = self.kernel_.bounds\n                for iteration in range(self.n_restarts_optimizer):\n                    theta_initial = self._rng.uniform(bounds[:, 0], bounds[:, 1])\n                    optima.append(\n                        self._constrained_optimization(obj_func, theta_initial, bounds)\n                    )\n            # Select result from run with minimal (negative) log-marginal\n            # likelihood\n            lml_values = list(map(itemgetter(1), optima))\n            self.kernel_.theta = optima[np.argmin(lml_values)][0]\n            self.kernel_._check_bounds_params()\n\n            self.log_marginal_likelihood_value_ = -np.min(lml_values)\n        else:\n            self.log_marginal_likelihood_value_ = self.log_marginal_likelihood(\n                self.kernel_.theta, clone_kernel=False\n            )\n\n        # Precompute quantities required for predictions which are independent\n        # of actual query points\n        # Alg. 2.1, page 19, line 2 -> L = cholesky(K + sigma^2 I)\n        K = self.kernel_(self.X_train_)\n        K[np.diag_indices_from(K)] += self.alpha\n        try:\n            self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n        except np.linalg.LinAlgError as exc:\n            exc.args = (\n                (\n                    f\"The kernel, {self.kernel_}, is not returning a positive \"\n                    \"definite matrix. Try gradually increasing the 'alpha' \"\n                    \"parameter of your GaussianProcessRegressor estimator.\"\n                ),\n            ) + exc.args\n            raise\n        # Alg 2.1, page 19, line 3 -> alpha = L^T \\ (L \\ y)\n        self.alpha_ = cho_solve(\n            (self.L_, GPR_CHOLESKY_LOWER),\n            self.y_train_,\n            check_finite=False,\n        )\n        return self\n\n    def predict(self, X, return_std=False, return_cov=False):\n        \"\"\"Predict using the Gaussian process regression model.\n\n        We can also predict based on an unfitted model by using the GP prior.\n        In addition to the mean of the predictive distribution, optionally also\n        returns its standard deviation (`return_std=True`) or covariance\n        (`return_cov=True`). Note that at most one of the two can be requested.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or list of object\n            Query points where the GP is evaluated.\n\n        return_std : bool, default=False\n            If True, the standard-deviation of the predictive distribution at\n            the query points is returned along with the mean.\n\n        return_cov : bool, default=False\n            If True, the covariance of the joint predictive distribution at\n            the query points is returned along with the mean.\n\n        Returns\n        -------\n        y_mean : ndarray of shape (n_samples,) or (n_samples, n_targets)\n            Mean of predictive distribution at query points.\n\n        y_std : ndarray of shape (n_samples,) or (n_samples, n_targets), optional\n            Standard deviation of predictive distribution at query points.\n            Only returned when `return_std` is True.\n\n        y_cov : ndarray of shape (n_samples, n_samples) or \\\n                (n_samples, n_samples, n_targets), optional\n            Covariance of joint predictive distribution at query points.\n            Only returned when `return_cov` is True.\n        \"\"\"\n        if return_std and return_cov:\n            raise RuntimeError(\n                \"At most one of return_std or return_cov can be requested.\"\n            )\n\n        if self.kernel is None or self.kernel.requires_vector_input:\n            dtype, ensure_2d = \"numeric\", True\n        else:\n            dtype, ensure_2d = None, False\n\n        X = self._validate_data(X, ensure_2d=ensure_2d, dtype=dtype, reset=False)\n\n        if not hasattr(self, \"X_train_\"):  # Unfitted;predict based on GP prior\n            if self.kernel is None:\n                kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(\n                    1.0, length_scale_bounds=\"fixed\"\n                )\n            else:\n                kernel = self.kernel\n\n            n_targets = self.n_targets if self.n_targets is not None else 1\n            y_mean = np.zeros(shape=(X.shape[0], n_targets)).squeeze()\n\n            if return_cov:\n                y_cov = kernel(X)\n                if n_targets > 1:\n                    y_cov = np.repeat(\n                        np.expand_dims(y_cov, -1), repeats=n_targets, axis=-1\n                    )\n                return y_mean, y_cov\n            elif return_std:\n                y_var = kernel.diag(X)\n                if n_targets > 1:\n                    y_var = np.repeat(\n                        np.expand_dims(y_var, -1), repeats=n_targets, axis=-1\n                    )\n                return y_mean, np.sqrt(y_var)\n            else:\n                return y_mean\n        else:  # Predict based on GP posterior\n            # Alg 2.1, page 19, line 4 -> f*_bar = K(X_test, X_train) . alpha\n            K_trans = self.kernel_(X, self.X_train_)\n            y_mean = K_trans @ self.alpha_\n\n            # undo normalisation\n            y_mean = self._y_train_std * y_mean + self._y_train_mean\n\n            # if y_mean has shape (n_samples, 1), reshape to (n_samples,)\n            if y_mean.ndim > 1 and y_mean.shape[1] == 1:\n                y_mean = np.squeeze(y_mean, axis=1)\n\n            # Alg 2.1, page 19, line 5 -> v = L \\ K(X_test, X_train)^T\n            V = solve_triangular(\n                self.L_, K_trans.T, lower=GPR_CHOLESKY_LOWER, check_finite=False\n            )\n\n            if return_cov:\n                # Alg 2.1, page 19, line 6 -> K(X_test, X_test) - v^T. v\n                y_cov = self.kernel_(X) - V.T @ V\n\n                # undo normalisation\n                y_cov = np.outer(y_cov, self._y_train_std**2).reshape(*y_cov.shape, -1)\n                # if y_cov has shape (n_samples, n_samples, 1), reshape to\n                # (n_samples, n_samples)\n                if y_cov.shape[2] == 1:\n                    y_cov = np.squeeze(y_cov, axis=2)\n\n                return y_mean, y_cov\n            elif return_std:\n                # Compute variance of predictive distribution\n                # Use einsum to avoid explicitly forming the large matrix\n                # V^T @ V just to extract its diagonal afterward.\n                y_var = self.kernel_.diag(X).copy()\n                y_var -= np.einsum(\"ij,ji->i\", V.T, V)\n\n                # Check if any of the variances is negative because of\n                # numerical issues. If yes: set the variance to 0.\n                y_var_negative = y_var < 0\n                if np.any(y_var_negative):\n                    warnings.warn(\n                        \"Predicted variances smaller than 0. \"\n                        \"Setting those variances to 0.\"\n                    )\n                    y_var[y_var_negative] = 0.0\n\n                # undo normalisation\n                y_var = np.outer(y_var, self._y_train_std**2).reshape(*y_var.shape, -1)\n\n                # if y_var has shape (n_samples, 1), reshape to (n_samples,)\n                if y_var.shape[1] == 1:\n                    y_var = np.squeeze(y_var, axis=1)\n\n                return y_mean, np.sqrt(y_var)\n            else:\n                return y_mean\n\n    def sample_y(self, X, n_samples=1, random_state=0):\n        \"\"\"Draw samples from Gaussian process and evaluate at X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples_X, n_features) or list of object\n            Query points where the GP is evaluated.\n\n        n_samples : int, default=1\n            Number of samples drawn from the Gaussian process per query point.\n\n        random_state : int, RandomState instance or None, default=0\n            Determines random number generation to randomly draw samples.\n            Pass an int for reproducible results across multiple function\n            calls.\n            See :term:`Glossary <random_state>`.\n\n        Returns\n        -------\n        y_samples : ndarray of shape (n_samples_X, n_samples), or \\\n            (n_samples_X, n_targets, n_samples)\n            Values of n_samples samples drawn from Gaussian process and\n            evaluated at query points.\n        \"\"\"\n        rng = check_random_state(random_state)\n\n        y_mean, y_cov = self.predict(X, return_cov=True)\n        if y_mean.ndim == 1:\n            y_samples = rng.multivariate_normal(y_mean, y_cov, n_samples).T\n        else:\n            y_samples = [\n                rng.multivariate_normal(\n                    y_mean[:, target], y_cov[..., target], n_samples\n                ).T[:, np.newaxis]\n                for target in range(y_mean.shape[1])\n            ]\n            y_samples = np.hstack(y_samples)\n        return y_samples\n\n    def log_marginal_likelihood(\n        self, theta=None, eval_gradient=False, clone_kernel=True\n    ):\n        \"\"\"Return log-marginal likelihood of theta for training data.\n\n        Parameters\n        ----------\n        theta : array-like of shape (n_kernel_params,) default=None\n            Kernel hyperparameters for which the log-marginal likelihood is\n            evaluated. If None, the precomputed log_marginal_likelihood\n            of ``self.kernel_.theta`` is returned.\n\n        eval_gradient : bool, default=False\n            If True, the gradient of the log-marginal likelihood with respect\n            to the kernel hyperparameters at position theta is returned\n            additionally. If True, theta must not be None.\n\n        clone_kernel : bool, default=True\n            If True, the kernel attribute is copied. If False, the kernel\n            attribute is modified, but may result in a performance improvement.\n\n        Returns\n        -------\n        log_likelihood : float\n            Log-marginal likelihood of theta for training data.\n\n        log_likelihood_gradient : ndarray of shape (n_kernel_params,), optional\n            Gradient of the log-marginal likelihood with respect to the kernel\n            hyperparameters at position theta.\n            Only returned when eval_gradient is True.\n        \"\"\"\n        if theta is None:\n            if eval_gradient:\n                raise ValueError(\"Gradient can only be evaluated for theta!=None\")\n            return self.log_marginal_likelihood_value_\n\n        if clone_kernel:\n            kernel = self.kernel_.clone_with_theta(theta)\n        else:\n            kernel = self.kernel_\n            kernel.theta = theta\n\n        if eval_gradient:\n            K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n        else:\n            K = kernel(self.X_train_)\n\n        # Alg. 2.1, page 19, line 2 -> L = cholesky(K + sigma^2 I)\n        K[np.diag_indices_from(K)] += self.alpha\n        try:\n            L = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n        except np.linalg.LinAlgError:\n            return (-np.inf, np.zeros_like(theta)) if eval_gradient else -np.inf\n\n        # Support multi-dimensional output of self.y_train_\n        y_train = self.y_train_\n        if y_train.ndim == 1:\n            y_train = y_train[:, np.newaxis]\n\n        # Alg 2.1, page 19, line 3 -> alpha = L^T \\ (L \\ y)\n        alpha = cho_solve((L, GPR_CHOLESKY_LOWER), y_train, check_finite=False)\n\n        # Alg 2.1, page 19, line 7\n        # -0.5 . y^T . alpha - sum(log(diag(L))) - n_samples / 2 log(2*pi)\n        # y is originally thought to be a (1, n_samples) row vector. However,\n        # in multioutputs, y is of shape (n_samples, 2) and we need to compute\n        # y^T . alpha for each output, independently using einsum. Thus, it\n        # is equivalent to:\n        # for output_idx in range(n_outputs):\n        #     log_likelihood_dims[output_idx] = (\n        #         y_train[:, [output_idx]] @ alpha[:, [output_idx]]\n        #     )\n        log_likelihood_dims = -0.5 * np.einsum(\"ik,ik->k\", y_train, alpha)\n        log_likelihood_dims -= np.log(np.diag(L)).sum()\n        log_likelihood_dims -= K.shape[0] / 2 * np.log(2 * np.pi)\n        # the log likehood is sum-up across the outputs\n        log_likelihood = log_likelihood_dims.sum(axis=-1)\n\n        if eval_gradient:\n            # Eq. 5.9, p. 114, and footnote 5 in p. 114\n            # 0.5 * trace((alpha . alpha^T - K^-1) . K_gradient)\n            # alpha is supposed to be a vector of (n_samples,) elements. With\n            # multioutputs, alpha is a matrix of size (n_samples, n_outputs).\n            # Therefore, we want to construct a matrix of\n            # (n_samples, n_samples, n_outputs) equivalent to\n            # for output_idx in range(n_outputs):\n            #     output_alpha = alpha[:, [output_idx]]\n            #     inner_term[..., output_idx] = output_alpha @ output_alpha.T\n            inner_term = np.einsum(\"ik,jk->ijk\", alpha, alpha)\n            # compute K^-1 of shape (n_samples, n_samples)\n            K_inv = cho_solve(\n                (L, GPR_CHOLESKY_LOWER), np.eye(K.shape[0]), check_finite=False\n            )\n            # create a new axis to use broadcasting between inner_term and\n            # K_inv\n            inner_term -= K_inv[..., np.newaxis]\n            # Since we are interested about the trace of\n            # inner_term @ K_gradient, we don't explicitly compute the\n            # matrix-by-matrix operation and instead use an einsum. Therefore\n            # it is equivalent to:\n            # for param_idx in range(n_kernel_params):\n            #     for output_idx in range(n_output):\n            #         log_likehood_gradient_dims[param_idx, output_idx] = (\n            #             inner_term[..., output_idx] @\n            #             K_gradient[..., param_idx]\n            #         )\n            log_likelihood_gradient_dims = 0.5 * np.einsum(\n                \"ijl,jik->kl\", inner_term, K_gradient\n            )\n            # the log likehood gradient is the sum-up across the outputs\n            log_likelihood_gradient = log_likelihood_gradient_dims.sum(axis=-1)\n\n        if eval_gradient:\n            return log_likelihood, log_likelihood_gradient\n        else:\n            return log_likelihood\n\n    def _constrained_optimization(self, obj_func, initial_theta, bounds):\n        if self.optimizer == \"fmin_l_bfgs_b\":\n            opt_res = scipy.optimize.minimize(\n                obj_func,\n                initial_theta,\n                method=\"L-BFGS-B\",\n                jac=True,\n                bounds=bounds,\n            )\n            _check_optimize_result(\"lbfgs\", opt_res)\n            theta_opt, func_min = opt_res.x, opt_res.fun\n        elif callable(self.optimizer):\n            theta_opt, func_min = self.optimizer(obj_func, initial_theta, bounds=bounds)\n        else:\n            raise ValueError(f\"Unknown optimizer {self.optimizer}.\")\n\n        return theta_opt, func_min\n\n    def _more_tags(self):\n        return {\"requires_fit\": False}\n", "class_fn": true, "question_id": "sklearn/sklearn.gaussian_process._gpr/GaussianProcessRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/gaussian_process/kernels.py", "fn_id": "", "content": "class DotProduct(Kernel):\n    r\"\"\"Dot-Product kernel.\n\n    The DotProduct kernel is non-stationary and can be obtained from linear\n    regression by putting :math:`N(0, 1)` priors on the coefficients\n    of :math:`x_d (d = 1, . . . , D)` and a prior of :math:`N(0, \\sigma_0^2)`\n    on the bias. The DotProduct kernel is invariant to a rotation of\n    the coordinates about the origin, but not translations.\n    It is parameterized by a parameter sigma_0 :math:`\\sigma`\n    which controls the inhomogenity of the kernel. For :math:`\\sigma_0^2 =0`,\n    the kernel is called the homogeneous linear kernel, otherwise\n    it is inhomogeneous. The kernel is given by\n\n    .. math::\n        k(x_i, x_j) = \\sigma_0 ^ 2 + x_i \\cdot x_j\n\n    The DotProduct kernel is commonly combined with exponentiation.\n\n    See [1]_, Chapter 4, Section 4.2, for further details regarding the\n    DotProduct kernel.\n\n    Read more in the :ref:`User Guide <gp_kernels>`.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    sigma_0 : float >= 0, default=1.0\n        Parameter controlling the inhomogenity of the kernel. If sigma_0=0,\n        the kernel is homogeneous.\n\n    sigma_0_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n        The lower and upper bound on 'sigma_0'.\n        If set to \"fixed\", 'sigma_0' cannot be changed during\n        hyperparameter tuning.\n\n    References\n    ----------\n    .. [1] `Carl Edward Rasmussen, Christopher K. I. Williams (2006).\n        \"Gaussian Processes for Machine Learning\". The MIT Press.\n        <http://www.gaussianprocess.org/gpml/>`_\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_friedman2\n    >>> from sklearn.gaussian_process import GaussianProcessRegressor\n    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n    >>> kernel = DotProduct() + WhiteKernel()\n    >>> gpr = GaussianProcessRegressor(kernel=kernel,\n    ...         random_state=0).fit(X, y)\n    >>> gpr.score(X, y)\n    0.3680...\n    >>> gpr.predict(X[:2,:], return_std=True)\n    (array([653.0..., 592.1...]), array([316.6..., 316.6...]))\n    \"\"\"\n\n    def __init__(self, sigma_0=1.0, sigma_0_bounds=(1e-5, 1e5)):\n        self.sigma_0 = sigma_0\n        self.sigma_0_bounds = sigma_0_bounds\n\n    @property\n    def hyperparameter_sigma_0(self):\n        return Hyperparameter(\"sigma_0\", \"numeric\", self.sigma_0_bounds)\n\n    def __call__(self, X, Y=None, eval_gradient=False):\n        \"\"\"Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : ndarray of shape (n_samples_Y, n_features), default=None\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool, default=False\n            Determines whether the gradient with respect to the log of\n            the kernel hyperparameter is computed.\n            Only supported when Y is None.\n\n        Returns\n        -------\n        K : ndarray of shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),\\\n                optional\n            The gradient of the kernel k(X, X) with respect to the log of the\n            hyperparameter of the kernel. Only returned when `eval_gradient`\n            is True.\n        \"\"\"\n        X = np.atleast_2d(X)\n        if Y is None:\n            K = np.inner(X, X) + self.sigma_0**2\n        else:\n            if eval_gradient:\n                raise ValueError(\"Gradient can only be evaluated when Y is None.\")\n            K = np.inner(X, Y) + self.sigma_0**2\n\n        if eval_gradient:\n            if not self.hyperparameter_sigma_0.fixed:\n                K_gradient = np.empty((K.shape[0], K.shape[1], 1))\n                K_gradient[..., 0] = 2 * self.sigma_0**2\n                return K, K_gradient\n            else:\n                return K, np.empty((X.shape[0], X.shape[0], 0))\n        else:\n            return K\n\n    def diag(self, X):\n        \"\"\"Returns the diagonal of the kernel k(X, X).\n\n        The result of this method is identical to np.diag(self(X)); however,\n        it can be evaluated more efficiently since only the diagonal is\n        evaluated.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y).\n\n        Returns\n        -------\n        K_diag : ndarray of shape (n_samples_X,)\n            Diagonal of kernel k(X, X).\n        \"\"\"\n        return np.einsum(\"ij,ij->i\", X, X) + self.sigma_0**2\n\n    def is_stationary(self):\n        \"\"\"Returns whether the kernel is stationary.\"\"\"\n        return False\n\n    def __repr__(self):\n        return \"{0}(sigma_0={1:.3g})\".format(self.__class__.__name__, self.sigma_0)\n", "class_fn": true, "question_id": "sklearn/sklearn.gaussian_process.kernels/DotProduct", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/gaussian_process/kernels.py", "fn_id": "", "content": "class GenericKernelMixin:\n    \"\"\"Mixin for kernels which operate on generic objects such as variable-\n    length sequences, trees, and graphs.\n\n    .. versionadded:: 0.22\n    \"\"\"\n\n    @property\n    def requires_vector_input(self):\n        \"\"\"Whether the kernel works only on fixed-length feature vectors.\"\"\"\n        return False\n", "class_fn": true, "question_id": "sklearn/sklearn.gaussian_process.kernels/GenericKernelMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/gaussian_process/kernels.py", "fn_id": "", "content": "class KernelOperator(Kernel):\n    \"\"\"Base class for all kernel operators.\n\n    .. versionadded:: 0.18\n    \"\"\"\n\n    def __init__(self, k1, k2):\n        self.k1 = k1\n        self.k2 = k2\n\n    def get_params(self, deep=True):\n        \"\"\"Get parameters of this kernel.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        params = dict(k1=self.k1, k2=self.k2)\n        if deep:\n            deep_items = self.k1.get_params().items()\n            params.update((\"k1__\" + k, val) for k, val in deep_items)\n            deep_items = self.k2.get_params().items()\n            params.update((\"k2__\" + k, val) for k, val in deep_items)\n\n        return params\n\n    @property\n    def hyperparameters(self):\n        \"\"\"Returns a list of all hyperparameter.\"\"\"\n        r = [\n            Hyperparameter(\n                \"k1__\" + hyperparameter.name,\n                hyperparameter.value_type,\n                hyperparameter.bounds,\n                hyperparameter.n_elements,\n            )\n            for hyperparameter in self.k1.hyperparameters\n        ]\n\n        for hyperparameter in self.k2.hyperparameters:\n            r.append(\n                Hyperparameter(\n                    \"k2__\" + hyperparameter.name,\n                    hyperparameter.value_type,\n                    hyperparameter.bounds,\n                    hyperparameter.n_elements,\n                )\n            )\n        return r\n\n    @property\n    def theta(self):\n        \"\"\"Returns the (flattened, log-transformed) non-fixed hyperparameters.\n\n        Note that theta are typically the log-transformed values of the\n        kernel's hyperparameters as this representation of the search space\n        is more amenable for hyperparameter search, as hyperparameters like\n        length-scales naturally live on a log-scale.\n\n        Returns\n        -------\n        theta : ndarray of shape (n_dims,)\n            The non-fixed, log-transformed hyperparameters of the kernel\n        \"\"\"\n        return np.append(self.k1.theta, self.k2.theta)\n\n    @theta.setter\n    def theta(self, theta):\n        \"\"\"Sets the (flattened, log-transformed) non-fixed hyperparameters.\n\n        Parameters\n        ----------\n        theta : ndarray of shape (n_dims,)\n            The non-fixed, log-transformed hyperparameters of the kernel\n        \"\"\"\n        k1_dims = self.k1.n_dims\n        self.k1.theta = theta[:k1_dims]\n        self.k2.theta = theta[k1_dims:]\n\n    @property\n    def bounds(self):\n        \"\"\"Returns the log-transformed bounds on the theta.\n\n        Returns\n        -------\n        bounds : ndarray of shape (n_dims, 2)\n            The log-transformed bounds on the kernel's hyperparameters theta\n        \"\"\"\n        if self.k1.bounds.size == 0:\n            return self.k2.bounds\n        if self.k2.bounds.size == 0:\n            return self.k1.bounds\n        return np.vstack((self.k1.bounds, self.k2.bounds))\n\n    def __eq__(self, b):\n        if type(self) != type(b):\n            return False\n        return (self.k1 == b.k1 and self.k2 == b.k2) or (\n            self.k1 == b.k2 and self.k2 == b.k1\n        )\n\n    def is_stationary(self):\n        \"\"\"Returns whether the kernel is stationary.\"\"\"\n        return self.k1.is_stationary() and self.k2.is_stationary()\n\n    @property\n    def requires_vector_input(self):\n        \"\"\"Returns whether the kernel is stationary.\"\"\"\n        return self.k1.requires_vector_input or self.k2.requires_vector_input\n", "class_fn": true, "question_id": "sklearn/sklearn.gaussian_process.kernels/KernelOperator", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
