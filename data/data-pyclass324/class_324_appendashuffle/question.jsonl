{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/__config__.py", "fn_id": "", "content": "class DisplayModes(Enum):\n    stdout = 'stdout'\n    dicts = 'dicts'", "class_fn": true, "question_id": "numpy/numpy.__config__/DisplayModes", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_typing/_array_like.py", "fn_id": "", "content": "@runtime_checkable\nclass _SupportsArrayFunc(Protocol):\n    \"\"\"A protocol class representing `~class.__array_function__`.\"\"\"\n\n    def __array_function__(self, func: Callable[..., Any], types: Collection[type[Any]], args: tuple[Any, ...], kwargs: dict[str, Any]) -> object:\n        ...", "class_fn": true, "question_id": "numpy/numpy._typing._array_like/_SupportsArrayFunc", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_utils/_pep440.py", "fn_id": "", "content": "class LegacyVersion(_BaseVersion):\n\n    def _compare(self, other, method):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n        return method(self._key, other._key)\n\n    def __repr__(self):\n        return '<LegacyVersion({0})>'.format(repr(str(self)))\n\n    def __hash__(self):\n        return hash(self._key)\n\n    @property\n    def public(self):\n        return self._version\n\n    @property\n    def base_version(self):\n        return self._version\n\n    @property\n    def local(self):\n        return None\n\n    @property\n    def is_prerelease(self):\n        return False\n\n    @property\n    def is_postrelease(self):\n        return False\n\n    def __le__(self, other):\n        return self._compare(other, lambda s, o: s <= o)\n\n    def __str__(self):\n        return self._version\n\n    def __init__(self, version):\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)", "class_fn": true, "question_id": "numpy/numpy._utils._pep440/LegacyVersion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/array_api/_typing.py", "fn_id": "", "content": "class NestedSequence(Protocol[_T_co]):\n\n    def __getitem__(self, key: int, /) -> _T_co | NestedSequence[_T_co]:\n        ...\n\n    def __len__(self, /) -> int:\n        ...", "class_fn": true, "question_id": "numpy/numpy.array_api._typing/NestedSequence", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/_exceptions.py", "fn_id": "", "content": "class UFuncTypeError(TypeError):\n    \"\"\" Base class for all ufunc exceptions \"\"\"\n\n    def __init__(self, ufunc):\n        self.ufunc = ufunc", "class_fn": true, "question_id": "numpy/numpy.core._exceptions/UFuncTypeError", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/_exceptions.py", "fn_id": "", "content": "@_display_as_base\nclass _UFuncCastingError(UFuncTypeError):\n\n    def __init__(self, ufunc, casting, from_, to):\n        super().__init__(ufunc)\n        self.casting = casting\n        self.from_ = from_\n        self.to = to", "class_fn": true, "question_id": "numpy/numpy.core._exceptions/_UFuncCastingError", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/_exceptions.py", "fn_id": "", "content": "@_display_as_base\nclass _UFuncOutputCastingError(_UFuncCastingError):\n    \"\"\" Thrown when a ufunc output cannot be casted \"\"\"\n\n    def __init__(self, ufunc, casting, from_, to, i):\n        super().__init__(ufunc, casting, from_, to)\n        self.out_i = i\n\n    def __str__(self):\n        i_str = '{} '.format(self.out_i) if self.ufunc.nout != 1 else ''\n        return 'Cannot cast ufunc {!r} output {}from {!r} to {!r} with casting rule {!r}'.format(self.ufunc.__name__, i_str, self.from_, self.to, self.casting)", "class_fn": true, "question_id": "numpy/numpy.core._exceptions/_UFuncOutputCastingError", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/_internal.py", "fn_id": "", "content": "class dummy_ctype:\n\n    def __init__(self, cls):\n        self._cls = cls\n\n    def __mul__(self, other):\n        return self\n\n    def __call__(self, *other):\n        return self._cls(other)\n\n    def __eq__(self, other):\n        return self._cls == other._cls\n\n    def __ne__(self, other):\n        return self._cls != other._cls", "class_fn": true, "question_id": "numpy/numpy.core._internal/dummy_ctype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/arrayprint.py", "fn_id": "", "content": "class DatetimeFormat(_TimelikeFormat):\n\n    def __init__(self, x, unit=None, timezone=None, casting='same_kind', legacy=False):\n        if unit is None:\n            if x.dtype.kind == 'M':\n                unit = datetime_data(x.dtype)[0]\n            else:\n                unit = 's'\n        if timezone is None:\n            timezone = 'naive'\n        self.timezone = timezone\n        self.unit = unit\n        self.casting = casting\n        self.legacy = legacy\n        super().__init__(x)\n\n    def __call__(self, x):\n        if self.legacy <= 113:\n            return self._format_non_nat(x)\n        return super().__call__(x)\n\n    def _format_non_nat(self, x):\n        return \"'%s'\" % datetime_as_string(x, unit=self.unit, timezone=self.timezone, casting=self.casting)", "class_fn": true, "question_id": "numpy/numpy.core.arrayprint/DatetimeFormat", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/arrayprint.py", "fn_id": "", "content": "class SubArrayFormat:\n\n    def __init__(self, format_function, **options):\n        self.format_function = format_function\n        self.threshold = options['threshold']\n        self.edge_items = options['edgeitems']\n\n    def __call__(self, a):\n        self.summary_insert = '...' if a.size > self.threshold else ''\n        return self.format_array(a)\n\n    def format_array(self, a):\n        if np.ndim(a) == 0:\n            return self.format_function(a)\n        if self.summary_insert and a.shape[0] > 2 * self.edge_items:\n            formatted = [self.format_array(a_) for a_ in a[:self.edge_items]] + [self.summary_insert] + [self.format_array(a_) for a_ in a[-self.edge_items:]]\n        else:\n            formatted = [self.format_array(a_) for a_ in a]\n        return '[' + ', '.join(formatted) + ']'", "class_fn": true, "question_id": "numpy/numpy.core.arrayprint/SubArrayFormat", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/tests/test_machar.py", "fn_id": "", "content": "class TestMachAr:\n\n    def _run_machar_highprec(self):\n        try:\n            hiprec = ntypes.float96\n            MachAr(lambda v: array(v, hiprec))\n        except AttributeError:\n            'Skipping test: no ntypes.float96 available on this platform.'\n\n    def test_underlow(self):\n        with errstate(all='raise'):\n            try:\n                self._run_machar_highprec()\n            except FloatingPointError as e:\n                msg = 'Caught %s exception, should not have been raised.' % e\n                raise AssertionError(msg)", "class_fn": true, "question_id": "numpy/numpy.core.tests.test_machar/TestMachAr", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ctypeslib.py", "fn_id": "", "content": "class _ndptr(_ndptr_base):\n\n    @classmethod\n    def from_param(cls, obj):\n        if not isinstance(obj, ndarray):\n            raise TypeError('argument must be an ndarray')\n        if cls._dtype_ is not None and obj.dtype != cls._dtype_:\n            raise TypeError('array must have data type %s' % cls._dtype_)\n        if cls._ndim_ is not None and obj.ndim != cls._ndim_:\n            raise TypeError('array must have %d dimension(s)' % cls._ndim_)\n        if cls._shape_ is not None and obj.shape != cls._shape_:\n            raise TypeError('array must have shape %s' % str(cls._shape_))\n        if cls._flags_ is not None and obj.flags.num & cls._flags_ != cls._flags_:\n            raise TypeError('array must have flags %s' % _flags_fromnum(cls._flags_))\n        return obj.ctypes", "class_fn": true, "question_id": "numpy/numpy.ctypeslib/_ndptr", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/_shell_utils.py", "fn_id": "", "content": "class WindowsParser:\n    \"\"\"\n    The parsing behavior used by `subprocess.call(\"string\")` on Windows, which\n    matches the Microsoft C/C++ runtime.\n\n    Note that this is _not_ the behavior of cmd.\n    \"\"\"\n\n    @staticmethod\n    def join(argv):\n        return subprocess.list2cmdline(argv)\n\n    @staticmethod\n    def split(cmd):\n        import ctypes\n        try:\n            ctypes.windll\n        except AttributeError:\n            raise NotImplementedError\n        if not cmd:\n            return []\n        cmd = 'dummy ' + cmd\n        CommandLineToArgvW = ctypes.windll.shell32.CommandLineToArgvW\n        CommandLineToArgvW.restype = ctypes.POINTER(ctypes.c_wchar_p)\n        CommandLineToArgvW.argtypes = (ctypes.c_wchar_p, ctypes.POINTER(ctypes.c_int))\n        nargs = ctypes.c_int()\n        lpargs = CommandLineToArgvW(cmd, ctypes.byref(nargs))\n        args = [lpargs[i] for i in range(nargs.value)]\n        assert not ctypes.windll.kernel32.LocalFree(lpargs)\n        assert args[0] == 'dummy'\n        return args[1:]", "class_fn": true, "question_id": "numpy/numpy.distutils._shell_utils/WindowsParser", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/build_py.py", "fn_id": "", "content": "class build_py(old_build_py):\n\n    def run(self):\n        build_src = self.get_finalized_command('build_src')\n        if build_src.py_modules_dict and self.packages is None:\n            self.packages = list(build_src.py_modules_dict.keys())\n        old_build_py.run(self)\n\n    def find_package_modules(self, package, package_dir):\n        modules = old_build_py.find_package_modules(self, package, package_dir)\n        build_src = self.get_finalized_command('build_src')\n        modules += build_src.py_modules_dict.get(package, [])\n        return modules\n\n    def find_modules(self):\n        old_py_modules = self.py_modules[:]\n        new_py_modules = [_m for _m in self.py_modules if is_string(_m)]\n        self.py_modules[:] = new_py_modules\n        modules = old_build_py.find_modules(self)\n        self.py_modules[:] = old_py_modules\n        return modules", "class_fn": true, "question_id": "numpy/numpy.distutils.command.build_py/build_py", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/config_compiler.py", "fn_id": "", "content": "class config_cc(Command):\n    \"\"\" Distutils command to hold user specified options\n    to C/C++ compilers.\n    \"\"\"\n    description = 'specify C/C++ compiler information'\n    user_options = [('compiler=', None, 'specify C/C++ compiler type')]\n\n    def initialize_options(self):\n        self.compiler = None\n\n    def finalize_options(self):\n        log.info('unifing config_cc, config, build_clib, build_ext, build commands --compiler options')\n        build_clib = self.get_finalized_command('build_clib')\n        build_ext = self.get_finalized_command('build_ext')\n        config = self.get_finalized_command('config')\n        build = self.get_finalized_command('build')\n        cmd_list = [self, config, build_clib, build_ext, build]\n        for a in ['compiler']:\n            l = []\n            for c in cmd_list:\n                v = getattr(c, a)\n                if v is not None:\n                    if not isinstance(v, str):\n                        v = v.compiler_type\n                    if v not in l:\n                        l.append(v)\n            if not l:\n                v1 = None\n            else:\n                v1 = l[0]\n            if len(l) > 1:\n                log.warn('  commands have different --%s options: %s, using first in list as default' % (a, l))\n            if v1:\n                for c in cmd_list:\n                    if getattr(c, a) is None:\n                        setattr(c, a, v1)\n        return\n\n    def run(self):\n        return", "class_fn": true, "question_id": "numpy/numpy.distutils.command.config_compiler/config_cc", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/install_clib.py", "fn_id": "", "content": "class install_clib(Command):\n    description = 'Command to install installable C libraries'\n    user_options = []\n\n    def initialize_options(self):\n        self.install_dir = None\n        self.outfiles = []\n\n    def finalize_options(self):\n        self.set_undefined_options('install', ('install_lib', 'install_dir'))\n\n    def run(self):\n        build_clib_cmd = get_cmd('build_clib')\n        if not build_clib_cmd.build_clib:\n            build_clib_cmd.finalize_options()\n        build_dir = build_clib_cmd.build_clib\n        if not build_clib_cmd.compiler:\n            compiler = new_compiler(compiler=None)\n            compiler.customize(self.distribution)\n        else:\n            compiler = build_clib_cmd.compiler\n        for l in self.distribution.installed_libraries:\n            target_dir = os.path.join(self.install_dir, l.target_dir)\n            name = compiler.library_filename(l.name)\n            source = os.path.join(build_dir, name)\n            self.mkpath(target_dir)\n            self.outfiles.append(self.copy_file(source, target_dir)[0])\n\n    def get_outputs(self):\n        return self.outfiles", "class_fn": true, "question_id": "numpy/numpy.distutils.command.install_clib/install_clib", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/sdist.py", "fn_id": "", "content": "class sdist(old_sdist):\n\n    def add_defaults(self):\n        old_sdist.add_defaults(self)\n        dist = self.distribution\n        if dist.has_data_files():\n            for data in dist.data_files:\n                self.filelist.extend(get_data_files(data))\n        if dist.has_headers():\n            headers = []\n            for h in dist.headers:\n                if isinstance(h, str):\n                    headers.append(h)\n                else:\n                    headers.append(h[1])\n            self.filelist.extend(headers)\n        return", "class_fn": true, "question_id": "numpy/numpy.distutils.command.sdist/sdist", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/arm.py", "fn_id": "", "content": "class ArmFlangCompiler(FCompiler):\n    compiler_type = 'arm'\n    description = 'Arm Compiler'\n    version_pattern = '\\\\s*Arm.*version (?P<version>[\\\\d.-]+).*'\n    ar_exe = 'lib.exe'\n    possible_executables = ['armflang']\n    executables = {'version_cmd': ['', '--version'], 'compiler_f77': ['armflang', '-fPIC'], 'compiler_fix': ['armflang', '-fPIC', '-ffixed-form'], 'compiler_f90': ['armflang', '-fPIC'], 'linker_so': ['armflang', '-fPIC', '-shared'], 'archiver': ['ar', '-cr'], 'ranlib': None}\n    pic_flags = ['-fPIC', '-DPIC']\n    c_compiler = 'arm'\n    module_dir_switch = '-module '\n\n    def get_libraries(self):\n        opt = FCompiler.get_libraries(self)\n        opt.extend(['flang', 'flangrti', 'ompstub'])\n        return opt\n\n    @functools.lru_cache(maxsize=128)\n    def get_library_dirs(self):\n        \"\"\"List of compiler library directories.\"\"\"\n        opt = FCompiler.get_library_dirs(self)\n        flang_dir = dirname(self.executables['compiler_f77'][0])\n        opt.append(normpath(join(flang_dir, '..', 'lib')))\n        return opt\n\n    def _command_property(key):\n\n        def fget(self):\n            assert self._is_customised\n            return self.executables[key]\n        return property(fget=fget)\n\n    def get_flags_opt(self):\n        return ['-O3']\n\n    def customize(self, dist=None):\n        \"\"\"Customize Fortran compiler.\n\n        This method gets Fortran compiler specific information from\n        (i) class definition, (ii) environment, (iii) distutils config\n        files, and (iv) command line (later overrides earlier).\n\n        This method should be always called after constructing a\n        compiler instance. But not in __init__ because Distribution\n        instance is needed for (iii) and (iv).\n        \"\"\"\n        log.info('customize %s' % self.__class__.__name__)\n        self._is_customised = True\n        self.distutils_vars.use_distribution(dist)\n        self.command_vars.use_distribution(dist)\n        self.flag_vars.use_distribution(dist)\n        self.update_executables()\n        self.find_executables()\n        noopt = self.distutils_vars.get('noopt', False)\n        noarch = self.distutils_vars.get('noarch', noopt)\n        debug = self.distutils_vars.get('debug', False)\n        f77 = self.command_vars.compiler_f77\n        f90 = self.command_vars.compiler_f90\n        f77flags = []\n        f90flags = []\n        freeflags = []\n        fixflags = []\n        if f77:\n            f77 = _shell_utils.NativeParser.split(f77)\n            f77flags = self.flag_vars.f77\n        if f90:\n            f90 = _shell_utils.NativeParser.split(f90)\n            f90flags = self.flag_vars.f90\n            freeflags = self.flag_vars.free\n        fix = self.command_vars.compiler_fix\n        if fix:\n            fix = _shell_utils.NativeParser.split(fix)\n            fixflags = self.flag_vars.fix + f90flags\n        (oflags, aflags, dflags) = ([], [], [])\n\n        def get_flags(tag, flags):\n            flags.extend(getattr(self.flag_vars, tag))\n            this_get = getattr(self, 'get_flags_' + tag)\n            for (name, c, flagvar) in [('f77', f77, f77flags), ('f90', f90, f90flags), ('f90', fix, fixflags)]:\n                t = '%s_%s' % (tag, name)\n                if c and this_get is not getattr(self, 'get_flags_' + t):\n                    flagvar.extend(getattr(self.flag_vars, t))\n        if not noopt:\n            get_flags('opt', oflags)\n            if not noarch:\n                get_flags('arch', aflags)\n        if debug:\n            get_flags('debug', dflags)\n        fflags = self.flag_vars.flags + dflags + oflags + aflags\n        if f77:\n            self.set_commands(compiler_f77=f77 + f77flags + fflags)\n        if f90:\n            self.set_commands(compiler_f90=f90 + freeflags + f90flags + fflags)\n        if fix:\n            self.set_commands(compiler_fix=fix + fixflags + fflags)\n        linker_so = self.linker_so\n        if linker_so:\n            linker_so_flags = self.flag_vars.linker_so\n            if sys.platform.startswith('aix'):\n                python_lib = get_python_lib(standard_lib=1)\n                ld_so_aix = os.path.join(python_lib, 'config', 'ld_so_aix')\n                python_exp = os.path.join(python_lib, 'config', 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            if sys.platform.startswith('os400'):\n                from distutils.sysconfig import get_config_var\n                python_config = get_config_var('LIBPL')\n                ld_so_aix = os.path.join(python_config, 'ld_so_aix')\n                python_exp = os.path.join(python_config, 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            self.set_commands(linker_so=linker_so + linker_so_flags)\n        linker_exe = self.linker_exe\n        if linker_exe:\n            linker_exe_flags = self.flag_vars.linker_exe\n            self.set_commands(linker_exe=linker_exe + linker_exe_flags)\n        ar = self.command_vars.archiver\n        if ar:\n            arflags = self.flag_vars.ar\n            self.set_commands(archiver=[ar] + arflags)\n        self.set_library_dirs(self.get_library_dirs())\n        self.set_libraries(self.get_libraries())\n\n    def get_flags_arch(self):\n        return []\n\n    def get_version(self, force=False, ok_status=[0]):\n        assert self._is_customised\n        version = CCompiler.get_version(self, force=force, ok_status=ok_status)\n        if version is None:\n            raise CompilerNotFound()\n        return version\n\n    def get_flags(self):\n        return []\n\n    def get_flags_debug(self):\n        return ['-g']\n\n    def get_flags_free(self):\n        return []\n\n    def runtime_library_dir_option(self, dir):\n        return '-Wl,-rpath=%s' % dir", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.arm/ArmFlangCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/fujitsu.py", "fn_id": "", "content": "class FujitsuFCompiler(FCompiler):\n    compiler_type = 'fujitsu'\n    description = 'Fujitsu Fortran Compiler'\n    possible_executables = ['frt']\n    version_pattern = 'frt \\\\(FRT\\\\) (?P<version>[a-z\\\\d.]+)'\n    executables = {'version_cmd': ['<F77>', '--version'], 'compiler_f77': ['frt', '-Fixed'], 'compiler_fix': ['frt', '-Fixed'], 'compiler_f90': ['frt'], 'linker_so': ['frt', '-shared'], 'archiver': ['ar', '-cr'], 'ranlib': ['ranlib']}\n    pic_flags = ['-KPIC']\n    module_dir_switch = '-M'\n    module_include_switch = '-I'\n\n    def get_version(self, force=False, ok_status=[0]):\n        assert self._is_customised\n        version = CCompiler.get_version(self, force=force, ok_status=ok_status)\n        if version is None:\n            raise CompilerNotFound()\n        return version\n\n    def _command_property(key):\n\n        def fget(self):\n            assert self._is_customised\n            return self.executables[key]\n        return property(fget=fget)\n\n    def get_flags_opt(self):\n        return ['-O3']\n\n    def runtime_library_dir_option(self, dir):\n        return f'-Wl,-rpath={dir}'\n\n    def customize(self, dist=None):\n        \"\"\"Customize Fortran compiler.\n\n        This method gets Fortran compiler specific information from\n        (i) class definition, (ii) environment, (iii) distutils config\n        files, and (iv) command line (later overrides earlier).\n\n        This method should be always called after constructing a\n        compiler instance. But not in __init__ because Distribution\n        instance is needed for (iii) and (iv).\n        \"\"\"\n        log.info('customize %s' % self.__class__.__name__)\n        self._is_customised = True\n        self.distutils_vars.use_distribution(dist)\n        self.command_vars.use_distribution(dist)\n        self.flag_vars.use_distribution(dist)\n        self.update_executables()\n        self.find_executables()\n        noopt = self.distutils_vars.get('noopt', False)\n        noarch = self.distutils_vars.get('noarch', noopt)\n        debug = self.distutils_vars.get('debug', False)\n        f77 = self.command_vars.compiler_f77\n        f90 = self.command_vars.compiler_f90\n        f77flags = []\n        f90flags = []\n        freeflags = []\n        fixflags = []\n        if f77:\n            f77 = _shell_utils.NativeParser.split(f77)\n            f77flags = self.flag_vars.f77\n        if f90:\n            f90 = _shell_utils.NativeParser.split(f90)\n            f90flags = self.flag_vars.f90\n            freeflags = self.flag_vars.free\n        fix = self.command_vars.compiler_fix\n        if fix:\n            fix = _shell_utils.NativeParser.split(fix)\n            fixflags = self.flag_vars.fix + f90flags\n        (oflags, aflags, dflags) = ([], [], [])\n\n        def get_flags(tag, flags):\n            flags.extend(getattr(self.flag_vars, tag))\n            this_get = getattr(self, 'get_flags_' + tag)\n            for (name, c, flagvar) in [('f77', f77, f77flags), ('f90', f90, f90flags), ('f90', fix, fixflags)]:\n                t = '%s_%s' % (tag, name)\n                if c and this_get is not getattr(self, 'get_flags_' + t):\n                    flagvar.extend(getattr(self.flag_vars, t))\n        if not noopt:\n            get_flags('opt', oflags)\n            if not noarch:\n                get_flags('arch', aflags)\n        if debug:\n            get_flags('debug', dflags)\n        fflags = self.flag_vars.flags + dflags + oflags + aflags\n        if f77:\n            self.set_commands(compiler_f77=f77 + f77flags + fflags)\n        if f90:\n            self.set_commands(compiler_f90=f90 + freeflags + f90flags + fflags)\n        if fix:\n            self.set_commands(compiler_fix=fix + fixflags + fflags)\n        linker_so = self.linker_so\n        if linker_so:\n            linker_so_flags = self.flag_vars.linker_so\n            if sys.platform.startswith('aix'):\n                python_lib = get_python_lib(standard_lib=1)\n                ld_so_aix = os.path.join(python_lib, 'config', 'ld_so_aix')\n                python_exp = os.path.join(python_lib, 'config', 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            if sys.platform.startswith('os400'):\n                from distutils.sysconfig import get_config_var\n                python_config = get_config_var('LIBPL')\n                ld_so_aix = os.path.join(python_config, 'ld_so_aix')\n                python_exp = os.path.join(python_config, 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            self.set_commands(linker_so=linker_so + linker_so_flags)\n        linker_exe = self.linker_exe\n        if linker_exe:\n            linker_exe_flags = self.flag_vars.linker_exe\n            self.set_commands(linker_exe=linker_exe + linker_exe_flags)\n        ar = self.command_vars.archiver\n        if ar:\n            arflags = self.flag_vars.ar\n            self.set_commands(archiver=[ar] + arflags)\n        self.set_library_dirs(self.get_library_dirs())\n        self.set_libraries(self.get_libraries())\n\n    def get_libraries(self):\n        return ['fj90f', 'fj90i', 'fjsrcinfo']\n\n    def get_flags_debug(self):\n        return ['-g']", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.fujitsu/FujitsuFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/intel.py", "fn_id": "", "content": "class BaseIntelFCompiler(FCompiler):\n\n    def get_version(self, force=False, ok_status=[0]):\n        assert self._is_customised\n        version = CCompiler.get_version(self, force=force, ok_status=ok_status)\n        if version is None:\n            raise CompilerNotFound()\n        return version\n\n    def update_executables(self):\n        f = dummy_fortran_file()\n        self.executables['version_cmd'] = ['<F77>', '-FI', '-V', '-c', f + '.f', '-o', f + '.o']\n\n    def runtime_library_dir_option(self, dir):\n        assert ',' not in dir\n        return '-Wl,-rpath=%s' % dir\n\n    def customize(self, dist=None):\n        \"\"\"Customize Fortran compiler.\n\n        This method gets Fortran compiler specific information from\n        (i) class definition, (ii) environment, (iii) distutils config\n        files, and (iv) command line (later overrides earlier).\n\n        This method should be always called after constructing a\n        compiler instance. But not in __init__ because Distribution\n        instance is needed for (iii) and (iv).\n        \"\"\"\n        log.info('customize %s' % self.__class__.__name__)\n        self._is_customised = True\n        self.distutils_vars.use_distribution(dist)\n        self.command_vars.use_distribution(dist)\n        self.flag_vars.use_distribution(dist)\n        self.update_executables()\n        self.find_executables()\n        noopt = self.distutils_vars.get('noopt', False)\n        noarch = self.distutils_vars.get('noarch', noopt)\n        debug = self.distutils_vars.get('debug', False)\n        f77 = self.command_vars.compiler_f77\n        f90 = self.command_vars.compiler_f90\n        f77flags = []\n        f90flags = []\n        freeflags = []\n        fixflags = []\n        if f77:\n            f77 = _shell_utils.NativeParser.split(f77)\n            f77flags = self.flag_vars.f77\n        if f90:\n            f90 = _shell_utils.NativeParser.split(f90)\n            f90flags = self.flag_vars.f90\n            freeflags = self.flag_vars.free\n        fix = self.command_vars.compiler_fix\n        if fix:\n            fix = _shell_utils.NativeParser.split(fix)\n            fixflags = self.flag_vars.fix + f90flags\n        (oflags, aflags, dflags) = ([], [], [])\n\n        def get_flags(tag, flags):\n            flags.extend(getattr(self.flag_vars, tag))\n            this_get = getattr(self, 'get_flags_' + tag)\n            for (name, c, flagvar) in [('f77', f77, f77flags), ('f90', f90, f90flags), ('f90', fix, fixflags)]:\n                t = '%s_%s' % (tag, name)\n                if c and this_get is not getattr(self, 'get_flags_' + t):\n                    flagvar.extend(getattr(self.flag_vars, t))\n        if not noopt:\n            get_flags('opt', oflags)\n            if not noarch:\n                get_flags('arch', aflags)\n        if debug:\n            get_flags('debug', dflags)\n        fflags = self.flag_vars.flags + dflags + oflags + aflags\n        if f77:\n            self.set_commands(compiler_f77=f77 + f77flags + fflags)\n        if f90:\n            self.set_commands(compiler_f90=f90 + freeflags + f90flags + fflags)\n        if fix:\n            self.set_commands(compiler_fix=fix + fixflags + fflags)\n        linker_so = self.linker_so\n        if linker_so:\n            linker_so_flags = self.flag_vars.linker_so\n            if sys.platform.startswith('aix'):\n                python_lib = get_python_lib(standard_lib=1)\n                ld_so_aix = os.path.join(python_lib, 'config', 'ld_so_aix')\n                python_exp = os.path.join(python_lib, 'config', 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            if sys.platform.startswith('os400'):\n                from distutils.sysconfig import get_config_var\n                python_config = get_config_var('LIBPL')\n                ld_so_aix = os.path.join(python_config, 'ld_so_aix')\n                python_exp = os.path.join(python_config, 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            self.set_commands(linker_so=linker_so + linker_so_flags)\n        linker_exe = self.linker_exe\n        if linker_exe:\n            linker_exe_flags = self.flag_vars.linker_exe\n            self.set_commands(linker_exe=linker_exe + linker_exe_flags)\n        ar = self.command_vars.archiver\n        if ar:\n            arflags = self.flag_vars.ar\n            self.set_commands(archiver=[ar] + arflags)\n        self.set_library_dirs(self.get_library_dirs())\n        self.set_libraries(self.get_libraries())\n\n    def _command_property(key):\n\n        def fget(self):\n            assert self._is_customised\n            return self.executables[key]\n        return property(fget=fget)", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.intel/BaseIntelFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/intel.py", "fn_id": "", "content": "class IntelFCompiler(BaseIntelFCompiler):\n    compiler_type = 'intel'\n    compiler_aliases = ('ifort',)\n    description = 'Intel Fortran Compiler for 32-bit apps'\n    version_match = intel_version_match('32-bit|IA-32')\n    possible_executables = ['ifort', 'ifc']\n    executables = {'version_cmd': None, 'compiler_f77': [None, '-72', '-w90', '-w95'], 'compiler_f90': [None], 'compiler_fix': [None, '-FI'], 'linker_so': ['<F90>', '-shared'], 'archiver': ['ar', '-cr'], 'ranlib': ['ranlib']}\n    pic_flags = ['-fPIC']\n    module_dir_switch = '-module '\n    module_include_switch = '-I'\n\n    def get_flags_free(self):\n        return ['-FR']\n\n    def get_flags_opt(self):\n        v = self.get_version()\n        mpopt = 'openmp' if v and v < '15' else 'qopenmp'\n        return ['-fp-model', 'strict', '-O1', '-assume', 'minus0', '-{}'.format(mpopt)]\n\n    def runtime_library_dir_option(self, dir):\n        assert ',' not in dir\n        return '-Wl,-rpath=%s' % dir\n\n    def get_flags_linker_so(self):\n        opt = FCompiler.get_flags_linker_so(self)\n        v = self.get_version()\n        if v and v >= '8.0':\n            opt.append('-nofor_main')\n        if sys.platform == 'darwin':\n            try:\n                idx = opt.index('-shared')\n                opt.remove('-shared')\n            except ValueError:\n                idx = 0\n            opt[idx:idx] = ['-dynamiclib', '-Wl,-undefined,dynamic_lookup']\n        return opt\n\n    def update_executables(self):\n        f = dummy_fortran_file()\n        self.executables['version_cmd'] = ['<F77>', '-FI', '-V', '-c', f + '.f', '-o', f + '.o']\n\n    def get_flags_arch(self):\n        return []\n\n    def get_flags(self):\n        return ['-fPIC']", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.intel/IntelFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/intel.py", "fn_id": "", "content": "class IntelVisualFCompiler(BaseIntelFCompiler):\n    compiler_type = 'intelv'\n    description = 'Intel Visual Fortran Compiler for 32-bit apps'\n    version_match = intel_version_match('32-bit|IA-32')\n\n    def update_executables(self):\n        f = dummy_fortran_file()\n        self.executables['version_cmd'] = ['<F77>', '/FI', '/c', f + '.f', '/o', f + '.o']\n    ar_exe = 'lib.exe'\n    possible_executables = ['ifort', 'ifl']\n    executables = {'version_cmd': None, 'compiler_f77': [None], 'compiler_fix': [None], 'compiler_f90': [None], 'linker_so': [None], 'archiver': [ar_exe, '/verbose', '/OUT:'], 'ranlib': None}\n    compile_switch = '/c '\n    object_switch = '/Fo'\n    library_switch = '/OUT:'\n    module_dir_switch = '/module:'\n    module_include_switch = '/I'\n\n    def get_flags(self):\n        opt = ['/nologo', '/MD', '/nbs', '/names:lowercase', '/assume:underscore', '/fpp']\n        return opt\n\n    def get_flags_free(self):\n        return []\n\n    def get_flags_debug(self):\n        return ['/4Yb', '/d2']\n\n    def get_flags_opt(self):\n        return ['/O1', '/assume:minus0']\n\n    def get_flags_arch(self):\n        return ['/arch:IA32', '/QaxSSE3']\n\n    def runtime_library_dir_option(self, dir):\n        raise NotImplementedError", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.intel/IntelVisualFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/nag.py", "fn_id": "", "content": "class BaseNAGFCompiler(FCompiler):\n    version_pattern = 'NAG.* Release (?P<version>[^(\\\\s]*)'\n\n    def customize(self, dist=None):\n        \"\"\"Customize Fortran compiler.\n\n        This method gets Fortran compiler specific information from\n        (i) class definition, (ii) environment, (iii) distutils config\n        files, and (iv) command line (later overrides earlier).\n\n        This method should be always called after constructing a\n        compiler instance. But not in __init__ because Distribution\n        instance is needed for (iii) and (iv).\n        \"\"\"\n        log.info('customize %s' % self.__class__.__name__)\n        self._is_customised = True\n        self.distutils_vars.use_distribution(dist)\n        self.command_vars.use_distribution(dist)\n        self.flag_vars.use_distribution(dist)\n        self.update_executables()\n        self.find_executables()\n        noopt = self.distutils_vars.get('noopt', False)\n        noarch = self.distutils_vars.get('noarch', noopt)\n        debug = self.distutils_vars.get('debug', False)\n        f77 = self.command_vars.compiler_f77\n        f90 = self.command_vars.compiler_f90\n        f77flags = []\n        f90flags = []\n        freeflags = []\n        fixflags = []\n        if f77:\n            f77 = _shell_utils.NativeParser.split(f77)\n            f77flags = self.flag_vars.f77\n        if f90:\n            f90 = _shell_utils.NativeParser.split(f90)\n            f90flags = self.flag_vars.f90\n            freeflags = self.flag_vars.free\n        fix = self.command_vars.compiler_fix\n        if fix:\n            fix = _shell_utils.NativeParser.split(fix)\n            fixflags = self.flag_vars.fix + f90flags\n        (oflags, aflags, dflags) = ([], [], [])\n\n        def get_flags(tag, flags):\n            flags.extend(getattr(self.flag_vars, tag))\n            this_get = getattr(self, 'get_flags_' + tag)\n            for (name, c, flagvar) in [('f77', f77, f77flags), ('f90', f90, f90flags), ('f90', fix, fixflags)]:\n                t = '%s_%s' % (tag, name)\n                if c and this_get is not getattr(self, 'get_flags_' + t):\n                    flagvar.extend(getattr(self.flag_vars, t))\n        if not noopt:\n            get_flags('opt', oflags)\n            if not noarch:\n                get_flags('arch', aflags)\n        if debug:\n            get_flags('debug', dflags)\n        fflags = self.flag_vars.flags + dflags + oflags + aflags\n        if f77:\n            self.set_commands(compiler_f77=f77 + f77flags + fflags)\n        if f90:\n            self.set_commands(compiler_f90=f90 + freeflags + f90flags + fflags)\n        if fix:\n            self.set_commands(compiler_fix=fix + fixflags + fflags)\n        linker_so = self.linker_so\n        if linker_so:\n            linker_so_flags = self.flag_vars.linker_so\n            if sys.platform.startswith('aix'):\n                python_lib = get_python_lib(standard_lib=1)\n                ld_so_aix = os.path.join(python_lib, 'config', 'ld_so_aix')\n                python_exp = os.path.join(python_lib, 'config', 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            if sys.platform.startswith('os400'):\n                from distutils.sysconfig import get_config_var\n                python_config = get_config_var('LIBPL')\n                ld_so_aix = os.path.join(python_config, 'ld_so_aix')\n                python_exp = os.path.join(python_config, 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            self.set_commands(linker_so=linker_so + linker_so_flags)\n        linker_exe = self.linker_exe\n        if linker_exe:\n            linker_exe_flags = self.flag_vars.linker_exe\n            self.set_commands(linker_exe=linker_exe + linker_exe_flags)\n        ar = self.command_vars.archiver\n        if ar:\n            arflags = self.flag_vars.ar\n            self.set_commands(archiver=[ar] + arflags)\n        self.set_library_dirs(self.get_library_dirs())\n        self.set_libraries(self.get_libraries())\n\n    def version_match(self, version_string):\n        m = re.search(self.version_pattern, version_string)\n        if m:\n            return m.group('version')\n        else:\n            return None\n\n    def get_flags_opt(self):\n        return ['-O4']\n\n    def _command_property(key):\n\n        def fget(self):\n            assert self._is_customised\n            return self.executables[key]\n        return property(fget=fget)\n\n    def get_version(self, force=False, ok_status=[0]):\n        assert self._is_customised\n        version = CCompiler.get_version(self, force=force, ok_status=ok_status)\n        if version is None:\n            raise CompilerNotFound()\n        return version\n\n    def get_flags_linker_so(self):\n        return ['-Wl,-shared']\n\n    def get_flags_arch(self):\n        return []", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.nag/BaseNAGFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/none.py", "fn_id": "", "content": "class NoneFCompiler(FCompiler):\n    compiler_type = 'none'\n    description = 'Fake Fortran compiler'\n    executables = {'compiler_f77': None, 'compiler_f90': None, 'compiler_fix': None, 'linker_so': None, 'linker_exe': None, 'archiver': None, 'ranlib': None, 'version_cmd': None}\n\n    def find_executables(self):\n        pass\n\n    def get_version(self, force=False, ok_status=[0]):\n        assert self._is_customised\n        version = CCompiler.get_version(self, force=force, ok_status=ok_status)\n        if version is None:\n            raise CompilerNotFound()\n        return version\n\n    def _command_property(key):\n\n        def fget(self):\n            assert self._is_customised\n            return self.executables[key]\n        return property(fget=fget)\n\n    def customize(self, dist=None):\n        \"\"\"Customize Fortran compiler.\n\n        This method gets Fortran compiler specific information from\n        (i) class definition, (ii) environment, (iii) distutils config\n        files, and (iv) command line (later overrides earlier).\n\n        This method should be always called after constructing a\n        compiler instance. But not in __init__ because Distribution\n        instance is needed for (iii) and (iv).\n        \"\"\"\n        log.info('customize %s' % self.__class__.__name__)\n        self._is_customised = True\n        self.distutils_vars.use_distribution(dist)\n        self.command_vars.use_distribution(dist)\n        self.flag_vars.use_distribution(dist)\n        self.update_executables()\n        self.find_executables()\n        noopt = self.distutils_vars.get('noopt', False)\n        noarch = self.distutils_vars.get('noarch', noopt)\n        debug = self.distutils_vars.get('debug', False)\n        f77 = self.command_vars.compiler_f77\n        f90 = self.command_vars.compiler_f90\n        f77flags = []\n        f90flags = []\n        freeflags = []\n        fixflags = []\n        if f77:\n            f77 = _shell_utils.NativeParser.split(f77)\n            f77flags = self.flag_vars.f77\n        if f90:\n            f90 = _shell_utils.NativeParser.split(f90)\n            f90flags = self.flag_vars.f90\n            freeflags = self.flag_vars.free\n        fix = self.command_vars.compiler_fix\n        if fix:\n            fix = _shell_utils.NativeParser.split(fix)\n            fixflags = self.flag_vars.fix + f90flags\n        (oflags, aflags, dflags) = ([], [], [])\n\n        def get_flags(tag, flags):\n            flags.extend(getattr(self.flag_vars, tag))\n            this_get = getattr(self, 'get_flags_' + tag)\n            for (name, c, flagvar) in [('f77', f77, f77flags), ('f90', f90, f90flags), ('f90', fix, fixflags)]:\n                t = '%s_%s' % (tag, name)\n                if c and this_get is not getattr(self, 'get_flags_' + t):\n                    flagvar.extend(getattr(self.flag_vars, t))\n        if not noopt:\n            get_flags('opt', oflags)\n            if not noarch:\n                get_flags('arch', aflags)\n        if debug:\n            get_flags('debug', dflags)\n        fflags = self.flag_vars.flags + dflags + oflags + aflags\n        if f77:\n            self.set_commands(compiler_f77=f77 + f77flags + fflags)\n        if f90:\n            self.set_commands(compiler_f90=f90 + freeflags + f90flags + fflags)\n        if fix:\n            self.set_commands(compiler_fix=fix + fixflags + fflags)\n        linker_so = self.linker_so\n        if linker_so:\n            linker_so_flags = self.flag_vars.linker_so\n            if sys.platform.startswith('aix'):\n                python_lib = get_python_lib(standard_lib=1)\n                ld_so_aix = os.path.join(python_lib, 'config', 'ld_so_aix')\n                python_exp = os.path.join(python_lib, 'config', 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            if sys.platform.startswith('os400'):\n                from distutils.sysconfig import get_config_var\n                python_config = get_config_var('LIBPL')\n                ld_so_aix = os.path.join(python_config, 'ld_so_aix')\n                python_exp = os.path.join(python_config, 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            self.set_commands(linker_so=linker_so + linker_so_flags)\n        linker_exe = self.linker_exe\n        if linker_exe:\n            linker_exe_flags = self.flag_vars.linker_exe\n            self.set_commands(linker_exe=linker_exe + linker_exe_flags)\n        ar = self.command_vars.archiver\n        if ar:\n            arflags = self.flag_vars.ar\n            self.set_commands(archiver=[ar] + arflags)\n        self.set_library_dirs(self.get_library_dirs())\n        self.set_libraries(self.get_libraries())", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.none/NoneFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/pg.py", "fn_id": "", "content": "class PGroupFCompiler(FCompiler):\n    compiler_type = 'pg'\n    description = 'Portland Group Fortran Compiler'\n    version_pattern = '\\\\s*pg(f77|f90|hpf|fortran) (?P<version>[\\\\d.-]+).*'\n    if platform == 'darwin':\n        executables = {'version_cmd': ['<F77>', '-V'], 'compiler_f77': ['pgfortran', '-dynamiclib'], 'compiler_fix': ['pgfortran', '-Mfixed', '-dynamiclib'], 'compiler_f90': ['pgfortran', '-dynamiclib'], 'linker_so': ['libtool'], 'archiver': ['ar', '-cr'], 'ranlib': ['ranlib']}\n        pic_flags = ['']\n    else:\n        executables = {'version_cmd': ['<F77>', '-V'], 'compiler_f77': ['pgfortran'], 'compiler_fix': ['pgfortran', '-Mfixed'], 'compiler_f90': ['pgfortran'], 'linker_so': ['<F90>'], 'archiver': ['ar', '-cr'], 'ranlib': ['ranlib']}\n        pic_flags = ['-fpic']\n    module_dir_switch = '-module '\n    module_include_switch = '-I'\n\n    def get_flags(self):\n        opt = ['-Minform=inform', '-Mnosecond_underscore']\n        return self.pic_flags + opt\n\n    def get_flags_opt(self):\n        return ['-fast']\n\n    def get_flags_debug(self):\n        return ['-g']\n    if platform == 'darwin':\n\n        def get_flags_linker_so(self):\n            return ['-dynamic', '-undefined', 'dynamic_lookup']\n    else:\n\n        def get_flags_linker_so(self):\n            return ['-shared', '-fpic']\n\n    def runtime_library_dir_option(self, dir):\n        return '-R%s' % dir\n\n    def _command_property(key):\n\n        def fget(self):\n            assert self._is_customised\n            return self.executables[key]\n        return property(fget=fget)\n\n    def get_version(self, force=False, ok_status=[0]):\n        assert self._is_customised\n        version = CCompiler.get_version(self, force=force, ok_status=ok_status)\n        if version is None:\n            raise CompilerNotFound()\n        return version\n\n    def customize(self, dist=None):\n        \"\"\"Customize Fortran compiler.\n\n        This method gets Fortran compiler specific information from\n        (i) class definition, (ii) environment, (iii) distutils config\n        files, and (iv) command line (later overrides earlier).\n\n        This method should be always called after constructing a\n        compiler instance. But not in __init__ because Distribution\n        instance is needed for (iii) and (iv).\n        \"\"\"\n        log.info('customize %s' % self.__class__.__name__)\n        self._is_customised = True\n        self.distutils_vars.use_distribution(dist)\n        self.command_vars.use_distribution(dist)\n        self.flag_vars.use_distribution(dist)\n        self.update_executables()\n        self.find_executables()\n        noopt = self.distutils_vars.get('noopt', False)\n        noarch = self.distutils_vars.get('noarch', noopt)\n        debug = self.distutils_vars.get('debug', False)\n        f77 = self.command_vars.compiler_f77\n        f90 = self.command_vars.compiler_f90\n        f77flags = []\n        f90flags = []\n        freeflags = []\n        fixflags = []\n        if f77:\n            f77 = _shell_utils.NativeParser.split(f77)\n            f77flags = self.flag_vars.f77\n        if f90:\n            f90 = _shell_utils.NativeParser.split(f90)\n            f90flags = self.flag_vars.f90\n            freeflags = self.flag_vars.free\n        fix = self.command_vars.compiler_fix\n        if fix:\n            fix = _shell_utils.NativeParser.split(fix)\n            fixflags = self.flag_vars.fix + f90flags\n        (oflags, aflags, dflags) = ([], [], [])\n\n        def get_flags(tag, flags):\n            flags.extend(getattr(self.flag_vars, tag))\n            this_get = getattr(self, 'get_flags_' + tag)\n            for (name, c, flagvar) in [('f77', f77, f77flags), ('f90', f90, f90flags), ('f90', fix, fixflags)]:\n                t = '%s_%s' % (tag, name)\n                if c and this_get is not getattr(self, 'get_flags_' + t):\n                    flagvar.extend(getattr(self.flag_vars, t))\n        if not noopt:\n            get_flags('opt', oflags)\n            if not noarch:\n                get_flags('arch', aflags)\n        if debug:\n            get_flags('debug', dflags)\n        fflags = self.flag_vars.flags + dflags + oflags + aflags\n        if f77:\n            self.set_commands(compiler_f77=f77 + f77flags + fflags)\n        if f90:\n            self.set_commands(compiler_f90=f90 + freeflags + f90flags + fflags)\n        if fix:\n            self.set_commands(compiler_fix=fix + fixflags + fflags)\n        linker_so = self.linker_so\n        if linker_so:\n            linker_so_flags = self.flag_vars.linker_so\n            if sys.platform.startswith('aix'):\n                python_lib = get_python_lib(standard_lib=1)\n                ld_so_aix = os.path.join(python_lib, 'config', 'ld_so_aix')\n                python_exp = os.path.join(python_lib, 'config', 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            if sys.platform.startswith('os400'):\n                from distutils.sysconfig import get_config_var\n                python_config = get_config_var('LIBPL')\n                ld_so_aix = os.path.join(python_config, 'ld_so_aix')\n                python_exp = os.path.join(python_config, 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            self.set_commands(linker_so=linker_so + linker_so_flags)\n        linker_exe = self.linker_exe\n        if linker_exe:\n            linker_exe_flags = self.flag_vars.linker_exe\n            self.set_commands(linker_exe=linker_exe + linker_exe_flags)\n        ar = self.command_vars.archiver\n        if ar:\n            arflags = self.flag_vars.ar\n            self.set_commands(archiver=[ar] + arflags)\n        self.set_library_dirs(self.get_library_dirs())\n        self.set_libraries(self.get_libraries())", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.pg/PGroupFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/vast.py", "fn_id": "", "content": "class VastFCompiler(GnuFCompiler):\n    compiler_type = 'vast'\n    compiler_aliases = ()\n    description = 'Pacific-Sierra Research Fortran 90 Compiler'\n    version_pattern = '\\\\s*Pacific-Sierra Research vf90 (Personal|Professional)\\\\s+(?P<version>[^\\\\s]*)'\n    object_switch = ' && function _mvfile { mv -v `basename $1` $1 ; } && _mvfile '\n    executables = {'version_cmd': ['vf90', '-v'], 'compiler_f77': ['g77'], 'compiler_fix': ['f90', '-Wv,-ya'], 'compiler_f90': ['f90'], 'linker_so': ['<F90>'], 'archiver': ['ar', '-cr'], 'ranlib': ['ranlib']}\n    module_dir_switch = None\n    module_include_switch = None\n\n    def get_flags_arch(self):\n        vast_version = self.get_version()\n        gnu = GnuFCompiler()\n        gnu.customize(None)\n        self.version = gnu.get_version()\n        opt = GnuFCompiler.get_flags_arch(self)\n        self.version = vast_version\n        return opt\n\n    def get_version_cmd(self):\n        f90 = self.compiler_f90[0]\n        (d, b) = os.path.split(f90)\n        vf90 = os.path.join(d, 'v' + b)\n        return vf90\n\n    def runtime_library_dir_option(self, dir):\n        if sys.platform == 'win32' or sys.platform == 'cygwin':\n            raise NotImplementedError\n        assert ',' not in dir\n        if sys.platform == 'darwin':\n            return f'-Wl,-rpath,{dir}'\n        elif sys.platform.startswith(('aix', 'os400')):\n            return f'-Wl,-blibpath:{dir}'\n        else:\n            return f'-Wl,-rpath={dir}'\n\n    def _c_arch_flags(self):\n        \"\"\" Return detected arch flags from CFLAGS \"\"\"\n        import sysconfig\n        try:\n            cflags = sysconfig.get_config_vars()['CFLAGS']\n        except KeyError:\n            return []\n        arch_re = re.compile('-arch\\\\s+(\\\\w+)')\n        arch_flags = []\n        for arch in arch_re.findall(cflags):\n            arch_flags += ['-arch', arch]\n        return arch_flags\n\n    def get_flags_opt(self):\n        v = self.get_version()\n        if v and v <= '3.3.3':\n            opt = ['-O2']\n        else:\n            opt = ['-O3']\n        opt.append('-funroll-loops')\n        return opt\n\n    def find_executables(self):\n        pass", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.vast/VastFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/intelccompiler.py", "fn_id": "", "content": "class IntelEM64TCCompiler(UnixCCompiler):\n    \"\"\"\n    A modified Intel x86_64 compiler compatible with a 64bit GCC-built Python.\n    \"\"\"\n    compiler_type = 'intelem'\n    cc_exe = 'icc -m64'\n    cc_args = '-fPIC'\n\n    def __init__(self, verbose=0, dry_run=0, force=0):\n        UnixCCompiler.__init__(self, verbose, dry_run, force)\n        v = self.get_version()\n        mpopt = 'openmp' if v and v < '15' else 'qopenmp'\n        self.cc_exe = 'icc -std=c99 -m64 -fPIC -fp-model strict -O3 -fomit-frame-pointer -{}'.format(mpopt)\n        compiler = self.cc_exe\n        if platform.system() == 'Darwin':\n            shared_flag = '-Wl,-undefined,dynamic_lookup'\n        else:\n            shared_flag = '-shared'\n        self.set_executables(compiler=compiler, compiler_so=compiler, compiler_cxx=compiler, archiver='xiar' + ' cru', linker_exe=compiler + ' -shared-intel', linker_so=compiler + ' ' + shared_flag + ' -shared-intel')", "class_fn": true, "question_id": "numpy/numpy.distutils.intelccompiler/IntelEM64TCCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/misc_util.py", "fn_id": "", "content": "class InstallableLib:\n    \"\"\"\n    Container to hold information on an installable library.\n\n    Parameters\n    ----------\n    name : str\n        Name of the installed library.\n    build_info : dict\n        Dictionary holding build information.\n    target_dir : str\n        Absolute path specifying where to install the library.\n\n    See Also\n    --------\n    Configuration.add_installed_library\n\n    Notes\n    -----\n    The three parameters are stored as attributes with the same names.\n\n    \"\"\"\n\n    def __init__(self, name, build_info, target_dir):\n        self.name = name\n        self.build_info = build_info\n        self.target_dir = target_dir", "class_fn": true, "question_id": "numpy/numpy.distutils.misc_util/InstallableLib", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/npy_pkg_config.py", "fn_id": "", "content": "class PkgNotFound(OSError):\n    \"\"\"Exception raised when a package can not be located.\"\"\"\n\n    def __init__(self, msg):\n        self.msg = msg\n\n    def __str__(self):\n        return self.msg", "class_fn": true, "question_id": "numpy/numpy.distutils.npy_pkg_config/PkgNotFound", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/pathccompiler.py", "fn_id": "", "content": "class PathScaleCCompiler(UnixCCompiler):\n    \"\"\"\n    PathScale compiler compatible with an gcc built Python.\n    \"\"\"\n    compiler_type = 'pathcc'\n    cc_exe = 'pathcc'\n    cxx_exe = 'pathCC'\n\n    def __init__(self, verbose=0, dry_run=0, force=0):\n        UnixCCompiler.__init__(self, verbose, dry_run, force)\n        cc_compiler = self.cc_exe\n        cxx_compiler = self.cxx_exe\n        self.set_executables(compiler=cc_compiler, compiler_so=cc_compiler, compiler_cxx=cxx_compiler, linker_exe=cc_compiler, linker_so=cc_compiler + ' -shared')", "class_fn": true, "question_id": "numpy/numpy.distutils.pathccompiler/PathScaleCCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class accelerate_lapack_info(accelerate_info):\n\n    def calc_info(self):\n        libraries = os.environ.get('ACCELERATE')\n        if libraries:\n            libraries = [libraries]\n        else:\n            libraries = self.get_libs('libraries', self._lib_names)\n        libraries = [lib.strip().lower() for lib in libraries]\n        if sys.platform == 'darwin' and (not os.getenv('_PYTHON_HOST_PLATFORM', None)):\n            args = []\n            link_args = []\n            if get_platform()[-4:] == 'i386' or 'intel' in get_platform() or 'x86_64' in get_platform() or ('i386' in platform.platform()):\n                intel = 1\n            else:\n                intel = 0\n            if os.path.exists('/System/Library/Frameworks/Accelerate.framework/') and 'accelerate' in libraries:\n                if intel:\n                    args.extend(['-msse3'])\n                args.extend(['-I/System/Library/Frameworks/vecLib.framework/Headers'])\n                link_args.extend(['-Wl,-framework', '-Wl,Accelerate'])\n            elif os.path.exists('/System/Library/Frameworks/vecLib.framework/') and 'veclib' in libraries:\n                if intel:\n                    args.extend(['-msse3'])\n                args.extend(['-I/System/Library/Frameworks/vecLib.framework/Headers'])\n                link_args.extend(['-Wl,-framework', '-Wl,vecLib'])\n            if args:\n                macros = [('NO_ATLAS_INFO', 3), ('HAVE_CBLAS', None), ('ACCELERATE_NEW_LAPACK', None)]\n                if os.getenv('NPY_USE_BLAS_ILP64', None):\n                    print('Setting HAVE_BLAS_ILP64')\n                    macros += [('HAVE_BLAS_ILP64', None), ('ACCELERATE_LAPACK_ILP64', None)]\n                self.set_info(extra_compile_args=args, extra_link_args=link_args, define_macros=macros)\n        return\n\n    def _calc_info(self):\n        return super()._calc_info()", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/accelerate_lapack_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class armpl_info(system_info):\n    section = 'armpl'\n    dir_env_var = 'ARMPL_DIR'\n    _lib_armpl = ['armpl_lp64_mp']\n\n    def _check_libs(self, lib_dirs, libs, opt_libs, exts):\n        \"\"\"Find mandatory and optional libs in expected paths.\n\n        Missing optional libraries are silently forgotten.\n        \"\"\"\n        if not is_sequence(lib_dirs):\n            lib_dirs = [lib_dirs]\n        (found_dirs, found_libs) = self._find_libs(lib_dirs, libs, exts)\n        if len(found_libs) > 0 and len(found_libs) == len(libs):\n            (opt_found_dirs, opt_found_libs) = self._find_libs(lib_dirs, opt_libs, exts)\n            found_libs.extend(opt_found_libs)\n            for lib_dir in opt_found_dirs:\n                if lib_dir not in found_dirs:\n                    found_dirs.append(lib_dir)\n            info = {'libraries': found_libs, 'library_dirs': found_dirs}\n            return info\n        else:\n            return None\n\n    def calc_extra_info(self):\n        \"\"\" Updates the information in the current information with\n        respect to these flags:\n          extra_compile_args\n          extra_link_args\n        \"\"\"\n        info = {}\n        for key in ['extra_compile_args', 'extra_link_args']:\n            opt = self.cp.get(self.section, key)\n            opt = _shell_utils.NativeParser.split(opt)\n            if opt:\n                tmp = {key: opt}\n                dict_append(info, **tmp)\n        return info\n\n    def get_libs(self, key, default):\n        try:\n            libs = self.cp.get(self.section, key)\n        except NoOptionError:\n            if not default:\n                return []\n            if is_string(default):\n                return [default]\n            return default\n        return [b for b in [a.strip() for a in libs.split(',')] if b]\n\n    def calc_info(self):\n        lib_dirs = self.get_lib_dirs()\n        incl_dirs = self.get_include_dirs()\n        armpl_libs = self.get_libs('armpl_libs', self._lib_armpl)\n        info = self.check_libs2(lib_dirs, armpl_libs)\n        if info is None:\n            return\n        dict_append(info, define_macros=[('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)], include_dirs=incl_dirs)\n        self.set_info(**info)", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/armpl_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class atlas_3_10_info(atlas_info):\n    _lib_names = ['satlas']\n    _lib_atlas = _lib_names\n    _lib_lapack = _lib_names\n\n    def calc_info(self):\n        lib_dirs = self.get_lib_dirs()\n        info = {}\n        opt = self.get_option_single('atlas_libs', 'libraries')\n        atlas_libs = self.get_libs(opt, self._lib_names + self._lib_atlas)\n        lapack_libs = self.get_libs('lapack_libs', self._lib_lapack)\n        atlas = None\n        lapack = None\n        atlas_1 = None\n        for d in lib_dirs:\n            atlas = self.check_libs2(d, atlas_libs, [])\n            if atlas is not None:\n                lib_dirs2 = [d] + self.combine_paths(d, ['atlas*', 'ATLAS*'])\n                lapack = self.check_libs2(lib_dirs2, lapack_libs, [])\n                if lapack is not None:\n                    break\n            if atlas:\n                atlas_1 = atlas\n        log.info(self.__class__)\n        if atlas is None:\n            atlas = atlas_1\n        if atlas is None:\n            return\n        include_dirs = self.get_include_dirs()\n        h = self.combine_paths(lib_dirs + include_dirs, 'cblas.h') or [None]\n        h = h[0]\n        if h:\n            h = os.path.dirname(h)\n            dict_append(info, include_dirs=[h])\n        info['language'] = 'c'\n        if lapack is not None:\n            dict_append(info, **lapack)\n            dict_append(info, **atlas)\n        elif 'lapack_atlas' in atlas['libraries']:\n            dict_append(info, **atlas)\n            dict_append(info, define_macros=[('ATLAS_WITH_LAPACK_ATLAS', None)])\n            self.set_info(**info)\n            return\n        else:\n            dict_append(info, **atlas)\n            dict_append(info, define_macros=[('ATLAS_WITHOUT_LAPACK', None)])\n            message = textwrap.dedent('\\n                *********************************************************************\\n                    Could not find lapack library within the ATLAS installation.\\n                *********************************************************************\\n                ')\n            warnings.warn(message, stacklevel=2)\n            self.set_info(**info)\n            return\n        lapack_dir = lapack['library_dirs'][0]\n        lapack_name = lapack['libraries'][0]\n        lapack_lib = None\n        lib_prefixes = ['lib']\n        if sys.platform == 'win32':\n            lib_prefixes.append('')\n        for e in self.library_extensions():\n            for prefix in lib_prefixes:\n                fn = os.path.join(lapack_dir, prefix + lapack_name + e)\n                if os.path.exists(fn):\n                    lapack_lib = fn\n                    break\n            if lapack_lib:\n                break\n        if lapack_lib is not None:\n            sz = os.stat(lapack_lib)[6]\n            if sz <= 4000 * 1024:\n                message = textwrap.dedent('\\n                    *********************************************************************\\n                        Lapack library (from ATLAS) is probably incomplete:\\n                          size of %s is %sk (expected >4000k)\\n\\n                        Follow the instructions in the KNOWN PROBLEMS section of the file\\n                        numpy/INSTALL.txt.\\n                    *********************************************************************\\n                    ') % (lapack_lib, sz / 1024)\n                warnings.warn(message, stacklevel=2)\n            else:\n                info['language'] = 'f77'\n        (atlas_version, atlas_extra_info) = get_atlas_version(**atlas)\n        dict_append(info, **atlas_extra_info)\n        self.set_info(**info)\n\n    def get_paths(self, section, key):\n        pre_dirs = system_info.get_paths(self, section, key)\n        dirs = []\n        for d in pre_dirs:\n            dirs.extend(self.combine_paths(d, ['atlas*', 'ATLAS*', 'sse', '3dnow', 'sse2']) + [d])\n        return [d for d in dirs if os.path.isdir(d)]", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/atlas_3_10_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class atlas_threads_info(atlas_info):\n    dir_env_var = ['PTATLAS', 'ATLAS']\n    _lib_names = ['ptf77blas', 'ptcblas']\n\n    def calc_info(self):\n        lib_dirs = self.get_lib_dirs()\n        info = {}\n        opt = self.get_option_single('atlas_libs', 'libraries')\n        atlas_libs = self.get_libs(opt, self._lib_names + self._lib_atlas)\n        lapack_libs = self.get_libs('lapack_libs', self._lib_lapack)\n        atlas = None\n        lapack = None\n        atlas_1 = None\n        for d in lib_dirs:\n            atlas = self.check_libs2(d, atlas_libs, [])\n            if atlas is not None:\n                lib_dirs2 = [d] + self.combine_paths(d, ['atlas*', 'ATLAS*'])\n                lapack = self.check_libs2(lib_dirs2, lapack_libs, [])\n                if lapack is not None:\n                    break\n            if atlas:\n                atlas_1 = atlas\n        log.info(self.__class__)\n        if atlas is None:\n            atlas = atlas_1\n        if atlas is None:\n            return\n        include_dirs = self.get_include_dirs()\n        h = self.combine_paths(lib_dirs + include_dirs, 'cblas.h') or [None]\n        h = h[0]\n        if h:\n            h = os.path.dirname(h)\n            dict_append(info, include_dirs=[h])\n        info['language'] = 'c'\n        if lapack is not None:\n            dict_append(info, **lapack)\n            dict_append(info, **atlas)\n        elif 'lapack_atlas' in atlas['libraries']:\n            dict_append(info, **atlas)\n            dict_append(info, define_macros=[('ATLAS_WITH_LAPACK_ATLAS', None)])\n            self.set_info(**info)\n            return\n        else:\n            dict_append(info, **atlas)\n            dict_append(info, define_macros=[('ATLAS_WITHOUT_LAPACK', None)])\n            message = textwrap.dedent('\\n                *********************************************************************\\n                    Could not find lapack library within the ATLAS installation.\\n                *********************************************************************\\n                ')\n            warnings.warn(message, stacklevel=2)\n            self.set_info(**info)\n            return\n        lapack_dir = lapack['library_dirs'][0]\n        lapack_name = lapack['libraries'][0]\n        lapack_lib = None\n        lib_prefixes = ['lib']\n        if sys.platform == 'win32':\n            lib_prefixes.append('')\n        for e in self.library_extensions():\n            for prefix in lib_prefixes:\n                fn = os.path.join(lapack_dir, prefix + lapack_name + e)\n                if os.path.exists(fn):\n                    lapack_lib = fn\n                    break\n            if lapack_lib:\n                break\n        if lapack_lib is not None:\n            sz = os.stat(lapack_lib)[6]\n            if sz <= 4000 * 1024:\n                message = textwrap.dedent('\\n                    *********************************************************************\\n                        Lapack library (from ATLAS) is probably incomplete:\\n                          size of %s is %sk (expected >4000k)\\n\\n                        Follow the instructions in the KNOWN PROBLEMS section of the file\\n                        numpy/INSTALL.txt.\\n                    *********************************************************************\\n                    ') % (lapack_lib, sz / 1024)\n                warnings.warn(message, stacklevel=2)\n            else:\n                info['language'] = 'f77'\n        (atlas_version, atlas_extra_info) = get_atlas_version(**atlas)\n        dict_append(info, **atlas_extra_info)\n        self.set_info(**info)\n\n    def get_paths(self, section, key):\n        pre_dirs = system_info.get_paths(self, section, key)\n        dirs = []\n        for d in pre_dirs:\n            dirs.extend(self.combine_paths(d, ['atlas*', 'ATLAS*', 'sse', '3dnow', 'sse2']) + [d])\n        return [d for d in dirs if os.path.isdir(d)]", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/atlas_threads_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class blas_ilp64_plain_opt_info(blas_ilp64_opt_info):\n    symbol_prefix = ''\n    symbol_suffix = ''\n\n    def _calc_info(self, name):\n        info = get_info(name)\n        if self._check_info(info):\n            self.set_info(**info)\n            return True\n        return False", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/blas_ilp64_plain_opt_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class cblas_info(system_info):\n    section = 'cblas'\n    dir_env_var = 'CBLAS'\n    _lib_names = []\n    notfounderror = BlasNotFoundError\n\n    def calc_extra_info(self):\n        \"\"\" Updates the information in the current information with\n        respect to these flags:\n          extra_compile_args\n          extra_link_args\n        \"\"\"\n        info = {}\n        for key in ['extra_compile_args', 'extra_link_args']:\n            opt = self.cp.get(self.section, key)\n            opt = _shell_utils.NativeParser.split(opt)\n            if opt:\n                tmp = {key: opt}\n                dict_append(info, **tmp)\n        return info\n\n    def get_libs(self, key, default):\n        try:\n            libs = self.cp.get(self.section, key)\n        except NoOptionError:\n            if not default:\n                return []\n            if is_string(default):\n                return [default]\n            return default\n        return [b for b in [a.strip() for a in libs.split(',')] if b]\n\n    def _check_libs(self, lib_dirs, libs, opt_libs, exts):\n        \"\"\"Find mandatory and optional libs in expected paths.\n\n        Missing optional libraries are silently forgotten.\n        \"\"\"\n        if not is_sequence(lib_dirs):\n            lib_dirs = [lib_dirs]\n        (found_dirs, found_libs) = self._find_libs(lib_dirs, libs, exts)\n        if len(found_libs) > 0 and len(found_libs) == len(libs):\n            (opt_found_dirs, opt_found_libs) = self._find_libs(lib_dirs, opt_libs, exts)\n            found_libs.extend(opt_found_libs)\n            for lib_dir in opt_found_dirs:\n                if lib_dir not in found_dirs:\n                    found_dirs.append(lib_dir)\n            info = {'libraries': found_libs, 'library_dirs': found_dirs}\n            return info\n        else:\n            return None", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/cblas_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class djbfft_info(system_info):\n    section = 'djbfft'\n    dir_env_var = 'DJBFFT'\n    notfounderror = DJBFFTNotFoundError\n\n    def get_paths(self, section, key):\n        pre_dirs = system_info.get_paths(self, section, key)\n        dirs = []\n        for d in pre_dirs:\n            dirs.extend(self.combine_paths(d, ['djbfft']) + [d])\n        return [d for d in dirs if os.path.isdir(d)]\n\n    def get_libs(self, key, default):\n        try:\n            libs = self.cp.get(self.section, key)\n        except NoOptionError:\n            if not default:\n                return []\n            if is_string(default):\n                return [default]\n            return default\n        return [b for b in [a.strip() for a in libs.split(',')] if b]\n\n    def _check_libs(self, lib_dirs, libs, opt_libs, exts):\n        \"\"\"Find mandatory and optional libs in expected paths.\n\n        Missing optional libraries are silently forgotten.\n        \"\"\"\n        if not is_sequence(lib_dirs):\n            lib_dirs = [lib_dirs]\n        (found_dirs, found_libs) = self._find_libs(lib_dirs, libs, exts)\n        if len(found_libs) > 0 and len(found_libs) == len(libs):\n            (opt_found_dirs, opt_found_libs) = self._find_libs(lib_dirs, opt_libs, exts)\n            found_libs.extend(opt_found_libs)\n            for lib_dir in opt_found_dirs:\n                if lib_dir not in found_dirs:\n                    found_dirs.append(lib_dir)\n            info = {'libraries': found_libs, 'library_dirs': found_dirs}\n            return info\n        else:\n            return None\n\n    def calc_info(self):\n        lib_dirs = self.get_lib_dirs()\n        incl_dirs = self.get_include_dirs()\n        info = None\n        for d in lib_dirs:\n            p = self.combine_paths(d, ['djbfft.a'])\n            if p:\n                info = {'extra_objects': p}\n                break\n            p = self.combine_paths(d, ['libdjbfft.a', 'libdjbfft' + so_ext])\n            if p:\n                info = {'libraries': ['djbfft'], 'library_dirs': [d]}\n                break\n        if info is None:\n            return\n        for d in incl_dirs:\n            if len(self.combine_paths(d, ['fftc8.h', 'fftfreq.h'])) == 2:\n                dict_append(info, include_dirs=[d], define_macros=[('SCIPY_DJBFFT_H', None)])\n                self.set_info(**info)\n                return\n        return\n\n    def calc_extra_info(self):\n        \"\"\" Updates the information in the current information with\n        respect to these flags:\n          extra_compile_args\n          extra_link_args\n        \"\"\"\n        info = {}\n        for key in ['extra_compile_args', 'extra_link_args']:\n            opt = self.cp.get(self.section, key)\n            opt = _shell_utils.NativeParser.split(opt)\n            if opt:\n                tmp = {key: opt}\n                dict_append(info, **tmp)\n        return info", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/djbfft_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class fftw2_info(fftw_info):\n    section = 'fftw'\n    dir_env_var = 'FFTW'\n    notfounderror = FFTWNotFoundError\n    ver_info = [{'name': 'fftw2', 'libs': ['rfftw', 'fftw'], 'includes': ['fftw.h', 'rfftw.h'], 'macros': [('SCIPY_FFTW_H', None)]}]\n\n    def calc_info(self):\n        for i in self.ver_info:\n            if self.calc_ver_info(i):\n                break\n\n    def calc_ver_info(self, ver_param):\n        \"\"\"Returns True on successful version detection, else False\"\"\"\n        lib_dirs = self.get_lib_dirs()\n        incl_dirs = self.get_include_dirs()\n        opt = self.get_option_single(self.section + '_libs', 'libraries')\n        libs = self.get_libs(opt, ver_param['libs'])\n        info = self.check_libs(lib_dirs, libs)\n        if info is not None:\n            flag = 0\n            for d in incl_dirs:\n                if len(self.combine_paths(d, ver_param['includes'])) == len(ver_param['includes']):\n                    dict_append(info, include_dirs=[d])\n                    flag = 1\n                    break\n            if flag:\n                dict_append(info, define_macros=ver_param['macros'])\n            else:\n                info = None\n        if info is not None:\n            self.set_info(**info)\n            return True\n        else:\n            log.info('  %s not found' % ver_param['name'])\n            return False", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/fftw2_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class fftw_info(system_info):\n    section = 'fftw'\n    dir_env_var = 'FFTW'\n    notfounderror = FFTWNotFoundError\n    ver_info = [{'name': 'fftw3', 'libs': ['fftw3'], 'includes': ['fftw3.h'], 'macros': [('SCIPY_FFTW3_H', None)]}, {'name': 'fftw2', 'libs': ['rfftw', 'fftw'], 'includes': ['fftw.h', 'rfftw.h'], 'macros': [('SCIPY_FFTW_H', None)]}]\n\n    def calc_ver_info(self, ver_param):\n        \"\"\"Returns True on successful version detection, else False\"\"\"\n        lib_dirs = self.get_lib_dirs()\n        incl_dirs = self.get_include_dirs()\n        opt = self.get_option_single(self.section + '_libs', 'libraries')\n        libs = self.get_libs(opt, ver_param['libs'])\n        info = self.check_libs(lib_dirs, libs)\n        if info is not None:\n            flag = 0\n            for d in incl_dirs:\n                if len(self.combine_paths(d, ver_param['includes'])) == len(ver_param['includes']):\n                    dict_append(info, include_dirs=[d])\n                    flag = 1\n                    break\n            if flag:\n                dict_append(info, define_macros=ver_param['macros'])\n            else:\n                info = None\n        if info is not None:\n            self.set_info(**info)\n            return True\n        else:\n            log.info('  %s not found' % ver_param['name'])\n            return False\n\n    def calc_extra_info(self):\n        \"\"\" Updates the information in the current information with\n        respect to these flags:\n          extra_compile_args\n          extra_link_args\n        \"\"\"\n        info = {}\n        for key in ['extra_compile_args', 'extra_link_args']:\n            opt = self.cp.get(self.section, key)\n            opt = _shell_utils.NativeParser.split(opt)\n            if opt:\n                tmp = {key: opt}\n                dict_append(info, **tmp)\n        return info\n\n    def get_libs(self, key, default):\n        try:\n            libs = self.cp.get(self.section, key)\n        except NoOptionError:\n            if not default:\n                return []\n            if is_string(default):\n                return [default]\n            return default\n        return [b for b in [a.strip() for a in libs.split(',')] if b]\n\n    def calc_info(self):\n        for i in self.ver_info:\n            if self.calc_ver_info(i):\n                break\n\n    def _check_libs(self, lib_dirs, libs, opt_libs, exts):\n        \"\"\"Find mandatory and optional libs in expected paths.\n\n        Missing optional libraries are silently forgotten.\n        \"\"\"\n        if not is_sequence(lib_dirs):\n            lib_dirs = [lib_dirs]\n        (found_dirs, found_libs) = self._find_libs(lib_dirs, libs, exts)\n        if len(found_libs) > 0 and len(found_libs) == len(libs):\n            (opt_found_dirs, opt_found_libs) = self._find_libs(lib_dirs, opt_libs, exts)\n            found_libs.extend(opt_found_libs)\n            for lib_dir in opt_found_dirs:\n                if lib_dir not in found_dirs:\n                    found_dirs.append(lib_dir)\n            info = {'libraries': found_libs, 'library_dirs': found_dirs}\n            return info\n        else:\n            return None", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/fftw_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class gdk_2_info(_pkg_config_info):\n    section = 'gdk_2'\n    append_config_exe = 'gdk-2.0'\n    version_macro_name = 'GDK_VERSION'\n\n    def get_config_output(self, config_exe, option):\n        cmd = config_exe + ' ' + self.append_config_exe + ' ' + option\n        try:\n            o = subprocess.check_output(cmd)\n        except (OSError, subprocess.CalledProcessError):\n            pass\n        else:\n            o = filepath_from_subprocess_output(o)\n            return o\n\n    def get_config_exe(self):\n        if self.config_env_var in os.environ:\n            return os.environ[self.config_env_var]\n        return self.default_config_exe\n\n    def calc_info(self):\n        config_exe = find_executable(self.get_config_exe())\n        if not config_exe:\n            log.warn('File not found: %s. Cannot determine %s info.' % (config_exe, self.section))\n            return\n        info = {}\n        macros = []\n        libraries = []\n        library_dirs = []\n        include_dirs = []\n        extra_link_args = []\n        extra_compile_args = []\n        version = self.get_config_output(config_exe, self.version_flag)\n        if version:\n            macros.append((self.__class__.__name__.split('.')[-1].upper(), _c_string_literal(version)))\n            if self.version_macro_name:\n                macros.append((self.version_macro_name + '_%s' % version.replace('.', '_'), None))\n        if self.release_macro_name:\n            release = self.get_config_output(config_exe, '--release')\n            if release:\n                macros.append((self.release_macro_name + '_%s' % release.replace('.', '_'), None))\n        opts = self.get_config_output(config_exe, '--libs')\n        if opts:\n            for opt in opts.split():\n                if opt[:2] == '-l':\n                    libraries.append(opt[2:])\n                elif opt[:2] == '-L':\n                    library_dirs.append(opt[2:])\n                else:\n                    extra_link_args.append(opt)\n        opts = self.get_config_output(config_exe, self.cflags_flag)\n        if opts:\n            for opt in opts.split():\n                if opt[:2] == '-I':\n                    include_dirs.append(opt[2:])\n                elif opt[:2] == '-D':\n                    if '=' in opt:\n                        (n, v) = opt[2:].split('=')\n                        macros.append((n, v))\n                    else:\n                        macros.append((opt[2:], None))\n                else:\n                    extra_compile_args.append(opt)\n        if macros:\n            dict_append(info, define_macros=macros)\n        if libraries:\n            dict_append(info, libraries=libraries)\n        if library_dirs:\n            dict_append(info, library_dirs=library_dirs)\n        if include_dirs:\n            dict_append(info, include_dirs=include_dirs)\n        if extra_link_args:\n            dict_append(info, extra_link_args=extra_link_args)\n        if extra_compile_args:\n            dict_append(info, extra_compile_args=extra_compile_args)\n        if info:\n            self.set_info(**info)\n        return", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/gdk_2_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class gdk_pixbuf_xlib_2_info(_pkg_config_info):\n    section = 'gdk_pixbuf_xlib_2'\n    append_config_exe = 'gdk-pixbuf-xlib-2.0'\n    version_macro_name = 'GDK_PIXBUF_XLIB_VERSION'\n\n    def get_config_output(self, config_exe, option):\n        cmd = config_exe + ' ' + self.append_config_exe + ' ' + option\n        try:\n            o = subprocess.check_output(cmd)\n        except (OSError, subprocess.CalledProcessError):\n            pass\n        else:\n            o = filepath_from_subprocess_output(o)\n            return o\n\n    def calc_info(self):\n        config_exe = find_executable(self.get_config_exe())\n        if not config_exe:\n            log.warn('File not found: %s. Cannot determine %s info.' % (config_exe, self.section))\n            return\n        info = {}\n        macros = []\n        libraries = []\n        library_dirs = []\n        include_dirs = []\n        extra_link_args = []\n        extra_compile_args = []\n        version = self.get_config_output(config_exe, self.version_flag)\n        if version:\n            macros.append((self.__class__.__name__.split('.')[-1].upper(), _c_string_literal(version)))\n            if self.version_macro_name:\n                macros.append((self.version_macro_name + '_%s' % version.replace('.', '_'), None))\n        if self.release_macro_name:\n            release = self.get_config_output(config_exe, '--release')\n            if release:\n                macros.append((self.release_macro_name + '_%s' % release.replace('.', '_'), None))\n        opts = self.get_config_output(config_exe, '--libs')\n        if opts:\n            for opt in opts.split():\n                if opt[:2] == '-l':\n                    libraries.append(opt[2:])\n                elif opt[:2] == '-L':\n                    library_dirs.append(opt[2:])\n                else:\n                    extra_link_args.append(opt)\n        opts = self.get_config_output(config_exe, self.cflags_flag)\n        if opts:\n            for opt in opts.split():\n                if opt[:2] == '-I':\n                    include_dirs.append(opt[2:])\n                elif opt[:2] == '-D':\n                    if '=' in opt:\n                        (n, v) = opt[2:].split('=')\n                        macros.append((n, v))\n                    else:\n                        macros.append((opt[2:], None))\n                else:\n                    extra_compile_args.append(opt)\n        if macros:\n            dict_append(info, define_macros=macros)\n        if libraries:\n            dict_append(info, libraries=libraries)\n        if library_dirs:\n            dict_append(info, library_dirs=library_dirs)\n        if include_dirs:\n            dict_append(info, include_dirs=include_dirs)\n        if extra_link_args:\n            dict_append(info, extra_link_args=extra_link_args)\n        if extra_compile_args:\n            dict_append(info, extra_compile_args=extra_compile_args)\n        if info:\n            self.set_info(**info)\n        return\n\n    def get_config_exe(self):\n        if self.config_env_var in os.environ:\n            return os.environ[self.config_env_var]\n        return self.default_config_exe", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/gdk_pixbuf_xlib_2_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class gtkp_x11_2_info(_pkg_config_info):\n    section = 'gtkp_x11_2'\n    append_config_exe = 'gtk+-x11-2.0'\n    version_macro_name = 'GTK_X11_VERSION'\n\n    def calc_info(self):\n        config_exe = find_executable(self.get_config_exe())\n        if not config_exe:\n            log.warn('File not found: %s. Cannot determine %s info.' % (config_exe, self.section))\n            return\n        info = {}\n        macros = []\n        libraries = []\n        library_dirs = []\n        include_dirs = []\n        extra_link_args = []\n        extra_compile_args = []\n        version = self.get_config_output(config_exe, self.version_flag)\n        if version:\n            macros.append((self.__class__.__name__.split('.')[-1].upper(), _c_string_literal(version)))\n            if self.version_macro_name:\n                macros.append((self.version_macro_name + '_%s' % version.replace('.', '_'), None))\n        if self.release_macro_name:\n            release = self.get_config_output(config_exe, '--release')\n            if release:\n                macros.append((self.release_macro_name + '_%s' % release.replace('.', '_'), None))\n        opts = self.get_config_output(config_exe, '--libs')\n        if opts:\n            for opt in opts.split():\n                if opt[:2] == '-l':\n                    libraries.append(opt[2:])\n                elif opt[:2] == '-L':\n                    library_dirs.append(opt[2:])\n                else:\n                    extra_link_args.append(opt)\n        opts = self.get_config_output(config_exe, self.cflags_flag)\n        if opts:\n            for opt in opts.split():\n                if opt[:2] == '-I':\n                    include_dirs.append(opt[2:])\n                elif opt[:2] == '-D':\n                    if '=' in opt:\n                        (n, v) = opt[2:].split('=')\n                        macros.append((n, v))\n                    else:\n                        macros.append((opt[2:], None))\n                else:\n                    extra_compile_args.append(opt)\n        if macros:\n            dict_append(info, define_macros=macros)\n        if libraries:\n            dict_append(info, libraries=libraries)\n        if library_dirs:\n            dict_append(info, library_dirs=library_dirs)\n        if include_dirs:\n            dict_append(info, include_dirs=include_dirs)\n        if extra_link_args:\n            dict_append(info, extra_link_args=extra_link_args)\n        if extra_compile_args:\n            dict_append(info, extra_compile_args=extra_compile_args)\n        if info:\n            self.set_info(**info)\n        return\n\n    def get_config_output(self, config_exe, option):\n        cmd = config_exe + ' ' + self.append_config_exe + ' ' + option\n        try:\n            o = subprocess.check_output(cmd)\n        except (OSError, subprocess.CalledProcessError):\n            pass\n        else:\n            o = filepath_from_subprocess_output(o)\n            return o\n\n    def get_config_exe(self):\n        if self.config_env_var in os.environ:\n            return os.environ[self.config_env_var]\n        return self.default_config_exe", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/gtkp_x11_2_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class lapack_ilp64_plain_opt_info(lapack_ilp64_opt_info):\n    symbol_prefix = ''\n    symbol_suffix = ''\n\n    def _calc_info(self, name):\n        print('lapack_ilp64_opt_info._calc_info(name=%s)' % name)\n        info = get_info(name + '_lapack')\n        if self._check_info(info):\n            self.set_info(**info)\n            return True\n        else:\n            print('%s_lapack does not exist' % name)\n        return False", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/lapack_ilp64_plain_opt_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class numerix_info(system_info):\n    section = 'numerix'\n\n    def calc_extra_info(self):\n        \"\"\" Updates the information in the current information with\n        respect to these flags:\n          extra_compile_args\n          extra_link_args\n        \"\"\"\n        info = {}\n        for key in ['extra_compile_args', 'extra_link_args']:\n            opt = self.cp.get(self.section, key)\n            opt = _shell_utils.NativeParser.split(opt)\n            if opt:\n                tmp = {key: opt}\n                dict_append(info, **tmp)\n        return info\n\n    def calc_info(self):\n        which = (None, None)\n        if os.getenv('NUMERIX'):\n            which = (os.getenv('NUMERIX'), 'environment var')\n        if which[0] is None:\n            which = ('numpy', 'defaulted')\n            try:\n                import numpy\n                which = ('numpy', 'defaulted')\n            except ImportError as e:\n                msg1 = str(e)\n                try:\n                    import Numeric\n                    which = ('numeric', 'defaulted')\n                except ImportError as e:\n                    msg2 = str(e)\n                    try:\n                        import numarray\n                        which = ('numarray', 'defaulted')\n                    except ImportError as e:\n                        msg3 = str(e)\n                        log.info(msg1)\n                        log.info(msg2)\n                        log.info(msg3)\n        which = (which[0].strip().lower(), which[1])\n        if which[0] not in ['numeric', 'numarray', 'numpy']:\n            raise ValueError(\"numerix selector must be either 'Numeric' or 'numarray' or 'numpy' but the value obtained from the %s was '%s'.\" % (which[1], which[0]))\n        os.environ['NUMERIX'] = which[0]\n        self.set_info(**get_info(which[0]))\n\n    def get_libs(self, key, default):\n        try:\n            libs = self.cp.get(self.section, key)\n        except NoOptionError:\n            if not default:\n                return []\n            if is_string(default):\n                return [default]\n            return default\n        return [b for b in [a.strip() for a in libs.split(',')] if b]\n\n    def _check_libs(self, lib_dirs, libs, opt_libs, exts):\n        \"\"\"Find mandatory and optional libs in expected paths.\n\n        Missing optional libraries are silently forgotten.\n        \"\"\"\n        if not is_sequence(lib_dirs):\n            lib_dirs = [lib_dirs]\n        (found_dirs, found_libs) = self._find_libs(lib_dirs, libs, exts)\n        if len(found_libs) > 0 and len(found_libs) == len(libs):\n            (opt_found_dirs, opt_found_libs) = self._find_libs(lib_dirs, opt_libs, exts)\n            found_libs.extend(opt_found_libs)\n            for lib_dir in opt_found_dirs:\n                if lib_dir not in found_dirs:\n                    found_dirs.append(lib_dir)\n            info = {'libraries': found_libs, 'library_dirs': found_dirs}\n            return info\n        else:\n            return None", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/numerix_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class openblas_ilp64_info(openblas_info):\n    section = 'openblas_ilp64'\n    dir_env_var = 'OPENBLAS_ILP64'\n    _lib_names = ['openblas64']\n    _require_symbols = ['dgemm_', 'cblas_dgemm']\n    notfounderror = BlasILP64NotFoundError\n\n    def check_msvc_gfortran_libs(self, library_dirs, libraries):\n        library_paths = []\n        for library in libraries:\n            for library_dir in library_dirs:\n                fullpath = os.path.join(library_dir, library + '.a')\n                if os.path.isfile(fullpath):\n                    library_paths.append(fullpath)\n                    break\n            else:\n                return None\n        basename = self.__class__.__name__\n        tmpdir = os.path.join(os.getcwd(), 'build', basename)\n        if not os.path.isdir(tmpdir):\n            os.makedirs(tmpdir)\n        info = {'library_dirs': [tmpdir], 'libraries': [basename], 'language': 'f77'}\n        fake_lib_file = os.path.join(tmpdir, basename + '.fobjects')\n        fake_clib_file = os.path.join(tmpdir, basename + '.cobjects')\n        with open(fake_lib_file, 'w') as f:\n            f.write('\\n'.join(library_paths))\n        with open(fake_clib_file, 'w') as f:\n            pass\n        return info\n\n    def calc_info(self):\n        info = self._calc_info()\n        if info is not None:\n            self.set_info(**info)\n\n    def check_symbols(self, info):\n        res = False\n        c = customized_ccompiler()\n        tmpdir = tempfile.mkdtemp()\n        prototypes = '\\n'.join(('void %s%s%s();' % (self.symbol_prefix, symbol_name, self.symbol_suffix) for symbol_name in self._require_symbols))\n        calls = '\\n'.join(('%s%s%s();' % (self.symbol_prefix, symbol_name, self.symbol_suffix) for symbol_name in self._require_symbols))\n        s = textwrap.dedent('            %(prototypes)s\\n            int main(int argc, const char *argv[])\\n            {\\n                %(calls)s\\n                return 0;\\n            }') % dict(prototypes=prototypes, calls=calls)\n        src = os.path.join(tmpdir, 'source.c')\n        out = os.path.join(tmpdir, 'a.out')\n        try:\n            extra_args = info['extra_link_args']\n        except Exception:\n            extra_args = []\n        try:\n            with open(src, 'w') as f:\n                f.write(s)\n            obj = c.compile([src], output_dir=tmpdir)\n            try:\n                c.link_executable(obj, out, libraries=info['libraries'], library_dirs=info['library_dirs'], extra_postargs=extra_args)\n                res = True\n            except distutils.ccompiler.LinkError:\n                res = False\n        finally:\n            shutil.rmtree(tmpdir)\n        return res\n\n    def _calc_info(self):\n        info = super()._calc_info()\n        if info is not None:\n            info['define_macros'] += [('HAVE_BLAS_ILP64', None)]\n        return info", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/openblas_ilp64_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class sfftw_threads_info(fftw_info):\n    section = 'fftw'\n    dir_env_var = 'FFTW'\n    ver_info = [{'name': 'sfftw threads', 'libs': ['srfftw_threads', 'sfftw_threads'], 'includes': ['sfftw_threads.h', 'srfftw_threads.h'], 'macros': [('SCIPY_SFFTW_THREADS_H', None)]}]\n\n    def calc_info(self):\n        for i in self.ver_info:\n            if self.calc_ver_info(i):\n                break\n\n    def calc_ver_info(self, ver_param):\n        \"\"\"Returns True on successful version detection, else False\"\"\"\n        lib_dirs = self.get_lib_dirs()\n        incl_dirs = self.get_include_dirs()\n        opt = self.get_option_single(self.section + '_libs', 'libraries')\n        libs = self.get_libs(opt, ver_param['libs'])\n        info = self.check_libs(lib_dirs, libs)\n        if info is not None:\n            flag = 0\n            for d in incl_dirs:\n                if len(self.combine_paths(d, ver_param['includes'])) == len(ver_param['includes']):\n                    dict_append(info, include_dirs=[d])\n                    flag = 1\n                    break\n            if flag:\n                dict_append(info, define_macros=ver_param['macros'])\n            else:\n                info = None\n        if info is not None:\n            self.set_info(**info)\n            return True\n        else:\n            log.info('  %s not found' % ver_param['name'])\n            return False", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/sfftw_threads_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/system_info.py", "fn_id": "", "content": "class wx_info(_pkg_config_info):\n    section = 'wx'\n    config_env_var = 'WX_CONFIG'\n    default_config_exe = 'wx-config'\n    append_config_exe = ''\n    version_macro_name = 'WX_VERSION'\n    release_macro_name = 'WX_RELEASE'\n    version_flag = '--version'\n    cflags_flag = '--cxxflags'\n\n    def get_config_output(self, config_exe, option):\n        cmd = config_exe + ' ' + self.append_config_exe + ' ' + option\n        try:\n            o = subprocess.check_output(cmd)\n        except (OSError, subprocess.CalledProcessError):\n            pass\n        else:\n            o = filepath_from_subprocess_output(o)\n            return o\n\n    def get_config_exe(self):\n        if self.config_env_var in os.environ:\n            return os.environ[self.config_env_var]\n        return self.default_config_exe\n\n    def calc_info(self):\n        config_exe = find_executable(self.get_config_exe())\n        if not config_exe:\n            log.warn('File not found: %s. Cannot determine %s info.' % (config_exe, self.section))\n            return\n        info = {}\n        macros = []\n        libraries = []\n        library_dirs = []\n        include_dirs = []\n        extra_link_args = []\n        extra_compile_args = []\n        version = self.get_config_output(config_exe, self.version_flag)\n        if version:\n            macros.append((self.__class__.__name__.split('.')[-1].upper(), _c_string_literal(version)))\n            if self.version_macro_name:\n                macros.append((self.version_macro_name + '_%s' % version.replace('.', '_'), None))\n        if self.release_macro_name:\n            release = self.get_config_output(config_exe, '--release')\n            if release:\n                macros.append((self.release_macro_name + '_%s' % release.replace('.', '_'), None))\n        opts = self.get_config_output(config_exe, '--libs')\n        if opts:\n            for opt in opts.split():\n                if opt[:2] == '-l':\n                    libraries.append(opt[2:])\n                elif opt[:2] == '-L':\n                    library_dirs.append(opt[2:])\n                else:\n                    extra_link_args.append(opt)\n        opts = self.get_config_output(config_exe, self.cflags_flag)\n        if opts:\n            for opt in opts.split():\n                if opt[:2] == '-I':\n                    include_dirs.append(opt[2:])\n                elif opt[:2] == '-D':\n                    if '=' in opt:\n                        (n, v) = opt[2:].split('=')\n                        macros.append((n, v))\n                    else:\n                        macros.append((opt[2:], None))\n                else:\n                    extra_compile_args.append(opt)\n        if macros:\n            dict_append(info, define_macros=macros)\n        if libraries:\n            dict_append(info, libraries=libraries)\n        if library_dirs:\n            dict_append(info, library_dirs=library_dirs)\n        if include_dirs:\n            dict_append(info, include_dirs=include_dirs)\n        if extra_link_args:\n            dict_append(info, extra_link_args=extra_link_args)\n        if extra_compile_args:\n            dict_append(info, extra_compile_args=extra_compile_args)\n        if info:\n            self.set_info(**info)\n        return", "class_fn": true, "question_id": "numpy/numpy.distutils.system_info/wx_info", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/tests/test_ccompiler_opt.py", "fn_id": "", "content": "class FakeCCompilerOpt(CCompilerOpt):\n    fake_info = ''\n\n    def dist_info(self):\n        return FakeCCompilerOpt.fake_info\n\n    def cpu_dispatch_names(self):\n        \"\"\"\n        return a list of final CPU dispatch feature names\n        \"\"\"\n        return self.parse_dispatch_names\n\n    def generate_dispatch_header(self, header_path):\n        \"\"\"\n        Generate the dispatch header which contains the #definitions and headers\n        for platform-specific instruction-sets for the enabled CPU baseline and\n        dispatch-able features.\n\n        Its highly recommended to take a look at the generated header\n        also the generated source files via `try_dispatch()`\n        in order to get the full picture.\n        \"\"\"\n        self.dist_log('generate CPU dispatch header: (%s)' % header_path)\n        baseline_names = self.cpu_baseline_names()\n        dispatch_names = self.cpu_dispatch_names()\n        baseline_len = len(baseline_names)\n        dispatch_len = len(dispatch_names)\n        header_dir = os.path.dirname(header_path)\n        if not os.path.exists(header_dir):\n            self.dist_log(f'dispatch header dir {header_dir} does not exist, creating it', stderr=True)\n            os.makedirs(header_dir)\n        with open(header_path, 'w') as f:\n            baseline_calls = ' \\\\\\n'.join(['\\t%sWITH_CPU_EXPAND_(MACRO_TO_CALL(%s, __VA_ARGS__))' % (self.conf_c_prefix, f) for f in baseline_names])\n            dispatch_calls = ' \\\\\\n'.join(['\\t%sWITH_CPU_EXPAND_(MACRO_TO_CALL(%s, __VA_ARGS__))' % (self.conf_c_prefix, f) for f in dispatch_names])\n            f.write(textwrap.dedent('                /*\\n                 * AUTOGENERATED DON\\'T EDIT\\n                 * Please make changes to the code generator (distutils/ccompiler_opt.py)\\n                */\\n                #define {pfx}WITH_CPU_BASELINE  \"{baseline_str}\"\\n                #define {pfx}WITH_CPU_DISPATCH  \"{dispatch_str}\"\\n                #define {pfx}WITH_CPU_BASELINE_N {baseline_len}\\n                #define {pfx}WITH_CPU_DISPATCH_N {dispatch_len}\\n                #define {pfx}WITH_CPU_EXPAND_(X) X\\n                #define {pfx}WITH_CPU_BASELINE_CALL(MACRO_TO_CALL, ...) \\\\\\n                {baseline_calls}\\n                #define {pfx}WITH_CPU_DISPATCH_CALL(MACRO_TO_CALL, ...) \\\\\\n                {dispatch_calls}\\n            ').format(pfx=self.conf_c_prefix, baseline_str=' '.join(baseline_names), dispatch_str=' '.join(dispatch_names), baseline_len=baseline_len, dispatch_len=dispatch_len, baseline_calls=baseline_calls, dispatch_calls=dispatch_calls))\n            baseline_pre = ''\n            for name in baseline_names:\n                baseline_pre += self.feature_c_preprocessor(name, tabs=1) + '\\n'\n            dispatch_pre = ''\n            for name in dispatch_names:\n                dispatch_pre += textwrap.dedent('                #ifdef {pfx}CPU_TARGET_{name}\\n                {pre}\\n                #endif /*{pfx}CPU_TARGET_{name}*/\\n                ').format(pfx=self.conf_c_prefix_, name=name, pre=self.feature_c_preprocessor(name, tabs=1))\n            f.write(textwrap.dedent('            /******* baseline features *******/\\n            {baseline_pre}\\n            /******* dispatch features *******/\\n            {dispatch_pre}\\n            ').format(pfx=self.conf_c_prefix_, baseline_pre=baseline_pre, dispatch_pre=dispatch_pre))\n\n    def __repr__(self):\n        return textwrap.dedent('            <<<<\\n            march    : {}\\n            compiler : {}\\n            ----------------\\n            {}\\n            >>>>\\n        ').format(self.cc_march, self.cc_name, self.report())\n\n    @staticmethod\n    def dist_log(*args, stderr=False):\n        pass\n\n    def dist_compile(self, sources, flags, **kwargs):\n        assert isinstance(sources, list)\n        assert isinstance(flags, list)\n        if self.fake_trap_files:\n            for src in sources:\n                if re.match(self.fake_trap_files, src):\n                    self.dist_error('source is trapped by a fake interface')\n        if self.fake_trap_flags:\n            for f in flags:\n                if re.match(self.fake_trap_flags, f):\n                    self.dist_error('flag is trapped by a fake interface')\n        return zip(sources, [' '.join(flags)] * len(sources))\n\n    def __init__(self, trap_files='', trap_flags='', *args, **kwargs):\n        self.fake_trap_files = trap_files\n        self.fake_trap_flags = trap_flags\n        CCompilerOpt.__init__(self, None, **kwargs)\n\n    def try_dispatch(self, sources, src_dir=None, ccompiler=None, **kwargs):\n        \"\"\"\n        Compile one or more dispatch-able sources and generates object files,\n        also generates abstract C config headers and macros that\n        used later for the final runtime dispatching process.\n\n        The mechanism behind it is to takes each source file that specified\n        in 'sources' and branching it into several files depend on\n        special configuration statements that must be declared in the\n        top of each source which contains targeted CPU features,\n        then it compiles every branched source with the proper compiler flags.\n\n        Parameters\n        ----------\n        sources : list\n            Must be a list of dispatch-able sources file paths,\n            and configuration statements must be declared inside\n            each file.\n\n        src_dir : str\n            Path of parent directory for the generated headers and wrapped sources.\n            If None(default) the files will generated in-place.\n\n        ccompiler : CCompiler\n            Distutils `CCompiler` instance to be used for compilation.\n            If None (default), the provided instance during the initialization\n            will be used instead.\n\n        **kwargs : any\n            Arguments to pass on to the `CCompiler.compile()`\n\n        Returns\n        -------\n        list : generated object files\n\n        Raises\n        ------\n        CompileError\n            Raises by `CCompiler.compile()` on compiling failure.\n        DistutilsError\n            Some errors during checking the sanity of configuration statements.\n\n        See Also\n        --------\n        parse_targets :\n            Parsing the configuration statements of dispatch-able sources.\n        \"\"\"\n        to_compile = {}\n        baseline_flags = self.cpu_baseline_flags()\n        include_dirs = kwargs.setdefault('include_dirs', [])\n        for src in sources:\n            output_dir = os.path.dirname(src)\n            if src_dir:\n                if not output_dir.startswith(src_dir):\n                    output_dir = os.path.join(src_dir, output_dir)\n                if output_dir not in include_dirs:\n                    include_dirs.append(output_dir)\n            (has_baseline, targets, extra_flags) = self.parse_targets(src)\n            nochange = self._generate_config(output_dir, src, targets, has_baseline)\n            for tar in targets:\n                tar_src = self._wrap_target(output_dir, src, tar, nochange=nochange)\n                flags = tuple(extra_flags + self.feature_flags(tar))\n                to_compile.setdefault(flags, []).append(tar_src)\n            if has_baseline:\n                flags = tuple(extra_flags + baseline_flags)\n                to_compile.setdefault(flags, []).append(src)\n            self.sources_status[src] = (has_baseline, targets)\n        objects = []\n        for (flags, srcs) in to_compile.items():\n            objects += self.dist_compile(srcs, list(flags), ccompiler=ccompiler, **kwargs)\n        return objects", "class_fn": true, "question_id": "numpy/numpy.distutils.tests.test_ccompiler_opt/FakeCCompilerOpt", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/tests/test_fcompiler_gnu.py", "fn_id": "", "content": "class TestG77Versions:\n\n    def test_g77_version(self):\n        fc = numpy.distutils.fcompiler.new_fcompiler(compiler='gnu')\n        for (vs, version) in g77_version_strings:\n            v = fc.version_match(vs)\n            assert_(v == version, (vs, v))\n\n    def test_not_g77(self):\n        fc = numpy.distutils.fcompiler.new_fcompiler(compiler='gnu')\n        for (vs, _) in gfortran_version_strings:\n            v = fc.version_match(vs)\n            assert_(v is None, (vs, v))", "class_fn": true, "question_id": "numpy/numpy.distutils.tests.test_fcompiler_gnu/TestG77Versions", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/tests/test_fcompiler_intel.py", "fn_id": "", "content": "class TestIntelFCompilerVersions:\n\n    def test_32bit_version(self):\n        fc = numpy.distutils.fcompiler.new_fcompiler(compiler='intel')\n        for (vs, version) in intel_32bit_version_strings:\n            v = fc.version_match(vs)\n            assert_(v == version)", "class_fn": true, "question_id": "numpy/numpy.distutils.tests.test_fcompiler_intel/TestIntelFCompilerVersions", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/tests/test_misc_util.py", "fn_id": "", "content": "class TestSharedExtension:\n\n    def test_get_shared_lib_extension(self):\n        import sys\n        ext = get_shared_lib_extension(is_python_ext=False)\n        if sys.platform.startswith('linux'):\n            assert_equal(ext, '.so')\n        elif sys.platform.startswith('gnukfreebsd'):\n            assert_equal(ext, '.so')\n        elif sys.platform.startswith('darwin'):\n            assert_equal(ext, '.dylib')\n        elif sys.platform.startswith('win'):\n            assert_equal(ext, '.dll')\n        assert_(get_shared_lib_extension(is_python_ext=True))", "class_fn": true, "question_id": "numpy/numpy.distutils.tests.test_misc_util/TestSharedExtension", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/exceptions.py", "fn_id": "", "content": "class TooHardError(RuntimeError):\n    \"\"\"max_work was exceeded.\n\n    This is raised whenever the maximum number of candidate solutions\n    to consider specified by the ``max_work`` parameter is exceeded.\n    Assigning a finite number to max_work may have caused the operation\n    to fail.\n\n    \"\"\"\n    pass", "class_fn": true, "question_id": "numpy/numpy.exceptions/TooHardError", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/f2py/f2py2e.py", "fn_id": "", "content": "class CombineIncludePaths(argparse.Action):\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        include_paths_set = set(getattr(namespace, 'include_paths', []) or [])\n        if option_string == '--include_paths':\n            outmess('Use --include-paths or -I instead of --include_paths which will be removed')\n        if option_string == '--include-paths' or option_string == '--include_paths':\n            include_paths_set.update(values.split(':'))\n        else:\n            include_paths_set.add(values)\n        setattr(namespace, 'include_paths', list(include_paths_set))", "class_fn": true, "question_id": "numpy/numpy.f2py.f2py2e/CombineIncludePaths", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/fft/tests/test_helper.py", "fn_id": "", "content": "class TestFFTFreq:\n\n    def test_definition(self):\n        x = [0, 1, 2, 3, 4, -4, -3, -2, -1]\n        assert_array_almost_equal(9 * fft.fftfreq(9), x)\n        assert_array_almost_equal(9 * pi * fft.fftfreq(9, pi), x)\n        x = [0, 1, 2, 3, 4, -5, -4, -3, -2, -1]\n        assert_array_almost_equal(10 * fft.fftfreq(10), x)\n        assert_array_almost_equal(10 * pi * fft.fftfreq(10, pi), x)", "class_fn": true, "question_id": "numpy/numpy.fft.tests.test_helper/TestFFTFreq", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/index_tricks.py", "fn_id": "", "content": "class CClass(AxisConcatenator):\n    \"\"\"\n    Translates slice objects to concatenation along the second axis.\n\n    This is short-hand for ``np.r_['-1,2,0', index expression]``, which is\n    useful because of its common occurrence. In particular, arrays will be\n    stacked along their last axis after being upgraded to at least 2-D with\n    1's post-pended to the shape (column vectors made out of 1-D arrays).\n\n    See Also\n    --------\n    column_stack : Stack 1-D arrays as columns into a 2-D array.\n    r_ : For more detailed documentation.\n\n    Examples\n    --------\n    >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\n    array([[1, 4],\n           [2, 5],\n           [3, 6]])\n    >>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\n    array([[1, 2, 3, ..., 4, 5, 6]])\n\n    \"\"\"\n\n    def __len__(self):\n        return 0\n\n    def __getitem__(self, key):\n        if isinstance(key, str):\n            frame = sys._getframe().f_back\n            mymat = matrixlib.bmat(key, frame.f_globals, frame.f_locals)\n            return mymat\n        if not isinstance(key, tuple):\n            key = (key,)\n        trans1d = self.trans1d\n        ndmin = self.ndmin\n        matrix = self.matrix\n        axis = self.axis\n        objs = []\n        result_type_objs = []\n        for (k, item) in enumerate(key):\n            scalar = False\n            if isinstance(item, slice):\n                step = item.step\n                start = item.start\n                stop = item.stop\n                if start is None:\n                    start = 0\n                if step is None:\n                    step = 1\n                if isinstance(step, (_nx.complexfloating, complex)):\n                    size = int(abs(step))\n                    newobj = linspace(start, stop, num=size)\n                else:\n                    newobj = _nx.arange(start, stop, step)\n                if ndmin > 1:\n                    newobj = array(newobj, copy=False, ndmin=ndmin)\n                    if trans1d != -1:\n                        newobj = newobj.swapaxes(-1, trans1d)\n            elif isinstance(item, str):\n                if k != 0:\n                    raise ValueError('special directives must be the first entry.')\n                if item in ('r', 'c'):\n                    matrix = True\n                    col = item == 'c'\n                    continue\n                if ',' in item:\n                    vec = item.split(',')\n                    try:\n                        (axis, ndmin) = [int(x) for x in vec[:2]]\n                        if len(vec) == 3:\n                            trans1d = int(vec[2])\n                        continue\n                    except Exception as e:\n                        raise ValueError('unknown special directive {!r}'.format(item)) from e\n                try:\n                    axis = int(item)\n                    continue\n                except (ValueError, TypeError) as e:\n                    raise ValueError('unknown special directive') from e\n            elif type(item) in ScalarType:\n                scalar = True\n                newobj = item\n            else:\n                item_ndim = np.ndim(item)\n                newobj = array(item, copy=False, subok=True, ndmin=ndmin)\n                if trans1d != -1 and item_ndim < ndmin:\n                    k2 = ndmin - item_ndim\n                    k1 = trans1d\n                    if k1 < 0:\n                        k1 += k2 + 1\n                    defaxes = list(range(ndmin))\n                    axes = defaxes[:k1] + defaxes[k2:] + defaxes[k1:k2]\n                    newobj = newobj.transpose(axes)\n            objs.append(newobj)\n            if scalar:\n                result_type_objs.append(item)\n            else:\n                result_type_objs.append(newobj.dtype)\n        if len(result_type_objs) != 0:\n            final_dtype = _nx.result_type(*result_type_objs)\n            objs = [array(obj, copy=False, subok=True, ndmin=ndmin, dtype=final_dtype) for obj in objs]\n        res = self.concatenate(tuple(objs), axis=axis)\n        if matrix:\n            oldndim = res.ndim\n            res = self.makemat(res)\n            if oldndim == 1 and col:\n                res = res.T\n        return res\n\n    def __init__(self):\n        AxisConcatenator.__init__(self, -1, ndmin=2, trans1d=0)", "class_fn": true, "question_id": "numpy/numpy.lib.index_tricks/CClass", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/index_tricks.py", "fn_id": "", "content": "class OGridClass(nd_grid):\n    \"\"\"\n    An instance which returns an open multi-dimensional \"meshgrid\".\n\n    An instance which returns an open (i.e. not fleshed out) mesh-grid\n    when indexed, so that only one dimension of each returned array is\n    greater than 1.  The dimension and number of the output arrays are\n    equal to the number of indexing dimensions.  If the step length is\n    not a complex number, then the stop is not inclusive.\n\n    However, if the step length is a **complex number** (e.g. 5j), then\n    the integer part of its magnitude is interpreted as specifying the\n    number of points to create between the start and stop values, where\n    the stop value **is inclusive**.\n\n    Returns\n    -------\n    mesh-grid\n        `ndarrays` with only one dimension not equal to 1\n\n    See Also\n    --------\n    mgrid : like `ogrid` but returns dense (or fleshed out) mesh grids\n    meshgrid: return coordinate matrices from coordinate vectors\n    r_ : array concatenator\n    :ref:`how-to-partition`\n\n    Examples\n    --------\n    >>> from numpy import ogrid\n    >>> ogrid[-1:1:5j]\n    array([-1. , -0.5,  0. ,  0.5,  1. ])\n    >>> ogrid[0:5,0:5]\n    [array([[0],\n            [1],\n            [2],\n            [3],\n            [4]]), array([[0, 1, 2, 3, 4]])]\n\n    \"\"\"\n\n    def __getitem__(self, key):\n        try:\n            size = []\n            num_list = [0]\n            for k in range(len(key)):\n                step = key[k].step\n                start = key[k].start\n                stop = key[k].stop\n                if start is None:\n                    start = 0\n                if step is None:\n                    step = 1\n                if isinstance(step, (_nx.complexfloating, complex)):\n                    step = abs(step)\n                    size.append(int(step))\n                else:\n                    size.append(int(math.ceil((stop - start) / (step * 1.0))))\n                num_list += [start, stop, step]\n            typ = _nx.result_type(*num_list)\n            if self.sparse:\n                nn = [_nx.arange(_x, dtype=_t) for (_x, _t) in zip(size, (typ,) * len(size))]\n            else:\n                nn = _nx.indices(size, typ)\n            for (k, kk) in enumerate(key):\n                step = kk.step\n                start = kk.start\n                if start is None:\n                    start = 0\n                if step is None:\n                    step = 1\n                if isinstance(step, (_nx.complexfloating, complex)):\n                    step = int(abs(step))\n                    if step != 1:\n                        step = (kk.stop - start) / float(step - 1)\n                nn[k] = nn[k] * step + start\n            if self.sparse:\n                slobj = [_nx.newaxis] * len(size)\n                for k in range(len(size)):\n                    slobj[k] = slice(None, None)\n                    nn[k] = nn[k][tuple(slobj)]\n                    slobj[k] = _nx.newaxis\n            return nn\n        except (IndexError, TypeError):\n            step = key.step\n            stop = key.stop\n            start = key.start\n            if start is None:\n                start = 0\n            if isinstance(step, (_nx.complexfloating, complex)):\n                step_float = abs(step)\n                step = length = int(step_float)\n                if step != 1:\n                    step = (key.stop - start) / float(step - 1)\n                typ = _nx.result_type(start, stop, step_float)\n                return _nx.arange(0, length, 1, dtype=typ) * step + start\n            else:\n                return _nx.arange(start, stop, step)\n\n    def __init__(self):\n        super().__init__(sparse=True)", "class_fn": true, "question_id": "numpy/numpy.lib.index_tricks/OGridClass", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/tests/test_type_check.py", "fn_id": "", "content": "class TestArrayConversion:\n\n    def test_asfarray(self):\n        a = asfarray(np.array([1, 2, 3]))\n        assert_equal(a.__class__, np.ndarray)\n        assert_(np.issubdtype(a.dtype, np.floating))\n        assert_raises(TypeError, asfarray, np.array([1, 2, 3]), dtype=np.array(1.0))", "class_fn": true, "question_id": "numpy/numpy.lib.tests.test_type_check/TestArrayConversion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/tests/test_type_check.py", "fn_id": "", "content": "class TestIscomplex:\n\n    def test_fail(self):\n        z = np.array([-1, 0, 1])\n        res = iscomplex(z)\n        assert_(not np.any(res, axis=0))\n\n    def test_pass(self):\n        z = np.array([-1j, 1, 0])\n        res = iscomplex(z)\n        assert_array_equal(res, [1, 0, 0])", "class_fn": true, "question_id": "numpy/numpy.lib.tests.test_type_check/TestIscomplex", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/tests/test_type_check.py", "fn_id": "", "content": "class TestIsinf:\n\n    def test_goodvalues(self):\n        z = np.array((-1.0, 0.0, 1.0))\n        res = np.isinf(z) == 0\n        assert_all(np.all(res, axis=0))\n\n    def test_posinf(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            assert_all(np.isinf(np.array((1.0,)) / 0.0) == 1)\n\n    def test_posinf_scalar(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            assert_all(np.isinf(np.array(1.0) / 0.0) == 1)\n\n    def test_neginf(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            assert_all(np.isinf(np.array((-1.0,)) / 0.0) == 1)\n\n    def test_neginf_scalar(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            assert_all(np.isinf(np.array(-1.0) / 0.0) == 1)\n\n    def test_ind(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            assert_all(np.isinf(np.array((0.0,)) / 0.0) == 0)", "class_fn": true, "question_id": "numpy/numpy.lib.tests.test_type_check/TestIsinf", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/tests/test_type_check.py", "fn_id": "", "content": "class TestIsposinf:\n\n    def test_generic(self):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            vals = isposinf(np.array((-1.0, 0, 1)) / 0.0)\n        assert_(vals[0] == 0)\n        assert_(vals[1] == 0)\n        assert_(vals[2] == 1)", "class_fn": true, "question_id": "numpy/numpy.lib.tests.test_type_check/TestIsposinf", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/tests/test_type_check.py", "fn_id": "", "content": "class TestMintypecode:\n\n    def test_default_1(self):\n        for itype in '1bcsuwil':\n            assert_equal(mintypecode(itype), 'd')\n        assert_equal(mintypecode('f'), 'f')\n        assert_equal(mintypecode('d'), 'd')\n        assert_equal(mintypecode('F'), 'F')\n        assert_equal(mintypecode('D'), 'D')\n\n    def test_default_2(self):\n        for itype in '1bcsuwil':\n            assert_equal(mintypecode(itype + 'f'), 'f')\n            assert_equal(mintypecode(itype + 'd'), 'd')\n            assert_equal(mintypecode(itype + 'F'), 'F')\n            assert_equal(mintypecode(itype + 'D'), 'D')\n        assert_equal(mintypecode('ff'), 'f')\n        assert_equal(mintypecode('fd'), 'd')\n        assert_equal(mintypecode('fF'), 'F')\n        assert_equal(mintypecode('fD'), 'D')\n        assert_equal(mintypecode('df'), 'd')\n        assert_equal(mintypecode('dd'), 'd')\n        assert_equal(mintypecode('dF'), 'D')\n        assert_equal(mintypecode('dD'), 'D')\n        assert_equal(mintypecode('Ff'), 'F')\n        assert_equal(mintypecode('Fd'), 'D')\n        assert_equal(mintypecode('FF'), 'F')\n        assert_equal(mintypecode('FD'), 'D')\n        assert_equal(mintypecode('Df'), 'D')\n        assert_equal(mintypecode('Dd'), 'D')\n        assert_equal(mintypecode('DF'), 'D')\n        assert_equal(mintypecode('DD'), 'D')\n\n    def test_default_3(self):\n        assert_equal(mintypecode('fdF'), 'D')\n        assert_equal(mintypecode('fdD'), 'D')\n        assert_equal(mintypecode('fFD'), 'D')\n        assert_equal(mintypecode('dFD'), 'D')\n        assert_equal(mintypecode('ifd'), 'd')\n        assert_equal(mintypecode('ifF'), 'F')\n        assert_equal(mintypecode('ifD'), 'D')\n        assert_equal(mintypecode('idF'), 'D')\n        assert_equal(mintypecode('idD'), 'D')", "class_fn": true, "question_id": "numpy/numpy.lib.tests.test_type_check/TestMintypecode", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/lib/utils.py", "fn_id": "", "content": "class _Deprecate:\n    \"\"\"\n    Decorator class to deprecate old functions.\n\n    Refer to `deprecate` for details.\n\n    See Also\n    --------\n    deprecate\n\n    \"\"\"\n\n    def __init__(self, old_name=None, new_name=None, message=None):\n        self.old_name = old_name\n        self.new_name = new_name\n        self.message = message\n\n    def __call__(self, func, *args, **kwargs):\n        \"\"\"\n        Decorator call.  Refer to ``decorate``.\n\n        \"\"\"\n        old_name = self.old_name\n        new_name = self.new_name\n        message = self.message\n        if old_name is None:\n            old_name = func.__name__\n        if new_name is None:\n            depdoc = '`%s` is deprecated!' % old_name\n        else:\n            depdoc = '`%s` is deprecated, use `%s` instead!' % (old_name, new_name)\n        if message is not None:\n            depdoc += '\\n' + message\n\n        @functools.wraps(func)\n        def newfunc(*args, **kwds):\n            warnings.warn(depdoc, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwds)\n        newfunc.__name__ = old_name\n        doc = func.__doc__\n        if doc is None:\n            doc = depdoc\n        else:\n            lines = doc.expandtabs().split('\\n')\n            indent = _get_indent(lines[1:])\n            if lines[0].lstrip():\n                doc = indent * ' ' + doc\n            else:\n                skip = len(lines[0]) + 1\n                for line in lines[1:]:\n                    if len(line) > indent:\n                        break\n                    skip += len(line) + 1\n                doc = doc[skip:]\n            depdoc = textwrap.indent(depdoc, ' ' * indent)\n            doc = '\\n\\n'.join([depdoc, doc])\n        newfunc.__doc__ = doc\n        return newfunc", "class_fn": true, "question_id": "numpy/numpy.lib.utils/_Deprecate", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/core.py", "fn_id": "", "content": "class _DomainGreaterEqual:\n    \"\"\"\n    DomainGreaterEqual(v)(x) is True where x < v.\n\n    \"\"\"\n\n    def __init__(self, critical_value):\n        \"\"\"DomainGreaterEqual(v)(x) = true where x < v\"\"\"\n        self.critical_value = critical_value\n\n    def __call__(self, x):\n        \"\"\"Executes the call behavior.\"\"\"\n        with np.errstate(invalid='ignore'):\n            return umath.less(x, self.critical_value)", "class_fn": true, "question_id": "numpy/numpy.ma.core/_DomainGreaterEqual", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/core.py", "fn_id": "", "content": "class _MaskedPrintOption:\n    \"\"\"\n    Handle the string used to represent missing data in a masked array.\n\n    \"\"\"\n\n    def __init__(self, display):\n        \"\"\"\n        Create the masked_print_option object.\n\n        \"\"\"\n        self._display = display\n        self._enabled = True\n\n    def display(self):\n        \"\"\"\n        Display the string to print for masked values.\n\n        \"\"\"\n        return self._display\n\n    def set_display(self, s):\n        \"\"\"\n        Set the string to print for masked values.\n\n        \"\"\"\n        self._display = s\n\n    def enabled(self):\n        \"\"\"\n        Is the use of the display value enabled?\n\n        \"\"\"\n        return self._enabled\n\n    def enable(self, shrink=1):\n        \"\"\"\n        Set the enabling shrink to `shrink`.\n\n        \"\"\"\n        self._enabled = shrink\n\n    def __str__(self):\n        return str(self._display)\n    __repr__ = __str__", "class_fn": true, "question_id": "numpy/numpy.ma.core/_MaskedPrintOption", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/extras.py", "fn_id": "", "content": "class MAxisConcatenator(AxisConcatenator):\n    \"\"\"\n    Translate slice objects to concatenation along an axis.\n\n    For documentation on usage, see `mr_class`.\n\n    See Also\n    --------\n    mr_class\n\n    \"\"\"\n    concatenate = staticmethod(concatenate)\n\n    def __getitem__(self, key):\n        if isinstance(key, str):\n            raise MAError('Unavailable for masked array.')\n        return super().__getitem__(key)\n\n    def __len__(self):\n        return 0\n\n    @classmethod\n    def makemat(cls, arr):\n        data = super().makemat(arr.data, copy=False)\n        return array(data, mask=arr.mask)\n\n    def __init__(self, axis=0, matrix=False, ndmin=1, trans1d=-1):\n        self.axis = axis\n        self.matrix = matrix\n        self.trans1d = trans1d\n        self.ndmin = ndmin", "class_fn": true, "question_id": "numpy/numpy.ma.extras/MAxisConcatenator", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/extras.py", "fn_id": "", "content": "class _fromnxfunction_args(_fromnxfunction):\n    \"\"\"\n    A version of `_fromnxfunction` that is called with multiple array\n    arguments. The first non-array-like input marks the beginning of the\n    arguments that are passed verbatim for both the data and mask calls.\n    Array arguments are processed independently and the results are\n    returned in a list. If only one array is found, the return value is\n    just the processed array instead of a list.\n    \"\"\"\n\n    def __init__(self, funcname):\n        self.__name__ = funcname\n        self.__doc__ = self.getdoc()\n\n    def __call__(self, *args, **params):\n        func = getattr(np, self.__name__)\n        arrays = []\n        args = list(args)\n        while len(args) > 0 and issequence(args[0]):\n            arrays.append(args.pop(0))\n        res = []\n        for x in arrays:\n            _d = func(np.asarray(x), *args, **params)\n            _m = func(getmaskarray(x), *args, **params)\n            res.append(masked_array(_d, mask=_m))\n        if len(arrays) == 1:\n            return res[0]\n        return res\n\n    def getdoc(self):\n        \"\"\"\n        Retrieve the docstring and signature from the function.\n\n        The ``__doc__`` attribute of the function is used as the docstring for\n        the new masked array version of the function. A note on application\n        of the function to the mask is appended.\n\n        Parameters\n        ----------\n        None\n\n        \"\"\"\n        npfunc = getattr(np, self.__name__, None)\n        doc = getattr(npfunc, '__doc__', None)\n        if doc:\n            sig = self.__name__ + ma.get_object_signature(npfunc)\n            doc = ma.doc_note(doc, 'The function is applied to both the _data and the _mask, if any.')\n            return '\\n\\n'.join((sig, doc))\n        return", "class_fn": true, "question_id": "numpy/numpy.ma.extras/_fromnxfunction_args", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/extras.py", "fn_id": "", "content": "class mr_class(MAxisConcatenator):\n    \"\"\"\n    Translate slice objects to concatenation along the first axis.\n\n    This is the masked array version of `lib.index_tricks.RClass`.\n\n    See Also\n    --------\n    lib.index_tricks.RClass\n\n    Examples\n    --------\n    >>> np.ma.mr_[np.ma.array([1,2,3]), 0, 0, np.ma.array([4,5,6])]\n    masked_array(data=[1, 2, 3, ..., 4, 5, 6],\n                 mask=False,\n           fill_value=999999)\n\n    \"\"\"\n\n    def __getitem__(self, key):\n        if isinstance(key, str):\n            raise MAError('Unavailable for masked array.')\n        return super().__getitem__(key)\n\n    def __init__(self):\n        MAxisConcatenator.__init__(self, 0)", "class_fn": true, "question_id": "numpy/numpy.ma.extras/mr_class", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/tests/test_subclassing.py", "fn_id": "", "content": "class CSAIterator:\n    \"\"\"\n    Flat iterator object that uses its own setter/getter\n    (works around ndarray.flat not propagating subclass setters/getters\n    see https://github.com/numpy/numpy/issues/4564)\n    roughly following MaskedIterator\n    \"\"\"\n\n    def __init__(self, a):\n        self._original = a\n        self._dataiter = a.view(np.ndarray).flat\n\n    def __iter__(self):\n        return self\n\n    def __getitem__(self, indx):\n        out = self._dataiter.__getitem__(indx)\n        if not isinstance(out, np.ndarray):\n            out = out.__array__()\n        out = out.view(type(self._original))\n        return out\n\n    def __setitem__(self, index, value):\n        self._dataiter[index] = self._original._validate_input(value)\n\n    def __next__(self):\n        return next(self._dataiter).__array__().view(type(self._original))", "class_fn": true, "question_id": "numpy/numpy.ma.tests.test_subclassing/CSAIterator", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/tests/test_subclassing.py", "fn_id": "", "content": "class SubArray(np.ndarray):\n\n    def __new__(cls, arr, info={}):\n        x = np.asanyarray(arr).view(cls)\n        x.info = info.copy()\n        return x\n\n    def __array_finalize__(self, obj):\n        super().__array_finalize__(obj)\n        self.info = getattr(obj, 'info', {}).copy()\n        return\n\n    def __add__(self, other):\n        result = super().__add__(other)\n        result.info['added'] = result.info.get('added', 0) + 1\n        return result\n\n    def __iadd__(self, other):\n        result = super().__iadd__(other)\n        result.info['iadded'] = result.info.get('iadded', 0) + 1\n        return result", "class_fn": true, "question_id": "numpy/numpy.ma.tests.test_subclassing/SubArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/ma/tests/test_subclassing.py", "fn_id": "", "content": "class WrappedArray(NDArrayOperatorsMixin):\n    \"\"\"\n    Wrapping a MaskedArray rather than subclassing to test that\n    ufunc deferrals are commutative.\n    See: https://github.com/numpy/numpy/issues/15200)\n    \"\"\"\n    __slots__ = ('_array', 'attrs')\n    __array_priority__ = 20\n\n    def __init__(self, array, **attrs):\n        self._array = array\n        self.attrs = attrs\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(\\n{self._array}\\n{self.attrs}\\n)'\n\n    def __array__(self):\n        return np.asarray(self._array)\n\n    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n        if method == '__call__':\n            inputs = [arg._array if isinstance(arg, self.__class__) else arg for arg in inputs]\n            return self.__class__(ufunc(*inputs, **kwargs), **self.attrs)\n        else:\n            return NotImplemented", "class_fn": true, "question_id": "numpy/numpy.ma.tests.test_subclassing/WrappedArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/matrixlib/tests/test_defmatrix.py", "fn_id": "", "content": "class TestCtor:\n\n    def test_basic(self):\n        A = np.array([[1, 2], [3, 4]])\n        mA = matrix(A)\n        assert_(np.all(mA.A == A))\n        B = bmat('A,A;A,A')\n        C = bmat([[A, A], [A, A]])\n        D = np.array([[1, 2, 1, 2], [3, 4, 3, 4], [1, 2, 1, 2], [3, 4, 3, 4]])\n        assert_(np.all(B.A == D))\n        assert_(np.all(C.A == D))\n        E = np.array([[5, 6], [7, 8]])\n        AEresult = matrix([[1, 2, 5, 6], [3, 4, 7, 8]])\n        assert_(np.all(bmat([A, E]) == AEresult))\n        vec = np.arange(5)\n        mvec = matrix(vec)\n        assert_(mvec.shape == (1, 5))\n\n    def test_exceptions(self):\n        assert_raises(ValueError, matrix, 'invalid')\n\n    def test_bmat_nondefault_str(self):\n        A = np.array([[1, 2], [3, 4]])\n        B = np.array([[5, 6], [7, 8]])\n        Aresult = np.array([[1, 2, 1, 2], [3, 4, 3, 4], [1, 2, 1, 2], [3, 4, 3, 4]])\n        mixresult = np.array([[1, 2, 5, 6], [3, 4, 7, 8], [5, 6, 1, 2], [7, 8, 3, 4]])\n        assert_(np.all(bmat('A,A;A,A') == Aresult))\n        assert_(np.all(bmat('A,A;A,A', ldict={'A': B}) == Aresult))\n        assert_raises(TypeError, bmat, 'A,A;A,A', gdict={'A': B})\n        assert_(np.all(bmat('A,A;A,A', ldict={'A': A}, gdict={'A': B}) == Aresult))\n        b2 = bmat('A,B;C,D', ldict={'A': A, 'B': B}, gdict={'C': B, 'D': A})\n        assert_(np.all(b2 == mixresult))", "class_fn": true, "question_id": "numpy/numpy.matrixlib.tests.test_defmatrix/TestCtor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/matrixlib/tests/test_defmatrix.py", "fn_id": "", "content": "class TestPower:\n\n    def test_returntype(self):\n        a = np.array([[0, 1], [0, 0]])\n        assert_(type(matrix_power(a, 2)) is np.ndarray)\n        a = mat(a)\n        assert_(type(matrix_power(a, 2)) is matrix)\n\n    def test_list(self):\n        assert_array_equal(matrix_power([[0, 1], [0, 0]], 2), [[0, 0], [0, 0]])", "class_fn": true, "question_id": "numpy/numpy.matrixlib.tests.test_defmatrix/TestPower", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/matrixlib/tests/test_masked_matrix.py", "fn_id": "", "content": "class TestSubclassing:\n\n    def setup_method(self):\n        x = np.arange(5, dtype='float')\n        mx = MMatrix(x, mask=[0, 1, 0, 0, 0])\n        self.data = (x, mx)\n\n    def test_maskedarray_subclassing(self):\n        (x, mx) = self.data\n        assert_(isinstance(mx._data, np.matrix))\n\n    def test_masked_unary_operations(self):\n        (x, mx) = self.data\n        with np.errstate(divide='ignore'):\n            assert_(isinstance(log(mx), MMatrix))\n            assert_equal(log(x), np.log(x))\n\n    def test_masked_binary_operations(self):\n        (x, mx) = self.data\n        assert_(isinstance(add(mx, mx), MMatrix))\n        assert_(isinstance(add(mx, x), MMatrix))\n        assert_equal(add(mx, x), mx + x)\n        assert_(isinstance(add(mx, mx)._data, np.matrix))\n        with assert_warns(DeprecationWarning):\n            assert_(isinstance(add.outer(mx, mx), MMatrix))\n        assert_(isinstance(hypot(mx, mx), MMatrix))\n        assert_(isinstance(hypot(mx, x), MMatrix))\n\n    def test_masked_binary_operations2(self):\n        (x, mx) = self.data\n        xmx = masked_array(mx.data.__array__(), mask=mx.mask)\n        assert_(isinstance(divide(mx, mx), MMatrix))\n        assert_(isinstance(divide(mx, x), MMatrix))\n        assert_equal(divide(mx, mx), divide(xmx, xmx))", "class_fn": true, "question_id": "numpy/numpy.matrixlib.tests.test_masked_matrix/TestSubclassing", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/matrixlib/tests/test_regression.py", "fn_id": "", "content": "class TestRegression:\n\n    def test_kron_matrix(self):\n        x = np.matrix('[1 0; 1 0]')\n        assert_equal(type(np.kron(x, x)), type(x))\n\n    def test_matrix_properties(self):\n        a = np.matrix([1.0], dtype=float)\n        assert_(type(a.real) is np.matrix)\n        assert_(type(a.imag) is np.matrix)\n        (c, d) = np.matrix([0.0]).nonzero()\n        assert_(type(c) is np.ndarray)\n        assert_(type(d) is np.ndarray)\n\n    def test_matrix_multiply_by_1d_vector(self):\n\n        def mul():\n            np.mat(np.eye(2)) * np.ones(2)\n        assert_raises(ValueError, mul)\n\n    def test_matrix_std_argmax(self):\n        x = np.asmatrix(np.random.uniform(0, 1, (3, 3)))\n        assert_equal(x.std().shape, ())\n        assert_equal(x.argmax().shape, ())", "class_fn": true, "question_id": "numpy/numpy.matrixlib.tests.test_regression/TestRegression", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/laguerre.py", "fn_id": "", "content": "class Laguerre(ABCPolyBase):\n    \"\"\"A Laguerre series class.\n\n    The Laguerre class provides the standard Python numerical methods\n    '+', '-', '*', '//', '%', 'divmod', '**', and '()' as well as the\n    attributes and methods listed in the `ABCPolyBase` documentation.\n\n    Parameters\n    ----------\n    coef : array_like\n        Laguerre coefficients in order of increasing degree, i.e,\n        ``(1, 2, 3)`` gives ``1*L_0(x) + 2*L_1(X) + 3*L_2(x)``.\n    domain : (2,) array_like, optional\n        Domain to use. The interval ``[domain[0], domain[1]]`` is mapped\n        to the interval ``[window[0], window[1]]`` by shifting and scaling.\n        The default value is [0, 1].\n    window : (2,) array_like, optional\n        Window, see `domain` for its use. The default value is [0, 1].\n\n        .. versionadded:: 1.6.0\n    symbol : str, optional\n        Symbol used to represent the independent variable in string\n        representations of the polynomial expression, e.g. for printing.\n        The symbol must be a valid Python identifier. Default value is 'x'.\n\n        .. versionadded:: 1.24\n\n    \"\"\"\n    _add = staticmethod(lagadd)\n    _sub = staticmethod(lagsub)\n    _mul = staticmethod(lagmul)\n    _div = staticmethod(lagdiv)\n    _pow = staticmethod(lagpow)\n    _val = staticmethod(lagval)\n    _int = staticmethod(lagint)\n    _der = staticmethod(lagder)\n    _fit = staticmethod(lagfit)\n    _line = staticmethod(lagline)\n    _roots = staticmethod(lagroots)\n    _fromroots = staticmethod(lagfromroots)\n    domain = np.array(lagdomain)\n    window = np.array(lagdomain)\n    basis_name = 'L'\n\n    def __divmod__(self, other):\n        othercoef = self._get_coefficients(other)\n        try:\n            (quo, rem) = self._div(self.coef, othercoef)\n        except ZeroDivisionError:\n            raise\n        except Exception:\n            return NotImplemented\n        quo = self.__class__(quo, self.domain, self.window, self.symbol)\n        rem = self.__class__(rem, self.domain, self.window, self.symbol)\n        return (quo, rem)\n\n    def __mod__(self, other):\n        res = self.__divmod__(other)\n        if res is NotImplemented:\n            return res\n        return res[1]\n\n    def has_samecoef(self, other):\n        \"\"\"Check if coefficients match.\n\n        .. versionadded:: 1.6.0\n\n        Parameters\n        ----------\n        other : class instance\n            The other class must have the ``coef`` attribute.\n\n        Returns\n        -------\n        bool : boolean\n            True if the coefficients are the same, False otherwise.\n\n        \"\"\"\n        if len(self.coef) != len(other.coef):\n            return False\n        elif not np.all(self.coef == other.coef):\n            return False\n        else:\n            return True", "class_fn": true, "question_id": "numpy/numpy.polynomial.laguerre/Laguerre", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_chebyshev.py", "fn_id": "", "content": "class TestConstants:\n\n    def test_chebdomain(self):\n        assert_equal(cheb.chebdomain, [-1, 1])\n\n    def test_chebzero(self):\n        assert_equal(cheb.chebzero, [0])\n\n    def test_chebone(self):\n        assert_equal(cheb.chebone, [1])\n\n    def test_chebx(self):\n        assert_equal(cheb.chebx, [0, 1])", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_chebyshev/TestConstants", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_chebyshev.py", "fn_id": "", "content": "class TestInterpolate:\n\n    def f(self, x):\n        return x * (x - 1) * (x - 2)\n\n    def test_raises(self):\n        assert_raises(ValueError, cheb.chebinterpolate, self.f, -1)\n        assert_raises(TypeError, cheb.chebinterpolate, self.f, 10.0)\n\n    def test_dimensions(self):\n        for deg in range(1, 5):\n            assert_(cheb.chebinterpolate(self.f, deg).shape == (deg + 1,))\n\n    def test_approximation(self):\n\n        def powx(x, p):\n            return x ** p\n        x = np.linspace(-1, 1, 10)\n        for deg in range(0, 10):\n            for p in range(0, deg + 1):\n                c = cheb.chebinterpolate(powx, deg, (p,))\n                assert_almost_equal(cheb.chebval(x, c), powx(x, p), decimal=12)", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_chebyshev/TestInterpolate", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_hermite.py", "fn_id": "", "content": "class TestCompanion:\n\n    def test_raises(self):\n        assert_raises(ValueError, herm.hermcompanion, [])\n        assert_raises(ValueError, herm.hermcompanion, [1])\n\n    def test_dimensions(self):\n        for i in range(1, 5):\n            coef = [0] * i + [1]\n            assert_(herm.hermcompanion(coef).shape == (i, i))\n\n    def test_linear_root(self):\n        assert_(herm.hermcompanion([1, 2])[0, 0] == -0.25)", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_hermite/TestCompanion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_hermite.py", "fn_id": "", "content": "class TestGauss:\n\n    def test_100(self):\n        (x, w) = herm.hermgauss(100)\n        v = herm.hermvander(x, 99)\n        vv = np.dot(v.T * w, v)\n        vd = 1 / np.sqrt(vv.diagonal())\n        vv = vd[:, None] * vv * vd\n        assert_almost_equal(vv, np.eye(100))\n        tgt = np.sqrt(np.pi)\n        assert_almost_equal(w.sum(), tgt)", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_hermite/TestGauss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_hermite_e.py", "fn_id": "", "content": "class TestCompanion:\n\n    def test_raises(self):\n        assert_raises(ValueError, herme.hermecompanion, [])\n        assert_raises(ValueError, herme.hermecompanion, [1])\n\n    def test_dimensions(self):\n        for i in range(1, 5):\n            coef = [0] * i + [1]\n            assert_(herme.hermecompanion(coef).shape == (i, i))\n\n    def test_linear_root(self):\n        assert_(herme.hermecompanion([1, 2])[0, 0] == -0.5)", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_hermite_e/TestCompanion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_hermite_e.py", "fn_id": "", "content": "class TestGauss:\n\n    def test_100(self):\n        (x, w) = herme.hermegauss(100)\n        v = herme.hermevander(x, 99)\n        vv = np.dot(v.T * w, v)\n        vd = 1 / np.sqrt(vv.diagonal())\n        vv = vd[:, None] * vv * vd\n        assert_almost_equal(vv, np.eye(100))\n        tgt = np.sqrt(2 * np.pi)\n        assert_almost_equal(w.sum(), tgt)", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_hermite_e/TestGauss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_laguerre.py", "fn_id": "", "content": "class TestCompanion:\n\n    def test_raises(self):\n        assert_raises(ValueError, lag.lagcompanion, [])\n        assert_raises(ValueError, lag.lagcompanion, [1])\n\n    def test_dimensions(self):\n        for i in range(1, 5):\n            coef = [0] * i + [1]\n            assert_(lag.lagcompanion(coef).shape == (i, i))\n\n    def test_linear_root(self):\n        assert_(lag.lagcompanion([1, 2])[0, 0] == 1.5)", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_laguerre/TestCompanion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_laguerre.py", "fn_id": "", "content": "class TestGauss:\n\n    def test_100(self):\n        (x, w) = lag.laggauss(100)\n        v = lag.lagvander(x, 99)\n        vv = np.dot(v.T * w, v)\n        vd = 1 / np.sqrt(vv.diagonal())\n        vv = vd[:, None] * vv * vd\n        assert_almost_equal(vv, np.eye(100))\n        tgt = 1.0\n        assert_almost_equal(w.sum(), tgt)", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_laguerre/TestGauss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_legendre.py", "fn_id": "", "content": "class TestCompanion:\n\n    def test_raises(self):\n        assert_raises(ValueError, leg.legcompanion, [])\n        assert_raises(ValueError, leg.legcompanion, [1])\n\n    def test_dimensions(self):\n        for i in range(1, 5):\n            coef = [0] * i + [1]\n            assert_(leg.legcompanion(coef).shape == (i, i))\n\n    def test_linear_root(self):\n        assert_(leg.legcompanion([1, 2])[0, 0] == -0.5)", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_legendre/TestCompanion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_legendre.py", "fn_id": "", "content": "class TestGauss:\n\n    def test_100(self):\n        (x, w) = leg.leggauss(100)\n        v = leg.legvander(x, 99)\n        vv = np.dot(v.T * w, v)\n        vd = 1 / np.sqrt(vv.diagonal())\n        vv = vd[:, None] * vv * vd\n        assert_almost_equal(vv, np.eye(100))\n        tgt = 2.0\n        assert_almost_equal(w.sum(), tgt)", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_legendre/TestGauss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_polynomial.py", "fn_id": "", "content": "class TestCompanion:\n\n    def test_raises(self):\n        assert_raises(ValueError, poly.polycompanion, [])\n        assert_raises(ValueError, poly.polycompanion, [1])\n\n    def test_dimensions(self):\n        for i in range(1, 5):\n            coef = [0] * i + [1]\n            assert_(poly.polycompanion(coef).shape == (i, i))\n\n    def test_linear_root(self):\n        assert_(poly.polycompanion([1, 2])[0, 0] == -0.5)", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_polynomial/TestCompanion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/polynomial/tests/test_polynomial.py", "fn_id": "", "content": "class TestVander:\n    x = np.random.random((3, 5)) * 2 - 1\n\n    def test_polyvander(self):\n        x = np.arange(3)\n        v = poly.polyvander(x, 3)\n        assert_(v.shape == (3, 4))\n        for i in range(4):\n            coef = [0] * i + [1]\n            assert_almost_equal(v[..., i], poly.polyval(x, coef))\n        x = np.array([[1, 2], [3, 4], [5, 6]])\n        v = poly.polyvander(x, 3)\n        assert_(v.shape == (3, 2, 4))\n        for i in range(4):\n            coef = [0] * i + [1]\n            assert_almost_equal(v[..., i], poly.polyval(x, coef))\n\n    def test_polyvander2d(self):\n        (x1, x2, x3) = self.x\n        c = np.random.random((2, 3))\n        van = poly.polyvander2d(x1, x2, [1, 2])\n        tgt = poly.polyval2d(x1, x2, c)\n        res = np.dot(van, c.flat)\n        assert_almost_equal(res, tgt)\n        van = poly.polyvander2d([x1], [x2], [1, 2])\n        assert_(van.shape == (1, 5, 6))\n\n    def test_polyvander3d(self):\n        (x1, x2, x3) = self.x\n        c = np.random.random((2, 3, 4))\n        van = poly.polyvander3d(x1, x2, x3, [1, 2, 3])\n        tgt = poly.polyval3d(x1, x2, x3, c)\n        res = np.dot(van, c.flat)\n        assert_almost_equal(res, tgt)\n        van = poly.polyvander3d([x1], [x2], [x3], [1, 2, 3])\n        assert_(van.shape == (1, 5, 24))\n\n    def test_polyvandernegdeg(self):\n        x = np.arange(3)\n        assert_raises(ValueError, poly.polyvander, x, -1)", "class_fn": true, "question_id": "numpy/numpy.polynomial.tests.test_polynomial/TestVander", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/testing/_private/utils.py", "fn_id": "", "content": "class IgnoreException(Exception):\n    \"\"\"Ignoring this exception due to disabled feature\"\"\"\n    pass", "class_fn": true, "question_id": "numpy/numpy.testing._private.utils/IgnoreException", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/testing/print_coercion_tables.py", "fn_id": "", "content": "class GenericObject:\n\n    def __init__(self, v):\n        self.v = v\n\n    def __add__(self, other):\n        return self\n\n    def __radd__(self, other):\n        return self\n    dtype = np.dtype('O')", "class_fn": true, "question_id": "numpy/numpy.testing.print_coercion_tables/GenericObject", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_typing/__init__.py", "fn_id": "", "content": "class _128Bit(_256Bit):\n    pass", "class_fn": true, "question_id": "numpy/numpy._typing/_128Bit", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_typing/__init__.py", "fn_id": "", "content": "class _32Bit(_64Bit):\n    pass", "class_fn": true, "question_id": "numpy/numpy._typing/_32Bit", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_typing/__init__.py", "fn_id": "", "content": "class _8Bit(_16Bit):\n    pass", "class_fn": true, "question_id": "numpy/numpy._typing/_8Bit", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_typing/_dtype_like.py", "fn_id": "", "content": "class _DTypeDictBase(TypedDict):\n    names: Sequence[str]\n    formats: Sequence[_DTypeLikeNested]", "class_fn": true, "question_id": "numpy/numpy._typing._dtype_like/_DTypeDictBase", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/_utils/_pep440.py", "fn_id": "", "content": "class Version(_BaseVersion):\n    _regex = re.compile('^\\\\s*' + VERSION_PATTERN + '\\\\s*$', re.VERBOSE | re.IGNORECASE)\n\n    def __str__(self):\n        parts = []\n        if self._version.epoch != 0:\n            parts.append('{0}!'.format(self._version.epoch))\n        parts.append('.'.join((str(x) for x in self._version.release)))\n        if self._version.pre is not None:\n            parts.append(''.join((str(x) for x in self._version.pre)))\n        if self._version.post is not None:\n            parts.append('.post{0}'.format(self._version.post[1]))\n        if self._version.dev is not None:\n            parts.append('.dev{0}'.format(self._version.dev[1]))\n        if self._version.local is not None:\n            parts.append('+{0}'.format('.'.join((str(x) for x in self._version.local))))\n        return ''.join(parts)\n\n    def __init__(self, version):\n        match = self._regex.search(version)\n        if not match:\n            raise InvalidVersion(\"Invalid version: '{0}'\".format(version))\n        self._version = _Version(epoch=int(match.group('epoch')) if match.group('epoch') else 0, release=tuple((int(i) for i in match.group('release').split('.'))), pre=_parse_letter_version(match.group('pre_l'), match.group('pre_n')), post=_parse_letter_version(match.group('post_l'), match.group('post_n1') or match.group('post_n2')), dev=_parse_letter_version(match.group('dev_l'), match.group('dev_n')), local=_parse_local_version(match.group('local')))\n        self._key = _cmpkey(self._version.epoch, self._version.release, self._version.pre, self._version.post, self._version.dev, self._version.local)\n\n    def __le__(self, other):\n        return self._compare(other, lambda s, o: s <= o)\n\n    @property\n    def public(self):\n        return str(self).split('+', 1)[0]\n\n    @property\n    def base_version(self):\n        parts = []\n        if self._version.epoch != 0:\n            parts.append('{0}!'.format(self._version.epoch))\n        parts.append('.'.join((str(x) for x in self._version.release)))\n        return ''.join(parts)\n\n    @property\n    def local(self):\n        version_string = str(self)\n        if '+' in version_string:\n            return version_string.split('+', 1)[1]\n\n    @property\n    def is_prerelease(self):\n        return bool(self._version.dev or self._version.pre)\n\n    @property\n    def is_postrelease(self):\n        return bool(self._version.post)\n\n    def _compare(self, other, method):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n        return method(self._key, other._key)\n\n    def __repr__(self):\n        return '<Version({0})>'.format(repr(str(self)))\n\n    def __hash__(self):\n        return hash(self._key)", "class_fn": true, "question_id": "numpy/numpy._utils._pep440/Version", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/array_api/_set_functions.py", "fn_id": "", "content": "class UniqueAllResult(NamedTuple):\n    values: Array\n    indices: Array\n    inverse_indices: Array\n    counts: Array", "class_fn": true, "question_id": "numpy/numpy.array_api._set_functions/UniqueAllResult", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/array_api/linalg.py", "fn_id": "", "content": "class EighResult(NamedTuple):\n    eigenvalues: Array\n    eigenvectors: Array", "class_fn": true, "question_id": "numpy/numpy.array_api.linalg/EighResult", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/array_api/linalg.py", "fn_id": "", "content": "class SlogdetResult(NamedTuple):\n    sign: Array\n    logabsdet: Array", "class_fn": true, "question_id": "numpy/numpy.array_api.linalg/SlogdetResult", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/_ufunc_config.py", "fn_id": "", "content": "class _unspecified:\n    pass", "class_fn": true, "question_id": "numpy/numpy.core._ufunc_config/_unspecified", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/core/numerictypes.py", "fn_id": "", "content": "class _typedict(dict):\n    \"\"\"\n    Base object for a dictionary for look-up with any alias for an array dtype.\n\n    Instances of `_typedict` can not be used as dictionaries directly,\n    first they have to be populated.\n\n    \"\"\"\n\n    def __getitem__(self, obj):\n        return dict.__getitem__(self, obj2sctype(obj))", "class_fn": true, "question_id": "numpy/numpy.core.numerictypes/_typedict", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/ccompiler_opt.py", "fn_id": "", "content": "class CCompilerOpt(_Config, _Distutils, _Cache, _CCompiler, _Feature, _Parse):\n    \"\"\"\n    A helper class for `CCompiler` aims to provide extra build options\n    to effectively control of compiler optimizations that are directly\n    related to CPU features.\n    \"\"\"\n\n    def generate_dispatch_header(self, header_path):\n        \"\"\"\n        Generate the dispatch header which contains the #definitions and headers\n        for platform-specific instruction-sets for the enabled CPU baseline and\n        dispatch-able features.\n\n        Its highly recommended to take a look at the generated header\n        also the generated source files via `try_dispatch()`\n        in order to get the full picture.\n        \"\"\"\n        self.dist_log('generate CPU dispatch header: (%s)' % header_path)\n        baseline_names = self.cpu_baseline_names()\n        dispatch_names = self.cpu_dispatch_names()\n        baseline_len = len(baseline_names)\n        dispatch_len = len(dispatch_names)\n        header_dir = os.path.dirname(header_path)\n        if not os.path.exists(header_dir):\n            self.dist_log(f'dispatch header dir {header_dir} does not exist, creating it', stderr=True)\n            os.makedirs(header_dir)\n        with open(header_path, 'w') as f:\n            baseline_calls = ' \\\\\\n'.join(['\\t%sWITH_CPU_EXPAND_(MACRO_TO_CALL(%s, __VA_ARGS__))' % (self.conf_c_prefix, f) for f in baseline_names])\n            dispatch_calls = ' \\\\\\n'.join(['\\t%sWITH_CPU_EXPAND_(MACRO_TO_CALL(%s, __VA_ARGS__))' % (self.conf_c_prefix, f) for f in dispatch_names])\n            f.write(textwrap.dedent('                /*\\n                 * AUTOGENERATED DON\\'T EDIT\\n                 * Please make changes to the code generator (distutils/ccompiler_opt.py)\\n                */\\n                #define {pfx}WITH_CPU_BASELINE  \"{baseline_str}\"\\n                #define {pfx}WITH_CPU_DISPATCH  \"{dispatch_str}\"\\n                #define {pfx}WITH_CPU_BASELINE_N {baseline_len}\\n                #define {pfx}WITH_CPU_DISPATCH_N {dispatch_len}\\n                #define {pfx}WITH_CPU_EXPAND_(X) X\\n                #define {pfx}WITH_CPU_BASELINE_CALL(MACRO_TO_CALL, ...) \\\\\\n                {baseline_calls}\\n                #define {pfx}WITH_CPU_DISPATCH_CALL(MACRO_TO_CALL, ...) \\\\\\n                {dispatch_calls}\\n            ').format(pfx=self.conf_c_prefix, baseline_str=' '.join(baseline_names), dispatch_str=' '.join(dispatch_names), baseline_len=baseline_len, dispatch_len=dispatch_len, baseline_calls=baseline_calls, dispatch_calls=dispatch_calls))\n            baseline_pre = ''\n            for name in baseline_names:\n                baseline_pre += self.feature_c_preprocessor(name, tabs=1) + '\\n'\n            dispatch_pre = ''\n            for name in dispatch_names:\n                dispatch_pre += textwrap.dedent('                #ifdef {pfx}CPU_TARGET_{name}\\n                {pre}\\n                #endif /*{pfx}CPU_TARGET_{name}*/\\n                ').format(pfx=self.conf_c_prefix_, name=name, pre=self.feature_c_preprocessor(name, tabs=1))\n            f.write(textwrap.dedent('            /******* baseline features *******/\\n            {baseline_pre}\\n            /******* dispatch features *******/\\n            {dispatch_pre}\\n            ').format(pfx=self.conf_c_prefix_, baseline_pre=baseline_pre, dispatch_pre=dispatch_pre))\n\n    def _generate_config(self, output_dir, dispatch_src, targets, has_baseline=False):\n        config_path = os.path.basename(dispatch_src)\n        config_path = os.path.splitext(config_path)[0] + '.h'\n        config_path = os.path.join(output_dir, config_path)\n        cache_hash = self.cache_hash(targets, has_baseline)\n        try:\n            with open(config_path) as f:\n                last_hash = f.readline().split('cache_hash:')\n                if len(last_hash) == 2 and int(last_hash[1]) == cache_hash:\n                    return True\n        except OSError:\n            pass\n        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n        self.dist_log('generate dispatched config -> ', config_path)\n        dispatch_calls = []\n        for tar in targets:\n            if isinstance(tar, str):\n                target_name = tar\n            else:\n                target_name = '__'.join([t for t in tar])\n            req_detect = self.feature_detect(tar)\n            req_detect = '&&'.join(['CHK(%s)' % f for f in req_detect])\n            dispatch_calls.append('\\t%sCPU_DISPATCH_EXPAND_(CB((%s), %s, __VA_ARGS__))' % (self.conf_c_prefix_, req_detect, target_name))\n        dispatch_calls = ' \\\\\\n'.join(dispatch_calls)\n        if has_baseline:\n            baseline_calls = '\\t%sCPU_DISPATCH_EXPAND_(CB(__VA_ARGS__))' % self.conf_c_prefix_\n        else:\n            baseline_calls = ''\n        with open(config_path, 'w') as fd:\n            fd.write(textwrap.dedent(\"            // cache_hash:{cache_hash}\\n            /**\\n             * AUTOGENERATED DON'T EDIT\\n             * Please make changes to the code generator (distutils/ccompiler_opt.py)\\n             */\\n            #ifndef {pfx}CPU_DISPATCH_EXPAND_\\n                #define {pfx}CPU_DISPATCH_EXPAND_(X) X\\n            #endif\\n            #undef {pfx}CPU_DISPATCH_BASELINE_CALL\\n            #undef {pfx}CPU_DISPATCH_CALL\\n            #define {pfx}CPU_DISPATCH_BASELINE_CALL(CB, ...) \\\\\\n            {baseline_calls}\\n            #define {pfx}CPU_DISPATCH_CALL(CHK, CB, ...) \\\\\\n            {dispatch_calls}\\n            \").format(pfx=self.conf_c_prefix_, baseline_calls=baseline_calls, dispatch_calls=dispatch_calls, cache_hash=cache_hash))\n        return False\n\n    def _wrap_target(self, output_dir, dispatch_src, target, nochange=False):\n        assert isinstance(target, (str, tuple))\n        if isinstance(target, str):\n            ext_name = target_name = target\n        else:\n            ext_name = '.'.join(target)\n            target_name = '__'.join(target)\n        wrap_path = os.path.join(output_dir, os.path.basename(dispatch_src))\n        wrap_path = '{0}.{2}{1}'.format(*os.path.splitext(wrap_path), ext_name.lower())\n        if nochange and os.path.exists(wrap_path):\n            return wrap_path\n        self.dist_log('wrap dispatch-able target -> ', wrap_path)\n        features = self.feature_sorted(self.feature_implies_c(target))\n        target_join = '#define %sCPU_TARGET_' % self.conf_c_prefix_\n        target_defs = [target_join + f for f in features]\n        target_defs = '\\n'.join(target_defs)\n        with open(wrap_path, 'w') as fd:\n            fd.write(textwrap.dedent('            /**\\n             * AUTOGENERATED DON\\'T EDIT\\n             * Please make changes to the code generator              (distutils/ccompiler_opt.py)\\n             */\\n            #define {pfx}CPU_TARGET_MODE\\n            #define {pfx}CPU_TARGET_CURRENT {target_name}\\n            {target_defs}\\n            #include \"{path}\"\\n            ').format(pfx=self.conf_c_prefix_, target_name=target_name, path=os.path.abspath(dispatch_src), target_defs=target_defs))\n        return wrap_path\n\n    def cpu_baseline_names(self):\n        \"\"\"\n        return a list of final CPU baseline feature names\n        \"\"\"\n        return self.parse_baseline_names\n\n    def _parse_token_group(self, token, has_baseline, final_targets, extra_flags):\n        \"\"\"validate group token\"\"\"\n        if len(token) <= 1 or token[-1:] == token[0]:\n            self.dist_fatal(\"'#' must stuck in the begin of group name\")\n        token = token[1:]\n        (ghas_baseline, gtargets, gextra_flags) = self.parse_target_groups.get(token, (False, None, []))\n        if gtargets is None:\n            self.dist_fatal(\"'%s' is an invalid target group name, \" % token + 'available target groups are', self.parse_target_groups.keys())\n        if ghas_baseline:\n            has_baseline = True\n        final_targets += [f for f in gtargets if f not in final_targets]\n        extra_flags += [f for f in gextra_flags if f not in extra_flags]\n        return (has_baseline, final_targets, extra_flags)\n\n    def is_cached(self):\n        \"\"\"\n        Returns True if the class loaded from the cache file\n        \"\"\"\n        return self.cache_infile and self.hit_cache\n\n    def cc_normalize_flags(self, flags):\n        \"\"\"\n        Remove the conflicts that caused due gathering implied features flags.\n\n        Parameters\n        ----------\n        'flags' list, compiler flags\n            flags should be sorted from the lowest to the highest interest.\n\n        Returns\n        -------\n        list, filtered from any conflicts.\n\n        Examples\n        --------\n        >>> self.cc_normalize_flags(['-march=armv8.2-a+fp16', '-march=armv8.2-a+dotprod'])\n        ['armv8.2-a+fp16+dotprod']\n\n        >>> self.cc_normalize_flags(\n            ['-msse', '-msse2', '-msse3', '-mssse3', '-msse4.1', '-msse4.2', '-mavx', '-march=core-avx2']\n        )\n        ['-march=core-avx2']\n        \"\"\"\n        assert isinstance(flags, list)\n        if self.cc_is_gcc or self.cc_is_clang or self.cc_is_icc:\n            return self._cc_normalize_unix(flags)\n        if self.cc_is_msvc or self.cc_is_iccw:\n            return self._cc_normalize_win(flags)\n        return flags\n\n    def _parse_token_policy(self, token):\n        \"\"\"validate policy token\"\"\"\n        if len(token) <= 1 or token[-1:] == token[0]:\n            self.dist_fatal(\"'$' must stuck in the begin of policy name\")\n        token = token[1:]\n        if token not in self._parse_policies:\n            self.dist_fatal(\"'%s' is an invalid policy name, available policies are\" % token, self._parse_policies.keys())\n        return token\n\n    def report(self, full=False):\n        report = []\n        platform_rows = []\n        baseline_rows = []\n        dispatch_rows = []\n        report.append(('Platform', platform_rows))\n        report.append(('', ''))\n        report.append(('CPU baseline', baseline_rows))\n        report.append(('', ''))\n        report.append(('CPU dispatch', dispatch_rows))\n        platform_rows.append(('Architecture', 'unsupported' if self.cc_on_noarch else self.cc_march))\n        platform_rows.append(('Compiler', 'unix-like' if self.cc_is_nocc else self.cc_name))\n        if self.cc_noopt:\n            baseline_rows.append(('Requested', 'optimization disabled'))\n        else:\n            baseline_rows.append(('Requested', repr(self._requested_baseline)))\n        baseline_names = self.cpu_baseline_names()\n        baseline_rows.append(('Enabled', ' '.join(baseline_names) if baseline_names else 'none'))\n        baseline_flags = self.cpu_baseline_flags()\n        baseline_rows.append(('Flags', ' '.join(baseline_flags) if baseline_flags else 'none'))\n        extra_checks = []\n        for name in baseline_names:\n            extra_checks += self.feature_extra_checks(name)\n        baseline_rows.append(('Extra checks', ' '.join(extra_checks) if extra_checks else 'none'))\n        if self.cc_noopt:\n            baseline_rows.append(('Requested', 'optimization disabled'))\n        else:\n            dispatch_rows.append(('Requested', repr(self._requested_dispatch)))\n        dispatch_names = self.cpu_dispatch_names()\n        dispatch_rows.append(('Enabled', ' '.join(dispatch_names) if dispatch_names else 'none'))\n        target_sources = {}\n        for (source, (_, targets)) in self.sources_status.items():\n            for tar in targets:\n                target_sources.setdefault(tar, []).append(source)\n        if not full or not target_sources:\n            generated = ''\n            for tar in self.feature_sorted(target_sources):\n                sources = target_sources[tar]\n                name = tar if isinstance(tar, str) else '(%s)' % ' '.join(tar)\n                generated += name + '[%d] ' % len(sources)\n            dispatch_rows.append(('Generated', generated[:-1] if generated else 'none'))\n        else:\n            dispatch_rows.append(('Generated', ''))\n            for tar in self.feature_sorted(target_sources):\n                sources = target_sources[tar]\n                pretty_name = tar if isinstance(tar, str) else '(%s)' % ' '.join(tar)\n                flags = ' '.join(self.feature_flags(tar))\n                implies = ' '.join(self.feature_sorted(self.feature_implies(tar)))\n                detect = ' '.join(self.feature_detect(tar))\n                extra_checks = []\n                for name in (tar,) if isinstance(tar, str) else tar:\n                    extra_checks += self.feature_extra_checks(name)\n                extra_checks = ' '.join(extra_checks) if extra_checks else 'none'\n                dispatch_rows.append(('', ''))\n                dispatch_rows.append((pretty_name, implies))\n                dispatch_rows.append(('Flags', flags))\n                dispatch_rows.append(('Extra checks', extra_checks))\n                dispatch_rows.append(('Detect', detect))\n                for src in sources:\n                    dispatch_rows.append(('', src))\n        text = []\n        secs_len = [len(secs) for (secs, _) in report]\n        cols_len = [len(col) for (_, rows) in report for (col, _) in rows]\n        tab = ' ' * 2\n        pad = max(max(secs_len), max(cols_len))\n        for (sec, rows) in report:\n            if not sec:\n                text.append('')\n                continue\n            sec += ' ' * (pad - len(sec))\n            text.append(sec + tab + ': ')\n            for (col, val) in rows:\n                col += ' ' * (pad - len(col))\n                text.append(tab + col + ': ' + val)\n        return '\\n'.join(text)\n\n    def __init__(self, ccompiler, cpu_baseline='min', cpu_dispatch='max', cache_path=None):\n        _Config.__init__(self)\n        _Distutils.__init__(self, ccompiler)\n        _Cache.__init__(self, cache_path, self.dist_info(), cpu_baseline, cpu_dispatch)\n        _CCompiler.__init__(self)\n        _Feature.__init__(self)\n        if not self.cc_noopt and self.cc_has_native:\n            self.dist_log(\"native flag is specified through environment variables. force cpu-baseline='native'\")\n            cpu_baseline = 'native'\n        _Parse.__init__(self, cpu_baseline, cpu_dispatch)\n        self._requested_baseline = cpu_baseline\n        self._requested_dispatch = cpu_dispatch\n        self.sources_status = getattr(self, 'sources_status', {})\n        self.cache_private.add('sources_status')\n        self.hit_cache = hasattr(self, 'hit_cache')\n\n    def cpu_baseline_flags(self):\n        \"\"\"\n        Returns a list of final CPU baseline compiler flags\n        \"\"\"\n        return self.parse_baseline_flags\n\n    def cpu_dispatch_names(self):\n        \"\"\"\n        return a list of final CPU dispatch feature names\n        \"\"\"\n        return self.parse_dispatch_names\n\n    def try_dispatch(self, sources, src_dir=None, ccompiler=None, **kwargs):\n        \"\"\"\n        Compile one or more dispatch-able sources and generates object files,\n        also generates abstract C config headers and macros that\n        used later for the final runtime dispatching process.\n\n        The mechanism behind it is to takes each source file that specified\n        in 'sources' and branching it into several files depend on\n        special configuration statements that must be declared in the\n        top of each source which contains targeted CPU features,\n        then it compiles every branched source with the proper compiler flags.\n\n        Parameters\n        ----------\n        sources : list\n            Must be a list of dispatch-able sources file paths,\n            and configuration statements must be declared inside\n            each file.\n\n        src_dir : str\n            Path of parent directory for the generated headers and wrapped sources.\n            If None(default) the files will generated in-place.\n\n        ccompiler : CCompiler\n            Distutils `CCompiler` instance to be used for compilation.\n            If None (default), the provided instance during the initialization\n            will be used instead.\n\n        **kwargs : any\n            Arguments to pass on to the `CCompiler.compile()`\n\n        Returns\n        -------\n        list : generated object files\n\n        Raises\n        ------\n        CompileError\n            Raises by `CCompiler.compile()` on compiling failure.\n        DistutilsError\n            Some errors during checking the sanity of configuration statements.\n\n        See Also\n        --------\n        parse_targets :\n            Parsing the configuration statements of dispatch-able sources.\n        \"\"\"\n        to_compile = {}\n        baseline_flags = self.cpu_baseline_flags()\n        include_dirs = kwargs.setdefault('include_dirs', [])\n        for src in sources:\n            output_dir = os.path.dirname(src)\n            if src_dir:\n                if not output_dir.startswith(src_dir):\n                    output_dir = os.path.join(src_dir, output_dir)\n                if output_dir not in include_dirs:\n                    include_dirs.append(output_dir)\n            (has_baseline, targets, extra_flags) = self.parse_targets(src)\n            nochange = self._generate_config(output_dir, src, targets, has_baseline)\n            for tar in targets:\n                tar_src = self._wrap_target(output_dir, src, tar, nochange=nochange)\n                flags = tuple(extra_flags + self.feature_flags(tar))\n                to_compile.setdefault(flags, []).append(tar_src)\n            if has_baseline:\n                flags = tuple(extra_flags + baseline_flags)\n                to_compile.setdefault(flags, []).append(src)\n            self.sources_status[src] = (has_baseline, targets)\n        objects = []\n        for (flags, srcs) in to_compile.items():\n            objects += self.dist_compile(srcs, list(flags), ccompiler=ccompiler, **kwargs)\n        return objects", "class_fn": true, "question_id": "numpy/numpy.distutils.ccompiler_opt/CCompilerOpt", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/ccompiler_opt.py", "fn_id": "", "content": "class _Config:\n    \"\"\"An abstract class holds all configurable attributes of `CCompilerOpt`,\n    these class attributes can be used to change the default behavior\n    of `CCompilerOpt` in order to fit other requirements.\n\n    Attributes\n    ----------\n    conf_nocache : bool\n        Set True to disable memory and file cache.\n        Default is False.\n\n    conf_noopt : bool\n        Set True to forces the optimization to be disabled,\n        in this case `CCompilerOpt` tends to generate all\n        expected headers in order to 'not' break the build.\n        Default is False.\n\n    conf_cache_factors : list\n        Add extra factors to the primary caching factors. The caching factors\n        are utilized to determine if there are changes had happened that\n        requires to discard the cache and re-updating it. The primary factors\n        are the arguments of `CCompilerOpt` and `CCompiler`'s properties(type, flags, etc).\n        Default is list of two items, containing the time of last modification\n        of `ccompiler_opt` and value of attribute \"conf_noopt\"\n\n    conf_tmp_path : str,\n        The path of temporary directory. Default is auto-created\n        temporary directory via ``tempfile.mkdtemp()``.\n\n    conf_check_path : str\n        The path of testing files. Each added CPU feature must have a\n        **C** source file contains at least one intrinsic or instruction that\n        related to this feature, so it can be tested against the compiler.\n        Default is ``./distutils/checks``.\n\n    conf_target_groups : dict\n        Extra tokens that can be reached from dispatch-able sources through\n        the special mark ``@targets``. Default is an empty dictionary.\n\n        **Notes**:\n            - case-insensitive for tokens and group names\n            - sign '#' must stick in the begin of group name and only within ``@targets``\n\n        **Example**:\n            .. code-block:: console\n\n                $ \"@targets #avx_group other_tokens\" > group_inside.c\n\n            >>> CCompilerOpt.conf_target_groups[\"avx_group\"] = \\\\\n            \"$werror $maxopt avx2 avx512f avx512_skx\"\n            >>> cco = CCompilerOpt(cc_instance)\n            >>> cco.try_dispatch([\"group_inside.c\"])\n\n    conf_c_prefix : str\n        The prefix of public C definitions. Default is ``\"NPY_\"``.\n\n    conf_c_prefix_ : str\n        The prefix of internal C definitions. Default is ``\"NPY__\"``.\n\n    conf_cc_flags : dict\n        Nested dictionaries defining several compiler flags\n        that linked to some major functions, the main key\n        represent the compiler name and sub-keys represent\n        flags names. Default is already covers all supported\n        **C** compilers.\n\n        Sub-keys explained as follows:\n\n        \"native\": str or None\n            used by argument option `native`, to detect the current\n            machine support via the compiler.\n        \"werror\": str or None\n            utilized to treat warning as errors during testing CPU features\n            against the compiler and also for target's policy `$werror`\n            via dispatch-able sources.\n        \"maxopt\": str or None\n            utilized for target's policy '$maxopt' and the value should\n            contains the maximum acceptable optimization by the compiler.\n            e.g. in gcc `'-O3'`\n\n        **Notes**:\n            * case-sensitive for compiler names and flags\n            * use space to separate multiple flags\n            * any flag will tested against the compiler and it will skipped\n              if it's not applicable.\n\n    conf_min_features : dict\n        A dictionary defines the used CPU features for\n        argument option `'min'`, the key represent the CPU architecture\n        name e.g. `'x86'`. Default values provide the best effort\n        on wide range of users platforms.\n\n        **Note**: case-sensitive for architecture names.\n\n    conf_features : dict\n        Nested dictionaries used for identifying the CPU features.\n        the primary key is represented as a feature name or group name\n        that gathers several features. Default values covers all\n        supported features but without the major options like \"flags\",\n        these undefined options handle it by method `conf_features_partial()`.\n        Default value is covers almost all CPU features for *X86*, *IBM/Power64*\n        and *ARM 7/8*.\n\n        Sub-keys explained as follows:\n\n        \"implies\" : str or list, optional,\n            List of CPU feature names to be implied by it,\n            the feature name must be defined within `conf_features`.\n            Default is None.\n\n        \"flags\": str or list, optional\n            List of compiler flags. Default is None.\n\n        \"detect\": str or list, optional\n            List of CPU feature names that required to be detected\n            in runtime. By default, its the feature name or features\n            in \"group\" if its specified.\n\n        \"implies_detect\": bool, optional\n            If True, all \"detect\" of implied features will be combined.\n            Default is True. see `feature_detect()`.\n\n        \"group\": str or list, optional\n            Same as \"implies\" but doesn't require the feature name to be\n            defined within `conf_features`.\n\n        \"interest\": int, required\n            a key for sorting CPU features\n\n        \"headers\": str or list, optional\n            intrinsics C header file\n\n        \"disable\": str, optional\n            force disable feature, the string value should contains the\n            reason of disabling.\n\n        \"autovec\": bool or None, optional\n            True or False to declare that CPU feature can be auto-vectorized\n            by the compiler.\n            By default(None), treated as True if the feature contains at\n            least one applicable flag. see `feature_can_autovec()`\n\n        \"extra_checks\": str or list, optional\n            Extra test case names for the CPU feature that need to be tested\n            against the compiler.\n\n            Each test case must have a C file named ``extra_xxxx.c``, where\n            ``xxxx`` is the case name in lower case, under 'conf_check_path'.\n            It should contain at least one intrinsic or function related to the test case.\n\n            If the compiler able to successfully compile the C file then `CCompilerOpt`\n            will add a C ``#define`` for it into the main dispatch header, e.g.\n            ``#define {conf_c_prefix}_XXXX`` where ``XXXX`` is the case name in upper case.\n\n        **NOTES**:\n            * space can be used as separator with options that supports \"str or list\"\n            * case-sensitive for all values and feature name must be in upper-case.\n            * if flags aren't applicable, its will skipped rather than disable the\n              CPU feature\n            * the CPU feature will disabled if the compiler fail to compile\n              the test file\n    \"\"\"\n    conf_nocache = False\n    conf_noopt = False\n    conf_cache_factors = None\n    conf_tmp_path = None\n    conf_check_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'checks')\n    conf_target_groups = {}\n    conf_c_prefix = 'NPY_'\n    conf_c_prefix_ = 'NPY__'\n    conf_cc_flags = dict(gcc=dict(native='-march=native', opt='-O3', werror='-Werror'), clang=dict(native='-march=native', opt='-O3', werror='-Werror=switch -Werror'), icc=dict(native='-xHost', opt='-O3', werror='-Werror'), iccw=dict(native='/QxHost', opt='/O3', werror='/Werror'), msvc=dict(native=None, opt='/O2', werror='/WX'), fcc=dict(native='-mcpu=a64fx', opt=None, werror=None))\n    conf_min_features = dict(x86='SSE SSE2', x64='SSE SSE2 SSE3', ppc64='', ppc64le='VSX VSX2', s390x='', armhf='', aarch64='NEON NEON_FP16 NEON_VFPV4 ASIMD')\n    conf_features = dict(SSE=dict(interest=1, headers='xmmintrin.h', implies='SSE2'), SSE2=dict(interest=2, implies='SSE', headers='emmintrin.h'), SSE3=dict(interest=3, implies='SSE2', headers='pmmintrin.h'), SSSE3=dict(interest=4, implies='SSE3', headers='tmmintrin.h'), SSE41=dict(interest=5, implies='SSSE3', headers='smmintrin.h'), POPCNT=dict(interest=6, implies='SSE41', headers='popcntintrin.h'), SSE42=dict(interest=7, implies='POPCNT'), AVX=dict(interest=8, implies='SSE42', headers='immintrin.h', implies_detect=False), XOP=dict(interest=9, implies='AVX', headers='x86intrin.h'), FMA4=dict(interest=10, implies='AVX', headers='x86intrin.h'), F16C=dict(interest=11, implies='AVX'), FMA3=dict(interest=12, implies='F16C'), AVX2=dict(interest=13, implies='F16C'), AVX512F=dict(interest=20, implies='FMA3 AVX2', implies_detect=False, extra_checks='AVX512F_REDUCE'), AVX512CD=dict(interest=21, implies='AVX512F'), AVX512_KNL=dict(interest=40, implies='AVX512CD', group='AVX512ER AVX512PF', detect='AVX512_KNL', implies_detect=False), AVX512_KNM=dict(interest=41, implies='AVX512_KNL', group='AVX5124FMAPS AVX5124VNNIW AVX512VPOPCNTDQ', detect='AVX512_KNM', implies_detect=False), AVX512_SKX=dict(interest=42, implies='AVX512CD', group='AVX512VL AVX512BW AVX512DQ', detect='AVX512_SKX', implies_detect=False, extra_checks='AVX512BW_MASK AVX512DQ_MASK'), AVX512_CLX=dict(interest=43, implies='AVX512_SKX', group='AVX512VNNI', detect='AVX512_CLX'), AVX512_CNL=dict(interest=44, implies='AVX512_SKX', group='AVX512IFMA AVX512VBMI', detect='AVX512_CNL', implies_detect=False), AVX512_ICL=dict(interest=45, implies='AVX512_CLX AVX512_CNL', group='AVX512VBMI2 AVX512BITALG AVX512VPOPCNTDQ', detect='AVX512_ICL', implies_detect=False), AVX512_SPR=dict(interest=46, implies='AVX512_ICL', group='AVX512FP16', detect='AVX512_SPR', implies_detect=False), VSX=dict(interest=1, headers='altivec.h', extra_checks='VSX_ASM'), VSX2=dict(interest=2, implies='VSX', implies_detect=False), VSX3=dict(interest=3, implies='VSX2', implies_detect=False, extra_checks='VSX3_HALF_DOUBLE'), VSX4=dict(interest=4, implies='VSX3', implies_detect=False, extra_checks='VSX4_MMA'), VX=dict(interest=1, headers='vecintrin.h'), VXE=dict(interest=2, implies='VX', implies_detect=False), VXE2=dict(interest=3, implies='VXE', implies_detect=False), NEON=dict(interest=1, headers='arm_neon.h'), NEON_FP16=dict(interest=2, implies='NEON'), NEON_VFPV4=dict(interest=3, implies='NEON_FP16'), ASIMD=dict(interest=4, implies='NEON_FP16 NEON_VFPV4', implies_detect=False), ASIMDHP=dict(interest=5, implies='ASIMD'), ASIMDDP=dict(interest=6, implies='ASIMD'), ASIMDFHM=dict(interest=7, implies='ASIMDHP'))\n\n    def conf_features_partial(self):\n        \"\"\"Return a dictionary of supported CPU features by the platform,\n        and accumulate the rest of undefined options in `conf_features`,\n        the returned dict has same rules and notes in\n        class attribute `conf_features`, also its override\n        any options that been set in 'conf_features'.\n        \"\"\"\n        if self.cc_noopt:\n            return {}\n        on_x86 = self.cc_on_x86 or self.cc_on_x64\n        is_unix = self.cc_is_gcc or self.cc_is_clang or self.cc_is_fcc\n        if on_x86 and is_unix:\n            return dict(SSE=dict(flags='-msse'), SSE2=dict(flags='-msse2'), SSE3=dict(flags='-msse3'), SSSE3=dict(flags='-mssse3'), SSE41=dict(flags='-msse4.1'), POPCNT=dict(flags='-mpopcnt'), SSE42=dict(flags='-msse4.2'), AVX=dict(flags='-mavx'), F16C=dict(flags='-mf16c'), XOP=dict(flags='-mxop'), FMA4=dict(flags='-mfma4'), FMA3=dict(flags='-mfma'), AVX2=dict(flags='-mavx2'), AVX512F=dict(flags='-mavx512f -mno-mmx'), AVX512CD=dict(flags='-mavx512cd'), AVX512_KNL=dict(flags='-mavx512er -mavx512pf'), AVX512_KNM=dict(flags='-mavx5124fmaps -mavx5124vnniw -mavx512vpopcntdq'), AVX512_SKX=dict(flags='-mavx512vl -mavx512bw -mavx512dq'), AVX512_CLX=dict(flags='-mavx512vnni'), AVX512_CNL=dict(flags='-mavx512ifma -mavx512vbmi'), AVX512_ICL=dict(flags='-mavx512vbmi2 -mavx512bitalg -mavx512vpopcntdq'), AVX512_SPR=dict(flags='-mavx512fp16'))\n        if on_x86 and self.cc_is_icc:\n            return dict(SSE=dict(flags='-msse'), SSE2=dict(flags='-msse2'), SSE3=dict(flags='-msse3'), SSSE3=dict(flags='-mssse3'), SSE41=dict(flags='-msse4.1'), POPCNT={}, SSE42=dict(flags='-msse4.2'), AVX=dict(flags='-mavx'), F16C={}, XOP=dict(disable=\"Intel Compiler doesn't support it\"), FMA4=dict(disable=\"Intel Compiler doesn't support it\"), FMA3=dict(implies='F16C AVX2', flags='-march=core-avx2'), AVX2=dict(implies='FMA3', flags='-march=core-avx2'), AVX512F=dict(implies='AVX2 AVX512CD', flags='-march=common-avx512'), AVX512CD=dict(implies='AVX2 AVX512F', flags='-march=common-avx512'), AVX512_KNL=dict(flags='-xKNL'), AVX512_KNM=dict(flags='-xKNM'), AVX512_SKX=dict(flags='-xSKYLAKE-AVX512'), AVX512_CLX=dict(flags='-xCASCADELAKE'), AVX512_CNL=dict(flags='-xCANNONLAKE'), AVX512_ICL=dict(flags='-xICELAKE-CLIENT'), AVX512_SPR=dict(disable='Not supported yet'))\n        if on_x86 and self.cc_is_iccw:\n            return dict(SSE=dict(flags='/arch:SSE'), SSE2=dict(flags='/arch:SSE2'), SSE3=dict(flags='/arch:SSE3'), SSSE3=dict(flags='/arch:SSSE3'), SSE41=dict(flags='/arch:SSE4.1'), POPCNT={}, SSE42=dict(flags='/arch:SSE4.2'), AVX=dict(flags='/arch:AVX'), F16C={}, XOP=dict(disable=\"Intel Compiler doesn't support it\"), FMA4=dict(disable=\"Intel Compiler doesn't support it\"), FMA3=dict(implies='F16C AVX2', flags='/arch:CORE-AVX2'), AVX2=dict(implies='FMA3', flags='/arch:CORE-AVX2'), AVX512F=dict(implies='AVX2 AVX512CD', flags='/Qx:COMMON-AVX512'), AVX512CD=dict(implies='AVX2 AVX512F', flags='/Qx:COMMON-AVX512'), AVX512_KNL=dict(flags='/Qx:KNL'), AVX512_KNM=dict(flags='/Qx:KNM'), AVX512_SKX=dict(flags='/Qx:SKYLAKE-AVX512'), AVX512_CLX=dict(flags='/Qx:CASCADELAKE'), AVX512_CNL=dict(flags='/Qx:CANNONLAKE'), AVX512_ICL=dict(flags='/Qx:ICELAKE-CLIENT'), AVX512_SPR=dict(disable='Not supported yet'))\n        if on_x86 and self.cc_is_msvc:\n            return dict(SSE=dict(flags='/arch:SSE') if self.cc_on_x86 else {}, SSE2=dict(flags='/arch:SSE2') if self.cc_on_x86 else {}, SSE3={}, SSSE3={}, SSE41={}, POPCNT=dict(headers='nmmintrin.h'), SSE42={}, AVX=dict(flags='/arch:AVX'), F16C={}, XOP=dict(headers='ammintrin.h'), FMA4=dict(headers='ammintrin.h'), FMA3=dict(implies='F16C AVX2', flags='/arch:AVX2'), AVX2=dict(implies='F16C FMA3', flags='/arch:AVX2'), AVX512F=dict(implies='AVX2 AVX512CD AVX512_SKX', flags='/arch:AVX512'), AVX512CD=dict(implies='AVX512F AVX512_SKX', flags='/arch:AVX512'), AVX512_KNL=dict(disable=\"MSVC compiler doesn't support it\"), AVX512_KNM=dict(disable=\"MSVC compiler doesn't support it\"), AVX512_SKX=dict(flags='/arch:AVX512'), AVX512_CLX={}, AVX512_CNL={}, AVX512_ICL={}, AVX512_SPR=dict(disable=\"MSVC compiler doesn't support it\"))\n        on_power = self.cc_on_ppc64le or self.cc_on_ppc64\n        if on_power:\n            partial = dict(VSX=dict(implies='VSX2' if self.cc_on_ppc64le else '', flags='-mvsx'), VSX2=dict(flags='-mcpu=power8', implies_detect=False), VSX3=dict(flags='-mcpu=power9 -mtune=power9', implies_detect=False), VSX4=dict(flags='-mcpu=power10 -mtune=power10', implies_detect=False))\n            if self.cc_is_clang:\n                partial['VSX']['flags'] = '-maltivec -mvsx'\n                partial['VSX2']['flags'] = '-mcpu=power8'\n                partial['VSX3']['flags'] = '-mcpu=power9'\n                partial['VSX4']['flags'] = '-mcpu=power10'\n            return partial\n        on_zarch = self.cc_on_s390x\n        if on_zarch:\n            partial = dict(VX=dict(flags='-march=arch11 -mzvector'), VXE=dict(flags='-march=arch12', implies_detect=False), VXE2=dict(flags='-march=arch13', implies_detect=False))\n            return partial\n        if self.cc_on_aarch64 and is_unix:\n            return dict(NEON=dict(implies='NEON_FP16 NEON_VFPV4 ASIMD', autovec=True), NEON_FP16=dict(implies='NEON NEON_VFPV4 ASIMD', autovec=True), NEON_VFPV4=dict(implies='NEON NEON_FP16 ASIMD', autovec=True), ASIMD=dict(implies='NEON NEON_FP16 NEON_VFPV4', autovec=True), ASIMDHP=dict(flags='-march=armv8.2-a+fp16'), ASIMDDP=dict(flags='-march=armv8.2-a+dotprod'), ASIMDFHM=dict(flags='-march=armv8.2-a+fp16fml'))\n        if self.cc_on_armhf and is_unix:\n            return dict(NEON=dict(flags='-mfpu=neon'), NEON_FP16=dict(flags='-mfpu=neon-fp16 -mfp16-format=ieee'), NEON_VFPV4=dict(flags='-mfpu=neon-vfpv4'), ASIMD=dict(flags='-mfpu=neon-fp-armv8 -march=armv8-a+simd'), ASIMDHP=dict(flags='-march=armv8.2-a+fp16'), ASIMDDP=dict(flags='-march=armv8.2-a+dotprod'), ASIMDFHM=dict(flags='-march=armv8.2-a+fp16fml'))\n        return {}\n\n    def __init__(self):\n        if self.conf_tmp_path is None:\n            import shutil\n            import tempfile\n            tmp = tempfile.mkdtemp()\n\n            def rm_temp():\n                try:\n                    shutil.rmtree(tmp)\n                except OSError:\n                    pass\n            atexit.register(rm_temp)\n            self.conf_tmp_path = tmp\n        if self.conf_cache_factors is None:\n            self.conf_cache_factors = [os.path.getmtime(__file__), self.conf_nocache]", "class_fn": true, "question_id": "numpy/numpy.distutils.ccompiler_opt/_Config", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/ccompiler_opt.py", "fn_id": "", "content": "class _Parse:\n    \"\"\"A helper class that parsing main arguments of `CCompilerOpt`,\n    also parsing configuration statements in dispatch-able sources.\n\n    Parameters\n    ----------\n    cpu_baseline : str or None\n        minimal set of required CPU features or special options.\n\n    cpu_dispatch : str or None\n        dispatched set of additional CPU features or special options.\n\n    Special options can be:\n        - **MIN**: Enables the minimum CPU features that utilized via `_Config.conf_min_features`\n        - **MAX**: Enables all supported CPU features by the Compiler and platform.\n        - **NATIVE**: Enables all CPU features that supported by the current machine.\n        - **NONE**: Enables nothing\n        - **Operand +/-**: remove or add features, useful with options **MAX**, **MIN** and **NATIVE**.\n            NOTE: operand + is only added for nominal reason.\n\n    NOTES:\n        - Case-insensitive among all CPU features and special options.\n        - Comma or space can be used as a separator.\n        - If the CPU feature is not supported by the user platform or compiler,\n          it will be skipped rather than raising a fatal error.\n        - Any specified CPU features to 'cpu_dispatch' will be skipped if its part of CPU baseline features\n        - 'cpu_baseline' force enables implied features.\n\n    Attributes\n    ----------\n    parse_baseline_names : list\n        Final CPU baseline's feature names(sorted from low to high)\n    parse_baseline_flags : list\n        Compiler flags of baseline features\n    parse_dispatch_names : list\n        Final CPU dispatch-able feature names(sorted from low to high)\n    parse_target_groups : dict\n        Dictionary containing initialized target groups that configured\n        through class attribute `conf_target_groups`.\n\n        The key is represent the group name and value is a tuple\n        contains three items :\n            - bool, True if group has the 'baseline' option.\n            - list, list of CPU features.\n            - list, list of extra compiler flags.\n\n    \"\"\"\n\n    def __init__(self, cpu_baseline, cpu_dispatch):\n        self._parse_policies = dict(KEEP_BASELINE=(None, self._parse_policy_not_keepbase, []), KEEP_SORT=(self._parse_policy_keepsort, self._parse_policy_not_keepsort, []), MAXOPT=(self._parse_policy_maxopt, None, []), WERROR=(self._parse_policy_werror, None, []), AUTOVEC=(self._parse_policy_autovec, None, ['MAXOPT']))\n        if hasattr(self, 'parse_is_cached'):\n            return\n        self.parse_baseline_names = []\n        self.parse_baseline_flags = []\n        self.parse_dispatch_names = []\n        self.parse_target_groups = {}\n        if self.cc_noopt:\n            cpu_baseline = cpu_dispatch = None\n        self.dist_log('check requested baseline')\n        if cpu_baseline is not None:\n            cpu_baseline = self._parse_arg_features('cpu_baseline', cpu_baseline)\n            baseline_names = self.feature_names(cpu_baseline)\n            self.parse_baseline_flags = self.feature_flags(baseline_names)\n            self.parse_baseline_names = self.feature_sorted(self.feature_implies_c(baseline_names))\n        self.dist_log('check requested dispatch-able features')\n        if cpu_dispatch is not None:\n            cpu_dispatch_ = self._parse_arg_features('cpu_dispatch', cpu_dispatch)\n            cpu_dispatch = {f for f in cpu_dispatch_ if f not in self.parse_baseline_names}\n            conflict_baseline = cpu_dispatch_.difference(cpu_dispatch)\n            self.parse_dispatch_names = self.feature_sorted(self.feature_names(cpu_dispatch))\n            if len(conflict_baseline) > 0:\n                self.dist_log('skip features', conflict_baseline, 'since its part of baseline')\n        self.dist_log('initialize targets groups')\n        for (group_name, tokens) in self.conf_target_groups.items():\n            self.dist_log('parse target group', group_name)\n            GROUP_NAME = group_name.upper()\n            if not tokens or not tokens.strip():\n                self.parse_target_groups[GROUP_NAME] = (False, [], [])\n                continue\n            (has_baseline, features, extra_flags) = self._parse_target_tokens(tokens)\n            self.parse_target_groups[GROUP_NAME] = (has_baseline, features, extra_flags)\n        self.parse_is_cached = True\n\n    def parse_targets(self, source):\n        \"\"\"\n        Fetch and parse configuration statements that required for\n        defining the targeted CPU features, statements should be declared\n        in the top of source in between **C** comment and start\n        with a special mark **@targets**.\n\n        Configuration statements are sort of keywords representing\n        CPU features names, group of statements and policies, combined\n        together to determine the required optimization.\n\n        Parameters\n        ----------\n        source : str\n            the path of **C** source file.\n\n        Returns\n        -------\n        - bool, True if group has the 'baseline' option\n        - list, list of CPU features\n        - list, list of extra compiler flags\n        \"\"\"\n        self.dist_log(\"looking for '@targets' inside -> \", source)\n        with open(source) as fd:\n            tokens = ''\n            max_to_reach = 1000\n            start_with = '@targets'\n            start_pos = -1\n            end_with = '*/'\n            end_pos = -1\n            for (current_line, line) in enumerate(fd):\n                if current_line == max_to_reach:\n                    self.dist_fatal('reached the max of lines')\n                    break\n                if start_pos == -1:\n                    start_pos = line.find(start_with)\n                    if start_pos == -1:\n                        continue\n                    start_pos += len(start_with)\n                tokens += line\n                end_pos = line.find(end_with)\n                if end_pos != -1:\n                    end_pos += len(tokens) - len(line)\n                    break\n        if start_pos == -1:\n            self.dist_fatal(\"expected to find '%s' within a C comment\" % start_with)\n        if end_pos == -1:\n            self.dist_fatal(\"expected to end with '%s'\" % end_with)\n        tokens = tokens[start_pos:end_pos]\n        return self._parse_target_tokens(tokens)\n    _parse_regex_arg = re.compile('\\\\s|,|([+-])')\n\n    def _parse_arg_features(self, arg_name, req_features):\n        if not isinstance(req_features, str):\n            self.dist_fatal(\"expected a string in '%s'\" % arg_name)\n        final_features = set()\n        tokens = list(filter(None, re.split(self._parse_regex_arg, req_features)))\n        append = True\n        for tok in tokens:\n            if tok[0] in ('#', '$'):\n                self.dist_fatal(arg_name, \"target groups and policies aren't allowed from arguments, only from dispatch-able sources\")\n            if tok == '+':\n                append = True\n                continue\n            if tok == '-':\n                append = False\n                continue\n            TOK = tok.upper()\n            features_to = set()\n            if TOK == 'NONE':\n                pass\n            elif TOK == 'NATIVE':\n                native = self.cc_flags['native']\n                if not native:\n                    self.dist_fatal(arg_name, \"native option isn't supported by the compiler\")\n                features_to = self.feature_names(force_flags=native, macros=[('DETECT_FEATURES', 1)])\n            elif TOK == 'MAX':\n                features_to = self.feature_supported.keys()\n            elif TOK == 'MIN':\n                features_to = self.feature_min\n            elif TOK in self.feature_supported:\n                features_to.add(TOK)\n            elif not self.feature_is_exist(TOK):\n                self.dist_fatal(arg_name, \", '%s' isn't a known feature or option\" % tok)\n            if append:\n                final_features = final_features.union(features_to)\n            else:\n                final_features = final_features.difference(features_to)\n            append = True\n        return final_features\n    _parse_regex_target = re.compile('\\\\s|[*,/]|([()])')\n\n    def _parse_target_tokens(self, tokens):\n        assert isinstance(tokens, str)\n        final_targets = []\n        extra_flags = []\n        has_baseline = False\n        skipped = set()\n        policies = set()\n        multi_target = None\n        tokens = list(filter(None, re.split(self._parse_regex_target, tokens)))\n        if not tokens:\n            self.dist_fatal('expected one token at least')\n        for tok in tokens:\n            TOK = tok.upper()\n            ch = tok[0]\n            if ch in ('+', '-'):\n                self.dist_fatal(\"+/- are 'not' allowed from target's groups or @targets, only from cpu_baseline and cpu_dispatch parms\")\n            elif ch == '$':\n                if multi_target is not None:\n                    self.dist_fatal(\"policies aren't allowed inside multi-target '()', only CPU features\")\n                policies.add(self._parse_token_policy(TOK))\n            elif ch == '#':\n                if multi_target is not None:\n                    self.dist_fatal(\"target groups aren't allowed inside multi-target '()', only CPU features\")\n                (has_baseline, final_targets, extra_flags) = self._parse_token_group(TOK, has_baseline, final_targets, extra_flags)\n            elif ch == '(':\n                if multi_target is not None:\n                    self.dist_fatal(\"unclosed multi-target, missing ')'\")\n                multi_target = set()\n            elif ch == ')':\n                if multi_target is None:\n                    self.dist_fatal(\"multi-target opener '(' wasn't found\")\n                targets = self._parse_multi_target(multi_target)\n                if targets is None:\n                    skipped.add(tuple(multi_target))\n                else:\n                    if len(targets) == 1:\n                        targets = targets[0]\n                    if targets and targets not in final_targets:\n                        final_targets.append(targets)\n                multi_target = None\n            else:\n                if TOK == 'BASELINE':\n                    if multi_target is not None:\n                        self.dist_fatal(\"baseline isn't allowed inside multi-target '()'\")\n                    has_baseline = True\n                    continue\n                if multi_target is not None:\n                    multi_target.add(TOK)\n                    continue\n                if not self.feature_is_exist(TOK):\n                    self.dist_fatal(\"invalid target name '%s'\" % TOK)\n                is_enabled = TOK in self.parse_baseline_names or TOK in self.parse_dispatch_names\n                if is_enabled:\n                    if TOK not in final_targets:\n                        final_targets.append(TOK)\n                    continue\n                skipped.add(TOK)\n        if multi_target is not None:\n            self.dist_fatal(\"unclosed multi-target, missing ')'\")\n        if skipped:\n            self.dist_log('skip targets', skipped, 'not part of baseline or dispatch-able features')\n        final_targets = self.feature_untied(final_targets)\n        for p in list(policies):\n            (_, _, deps) = self._parse_policies[p]\n            for d in deps:\n                if d in policies:\n                    continue\n                self.dist_log(\"policy '%s' force enables '%s'\" % (p, d))\n                policies.add(d)\n        for (p, (have, nhave, _)) in self._parse_policies.items():\n            func = None\n            if p in policies:\n                func = have\n                self.dist_log(\"policy '%s' is ON\" % p)\n            else:\n                func = nhave\n            if not func:\n                continue\n            (has_baseline, final_targets, extra_flags) = func(has_baseline, final_targets, extra_flags)\n        return (has_baseline, final_targets, extra_flags)\n\n    def _parse_token_policy(self, token):\n        \"\"\"validate policy token\"\"\"\n        if len(token) <= 1 or token[-1:] == token[0]:\n            self.dist_fatal(\"'$' must stuck in the begin of policy name\")\n        token = token[1:]\n        if token not in self._parse_policies:\n            self.dist_fatal(\"'%s' is an invalid policy name, available policies are\" % token, self._parse_policies.keys())\n        return token\n\n    def _parse_token_group(self, token, has_baseline, final_targets, extra_flags):\n        \"\"\"validate group token\"\"\"\n        if len(token) <= 1 or token[-1:] == token[0]:\n            self.dist_fatal(\"'#' must stuck in the begin of group name\")\n        token = token[1:]\n        (ghas_baseline, gtargets, gextra_flags) = self.parse_target_groups.get(token, (False, None, []))\n        if gtargets is None:\n            self.dist_fatal(\"'%s' is an invalid target group name, \" % token + 'available target groups are', self.parse_target_groups.keys())\n        if ghas_baseline:\n            has_baseline = True\n        final_targets += [f for f in gtargets if f not in final_targets]\n        extra_flags += [f for f in gextra_flags if f not in extra_flags]\n        return (has_baseline, final_targets, extra_flags)\n\n    def _parse_multi_target(self, targets):\n        \"\"\"validate multi targets that defined between parentheses()\"\"\"\n        if not targets:\n            self.dist_fatal(\"empty multi-target '()'\")\n        if not all([self.feature_is_exist(tar) for tar in targets]):\n            self.dist_fatal('invalid target name in multi-target', targets)\n        if not all([tar in self.parse_baseline_names or tar in self.parse_dispatch_names for tar in targets]):\n            return None\n        targets = self.feature_ahead(targets)\n        if not targets:\n            return None\n        targets = self.feature_sorted(targets)\n        targets = tuple(targets)\n        return targets\n\n    def _parse_policy_not_keepbase(self, has_baseline, final_targets, extra_flags):\n        \"\"\"skip all baseline features\"\"\"\n        skipped = []\n        for tar in final_targets[:]:\n            is_base = False\n            if isinstance(tar, str):\n                is_base = tar in self.parse_baseline_names\n            else:\n                is_base = all([f in self.parse_baseline_names for f in tar])\n            if is_base:\n                skipped.append(tar)\n                final_targets.remove(tar)\n        if skipped:\n            self.dist_log('skip baseline features', skipped)\n        return (has_baseline, final_targets, extra_flags)\n\n    def _parse_policy_keepsort(self, has_baseline, final_targets, extra_flags):\n        \"\"\"leave a notice that $keep_sort is on\"\"\"\n        self.dist_log(\"policy 'keep_sort' is on, dispatch-able targets\", final_targets, \"\\nare 'not' sorted depend on the highest interest butas specified in the dispatch-able source or the extra group\")\n        return (has_baseline, final_targets, extra_flags)\n\n    def _parse_policy_not_keepsort(self, has_baseline, final_targets, extra_flags):\n        \"\"\"sorted depend on the highest interest\"\"\"\n        final_targets = self.feature_sorted(final_targets, reverse=True)\n        return (has_baseline, final_targets, extra_flags)\n\n    def _parse_policy_maxopt(self, has_baseline, final_targets, extra_flags):\n        \"\"\"append the compiler optimization flags\"\"\"\n        if self.cc_has_debug:\n            self.dist_log(\"debug mode is detected, policy 'maxopt' is skipped.\")\n        elif self.cc_noopt:\n            self.dist_log(\"optimization is disabled, policy 'maxopt' is skipped.\")\n        else:\n            flags = self.cc_flags['opt']\n            if not flags:\n                self.dist_log(\"current compiler doesn't support optimization flags, policy 'maxopt' is skipped\", stderr=True)\n            else:\n                extra_flags += flags\n        return (has_baseline, final_targets, extra_flags)\n\n    def _parse_policy_werror(self, has_baseline, final_targets, extra_flags):\n        \"\"\"force warnings to treated as errors\"\"\"\n        flags = self.cc_flags['werror']\n        if not flags:\n            self.dist_log(\"current compiler doesn't support werror flags, warnings will 'not' treated as errors\", stderr=True)\n        else:\n            self.dist_log('compiler warnings are treated as errors')\n            extra_flags += flags\n        return (has_baseline, final_targets, extra_flags)\n\n    def _parse_policy_autovec(self, has_baseline, final_targets, extra_flags):\n        \"\"\"skip features that has no auto-vectorized support by compiler\"\"\"\n        skipped = []\n        for tar in final_targets[:]:\n            if isinstance(tar, str):\n                can = self.feature_can_autovec(tar)\n            else:\n                can = all([self.feature_can_autovec(t) for t in tar])\n            if not can:\n                final_targets.remove(tar)\n                skipped.append(tar)\n        if skipped:\n            self.dist_log('skip non auto-vectorized features', skipped)\n        return (has_baseline, final_targets, extra_flags)", "class_fn": true, "question_id": "numpy/numpy.distutils.ccompiler_opt/_Parse", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/build_ext.py", "fn_id": "", "content": "class build_ext(old_build_ext):\n    description = 'build C/C++/F extensions (compile/link to build directory)'\n    user_options = old_build_ext.user_options + [('fcompiler=', None, 'specify the Fortran compiler type'), ('parallel=', 'j', 'number of parallel jobs'), ('warn-error', None, 'turn all warnings into errors (-Werror)'), ('cpu-baseline=', None, 'specify a list of enabled baseline CPU optimizations'), ('cpu-dispatch=', None, 'specify a list of dispatched CPU optimizations'), ('disable-optimization', None, 'disable CPU optimized code(dispatch,simd,fast...)'), ('simd-test=', None, 'specify a list of CPU optimizations to be tested against NumPy SIMD interface')]\n    help_options = old_build_ext.help_options + [('help-fcompiler', None, 'list available Fortran compilers', show_fortran_compilers)]\n    boolean_options = old_build_ext.boolean_options + ['warn-error', 'disable-optimization']\n\n    def initialize_options(self):\n        old_build_ext.initialize_options(self)\n        self.fcompiler = None\n        self.parallel = None\n        self.warn_error = None\n        self.cpu_baseline = None\n        self.cpu_dispatch = None\n        self.disable_optimization = None\n        self.simd_test = None\n\n    def finalize_options(self):\n        if self.parallel:\n            try:\n                self.parallel = int(self.parallel)\n            except ValueError as e:\n                raise ValueError('--parallel/-j argument must be an integer') from e\n        if isinstance(self.include_dirs, str):\n            self.include_dirs = self.include_dirs.split(os.pathsep)\n        incl_dirs = self.include_dirs or []\n        if self.distribution.include_dirs is None:\n            self.distribution.include_dirs = []\n        self.include_dirs = self.distribution.include_dirs\n        self.include_dirs.extend(incl_dirs)\n        old_build_ext.finalize_options(self)\n        self.set_undefined_options('build', ('parallel', 'parallel'), ('warn_error', 'warn_error'), ('cpu_baseline', 'cpu_baseline'), ('cpu_dispatch', 'cpu_dispatch'), ('disable_optimization', 'disable_optimization'), ('simd_test', 'simd_test'))\n        CCompilerOpt.conf_target_groups['simd_test'] = self.simd_test\n\n    def run(self):\n        if not self.extensions:\n            return\n        self.run_command('build_src')\n        if self.distribution.has_c_libraries():\n            if self.inplace:\n                if self.distribution.have_run.get('build_clib'):\n                    log.warn('build_clib already run, it is too late to ensure in-place build of build_clib')\n                    build_clib = self.distribution.get_command_obj('build_clib')\n                else:\n                    build_clib = self.distribution.get_command_obj('build_clib')\n                    build_clib.inplace = 1\n                    build_clib.ensure_finalized()\n                    build_clib.run()\n                    self.distribution.have_run['build_clib'] = 1\n            else:\n                self.run_command('build_clib')\n                build_clib = self.get_finalized_command('build_clib')\n            self.library_dirs.append(build_clib.build_clib)\n        else:\n            build_clib = None\n        from distutils.ccompiler import new_compiler\n        from numpy.distutils.fcompiler import new_fcompiler\n        compiler_type = self.compiler\n        self.compiler = new_compiler(compiler=compiler_type, verbose=self.verbose, dry_run=self.dry_run, force=self.force)\n        self.compiler.customize(self.distribution)\n        self.compiler.customize_cmd(self)\n        if self.warn_error:\n            self.compiler.compiler.append('-Werror')\n            self.compiler.compiler_so.append('-Werror')\n        self.compiler.show_customization()\n        if not self.disable_optimization:\n            dispatch_hpath = os.path.join('numpy', 'distutils', 'include', 'npy_cpu_dispatch_config.h')\n            dispatch_hpath = os.path.join(self.get_finalized_command('build_src').build_src, dispatch_hpath)\n            opt_cache_path = os.path.abspath(os.path.join(self.build_temp, 'ccompiler_opt_cache_ext.py'))\n            if hasattr(self, 'compiler_opt'):\n                self.compiler_opt.cache_flush()\n            self.compiler_opt = new_ccompiler_opt(compiler=self.compiler, dispatch_hpath=dispatch_hpath, cpu_baseline=self.cpu_baseline, cpu_dispatch=self.cpu_dispatch, cache_path=opt_cache_path)\n\n            def report(copt):\n                log.info('\\n########### EXT COMPILER OPTIMIZATION ###########')\n                log.info(copt.report(full=True))\n            import atexit\n            atexit.register(report, self.compiler_opt)\n        self.extra_dll_dir = os.path.join(self.build_temp, '.libs')\n        if not os.path.isdir(self.extra_dll_dir):\n            os.makedirs(self.extra_dll_dir)\n        clibs = {}\n        if build_clib is not None:\n            for (libname, build_info) in build_clib.libraries or []:\n                if libname in clibs and clibs[libname] != build_info:\n                    log.warn('library %r defined more than once, overwriting build_info\\n%s... \\nwith\\n%s...' % (libname, repr(clibs[libname])[:300], repr(build_info)[:300]))\n                clibs[libname] = build_info\n        for (libname, build_info) in self.distribution.libraries or []:\n            if libname in clibs:\n                continue\n            clibs[libname] = build_info\n        all_languages = set()\n        for ext in self.extensions:\n            ext_languages = set()\n            c_libs = []\n            c_lib_dirs = []\n            macros = []\n            for libname in ext.libraries:\n                if libname in clibs:\n                    binfo = clibs[libname]\n                    c_libs += binfo.get('libraries', [])\n                    c_lib_dirs += binfo.get('library_dirs', [])\n                    for m in binfo.get('macros', []):\n                        if m not in macros:\n                            macros.append(m)\n                for l in clibs.get(libname, {}).get('source_languages', []):\n                    ext_languages.add(l)\n            if c_libs:\n                new_c_libs = ext.libraries + c_libs\n                log.info('updating extension %r libraries from %r to %r' % (ext.name, ext.libraries, new_c_libs))\n                ext.libraries = new_c_libs\n                ext.library_dirs = ext.library_dirs + c_lib_dirs\n            if macros:\n                log.info('extending extension %r defined_macros with %r' % (ext.name, macros))\n                ext.define_macros = ext.define_macros + macros\n            if has_f_sources(ext.sources):\n                ext_languages.add('f77')\n            if has_cxx_sources(ext.sources):\n                ext_languages.add('c++')\n            l = ext.language or self.compiler.detect_language(ext.sources)\n            if l:\n                ext_languages.add(l)\n            if 'c++' in ext_languages:\n                ext_language = 'c++'\n            else:\n                ext_language = 'c'\n            has_fortran = False\n            if 'f90' in ext_languages:\n                ext_language = 'f90'\n                has_fortran = True\n            elif 'f77' in ext_languages:\n                ext_language = 'f77'\n                has_fortran = True\n            if not ext.language or has_fortran:\n                if l and l != ext_language and ext.language:\n                    log.warn('resetting extension %r language from %r to %r.' % (ext.name, l, ext_language))\n            ext.language = ext_language\n            all_languages.update(ext_languages)\n        need_f90_compiler = 'f90' in all_languages\n        need_f77_compiler = 'f77' in all_languages\n        need_cxx_compiler = 'c++' in all_languages\n        if need_cxx_compiler:\n            self._cxx_compiler = new_compiler(compiler=compiler_type, verbose=self.verbose, dry_run=self.dry_run, force=self.force)\n            compiler = self._cxx_compiler\n            compiler.customize(self.distribution, need_cxx=need_cxx_compiler)\n            compiler.customize_cmd(self)\n            compiler.show_customization()\n            self._cxx_compiler = compiler.cxx_compiler()\n        else:\n            self._cxx_compiler = None\n        if need_f77_compiler:\n            ctype = self.fcompiler\n            self._f77_compiler = new_fcompiler(compiler=self.fcompiler, verbose=self.verbose, dry_run=self.dry_run, force=self.force, requiref90=False, c_compiler=self.compiler)\n            fcompiler = self._f77_compiler\n            if fcompiler:\n                ctype = fcompiler.compiler_type\n                fcompiler.customize(self.distribution)\n            if fcompiler and fcompiler.get_version():\n                fcompiler.customize_cmd(self)\n                fcompiler.show_customization()\n            else:\n                self.warn('f77_compiler=%s is not available.' % ctype)\n                self._f77_compiler = None\n        else:\n            self._f77_compiler = None\n        if need_f90_compiler:\n            ctype = self.fcompiler\n            self._f90_compiler = new_fcompiler(compiler=self.fcompiler, verbose=self.verbose, dry_run=self.dry_run, force=self.force, requiref90=True, c_compiler=self.compiler)\n            fcompiler = self._f90_compiler\n            if fcompiler:\n                ctype = fcompiler.compiler_type\n                fcompiler.customize(self.distribution)\n            if fcompiler and fcompiler.get_version():\n                fcompiler.customize_cmd(self)\n                fcompiler.show_customization()\n            else:\n                self.warn('f90_compiler=%s is not available.' % ctype)\n                self._f90_compiler = None\n        else:\n            self._f90_compiler = None\n        self.build_extensions()\n        pkg_roots = {self.get_ext_fullname(ext.name).split('.')[0] for ext in self.extensions}\n        for pkg_root in pkg_roots:\n            shared_lib_dir = os.path.join(pkg_root, '.libs')\n            if not self.inplace:\n                shared_lib_dir = os.path.join(self.build_lib, shared_lib_dir)\n            for fn in os.listdir(self.extra_dll_dir):\n                if not os.path.isdir(shared_lib_dir):\n                    os.makedirs(shared_lib_dir)\n                if not fn.lower().endswith('.dll'):\n                    continue\n                runtime_lib = os.path.join(self.extra_dll_dir, fn)\n                copy_file(runtime_lib, shared_lib_dir)\n\n    def swig_sources(self, sources, extensions=None):\n        return sources\n\n    def build_extension(self, ext):\n        sources = ext.sources\n        if sources is None or not is_sequence(sources):\n            raise DistutilsSetupError((\"in 'ext_modules' option (extension '%s'), \" + \"'sources' must be present and must be \" + 'a list of source filenames') % ext.name)\n        sources = list(sources)\n        if not sources:\n            return\n        fullname = self.get_ext_fullname(ext.name)\n        if self.inplace:\n            modpath = fullname.split('.')\n            package = '.'.join(modpath[0:-1])\n            base = modpath[-1]\n            build_py = self.get_finalized_command('build_py')\n            package_dir = build_py.get_package_dir(package)\n            ext_filename = os.path.join(package_dir, self.get_ext_filename(base))\n        else:\n            ext_filename = os.path.join(self.build_lib, self.get_ext_filename(fullname))\n        depends = sources + ext.depends\n        force_rebuild = self.force\n        if not self.disable_optimization and (not self.compiler_opt.is_cached()):\n            log.debug('Detected changes on compiler optimizations')\n            force_rebuild = True\n        if not (force_rebuild or newer_group(depends, ext_filename, 'newer')):\n            log.debug(\"skipping '%s' extension (up-to-date)\", ext.name)\n            return\n        else:\n            log.info(\"building '%s' extension\", ext.name)\n        extra_args = ext.extra_compile_args or []\n        extra_cflags = getattr(ext, 'extra_c_compile_args', None) or []\n        extra_cxxflags = getattr(ext, 'extra_cxx_compile_args', None) or []\n        macros = ext.define_macros[:]\n        for undef in ext.undef_macros:\n            macros.append((undef,))\n        (c_sources, cxx_sources, f_sources, fmodule_sources) = filter_sources(ext.sources)\n        if self.compiler.compiler_type == 'msvc':\n            if cxx_sources:\n                extra_args.append('/Zm1000')\n                extra_cflags += extra_cxxflags\n            c_sources += cxx_sources\n            cxx_sources = []\n        if ext.language == 'f90':\n            fcompiler = self._f90_compiler\n        elif ext.language == 'f77':\n            fcompiler = self._f77_compiler\n        else:\n            fcompiler = self._f90_compiler or self._f77_compiler\n        if fcompiler is not None:\n            fcompiler.extra_f77_compile_args = ext.extra_f77_compile_args or [] if hasattr(ext, 'extra_f77_compile_args') else []\n            fcompiler.extra_f90_compile_args = ext.extra_f90_compile_args or [] if hasattr(ext, 'extra_f90_compile_args') else []\n        cxx_compiler = self._cxx_compiler\n        if cxx_sources and cxx_compiler is None:\n            raise DistutilsError('extension %r has C++ sourcesbut no C++ compiler found' % ext.name)\n        if (f_sources or fmodule_sources) and fcompiler is None:\n            raise DistutilsError('extension %r has Fortran sources but no Fortran compiler found' % ext.name)\n        if ext.language in ['f77', 'f90'] and fcompiler is None:\n            self.warn('extension %r has Fortran libraries but no Fortran linker found, using default linker' % ext.name)\n        if ext.language == 'c++' and cxx_compiler is None:\n            self.warn('extension %r has C++ libraries but no C++ linker found, using default linker' % ext.name)\n        kws = {'depends': ext.depends}\n        output_dir = self.build_temp\n        include_dirs = ext.include_dirs + get_numpy_include_dirs()\n        copt_c_sources = []\n        copt_cxx_sources = []\n        copt_baseline_flags = []\n        copt_macros = []\n        if not self.disable_optimization:\n            bsrc_dir = self.get_finalized_command('build_src').build_src\n            dispatch_hpath = os.path.join('numpy', 'distutils', 'include')\n            dispatch_hpath = os.path.join(bsrc_dir, dispatch_hpath)\n            include_dirs.append(dispatch_hpath)\n            copt_build_src = bsrc_dir\n            for (_srcs, _dst, _ext) in (((c_sources,), copt_c_sources, ('.dispatch.c',)), ((c_sources, cxx_sources), copt_cxx_sources, ('.dispatch.cpp', '.dispatch.cxx'))):\n                for _src in _srcs:\n                    _dst += [_src.pop(_src.index(s)) for s in _src[:] if s.endswith(_ext)]\n            copt_baseline_flags = self.compiler_opt.cpu_baseline_flags()\n        else:\n            copt_macros.append(('NPY_DISABLE_OPTIMIZATION', 1))\n        c_objects = []\n        if copt_cxx_sources:\n            log.info('compiling C++ dispatch-able sources')\n            c_objects += self.compiler_opt.try_dispatch(copt_cxx_sources, output_dir=output_dir, src_dir=copt_build_src, macros=macros + copt_macros, include_dirs=include_dirs, debug=self.debug, extra_postargs=extra_args + extra_cxxflags, ccompiler=cxx_compiler, **kws)\n        if copt_c_sources:\n            log.info('compiling C dispatch-able sources')\n            c_objects += self.compiler_opt.try_dispatch(copt_c_sources, output_dir=output_dir, src_dir=copt_build_src, macros=macros + copt_macros, include_dirs=include_dirs, debug=self.debug, extra_postargs=extra_args + extra_cflags, **kws)\n        if c_sources:\n            log.info('compiling C sources')\n            c_objects += self.compiler.compile(c_sources, output_dir=output_dir, macros=macros + copt_macros, include_dirs=include_dirs, debug=self.debug, extra_postargs=extra_args + copt_baseline_flags + extra_cflags, **kws)\n        if cxx_sources:\n            log.info('compiling C++ sources')\n            c_objects += cxx_compiler.compile(cxx_sources, output_dir=output_dir, macros=macros + copt_macros, include_dirs=include_dirs, debug=self.debug, extra_postargs=extra_args + copt_baseline_flags + extra_cxxflags, **kws)\n        extra_postargs = []\n        f_objects = []\n        if fmodule_sources:\n            log.info('compiling Fortran 90 module sources')\n            module_dirs = ext.module_dirs[:]\n            module_build_dir = os.path.join(self.build_temp, os.path.dirname(self.get_ext_filename(fullname)))\n            self.mkpath(module_build_dir)\n            if fcompiler.module_dir_switch is None:\n                existing_modules = glob('*.mod')\n            extra_postargs += fcompiler.module_options(module_dirs, module_build_dir)\n            f_objects += fcompiler.compile(fmodule_sources, output_dir=self.build_temp, macros=macros, include_dirs=include_dirs, debug=self.debug, extra_postargs=extra_postargs, depends=ext.depends)\n            if fcompiler.module_dir_switch is None:\n                for f in glob('*.mod'):\n                    if f in existing_modules:\n                        continue\n                    t = os.path.join(module_build_dir, f)\n                    if os.path.abspath(f) == os.path.abspath(t):\n                        continue\n                    if os.path.isfile(t):\n                        os.remove(t)\n                    try:\n                        self.move_file(f, module_build_dir)\n                    except DistutilsFileError:\n                        log.warn('failed to move %r to %r' % (f, module_build_dir))\n        if f_sources:\n            log.info('compiling Fortran sources')\n            f_objects += fcompiler.compile(f_sources, output_dir=self.build_temp, macros=macros, include_dirs=include_dirs, debug=self.debug, extra_postargs=extra_postargs, depends=ext.depends)\n        if f_objects and (not fcompiler.can_ccompiler_link(self.compiler)):\n            unlinkable_fobjects = f_objects\n            objects = c_objects\n        else:\n            unlinkable_fobjects = []\n            objects = c_objects + f_objects\n        if ext.extra_objects:\n            objects.extend(ext.extra_objects)\n        extra_args = ext.extra_link_args or []\n        libraries = self.get_libraries(ext)[:]\n        library_dirs = ext.library_dirs[:]\n        linker = self.compiler.link_shared_object\n        if self.compiler.compiler_type in ('msvc', 'intelw', 'intelemw'):\n            self._libs_with_msvc_and_fortran(fcompiler, libraries, library_dirs)\n            if ext.runtime_library_dirs:\n                for d in ext.runtime_library_dirs:\n                    for f in glob(d + '/*.dll'):\n                        copy_file(f, self.extra_dll_dir)\n                ext.runtime_library_dirs = []\n        elif ext.language in ['f77', 'f90'] and fcompiler is not None:\n            linker = fcompiler.link_shared_object\n        if ext.language == 'c++' and cxx_compiler is not None:\n            linker = cxx_compiler.link_shared_object\n        if fcompiler is not None:\n            (objects, libraries) = self._process_unlinkable_fobjects(objects, libraries, fcompiler, library_dirs, unlinkable_fobjects)\n        linker(objects, ext_filename, libraries=libraries, library_dirs=library_dirs, runtime_library_dirs=ext.runtime_library_dirs, extra_postargs=extra_args, export_symbols=self.get_export_symbols(ext), debug=self.debug, build_temp=self.build_temp, target_lang=ext.language)\n\n    def _add_dummy_mingwex_sym(self, c_sources):\n        build_src = self.get_finalized_command('build_src').build_src\n        build_clib = self.get_finalized_command('build_clib').build_clib\n        objects = self.compiler.compile([os.path.join(build_src, 'gfortran_vs2003_hack.c')], output_dir=self.build_temp)\n        self.compiler.create_static_lib(objects, '_gfortran_workaround', output_dir=build_clib, debug=self.debug)\n\n    def _process_unlinkable_fobjects(self, objects, libraries, fcompiler, library_dirs, unlinkable_fobjects):\n        libraries = list(libraries)\n        objects = list(objects)\n        unlinkable_fobjects = list(unlinkable_fobjects)\n        for lib in libraries[:]:\n            for libdir in library_dirs:\n                fake_lib = os.path.join(libdir, lib + '.fobjects')\n                if os.path.isfile(fake_lib):\n                    libraries.remove(lib)\n                    with open(fake_lib) as f:\n                        unlinkable_fobjects.extend(f.read().splitlines())\n                    c_lib = os.path.join(libdir, lib + '.cobjects')\n                    with open(c_lib) as f:\n                        objects.extend(f.read().splitlines())\n        if unlinkable_fobjects:\n            fobjects = [os.path.abspath(obj) for obj in unlinkable_fobjects]\n            wrapped = fcompiler.wrap_unlinkable_objects(fobjects, output_dir=self.build_temp, extra_dll_dir=self.extra_dll_dir)\n            objects.extend(wrapped)\n        return (objects, libraries)\n\n    def _libs_with_msvc_and_fortran(self, fcompiler, c_libraries, c_library_dirs):\n        if fcompiler is None:\n            return\n        for libname in c_libraries:\n            if libname.startswith('msvc'):\n                continue\n            fileexists = False\n            for libdir in c_library_dirs or []:\n                libfile = os.path.join(libdir, '%s.lib' % libname)\n                if os.path.isfile(libfile):\n                    fileexists = True\n                    break\n            if fileexists:\n                continue\n            fileexists = False\n            for libdir in c_library_dirs:\n                libfile = os.path.join(libdir, 'lib%s.a' % libname)\n                if os.path.isfile(libfile):\n                    libfile2 = os.path.join(self.build_temp, libname + '.lib')\n                    copy_file(libfile, libfile2)\n                    if self.build_temp not in c_library_dirs:\n                        c_library_dirs.append(self.build_temp)\n                    fileexists = True\n                    break\n            if fileexists:\n                continue\n            log.warn('could not find library %r in directories %s' % (libname, c_library_dirs))\n        f_lib_dirs = []\n        for dir in fcompiler.library_dirs:\n            if dir.startswith('/usr/lib'):\n                try:\n                    dir = subprocess.check_output(['cygpath', '-w', dir])\n                except (OSError, subprocess.CalledProcessError):\n                    pass\n                else:\n                    dir = filepath_from_subprocess_output(dir)\n            f_lib_dirs.append(dir)\n        c_library_dirs.extend(f_lib_dirs)\n        for lib in fcompiler.libraries:\n            if not lib.startswith('msvc'):\n                c_libraries.append(lib)\n                p = combine_paths(f_lib_dirs, 'lib' + lib + '.a')\n                if p:\n                    dst_name = os.path.join(self.build_temp, lib + '.lib')\n                    if not os.path.isfile(dst_name):\n                        copy_file(p[0], dst_name)\n                    if self.build_temp not in c_library_dirs:\n                        c_library_dirs.append(self.build_temp)\n\n    def get_source_files(self):\n        self.check_extensions_list(self.extensions)\n        filenames = []\n        for ext in self.extensions:\n            filenames.extend(get_ext_source_files(ext))\n        return filenames\n\n    def get_outputs(self):\n        self.check_extensions_list(self.extensions)\n        outputs = []\n        for ext in self.extensions:\n            if not ext.sources:\n                continue\n            fullname = self.get_ext_fullname(ext.name)\n            outputs.append(os.path.join(self.build_lib, self.get_ext_filename(fullname)))\n        return outputs", "class_fn": true, "question_id": "numpy/numpy.distutils.command.build_ext/build_ext", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/command/config_compiler.py", "fn_id": "", "content": "class config_fc(Command):\n    \"\"\" Distutils command to hold user specified options\n    to Fortran compilers.\n\n    config_fc command is used by the FCompiler.customize() method.\n    \"\"\"\n    description = 'specify Fortran 77/Fortran 90 compiler information'\n    user_options = [('fcompiler=', None, 'specify Fortran compiler type'), ('f77exec=', None, 'specify F77 compiler command'), ('f90exec=', None, 'specify F90 compiler command'), ('f77flags=', None, 'specify F77 compiler flags'), ('f90flags=', None, 'specify F90 compiler flags'), ('opt=', None, 'specify optimization flags'), ('arch=', None, 'specify architecture specific optimization flags'), ('debug', 'g', 'compile with debugging information'), ('noopt', None, 'compile without optimization'), ('noarch', None, 'compile without arch-dependent optimization')]\n    help_options = [('help-fcompiler', None, 'list available Fortran compilers', show_fortran_compilers)]\n    boolean_options = ['debug', 'noopt', 'noarch']\n\n    def initialize_options(self):\n        self.fcompiler = None\n        self.f77exec = None\n        self.f90exec = None\n        self.f77flags = None\n        self.f90flags = None\n        self.opt = None\n        self.arch = None\n        self.debug = None\n        self.noopt = None\n        self.noarch = None\n\n    def finalize_options(self):\n        log.info('unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options')\n        build_clib = self.get_finalized_command('build_clib')\n        build_ext = self.get_finalized_command('build_ext')\n        config = self.get_finalized_command('config')\n        build = self.get_finalized_command('build')\n        cmd_list = [self, config, build_clib, build_ext, build]\n        for a in ['fcompiler']:\n            l = []\n            for c in cmd_list:\n                v = getattr(c, a)\n                if v is not None:\n                    if not isinstance(v, str):\n                        v = v.compiler_type\n                    if v not in l:\n                        l.append(v)\n            if not l:\n                v1 = None\n            else:\n                v1 = l[0]\n            if len(l) > 1:\n                log.warn('  commands have different --%s options: %s, using first in list as default' % (a, l))\n            if v1:\n                for c in cmd_list:\n                    if getattr(c, a) is None:\n                        setattr(c, a, v1)\n\n    def run(self):\n        return", "class_fn": true, "question_id": "numpy/numpy.distutils.command.config_compiler/config_fc", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/cpuinfo.py", "fn_id": "", "content": "class LinuxCPUInfo(CPUInfoBase):\n    info = None\n\n    def _try_call(self, func):\n        try:\n            return func()\n        except Exception:\n            pass\n\n    def _has_ssse3(self):\n        return re.match('.*?\\\\bssse3\\\\b', self.info[0]['flags']) is not None\n\n    def _is_Celeron(self):\n        return re.match('.*?Celeron', self.info[0]['model name']) is not None\n\n    def _is_PentiumIV(self):\n        return re.match('.*?Pentium.*?(IV|4)\\\\b', self.info[0]['model name']) is not None\n\n    def _is_PentiumMMX(self):\n        return re.match('.*?Pentium.*?MMX\\\\b', self.info[0]['model name']) is not None\n\n    def _is_AthlonK6(self):\n        return re.match('.*?AMD-K6', self.info[0]['model name']) is not None\n\n    def _is_PentiumPro(self):\n        return re.match('.*?PentiumPro\\\\b', self.info[0]['model name']) is not None\n\n    def _has_mmx(self):\n        return re.match('.*?\\\\bmmx\\\\b', self.info[0]['flags']) is not None\n\n    def _is_64bit(self):\n        return self.__get_nbits() == '64'\n\n    def __init__(self):\n        if self.info is not None:\n            return\n        info = [{}]\n        (ok, output) = getoutput('uname -m')\n        if ok:\n            info[0]['uname_m'] = output.strip()\n        try:\n            fo = open('/proc/cpuinfo')\n        except OSError as e:\n            warnings.warn(str(e), UserWarning, stacklevel=2)\n        else:\n            for line in fo:\n                name_value = [s.strip() for s in line.split(':', 1)]\n                if len(name_value) != 2:\n                    continue\n                (name, value) = name_value\n                if not info or name in info[-1]:\n                    info.append({})\n                info[-1][name] = value\n            fo.close()\n        self.__class__.info = info\n\n    def _is_Core2(self):\n        return self.is_64bit() and self.is_Intel() and (re.match('.*?Core\\\\(TM\\\\)2\\\\b', self.info[0]['model name']) is not None)\n\n    def _is_Athlon64(self):\n        return re.match('.*?Athlon\\\\(tm\\\\) 64\\\\b', self.info[0]['model name']) is not None\n\n    def _is_PentiumIII(self):\n        return re.match('.*?Pentium.*?III\\\\b', self.info[0]['model name']) is not None\n\n    def _is_Hammer(self):\n        return re.match('.*?Hammer\\\\b', self.info[0]['model name']) is not None\n\n    def _is_Alpha(self):\n        return self.info[0]['cpu'] == 'Alpha'\n\n    def _has_sse(self):\n        return re.match('.*?\\\\bsse\\\\b', self.info[0]['flags']) is not None\n\n    def _is_Pentium(self):\n        return re.match('.*?Pentium', self.info[0]['model name']) is not None\n\n    def _is_EV4(self):\n        return self.is_Alpha() and self.info[0]['cpu model'] == 'EV4'\n    _is_i386 = _not_impl\n\n    def _is_singleCPU(self):\n        return len(self.info) == 1\n\n    def _is_EV5(self):\n        return self.is_Alpha() and self.info[0]['cpu model'] == 'EV5'\n\n    def _is_Prescott(self):\n        return self.is_PentiumIV() and self.has_sse3()\n\n    def _is_i586(self):\n        return self.is_Intel() and self.info[0]['cpu family'] == '5'\n\n    def _is_Itanium(self):\n        return re.match('.*?Itanium\\\\b', self.info[0]['family']) is not None\n\n    def _is_i486(self):\n        return self.info[0]['cpu'] == 'i486'\n\n    def _is_Opteron(self):\n        return re.match('.*?Opteron\\\\b', self.info[0]['model name']) is not None\n\n    def _is_i686(self):\n        return self.is_Intel() and self.info[0]['cpu family'] == '6'\n\n    def __get_nbits(self):\n        abits = platform.architecture()[0]\n        nbits = re.compile('(\\\\d+)bit').search(abits).group(1)\n        return nbits\n\n    def _is_AthlonHX(self):\n        return re.match('.*?Athlon HX\\\\b', self.info[0]['model name']) is not None\n\n    def _is_AthlonK6_2(self):\n        return self._is_AMD() and self.info[0]['model'] == '2'\n\n    def _has_3dnow(self):\n        return re.match('.*?\\\\b3dnow\\\\b', self.info[0]['flags']) is not None\n\n    def _has_f00f_bug(self):\n        return self.info[0]['f00f_bug'] == 'yes'\n\n    def _is_AMD(self):\n        return self.info[0]['vendor_id'] == 'AuthenticAMD'\n\n    def _is_AthlonK7(self):\n        return re.match('.*?AMD-K7', self.info[0]['model name']) is not None\n\n    def _has_fdiv_bug(self):\n        return self.info[0]['fdiv_bug'] == 'yes'\n\n    def _is_PCA56(self):\n        return self.is_Alpha() and self.info[0]['cpu model'] == 'PCA56'\n    _is_Xeon = _is_XEON\n\n    def _is_PentiumM(self):\n        return re.match('.*?Pentium.*?M\\\\b', self.info[0]['model name']) is not None\n\n    def _getNCPUs(self):\n        return len(self.info)\n\n    def _has_3dnowext(self):\n        return re.match('.*?\\\\b3dnowext\\\\b', self.info[0]['flags']) is not None\n\n    def _is_AthlonMP(self):\n        return re.match('.*?Athlon\\\\(tm\\\\) MP\\\\b', self.info[0]['model name']) is not None\n\n    def _is_PentiumII(self):\n        return re.match('.*?Pentium.*?II\\\\b', self.info[0]['model name']) is not None\n\n    def _is_AMD64(self):\n        return self.is_AMD() and self.info[0]['family'] == '15'\n\n    def _is_XEON(self):\n        return re.match('.*?XEON\\\\b', self.info[0]['model name'], re.IGNORECASE) is not None\n\n    def _not_impl(self):\n        pass\n\n    def _is_EV56(self):\n        return self.is_Alpha() and self.info[0]['cpu model'] == 'EV56'\n\n    def _is_Nocona(self):\n        return self.is_Intel() and (self.info[0]['cpu family'] == '6' or self.info[0]['cpu family'] == '15') and (self.has_sse3() and (not self.has_ssse3())) and (re.match('.*?\\\\blm\\\\b', self.info[0]['flags']) is not None)\n\n    def _is_AthlonK6_3(self):\n        return self._is_AMD() and self.info[0]['model'] == '3'\n\n    def _is_Intel(self):\n        return self.info[0]['vendor_id'] == 'GenuineIntel'\n\n    def _has_sse2(self):\n        return re.match('.*?\\\\bsse2\\\\b', self.info[0]['flags']) is not None\n\n    def _has_sse3(self):\n        return re.match('.*?\\\\bpni\\\\b', self.info[0]['flags']) is not None", "class_fn": true, "question_id": "numpy/numpy.distutils.cpuinfo/LinuxCPUInfo", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/extension.py", "fn_id": "", "content": "class Extension(old_Extension):\n    \"\"\"\n    Parameters\n    ----------\n    name : str\n        Extension name.\n    sources : list of str\n        List of source file locations relative to the top directory of\n        the package.\n    extra_compile_args : list of str\n        Extra command line arguments to pass to the compiler.\n    extra_f77_compile_args : list of str\n        Extra command line arguments to pass to the fortran77 compiler.\n    extra_f90_compile_args : list of str\n        Extra command line arguments to pass to the fortran90 compiler.\n    \"\"\"\n\n    def __init__(self, name, sources, include_dirs=None, define_macros=None, undef_macros=None, library_dirs=None, libraries=None, runtime_library_dirs=None, extra_objects=None, extra_compile_args=None, extra_link_args=None, export_symbols=None, swig_opts=None, depends=None, language=None, f2py_options=None, module_dirs=None, extra_c_compile_args=None, extra_cxx_compile_args=None, extra_f77_compile_args=None, extra_f90_compile_args=None):\n        old_Extension.__init__(self, name, [], include_dirs=include_dirs, define_macros=define_macros, undef_macros=undef_macros, library_dirs=library_dirs, libraries=libraries, runtime_library_dirs=runtime_library_dirs, extra_objects=extra_objects, extra_compile_args=extra_compile_args, extra_link_args=extra_link_args, export_symbols=export_symbols)\n        self.sources = sources\n        self.swig_opts = swig_opts or []\n        if isinstance(self.swig_opts, str):\n            import warnings\n            msg = 'swig_opts is specified as a string instead of a list'\n            warnings.warn(msg, SyntaxWarning, stacklevel=2)\n            self.swig_opts = self.swig_opts.split()\n        self.depends = depends or []\n        self.language = language\n        self.f2py_options = f2py_options or []\n        self.module_dirs = module_dirs or []\n        self.extra_c_compile_args = extra_c_compile_args or []\n        self.extra_cxx_compile_args = extra_cxx_compile_args or []\n        self.extra_f77_compile_args = extra_f77_compile_args or []\n        self.extra_f90_compile_args = extra_f90_compile_args or []\n        return\n\n    def has_cxx_sources(self):\n        for source in self.sources:\n            if cxx_ext_re(str(source)):\n                return True\n        return False\n\n    def has_f2py_sources(self):\n        for source in self.sources:\n            if fortran_pyf_ext_re(source):\n                return True\n        return False", "class_fn": true, "question_id": "numpy/numpy.distutils.extension/Extension", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/absoft.py", "fn_id": "", "content": "class AbsoftFCompiler(FCompiler):\n    compiler_type = 'absoft'\n    description = 'Absoft Corp Fortran Compiler'\n    version_pattern = '(f90:.*?(Absoft Pro FORTRAN Version|FORTRAN 77 Compiler|Absoft Fortran Compiler Version|Copyright Absoft Corporation.*?Version))' + ' (?P<version>[^\\\\s*,]*)(.*?Absoft Corp|)'\n    executables = {'version_cmd': None, 'compiler_f77': ['f77'], 'compiler_fix': ['f90'], 'compiler_f90': ['f90'], 'linker_so': ['<F90>'], 'archiver': ['ar', '-cr'], 'ranlib': ['ranlib']}\n    if os.name == 'nt':\n        library_switch = '/out:'\n    module_dir_switch = None\n    module_include_switch = '-p'\n\n    def library_dir_option(self, dir):\n        if os.name == 'nt':\n            return ['-link', '/PATH:%s' % dir]\n        return '-L' + dir\n\n    def get_version(self, force=False, ok_status=[0]):\n        assert self._is_customised\n        version = CCompiler.get_version(self, force=force, ok_status=ok_status)\n        if version is None:\n            raise CompilerNotFound()\n        return version\n\n    def customize(self, dist=None):\n        \"\"\"Customize Fortran compiler.\n\n        This method gets Fortran compiler specific information from\n        (i) class definition, (ii) environment, (iii) distutils config\n        files, and (iv) command line (later overrides earlier).\n\n        This method should be always called after constructing a\n        compiler instance. But not in __init__ because Distribution\n        instance is needed for (iii) and (iv).\n        \"\"\"\n        log.info('customize %s' % self.__class__.__name__)\n        self._is_customised = True\n        self.distutils_vars.use_distribution(dist)\n        self.command_vars.use_distribution(dist)\n        self.flag_vars.use_distribution(dist)\n        self.update_executables()\n        self.find_executables()\n        noopt = self.distutils_vars.get('noopt', False)\n        noarch = self.distutils_vars.get('noarch', noopt)\n        debug = self.distutils_vars.get('debug', False)\n        f77 = self.command_vars.compiler_f77\n        f90 = self.command_vars.compiler_f90\n        f77flags = []\n        f90flags = []\n        freeflags = []\n        fixflags = []\n        if f77:\n            f77 = _shell_utils.NativeParser.split(f77)\n            f77flags = self.flag_vars.f77\n        if f90:\n            f90 = _shell_utils.NativeParser.split(f90)\n            f90flags = self.flag_vars.f90\n            freeflags = self.flag_vars.free\n        fix = self.command_vars.compiler_fix\n        if fix:\n            fix = _shell_utils.NativeParser.split(fix)\n            fixflags = self.flag_vars.fix + f90flags\n        (oflags, aflags, dflags) = ([], [], [])\n\n        def get_flags(tag, flags):\n            flags.extend(getattr(self.flag_vars, tag))\n            this_get = getattr(self, 'get_flags_' + tag)\n            for (name, c, flagvar) in [('f77', f77, f77flags), ('f90', f90, f90flags), ('f90', fix, fixflags)]:\n                t = '%s_%s' % (tag, name)\n                if c and this_get is not getattr(self, 'get_flags_' + t):\n                    flagvar.extend(getattr(self.flag_vars, t))\n        if not noopt:\n            get_flags('opt', oflags)\n            if not noarch:\n                get_flags('arch', aflags)\n        if debug:\n            get_flags('debug', dflags)\n        fflags = self.flag_vars.flags + dflags + oflags + aflags\n        if f77:\n            self.set_commands(compiler_f77=f77 + f77flags + fflags)\n        if f90:\n            self.set_commands(compiler_f90=f90 + freeflags + f90flags + fflags)\n        if fix:\n            self.set_commands(compiler_fix=fix + fixflags + fflags)\n        linker_so = self.linker_so\n        if linker_so:\n            linker_so_flags = self.flag_vars.linker_so\n            if sys.platform.startswith('aix'):\n                python_lib = get_python_lib(standard_lib=1)\n                ld_so_aix = os.path.join(python_lib, 'config', 'ld_so_aix')\n                python_exp = os.path.join(python_lib, 'config', 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            if sys.platform.startswith('os400'):\n                from distutils.sysconfig import get_config_var\n                python_config = get_config_var('LIBPL')\n                ld_so_aix = os.path.join(python_config, 'ld_so_aix')\n                python_exp = os.path.join(python_config, 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            self.set_commands(linker_so=linker_so + linker_so_flags)\n        linker_exe = self.linker_exe\n        if linker_exe:\n            linker_exe_flags = self.flag_vars.linker_exe\n            self.set_commands(linker_exe=linker_exe + linker_exe_flags)\n        ar = self.command_vars.archiver\n        if ar:\n            arflags = self.flag_vars.ar\n            self.set_commands(archiver=[ar] + arflags)\n        self.set_library_dirs(self.get_library_dirs())\n        self.set_libraries(self.get_libraries())\n\n    def get_flags_f90(self):\n        opt = FCompiler.get_flags_f90(self)\n        opt.extend(['-YCFRL=1', '-YCOM_NAMES=LCS', '-YCOM_PFX', '-YEXT_PFX', '-YCOM_SFX=_', '-YEXT_SFX=_', '-YEXT_NAMES=LCS'])\n        if self.get_version():\n            if self.get_version() > '4.6':\n                opt.extend(['-YDEALLOC=ALL'])\n        return opt\n\n    def get_flags_fix(self):\n        opt = FCompiler.get_flags_fix(self)\n        opt.extend(['-YCFRL=1', '-YCOM_NAMES=LCS', '-YCOM_PFX', '-YEXT_PFX', '-YCOM_SFX=_', '-YEXT_SFX=_', '-YEXT_NAMES=LCS'])\n        opt.extend(['-f', 'fixed'])\n        return opt\n\n    def library_option(self, lib):\n        if os.name == 'nt':\n            return '%s.lib' % lib\n        return '-l' + lib\n\n    def _command_property(key):\n\n        def fget(self):\n            assert self._is_customised\n            return self.executables[key]\n        return property(fget=fget)\n\n    def get_flags_linker_so(self):\n        if os.name == 'nt':\n            opt = ['/dll']\n        elif self.get_version() >= '9.0':\n            opt = ['-shared']\n        else:\n            opt = ['-K', 'shared']\n        return opt\n\n    def get_library_dirs(self):\n        opt = FCompiler.get_library_dirs(self)\n        d = os.environ.get('ABSOFT')\n        if d:\n            if self.get_version() >= '10.0':\n                prefix = 'sh'\n            else:\n                prefix = ''\n            if cpu.is_64bit():\n                suffix = '64'\n            else:\n                suffix = ''\n            opt.append(os.path.join(d, '%slib%s' % (prefix, suffix)))\n        return opt\n\n    def get_flags_f77(self):\n        opt = FCompiler.get_flags_f77(self)\n        opt.extend(['-N22', '-N90', '-N110'])\n        v = self.get_version()\n        if os.name == 'nt':\n            if v and v >= '8.0':\n                opt.extend(['-f', '-N15'])\n        else:\n            opt.append('-f')\n            if v:\n                if v <= '4.6':\n                    opt.append('-B108')\n                else:\n                    opt.append('-N15')\n        return opt\n\n    def get_flags_opt(self):\n        opt = ['-O']\n        return opt\n\n    def get_libraries(self):\n        opt = FCompiler.get_libraries(self)\n        if self.get_version() >= '11.0':\n            opt.extend(['af90math', 'afio', 'af77math', 'amisc'])\n        elif self.get_version() >= '10.0':\n            opt.extend(['af90math', 'afio', 'af77math', 'U77'])\n        elif self.get_version() >= '8.0':\n            opt.extend(['f90math', 'fio', 'f77math', 'U77'])\n        else:\n            opt.extend(['fio', 'f90math', 'fmath', 'U77'])\n        if os.name == 'nt':\n            opt.append('COMDLG32')\n        return opt\n\n    def get_flags(self):\n        opt = FCompiler.get_flags(self)\n        if os.name != 'nt':\n            opt.extend(['-s'])\n            if self.get_version():\n                if self.get_version() >= '8.2':\n                    opt.append('-fpic')\n        return opt\n\n    def update_executables(self):\n        f = cyg2win32(dummy_fortran_file())\n        self.executables['version_cmd'] = ['<F90>', '-V', '-c', f + '.f', '-o', f + '.o']", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.absoft/AbsoftFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "numpy", "url": "numpy==1.26.4", "last_update_at": "", "file": "numpy/distutils/fcompiler/gnu.py", "fn_id": "", "content": "class GnuFCompiler(FCompiler):\n    compiler_type = 'gnu'\n    compiler_aliases = ('g77',)\n    description = 'GNU Fortran 77 compiler'\n\n    def get_library_dirs(self):\n        opt = []\n        if sys.platform[:5] != 'linux':\n            d = self.get_libgcc_dir()\n            if d:\n                if sys.platform == 'win32' and (not d.startswith('/usr/lib')):\n                    d = os.path.normpath(d)\n                    path = os.path.join(d, 'lib%s.a' % self.g2c)\n                    if not os.path.exists(path):\n                        root = os.path.join(d, *(os.pardir,) * 4)\n                        d2 = os.path.abspath(os.path.join(root, 'lib'))\n                        path = os.path.join(d2, 'lib%s.a' % self.g2c)\n                        if os.path.exists(path):\n                            opt.append(d2)\n                opt.append(d)\n        lib_gfortran_dir = self.get_libgfortran_dir()\n        if lib_gfortran_dir:\n            opt.append(lib_gfortran_dir)\n        return opt\n\n    def _command_property(key):\n\n        def fget(self):\n            assert self._is_customised\n            return self.executables[key]\n        return property(fget=fget)\n    possible_executables = ['g77', 'f77']\n    executables = {'version_cmd': [None, '-dumpversion'], 'compiler_f77': [None, '-g', '-Wall', '-fno-second-underscore'], 'compiler_f90': None, 'compiler_fix': None, 'linker_so': [None, '-g', '-Wall'], 'archiver': ['ar', '-cr'], 'ranlib': ['ranlib'], 'linker_exe': [None, '-g', '-Wall']}\n    module_dir_switch = None\n    module_include_switch = None\n    if os.name != 'nt' and sys.platform != 'cygwin':\n        pic_flags = ['-fPIC']\n    if sys.platform == 'win32':\n        for key in ['version_cmd', 'compiler_f77', 'linker_so', 'linker_exe']:\n            executables[key].append('-mno-cygwin')\n    g2c = 'g2c'\n    suggested_f90_compiler = 'gnu95'\n\n    def get_libraries(self):\n        opt = []\n        d = self.get_libgcc_dir()\n        if d is not None:\n            g2c = self.g2c + '-pic'\n            f = self.static_lib_format % (g2c, self.static_lib_extension)\n            if not os.path.isfile(os.path.join(d, f)):\n                g2c = self.g2c\n        else:\n            g2c = self.g2c\n        if g2c is not None:\n            opt.append(g2c)\n        c_compiler = self.c_compiler\n        if sys.platform == 'win32' and c_compiler and (c_compiler.compiler_type == 'msvc'):\n            opt.append('gcc')\n        if sys.platform == 'darwin':\n            opt.append('cc_dynamic')\n        return opt\n\n    def get_flags_linker_so(self):\n        opt = self.linker_so[1:]\n        if sys.platform == 'darwin':\n            target = os.environ.get('MACOSX_DEPLOYMENT_TARGET', None)\n            if not target:\n                import sysconfig\n                target = sysconfig.get_config_var('MACOSX_DEPLOYMENT_TARGET')\n                if not target:\n                    target = '10.9'\n                    s = f'Env. variable MACOSX_DEPLOYMENT_TARGET set to {target}'\n                    warnings.warn(s, stacklevel=2)\n                os.environ['MACOSX_DEPLOYMENT_TARGET'] = str(target)\n            opt.extend(['-undefined', 'dynamic_lookup', '-bundle'])\n        else:\n            opt.append('-shared')\n        if sys.platform.startswith('sunos'):\n            opt.append('-mimpure-text')\n        return opt\n\n    def get_flags_arch(self):\n        return []\n\n    def gnu_version_match(self, version_string):\n        \"\"\"Handle the different versions of GNU fortran compilers\"\"\"\n        while version_string.startswith('gfortran: warning'):\n            version_string = version_string[version_string.find('\\n') + 1:].strip()\n        if len(version_string) <= 20:\n            m = re.search('([0-9.]+)', version_string)\n            if m:\n                if version_string.startswith('GNU Fortran'):\n                    return ('g77', m.group(1))\n                elif m.start() == 0:\n                    return ('gfortran', m.group(1))\n        else:\n            m = re.search('GNU Fortran\\\\s+95.*?([0-9-.]+)', version_string)\n            if m:\n                return ('gfortran', m.group(1))\n            m = re.search('GNU Fortran.*?\\\\-?([0-9-.]+\\\\.[0-9-.]+)', version_string)\n            if m:\n                v = m.group(1)\n                if v.startswith('0') or v.startswith('2') or v.startswith('3'):\n                    return ('g77', v)\n                else:\n                    return ('gfortran', v)\n        err = 'A valid Fortran version was not found in this string:\\n'\n        raise ValueError(err + version_string)\n\n    def get_libgfortran_dir(self):\n        if sys.platform[:5] == 'linux':\n            libgfortran_name = 'libgfortran.so'\n        elif sys.platform == 'darwin':\n            libgfortran_name = 'libgfortran.dylib'\n        else:\n            libgfortran_name = None\n        libgfortran_dir = None\n        if libgfortran_name:\n            find_lib_arg = ['-print-file-name={0}'.format(libgfortran_name)]\n            try:\n                output = subprocess.check_output(self.compiler_f77 + find_lib_arg)\n            except (OSError, subprocess.CalledProcessError):\n                pass\n            else:\n                output = filepath_from_subprocess_output(output)\n                libgfortran_dir = os.path.dirname(output)\n        return libgfortran_dir\n\n    def version_match(self, version_string):\n        v = self.gnu_version_match(version_string)\n        if not v or v[0] != 'g77':\n            return None\n        return v[1]\n\n    def get_version(self, force=False, ok_status=[0]):\n        assert self._is_customised\n        version = CCompiler.get_version(self, force=force, ok_status=ok_status)\n        if version is None:\n            raise CompilerNotFound()\n        return version\n\n    def customize(self, dist=None):\n        \"\"\"Customize Fortran compiler.\n\n        This method gets Fortran compiler specific information from\n        (i) class definition, (ii) environment, (iii) distutils config\n        files, and (iv) command line (later overrides earlier).\n\n        This method should be always called after constructing a\n        compiler instance. But not in __init__ because Distribution\n        instance is needed for (iii) and (iv).\n        \"\"\"\n        log.info('customize %s' % self.__class__.__name__)\n        self._is_customised = True\n        self.distutils_vars.use_distribution(dist)\n        self.command_vars.use_distribution(dist)\n        self.flag_vars.use_distribution(dist)\n        self.update_executables()\n        self.find_executables()\n        noopt = self.distutils_vars.get('noopt', False)\n        noarch = self.distutils_vars.get('noarch', noopt)\n        debug = self.distutils_vars.get('debug', False)\n        f77 = self.command_vars.compiler_f77\n        f90 = self.command_vars.compiler_f90\n        f77flags = []\n        f90flags = []\n        freeflags = []\n        fixflags = []\n        if f77:\n            f77 = _shell_utils.NativeParser.split(f77)\n            f77flags = self.flag_vars.f77\n        if f90:\n            f90 = _shell_utils.NativeParser.split(f90)\n            f90flags = self.flag_vars.f90\n            freeflags = self.flag_vars.free\n        fix = self.command_vars.compiler_fix\n        if fix:\n            fix = _shell_utils.NativeParser.split(fix)\n            fixflags = self.flag_vars.fix + f90flags\n        (oflags, aflags, dflags) = ([], [], [])\n\n        def get_flags(tag, flags):\n            flags.extend(getattr(self.flag_vars, tag))\n            this_get = getattr(self, 'get_flags_' + tag)\n            for (name, c, flagvar) in [('f77', f77, f77flags), ('f90', f90, f90flags), ('f90', fix, fixflags)]:\n                t = '%s_%s' % (tag, name)\n                if c and this_get is not getattr(self, 'get_flags_' + t):\n                    flagvar.extend(getattr(self.flag_vars, t))\n        if not noopt:\n            get_flags('opt', oflags)\n            if not noarch:\n                get_flags('arch', aflags)\n        if debug:\n            get_flags('debug', dflags)\n        fflags = self.flag_vars.flags + dflags + oflags + aflags\n        if f77:\n            self.set_commands(compiler_f77=f77 + f77flags + fflags)\n        if f90:\n            self.set_commands(compiler_f90=f90 + freeflags + f90flags + fflags)\n        if fix:\n            self.set_commands(compiler_fix=fix + fixflags + fflags)\n        linker_so = self.linker_so\n        if linker_so:\n            linker_so_flags = self.flag_vars.linker_so\n            if sys.platform.startswith('aix'):\n                python_lib = get_python_lib(standard_lib=1)\n                ld_so_aix = os.path.join(python_lib, 'config', 'ld_so_aix')\n                python_exp = os.path.join(python_lib, 'config', 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            if sys.platform.startswith('os400'):\n                from distutils.sysconfig import get_config_var\n                python_config = get_config_var('LIBPL')\n                ld_so_aix = os.path.join(python_config, 'ld_so_aix')\n                python_exp = os.path.join(python_config, 'python.exp')\n                linker_so = [ld_so_aix] + linker_so + ['-bI:' + python_exp]\n            self.set_commands(linker_so=linker_so + linker_so_flags)\n        linker_exe = self.linker_exe\n        if linker_exe:\n            linker_exe_flags = self.flag_vars.linker_exe\n            self.set_commands(linker_exe=linker_exe + linker_exe_flags)\n        ar = self.command_vars.archiver\n        if ar:\n            arflags = self.flag_vars.ar\n            self.set_commands(archiver=[ar] + arflags)\n        self.set_library_dirs(self.get_library_dirs())\n        self.set_libraries(self.get_libraries())\n\n    def runtime_library_dir_option(self, dir):\n        if sys.platform == 'win32' or sys.platform == 'cygwin':\n            raise NotImplementedError\n        assert ',' not in dir\n        if sys.platform == 'darwin':\n            return f'-Wl,-rpath,{dir}'\n        elif sys.platform.startswith(('aix', 'os400')):\n            return f'-Wl,-blibpath:{dir}'\n        else:\n            return f'-Wl,-rpath={dir}'\n\n    def get_flags_debug(self):\n        return ['-g']\n\n    def get_libgcc_dir(self):\n        try:\n            output = subprocess.check_output(self.compiler_f77 + ['-print-libgcc-file-name'])\n        except (OSError, subprocess.CalledProcessError):\n            pass\n        else:\n            output = filepath_from_subprocess_output(output)\n            return os.path.dirname(output)\n        return None\n\n    def _c_arch_flags(self):\n        \"\"\" Return detected arch flags from CFLAGS \"\"\"\n        import sysconfig\n        try:\n            cflags = sysconfig.get_config_vars()['CFLAGS']\n        except KeyError:\n            return []\n        arch_re = re.compile('-arch\\\\s+(\\\\w+)')\n        arch_flags = []\n        for arch in arch_re.findall(cflags):\n            arch_flags += ['-arch', arch]\n        return arch_flags\n\n    def get_flags_opt(self):\n        v = self.get_version()\n        if v and v <= '3.3.3':\n            opt = ['-O2']\n        else:\n            opt = ['-O3']\n        opt.append('-funroll-loops')\n        return opt", "class_fn": true, "question_id": "numpy/numpy.distutils.fcompiler.gnu/GnuFCompiler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_config/config.py", "fn_id": "", "content": "class CallableDynamicDoc(Generic[T]):\n\n    def __init__(self, func: Callable[..., T], doc_tmpl: str) -> None:\n        self.__doc_tmpl__ = doc_tmpl\n        self.__func__ = func\n\n    def __call__(self, *args, **kwds) -> T:\n        return self.__func__(*args, **kwds)\n\n    @property\n    def __doc__(self) -> str:\n        opts_desc = _describe_option('all', _print_desc=False)\n        opts_list = pp_options_list(list(_registered_options.keys()))\n        return self.__doc_tmpl__.format(opts_desc=opts_desc, opts_list=opts_list)", "class_fn": true, "question_id": "pandas/pandas._config.config/CallableDynamicDoc", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_testing/__init__.py", "fn_id": "", "content": "class SubclassedDataFrame(DataFrame):\n    _metadata = ['testattr']\n\n    @property\n    def _constructor(self):\n        return lambda *args, **kwargs: SubclassedDataFrame(*args, **kwargs)\n\n    @property\n    def _constructor_sliced(self):\n        return lambda *args, **kwargs: SubclassedSeries(*args, **kwargs)\n\n    def _setitem_array(self, key, value):\n        if com.is_bool_indexer(key):\n            if len(key) != len(self.index):\n                raise ValueError(f'Item wrong length {len(key)} instead of {len(self.index)}!')\n            key = check_bool_indexer(self.index, key)\n            indexer = key.nonzero()[0]\n            self._check_setitem_copy()\n            if isinstance(value, DataFrame):\n                value = value.reindex(self.index.take(indexer))\n            self.iloc[indexer] = value\n        elif isinstance(value, DataFrame):\n            check_key_length(self.columns, key, value)\n            for (k1, k2) in zip(key, value.columns):\n                self[k1] = value[k2]\n        elif not is_list_like(value):\n            for col in key:\n                self[col] = value\n        elif isinstance(value, np.ndarray) and value.ndim == 2:\n            self._iset_not_inplace(key, value)\n        elif np.ndim(value) > 1:\n            value = DataFrame(value).values\n            return self._setitem_array(key, value)\n        else:\n            self._iset_not_inplace(key, value)\n\n    @doc(make_doc('sum', ndim=2))\n    def sum(self, axis: Axis | None=0, skipna: bool=True, numeric_only: bool=False, min_count: int=0, **kwargs):\n        result = super().sum(axis, skipna, numeric_only, min_count, **kwargs)\n        return result.__finalize__(self, method='sum')\n\n    @doc(make_doc('prod', ndim=2))\n    def prod(self, axis: Axis | None=0, skipna: bool=True, numeric_only: bool=False, min_count: int=0, **kwargs):\n        result = super().prod(axis, skipna, numeric_only, min_count, **kwargs)\n        return result.__finalize__(self, method='prod')", "class_fn": true, "question_id": "pandas/pandas._testing/SubclassedDataFrame", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_typing.py", "fn_id": "", "content": "class ReadBuffer(BaseBuffer, Protocol[AnyStr_co]):\n\n    def read(self, __n: int=...) -> AnyStr_co:\n        ...\n\n    def tell(self) -> int:\n        ...\n\n    def seek(self, __offset: int, __whence: int=...) -> int:\n        ...\n\n    def seekable(self) -> bool:\n        ...", "class_fn": true, "question_id": "pandas/pandas._typing/ReadBuffer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_typing.py", "fn_id": "", "content": "class SequenceNotStr(Protocol[_T_co]):\n\n    @overload\n    def __getitem__(self, index: SupportsIndex, /) -> _T_co:\n        ...\n\n    @overload\n    def __getitem__(self, index: slice, /) -> Sequence[_T_co]:\n        ...\n\n    def __contains__(self, value: object, /) -> bool:\n        ...\n\n    def __len__(self) -> int:\n        ...\n\n    def __iter__(self) -> Iterator[_T_co]:\n        ...\n\n    def index(self, value: Any, /, start: int=0, stop: int=...) -> int:\n        ...\n\n    def count(self, value: Any, /) -> int:\n        ...\n\n    def __reversed__(self) -> Iterator[_T_co]:\n        ...", "class_fn": true, "question_id": "pandas/pandas._typing/SequenceNotStr", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/compat/compressors.py", "fn_id": "", "content": "    class BZ2File(bz2.BZ2File):\n        if not PY310:\n\n            def write(self, b) -> int:\n                return super().write(flatten_buffer(b))", "class_fn": true, "question_id": "pandas/pandas.compat.compressors/BZ2File", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/compat/pickle_compat.py", "fn_id": "", "content": "class Unpickler(pkl._Unpickler):\n\n    def find_class(self, module, name):\n        key = (module, name)\n        (module, name) = _class_locations_map.get(key, key)\n        return super().find_class(module, name)", "class_fn": true, "question_id": "pandas/pandas.compat.pickle_compat/Unpickler", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/apply.py", "fn_id": "", "content": "class NDFrameApply(Apply):\n    \"\"\"\n    Methods shared by FrameApply and SeriesApply but\n    not GroupByApply or ResamplerWindowApply\n    \"\"\"\n    obj: DataFrame | Series\n\n    @property\n    def index(self) -> Index:\n        return self.obj.index\n\n    @property\n    def agg_axis(self) -> Index:\n        return self.obj._get_agg_axis(self.axis)\n\n    def agg_or_apply_list_like(self, op_name: Literal['agg', 'apply']) -> DataFrame | Series:\n        obj = self.obj\n        kwargs = self.kwargs\n        if op_name == 'apply':\n            if isinstance(self, FrameApply):\n                by_row = self.by_row\n            elif isinstance(self, SeriesApply):\n                by_row = '_compat' if self.by_row else False\n            else:\n                by_row = False\n            kwargs = {**kwargs, 'by_row': by_row}\n        if getattr(obj, 'axis', 0) == 1:\n            raise NotImplementedError('axis other than 0 is not supported')\n        (keys, results) = self.compute_list_like(op_name, obj, kwargs)\n        result = self.wrap_results_list_like(keys, results)\n        return result\n\n    def normalize_dictlike_arg(self, how: str, obj: DataFrame | Series, func: AggFuncTypeDict) -> AggFuncTypeDict:\n        \"\"\"\n        Handler for dict-like argument.\n\n        Ensures that necessary columns exist if obj is a DataFrame, and\n        that a nested renamer is not passed. Also normalizes to all lists\n        when values consists of a mix of list and non-lists.\n        \"\"\"\n        assert how in ('apply', 'agg', 'transform')\n        if how == 'agg' and isinstance(obj, ABCSeries) and any((is_list_like(v) for (_, v) in func.items())) or any((is_dict_like(v) for (_, v) in func.items())):\n            raise SpecificationError('nested renamer is not supported')\n        if obj.ndim != 1:\n            from pandas import Index\n            cols = Index(list(func.keys())).difference(obj.columns, sort=True)\n            if len(cols) > 0:\n                raise KeyError(f'Column(s) {list(cols)} do not exist')\n        aggregator_types = (list, tuple, dict)\n        if any((isinstance(x, aggregator_types) for (_, x) in func.items())):\n            new_func: AggFuncTypeDict = {}\n            for (k, v) in func.items():\n                if not isinstance(v, aggregator_types):\n                    new_func[k] = [v]\n                else:\n                    new_func[k] = v\n            func = new_func\n        return func\n\n    def compute_list_like(self, op_name: Literal['agg', 'apply'], selected_obj: Series | DataFrame, kwargs: dict[str, Any]) -> tuple[list[Hashable] | Index, list[Any]]:\n        \"\"\"\n        Compute agg/apply results for like-like input.\n\n        Parameters\n        ----------\n        op_name : {\"agg\", \"apply\"}\n            Operation being performed.\n        selected_obj : Series or DataFrame\n            Data to perform operation on.\n        kwargs : dict\n            Keyword arguments to pass to the functions.\n\n        Returns\n        -------\n        keys : list[Hashable] or Index\n            Index labels for result.\n        results : list\n            Data for result. When aggregating with a Series, this can contain any\n            Python objects.\n        \"\"\"\n        func = cast(list[AggFuncTypeBase], self.func)\n        obj = self.obj\n        results = []\n        keys = []\n        if selected_obj.ndim == 1:\n            for a in func:\n                colg = obj._gotitem(selected_obj.name, ndim=1, subset=selected_obj)\n                args = [self.axis, *self.args] if include_axis(op_name, colg) else self.args\n                new_res = getattr(colg, op_name)(a, *args, **kwargs)\n                results.append(new_res)\n                name = com.get_callable_name(a) or a\n                keys.append(name)\n        else:\n            indices = []\n            for (index, col) in enumerate(selected_obj):\n                colg = obj._gotitem(col, ndim=1, subset=selected_obj.iloc[:, index])\n                args = [self.axis, *self.args] if include_axis(op_name, colg) else self.args\n                new_res = getattr(colg, op_name)(func, *args, **kwargs)\n                results.append(new_res)\n                indices.append(index)\n            keys = selected_obj.columns.take(indices)\n        return (keys, results)\n\n    def apply_list_or_dict_like(self) -> DataFrame | Series:\n        \"\"\"\n        Compute apply in case of a list-like or dict-like.\n\n        Returns\n        -------\n        result: Series, DataFrame, or None\n            Result when self.func is a list-like or dict-like, None otherwise.\n        \"\"\"\n        if self.engine == 'numba':\n            raise NotImplementedError(\"The 'numba' engine doesn't support list-like/dict likes of callables yet.\")\n        if self.axis == 1 and isinstance(self.obj, ABCDataFrame):\n            return self.obj.T.apply(self.func, 0, args=self.args, **self.kwargs).T\n        func = self.func\n        kwargs = self.kwargs\n        if is_dict_like(func):\n            result = self.agg_or_apply_dict_like(op_name='apply')\n        else:\n            result = self.agg_or_apply_list_like(op_name='apply')\n        result = reconstruct_and_relabel_result(result, func, **kwargs)\n        return result\n\n    def agg_or_apply_dict_like(self, op_name: Literal['agg', 'apply']) -> DataFrame | Series:\n        assert op_name in ['agg', 'apply']\n        obj = self.obj\n        kwargs = {}\n        if op_name == 'apply':\n            by_row = '_compat' if self.by_row else False\n            kwargs.update({'by_row': by_row})\n        if getattr(obj, 'axis', 0) == 1:\n            raise NotImplementedError('axis other than 0 is not supported')\n        selection = None\n        (result_index, result_data) = self.compute_dict_like(op_name, obj, selection, kwargs)\n        result = self.wrap_results_dict_like(obj, result_index, result_data)\n        return result", "class_fn": true, "question_id": "pandas/pandas.core.apply/NDFrameApply", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/base.py", "fn_id": "", "content": "class ExtensionArraySupportsAnyAll(ExtensionArray):\n\n    def all(self, *, skipna: bool=True) -> bool:\n        raise AbstractMethodError(self)\n\n    def _where(self, mask: npt.NDArray[np.bool_], value) -> Self:\n        \"\"\"\n        Analogue to np.where(mask, self, value)\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool]\n        value : scalar or listlike\n\n        Returns\n        -------\n        same type as self\n        \"\"\"\n        result = self.copy()\n        if is_list_like(value):\n            val = value[~mask]\n        else:\n            val = value\n        result[~mask] = val\n        return result\n\n    def any(self, *, skipna: bool=True) -> bool:\n        raise AbstractMethodError(self)\n\n    def transpose(self, *axes: int) -> ExtensionArray:\n        \"\"\"\n        Return a transposed view on this array.\n\n        Because ExtensionArrays are always 1D, this is a no-op.  It is included\n        for compatibility with np.ndarray.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3]).transpose()\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        return self[:]\n\n    def unique(self) -> Self:\n        \"\"\"\n        Compute the ExtensionArray of unique values.\n\n        Returns\n        -------\n        pandas.api.extensions.ExtensionArray\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3, 1, 2, 3])\n        >>> arr.unique()\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        uniques = unique(self.astype(object))\n        return self._from_sequence(uniques, dtype=self.dtype)", "class_fn": true, "question_id": "pandas/pandas.core.arrays.base/ExtensionArraySupportsAnyAll", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/floating.py", "fn_id": "", "content": "class FloatingArray(NumericArray):\n    \"\"\"\n    Array of floating (optional missing) values.\n\n    .. warning::\n\n       FloatingArray is currently experimental, and its API or internal\n       implementation may change without warning. Especially the behaviour\n       regarding NaN (distinct from NA missing values) is subject to change.\n\n    We represent a FloatingArray with 2 numpy arrays:\n\n    - data: contains a numpy float array of the appropriate dtype\n    - mask: a boolean array holding a mask on the data, True is missing\n\n    To construct an FloatingArray from generic array-like input, use\n    :func:`pandas.array` with one of the float dtypes (see examples).\n\n    See :ref:`integer_na` for more.\n\n    Parameters\n    ----------\n    values : numpy.ndarray\n        A 1-d float-dtype array.\n    mask : numpy.ndarray\n        A 1-d boolean-dtype array indicating missing values.\n    copy : bool, default False\n        Whether to copy the `values` and `mask`.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Returns\n    -------\n    FloatingArray\n\n    Examples\n    --------\n    Create an FloatingArray with :func:`pandas.array`:\n\n    >>> pd.array([0.1, None, 0.3], dtype=pd.Float32Dtype())\n    <FloatingArray>\n    [0.1, <NA>, 0.3]\n    Length: 3, dtype: Float32\n\n    String aliases for the dtypes are also available. They are capitalized.\n\n    >>> pd.array([0.1, None, 0.3], dtype=\"Float32\")\n    <FloatingArray>\n    [0.1, <NA>, 0.3]\n    Length: 3, dtype: Float32\n    \"\"\"\n    _dtype_cls = FloatingDtype\n    _internal_fill_value = np.nan\n    _truthy_value = 1.0\n    _falsey_value = 0.0\n\n    def __init__(self, values: np.ndarray, mask: npt.NDArray[np.bool_], copy: bool=False) -> None:\n        checker = self._dtype_cls._checker\n        if not (isinstance(values, np.ndarray) and checker(values.dtype)):\n            descr = 'floating' if self._dtype_cls.kind == 'f' else 'integer'\n            raise TypeError(f\"values should be {descr} numpy array. Use the 'pd.array' function instead\")\n        if values.dtype == np.float16:\n            raise TypeError('FloatingArray does not support np.float16 dtype.')\n        super().__init__(values, mask, copy=copy)", "class_fn": true, "question_id": "pandas/pandas.core.arrays.floating/FloatingArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/integer.py", "fn_id": "", "content": "@register_extension_dtype\nclass Int32Dtype(IntegerDtype):\n    type = np.int32\n    name: ClassVar[str] = 'Int32'\n    __doc__ = _dtype_docstring.format(dtype='int32')", "class_fn": true, "question_id": "pandas/pandas.core.arrays.integer/Int32Dtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/integer.py", "fn_id": "", "content": "class IntegerArray(NumericArray):\n    \"\"\"\n    Array of integer (optional missing) values.\n\n    Uses :attr:`pandas.NA` as the missing value.\n\n    .. warning::\n\n       IntegerArray is currently experimental, and its API or internal\n       implementation may change without warning.\n\n    We represent an IntegerArray with 2 numpy arrays:\n\n    - data: contains a numpy integer array of the appropriate dtype\n    - mask: a boolean array holding a mask on the data, True is missing\n\n    To construct an IntegerArray from generic array-like input, use\n    :func:`pandas.array` with one of the integer dtypes (see examples).\n\n    See :ref:`integer_na` for more.\n\n    Parameters\n    ----------\n    values : numpy.ndarray\n        A 1-d integer-dtype array.\n    mask : numpy.ndarray\n        A 1-d boolean-dtype array indicating missing values.\n    copy : bool, default False\n        Whether to copy the `values` and `mask`.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Returns\n    -------\n    IntegerArray\n\n    Examples\n    --------\n    Create an IntegerArray with :func:`pandas.array`.\n\n    >>> int_array = pd.array([1, None, 3], dtype=pd.Int32Dtype())\n    >>> int_array\n    <IntegerArray>\n    [1, <NA>, 3]\n    Length: 3, dtype: Int32\n\n    String aliases for the dtypes are also available. They are capitalized.\n\n    >>> pd.array([1, None, 3], dtype='Int32')\n    <IntegerArray>\n    [1, <NA>, 3]\n    Length: 3, dtype: Int32\n\n    >>> pd.array([1, None, 3], dtype='UInt16')\n    <IntegerArray>\n    [1, <NA>, 3]\n    Length: 3, dtype: UInt16\n    \"\"\"\n    _dtype_cls = IntegerDtype\n    _internal_fill_value = 1\n    _truthy_value = 1\n    _falsey_value = 0\n\n    def __init__(self, values: np.ndarray, mask: npt.NDArray[np.bool_], copy: bool=False) -> None:\n        checker = self._dtype_cls._checker\n        if not (isinstance(values, np.ndarray) and checker(values.dtype)):\n            descr = 'floating' if self._dtype_cls.kind == 'f' else 'integer'\n            raise TypeError(f\"values should be {descr} numpy array. Use the 'pd.array' function instead\")\n        if values.dtype == np.float16:\n            raise TypeError('FloatingArray does not support np.float16 dtype.')\n        super().__init__(values, mask, copy=copy)", "class_fn": true, "question_id": "pandas/pandas.core.arrays.integer/IntegerArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/integer.py", "fn_id": "", "content": "@register_extension_dtype\nclass UInt32Dtype(IntegerDtype):\n    type = np.uint32\n    name: ClassVar[str] = 'UInt32'\n    __doc__ = _dtype_docstring.format(dtype='uint32')", "class_fn": true, "question_id": "pandas/pandas.core.arrays.integer/UInt32Dtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/numeric.py", "fn_id": "", "content": "class NumericArray(BaseMaskedArray):\n    \"\"\"\n    Base class for IntegerArray and FloatingArray.\n    \"\"\"\n    _dtype_cls: type[NumericDtype]\n\n    def _cmp_method(self, other, op) -> BooleanArray:\n        from pandas.core.arrays import BooleanArray\n        mask = None\n        if isinstance(other, BaseMaskedArray):\n            (other, mask) = (other._data, other._mask)\n        elif is_list_like(other):\n            other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError('can only perform ops with 1-d structures')\n            if len(self) != len(other):\n                raise ValueError('Lengths must match to compare')\n        if other is libmissing.NA:\n            result = np.zeros(self._data.shape, dtype='bool')\n            mask = np.ones(self._data.shape, dtype='bool')\n        else:\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore', 'elementwise', FutureWarning)\n                warnings.filterwarnings('ignore', 'elementwise', DeprecationWarning)\n                method = getattr(self._data, f'__{op.__name__}__')\n                result = method(other)\n                if result is NotImplemented:\n                    result = invalid_comparison(self._data, other, op)\n        mask = self._propagate_mask(mask, other)\n        return BooleanArray(result, mask, copy=False)\n\n    @cache_readonly\n    def dtype(self) -> NumericDtype:\n        mapping = self._dtype_cls._get_dtype_mapping()\n        return mapping[self._data.dtype]\n\n    def min(self, *, skipna: bool=True, axis: AxisInt | None=0, **kwargs):\n        nv.validate_min((), kwargs)\n        result = masked_reductions.min(self._data, self._mask, skipna=skipna, axis=axis)\n        return self._wrap_reduction_result('min', result, skipna=skipna, axis=axis)\n\n    @classmethod\n    def _coerce_to_array(cls, value, *, dtype: DtypeObj, copy: bool=False) -> tuple[np.ndarray, np.ndarray]:\n        dtype_cls = cls._dtype_cls\n        default_dtype = dtype_cls._default_np_dtype\n        (values, mask, _, _) = _coerce_to_data_and_mask(value, dtype, copy, dtype_cls, default_dtype)\n        return (values, mask)\n    _HANDLED_TYPES = (np.ndarray, numbers.Number)\n\n    def __init__(self, values: np.ndarray, mask: npt.NDArray[np.bool_], copy: bool=False) -> None:\n        checker = self._dtype_cls._checker\n        if not (isinstance(values, np.ndarray) and checker(values.dtype)):\n            descr = 'floating' if self._dtype_cls.kind == 'f' else 'integer'\n            raise TypeError(f\"values should be {descr} numpy array. Use the 'pd.array' function instead\")\n        if values.dtype == np.float16:\n            raise TypeError('FloatingArray does not support np.float16 dtype.')\n        super().__init__(values, mask, copy=copy)\n\n    @classmethod\n    def _from_sequence_of_strings(cls, strings, *, dtype: Dtype | None=None, copy: bool=False) -> Self:\n        from pandas.core.tools.numeric import to_numeric\n        scalars = to_numeric(strings, errors='raise', dtype_backend='numpy_nullable')\n        return cls._from_sequence(scalars, dtype=dtype, copy=copy)\n\n    @doc(ExtensionArray.fillna)\n    def fillna(self, value=None, method=None, limit: int | None=None, copy: bool=True) -> Self:\n        (value, method) = validate_fillna_kwargs(value, method)\n        mask = self._mask\n        value = missing.check_value_size(value, mask, len(self))\n        if mask.any():\n            if method is not None:\n                func = missing.get_fill_func(method, ndim=self.ndim)\n                npvalues = self._data.T\n                new_mask = mask.T\n                if copy:\n                    npvalues = npvalues.copy()\n                    new_mask = new_mask.copy()\n                func(npvalues, limit=limit, mask=new_mask)\n                return self._simple_new(npvalues.T, new_mask.T)\n            else:\n                if copy:\n                    new_values = self.copy()\n                else:\n                    new_values = self[:]\n                new_values[mask] = value\n        elif copy:\n            new_values = self.copy()\n        else:\n            new_values = self[:]\n        return new_values", "class_fn": true, "question_id": "pandas/pandas.core.arrays.numeric/NumericArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/base.py", "fn_id": "", "content": "class NoNewAttributesMixin:\n    \"\"\"\n    Mixin which prevents adding new attributes.\n\n    Prevents additional attributes via xxx.attribute = \"something\" after a\n    call to `self.__freeze()`. Mainly used to prevent the user from using\n    wrong attributes on an accessor (`Series.cat/.str/.dt`).\n\n    If you really want to add a new attribute at a later time, you need to use\n    `object.__setattr__(self, key, value)`.\n    \"\"\"\n\n    def _freeze(self) -> None:\n        \"\"\"\n        Prevents setting additional attributes.\n        \"\"\"\n        object.__setattr__(self, '__frozen', True)\n\n    def __setattr__(self, key: str, value) -> None:\n        if getattr(self, '__frozen', False) and (not (key == '_cache' or key in type(self).__dict__ or getattr(self, key, None) is not None)):\n            raise AttributeError(f\"You cannot add any new attribute '{key}'\")\n        object.__setattr__(self, key, value)", "class_fn": true, "question_id": "pandas/pandas.core.base/NoNewAttributesMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/engines.py", "fn_id": "", "content": "class NumExprEngine(AbstractEngine):\n    \"\"\"NumExpr engine class\"\"\"\n    has_neg_frac = True\n\n    def _evaluate(self):\n        import numexpr as ne\n        s = self.convert()\n        env = self.expr.env\n        scope = env.full_scope\n        _check_ne_builtin_clash(self.expr)\n        return ne.evaluate(s, local_dict=scope)\n\n    def evaluate(self) -> object:\n        \"\"\"\n        Run the engine on the expression.\n\n        This method performs alignment which is necessary no matter what engine\n        is being used, thus its implementation is in the base class.\n\n        Returns\n        -------\n        object\n            The result of the passed expression.\n        \"\"\"\n        if not self._is_aligned:\n            (self.result_type, self.aligned_axes) = align_terms(self.expr.terms)\n        res = self._evaluate()\n        return reconstruct_object(self.result_type, res, self.aligned_axes, self.expr.terms.return_type)\n\n    def __init__(self, expr) -> None:\n        self.expr = expr\n        self.aligned_axes = None\n        self.result_type = None\n\n    def convert(self) -> str:\n        \"\"\"\n        Convert an expression for evaluation.\n\n        Defaults to return the expression as a string.\n        \"\"\"\n        return printing.pprint_thing(self.expr)", "class_fn": true, "question_id": "pandas/pandas.core.computation.engines/NumExprEngine", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/expr.py", "fn_id": "", "content": "@disallow((_unsupported_nodes | _python_not_supported) - (_boolop_nodes | frozenset(['BoolOp', 'Attribute', 'In', 'NotIn', 'Tuple'])))\nclass PandasExprVisitor(BaseExprVisitor):\n\n    def visit_BinOp(self, node, **kwargs):\n        (op, op_class, left, right) = self._maybe_transform_eq_ne(node)\n        (left, right) = self._maybe_downcast_constants(left, right)\n        return self._maybe_evaluate_binop(op, op_class, left, right)\n\n    def visit_Name(self, node, **kwargs) -> Term:\n        return self.term_type(node.id, self.env, **kwargs)\n\n    def visit_UnaryOp(self, node, **kwargs):\n        op = self.visit(node.op)\n        operand = self.visit(node.operand)\n        return op(operand)\n\n    def __init__(self, env, engine, parser, preparser=partial(_preparse, f=_compose(_replace_locals, _replace_booleans, clean_backtick_quoted_toks))) -> None:\n        super().__init__(env, engine, parser, preparser)", "class_fn": true, "question_id": "pandas/pandas.core.computation.expr/PandasExprVisitor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/ops.py", "fn_id": "", "content": "class FuncNode:\n\n    def __init__(self, name: str) -> None:\n        if name not in MATHOPS:\n            raise ValueError(f'\"{name}\" is not a supported function')\n        self.name = name\n        self.func = getattr(np, name)\n\n    def __call__(self, *args) -> MathCall:\n        return MathCall(self, args)", "class_fn": true, "question_id": "pandas/pandas.core.computation.ops/FuncNode", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/ops.py", "fn_id": "", "content": "class UnaryOp(Op):\n    \"\"\"\n    Hold a unary operator and its operands.\n\n    Parameters\n    ----------\n    op : str\n        The token used to represent the operator.\n    operand : Term or Op\n        The Term or Op operand to the operator.\n\n    Raises\n    ------\n    ValueError\n        * If no function associated with the passed operator token is found.\n    \"\"\"\n\n    def __call__(self, env) -> MathCall:\n        operand = self.operand(env)\n        return self.func(operand)\n\n    def __iter__(self) -> Iterator:\n        return iter(self.operands)\n\n    def __repr__(self) -> str:\n        return pprint_thing(f'{self.op}({self.operand})')\n\n    @property\n    def return_type(self) -> np.dtype:\n        operand = self.operand\n        if operand.return_type == np.dtype('bool'):\n            return np.dtype('bool')\n        if isinstance(operand, Op) and (operand.op in _cmp_ops_dict or operand.op in _bool_ops_dict):\n            return np.dtype('bool')\n        return np.dtype('int')\n\n    def __init__(self, op: Literal['+', '-', '~', 'not'], operand) -> None:\n        super().__init__(op, (operand,))\n        self.operand = operand\n        try:\n            self.func = _unary_ops_dict[op]\n        except KeyError as err:\n            raise ValueError(f'Invalid unary operator {repr(op)}, valid operators are {UNARY_OPS_SYMS}') from err", "class_fn": true, "question_id": "pandas/pandas.core.computation.ops/UnaryOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/pytables.py", "fn_id": "", "content": "class FilterBinOp(BinOp):\n    filter: tuple[Any, Any, Index] | None = None\n\n    def generate_filter_op(self, invert: bool=False):\n        if self.op == '!=' and (not invert) or (self.op == '==' and invert):\n            return lambda axis, vals: ~axis.isin(vals)\n        else:\n            return lambda axis, vals: axis.isin(vals)\n\n    def format(self):\n        \"\"\"return the actual filter format\"\"\"\n        return [self.filter]\n\n    def __init__(self, op: str, lhs, rhs, queryables: dict[str, Any], encoding) -> None:\n        super().__init__(op, lhs, rhs)\n        self.queryables = queryables\n        self.encoding = encoding\n        self.condition = None\n\n    def convert_values(self) -> None:\n        pass\n\n    def invert(self) -> Self:\n        \"\"\"invert the filter\"\"\"\n        if self.filter is not None:\n            self.filter = (self.filter[0], self.generate_filter_op(invert=True), self.filter[2])\n        return self\n\n    def __repr__(self) -> str:\n        if self.filter is None:\n            return 'Filter: Not Initialized'\n        return pprint_thing(f'[Filter : [{self.filter[0]}] -> [{self.filter[1]}]')\n\n    def _disallow_scalar_only_bool_ops(self) -> None:\n        pass\n\n    def evaluate(self) -> Self | None:\n        if not self.is_valid:\n            raise ValueError(f'query term is not valid [{self}]')\n        rhs = self.conform(self.rhs)\n        values = list(rhs)\n        if self.is_in_table:\n            if self.op in ['==', '!='] and len(values) > self._max_selectors:\n                filter_op = self.generate_filter_op()\n                self.filter = (self.lhs, filter_op, Index(values))\n                return self\n            return None\n        if self.op in ['==', '!=']:\n            filter_op = self.generate_filter_op()\n            self.filter = (self.lhs, filter_op, Index(values))\n        else:\n            raise TypeError(f'passing a filterable condition to a non-table indexer [{self}]')\n        return self", "class_fn": true, "question_id": "pandas/pandas.core.computation.pytables/FilterBinOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/pytables.py", "fn_id": "", "content": "class PyTablesScope(_scope.Scope):\n    __slots__ = ('queryables',)\n    queryables: dict[str, Any]\n\n    def _update(self, level: int) -> None:\n        \"\"\"\n        Update the current scope by going back `level` levels.\n\n        Parameters\n        ----------\n        level : int\n        \"\"\"\n        sl = level + 1\n        stack = inspect.stack()\n        try:\n            self._get_vars(stack[:sl], scopes=['locals'])\n        finally:\n            del stack[:], stack\n\n    def swapkey(self, old_key: str, new_key: str, new_value=None) -> None:\n        \"\"\"\n        Replace a variable name, with a potentially new value.\n\n        Parameters\n        ----------\n        old_key : str\n            Current variable name to replace\n        new_key : str\n            New variable name to replace `old_key` with\n        new_value : object\n            Value to be replaced along with the possible renaming\n        \"\"\"\n        if self.has_resolvers:\n            maps = self.resolvers.maps + self.scope.maps\n        else:\n            maps = self.scope.maps\n        maps.append(self.temps)\n        for mapping in maps:\n            if old_key in mapping:\n                mapping[new_key] = new_value\n                return\n\n    def __init__(self, level: int, global_dict=None, local_dict=None, queryables: dict[str, Any] | None=None) -> None:\n        super().__init__(level + 1, global_dict=global_dict, local_dict=local_dict)\n        self.queryables = queryables or {}\n\n    def add_tmp(self, value) -> str:\n        \"\"\"\n        Add a temporary variable to the scope.\n\n        Parameters\n        ----------\n        value : object\n            An arbitrary object to be assigned to a temporary variable.\n\n        Returns\n        -------\n        str\n            The name of the temporary variable created.\n        \"\"\"\n        name = f'{type(value).__name__}_{self.ntemps}_{_raw_hex_id(self)}'\n        assert name not in self.temps\n        self.temps[name] = value\n        assert name in self.temps\n        return name", "class_fn": true, "question_id": "pandas/pandas.core.computation.pytables/PyTablesScope", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/pytables.py", "fn_id": "", "content": "class UnaryOp(ops.UnaryOp):\n\n    def __call__(self, env) -> MathCall:\n        operand = self.operand(env)\n        return self.func(operand)\n\n    def __repr__(self) -> str:\n        return pprint_thing(f'{self.op}({self.operand})')\n\n    def __init__(self, op: Literal['+', '-', '~', 'not'], operand) -> None:\n        super().__init__(op, (operand,))\n        self.operand = operand\n        try:\n            self.func = _unary_ops_dict[op]\n        except KeyError as err:\n            raise ValueError(f'Invalid unary operator {repr(op)}, valid operators are {UNARY_OPS_SYMS}') from err\n\n    def prune(self, klass):\n        if self.op != '~':\n            raise NotImplementedError('UnaryOp only support invert type ops')\n        operand = self.operand\n        operand = operand.prune(klass)\n        if operand is not None and (issubclass(klass, ConditionBinOp) and operand.condition is not None or (not issubclass(klass, ConditionBinOp) and issubclass(klass, FilterBinOp) and (operand.filter is not None))):\n            return operand.invert()\n        return None", "class_fn": true, "question_id": "pandas/pandas.core.computation.pytables/UnaryOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/dtypes/dtypes.py", "fn_id": "", "content": "class PandasExtensionDtype(ExtensionDtype):\n    \"\"\"\n    A np.dtype duck-typed class, suitable for holding a custom dtype.\n\n    THIS IS NOT A REAL NUMPY DTYPE\n    \"\"\"\n    type: Any\n    kind: Any\n    subdtype = None\n    str: str_type\n    num = 100\n    shape: tuple[int, ...] = ()\n    itemsize = 8\n    base: DtypeObj | None = None\n    isbuiltin = 0\n    isnative = 0\n    _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        \"\"\"\n        Return the common dtype, if one exists.\n\n        Used in `find_common_type` implementation. This is for example used\n        to determine the resulting dtype in a concat operation.\n\n        If no common dtype exists, return None (which gives the other dtypes\n        the chance to determine a common dtype). If all dtypes in the list\n        return None, then the common dtype will be \"object\" dtype (this means\n        it is never needed to return \"object\" dtype from this method itself).\n\n        Parameters\n        ----------\n        dtypes : list of dtypes\n            The dtypes for which to determine a common dtype. This is a list\n            of np.dtype or ExtensionDtype instances.\n\n        Returns\n        -------\n        Common dtype (np.dtype or ExtensionDtype) or None\n        \"\"\"\n        if len(set(dtypes)) == 1:\n            return self\n        else:\n            return None\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"\n        Check whether 'other' is equal to self.\n\n        By default, 'other' is considered equal if either\n\n        * it's a string matching 'self.name'.\n        * it's an instance of this type and all of the attributes\n          in ``self._metadata`` are equal between `self` and `other`.\n\n        Parameters\n        ----------\n        other : Any\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if isinstance(other, str):\n            try:\n                other = self.construct_from_string(other)\n            except TypeError:\n                return False\n        if isinstance(other, type(self)):\n            return all((getattr(self, attr) == getattr(other, attr) for attr in self._metadata))\n        return False\n\n    def __hash__(self) -> int:\n        raise NotImplementedError('sub-classes should implement an __hash__ method')\n\n    def __repr__(self) -> str_type:\n        \"\"\"\n        Return a string representation for a particular object.\n        \"\"\"\n        return str(self)\n\n    @classmethod\n    def reset_cache(cls) -> None:\n        \"\"\"clear the cache\"\"\"\n        cls._cache_dtypes = {}\n\n    def __getstate__(self) -> dict[str_type, Any]:\n        return {k: getattr(self, k, None) for k in self._metadata}\n\n    def __str__(self) -> str:\n        return self.name", "class_fn": true, "question_id": "pandas/pandas.core.dtypes.dtypes/PandasExtensionDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/indexing.py", "fn_id": "", "content": "class GroupByNthSelector:\n    \"\"\"\n    Dynamically substituted for GroupBy.nth to enable both call and index\n    \"\"\"\n\n    def __init__(self, groupby_object: groupby.GroupBy) -> None:\n        self.groupby_object = groupby_object\n\n    def __call__(self, n: PositionalIndexer | tuple, dropna: Literal['any', 'all', None]=None) -> DataFrame | Series:\n        return self.groupby_object._nth(n, dropna)\n\n    def __getitem__(self, n: PositionalIndexer | tuple) -> DataFrame | Series:\n        return self.groupby_object._nth(n)", "class_fn": true, "question_id": "pandas/pandas.core.groupby.indexing/GroupByNthSelector", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/ops.py", "fn_id": "", "content": "class FrameSplitter(DataSplitter):\n\n    def __init__(self, data: NDFrameT, labels: npt.NDArray[np.intp], ngroups: int, *, sort_idx: npt.NDArray[np.intp], sorted_ids: npt.NDArray[np.intp], axis: AxisInt=0) -> None:\n        self.data = data\n        self.labels = ensure_platform_int(labels)\n        self.ngroups = ngroups\n        self._slabels = sorted_ids\n        self._sort_idx = sort_idx\n        self.axis = axis\n        assert isinstance(axis, int), axis\n\n    def _chop(self, sdata: DataFrame, slice_obj: slice) -> DataFrame:\n        mgr = sdata._mgr.get_slice(slice_obj, axis=1 - self.axis)\n        df = sdata._constructor_from_mgr(mgr, axes=mgr.axes)\n        return df.__finalize__(sdata, method='groupby')\n\n    def __iter__(self) -> Iterator:\n        sdata = self._sorted_data\n        if self.ngroups == 0:\n            return\n        (starts, ends) = lib.generate_slices(self._slabels, self.ngroups)\n        for (start, end) in zip(starts, ends):\n            yield self._chop(sdata, slice(start, end))", "class_fn": true, "question_id": "pandas/pandas.core.groupby.ops/FrameSplitter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexers/objects.py", "fn_id": "", "content": "class ExpandingIndexer(BaseIndexer):\n    \"\"\"Calculate expanding window bounds, mimicking df.expanding()\"\"\"\n\n    def __init__(self, index_array: np.ndarray | None=None, window_size: int=0, **kwargs) -> None:\n        self.index_array = index_array\n        self.window_size = window_size\n        for (key, value) in kwargs.items():\n            setattr(self, key, value)\n\n    @Appender(get_window_bounds_doc)\n    def get_window_bounds(self, num_values: int=0, min_periods: int | None=None, center: bool | None=None, closed: str | None=None, step: int | None=None) -> tuple[np.ndarray, np.ndarray]:\n        return (np.zeros(num_values, dtype=np.int64), np.arange(1, num_values + 1, dtype=np.int64))", "class_fn": true, "question_id": "pandas/pandas.core.indexers.objects/ExpandingIndexer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexers/objects.py", "fn_id": "", "content": "class FixedWindowIndexer(BaseIndexer):\n    \"\"\"Creates window boundaries that are of fixed length.\"\"\"\n\n    def __init__(self, index_array: np.ndarray | None=None, window_size: int=0, **kwargs) -> None:\n        self.index_array = index_array\n        self.window_size = window_size\n        for (key, value) in kwargs.items():\n            setattr(self, key, value)\n\n    @Appender(get_window_bounds_doc)\n    def get_window_bounds(self, num_values: int=0, min_periods: int | None=None, center: bool | None=None, closed: str | None=None, step: int | None=None) -> tuple[np.ndarray, np.ndarray]:\n        if center or self.window_size == 0:\n            offset = (self.window_size - 1) // 2\n        else:\n            offset = 0\n        end = np.arange(1 + offset, num_values + 1 + offset, step, dtype='int64')\n        start = end - self.window_size\n        if closed in ['left', 'both']:\n            start -= 1\n        if closed in ['left', 'neither']:\n            end -= 1\n        end = np.clip(end, 0, num_values)\n        start = np.clip(start, 0, num_values)\n        return (start, end)", "class_fn": true, "question_id": "pandas/pandas.core.indexers.objects/FixedWindowIndexer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/accessors.py", "fn_id": "", "content": "@delegate_names(delegate=PeriodArray, accessors=PeriodArray._datetimelike_ops, typ='property')\n@delegate_names(delegate=PeriodArray, accessors=PeriodArray._datetimelike_methods, typ='method')\nclass PeriodProperties(Properties):\n    \"\"\"\n    Accessor object for datetimelike properties of the Series values.\n\n    Returns a Series indexed like the original Series.\n    Raises TypeError if the Series does not contain datetimelike values.\n\n    Examples\n    --------\n    >>> seconds_series = pd.Series(\n    ...     pd.period_range(\n    ...         start=\"2000-01-01 00:00:00\", end=\"2000-01-01 00:00:03\", freq=\"s\"\n    ...     )\n    ... )\n    >>> seconds_series\n    0    2000-01-01 00:00:00\n    1    2000-01-01 00:00:01\n    2    2000-01-01 00:00:02\n    3    2000-01-01 00:00:03\n    dtype: period[s]\n    >>> seconds_series.dt.second\n    0    0\n    1    1\n    2    2\n    3    3\n    dtype: int64\n\n    >>> hours_series = pd.Series(\n    ...     pd.period_range(start=\"2000-01-01 00:00\", end=\"2000-01-01 03:00\", freq=\"h\")\n    ... )\n    >>> hours_series\n    0    2000-01-01 00:00\n    1    2000-01-01 01:00\n    2    2000-01-01 02:00\n    3    2000-01-01 03:00\n    dtype: period[h]\n    >>> hours_series.dt.hour\n    0    0\n    1    1\n    2    2\n    3    3\n    dtype: int64\n\n    >>> quarters_series = pd.Series(\n    ...     pd.period_range(start=\"2000-01-01\", end=\"2000-12-31\", freq=\"Q-DEC\")\n    ... )\n    >>> quarters_series\n    0    2000Q1\n    1    2000Q2\n    2    2000Q3\n    3    2000Q4\n    dtype: period[Q-DEC]\n    >>> quarters_series.dt.quarter\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n    \"\"\"\n\n    def _get_values(self):\n        data = self._parent\n        if lib.is_np_dtype(data.dtype, 'M'):\n            return DatetimeIndex(data, copy=False, name=self.name)\n        elif isinstance(data.dtype, DatetimeTZDtype):\n            return DatetimeIndex(data, copy=False, name=self.name)\n        elif lib.is_np_dtype(data.dtype, 'm'):\n            return TimedeltaIndex(data, copy=False, name=self.name)\n        elif isinstance(data.dtype, PeriodDtype):\n            return PeriodArray(data, copy=False)\n        raise TypeError(f'cannot convert an object of type {type(data)} to a datetimelike index')\n\n    def _delegate_property_set(self, name: str, value, *args, **kwargs):\n        raise ValueError('modifications to a property of a datetimelike object are not supported. Change values on the original.')\n\n    def _delegate_property_get(self, name: str):\n        from pandas import Series\n        values = self._get_values()\n        result = getattr(values, name)\n        if isinstance(result, np.ndarray):\n            if is_integer_dtype(result):\n                result = result.astype('int64')\n        elif not is_list_like(result):\n            return result\n        result = np.asarray(result)\n        if self.orig is not None:\n            index = self.orig.index\n        else:\n            index = self._parent.index\n        result = Series(result, index=index, name=self.name).__finalize__(self._parent)\n        result._is_copy = 'modifications to a property of a datetimelike object are not supported and are discarded. Change values on the original.'\n        return result", "class_fn": true, "question_id": "pandas/pandas.core.indexes.accessors/PeriodProperties", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/multi.py", "fn_id": "", "content": "class MultiIndexPyIntEngine(libindex.BaseMultiIndexCodesEngine, libindex.ObjectEngine):\n    \"\"\"\n    This class manages those (extreme) cases in which the number of possible\n    label combinations overflows the 64 bits integers, and uses an ObjectEngine\n    containing Python integers.\n    \"\"\"\n    _base = libindex.ObjectEngine\n\n    def _codes_to_ints(self, codes):\n        \"\"\"\n        Transform combination(s) of uint64 in one Python integer (each), in a\n        strictly monotonic way (i.e. respecting the lexicographic order of\n        integer combinations): see BaseMultiIndexCodesEngine documentation.\n\n        Parameters\n        ----------\n        codes : 1- or 2-dimensional array of dtype uint64\n            Combinations of integers (one per row)\n\n        Returns\n        -------\n        int, or 1-dimensional array of dtype object\n            Integer(s) representing one combination (each).\n        \"\"\"\n        codes = codes.astype('object') << self.offsets\n        if codes.ndim == 1:\n            return np.bitwise_or.reduce(codes)\n        return np.bitwise_or.reduce(codes, axis=1)", "class_fn": true, "question_id": "pandas/pandas.core.indexes.multi/MultiIndexPyIntEngine", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexing.py", "fn_id": "", "content": "class _ScalarAccessIndexer(NDFrameIndexerBase):\n    \"\"\"\n    Access scalars quickly.\n    \"\"\"\n    _takeable: bool\n\n    def _convert_key(self, key):\n        raise AbstractMethodError(self)\n\n    def __getitem__(self, key):\n        if not isinstance(key, tuple):\n            if not is_list_like_indexer(key):\n                key = (key,)\n            else:\n                raise ValueError('Invalid call for scalar access (getting)!')\n        key = self._convert_key(key)\n        return self.obj._get_value(*key, takeable=self._takeable)\n\n    def __setitem__(self, key, value) -> None:\n        if isinstance(key, tuple):\n            key = tuple((com.apply_if_callable(x, self.obj) for x in key))\n        else:\n            key = com.apply_if_callable(key, self.obj)\n        if not isinstance(key, tuple):\n            key = _tuplify(self.ndim, key)\n        key = list(self._convert_key(key))\n        if len(key) != self.ndim:\n            raise ValueError('Not enough indexers for scalar access (setting)!')\n        self.obj._set_value(*key, value=value, takeable=self._takeable)", "class_fn": true, "question_id": "pandas/pandas.core.indexing/_ScalarAccessIndexer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/interchange/buffer.py", "fn_id": "", "content": "class PandasBufferPyarrow(Buffer):\n    \"\"\"\n    Data in the buffer is guaranteed to be contiguous in memory.\n    \"\"\"\n\n    def __init__(self, buffer: pa.Buffer, *, length: int) -> None:\n        \"\"\"\n        Handle pyarrow chunked arrays.\n        \"\"\"\n        self._buffer = buffer\n        self._length = length\n\n    @property\n    def bufsize(self) -> int:\n        \"\"\"\n        Buffer size in bytes.\n        \"\"\"\n        return self._buffer.size\n\n    @property\n    def ptr(self) -> int:\n        \"\"\"\n        Pointer to start of the buffer as an integer.\n        \"\"\"\n        return self._buffer.address\n\n    def __dlpack__(self) -> Any:\n        \"\"\"\n        Represent this structure as DLPack interface.\n        \"\"\"\n        raise NotImplementedError()\n\n    def __dlpack_device__(self) -> tuple[DlpackDeviceType, int | None]:\n        \"\"\"\n        Device type and device ID for where the data in the buffer resides.\n        \"\"\"\n        return (DlpackDeviceType.CPU, None)\n\n    def __repr__(self) -> str:\n        return 'PandasBuffer[pyarrow](' + str({'bufsize': self.bufsize, 'ptr': self.ptr, 'device': 'CPU'}) + ')'", "class_fn": true, "question_id": "pandas/pandas.core.interchange.buffer/PandasBufferPyarrow", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/interchange/dataframe_protocol.py", "fn_id": "", "content": "class ColumnBuffers(TypedDict):\n    data: tuple[Buffer, Any]\n    validity: tuple[Buffer, Any] | None\n    offsets: tuple[Buffer, Any] | None", "class_fn": true, "question_id": "pandas/pandas.core.interchange.dataframe_protocol/ColumnBuffers", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/interchange/dataframe_protocol.py", "fn_id": "", "content": "class DtypeKind(enum.IntEnum):\n    \"\"\"\n    Integer enum for data types.\n\n    Attributes\n    ----------\n    INT : int\n        Matches to signed integer data type.\n    UINT : int\n        Matches to unsigned integer data type.\n    FLOAT : int\n        Matches to floating point data type.\n    BOOL : int\n        Matches to boolean data type.\n    STRING : int\n        Matches to string data type (UTF-8 encoded).\n    DATETIME : int\n        Matches to datetime data type.\n    CATEGORICAL : int\n        Matches to categorical data type.\n    \"\"\"\n    INT = 0\n    UINT = 1\n    FLOAT = 2\n    BOOL = 20\n    STRING = 21\n    DATETIME = 22\n    CATEGORICAL = 23", "class_fn": true, "question_id": "pandas/pandas.core.interchange.dataframe_protocol/DtypeKind", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/internals/array_manager.py", "fn_id": "", "content": "class NullArrayProxy:\n    \"\"\"\n    Proxy object for an all-NA array.\n\n    Only stores the length of the array, and not the dtype. The dtype\n    will only be known when actually concatenating (after determining the\n    common dtype, for which this proxy is ignored).\n    Using this object avoids that the internals/concat.py needs to determine\n    the proper dtype and array type.\n    \"\"\"\n    ndim = 1\n\n    def __init__(self, n: int) -> None:\n        self.n = n\n\n    @property\n    def shape(self) -> tuple[int]:\n        return (self.n,)\n\n    def to_array(self, dtype: DtypeObj) -> ArrayLike:\n        \"\"\"\n        Helper function to create the actual all-NA array from the NullArrayProxy\n        object.\n\n        Parameters\n        ----------\n        arr : NullArrayProxy\n        dtype : the dtype for the resulting array\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n        \"\"\"\n        if isinstance(dtype, ExtensionDtype):\n            empty = dtype.construct_array_type()._from_sequence([], dtype=dtype)\n            indexer = -np.ones(self.n, dtype=np.intp)\n            return empty.take(indexer, allow_fill=True)\n        else:\n            dtype = ensure_dtype_can_hold_na(dtype)\n            fill_value = na_value_for_dtype(dtype)\n            arr = np.empty(self.n, dtype=dtype)\n            arr.fill(fill_value)\n            return ensure_wrapped_if_datetimelike(arr)", "class_fn": true, "question_id": "pandas/pandas.core.internals.array_manager/NullArrayProxy", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/internals/blocks.py", "fn_id": "", "content": "class DatetimeLikeBlock(NDArrayBackedExtensionBlock):\n    \"\"\"Block for datetime64[ns], timedelta64[ns].\"\"\"\n    __slots__ = ()\n    is_numeric = False\n    values: DatetimeArray | TimedeltaArray", "class_fn": true, "question_id": "pandas/pandas.core.internals.blocks/DatetimeLikeBlock", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/internals/blocks.py", "fn_id": "", "content": "class NumpyBlock(Block):\n    values: np.ndarray\n    __slots__ = ()\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\"return a boolean if I am possibly a view\"\"\"\n        return self.values.base is not None\n\n    @property\n    def array_values(self) -> ExtensionArray:\n        return NumpyExtensionArray(self.values)\n\n    def iget(self, i: int | tuple[int, int] | tuple[slice, int]) -> np.ndarray:\n        return self.values[i]\n\n    @cache_readonly\n    def is_numeric(self) -> bool:\n        dtype = self.values.dtype\n        kind = dtype.kind\n        return kind in 'fciub'\n\n    @final\n    def convert(self, *, copy: bool=True, using_cow: bool=False) -> list[Block]:\n        \"\"\"\n        Attempt to coerce any object types to better types. Return a copy\n        of the block (if copy = True).\n        \"\"\"\n        if not self.is_object:\n            if not copy and using_cow:\n                return [self.copy(deep=False)]\n            return [self.copy()] if copy else [self]\n        if self.ndim != 1 and self.shape[0] != 1:\n            blocks = self.split_and_operate(Block.convert, copy=copy, using_cow=using_cow)\n            if all((blk.dtype.kind == 'O' for blk in blocks)):\n                if using_cow:\n                    return [self.copy(deep=False)]\n                return [self.copy()] if copy else [self]\n            return blocks\n        values = self.values\n        if values.ndim == 2:\n            values = values[0]\n        res_values = lib.maybe_convert_objects(values, convert_non_numeric=True)\n        refs = None\n        if copy and res_values is values:\n            res_values = values.copy()\n        elif res_values is values:\n            refs = self.refs\n        res_values = ensure_block_shape(res_values, self.ndim)\n        res_values = maybe_coerce_values(res_values)\n        return [self.make_block(res_values, refs=refs)]\n\n    def get_values(self, dtype: DtypeObj | None=None) -> np.ndarray:\n        if dtype == _dtype_obj:\n            return self.values.astype(_dtype_obj)\n        return self.values\n\n    @final\n    def slice_block_columns(self, slc: slice) -> Self:\n        \"\"\"\n        Perform __getitem__-like, return result as block.\n        \"\"\"\n        new_mgr_locs = self._mgr_locs[slc]\n        new_values = self._slice(slc)\n        refs = self.refs\n        return type(self)(new_values, new_mgr_locs, self.ndim, refs=refs)", "class_fn": true, "question_id": "pandas/pandas.core.internals.blocks/NumpyBlock", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/methods/selectn.py", "fn_id": "", "content": "class SelectN:\n\n    def __init__(self, obj, n: int, keep: str) -> None:\n        self.obj = obj\n        self.n = n\n        self.keep = keep\n        if self.keep not in ('first', 'last', 'all'):\n            raise ValueError('keep must be either \"first\", \"last\" or \"all\"')\n\n    def compute(self, method: str) -> DataFrame | Series:\n        raise NotImplementedError\n\n    @final\n    def nlargest(self):\n        return self.compute('nlargest')\n\n    @final\n    def nsmallest(self):\n        return self.compute('nsmallest')\n\n    @final\n    @staticmethod\n    def is_valid_dtype_n_method(dtype: DtypeObj) -> bool:\n        \"\"\"\n        Helper function to determine if dtype is valid for\n        nsmallest/nlargest methods\n        \"\"\"\n        if is_numeric_dtype(dtype):\n            return not is_complex_dtype(dtype)\n        return needs_i8_conversion(dtype)", "class_fn": true, "question_id": "pandas/pandas.core.methods.selectn/SelectN", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/resample.py", "fn_id": "", "content": "class DatetimeIndexResamplerGroupby(_GroupByMixin, DatetimeIndexResampler):\n    \"\"\"\n    Provides a resample of a groupby implementation\n    \"\"\"\n\n    @property\n    def _resampler_cls(self):\n        return DatetimeIndexResampler\n\n    def _get_binner_for_time(self):\n        if self.kind == 'period':\n            return self._timegrouper._get_time_period_bins(self.ax)\n        return self._timegrouper._get_time_bins(self.ax)\n\n    @final\n    def _gotitem(self, key, ndim, subset=None):\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        if subset is None:\n            subset = self.obj\n            if key is not None:\n                subset = subset[key]\n            else:\n                assert subset.ndim == 1\n        try:\n            if isinstance(key, list) and self.key not in key and (self.key is not None):\n                key.append(self.key)\n            groupby = self._groupby[key]\n        except IndexError:\n            groupby = self._groupby\n        selection = self._infer_selection(key, subset)\n        new_rs = type(self)(groupby=groupby, parent=cast(Resampler, self), selection=selection)\n        return new_rs\n\n    @no_type_check\n    def _apply(self, f, *args, **kwargs):\n        \"\"\"\n        Dispatch to _upsample; we are stripping all of the _upsample kwargs and\n        performing the original function call on the grouped object.\n        \"\"\"\n\n        def func(x):\n            x = self._resampler_cls(x, timegrouper=self._timegrouper, gpr_index=self.ax)\n            if isinstance(f, str):\n                return getattr(x, f)(**kwargs)\n            return x.apply(f, *args, **kwargs)\n        result = _apply(self._groupby, func, include_groups=self.include_groups)\n        return self._wrap_result(result)", "class_fn": true, "question_id": "pandas/pandas.core.resample/DatetimeIndexResamplerGroupby", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/resample.py", "fn_id": "", "content": "class TimedeltaIndexResamplerGroupby(_GroupByMixin, TimedeltaIndexResampler):\n    \"\"\"\n    Provides a resample of a groupby implementation.\n    \"\"\"\n\n    @property\n    def _resampler_cls(self):\n        return TimedeltaIndexResampler\n\n    def _adjust_binner_for_upsample(self, binner):\n        \"\"\"\n        Adjust our binner when upsampling.\n\n        The range of a new index is allowed to be greater than original range\n        so we don't need to change the length of a binner, GH 13022\n        \"\"\"\n        return binner\n\n    def _get_binner_for_time(self):\n        return self._timegrouper._get_time_delta_bins(self.ax)\n\n    @final\n    def _gotitem(self, key, ndim, subset=None):\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        if subset is None:\n            subset = self.obj\n            if key is not None:\n                subset = subset[key]\n            else:\n                assert subset.ndim == 1\n        try:\n            if isinstance(key, list) and self.key not in key and (self.key is not None):\n                key.append(self.key)\n            groupby = self._groupby[key]\n        except IndexError:\n            groupby = self._groupby\n        selection = self._infer_selection(key, subset)\n        new_rs = type(self)(groupby=groupby, parent=cast(Resampler, self), selection=selection)\n        return new_rs", "class_fn": true, "question_id": "pandas/pandas.core.resample/TimedeltaIndexResamplerGroupby", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/window/expanding.py", "fn_id": "", "content": "class ExpandingGroupby(BaseWindowGroupby, Expanding):\n    \"\"\"\n    Provide a expanding groupby implementation.\n    \"\"\"\n    _attributes = Expanding._attributes + BaseWindowGroupby._attributes\n\n    @doc(template_header, create_section_header('Parameters'), dedent('\\n        ddof : int, default 1\\n            Delta Degrees of Freedom.  The divisor used in calculations\\n            is ``N - ddof``, where ``N`` represents the number of elements.\\n\\n        ').replace('\\n', '', 1), kwargs_numeric_only, window_agg_numba_parameters('1.4'), create_section_header('Returns'), template_returns, create_section_header('See Also'), 'numpy.std : Equivalent method for NumPy array.\\n', template_see_also, create_section_header('Notes'), dedent('\\n        The default ``ddof`` of 1 used in :meth:`Series.std` is different\\n        than the default ``ddof`` of 0 in :func:`numpy.std`.\\n\\n        A minimum of one period is required for the rolling calculation.\\n\\n        ').replace('\\n', '', 1), create_section_header('Examples'), dedent('\\n        >>> s = pd.Series([5, 5, 6, 7, 5, 5, 5])\\n\\n        >>> s.expanding(3).std()\\n        0         NaN\\n        1         NaN\\n        2    0.577350\\n        3    0.957427\\n        4    0.894427\\n        5    0.836660\\n        6    0.786796\\n        dtype: float64\\n        ').replace('\\n', '', 1), window_method='expanding', aggregation_description='standard deviation', agg_method='std')\n    def std(self, ddof: int=1, numeric_only: bool=False, engine: Literal['cython', 'numba'] | None=None, engine_kwargs: dict[str, bool] | None=None):\n        return super().std(ddof=ddof, numeric_only=numeric_only, engine=engine, engine_kwargs=engine_kwargs)\n\n    def _get_window_indexer(self) -> GroupbyIndexer:\n        \"\"\"\n        Return an indexer class that will compute the window start and end bounds\n\n        Returns\n        -------\n        GroupbyIndexer\n        \"\"\"\n        window_indexer = GroupbyIndexer(groupby_indices=self._grouper.indices, window_indexer=ExpandingIndexer)\n        return window_indexer\n\n    def _apply_pairwise(self, target: DataFrame | Series, other: DataFrame | Series | None, pairwise: bool | None, func: Callable[[DataFrame | Series, DataFrame | Series], DataFrame | Series], numeric_only: bool) -> DataFrame | Series:\n        \"\"\"\n        Apply the given pairwise function given 2 pandas objects (DataFrame/Series)\n        \"\"\"\n        target = target.drop(columns=self._grouper.names, errors='ignore')\n        result = super()._apply_pairwise(target, other, pairwise, func, numeric_only)\n        if other is not None and (not all((len(group) == len(other) for group in self._grouper.indices.values()))):\n            old_result_len = len(result)\n            result = concat([result.take(gb_indices).reindex(result.index) for gb_indices in self._grouper.indices.values()])\n            gb_pairs = (com.maybe_make_list(pair) for pair in self._grouper.indices.keys())\n            groupby_codes = []\n            groupby_levels = []\n            for gb_level_pair in map(list, zip(*gb_pairs)):\n                labels = np.repeat(np.array(gb_level_pair), old_result_len)\n                (codes, levels) = factorize(labels)\n                groupby_codes.append(codes)\n                groupby_levels.append(levels)\n        else:\n            groupby_codes = self._grouper.codes\n            groupby_levels = self._grouper.levels\n            group_indices = self._grouper.indices.values()\n            if group_indices:\n                indexer = np.concatenate(list(group_indices))\n            else:\n                indexer = np.array([], dtype=np.intp)\n            if target.ndim == 1:\n                repeat_by = 1\n            else:\n                repeat_by = len(target.columns)\n            groupby_codes = [np.repeat(c.take(indexer), repeat_by) for c in groupby_codes]\n        if isinstance(result.index, MultiIndex):\n            result_codes = list(result.index.codes)\n            result_levels = list(result.index.levels)\n            result_names = list(result.index.names)\n        else:\n            (idx_codes, idx_levels) = factorize(result.index)\n            result_codes = [idx_codes]\n            result_levels = [idx_levels]\n            result_names = [result.index.name]\n        result_codes = groupby_codes + result_codes\n        result_levels = groupby_levels + result_levels\n        result_names = self._grouper.names + result_names\n        result_index = MultiIndex(result_levels, result_codes, names=result_names, verify_integrity=False)\n        result.index = result_index\n        return result\n\n    def _apply(self, func: Callable[..., Any], name: str, numeric_only: bool=False, numba_args: tuple[Any, ...]=(), **kwargs) -> DataFrame | Series:\n        result = super()._apply(func, name, numeric_only, numba_args, **kwargs)\n        grouped_object_index = self.obj.index\n        grouped_index_name = [*grouped_object_index.names]\n        groupby_keys = copy.copy(self._grouper.names)\n        result_index_names = groupby_keys + grouped_index_name\n        drop_columns = [key for key in self._grouper.names if key not in self.obj.index.names or key is None]\n        if len(drop_columns) != len(groupby_keys):\n            result = result.drop(columns=drop_columns, errors='ignore')\n        codes = self._grouper.codes\n        levels = copy.copy(self._grouper.levels)\n        group_indices = self._grouper.indices.values()\n        if group_indices:\n            indexer = np.concatenate(list(group_indices))\n        else:\n            indexer = np.array([], dtype=np.intp)\n        codes = [c.take(indexer) for c in codes]\n        if grouped_object_index is not None:\n            idx = grouped_object_index.take(indexer)\n            if not isinstance(idx, MultiIndex):\n                idx = MultiIndex.from_arrays([idx])\n            codes.extend(list(idx.codes))\n            levels.extend(list(idx.levels))\n        result_index = MultiIndex(levels, codes, names=result_index_names, verify_integrity=False)\n        result.index = result_index\n        if not self._as_index:\n            result = result.reset_index(level=list(range(len(groupby_keys))))\n        return result", "class_fn": true, "question_id": "pandas/pandas.core.window.expanding/ExpandingGroupby", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/errors/__init__.py", "fn_id": "", "content": "class PyperclipWindowsException(PyperclipException):\n    \"\"\"\n    Exception raised when clipboard functionality is unsupported by Windows.\n\n    Access to the clipboard handle would be denied due to some other\n    window process is accessing it.\n    \"\"\"\n\n    def __init__(self, message: str) -> None:\n        message += f' ({ctypes.WinError()})'\n        super().__init__(message)", "class_fn": true, "question_id": "pandas/pandas.errors/PyperclipWindowsException", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/common.py", "fn_id": "", "content": "@dataclasses.dataclass\nclass IOArgs:\n    \"\"\"\n    Return value of io/common.py:_get_filepath_or_buffer.\n    \"\"\"\n    filepath_or_buffer: str | BaseBuffer\n    encoding: str\n    mode: str\n    compression: CompressionDict\n    should_close: bool = False", "class_fn": true, "question_id": "pandas/pandas.io.common/IOArgs", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/common.py", "fn_id": "", "content": "class _BytesIOWrapper:\n\n    def __init__(self, buffer: StringIO | TextIOBase, encoding: str='utf-8') -> None:\n        self.buffer = buffer\n        self.encoding = encoding\n        self.overflow = b''\n\n    def __getattr__(self, attr: str):\n        return getattr(self.buffer, attr)\n\n    def read(self, n: int | None=-1) -> bytes:\n        assert self.buffer is not None\n        bytestring = self.buffer.read(n).encode(self.encoding)\n        combined_bytestring = self.overflow + bytestring\n        if n is None or n < 0 or n >= len(combined_bytestring):\n            self.overflow = b''\n            return combined_bytestring\n        else:\n            to_return = combined_bytestring[:n]\n            self.overflow = combined_bytestring[n:]\n            return to_return", "class_fn": true, "question_id": "pandas/pandas.io.common/_BytesIOWrapper", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/common.py", "fn_id": "", "content": "class _IOWrapper:\n\n    def __init__(self, buffer: BaseBuffer) -> None:\n        self.buffer = buffer\n\n    def __getattr__(self, name: str):\n        return getattr(self.buffer, name)\n\n    def readable(self) -> bool:\n        if hasattr(self.buffer, 'readable'):\n            return self.buffer.readable()\n        return True\n\n    def seekable(self) -> bool:\n        if hasattr(self.buffer, 'seekable'):\n            return self.buffer.seekable()\n        return True\n\n    def writable(self) -> bool:\n        if hasattr(self.buffer, 'writable'):\n            return self.buffer.writable()\n        return True", "class_fn": true, "question_id": "pandas/pandas.io.common/_IOWrapper", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/format.py", "fn_id": "", "content": "class _Datetime64Formatter(_GenericArrayFormatter):\n    values: DatetimeArray\n\n    def get_result(self) -> list[str]:\n        fmt_values = self._format_strings()\n        return _make_fixed_width(fmt_values, self.justify)\n\n    def __init__(self, values: DatetimeArray, nat_rep: str='NaT', date_format: None=None, **kwargs) -> None:\n        super().__init__(values, **kwargs)\n        self.nat_rep = nat_rep\n        self.date_format = date_format\n\n    def _format_strings(self) -> list[str]:\n        \"\"\"we by definition have DO NOT have a TZ\"\"\"\n        values = self.values\n        if self.formatter is not None:\n            return [self.formatter(x) for x in values]\n        fmt_values = values._format_native_types(na_rep=self.nat_rep, date_format=self.date_format)\n        return fmt_values.tolist()", "class_fn": true, "question_id": "pandas/pandas.io.formats.format/_Datetime64Formatter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/format.py", "fn_id": "", "content": "class _IntArrayFormatter(_GenericArrayFormatter):\n\n    def __init__(self, values: ArrayLike, digits: int=7, formatter: Callable | None=None, na_rep: str='NaN', space: str | int=12, float_format: FloatFormatType | None=None, justify: str='right', decimal: str='.', quoting: int | None=None, fixed_width: bool=True, leading_space: bool | None=True, fallback_formatter: Callable | None=None) -> None:\n        self.values = values\n        self.digits = digits\n        self.na_rep = na_rep\n        self.space = space\n        self.formatter = formatter\n        self.float_format = float_format\n        self.justify = justify\n        self.decimal = decimal\n        self.quoting = quoting\n        self.fixed_width = fixed_width\n        self.leading_space = leading_space\n        self.fallback_formatter = fallback_formatter\n\n    def get_result(self) -> list[str]:\n        fmt_values = self._format_strings()\n        return _make_fixed_width(fmt_values, self.justify)\n\n    def _format_strings(self) -> list[str]:\n        if self.leading_space is False:\n            formatter_str = lambda x: f'{x:d}'.format(x=x)\n        else:\n            formatter_str = lambda x: f'{x: d}'.format(x=x)\n        formatter = self.formatter or formatter_str\n        fmt_values = [formatter(x) for x in self.values]\n        return fmt_values", "class_fn": true, "question_id": "pandas/pandas.io.formats.format/_IntArrayFormatter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/info.py", "fn_id": "", "content": "class DataFrameInfo(_BaseInfo):\n    \"\"\"\n    Class storing dataframe-specific info.\n    \"\"\"\n\n    def __init__(self, data: DataFrame, memory_usage: bool | str | None=None) -> None:\n        self.data: DataFrame = data\n        self.memory_usage = _initialize_memory_usage(memory_usage)\n\n    @property\n    def dtype_counts(self) -> Mapping[str, int]:\n        return _get_dataframe_dtype_counts(self.data)\n\n    @property\n    def dtypes(self) -> Iterable[Dtype]:\n        \"\"\"\n        Dtypes.\n\n        Returns\n        -------\n        dtypes\n            Dtype of each of the DataFrame's columns.\n        \"\"\"\n        return self.data.dtypes\n\n    @property\n    def ids(self) -> Index:\n        \"\"\"\n        Column names.\n\n        Returns\n        -------\n        ids : Index\n            DataFrame's column names.\n        \"\"\"\n        return self.data.columns\n\n    @property\n    def col_count(self) -> int:\n        \"\"\"Number of columns to be summarized.\"\"\"\n        return len(self.ids)\n\n    @property\n    def non_null_counts(self) -> Sequence[int]:\n        \"\"\"Sequence of non-null counts for all columns or column (if series).\"\"\"\n        return self.data.count()\n\n    @property\n    def memory_usage_bytes(self) -> int:\n        deep = self.memory_usage == 'deep'\n        return self.data.memory_usage(index=True, deep=deep).sum()\n\n    def render(self, *, buf: WriteBuffer[str] | None, max_cols: int | None, verbose: bool | None, show_counts: bool | None) -> None:\n        printer = _DataFrameInfoPrinter(info=self, max_cols=max_cols, verbose=verbose, show_counts=show_counts)\n        printer.to_buffer(buf)", "class_fn": true, "question_id": "pandas/pandas.io.formats.info/DataFrameInfo", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/info.py", "fn_id": "", "content": "class _DataFrameTableBuilderNonVerbose(_DataFrameTableBuilder):\n    \"\"\"\n    Dataframe info table builder for non-verbose output.\n    \"\"\"\n\n    def _fill_non_empty_info(self) -> None:\n        \"\"\"Add lines to the info table, pertaining to non-empty dataframe.\"\"\"\n        self.add_object_type_line()\n        self.add_index_range_line()\n        self.add_columns_summary_line()\n        self.add_dtypes_line()\n        if self.display_memory_usage:\n            self.add_memory_usage_line()\n\n    def _fill_empty_info(self) -> None:\n        \"\"\"Add lines to the info table, pertaining to empty dataframe.\"\"\"\n        self.add_object_type_line()\n        self.add_index_range_line()\n        self._lines.append(f'Empty {type(self.data).__name__}\\n')\n\n    def get_lines(self) -> list[str]:\n        self._lines = []\n        if self.col_count == 0:\n            self._fill_empty_info()\n        else:\n            self._fill_non_empty_info()\n        return self._lines\n\n    def __init__(self, *, info: DataFrameInfo) -> None:\n        self.info: DataFrameInfo = info\n\n    def add_columns_summary_line(self) -> None:\n        self._lines.append(self.ids._summary(name='Columns'))", "class_fn": true, "question_id": "pandas/pandas.io.formats.info/_DataFrameTableBuilderNonVerbose", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/info.py", "fn_id": "", "content": "class _SeriesTableBuilder(_TableBuilderAbstract):\n    \"\"\"\n    Abstract builder for series info table.\n\n    Parameters\n    ----------\n    info : SeriesInfo.\n        Instance of SeriesInfo.\n    \"\"\"\n\n    def add_object_type_line(self) -> None:\n        \"\"\"Add line with string representation of dataframe to the table.\"\"\"\n        self._lines.append(str(type(self.data)))\n\n    @abstractmethod\n    def _fill_non_empty_info(self) -> None:\n        \"\"\"Add lines to the info table, pertaining to non-empty series.\"\"\"\n\n    @property\n    def data(self) -> Series:\n        \"\"\"Series.\"\"\"\n        return self.info.data\n\n    def add_index_range_line(self) -> None:\n        \"\"\"Add line with range of indices to the table.\"\"\"\n        self._lines.append(self.data.index._summary())\n\n    def add_dtypes_line(self) -> None:\n        \"\"\"Add summary line with dtypes present in dataframe.\"\"\"\n        collected_dtypes = [f'{key}({val:d})' for (key, val) in sorted(self.dtype_counts.items())]\n        self._lines.append(f\"dtypes: {', '.join(collected_dtypes)}\")\n\n    def add_memory_usage_line(self) -> None:\n        \"\"\"Add line containing memory usage.\"\"\"\n        self._lines.append(f'memory usage: {self.memory_usage_string}')\n\n    def get_lines(self) -> list[str]:\n        self._lines = []\n        self._fill_non_empty_info()\n        return self._lines\n\n    def __init__(self, *, info: SeriesInfo) -> None:\n        self.info: SeriesInfo = info", "class_fn": true, "question_id": "pandas/pandas.io.formats.info/_SeriesTableBuilder", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/formats/info.py", "fn_id": "", "content": "class _TableBuilderAbstract(ABC):\n    \"\"\"\n    Abstract builder for info table.\n    \"\"\"\n    _lines: list[str]\n    info: _BaseInfo\n\n    @abstractmethod\n    def get_lines(self) -> list[str]:\n        \"\"\"Product in a form of list of lines (strings).\"\"\"\n\n    @property\n    def data(self) -> DataFrame | Series:\n        return self.info.data\n\n    @property\n    def dtypes(self) -> Iterable[Dtype]:\n        \"\"\"Dtypes of each of the DataFrame's columns.\"\"\"\n        return self.info.dtypes\n\n    @property\n    def dtype_counts(self) -> Mapping[str, int]:\n        \"\"\"Mapping dtype - number of counts.\"\"\"\n        return self.info.dtype_counts\n\n    @property\n    def display_memory_usage(self) -> bool:\n        \"\"\"Whether to display memory usage.\"\"\"\n        return bool(self.info.memory_usage)\n\n    @property\n    def memory_usage_string(self) -> str:\n        \"\"\"Memory usage string with proper size qualifier.\"\"\"\n        return self.info.memory_usage_string\n\n    @property\n    def non_null_counts(self) -> Sequence[int]:\n        return self.info.non_null_counts\n\n    def add_object_type_line(self) -> None:\n        \"\"\"Add line with string representation of dataframe to the table.\"\"\"\n        self._lines.append(str(type(self.data)))\n\n    def add_index_range_line(self) -> None:\n        \"\"\"Add line with range of indices to the table.\"\"\"\n        self._lines.append(self.data.index._summary())\n\n    def add_dtypes_line(self) -> None:\n        \"\"\"Add summary line with dtypes present in dataframe.\"\"\"\n        collected_dtypes = [f'{key}({val:d})' for (key, val) in sorted(self.dtype_counts.items())]\n        self._lines.append(f\"dtypes: {', '.join(collected_dtypes)}\")", "class_fn": true, "question_id": "pandas/pandas.io.formats.info/_TableBuilderAbstract", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/json/_json.py", "fn_id": "", "content": "class FrameWriter(Writer):\n    _default_orient = 'columns'\n\n    @property\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        if not self.index and self.orient == 'split':\n            obj_to_write = self.obj.to_dict(orient='split')\n            del obj_to_write['index']\n        else:\n            obj_to_write = self.obj\n        return obj_to_write\n\n    def write(self) -> str:\n        iso_dates = self.date_format == 'iso'\n        return ujson_dumps(self.obj_to_write, orient=self.orient, double_precision=self.double_precision, ensure_ascii=self.ensure_ascii, date_unit=self.date_unit, iso_dates=iso_dates, default_handler=self.default_handler, indent=self.indent)\n\n    def __init__(self, obj: NDFrame, orient: str | None, date_format: str, double_precision: int, ensure_ascii: bool, date_unit: str, index: bool, default_handler: Callable[[Any], JSONSerializable] | None=None, indent: int=0) -> None:\n        self.obj = obj\n        if orient is None:\n            orient = self._default_orient\n        self.orient = orient\n        self.date_format = date_format\n        self.double_precision = double_precision\n        self.ensure_ascii = ensure_ascii\n        self.date_unit = date_unit\n        self.default_handler = default_handler\n        self.index = index\n        self.indent = indent\n        self.is_copy = None\n        self._format_axes()\n\n    def _format_axes(self) -> None:\n        \"\"\"\n        Try to format axes if they are datelike.\n        \"\"\"\n        if not self.obj.index.is_unique and self.orient in ('index', 'columns'):\n            raise ValueError(f\"DataFrame index must be unique for orient='{self.orient}'.\")\n        if not self.obj.columns.is_unique and self.orient in ('index', 'columns', 'records'):\n            raise ValueError(f\"DataFrame columns must be unique for orient='{self.orient}'.\")", "class_fn": true, "question_id": "pandas/pandas.io.json._json/FrameWriter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/json/_json.py", "fn_id": "", "content": "class Writer(ABC):\n    _default_orient: str\n\n    def __init__(self, obj: NDFrame, orient: str | None, date_format: str, double_precision: int, ensure_ascii: bool, date_unit: str, index: bool, default_handler: Callable[[Any], JSONSerializable] | None=None, indent: int=0) -> None:\n        self.obj = obj\n        if orient is None:\n            orient = self._default_orient\n        self.orient = orient\n        self.date_format = date_format\n        self.double_precision = double_precision\n        self.ensure_ascii = ensure_ascii\n        self.date_unit = date_unit\n        self.default_handler = default_handler\n        self.index = index\n        self.indent = indent\n        self.is_copy = None\n        self._format_axes()\n\n    def _format_axes(self) -> None:\n        raise AbstractMethodError(self)\n\n    def write(self) -> str:\n        iso_dates = self.date_format == 'iso'\n        return ujson_dumps(self.obj_to_write, orient=self.orient, double_precision=self.double_precision, ensure_ascii=self.ensure_ascii, date_unit=self.date_unit, iso_dates=iso_dates, default_handler=self.default_handler, indent=self.indent)\n\n    @property\n    @abstractmethod\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        \"\"\"Object to write in JSON format.\"\"\"", "class_fn": true, "question_id": "pandas/pandas.io.json._json/Writer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/parsers/readers.py", "fn_id": "", "content": "class _Fwf_Defaults(TypedDict):\n    colspecs: Literal['infer']\n    infer_nrows: Literal[100]\n    widths: None", "class_fn": true, "question_id": "pandas/pandas.io.parsers.readers/_Fwf_Defaults", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/pytables.py", "fn_id": "", "content": "class AppendableSeriesTable(AppendableFrameTable):\n    \"\"\"support the new appendable table formats\"\"\"\n    pandas_kind = 'series_table'\n    table_type = 'appendable_series'\n    ndim = 2\n    obj_type = Series\n\n    @property\n    def is_transposed(self) -> bool:\n        return False\n\n    @classmethod\n    def get_object(cls, obj, transposed: bool):\n        return obj\n\n    def write(self, obj, data_columns=None, **kwargs) -> None:\n        \"\"\"we are going to write this as a frame table\"\"\"\n        if not isinstance(obj, DataFrame):\n            name = obj.name or 'values'\n            obj = obj.to_frame(name)\n        super().write(obj=obj, data_columns=obj.columns.tolist(), **kwargs)\n\n    def read(self, where=None, columns=None, start: int | None=None, stop: int | None=None) -> Series:\n        is_multi_index = self.is_multi_index\n        if columns is not None and is_multi_index:\n            assert isinstance(self.levels, list)\n            for n in self.levels:\n                if n not in columns:\n                    columns.insert(0, n)\n        s = super().read(where=where, columns=columns, start=start, stop=stop)\n        if is_multi_index:\n            s.set_index(self.levels, inplace=True)\n        s = s.iloc[:, 0]\n        if s.name == 'values':\n            s.name = None\n        return s", "class_fn": true, "question_id": "pandas/pandas.io.pytables/AppendableSeriesTable", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/pytables.py", "fn_id": "", "content": "class GenericIndexCol(IndexCol):\n    \"\"\"an index which is not represented in the data of the table\"\"\"\n\n    @property\n    def is_indexed(self) -> bool:\n        return False\n\n    def __init__(self, name: str, values=None, kind=None, typ=None, cname: str | None=None, axis=None, pos=None, freq=None, tz=None, index_name=None, ordered=None, table=None, meta=None, metadata=None) -> None:\n        if not isinstance(name, str):\n            raise ValueError('`name` must be a str.')\n        self.values = values\n        self.kind = kind\n        self.typ = typ\n        self.name = name\n        self.cname = cname or name\n        self.axis = axis\n        self.pos = pos\n        self.freq = freq\n        self.tz = tz\n        self.index_name = index_name\n        self.ordered = ordered\n        self.table = table\n        self.meta = meta\n        self.metadata = metadata\n        if pos is not None:\n            self.set_pos(pos)\n        assert isinstance(self.name, str)\n        assert isinstance(self.cname, str)\n\n    def maybe_set_size(self, min_itemsize=None) -> None:\n        \"\"\"\n        maybe set a string col itemsize:\n            min_itemsize can be an integer or a dict with this columns name\n            with an integer size\n        \"\"\"\n        if _ensure_decoded(self.kind) == 'string':\n            if isinstance(min_itemsize, dict):\n                min_itemsize = min_itemsize.get(self.name)\n            if min_itemsize is not None and self.typ.itemsize < min_itemsize:\n                self.typ = _tables().StringCol(itemsize=min_itemsize, pos=self.pos)\n\n    def set_attr(self) -> None:\n        pass\n\n    def convert(self, values: np.ndarray, nan_rep, encoding: str, errors: str) -> tuple[Index, Index]:\n        \"\"\"\n        Convert the data from this selection to the appropriate pandas type.\n\n        Parameters\n        ----------\n        values : np.ndarray\n        nan_rep : str\n        encoding : str\n        errors : str\n        \"\"\"\n        assert isinstance(values, np.ndarray), type(values)\n        index = RangeIndex(len(values))\n        return (index, index)\n\n    def validate_metadata(self, handler: AppendableTable) -> None:\n        \"\"\"validate that kind=category does not change the categories\"\"\"\n        if self.meta == 'category':\n            new_metadata = self.metadata\n            cur_metadata = handler.read_metadata(self.cname)\n            if new_metadata is not None and cur_metadata is not None and (not array_equivalent(new_metadata, cur_metadata, strict_nan=True, dtype_equal=True)):\n                raise ValueError('cannot append a categorical with different categories to the existing')", "class_fn": true, "question_id": "pandas/pandas.io.pytables/GenericIndexCol", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/sas/sas7bdat.py", "fn_id": "", "content": "class _Column:\n    col_id: int\n    name: str | bytes\n    label: str | bytes\n    format: str | bytes\n    ctype: bytes\n    length: int\n\n    def __init__(self, col_id: int, name: str | bytes, label: str | bytes, format: str | bytes, ctype: bytes, length: int) -> None:\n        self.col_id = col_id\n        self.name = name\n        self.label = label\n        self.format = format\n        self.ctype = ctype\n        self.length = length", "class_fn": true, "question_id": "pandas/pandas.io.sas.sas7bdat/_Column", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/io/sql.py", "fn_id": "", "content": "class PandasSQL(PandasObject, ABC):\n    \"\"\"\n    Subclasses Should define read_query and to_sql.\n    \"\"\"\n\n    def __sizeof__(self) -> int:\n        \"\"\"\n        Generates the total memory usage for an object that returns\n        either a value or Series of values\n        \"\"\"\n        memory_usage = getattr(self, 'memory_usage', None)\n        if memory_usage:\n            mem = memory_usage(deep=True)\n            return int(mem if is_scalar(mem) else mem.sum())\n        return super().__sizeof__()\n\n    @abstractmethod\n    def execute(self, sql: str | Select | TextClause, params=None):\n        pass\n\n    def read_table(self, table_name: str, index_col: str | list[str] | None=None, coerce_float: bool=True, parse_dates=None, columns=None, schema: str | None=None, chunksize: int | None=None, dtype_backend: DtypeBackend | Literal['numpy']='numpy') -> DataFrame | Iterator[DataFrame]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def _create_sql_schema(self, frame: DataFrame, table_name: str, keys: list[str] | None=None, dtype: DtypeArg | None=None, schema: str | None=None) -> str:\n        pass\n\n    def __enter__(self) -> Self:\n        return self\n\n    @abstractmethod\n    def read_query(self, sql: str, index_col: str | list[str] | None=None, coerce_float: bool=True, parse_dates=None, params=None, chunksize: int | None=None, dtype: DtypeArg | None=None, dtype_backend: DtypeBackend | Literal['numpy']='numpy') -> DataFrame | Iterator[DataFrame]:\n        pass\n\n    def __exit__(self, *args) -> None:\n        pass\n\n    @abstractmethod\n    def has_table(self, name: str, schema: str | None=None) -> bool:\n        pass\n\n    def _reset_cache(self, key: str | None=None) -> None:\n        \"\"\"\n        Reset cached properties. If ``key`` is passed, only clears that key.\n        \"\"\"\n        if not hasattr(self, '_cache'):\n            return\n        if key is None:\n            self._cache.clear()\n        else:\n            self._cache.pop(key, None)\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular object.\n        \"\"\"\n        return object.__repr__(self)\n\n    @abstractmethod\n    def to_sql(self, frame, name: str, if_exists: Literal['fail', 'replace', 'append']='fail', index: bool=True, index_label=None, schema=None, chunksize: int | None=None, dtype: DtypeArg | None=None, method: Literal['multi'] | Callable | None=None, engine: str='auto', **engine_kwargs) -> int | None:\n        pass", "class_fn": true, "question_id": "pandas/pandas.io.sql/PandasSQL", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/arithmetic/test_categorical.py", "fn_id": "", "content": "class TestCategoricalComparisons:\n\n    def test_categorical_nan_equality(self):\n        cat = Series(Categorical(['a', 'b', 'c', np.nan]))\n        expected = Series([True, True, True, False])\n        result = cat == cat\n        tm.assert_series_equal(result, expected)\n\n    def test_categorical_tuple_equality(self):\n        ser = Series([(0, 0), (0, 1), (0, 0), (1, 0), (1, 1)])\n        expected = Series([True, False, True, False, False])\n        result = ser == (0, 0)\n        tm.assert_series_equal(result, expected)\n        result = ser.astype('category') == (0, 0)\n        tm.assert_series_equal(result, expected)", "class_fn": true, "question_id": "pandas/pandas.tests.arithmetic.test_categorical/TestCategoricalComparisons", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/arrays/numpy_/test_indexing.py", "fn_id": "", "content": "class TestSearchsorted:\n\n    def test_searchsorted_string(self, string_dtype):\n        arr = pd.array(['a', 'b', 'c'], dtype=string_dtype)\n        result = arr.searchsorted('a', side='left')\n        assert is_scalar(result)\n        assert result == 0\n        result = arr.searchsorted('a', side='right')\n        assert is_scalar(result)\n        assert result == 1\n\n    def test_searchsorted_numeric_dtypes_scalar(self, any_real_numpy_dtype):\n        arr = pd.array([1, 3, 90], dtype=any_real_numpy_dtype)\n        result = arr.searchsorted(30)\n        assert is_scalar(result)\n        assert result == 2\n        result = arr.searchsorted([30])\n        expected = np.array([2], dtype=np.intp)\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_searchsorted_numeric_dtypes_vector(self, any_real_numpy_dtype):\n        arr = pd.array([1, 3, 90], dtype=any_real_numpy_dtype)\n        result = arr.searchsorted([2, 30])\n        expected = np.array([1, 2], dtype=np.intp)\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_searchsorted_sorter(self, any_real_numpy_dtype):\n        arr = pd.array([3, 1, 2], dtype=any_real_numpy_dtype)\n        result = arr.searchsorted([0, 3], sorter=np.argsort(arr))\n        expected = np.array([0, 2], dtype=np.intp)\n        tm.assert_numpy_array_equal(result, expected)", "class_fn": true, "question_id": "pandas/pandas.tests.arrays.numpy_.test_indexing/TestSearchsorted", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/extension/date/array.py", "fn_id": "", "content": "@register_extension_dtype\nclass DateDtype(ExtensionDtype):\n\n    @property\n    def type(self):\n        return dt.date\n\n    @property\n    def name(self):\n        return 'DateDtype'\n\n    def __repr__(self) -> str:\n        return self.name\n\n    def __hash__(self) -> int:\n        return object_hash(tuple((getattr(self, attr) for attr in self._metadata)))\n\n    @property\n    def na_value(self):\n        return dt.date.min\n\n    @classmethod\n    def construct_from_string(cls, string: str):\n        if not isinstance(string, str):\n            raise TypeError(f\"'construct_from_string' expects a string, got {type(string)}\")\n        if string == cls.__name__:\n            return cls()\n        else:\n            raise TypeError(f\"Cannot construct a '{cls.__name__}' from '{string}'\")\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        \"\"\"\n        Return the common dtype, if one exists.\n\n        Used in `find_common_type` implementation. This is for example used\n        to determine the resulting dtype in a concat operation.\n\n        If no common dtype exists, return None (which gives the other dtypes\n        the chance to determine a common dtype). If all dtypes in the list\n        return None, then the common dtype will be \"object\" dtype (this means\n        it is never needed to return \"object\" dtype from this method itself).\n\n        Parameters\n        ----------\n        dtypes : list of dtypes\n            The dtypes for which to determine a common dtype. This is a list\n            of np.dtype or ExtensionDtype instances.\n\n        Returns\n        -------\n        Common dtype (np.dtype or ExtensionDtype) or None\n        \"\"\"\n        if len(set(dtypes)) == 1:\n            return self\n        else:\n            return None\n\n    @classmethod\n    def construct_array_type(cls):\n        return DateArray\n\n    def __str__(self) -> str:\n        return self.name", "class_fn": true, "question_id": "pandas/pandas.tests.extension.date.array/DateDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/extension/list/array.py", "fn_id": "", "content": "class ListDtype(ExtensionDtype):\n    type = list\n    name = 'list'\n    na_value = np.nan\n\n    def __hash__(self) -> int:\n        return object_hash(tuple((getattr(self, attr) for attr in self._metadata)))\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        \"\"\"\n        Return the common dtype, if one exists.\n\n        Used in `find_common_type` implementation. This is for example used\n        to determine the resulting dtype in a concat operation.\n\n        If no common dtype exists, return None (which gives the other dtypes\n        the chance to determine a common dtype). If all dtypes in the list\n        return None, then the common dtype will be \"object\" dtype (this means\n        it is never needed to return \"object\" dtype from this method itself).\n\n        Parameters\n        ----------\n        dtypes : list of dtypes\n            The dtypes for which to determine a common dtype. This is a list\n            of np.dtype or ExtensionDtype instances.\n\n        Returns\n        -------\n        Common dtype (np.dtype or ExtensionDtype) or None\n        \"\"\"\n        if len(set(dtypes)) == 1:\n            return self\n        else:\n            return None\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"\n        Check whether 'other' is equal to self.\n\n        By default, 'other' is considered equal if either\n\n        * it's a string matching 'self.name'.\n        * it's an instance of this type and all of the attributes\n          in ``self._metadata`` are equal between `self` and `other`.\n\n        Parameters\n        ----------\n        other : Any\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if isinstance(other, str):\n            try:\n                other = self.construct_from_string(other)\n            except TypeError:\n                return False\n        if isinstance(other, type(self)):\n            return all((getattr(self, attr) == getattr(other, attr) for attr in self._metadata))\n        return False\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[ListArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        return ListArray", "class_fn": true, "question_id": "pandas/pandas.tests.extension.list.array/ListDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/frame/test_alter_axes.py", "fn_id": "", "content": "class TestDataFrameAlterAxes:\n\n    def test_set_axis_setattr_index(self):\n        df = DataFrame([{'ts': datetime(2014, 4, 1, tzinfo=pytz.utc), 'foo': 1}])\n        expected = df.set_index('ts')\n        df.index = df['ts']\n        df.pop('ts')\n        tm.assert_frame_equal(df, expected)\n\n    def test_assign_columns(self, float_frame):\n        float_frame['hi'] = 'there'\n        df = float_frame.copy()\n        df.columns = ['foo', 'bar', 'baz', 'quux', 'foo2']\n        tm.assert_series_equal(float_frame['C'], df['baz'], check_names=False)\n        tm.assert_series_equal(float_frame['hi'], df['foo2'], check_names=False)", "class_fn": true, "question_id": "pandas/pandas.tests.frame.test_alter_axes/TestDataFrameAlterAxes", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/datetimes/methods/test_to_frame.py", "fn_id": "", "content": "class TestToFrame:\n\n    def test_to_frame_datetime_tz(self):\n        idx = date_range(start='2019-01-01', end='2019-01-30', freq='D', tz='UTC')\n        result = idx.to_frame()\n        expected = DataFrame(idx, index=idx)\n        tm.assert_frame_equal(result, expected)\n\n    def test_to_frame_respects_none_name(self):\n        idx = date_range(start='2019-01-01', end='2019-01-30', freq='D', tz='UTC')\n        result = idx.to_frame(name=None)\n        exp_idx = Index([None], dtype=object)\n        tm.assert_index_equal(exp_idx, result.columns)\n        result = idx.rename('foo').to_frame(name=None)\n        exp_idx = Index([None], dtype=object)\n        tm.assert_index_equal(exp_idx, result.columns)", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.datetimes.methods.test_to_frame/TestToFrame", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/datetimes/test_npfuncs.py", "fn_id": "", "content": "class TestSplit:\n\n    def test_split_non_utc(self):\n        indices = date_range('2016-01-01 00:00:00+0200', freq='s', periods=10)\n        result = np.split(indices, indices_or_sections=[])[0]\n        expected = indices._with_freq(None)\n        tm.assert_index_equal(result, expected)", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.datetimes.test_npfuncs/TestSplit", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/multi/test_get_level_values.py", "fn_id": "", "content": "class TestGetLevelValues:\n\n    def test_get_level_values_box_datetime64(self):\n        dates = date_range('1/1/2000', periods=4)\n        levels = [dates, [0, 1]]\n        codes = [[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]]\n        index = MultiIndex(levels=levels, codes=codes)\n        assert isinstance(index.get_level_values(0)[0], Timestamp)", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.multi.test_get_level_values/TestGetLevelValues", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/period/methods/test_factorize.py", "fn_id": "", "content": "class TestFactorize:\n\n    def test_factorize_period(self):\n        idx1 = PeriodIndex(['2014-01', '2014-01', '2014-02', '2014-02', '2014-03', '2014-03'], freq='M')\n        exp_arr = np.array([0, 0, 1, 1, 2, 2], dtype=np.intp)\n        exp_idx = PeriodIndex(['2014-01', '2014-02', '2014-03'], freq='M')\n        (arr, idx) = idx1.factorize()\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)\n        (arr, idx) = idx1.factorize(sort=True)\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)\n\n    def test_factorize_period_nonmonotonic(self):\n        idx2 = PeriodIndex(['2014-03', '2014-03', '2014-02', '2014-01', '2014-03', '2014-01'], freq='M')\n        exp_idx = PeriodIndex(['2014-01', '2014-02', '2014-03'], freq='M')\n        exp_arr = np.array([2, 2, 1, 0, 2, 0], dtype=np.intp)\n        (arr, idx) = idx2.factorize(sort=True)\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)\n        exp_arr = np.array([0, 0, 1, 2, 0, 2], dtype=np.intp)\n        exp_idx = PeriodIndex(['2014-03', '2014-02', '2014-01'], freq='M')\n        (arr, idx) = idx2.factorize()\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.period.methods.test_factorize/TestFactorize", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/timedeltas/methods/test_factorize.py", "fn_id": "", "content": "class TestTimedeltaIndexFactorize:\n\n    def test_factorize(self):\n        idx1 = TimedeltaIndex(['1 day', '1 day', '2 day', '2 day', '3 day', '3 day'])\n        exp_arr = np.array([0, 0, 1, 1, 2, 2], dtype=np.intp)\n        exp_idx = TimedeltaIndex(['1 day', '2 day', '3 day'])\n        (arr, idx) = idx1.factorize()\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)\n        assert idx.freq == exp_idx.freq\n        (arr, idx) = idx1.factorize(sort=True)\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, exp_idx)\n        assert idx.freq == exp_idx.freq\n\n    def test_factorize_preserves_freq(self):\n        idx3 = timedelta_range('1 day', periods=4, freq='s')\n        exp_arr = np.array([0, 1, 2, 3], dtype=np.intp)\n        (arr, idx) = idx3.factorize()\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, idx3)\n        assert idx.freq == idx3.freq\n        (arr, idx) = factorize(idx3)\n        tm.assert_numpy_array_equal(arr, exp_arr)\n        tm.assert_index_equal(idx, idx3)\n        assert idx.freq == idx3.freq", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.timedeltas.methods.test_factorize/TestTimedeltaIndexFactorize", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/timedeltas/test_arithmetic.py", "fn_id": "", "content": "class TestTimedeltaIndexArithmetic:\n\n    def test_arithmetic_zero_freq(self):\n        tdi = timedelta_range(0, periods=100, freq='ns')\n        result = tdi / 2\n        assert result.freq is None\n        expected = tdi[:50].repeat(2)\n        tm.assert_index_equal(result, expected)\n        result2 = tdi // 2\n        assert result2.freq is None\n        expected2 = expected\n        tm.assert_index_equal(result2, expected2)\n        result3 = tdi * 0\n        assert result3.freq is None\n        expected3 = tdi[:1].repeat(100)\n        tm.assert_index_equal(result3, expected3)\n\n    def test_tdi_division(self, index_or_series):\n        scalar = Timedelta(days=31)\n        td = index_or_series([scalar, scalar, scalar + Timedelta(minutes=5, seconds=3), NaT], dtype='m8[ns]')\n        result = td / np.timedelta64(1, 'D')\n        expected = index_or_series([31, 31, (31 * 86400 + 5 * 60 + 3) / 86400.0, np.nan])\n        tm.assert_equal(result, expected)\n        result = td / np.timedelta64(1, 's')\n        expected = index_or_series([31 * 86400, 31 * 86400, 31 * 86400 + 5 * 60 + 3, np.nan])\n        tm.assert_equal(result, expected)", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.timedeltas.test_arithmetic/TestTimedeltaIndexArithmetic", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/indexes/timedeltas/test_pickle.py", "fn_id": "", "content": "class TestPickle:\n\n    def test_pickle_after_set_freq(self):\n        tdi = timedelta_range('1 day', periods=4, freq='s')\n        tdi = tdi._with_freq(None)\n        res = tm.round_trip_pickle(tdi)\n        tm.assert_index_equal(res, tdi)", "class_fn": true, "question_id": "pandas/pandas.tests.indexes.timedeltas.test_pickle/TestPickle", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/scalar/timestamp/methods/test_to_julian_date.py", "fn_id": "", "content": "class TestTimestampToJulianDate:\n\n    def test_compare_1700(self):\n        ts = Timestamp('1700-06-23')\n        res = ts.to_julian_date()\n        assert res == 2342145.5\n\n    def test_compare_2000(self):\n        ts = Timestamp('2000-04-12')\n        res = ts.to_julian_date()\n        assert res == 2451646.5\n\n    def test_compare_2100(self):\n        ts = Timestamp('2100-08-12')\n        res = ts.to_julian_date()\n        assert res == 2488292.5\n\n    def test_compare_hour01(self):\n        ts = Timestamp('2000-08-12T01:00:00')\n        res = ts.to_julian_date()\n        assert res == 2451768.5416666665\n\n    def test_compare_hour13(self):\n        ts = Timestamp('2000-08-12T13:00:00')\n        res = ts.to_julian_date()\n        assert res == 2451769.0416666665", "class_fn": true, "question_id": "pandas/pandas.tests.scalar.timestamp.methods.test_to_julian_date/TestTimestampToJulianDate", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/series/methods/test_autocorr.py", "fn_id": "", "content": "class TestAutoCorr:\n\n    def test_autocorr(self, datetime_series):\n        corr1 = datetime_series.autocorr()\n        corr2 = datetime_series.autocorr(lag=1)\n        if len(datetime_series) <= 2:\n            assert np.isnan(corr1)\n            assert np.isnan(corr2)\n        else:\n            assert corr1 == corr2\n        n = 1 + np.random.default_rng(2).integers(max(1, len(datetime_series) - 2))\n        corr1 = datetime_series.corr(datetime_series.shift(n))\n        corr2 = datetime_series.autocorr(lag=n)\n        if len(datetime_series) <= 2:\n            assert np.isnan(corr1)\n            assert np.isnan(corr2)\n        else:\n            assert corr1 == corr2", "class_fn": true, "question_id": "pandas/pandas.tests.series.methods.test_autocorr/TestAutoCorr", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/series/methods/test_dtypes.py", "fn_id": "", "content": "class TestSeriesDtypes:\n\n    def test_dtype(self, datetime_series):\n        assert datetime_series.dtype == np.dtype('float64')\n        assert datetime_series.dtypes == np.dtype('float64')", "class_fn": true, "question_id": "pandas/pandas.tests.series.methods.test_dtypes/TestSeriesDtypes", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/series/methods/test_is_monotonic.py", "fn_id": "", "content": "class TestIsMonotonic:\n\n    def test_is_monotonic_numeric(self):\n        ser = Series(np.random.default_rng(2).integers(0, 10, size=1000))\n        assert not ser.is_monotonic_increasing\n        ser = Series(np.arange(1000))\n        assert ser.is_monotonic_increasing is True\n        assert ser.is_monotonic_increasing is True\n        ser = Series(np.arange(1000, 0, -1))\n        assert ser.is_monotonic_decreasing is True\n\n    def test_is_monotonic_dt64(self):\n        ser = Series(date_range('20130101', periods=10))\n        assert ser.is_monotonic_increasing is True\n        assert ser.is_monotonic_increasing is True\n        ser = Series(list(reversed(ser)))\n        assert ser.is_monotonic_increasing is False\n        assert ser.is_monotonic_decreasing is True", "class_fn": true, "question_id": "pandas/pandas.tests.series.methods.test_is_monotonic/TestIsMonotonic", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tests/series/test_iteration.py", "fn_id": "", "content": "class TestIteration:\n\n    def test_keys(self, datetime_series):\n        assert datetime_series.keys() is datetime_series.index\n\n    def test_iter_datetimes(self, datetime_series):\n        for (i, val) in enumerate(datetime_series):\n            assert val == datetime_series.iloc[i]\n\n    def test_iter_strings(self, string_series):\n        for (i, val) in enumerate(string_series):\n            assert val == string_series.iloc[i]\n\n    def test_iteritems_datetimes(self, datetime_series):\n        for (idx, val) in datetime_series.items():\n            assert val == datetime_series[idx]\n\n    def test_iteritems_strings(self, string_series):\n        for (idx, val) in string_series.items():\n            assert val == string_series[idx]\n        assert not hasattr(string_series.items(), 'reverse')\n\n    def test_items_datetimes(self, datetime_series):\n        for (idx, val) in datetime_series.items():\n            assert val == datetime_series[idx]\n\n    def test_items_strings(self, string_series):\n        for (idx, val) in string_series.items():\n            assert val == string_series[idx]\n        assert not hasattr(string_series.items(), 'reverse')", "class_fn": true, "question_id": "pandas/pandas.tests.series.test_iteration/TestIteration", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/tseries/frequencies.py", "fn_id": "", "content": "class _TimedeltaFrequencyInferer(_FrequencyInferer):\n\n    def _get_quarterly_rule(self) -> str | None:\n        if len(self.mdiffs) > 1:\n            return None\n        if not self.mdiffs[0] % 3 == 0:\n            return None\n        pos_check = self.month_position_check()\n        if pos_check is None:\n            return None\n        else:\n            return {'cs': 'QS', 'bs': 'BQS', 'ce': 'QE', 'be': 'BQE'}.get(pos_check)\n\n    def _infer_daily_rule(self):\n        if self.is_unique:\n            return self._get_daily_rule()\n\n    def get_freq(self) -> str | None:\n        \"\"\"\n        Find the appropriate frequency string to describe the inferred\n        frequency of self.i8values\n\n        Returns\n        -------\n        str or None\n        \"\"\"\n        if not self.is_monotonic or not self.index._is_unique:\n            return None\n        delta = self.deltas[0]\n        ppd = periods_per_day(self._creso)\n        if delta and _is_multiple(delta, ppd):\n            return self._infer_daily_rule()\n        if self.hour_deltas in ([1, 17], [1, 65], [1, 17, 65]):\n            return 'bh'\n        if not self.is_unique_asi8:\n            return None\n        delta = self.deltas_asi8[0]\n        pph = ppd // 24\n        ppm = pph // 60\n        pps = ppm // 60\n        if _is_multiple(delta, pph):\n            return _maybe_add_count('h', delta / pph)\n        elif _is_multiple(delta, ppm):\n            return _maybe_add_count('min', delta / ppm)\n        elif _is_multiple(delta, pps):\n            return _maybe_add_count('s', delta / pps)\n        elif _is_multiple(delta, pps // 1000):\n            return _maybe_add_count('ms', delta / (pps // 1000))\n        elif _is_multiple(delta, pps // 1000000):\n            return _maybe_add_count('us', delta / (pps // 1000000))\n        else:\n            return _maybe_add_count('ns', delta)\n\n    def _get_wom_rule(self) -> str | None:\n        weekdays = unique(self.index.weekday)\n        if len(weekdays) > 1:\n            return None\n        week_of_months = unique((self.index.day - 1) // 7)\n        week_of_months = week_of_months[week_of_months < 4]\n        if len(week_of_months) == 0 or len(week_of_months) > 1:\n            return None\n        week = week_of_months[0] + 1\n        wd = int_to_weekday[weekdays[0]]\n        return f'WOM-{week}{wd}'", "class_fn": true, "question_id": "pandas/pandas.tseries.frequencies/_TimedeltaFrequencyInferer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/util/_decorators.py", "fn_id": "", "content": "class Appender:\n    \"\"\"\n    A function decorator that will append an addendum to the docstring\n    of the target function.\n\n    This decorator should be robust even if func.__doc__ is None\n    (for example, if -OO was passed to the interpreter).\n\n    Usage: construct a docstring.Appender with a string to be joined to\n    the original docstring. An optional 'join' parameter may be supplied\n    which will be used to join the docstring and addendum. e.g.\n\n    add_copyright = Appender(\"Copyright (c) 2009\", join='\n')\n\n    @add_copyright\n    def my_dog(has='fleas'):\n        \"This docstring will have a copyright below\"\n        pass\n    \"\"\"\n    addendum: str | None\n\n    def __init__(self, addendum: str | None, join: str='', indents: int=0) -> None:\n        if indents > 0:\n            self.addendum = indent(addendum, indents=indents)\n        else:\n            self.addendum = addendum\n        self.join = join\n\n    def __call__(self, func: T) -> T:\n        func.__doc__ = func.__doc__ if func.__doc__ else ''\n        self.addendum = self.addendum if self.addendum else ''\n        docitems = [func.__doc__, self.addendum]\n        func.__doc__ = dedent(self.join.join(docitems))\n        return func", "class_fn": true, "question_id": "pandas/pandas.util._decorators/Appender", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/util/version/__init__.py", "fn_id": "", "content": "class LegacyVersion(_BaseVersion):\n\n    def __ge__(self, other: _BaseVersion) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n        return self._key >= other._key\n\n    def __lt__(self, other: _BaseVersion) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n        return self._key < other._key\n\n    def __ne__(self, other: object) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n        return self._key != other._key\n\n    @property\n    def public(self) -> str:\n        return self._version\n\n    @property\n    def base_version(self) -> str:\n        return self._version\n\n    @property\n    def epoch(self) -> int:\n        return -1\n\n    @property\n    def release(self) -> None:\n        return None\n\n    @property\n    def pre(self) -> None:\n        return None\n\n    @property\n    def post(self) -> None:\n        return None\n\n    @property\n    def dev(self) -> None:\n        return None\n\n    @property\n    def local(self) -> None:\n        return None\n\n    @property\n    def is_prerelease(self) -> bool:\n        return False\n\n    @property\n    def is_postrelease(self) -> bool:\n        return False\n\n    @property\n    def is_devrelease(self) -> bool:\n        return False\n\n    def __init__(self, version: str) -> None:\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)\n        warnings.warn('Creating a LegacyVersion has been deprecated and will be removed in the next major release.', DeprecationWarning)\n\n    def __repr__(self) -> str:\n        return f\"<LegacyVersion('{self}')>\"\n\n    def __str__(self) -> str:\n        return self._version", "class_fn": true, "question_id": "pandas/pandas.util.version/LegacyVersion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_config/config.py", "fn_id": "", "content": "class DeprecatedOption(NamedTuple):\n    key: str\n    msg: str | None\n    rkey: str | None\n    removal_ver: str | None", "class_fn": true, "question_id": "pandas/pandas._config.config/DeprecatedOption", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/_version.py", "fn_id": "", "content": "class NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"", "class_fn": true, "question_id": "pandas/pandas._version/NotThisMethod", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/apply.py", "fn_id": "", "content": "class Apply(metaclass=abc.ABCMeta):\n    axis: AxisInt\n\n    def __init__(self, obj: AggObjType, func: AggFuncType, raw: bool, result_type: str | None, *, by_row: Literal[False, 'compat', '_compat']='compat', engine: str='python', engine_kwargs: dict[str, bool] | None=None, args, kwargs) -> None:\n        self.obj = obj\n        self.raw = raw\n        assert by_row is False or by_row in ['compat', '_compat']\n        self.by_row = by_row\n        self.args = args or ()\n        self.kwargs = kwargs or {}\n        self.engine = engine\n        self.engine_kwargs = {} if engine_kwargs is None else engine_kwargs\n        if result_type not in [None, 'reduce', 'broadcast', 'expand']:\n            raise ValueError(\"invalid value for result_type, must be one of {None, 'reduce', 'broadcast', 'expand'}\")\n        self.result_type = result_type\n        self.func = func\n\n    @abc.abstractmethod\n    def apply(self) -> DataFrame | Series:\n        pass\n\n    @abc.abstractmethod\n    def agg_or_apply_list_like(self, op_name: Literal['agg', 'apply']) -> DataFrame | Series:\n        pass\n\n    @abc.abstractmethod\n    def agg_or_apply_dict_like(self, op_name: Literal['agg', 'apply']) -> DataFrame | Series:\n        pass\n\n    def agg(self) -> DataFrame | Series | None:\n        \"\"\"\n        Provide an implementation for the aggregators.\n\n        Returns\n        -------\n        Result of aggregation, or None if agg cannot be performed by\n        this method.\n        \"\"\"\n        obj = self.obj\n        func = self.func\n        args = self.args\n        kwargs = self.kwargs\n        if isinstance(func, str):\n            return self.apply_str()\n        if is_dict_like(func):\n            return self.agg_dict_like()\n        elif is_list_like(func):\n            return self.agg_list_like()\n        if callable(func):\n            f = com.get_cython_func(func)\n            if f and (not args) and (not kwargs):\n                warn_alias_replacement(obj, func, f)\n                return getattr(obj, f)()\n        return None\n\n    def transform(self) -> DataFrame | Series:\n        \"\"\"\n        Transform a DataFrame or Series.\n\n        Returns\n        -------\n        DataFrame or Series\n            Result of applying ``func`` along the given axis of the\n            Series or DataFrame.\n\n        Raises\n        ------\n        ValueError\n            If the transform function fails or does not transform.\n        \"\"\"\n        obj = self.obj\n        func = self.func\n        axis = self.axis\n        args = self.args\n        kwargs = self.kwargs\n        is_series = obj.ndim == 1\n        if obj._get_axis_number(axis) == 1:\n            assert not is_series\n            return obj.T.transform(func, 0, *args, **kwargs).T\n        if is_list_like(func) and (not is_dict_like(func)):\n            func = cast(list[AggFuncTypeBase], func)\n            if is_series:\n                func = {com.get_callable_name(v) or v: v for v in func}\n            else:\n                func = {col: func for col in obj}\n        if is_dict_like(func):\n            func = cast(AggFuncTypeDict, func)\n            return self.transform_dict_like(func)\n        func = cast(AggFuncTypeBase, func)\n        try:\n            result = self.transform_str_or_callable(func)\n        except TypeError:\n            raise\n        except Exception as err:\n            raise ValueError('Transform function failed') from err\n        if isinstance(result, (ABCSeries, ABCDataFrame)) and result.empty and (not obj.empty):\n            raise ValueError('Transform function failed')\n        if not isinstance(result, (ABCSeries, ABCDataFrame)) or not result.index.equals(obj.index):\n            raise ValueError('Function did not transform')\n        return result\n\n    def transform_dict_like(self, func) -> DataFrame:\n        \"\"\"\n        Compute transform in the case of a dict-like func\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n        obj = self.obj\n        args = self.args\n        kwargs = self.kwargs\n        assert isinstance(obj, ABCNDFrame)\n        if len(func) == 0:\n            raise ValueError('No transform functions were provided')\n        func = self.normalize_dictlike_arg('transform', obj, func)\n        results: dict[Hashable, DataFrame | Series] = {}\n        for (name, how) in func.items():\n            colg = obj._gotitem(name, ndim=1)\n            results[name] = colg.transform(how, 0, *args, **kwargs)\n        return concat(results, axis=1)\n\n    def transform_str_or_callable(self, func) -> DataFrame | Series:\n        \"\"\"\n        Compute transform in the case of a string or callable func\n        \"\"\"\n        obj = self.obj\n        args = self.args\n        kwargs = self.kwargs\n        if isinstance(func, str):\n            return self._apply_str(obj, func, *args, **kwargs)\n        if not args and (not kwargs):\n            f = com.get_cython_func(func)\n            if f:\n                warn_alias_replacement(obj, func, f)\n                return getattr(obj, f)()\n        try:\n            return obj.apply(func, args=args, **kwargs)\n        except Exception:\n            return func(obj, *args, **kwargs)\n\n    def agg_list_like(self) -> DataFrame | Series:\n        \"\"\"\n        Compute aggregation in the case of a list-like argument.\n\n        Returns\n        -------\n        Result of aggregation.\n        \"\"\"\n        return self.agg_or_apply_list_like(op_name='agg')\n\n    def compute_list_like(self, op_name: Literal['agg', 'apply'], selected_obj: Series | DataFrame, kwargs: dict[str, Any]) -> tuple[list[Hashable] | Index, list[Any]]:\n        \"\"\"\n        Compute agg/apply results for like-like input.\n\n        Parameters\n        ----------\n        op_name : {\"agg\", \"apply\"}\n            Operation being performed.\n        selected_obj : Series or DataFrame\n            Data to perform operation on.\n        kwargs : dict\n            Keyword arguments to pass to the functions.\n\n        Returns\n        -------\n        keys : list[Hashable] or Index\n            Index labels for result.\n        results : list\n            Data for result. When aggregating with a Series, this can contain any\n            Python objects.\n        \"\"\"\n        func = cast(list[AggFuncTypeBase], self.func)\n        obj = self.obj\n        results = []\n        keys = []\n        if selected_obj.ndim == 1:\n            for a in func:\n                colg = obj._gotitem(selected_obj.name, ndim=1, subset=selected_obj)\n                args = [self.axis, *self.args] if include_axis(op_name, colg) else self.args\n                new_res = getattr(colg, op_name)(a, *args, **kwargs)\n                results.append(new_res)\n                name = com.get_callable_name(a) or a\n                keys.append(name)\n        else:\n            indices = []\n            for (index, col) in enumerate(selected_obj):\n                colg = obj._gotitem(col, ndim=1, subset=selected_obj.iloc[:, index])\n                args = [self.axis, *self.args] if include_axis(op_name, colg) else self.args\n                new_res = getattr(colg, op_name)(func, *args, **kwargs)\n                results.append(new_res)\n                indices.append(index)\n            keys = selected_obj.columns.take(indices)\n        return (keys, results)\n\n    def wrap_results_list_like(self, keys: Iterable[Hashable], results: list[Series | DataFrame]):\n        from pandas.core.reshape.concat import concat\n        obj = self.obj\n        try:\n            return concat(results, keys=keys, axis=1, sort=False)\n        except TypeError as err:\n            from pandas import Series\n            result = Series(results, index=keys, name=obj.name)\n            if is_nested_object(result):\n                raise ValueError('cannot combine transform and aggregation operations') from err\n            return result\n\n    def agg_dict_like(self) -> DataFrame | Series:\n        \"\"\"\n        Compute aggregation in the case of a dict-like argument.\n\n        Returns\n        -------\n        Result of aggregation.\n        \"\"\"\n        return self.agg_or_apply_dict_like(op_name='agg')\n\n    def compute_dict_like(self, op_name: Literal['agg', 'apply'], selected_obj: Series | DataFrame, selection: Hashable | Sequence[Hashable], kwargs: dict[str, Any]) -> tuple[list[Hashable], list[Any]]:\n        \"\"\"\n        Compute agg/apply results for dict-like input.\n\n        Parameters\n        ----------\n        op_name : {\"agg\", \"apply\"}\n            Operation being performed.\n        selected_obj : Series or DataFrame\n            Data to perform operation on.\n        selection : hashable or sequence of hashables\n            Used by GroupBy, Window, and Resample if selection is applied to the object.\n        kwargs : dict\n            Keyword arguments to pass to the functions.\n\n        Returns\n        -------\n        keys : list[hashable]\n            Index labels for result.\n        results : list\n            Data for result. When aggregating with a Series, this can contain any\n            Python object.\n        \"\"\"\n        from pandas.core.groupby.generic import DataFrameGroupBy, SeriesGroupBy\n        obj = self.obj\n        is_groupby = isinstance(obj, (DataFrameGroupBy, SeriesGroupBy))\n        func = cast(AggFuncTypeDict, self.func)\n        func = self.normalize_dictlike_arg(op_name, selected_obj, func)\n        is_non_unique_col = selected_obj.ndim == 2 and selected_obj.columns.nunique() < len(selected_obj.columns)\n        if selected_obj.ndim == 1:\n            colg = obj._gotitem(selection, ndim=1)\n            results = [getattr(colg, op_name)(how, **kwargs) for (_, how) in func.items()]\n            keys = list(func.keys())\n        elif not is_groupby and is_non_unique_col:\n            results = []\n            keys = []\n            for (key, how) in func.items():\n                indices = selected_obj.columns.get_indexer_for([key])\n                labels = selected_obj.columns.take(indices)\n                label_to_indices = defaultdict(list)\n                for (index, label) in zip(indices, labels):\n                    label_to_indices[label].append(index)\n                key_data = [getattr(selected_obj._ixs(indice, axis=1), op_name)(how, **kwargs) for (label, indices) in label_to_indices.items() for indice in indices]\n                keys += [key] * len(key_data)\n                results += key_data\n        else:\n            results = [getattr(obj._gotitem(key, ndim=1), op_name)(how, **kwargs) for (key, how) in func.items()]\n            keys = list(func.keys())\n        return (keys, results)\n\n    def wrap_results_dict_like(self, selected_obj: Series | DataFrame, result_index: list[Hashable], result_data: list):\n        from pandas import Index\n        from pandas.core.reshape.concat import concat\n        obj = self.obj\n        is_ndframe = [isinstance(r, ABCNDFrame) for r in result_data]\n        if all(is_ndframe):\n            results = dict(zip(result_index, result_data))\n            keys_to_use: Iterable[Hashable]\n            keys_to_use = [k for k in result_index if not results[k].empty]\n            keys_to_use = keys_to_use if keys_to_use != [] else result_index\n            if selected_obj.ndim == 2:\n                ktu = Index(keys_to_use)\n                ktu._set_names(selected_obj.columns.names)\n                keys_to_use = ktu\n            axis: AxisInt = 0 if isinstance(obj, ABCSeries) else 1\n            result = concat({k: results[k] for k in keys_to_use}, axis=axis, keys=keys_to_use)\n        elif any(is_ndframe):\n            raise ValueError('cannot perform both aggregation and transformation operations simultaneously')\n        else:\n            from pandas import Series\n            if obj.ndim == 1:\n                obj = cast('Series', obj)\n                name = obj.name\n            else:\n                name = None\n            result = Series(result_data, index=result_index, name=name)\n        return result\n\n    def apply_str(self) -> DataFrame | Series:\n        \"\"\"\n        Compute apply in case of a string.\n\n        Returns\n        -------\n        result: Series or DataFrame\n        \"\"\"\n        func = cast(str, self.func)\n        obj = self.obj\n        from pandas.core.groupby.generic import DataFrameGroupBy, SeriesGroupBy\n        method = getattr(obj, func, None)\n        if callable(method):\n            sig = inspect.getfullargspec(method)\n            arg_names = (*sig.args, *sig.kwonlyargs)\n            if self.axis != 0 and ('axis' not in arg_names or func in ('corrwith', 'skew')):\n                raise ValueError(f'Operation {func} does not support axis=1')\n            if 'axis' in arg_names:\n                if isinstance(obj, (SeriesGroupBy, DataFrameGroupBy)):\n                    default_axis = 0\n                    if func in ['idxmax', 'idxmin']:\n                        default_axis = self.obj.axis\n                    if default_axis != self.axis:\n                        self.kwargs['axis'] = self.axis\n                else:\n                    self.kwargs['axis'] = self.axis\n        return self._apply_str(obj, func, *self.args, **self.kwargs)\n\n    def apply_list_or_dict_like(self) -> DataFrame | Series:\n        \"\"\"\n        Compute apply in case of a list-like or dict-like.\n\n        Returns\n        -------\n        result: Series, DataFrame, or None\n            Result when self.func is a list-like or dict-like, None otherwise.\n        \"\"\"\n        if self.engine == 'numba':\n            raise NotImplementedError(\"The 'numba' engine doesn't support list-like/dict likes of callables yet.\")\n        if self.axis == 1 and isinstance(self.obj, ABCDataFrame):\n            return self.obj.T.apply(self.func, 0, args=self.args, **self.kwargs).T\n        func = self.func\n        kwargs = self.kwargs\n        if is_dict_like(func):\n            result = self.agg_or_apply_dict_like(op_name='apply')\n        else:\n            result = self.agg_or_apply_list_like(op_name='apply')\n        result = reconstruct_and_relabel_result(result, func, **kwargs)\n        return result\n\n    def normalize_dictlike_arg(self, how: str, obj: DataFrame | Series, func: AggFuncTypeDict) -> AggFuncTypeDict:\n        \"\"\"\n        Handler for dict-like argument.\n\n        Ensures that necessary columns exist if obj is a DataFrame, and\n        that a nested renamer is not passed. Also normalizes to all lists\n        when values consists of a mix of list and non-lists.\n        \"\"\"\n        assert how in ('apply', 'agg', 'transform')\n        if how == 'agg' and isinstance(obj, ABCSeries) and any((is_list_like(v) for (_, v) in func.items())) or any((is_dict_like(v) for (_, v) in func.items())):\n            raise SpecificationError('nested renamer is not supported')\n        if obj.ndim != 1:\n            from pandas import Index\n            cols = Index(list(func.keys())).difference(obj.columns, sort=True)\n            if len(cols) > 0:\n                raise KeyError(f'Column(s) {list(cols)} do not exist')\n        aggregator_types = (list, tuple, dict)\n        if any((isinstance(x, aggregator_types) for (_, x) in func.items())):\n            new_func: AggFuncTypeDict = {}\n            for (k, v) in func.items():\n                if not isinstance(v, aggregator_types):\n                    new_func[k] = [v]\n                else:\n                    new_func[k] = v\n            func = new_func\n        return func\n\n    def _apply_str(self, obj, func: str, *args, **kwargs):\n        \"\"\"\n        if arg is a string, then try to operate on it:\n        - try to find a function (or attribute) on obj\n        - try to find a numpy function\n        - raise\n        \"\"\"\n        assert isinstance(func, str)\n        if hasattr(obj, func):\n            f = getattr(obj, func)\n            if callable(f):\n                return f(*args, **kwargs)\n            assert len(args) == 0\n            assert len([kwarg for kwarg in kwargs if kwarg not in ['axis']]) == 0\n            return f\n        elif hasattr(np, func) and hasattr(obj, '__array__'):\n            f = getattr(np, func)\n            return f(obj, *args, **kwargs)\n        else:\n            msg = f\"'{func}' is not a valid function for '{type(obj).__name__}' object\"\n            raise AttributeError(msg)", "class_fn": true, "question_id": "pandas/pandas.core.apply/Apply", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/apply.py", "fn_id": "", "content": "class FrameRowApply(FrameApply):\n    axis: AxisInt = 0\n\n    @property\n    def series_generator(self) -> Generator[Series, None, None]:\n        return (self.obj._ixs(i, axis=1) for i in range(len(self.columns)))\n\n    @staticmethod\n    @functools.cache\n    def generate_numba_apply_func(func, nogil=True, nopython=True, parallel=False) -> Callable[[npt.NDArray, Index, Index], dict[int, Any]]:\n        numba = import_optional_dependency('numba')\n        from pandas import Series\n        from pandas.core._numba.extensions import maybe_cast_str\n        jitted_udf = numba.extending.register_jitable(func)\n\n        @numba.jit(nogil=nogil, nopython=nopython, parallel=parallel)\n        def numba_func(values, col_names, df_index):\n            results = {}\n            for j in range(values.shape[1]):\n                ser = Series(values[:, j], index=df_index, name=maybe_cast_str(col_names[j]))\n                results[j] = jitted_udf(ser)\n            return results\n        return numba_func\n\n    def apply_raw(self, engine='python', engine_kwargs=None):\n        \"\"\"apply to the values as a numpy array\"\"\"\n\n        def wrap_function(func):\n            \"\"\"\n            Wrap user supplied function to work around numpy issue.\n\n            see https://github.com/numpy/numpy/issues/8352\n            \"\"\"\n\n            def wrapper(*args, **kwargs):\n                result = func(*args, **kwargs)\n                if isinstance(result, str):\n                    result = np.array(result, dtype=object)\n                return result\n            return wrapper\n        if engine == 'numba':\n            engine_kwargs = {} if engine_kwargs is None else engine_kwargs\n            nb_looper = generate_apply_looper(self.func, **engine_kwargs)\n            result = nb_looper(self.values, self.axis)\n            result = np.squeeze(result)\n        else:\n            result = np.apply_along_axis(wrap_function(self.func), self.axis, self.values, *self.args, **self.kwargs)\n        if result.ndim == 2:\n            return self.obj._constructor(result, index=self.index, columns=self.columns)\n        else:\n            return self.obj._constructor_sliced(result, index=self.agg_axis)\n\n    @property\n    def result_index(self) -> Index:\n        return self.columns\n\n    @property\n    def result_columns(self) -> Index:\n        return self.index\n\n    def __init__(self, obj: AggObjType, func: AggFuncType, raw: bool, result_type: str | None, *, by_row: Literal[False, 'compat']=False, engine: str='python', engine_kwargs: dict[str, bool] | None=None, args, kwargs) -> None:\n        if by_row is not False and by_row != 'compat':\n            raise ValueError(f'by_row={by_row} not allowed')\n        super().__init__(obj, func, raw, result_type, by_row=by_row, engine=engine, engine_kwargs=engine_kwargs, args=args, kwargs=kwargs)\n\n    def apply_with_numba(self) -> dict[int, Any]:\n        nb_func = self.generate_numba_apply_func(cast(Callable, self.func), **self.engine_kwargs)\n        from pandas.core._numba.extensions import set_numba_data\n        index = self.obj.index\n        if index.dtype == 'string':\n            index = index.astype(object)\n        columns = self.obj.columns\n        if columns.dtype == 'string':\n            columns = columns.astype(object)\n        with set_numba_data(index) as index, set_numba_data(columns) as columns:\n            res = dict(nb_func(self.values, columns, index))\n        return res\n\n    def validate_values_for_numba(self):\n        for (colname, dtype) in self.obj.dtypes.items():\n            if not is_numeric_dtype(dtype):\n                raise ValueError(f\"Column {colname} must have a numeric dtype. Found '{dtype}' instead\")\n            if is_extension_array_dtype(dtype):\n                raise ValueError(f'Column {colname} is backed by an extension array, which is not supported by the numba engine.')\n\n    def wrap_results_for_axis(self, results: ResType, res_index: Index) -> DataFrame | Series:\n        \"\"\"return the results for the rows\"\"\"\n        if self.result_type == 'reduce':\n            res = self.obj._constructor_sliced(results)\n            res.index = res_index\n            return res\n        elif self.result_type is None and all((isinstance(x, dict) for x in results.values())):\n            res = self.obj._constructor_sliced(results)\n            res.index = res_index\n            return res\n        try:\n            result = self.obj._constructor(data=results)\n        except ValueError as err:\n            if 'All arrays must be of the same length' in str(err):\n                res = self.obj._constructor_sliced(results)\n                res.index = res_index\n                return res\n            else:\n                raise\n        if not isinstance(results[0], ABCSeries):\n            if len(result.index) == len(self.res_columns):\n                result.index = self.res_columns\n        if len(result.columns) == len(res_index):\n            result.columns = res_index\n        return result", "class_fn": true, "question_id": "pandas/pandas.core.apply/FrameRowApply", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arraylike.py", "fn_id": "", "content": "class OpsMixin:\n\n    def _cmp_method(self, other, op):\n        return NotImplemented\n\n    @unpack_zerodim_and_defer('__eq__')\n    def __eq__(self, other):\n        return self._cmp_method(other, operator.eq)\n\n    @unpack_zerodim_and_defer('__ne__')\n    def __ne__(self, other):\n        return self._cmp_method(other, operator.ne)\n\n    @unpack_zerodim_and_defer('__lt__')\n    def __lt__(self, other):\n        return self._cmp_method(other, operator.lt)\n\n    @unpack_zerodim_and_defer('__le__')\n    def __le__(self, other):\n        return self._cmp_method(other, operator.le)\n\n    @unpack_zerodim_and_defer('__gt__')\n    def __gt__(self, other):\n        return self._cmp_method(other, operator.gt)\n\n    @unpack_zerodim_and_defer('__ge__')\n    def __ge__(self, other):\n        return self._cmp_method(other, operator.ge)\n\n    def _logical_method(self, other, op):\n        return NotImplemented\n\n    @unpack_zerodim_and_defer('__and__')\n    def __and__(self, other):\n        return self._logical_method(other, operator.and_)\n\n    @unpack_zerodim_and_defer('__rand__')\n    def __rand__(self, other):\n        return self._logical_method(other, roperator.rand_)\n\n    @unpack_zerodim_and_defer('__or__')\n    def __or__(self, other):\n        return self._logical_method(other, operator.or_)\n\n    @unpack_zerodim_and_defer('__ror__')\n    def __ror__(self, other):\n        return self._logical_method(other, roperator.ror_)\n\n    @unpack_zerodim_and_defer('__xor__')\n    def __xor__(self, other):\n        return self._logical_method(other, operator.xor)\n\n    @unpack_zerodim_and_defer('__rxor__')\n    def __rxor__(self, other):\n        return self._logical_method(other, roperator.rxor)\n\n    def _arith_method(self, other, op):\n        return NotImplemented\n\n    @unpack_zerodim_and_defer('__add__')\n    def __add__(self, other):\n        \"\"\"\n        Get Addition of DataFrame and other, column-wise.\n\n        Equivalent to ``DataFrame.add(other)``.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, dict or DataFrame\n            Object to be added to the DataFrame.\n\n        Returns\n        -------\n        DataFrame\n            The result of adding ``other`` to DataFrame.\n\n        See Also\n        --------\n        DataFrame.add : Add a DataFrame and another object, with option for index-\n            or column-oriented addition.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'height': [1.5, 2.6], 'weight': [500, 800]},\n        ...                   index=['elk', 'moose'])\n        >>> df\n               height  weight\n        elk       1.5     500\n        moose     2.6     800\n\n        Adding a scalar affects all rows and columns.\n\n        >>> df[['height', 'weight']] + 1.5\n               height  weight\n        elk       3.0   501.5\n        moose     4.1   801.5\n\n        Each element of a list is added to a column of the DataFrame, in order.\n\n        >>> df[['height', 'weight']] + [0.5, 1.5]\n               height  weight\n        elk       2.0   501.5\n        moose     3.1   801.5\n\n        Keys of a dictionary are aligned to the DataFrame, based on column names;\n        each value in the dictionary is added to the corresponding column.\n\n        >>> df[['height', 'weight']] + {'height': 0.5, 'weight': 1.5}\n               height  weight\n        elk       2.0   501.5\n        moose     3.1   801.5\n\n        When `other` is a :class:`Series`, the index of `other` is aligned with the\n        columns of the DataFrame.\n\n        >>> s1 = pd.Series([0.5, 1.5], index=['weight', 'height'])\n        >>> df[['height', 'weight']] + s1\n               height  weight\n        elk       3.0   500.5\n        moose     4.1   800.5\n\n        Even when the index of `other` is the same as the index of the DataFrame,\n        the :class:`Series` will not be reoriented. If index-wise alignment is desired,\n        :meth:`DataFrame.add` should be used with `axis='index'`.\n\n        >>> s2 = pd.Series([0.5, 1.5], index=['elk', 'moose'])\n        >>> df[['height', 'weight']] + s2\n               elk  height  moose  weight\n        elk    NaN     NaN    NaN     NaN\n        moose  NaN     NaN    NaN     NaN\n\n        >>> df[['height', 'weight']].add(s2, axis='index')\n               height  weight\n        elk       2.0   500.5\n        moose     4.1   801.5\n\n        When `other` is a :class:`DataFrame`, both columns names and the\n        index are aligned.\n\n        >>> other = pd.DataFrame({'height': [0.2, 0.4, 0.6]},\n        ...                      index=['elk', 'moose', 'deer'])\n        >>> df[['height', 'weight']] + other\n               height  weight\n        deer      NaN     NaN\n        elk       1.7     NaN\n        moose     3.0     NaN\n        \"\"\"\n        return self._arith_method(other, operator.add)\n\n    @unpack_zerodim_and_defer('__radd__')\n    def __radd__(self, other):\n        return self._arith_method(other, roperator.radd)\n\n    @unpack_zerodim_and_defer('__sub__')\n    def __sub__(self, other):\n        return self._arith_method(other, operator.sub)\n\n    @unpack_zerodim_and_defer('__rsub__')\n    def __rsub__(self, other):\n        return self._arith_method(other, roperator.rsub)\n\n    @unpack_zerodim_and_defer('__mul__')\n    def __mul__(self, other):\n        return self._arith_method(other, operator.mul)\n\n    @unpack_zerodim_and_defer('__rmul__')\n    def __rmul__(self, other):\n        return self._arith_method(other, roperator.rmul)\n\n    @unpack_zerodim_and_defer('__truediv__')\n    def __truediv__(self, other):\n        return self._arith_method(other, operator.truediv)\n\n    @unpack_zerodim_and_defer('__rtruediv__')\n    def __rtruediv__(self, other):\n        return self._arith_method(other, roperator.rtruediv)\n\n    @unpack_zerodim_and_defer('__floordiv__')\n    def __floordiv__(self, other):\n        return self._arith_method(other, operator.floordiv)\n\n    @unpack_zerodim_and_defer('__rfloordiv')\n    def __rfloordiv__(self, other):\n        return self._arith_method(other, roperator.rfloordiv)\n\n    @unpack_zerodim_and_defer('__mod__')\n    def __mod__(self, other):\n        return self._arith_method(other, operator.mod)\n\n    @unpack_zerodim_and_defer('__rmod__')\n    def __rmod__(self, other):\n        return self._arith_method(other, roperator.rmod)\n\n    @unpack_zerodim_and_defer('__divmod__')\n    def __divmod__(self, other):\n        return self._arith_method(other, divmod)\n\n    @unpack_zerodim_and_defer('__rdivmod__')\n    def __rdivmod__(self, other):\n        return self._arith_method(other, roperator.rdivmod)\n\n    @unpack_zerodim_and_defer('__pow__')\n    def __pow__(self, other):\n        return self._arith_method(other, operator.pow)\n\n    @unpack_zerodim_and_defer('__rpow__')\n    def __rpow__(self, other):\n        return self._arith_method(other, roperator.rpow)", "class_fn": true, "question_id": "pandas/pandas.core.arraylike/OpsMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/arrow/accessors.py", "fn_id": "", "content": "class ListAccessor(ArrowAccessor):\n    \"\"\"\n    Accessor object for list data properties of the Series values.\n\n    Parameters\n    ----------\n    data : Series\n        Series containing Arrow list data.\n    \"\"\"\n\n    def __getitem__(self, key: int | slice) -> Series:\n        \"\"\"\n        Index or slice lists in the Series.\n\n        Parameters\n        ----------\n        key : int | slice\n            Index or slice of indices to access from each list.\n\n        Returns\n        -------\n        pandas.Series\n            The list at requested index.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> s = pd.Series(\n        ...     [\n        ...         [1, 2, 3],\n        ...         [3],\n        ...     ],\n        ...     dtype=pd.ArrowDtype(pa.list_(\n        ...         pa.int64()\n        ...     ))\n        ... )\n        >>> s.list[0]\n        0    1\n        1    3\n        dtype: int64[pyarrow]\n        \"\"\"\n        from pandas import Series\n        if isinstance(key, int):\n            element = pc.list_element(self._pa_array, key)\n            return Series(element, dtype=ArrowDtype(element.type))\n        elif isinstance(key, slice):\n            if pa_version_under11p0:\n                raise NotImplementedError(f'List slice not supported by pyarrow {pa.__version__}.')\n            (start, stop, step) = (key.start, key.stop, key.step)\n            if start is None:\n                start = 0\n            if step is None:\n                step = 1\n            sliced = pc.list_slice(self._pa_array, start, stop, step)\n            return Series(sliced, dtype=ArrowDtype(sliced.type))\n        else:\n            raise ValueError(f'key must be an int or slice, got {type(key).__name__}')\n\n    def _is_valid_pyarrow_dtype(self, pyarrow_dtype) -> bool:\n        return pa.types.is_list(pyarrow_dtype) or pa.types.is_fixed_size_list(pyarrow_dtype) or pa.types.is_large_list(pyarrow_dtype)\n\n    def __init__(self, data=None) -> None:\n        super().__init__(data, validation_msg=\"Can only use the '.list' accessor with 'list[pyarrow]' dtype, not {dtype}.\")\n\n    def flatten(self) -> Series:\n        \"\"\"\n        Flatten list values.\n\n        Returns\n        -------\n        pandas.Series\n            The data from all lists in the series flattened.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> s = pd.Series(\n        ...     [\n        ...         [1, 2, 3],\n        ...         [3],\n        ...     ],\n        ...     dtype=pd.ArrowDtype(pa.list_(\n        ...         pa.int64()\n        ...     ))\n        ... )\n        >>> s.list.flatten()\n        0    1\n        1    2\n        2    3\n        3    3\n        dtype: int64[pyarrow]\n        \"\"\"\n        from pandas import Series\n        flattened = pc.list_flatten(self._pa_array)\n        return Series(flattened, dtype=ArrowDtype(flattened.type))\n\n    def __iter__(self) -> Iterator:\n        raise TypeError(f\"'{type(self).__name__}' object is not iterable\")\n\n    def _validate(self, data):\n        dtype = data.dtype\n        if not isinstance(dtype, ArrowDtype):\n            raise AttributeError(self._validation_msg.format(dtype=dtype))\n        if not self._is_valid_pyarrow_dtype(dtype.pyarrow_dtype):\n            raise AttributeError(self._validation_msg.format(dtype=dtype))\n\n    def len(self) -> Series:\n        \"\"\"\n        Return the length of each list in the Series.\n\n        Returns\n        -------\n        pandas.Series\n            The length of each list.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> s = pd.Series(\n        ...     [\n        ...         [1, 2, 3],\n        ...         [3],\n        ...     ],\n        ...     dtype=pd.ArrowDtype(pa.list_(\n        ...         pa.int64()\n        ...     ))\n        ... )\n        >>> s.list.len()\n        0    3\n        1    1\n        dtype: int32[pyarrow]\n        \"\"\"\n        from pandas import Series\n        value_lengths = pc.list_value_length(self._pa_array)\n        return Series(value_lengths, dtype=ArrowDtype(value_lengths.type))", "class_fn": true, "question_id": "pandas/pandas.core.arrays.arrow.accessors/ListAccessor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/base.py", "fn_id": "", "content": "class ExtensionArray:\n    \"\"\"\n    Abstract base class for custom 1-D array types.\n\n    pandas will recognize instances of this class as proper arrays\n    with a custom type and will not attempt to coerce them to objects. They\n    may be stored directly inside a :class:`DataFrame` or :class:`Series`.\n\n    Attributes\n    ----------\n    dtype\n    nbytes\n    ndim\n    shape\n\n    Methods\n    -------\n    argsort\n    astype\n    copy\n    dropna\n    duplicated\n    factorize\n    fillna\n    equals\n    insert\n    interpolate\n    isin\n    isna\n    ravel\n    repeat\n    searchsorted\n    shift\n    take\n    tolist\n    unique\n    view\n    _accumulate\n    _concat_same_type\n    _explode\n    _formatter\n    _from_factorized\n    _from_sequence\n    _from_sequence_of_strings\n    _hash_pandas_object\n    _pad_or_backfill\n    _reduce\n    _values_for_argsort\n    _values_for_factorize\n\n    Notes\n    -----\n    The interface includes the following abstract methods that must be\n    implemented by subclasses:\n\n    * _from_sequence\n    * _from_factorized\n    * __getitem__\n    * __len__\n    * __eq__\n    * dtype\n    * nbytes\n    * isna\n    * take\n    * copy\n    * _concat_same_type\n    * interpolate\n\n    A default repr displaying the type, (truncated) data, length,\n    and dtype is provided. It can be customized or replaced by\n    by overriding:\n\n    * __repr__ : A default repr for the ExtensionArray.\n    * _formatter : Print scalars inside a Series or DataFrame.\n\n    Some methods require casting the ExtensionArray to an ndarray of Python\n    objects with ``self.astype(object)``, which may be expensive. When\n    performance is a concern, we highly recommend overriding the following\n    methods:\n\n    * fillna\n    * _pad_or_backfill\n    * dropna\n    * unique\n    * factorize / _values_for_factorize\n    * argsort, argmax, argmin / _values_for_argsort\n    * searchsorted\n    * map\n\n    The remaining methods implemented on this class should be performant,\n    as they only compose abstract methods. Still, a more efficient\n    implementation may be available, and these methods can be overridden.\n\n    One can implement methods to handle array accumulations or reductions.\n\n    * _accumulate\n    * _reduce\n\n    One can implement methods to handle parsing from strings that will be used\n    in methods such as ``pandas.io.parsers.read_csv``.\n\n    * _from_sequence_of_strings\n\n    This class does not inherit from 'abc.ABCMeta' for performance reasons.\n    Methods and properties required by the interface raise\n    ``pandas.errors.AbstractMethodError`` and no ``register`` method is\n    provided for registering virtual subclasses.\n\n    ExtensionArrays are limited to 1 dimension.\n\n    They may be backed by none, one, or many NumPy arrays. For example,\n    ``pandas.Categorical`` is an extension array backed by two arrays,\n    one for codes and one for categories. An array of IPv6 address may\n    be backed by a NumPy structured array with two fields, one for the\n    lower 64 bits and one for the upper 64 bits. Or they may be backed\n    by some other storage type, like Python lists. Pandas makes no\n    assumptions on how the data are stored, just that it can be converted\n    to a NumPy array.\n    The ExtensionArray interface does not impose any rules on how this data\n    is stored. However, currently, the backing data cannot be stored in\n    attributes called ``.values`` or ``._values`` to ensure full compatibility\n    with pandas internals. But other names as ``.data``, ``._data``,\n    ``._items``, ... can be freely used.\n\n    If implementing NumPy's ``__array_ufunc__`` interface, pandas expects\n    that\n\n    1. You defer by returning ``NotImplemented`` when any Series are present\n       in `inputs`. Pandas will extract the arrays and call the ufunc again.\n    2. You define a ``_HANDLED_TYPES`` tuple as an attribute on the class.\n       Pandas inspect this to determine whether the ufunc is valid for the\n       types present.\n\n    See :ref:`extending.extension.ufunc` for more.\n\n    By default, ExtensionArrays are not hashable.  Immutable subclasses may\n    override this behavior.\n\n    Examples\n    --------\n    Please see the following:\n\n    https://github.com/pandas-dev/pandas/blob/main/pandas/tests/extension/list/array.py\n    \"\"\"\n    _typ = 'extension'\n    __pandas_priority__ = 1000\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype: Dtype | None=None, copy: bool=False):\n        \"\"\"\n        Construct a new ExtensionArray from a sequence of scalars.\n\n        Parameters\n        ----------\n        scalars : Sequence\n            Each element will be an instance of the scalar type for this\n            array, ``cls.dtype.type`` or be converted into this type in this method.\n        dtype : dtype, optional\n            Construct for this particular dtype. This should be a Dtype\n            compatible with the ExtensionArray.\n        copy : bool, default False\n            If True, copy the underlying data.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> pd.arrays.IntegerArray._from_sequence([4, 5])\n        <IntegerArray>\n        [4, 5]\n        Length: 2, dtype: Int64\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _from_scalars(cls, scalars, *, dtype: DtypeObj) -> Self:\n        \"\"\"\n        Strict analogue to _from_sequence, allowing only sequences of scalars\n        that should be specifically inferred to the given dtype.\n\n        Parameters\n        ----------\n        scalars : sequence\n        dtype : ExtensionDtype\n\n        Raises\n        ------\n        TypeError or ValueError\n\n        Notes\n        -----\n        This is called in a try/except block when casting the result of a\n        pointwise operation.\n        \"\"\"\n        try:\n            return cls._from_sequence(scalars, dtype=dtype, copy=False)\n        except (ValueError, TypeError):\n            raise\n        except Exception:\n            warnings.warn('_from_scalars should only raise ValueError or TypeError. Consider overriding _from_scalars where appropriate.', stacklevel=find_stack_level())\n            raise\n\n    @classmethod\n    def _from_sequence_of_strings(cls, strings, *, dtype: Dtype | None=None, copy: bool=False):\n        \"\"\"\n        Construct a new ExtensionArray from a sequence of strings.\n\n        Parameters\n        ----------\n        strings : Sequence\n            Each element will be an instance of the scalar type for this\n            array, ``cls.dtype.type``.\n        dtype : dtype, optional\n            Construct for this particular dtype. This should be a Dtype\n            compatible with the ExtensionArray.\n        copy : bool, default False\n            If True, copy the underlying data.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> pd.arrays.IntegerArray._from_sequence_of_strings([\"1\", \"2\", \"3\"])\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        \"\"\"\n        Reconstruct an ExtensionArray after factorization.\n\n        Parameters\n        ----------\n        values : ndarray\n            An integer ndarray with the factorized values.\n        original : ExtensionArray\n            The original ExtensionArray that factorize was called on.\n\n        See Also\n        --------\n        factorize : Top-level factorize method that dispatches here.\n        ExtensionArray.factorize : Encode the extension array as an enumerated type.\n\n        Examples\n        --------\n        >>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1),\n        ...                                      pd.Interval(1, 5), pd.Interval(1, 5)])\n        >>> codes, uniques = pd.factorize(interv_arr)\n        >>> pd.arrays.IntervalArray._from_factorized(uniques, interv_arr)\n        <IntervalArray>\n        [(0, 1], (1, 5]]\n        Length: 2, dtype: interval[int64, right]\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    @overload\n    def __getitem__(self, item: ScalarIndexer) -> Any:\n        ...\n\n    @overload\n    def __getitem__(self, item: SequenceIndexer) -> Self:\n        ...\n\n    def __getitem__(self, item: PositionalIndexer) -> Self | Any:\n        \"\"\"\n        Select a subset of self.\n\n        Parameters\n        ----------\n        item : int, slice, or ndarray\n            * int: The position in 'self' to get.\n\n            * slice: A slice object, where 'start', 'stop', and 'step' are\n              integers or None\n\n            * ndarray: A 1-d boolean NumPy ndarray the same length as 'self'\n\n            * list[int]:  A list of int\n\n        Returns\n        -------\n        item : scalar or ExtensionArray\n\n        Notes\n        -----\n        For scalar ``item``, return a scalar value suitable for the array's\n        type. This should be an instance of ``self.dtype.type``.\n\n        For slice ``key``, return an instance of ``ExtensionArray``, even\n        if the slice is length 0 or 1.\n\n        For a boolean mask, return an instance of ``ExtensionArray``, filtered\n        to the values where ``item`` is True.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def __setitem__(self, key, value) -> None:\n        \"\"\"\n        Set one or more values inplace.\n\n        This method is not required to satisfy the pandas extension array\n        interface.\n\n        Parameters\n        ----------\n        key : int, ndarray, or slice\n            When called from, e.g. ``Series.__setitem__``, ``key`` will be\n            one of\n\n            * scalar int\n            * ndarray of integers.\n            * boolean ndarray\n            * slice object\n\n        value : ExtensionDtype.type, Sequence[ExtensionDtype.type], or object\n            value or values to be set of ``key``.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        raise NotImplementedError(f'{type(self)} does not implement __setitem__.')\n\n    def __len__(self) -> int:\n        \"\"\"\n        Length of this array\n\n        Returns\n        -------\n        length : int\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def __iter__(self) -> Iterator[Any]:\n        \"\"\"\n        Iterate over elements of the array.\n        \"\"\"\n        for i in range(len(self)):\n            yield self[i]\n\n    def __contains__(self, item: object) -> bool | np.bool_:\n        \"\"\"\n        Return for `item in self`.\n        \"\"\"\n        if is_scalar(item) and isna(item):\n            if not self._can_hold_na:\n                return False\n            elif item is self.dtype.na_value or isinstance(item, self.dtype.type):\n                return self._hasna\n            else:\n                return False\n        else:\n            return (item == self).any()\n\n    def __eq__(self, other: object) -> ArrayLike:\n        \"\"\"\n        Return for `self == other` (element-wise equality).\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def __ne__(self, other: object) -> ArrayLike:\n        \"\"\"\n        Return for `self != other` (element-wise in-equality).\n        \"\"\"\n        return ~(self == other)\n\n    def to_numpy(self, dtype: npt.DTypeLike | None=None, copy: bool=False, na_value: object=lib.no_default) -> np.ndarray:\n        \"\"\"\n        Convert to a NumPy ndarray.\n\n        This is similar to :meth:`numpy.asarray`, but may provide additional control\n        over how the conversion is done.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to pass to :meth:`numpy.asarray`.\n        copy : bool, default False\n            Whether to ensure that the returned value is a not a view on\n            another array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary.\n        na_value : Any, optional\n            The value to use for missing values. The default value depends\n            on `dtype` and the type of the array.\n\n        Returns\n        -------\n        numpy.ndarray\n        \"\"\"\n        result = np.asarray(self, dtype=dtype)\n        if copy or na_value is not lib.no_default:\n            result = result.copy()\n        if na_value is not lib.no_default:\n            result[self.isna()] = na_value\n        return result\n\n    @property\n    def dtype(self) -> ExtensionDtype:\n        \"\"\"\n        An instance of ExtensionDtype.\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3]).dtype\n        Int64Dtype()\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def shape(self) -> Shape:\n        \"\"\"\n        Return a tuple of the array dimensions.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.shape\n        (3,)\n        \"\"\"\n        return (len(self),)\n\n    @property\n    def size(self) -> int:\n        \"\"\"\n        The number of elements in the array.\n        \"\"\"\n        return np.prod(self.shape)\n\n    @property\n    def ndim(self) -> int:\n        \"\"\"\n        Extension Arrays are only allowed to be 1-dimensional.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.ndim\n        1\n        \"\"\"\n        return 1\n\n    @property\n    def nbytes(self) -> int:\n        \"\"\"\n        The number of bytes needed to store this object in memory.\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3]).nbytes\n        27\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @overload\n    def astype(self, dtype: npt.DTypeLike, copy: bool=...) -> np.ndarray:\n        ...\n\n    @overload\n    def astype(self, dtype: ExtensionDtype, copy: bool=...) -> ExtensionArray:\n        ...\n\n    @overload\n    def astype(self, dtype: AstypeArg, copy: bool=...) -> ArrayLike:\n        ...\n\n    def astype(self, dtype: AstypeArg, copy: bool=True) -> ArrayLike:\n        \"\"\"\n        Cast to a NumPy array or ExtensionArray with 'dtype'.\n\n        Parameters\n        ----------\n        dtype : str or dtype\n            Typecode or data-type to which the array is cast.\n        copy : bool, default True\n            Whether to copy the data, even if not necessary. If False,\n            a copy is made only if the old dtype does not match the\n            new dtype.\n\n        Returns\n        -------\n        np.ndarray or pandas.api.extensions.ExtensionArray\n            An ``ExtensionArray`` if ``dtype`` is ``ExtensionDtype``,\n            otherwise a Numpy ndarray with ``dtype`` for its dtype.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n\n        Casting to another ``ExtensionDtype`` returns an ``ExtensionArray``:\n\n        >>> arr1 = arr.astype('Float64')\n        >>> arr1\n        <FloatingArray>\n        [1.0, 2.0, 3.0]\n        Length: 3, dtype: Float64\n        >>> arr1.dtype\n        Float64Dtype()\n\n        Otherwise, we will get a Numpy ndarray:\n\n        >>> arr2 = arr.astype('float64')\n        >>> arr2\n        array([1., 2., 3.])\n        >>> arr2.dtype\n        dtype('float64')\n        \"\"\"\n        dtype = pandas_dtype(dtype)\n        if dtype == self.dtype:\n            if not copy:\n                return self\n            else:\n                return self.copy()\n        if isinstance(dtype, ExtensionDtype):\n            cls = dtype.construct_array_type()\n            return cls._from_sequence(self, dtype=dtype, copy=copy)\n        elif lib.is_np_dtype(dtype, 'M'):\n            from pandas.core.arrays import DatetimeArray\n            return DatetimeArray._from_sequence(self, dtype=dtype, copy=copy)\n        elif lib.is_np_dtype(dtype, 'm'):\n            from pandas.core.arrays import TimedeltaArray\n            return TimedeltaArray._from_sequence(self, dtype=dtype, copy=copy)\n        if not copy:\n            return np.asarray(self, dtype=dtype)\n        else:\n            return np.array(self, dtype=dtype, copy=copy)\n\n    def isna(self) -> np.ndarray | ExtensionArraySupportsAnyAll:\n        \"\"\"\n        A 1-D array indicating if each value is missing.\n\n        Returns\n        -------\n        numpy.ndarray or pandas.api.extensions.ExtensionArray\n            In most cases, this should return a NumPy ndarray. For\n            exceptional cases like ``SparseArray``, where returning\n            an ndarray would be expensive, an ExtensionArray may be\n            returned.\n\n        Notes\n        -----\n        If returning an ExtensionArray, then\n\n        * ``na_values._is_boolean`` should be True\n        * `na_values` should implement :func:`ExtensionArray._reduce`\n        * ``na_values.any`` and ``na_values.all`` should be implemented\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, np.nan, np.nan])\n        >>> arr.isna()\n        array([False, False,  True,  True])\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def _hasna(self) -> bool:\n        \"\"\"\n        Equivalent to `self.isna().any()`.\n\n        Some ExtensionArray subclasses may be able to optimize this check.\n        \"\"\"\n        return bool(self.isna().any())\n\n    def _values_for_argsort(self) -> np.ndarray:\n        \"\"\"\n        Return values for sorting.\n\n        Returns\n        -------\n        ndarray\n            The transformed values should maintain the ordering between values\n            within the array.\n\n        See Also\n        --------\n        ExtensionArray.argsort : Return the indices that would sort this array.\n\n        Notes\n        -----\n        The caller is responsible for *not* modifying these values in-place, so\n        it is safe for implementers to give views on ``self``.\n\n        Functions that use this (e.g. ``ExtensionArray.argsort``) should ignore\n        entries with missing values in the original array (according to\n        ``self.isna()``). This means that the corresponding entries in the returned\n        array don't need to be modified to sort correctly.\n\n        Examples\n        --------\n        In most cases, this is the underlying Numpy array of the ``ExtensionArray``:\n\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr._values_for_argsort()\n        array([1, 2, 3])\n        \"\"\"\n        return np.array(self)\n\n    def argsort(self, *, ascending: bool=True, kind: SortKind='quicksort', na_position: str='last', **kwargs) -> np.ndarray:\n        \"\"\"\n        Return the indices that would sort this array.\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            Whether the indices should result in an ascending\n            or descending sort.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n            Sorting algorithm.\n        na_position : {'first', 'last'}, default 'last'\n            If ``'first'``, put ``NaN`` values at the beginning.\n            If ``'last'``, put ``NaN`` values at the end.\n        *args, **kwargs:\n            Passed through to :func:`numpy.argsort`.\n\n        Returns\n        -------\n        np.ndarray[np.intp]\n            Array of indices that sort ``self``. If NaN values are contained,\n            NaN values are placed at the end.\n\n        See Also\n        --------\n        numpy.argsort : Sorting implementation used internally.\n\n        Examples\n        --------\n        >>> arr = pd.array([3, 1, 2, 5, 4])\n        >>> arr.argsort()\n        array([1, 2, 0, 4, 3])\n        \"\"\"\n        ascending = nv.validate_argsort_with_ascending(ascending, (), kwargs)\n        values = self._values_for_argsort()\n        return nargsort(values, kind=kind, ascending=ascending, na_position=na_position, mask=np.asarray(self.isna()))\n\n    def argmin(self, skipna: bool=True) -> int:\n        \"\"\"\n        Return the index of minimum value.\n\n        In case of multiple occurrences of the minimum value, the index\n        corresponding to the first occurrence is returned.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n\n        Returns\n        -------\n        int\n\n        See Also\n        --------\n        ExtensionArray.argmax : Return the index of the maximum value.\n\n        Examples\n        --------\n        >>> arr = pd.array([3, 1, 2, 5, 4])\n        >>> arr.argmin()\n        1\n        \"\"\"\n        validate_bool_kwarg(skipna, 'skipna')\n        if not skipna and self._hasna:\n            raise NotImplementedError\n        return nargminmax(self, 'argmin')\n\n    def argmax(self, skipna: bool=True) -> int:\n        \"\"\"\n        Return the index of maximum value.\n\n        In case of multiple occurrences of the maximum value, the index\n        corresponding to the first occurrence is returned.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n\n        Returns\n        -------\n        int\n\n        See Also\n        --------\n        ExtensionArray.argmin : Return the index of the minimum value.\n\n        Examples\n        --------\n        >>> arr = pd.array([3, 1, 2, 5, 4])\n        >>> arr.argmax()\n        3\n        \"\"\"\n        validate_bool_kwarg(skipna, 'skipna')\n        if not skipna and self._hasna:\n            raise NotImplementedError\n        return nargminmax(self, 'argmax')\n\n    def interpolate(self, *, method: InterpolateOptions, axis: int, index: Index, limit, limit_direction, limit_area, copy: bool, **kwargs) -> Self:\n        \"\"\"\n        See DataFrame.interpolate.__doc__.\n\n        Examples\n        --------\n        >>> arr = pd.arrays.NumpyExtensionArray(np.array([0, 1, np.nan, 3]))\n        >>> arr.interpolate(method=\"linear\",\n        ...                 limit=3,\n        ...                 limit_direction=\"forward\",\n        ...                 index=pd.Index([1, 2, 3, 4]),\n        ...                 fill_value=1,\n        ...                 copy=False,\n        ...                 axis=0,\n        ...                 limit_area=\"inside\"\n        ...                 )\n        <NumpyExtensionArray>\n        [0.0, 1.0, 2.0, 3.0]\n        Length: 4, dtype: float64\n        \"\"\"\n        raise NotImplementedError(f'{type(self).__name__} does not implement interpolate')\n\n    def _pad_or_backfill(self, *, method: FillnaOptions, limit: int | None=None, limit_area: Literal['inside', 'outside'] | None=None, copy: bool=True) -> Self:\n        \"\"\"\n        Pad or backfill values, used by Series/DataFrame ffill and bfill.\n\n        Parameters\n        ----------\n        method : {'backfill', 'bfill', 'pad', 'ffill'}\n            Method to use for filling holes in reindexed Series:\n\n            * pad / ffill: propagate last valid observation forward to next valid.\n            * backfill / bfill: use NEXT valid observation to fill gap.\n\n        limit : int, default None\n            This is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled.\n\n        copy : bool, default True\n            Whether to make a copy of the data before filling. If False, then\n            the original should be modified and no new memory should be allocated.\n            For ExtensionArray subclasses that cannot do this, it is at the\n            author's discretion whether to ignore \"copy=False\" or to raise.\n            The base class implementation ignores the keyword if any NAs are\n            present.\n\n        Returns\n        -------\n        Same type as self\n\n        Examples\n        --------\n        >>> arr = pd.array([np.nan, np.nan, 2, 3, np.nan, np.nan])\n        >>> arr._pad_or_backfill(method=\"backfill\", limit=1)\n        <IntegerArray>\n        [<NA>, 2, 2, 3, <NA>, <NA>]\n        Length: 6, dtype: Int64\n        \"\"\"\n        if type(self).fillna is not ExtensionArray.fillna and type(self)._pad_or_backfill is ExtensionArray._pad_or_backfill:\n            warnings.warn(\"ExtensionArray.fillna 'method' keyword is deprecated. In a future version. arr._pad_or_backfill will be called instead. 3rd-party ExtensionArray authors need to implement _pad_or_backfill.\", DeprecationWarning, stacklevel=find_stack_level())\n            if limit_area is not None:\n                raise NotImplementedError(f'{type(self).__name__} does not implement limit_area (added in pandas 2.2). 3rd-party ExtnsionArray authors need to add this argument to _pad_or_backfill.')\n            return self.fillna(method=method, limit=limit)\n        mask = self.isna()\n        if mask.any():\n            meth = missing.clean_fill_method(method)\n            npmask = np.asarray(mask)\n            if limit_area is not None and (not npmask.all()):\n                _fill_limit_area_1d(npmask, limit_area)\n            if meth == 'pad':\n                indexer = libalgos.get_fill_indexer(npmask, limit=limit)\n                return self.take(indexer, allow_fill=True)\n            else:\n                indexer = libalgos.get_fill_indexer(npmask[::-1], limit=limit)[::-1]\n                return self[::-1].take(indexer, allow_fill=True)\n        else:\n            if not copy:\n                return self\n            new_values = self.copy()\n        return new_values\n\n    def fillna(self, value: object | ArrayLike | None=None, method: FillnaOptions | None=None, limit: int | None=None, copy: bool=True) -> Self:\n        \"\"\"\n        Fill NA/NaN values using the specified method.\n\n        Parameters\n        ----------\n        value : scalar, array-like\n            If a scalar value is passed it is used to fill all missing values.\n            Alternatively, an array-like \"value\" can be given. It's expected\n            that the array-like have the same length as 'self'.\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n            Method to use for filling holes in reindexed Series:\n\n            * pad / ffill: propagate last valid observation forward to next valid.\n            * backfill / bfill: use NEXT valid observation to fill gap.\n\n            .. deprecated:: 2.1.0\n\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled.\n\n            .. deprecated:: 2.1.0\n\n        copy : bool, default True\n            Whether to make a copy of the data before filling. If False, then\n            the original should be modified and no new memory should be allocated.\n            For ExtensionArray subclasses that cannot do this, it is at the\n            author's discretion whether to ignore \"copy=False\" or to raise.\n            The base class implementation ignores the keyword in pad/backfill\n            cases.\n\n        Returns\n        -------\n        ExtensionArray\n            With NA/NaN filled.\n\n        Examples\n        --------\n        >>> arr = pd.array([np.nan, np.nan, 2, 3, np.nan, np.nan])\n        >>> arr.fillna(0)\n        <IntegerArray>\n        [0, 0, 2, 3, 0, 0]\n        Length: 6, dtype: Int64\n        \"\"\"\n        if method is not None:\n            warnings.warn(f\"The 'method' keyword in {type(self).__name__}.fillna is deprecated and will be removed in a future version.\", FutureWarning, stacklevel=find_stack_level())\n        (value, method) = validate_fillna_kwargs(value, method)\n        mask = self.isna()\n        value = missing.check_value_size(value, mask, len(self))\n        if mask.any():\n            if method is not None:\n                meth = missing.clean_fill_method(method)\n                npmask = np.asarray(mask)\n                if meth == 'pad':\n                    indexer = libalgos.get_fill_indexer(npmask, limit=limit)\n                    return self.take(indexer, allow_fill=True)\n                else:\n                    indexer = libalgos.get_fill_indexer(npmask[::-1], limit=limit)[::-1]\n                    return self[::-1].take(indexer, allow_fill=True)\n            else:\n                if not copy:\n                    new_values = self[:]\n                else:\n                    new_values = self.copy()\n                new_values[mask] = value\n        elif not copy:\n            new_values = self[:]\n        else:\n            new_values = self.copy()\n        return new_values\n\n    def dropna(self) -> Self:\n        \"\"\"\n        Return ExtensionArray without NA values.\n\n        Returns\n        -------\n\n        Examples\n        --------\n        >>> pd.array([1, 2, np.nan]).dropna()\n        <IntegerArray>\n        [1, 2]\n        Length: 2, dtype: Int64\n        \"\"\"\n        return self[~self.isna()]\n\n    def duplicated(self, keep: Literal['first', 'last', False]='first') -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Return boolean ndarray denoting duplicate values.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n            - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n            - False : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        ndarray[bool]\n\n        Examples\n        --------\n        >>> pd.array([1, 1, 2, 3, 3], dtype=\"Int64\").duplicated()\n        array([False,  True, False, False,  True])\n        \"\"\"\n        mask = self.isna().astype(np.bool_, copy=False)\n        return duplicated(values=self, keep=keep, mask=mask)\n\n    def shift(self, periods: int=1, fill_value: object=None) -> ExtensionArray:\n        \"\"\"\n        Shift values by desired number.\n\n        Newly introduced missing values are filled with\n        ``self.dtype.na_value``.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            The number of periods to shift. Negative values are allowed\n            for shifting backwards.\n\n        fill_value : object, optional\n            The scalar value to use for newly introduced missing values.\n            The default is ``self.dtype.na_value``.\n\n        Returns\n        -------\n        ExtensionArray\n            Shifted.\n\n        Notes\n        -----\n        If ``self`` is empty or ``periods`` is 0, a copy of ``self`` is\n        returned.\n\n        If ``periods > len(self)``, then an array of size\n        len(self) is returned, with all values filled with\n        ``self.dtype.na_value``.\n\n        For 2-dimensional ExtensionArrays, we are always shifting along axis=0.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.shift(2)\n        <IntegerArray>\n        [<NA>, <NA>, 1]\n        Length: 3, dtype: Int64\n        \"\"\"\n        if not len(self) or periods == 0:\n            return self.copy()\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n        empty = self._from_sequence([fill_value] * min(abs(periods), len(self)), dtype=self.dtype)\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods):]\n            b = empty\n        return self._concat_same_type([a, b])\n\n    def unique(self) -> Self:\n        \"\"\"\n        Compute the ExtensionArray of unique values.\n\n        Returns\n        -------\n        pandas.api.extensions.ExtensionArray\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3, 1, 2, 3])\n        >>> arr.unique()\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        uniques = unique(self.astype(object))\n        return self._from_sequence(uniques, dtype=self.dtype)\n\n    def searchsorted(self, value: NumpyValueArrayLike | ExtensionArray, side: Literal['left', 'right']='left', sorter: NumpySorter | None=None) -> npt.NDArray[np.intp] | np.intp:\n        \"\"\"\n        Find indices where elements should be inserted to maintain order.\n\n        Find the indices into a sorted array `self` (a) such that, if the\n        corresponding elements in `value` were inserted before the indices,\n        the order of `self` would be preserved.\n\n        Assuming that `self` is sorted:\n\n        ======  ================================\n        `side`  returned index `i` satisfies\n        ======  ================================\n        left    ``self[i-1] < value <= self[i]``\n        right   ``self[i-1] <= value < self[i]``\n        ======  ================================\n\n        Parameters\n        ----------\n        value : array-like, list or scalar\n            Value(s) to insert into `self`.\n        side : {'left', 'right'}, optional\n            If 'left', the index of the first suitable location found is given.\n            If 'right', return the last such index.  If there is no suitable\n            index, return either 0 or N (where N is the length of `self`).\n        sorter : 1-D array-like, optional\n            Optional array of integer indices that sort array a into ascending\n            order. They are typically the result of argsort.\n\n        Returns\n        -------\n        array of ints or int\n            If value is array-like, array of insertion points.\n            If value is scalar, a single integer.\n\n        See Also\n        --------\n        numpy.searchsorted : Similar method from NumPy.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3, 5])\n        >>> arr.searchsorted([4])\n        array([3])\n        \"\"\"\n        arr = self.astype(object)\n        if isinstance(value, ExtensionArray):\n            value = value.astype(object)\n        return arr.searchsorted(value, side=side, sorter=sorter)\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Return if another array is equivalent to this array.\n\n        Equivalent means that both arrays have the same shape and dtype, and\n        all values compare equal. Missing values in the same location are\n        considered equal (in contrast with normal equality).\n\n        Parameters\n        ----------\n        other : ExtensionArray\n            Array to compare to this Array.\n\n        Returns\n        -------\n        boolean\n            Whether the arrays are equivalent.\n\n        Examples\n        --------\n        >>> arr1 = pd.array([1, 2, np.nan])\n        >>> arr2 = pd.array([1, 2, np.nan])\n        >>> arr1.equals(arr2)\n        True\n        \"\"\"\n        if type(self) != type(other):\n            return False\n        other = cast(ExtensionArray, other)\n        if self.dtype != other.dtype:\n            return False\n        elif len(self) != len(other):\n            return False\n        else:\n            equal_values = self == other\n            if isinstance(equal_values, ExtensionArray):\n                equal_values = equal_values.fillna(False)\n            equal_na = self.isna() & other.isna()\n            return bool((equal_values | equal_na).all())\n\n    def isin(self, values: ArrayLike) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Pointwise comparison for set containment in the given values.\n\n        Roughly equivalent to `np.array([x in values for x in self])`\n\n        Parameters\n        ----------\n        values : np.ndarray or ExtensionArray\n\n        Returns\n        -------\n        np.ndarray[bool]\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.isin([1])\n        <BooleanArray>\n        [True, False, False]\n        Length: 3, dtype: boolean\n        \"\"\"\n        return isin(np.asarray(self), values)\n\n    def _values_for_factorize(self) -> tuple[np.ndarray, Any]:\n        \"\"\"\n        Return an array and missing value suitable for factorization.\n\n        Returns\n        -------\n        values : ndarray\n            An array suitable for factorization. This should maintain order\n            and be a supported dtype (Float64, Int64, UInt64, String, Object).\n            By default, the extension array is cast to object dtype.\n        na_value : object\n            The value in `values` to consider missing. This will be treated\n            as NA in the factorization routines, so it will be coded as\n            `-1` and not included in `uniques`. By default,\n            ``np.nan`` is used.\n\n        Notes\n        -----\n        The values returned by this method are also used in\n        :func:`pandas.util.hash_pandas_object`. If needed, this can be\n        overridden in the ``self._hash_pandas_object()`` method.\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3])._values_for_factorize()\n        (array([1, 2, 3], dtype=object), nan)\n        \"\"\"\n        return (self.astype(object), np.nan)\n\n    def factorize(self, use_na_sentinel: bool=True) -> tuple[np.ndarray, ExtensionArray]:\n        \"\"\"\n        Encode the extension array as an enumerated type.\n\n        Parameters\n        ----------\n        use_na_sentinel : bool, default True\n            If True, the sentinel -1 will be used for NaN values. If False,\n            NaN values will be encoded as non-negative integers and will not drop the\n            NaN from the uniques of the values.\n\n            .. versionadded:: 1.5.0\n\n        Returns\n        -------\n        codes : ndarray\n            An integer NumPy array that's an indexer into the original\n            ExtensionArray.\n        uniques : ExtensionArray\n            An ExtensionArray containing the unique values of `self`.\n\n            .. note::\n\n               uniques will *not* contain an entry for the NA value of\n               the ExtensionArray if there are any missing values present\n               in `self`.\n\n        See Also\n        --------\n        factorize : Top-level factorize method that dispatches here.\n\n        Notes\n        -----\n        :meth:`pandas.factorize` offers a `sort` keyword as well.\n\n        Examples\n        --------\n        >>> idx1 = pd.PeriodIndex([\"2014-01\", \"2014-01\", \"2014-02\", \"2014-02\",\n        ...                       \"2014-03\", \"2014-03\"], freq=\"M\")\n        >>> arr, idx = idx1.factorize()\n        >>> arr\n        array([0, 0, 1, 1, 2, 2])\n        >>> idx\n        PeriodIndex(['2014-01', '2014-02', '2014-03'], dtype='period[M]')\n        \"\"\"\n        (arr, na_value) = self._values_for_factorize()\n        (codes, uniques) = factorize_array(arr, use_na_sentinel=use_na_sentinel, na_value=na_value)\n        uniques_ea = self._from_factorized(uniques, self)\n        return (codes, uniques_ea)\n    _extension_array_shared_docs['repeat'] = \"\\n        Repeat elements of a %(klass)s.\\n\\n        Returns a new %(klass)s where each element of the current %(klass)s\\n        is repeated consecutively a given number of times.\\n\\n        Parameters\\n        ----------\\n        repeats : int or array of ints\\n            The number of repetitions for each element. This should be a\\n            non-negative integer. Repeating 0 times will return an empty\\n            %(klass)s.\\n        axis : None\\n            Must be ``None``. Has no effect but is accepted for compatibility\\n            with numpy.\\n\\n        Returns\\n        -------\\n        %(klass)s\\n            Newly created %(klass)s with repeated elements.\\n\\n        See Also\\n        --------\\n        Series.repeat : Equivalent function for Series.\\n        Index.repeat : Equivalent function for Index.\\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\\n        ExtensionArray.take : Take arbitrary positions.\\n\\n        Examples\\n        --------\\n        >>> cat = pd.Categorical(['a', 'b', 'c'])\\n        >>> cat\\n        ['a', 'b', 'c']\\n        Categories (3, object): ['a', 'b', 'c']\\n        >>> cat.repeat(2)\\n        ['a', 'a', 'b', 'b', 'c', 'c']\\n        Categories (3, object): ['a', 'b', 'c']\\n        >>> cat.repeat([1, 2, 3])\\n        ['a', 'b', 'b', 'c', 'c', 'c']\\n        Categories (3, object): ['a', 'b', 'c']\\n        \"\n\n    @Substitution(klass='ExtensionArray')\n    @Appender(_extension_array_shared_docs['repeat'])\n    def repeat(self, repeats: int | Sequence[int], axis: AxisInt | None=None) -> Self:\n        nv.validate_repeat((), {'axis': axis})\n        ind = np.arange(len(self)).repeat(repeats)\n        return self.take(ind)\n\n    def take(self, indices: TakeIndexer, *, allow_fill: bool=False, fill_value: Any=None) -> Self:\n        \"\"\"\n        Take elements from an array.\n\n        Parameters\n        ----------\n        indices : sequence of int or one-dimensional np.ndarray of int\n            Indices to be taken.\n        allow_fill : bool, default False\n            How to handle negative values in `indices`.\n\n            * False: negative values in `indices` indicate positional indices\n              from the right (the default). This is similar to\n              :func:`numpy.take`.\n\n            * True: negative values in `indices` indicate\n              missing values. These values are set to `fill_value`. Any other\n              other negative values raise a ``ValueError``.\n\n        fill_value : any, optional\n            Fill value to use for NA-indices when `allow_fill` is True.\n            This may be ``None``, in which case the default NA value for\n            the type, ``self.dtype.na_value``, is used.\n\n            For many ExtensionArrays, there will be two representations of\n            `fill_value`: a user-facing \"boxed\" scalar, and a low-level\n            physical NA value. `fill_value` should be the user-facing version,\n            and the implementation should handle translating that to the\n            physical version for processing the take if necessary.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Raises\n        ------\n        IndexError\n            When the indices are out of bounds for the array.\n        ValueError\n            When `indices` contains negative values other than ``-1``\n            and `allow_fill` is True.\n\n        See Also\n        --------\n        numpy.take : Take elements from an array along an axis.\n        api.extensions.take : Take elements from an array.\n\n        Notes\n        -----\n        ExtensionArray.take is called by ``Series.__getitem__``, ``.loc``,\n        ``iloc``, when `indices` is a sequence of values. Additionally,\n        it's called by :meth:`Series.reindex`, or any other method\n        that causes realignment, with a `fill_value`.\n\n        Examples\n        --------\n        Here's an example implementation, which relies on casting the\n        extension array to object dtype. This uses the helper method\n        :func:`pandas.api.extensions.take`.\n\n        .. code-block:: python\n\n           def take(self, indices, allow_fill=False, fill_value=None):\n               from pandas.core.algorithms import take\n\n               # If the ExtensionArray is backed by an ndarray, then\n               # just pass that here instead of coercing to object.\n               data = self.astype(object)\n\n               if allow_fill and fill_value is None:\n                   fill_value = self.dtype.na_value\n\n               # fill value should always be translated from the scalar\n               # type for the array, to the physical storage type for\n               # the data, before passing to take.\n\n               result = take(data, indices, fill_value=fill_value,\n                             allow_fill=allow_fill)\n               return self._from_sequence(result, dtype=self.dtype)\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def copy(self) -> Self:\n        \"\"\"\n        Return a copy of the array.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr2 = arr.copy()\n        >>> arr[0] = 2\n        >>> arr2\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def view(self, dtype: Dtype | None=None) -> ArrayLike:\n        \"\"\"\n        Return a view on the array.\n\n        Parameters\n        ----------\n        dtype : str, np.dtype, or ExtensionDtype, optional\n            Default None.\n\n        Returns\n        -------\n        ExtensionArray or np.ndarray\n            A view on the :class:`ExtensionArray`'s data.\n\n        Examples\n        --------\n        This gives view on the underlying data of an ``ExtensionArray`` and is not a\n        copy. Modifications on either the view or the original ``ExtensionArray``\n        will be reflectd on the underlying data:\n\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr2 = arr.view()\n        >>> arr[0] = 2\n        >>> arr2\n        <IntegerArray>\n        [2, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        if dtype is not None:\n            raise NotImplementedError(dtype)\n        return self[:]\n\n    def __repr__(self) -> str:\n        if self.ndim > 1:\n            return self._repr_2d()\n        from pandas.io.formats.printing import format_object_summary\n        data = format_object_summary(self, self._formatter(), indent_for_name=False).rstrip(', \\n')\n        class_name = f'<{type(self).__name__}>\\n'\n        footer = self._get_repr_footer()\n        return f'{class_name}{data}\\n{footer}'\n\n    def _get_repr_footer(self) -> str:\n        if self.ndim > 1:\n            return f'Shape: {self.shape}, dtype: {self.dtype}'\n        return f'Length: {len(self)}, dtype: {self.dtype}'\n\n    def _repr_2d(self) -> str:\n        from pandas.io.formats.printing import format_object_summary\n        lines = [format_object_summary(x, self._formatter(), indent_for_name=False).rstrip(', \\n') for x in self]\n        data = ',\\n'.join(lines)\n        class_name = f'<{type(self).__name__}>'\n        footer = self._get_repr_footer()\n        return f'{class_name}\\n[\\n{data}\\n]\\n{footer}'\n\n    def _formatter(self, boxed: bool=False) -> Callable[[Any], str | None]:\n        \"\"\"\n        Formatting function for scalar values.\n\n        This is used in the default '__repr__'. The returned formatting\n        function receives instances of your scalar type.\n\n        Parameters\n        ----------\n        boxed : bool, default False\n            An indicated for whether or not your array is being printed\n            within a Series, DataFrame, or Index (True), or just by\n            itself (False). This may be useful if you want scalar values\n            to appear differently within a Series versus on its own (e.g.\n            quoted or not).\n\n        Returns\n        -------\n        Callable[[Any], str]\n            A callable that gets instances of the scalar type and\n            returns a string. By default, :func:`repr` is used\n            when ``boxed=False`` and :func:`str` is used when\n            ``boxed=True``.\n\n        Examples\n        --------\n        >>> class MyExtensionArray(pd.arrays.NumpyExtensionArray):\n        ...     def _formatter(self, boxed=False):\n        ...         return lambda x: '*' + str(x) + '*' if boxed else repr(x) + '*'\n        >>> MyExtensionArray(np.array([1, 2, 3, 4]))\n        <MyExtensionArray>\n        [1*, 2*, 3*, 4*]\n        Length: 4, dtype: int64\n        \"\"\"\n        if boxed:\n            return str\n        return repr\n\n    def transpose(self, *axes: int) -> ExtensionArray:\n        \"\"\"\n        Return a transposed view on this array.\n\n        Because ExtensionArrays are always 1D, this is a no-op.  It is included\n        for compatibility with np.ndarray.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3]).transpose()\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        return self[:]\n\n    @property\n    def T(self) -> ExtensionArray:\n        return self.transpose()\n\n    def ravel(self, order: Literal['C', 'F', 'A', 'K'] | None='C') -> ExtensionArray:\n        \"\"\"\n        Return a flattened view on this array.\n\n        Parameters\n        ----------\n        order : {None, 'C', 'F', 'A', 'K'}, default 'C'\n\n        Returns\n        -------\n        ExtensionArray\n\n        Notes\n        -----\n        - Because ExtensionArrays are 1D-only, this is a no-op.\n        - The \"order\" argument is ignored, is for compatibility with NumPy.\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3]).ravel()\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        return self\n\n    @classmethod\n    def _concat_same_type(cls, to_concat: Sequence[Self]) -> Self:\n        \"\"\"\n        Concatenate multiple array of this dtype.\n\n        Parameters\n        ----------\n        to_concat : sequence of this type\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> arr1 = pd.array([1, 2, 3])\n        >>> arr2 = pd.array([4, 5, 6])\n        >>> pd.arrays.IntegerArray._concat_same_type([arr1, arr2])\n        <IntegerArray>\n        [1, 2, 3, 4, 5, 6]\n        Length: 6, dtype: Int64\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    @cache_readonly\n    def _can_hold_na(self) -> bool:\n        return self.dtype._can_hold_na\n\n    def _accumulate(self, name: str, *, skipna: bool=True, **kwargs) -> ExtensionArray:\n        \"\"\"\n        Return an ExtensionArray performing an accumulation operation.\n\n        The underlying data type might change.\n\n        Parameters\n        ----------\n        name : str\n            Name of the function, supported values are:\n            - cummin\n            - cummax\n            - cumsum\n            - cumprod\n        skipna : bool, default True\n            If True, skip NA values.\n        **kwargs\n            Additional keyword arguments passed to the accumulation function.\n            Currently, there is no supported kwarg.\n\n        Returns\n        -------\n        array\n\n        Raises\n        ------\n        NotImplementedError : subclass does not define accumulations\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr._accumulate(name='cumsum')\n        <IntegerArray>\n        [1, 3, 6]\n        Length: 3, dtype: Int64\n        \"\"\"\n        raise NotImplementedError(f'cannot perform {name} with type {self.dtype}')\n\n    def _reduce(self, name: str, *, skipna: bool=True, keepdims: bool=False, **kwargs):\n        \"\"\"\n        Return a scalar result of performing the reduction operation.\n\n        Parameters\n        ----------\n        name : str\n            Name of the function, supported values are:\n            { any, all, min, max, sum, mean, median, prod,\n            std, var, sem, kurt, skew }.\n        skipna : bool, default True\n            If True, skip NaN values.\n        keepdims : bool, default False\n            If False, a scalar is returned.\n            If True, the result has dimension with size one along the reduced axis.\n\n            .. versionadded:: 2.1\n\n               This parameter is not required in the _reduce signature to keep backward\n               compatibility, but will become required in the future. If the parameter\n               is not found in the method signature, a FutureWarning will be emitted.\n        **kwargs\n            Additional keyword arguments passed to the reduction function.\n            Currently, `ddof` is the only supported kwarg.\n\n        Returns\n        -------\n        scalar\n\n        Raises\n        ------\n        TypeError : subclass does not define reductions\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3])._reduce(\"min\")\n        1\n        \"\"\"\n        meth = getattr(self, name, None)\n        if meth is None:\n            raise TypeError(f\"'{type(self).__name__}' with dtype {self.dtype} does not support reduction '{name}'\")\n        result = meth(skipna=skipna, **kwargs)\n        if keepdims:\n            result = np.array([result])\n        return result\n    __hash__: ClassVar[None]\n\n    def _values_for_json(self) -> np.ndarray:\n        \"\"\"\n        Specify how to render our entries in to_json.\n\n        Notes\n        -----\n        The dtype on the returned ndarray is not restricted, but for non-native\n        types that are not specifically handled in objToJSON.c, to_json is\n        liable to raise. In these cases, it may be safer to return an ndarray\n        of strings.\n        \"\"\"\n        return np.asarray(self)\n\n    def _hash_pandas_object(self, *, encoding: str, hash_key: str, categorize: bool) -> npt.NDArray[np.uint64]:\n        \"\"\"\n        Hook for hash_pandas_object.\n\n        Default is to use the values returned by _values_for_factorize.\n\n        Parameters\n        ----------\n        encoding : str\n            Encoding for data & key when strings.\n        hash_key : str\n            Hash_key for string key to encode.\n        categorize : bool\n            Whether to first categorize object arrays before hashing. This is more\n            efficient when the array contains duplicate values.\n\n        Returns\n        -------\n        np.ndarray[uint64]\n\n        Examples\n        --------\n        >>> pd.array([1, 2])._hash_pandas_object(encoding='utf-8',\n        ...                                      hash_key=\"1000000000000000\",\n        ...                                      categorize=False\n        ...                                      )\n        array([ 6238072747940578789, 15839785061582574730], dtype=uint64)\n        \"\"\"\n        from pandas.core.util.hashing import hash_array\n        (values, _) = self._values_for_factorize()\n        return hash_array(values, encoding=encoding, hash_key=hash_key, categorize=categorize)\n\n    def _explode(self) -> tuple[Self, npt.NDArray[np.uint64]]:\n        \"\"\"\n        Transform each element of list-like to a row.\n\n        For arrays that do not contain list-like elements the default\n        implementation of this method just returns a copy and an array\n        of ones (unchanged index).\n\n        Returns\n        -------\n        ExtensionArray\n            Array with the exploded values.\n        np.ndarray[uint64]\n            The original lengths of each list-like for determining the\n            resulting index.\n\n        See Also\n        --------\n        Series.explode : The method on the ``Series`` object that this\n            extension array method is meant to support.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> a = pd.array([[1, 2, 3], [4], [5, 6]],\n        ...              dtype=pd.ArrowDtype(pa.list_(pa.int64())))\n        >>> a._explode()\n        (<ArrowExtensionArray>\n        [1, 2, 3, 4, 5, 6]\n        Length: 6, dtype: int64[pyarrow], array([3, 1, 2], dtype=int32))\n        \"\"\"\n        values = self.copy()\n        counts = np.ones(shape=(len(self),), dtype=np.uint64)\n        return (values, counts)\n\n    def tolist(self) -> list:\n        \"\"\"\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a pandas scalar\n        (for Timestamp/Timedelta/Interval/Period)\n\n        Returns\n        -------\n        list\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.tolist()\n        [1, 2, 3]\n        \"\"\"\n        if self.ndim > 1:\n            return [x.tolist() for x in self]\n        return list(self)\n\n    def delete(self, loc: PositionalIndexer) -> Self:\n        indexer = np.delete(np.arange(len(self)), loc)\n        return self.take(indexer)\n\n    def insert(self, loc: int, item) -> Self:\n        \"\"\"\n        Insert an item at the given position.\n\n        Parameters\n        ----------\n        loc : int\n        item : scalar-like\n\n        Returns\n        -------\n        same type as self\n\n        Notes\n        -----\n        This method should be both type and dtype-preserving.  If the item\n        cannot be held in an array of this type/dtype, either ValueError or\n        TypeError should be raised.\n\n        The default implementation relies on _from_sequence to raise on invalid\n        items.\n\n        Examples\n        --------\n        >>> arr = pd.array([1, 2, 3])\n        >>> arr.insert(2, -1)\n        <IntegerArray>\n        [1, 2, -1, 3]\n        Length: 4, dtype: Int64\n        \"\"\"\n        loc = validate_insert_loc(loc, len(self))\n        item_arr = type(self)._from_sequence([item], dtype=self.dtype)\n        return type(self)._concat_same_type([self[:loc], item_arr, self[loc:]])\n\n    def _putmask(self, mask: npt.NDArray[np.bool_], value) -> None:\n        \"\"\"\n        Analogue to np.putmask(self, mask, value)\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool]\n        value : scalar or listlike\n            If listlike, must be arraylike with same length as self.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        Unlike np.putmask, we do not repeat listlike values with mismatched length.\n        'value' should either be a scalar or an arraylike with the same length\n        as self.\n        \"\"\"\n        if is_list_like(value):\n            val = value[mask]\n        else:\n            val = value\n        self[mask] = val\n\n    def _where(self, mask: npt.NDArray[np.bool_], value) -> Self:\n        \"\"\"\n        Analogue to np.where(mask, self, value)\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool]\n        value : scalar or listlike\n\n        Returns\n        -------\n        same type as self\n        \"\"\"\n        result = self.copy()\n        if is_list_like(value):\n            val = value[~mask]\n        else:\n            val = value\n        result[~mask] = val\n        return result\n\n    def _fill_mask_inplace(self, method: str, limit: int | None, mask: npt.NDArray[np.bool_]) -> None:\n        \"\"\"\n        Replace values in locations specified by 'mask' using pad or backfill.\n\n        See also\n        --------\n        ExtensionArray.fillna\n        \"\"\"\n        func = missing.get_fill_func(method)\n        npvalues = self.astype(object)\n        func(npvalues, limit=limit, mask=mask.copy())\n        new_values = self._from_sequence(npvalues, dtype=self.dtype)\n        self[mask] = new_values[mask]\n\n    def _rank(self, *, axis: AxisInt=0, method: str='average', na_option: str='keep', ascending: bool=True, pct: bool=False):\n        \"\"\"\n        See Series.rank.__doc__.\n        \"\"\"\n        if axis != 0:\n            raise NotImplementedError\n        return rank(self._values_for_argsort(), axis=axis, method=method, na_option=na_option, ascending=ascending, pct=pct)\n\n    @classmethod\n    def _empty(cls, shape: Shape, dtype: ExtensionDtype):\n        \"\"\"\n        Create an ExtensionArray with the given shape and dtype.\n\n        See also\n        --------\n        ExtensionDtype.empty\n            ExtensionDtype.empty is the 'official' public version of this API.\n        \"\"\"\n        obj = cls._from_sequence([], dtype=dtype)\n        taker = np.broadcast_to(np.intp(-1), shape)\n        result = obj.take(taker, allow_fill=True)\n        if not isinstance(result, cls) or dtype != result.dtype:\n            raise NotImplementedError(f\"Default 'empty' implementation is invalid for dtype='{dtype}'\")\n        return result\n\n    def _quantile(self, qs: npt.NDArray[np.float64], interpolation: str) -> Self:\n        \"\"\"\n        Compute the quantiles of self for each quantile in `qs`.\n\n        Parameters\n        ----------\n        qs : np.ndarray[float64]\n        interpolation: str\n\n        Returns\n        -------\n        same type as self\n        \"\"\"\n        mask = np.asarray(self.isna())\n        arr = np.asarray(self)\n        fill_value = np.nan\n        res_values = quantile_with_mask(arr, mask, fill_value, qs, interpolation)\n        return type(self)._from_sequence(res_values)\n\n    def _mode(self, dropna: bool=True) -> Self:\n        \"\"\"\n        Returns the mode(s) of the ExtensionArray.\n\n        Always returns `ExtensionArray` even if only one value.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NA values.\n\n        Returns\n        -------\n        same type as self\n            Sorted, if possible.\n        \"\"\"\n        return mode(self, dropna=dropna)\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n        if any((isinstance(other, (ABCSeries, ABCIndex, ABCDataFrame)) for other in inputs)):\n            return NotImplemented\n        result = arraylike.maybe_dispatch_ufunc_to_dunder_op(self, ufunc, method, *inputs, **kwargs)\n        if result is not NotImplemented:\n            return result\n        if 'out' in kwargs:\n            return arraylike.dispatch_ufunc_with_out(self, ufunc, method, *inputs, **kwargs)\n        if method == 'reduce':\n            result = arraylike.dispatch_reduction_ufunc(self, ufunc, method, *inputs, **kwargs)\n            if result is not NotImplemented:\n                return result\n        return arraylike.default_array_ufunc(self, ufunc, method, *inputs, **kwargs)\n\n    def map(self, mapper, na_action=None):\n        \"\"\"\n        Map values using an input mapping or function.\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence. If 'ignore' is not supported, a\n            ``NotImplementedError`` should be raised.\n\n        Returns\n        -------\n        Union[ndarray, Index, ExtensionArray]\n            The output of the mapping function applied to the array.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n        return map_array(self, mapper, na_action=na_action)\n\n    def _groupby_op(self, *, how: str, has_dropped_na: bool, min_count: int, ngroups: int, ids: npt.NDArray[np.intp], **kwargs) -> ArrayLike:\n        \"\"\"\n        Dispatch GroupBy reduction or transformation operation.\n\n        This is an *experimental* API to allow ExtensionArray authors to implement\n        reductions and transformations. The API is subject to change.\n\n        Parameters\n        ----------\n        how : {'any', 'all', 'sum', 'prod', 'min', 'max', 'mean', 'median',\n               'median', 'var', 'std', 'sem', 'nth', 'last', 'ohlc',\n               'cumprod', 'cumsum', 'cummin', 'cummax', 'rank'}\n        has_dropped_na : bool\n        min_count : int\n        ngroups : int\n        ids : np.ndarray[np.intp]\n            ids[i] gives the integer label for the group that self[i] belongs to.\n        **kwargs : operation-specific\n            'any', 'all' -> ['skipna']\n            'var', 'std', 'sem' -> ['ddof']\n            'cumprod', 'cumsum', 'cummin', 'cummax' -> ['skipna']\n            'rank' -> ['ties_method', 'ascending', 'na_option', 'pct']\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n        \"\"\"\n        from pandas.core.arrays.string_ import StringDtype\n        from pandas.core.groupby.ops import WrappedCythonOp\n        kind = WrappedCythonOp.get_kind_from_how(how)\n        op = WrappedCythonOp(how=how, kind=kind, has_dropped_na=has_dropped_na)\n        if isinstance(self.dtype, StringDtype):\n            if op.how not in ['any', 'all']:\n                op._get_cython_function(op.kind, op.how, np.dtype(object), False)\n            npvalues = self.to_numpy(object, na_value=np.nan)\n        else:\n            raise NotImplementedError(f'function is not implemented for this dtype: {self.dtype}')\n        res_values = op._cython_op_ndim_compat(npvalues, min_count=min_count, ngroups=ngroups, comp_ids=ids, mask=None, **kwargs)\n        if op.how in op.cast_blocklist:\n            return res_values\n        if isinstance(self.dtype, StringDtype):\n            dtype = self.dtype\n            string_array_cls = dtype.construct_array_type()\n            return string_array_cls._from_sequence(res_values, dtype=dtype)\n        else:\n            raise NotImplementedError", "class_fn": true, "question_id": "pandas/pandas.core.arrays.base/ExtensionArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/boolean.py", "fn_id": "", "content": "class BooleanArray(BaseMaskedArray):\n    \"\"\"\n    Array of boolean (True/False) data with missing values.\n\n    This is a pandas Extension array for boolean data, under the hood\n    represented by 2 numpy arrays: a boolean array with the data and\n    a boolean array with the mask (True indicating missing).\n\n    BooleanArray implements Kleene logic (sometimes called three-value\n    logic) for logical operations. See :ref:`boolean.kleene` for more.\n\n    To construct an BooleanArray from generic array-like input, use\n    :func:`pandas.array` specifying ``dtype=\"boolean\"`` (see examples\n    below).\n\n    .. warning::\n\n       BooleanArray is considered experimental. The implementation and\n       parts of the API may change without warning.\n\n    Parameters\n    ----------\n    values : numpy.ndarray\n        A 1-d boolean-dtype array with the data.\n    mask : numpy.ndarray\n        A 1-d boolean-dtype array indicating missing values (True\n        indicates missing).\n    copy : bool, default False\n        Whether to copy the `values` and `mask` arrays.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Returns\n    -------\n    BooleanArray\n\n    Examples\n    --------\n    Create an BooleanArray with :func:`pandas.array`:\n\n    >>> pd.array([True, False, None], dtype=\"boolean\")\n    <BooleanArray>\n    [True, False, <NA>]\n    Length: 3, dtype: boolean\n    \"\"\"\n    _internal_fill_value = False\n    _truthy_value = True\n    _falsey_value = False\n    _TRUE_VALUES = {'True', 'TRUE', 'true', '1', '1.0'}\n    _FALSE_VALUES = {'False', 'FALSE', 'false', '0', '0.0'}\n\n    def _accumulate(self, name: str, *, skipna: bool=True, **kwargs) -> BaseMaskedArray:\n        data = self._data\n        mask = self._mask\n        if name in ('cummin', 'cummax'):\n            op = getattr(masked_accumulations, name)\n            (data, mask) = op(data, mask, skipna=skipna, **kwargs)\n            return self._simple_new(data, mask)\n        else:\n            from pandas.core.arrays import IntegerArray\n            return IntegerArray(data.astype(int), mask)._accumulate(name, skipna=skipna, **kwargs)\n\n    @classmethod\n    def _from_sequence_of_strings(cls, strings: list[str], *, dtype: Dtype | None=None, copy: bool=False, true_values: list[str] | None=None, false_values: list[str] | None=None) -> BooleanArray:\n        true_values_union = cls._TRUE_VALUES.union(true_values or [])\n        false_values_union = cls._FALSE_VALUES.union(false_values or [])\n\n        def map_string(s) -> bool:\n            if s in true_values_union:\n                return True\n            elif s in false_values_union:\n                return False\n            else:\n                raise ValueError(f'{s} cannot be cast to bool')\n        scalars = np.array(strings, dtype=object)\n        mask = isna(scalars)\n        scalars[~mask] = list(map(map_string, scalars[~mask]))\n        return cls._from_sequence(scalars, dtype=dtype, copy=copy)\n\n    @property\n    def dtype(self) -> BooleanDtype:\n        return self._dtype\n\n    def _cmp_method(self, other, op) -> BooleanArray:\n        from pandas.core.arrays import BooleanArray\n        mask = None\n        if isinstance(other, BaseMaskedArray):\n            (other, mask) = (other._data, other._mask)\n        elif is_list_like(other):\n            other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError('can only perform ops with 1-d structures')\n            if len(self) != len(other):\n                raise ValueError('Lengths must match to compare')\n        if other is libmissing.NA:\n            result = np.zeros(self._data.shape, dtype='bool')\n            mask = np.ones(self._data.shape, dtype='bool')\n        else:\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore', 'elementwise', FutureWarning)\n                warnings.filterwarnings('ignore', 'elementwise', DeprecationWarning)\n                method = getattr(self._data, f'__{op.__name__}__')\n                result = method(other)\n                if result is NotImplemented:\n                    result = invalid_comparison(self._data, other, op)\n        mask = self._propagate_mask(mask, other)\n        return BooleanArray(result, mask, copy=False)\n    _HANDLED_TYPES = (np.ndarray, numbers.Number, bool, np.bool_)\n\n    def __init__(self, values: np.ndarray, mask: np.ndarray, copy: bool=False) -> None:\n        if not (isinstance(values, np.ndarray) and values.dtype == np.bool_):\n            raise TypeError(\"values should be boolean numpy array. Use the 'pd.array' function instead\")\n        self._dtype = BooleanDtype()\n        super().__init__(values, mask, copy=copy)\n\n    def _logical_method(self, other, op):\n        assert op.__name__ in {'or_', 'ror_', 'and_', 'rand_', 'xor', 'rxor'}\n        other_is_scalar = lib.is_scalar(other)\n        mask = None\n        if isinstance(other, BooleanArray):\n            (other, mask) = (other._data, other._mask)\n        elif is_list_like(other):\n            other = np.asarray(other, dtype='bool')\n            if other.ndim > 1:\n                raise NotImplementedError('can only perform ops with 1-d structures')\n            (other, mask) = coerce_to_array(other, copy=False)\n        elif isinstance(other, np.bool_):\n            other = other.item()\n        if other_is_scalar and other is not libmissing.NA and (not lib.is_bool(other)):\n            raise TypeError(f\"'other' should be pandas.NA or a bool. Got {type(other).__name__} instead.\")\n        if not other_is_scalar and len(self) != len(other):\n            raise ValueError('Lengths must match')\n        if op.__name__ in {'or_', 'ror_'}:\n            (result, mask) = ops.kleene_or(self._data, other, self._mask, mask)\n        elif op.__name__ in {'and_', 'rand_'}:\n            (result, mask) = ops.kleene_and(self._data, other, self._mask, mask)\n        else:\n            (result, mask) = ops.kleene_xor(self._data, other, self._mask, mask)\n        return self._maybe_mask_result(result, mask)\n\n    def min(self, *, skipna: bool=True, axis: AxisInt | None=0, **kwargs):\n        nv.validate_min((), kwargs)\n        result = masked_reductions.min(self._data, self._mask, skipna=skipna, axis=axis)\n        return self._wrap_reduction_result('min', result, skipna=skipna, axis=axis)\n\n    @doc(ExtensionArray.fillna)\n    def fillna(self, value=None, method=None, limit: int | None=None, copy: bool=True) -> Self:\n        (value, method) = validate_fillna_kwargs(value, method)\n        mask = self._mask\n        value = missing.check_value_size(value, mask, len(self))\n        if mask.any():\n            if method is not None:\n                func = missing.get_fill_func(method, ndim=self.ndim)\n                npvalues = self._data.T\n                new_mask = mask.T\n                if copy:\n                    npvalues = npvalues.copy()\n                    new_mask = new_mask.copy()\n                func(npvalues, limit=limit, mask=new_mask)\n                return self._simple_new(npvalues.T, new_mask.T)\n            else:\n                if copy:\n                    new_values = self.copy()\n                else:\n                    new_values = self[:]\n                new_values[mask] = value\n        elif copy:\n            new_values = self.copy()\n        else:\n            new_values = self[:]\n        return new_values\n\n    @classmethod\n    def _simple_new(cls, values: np.ndarray, mask: npt.NDArray[np.bool_]) -> Self:\n        result = super()._simple_new(values, mask)\n        result._dtype = BooleanDtype()\n        return result\n\n    @classmethod\n    def _coerce_to_array(cls, value, *, dtype: DtypeObj, copy: bool=False) -> tuple[np.ndarray, np.ndarray]:\n        if dtype:\n            assert dtype == 'boolean'\n        return coerce_to_array(value, copy=copy)", "class_fn": true, "question_id": "pandas/pandas.core.arrays.boolean/BooleanArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/categorical.py", "fn_id": "", "content": "@delegate_names(delegate=Categorical, accessors=['categories', 'ordered'], typ='property')\n@delegate_names(delegate=Categorical, accessors=['rename_categories', 'reorder_categories', 'add_categories', 'remove_categories', 'remove_unused_categories', 'set_categories', 'as_ordered', 'as_unordered'], typ='method')\nclass CategoricalAccessor(PandasDelegate, PandasObject, NoNewAttributesMixin):\n    \"\"\"\n    Accessor object for categorical properties of the Series values.\n\n    Parameters\n    ----------\n    data : Series or CategoricalIndex\n\n    Examples\n    --------\n    >>> s = pd.Series(list(\"abbccc\")).astype(\"category\")\n    >>> s\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n\n    >>> s.cat.categories\n    Index(['a', 'b', 'c'], dtype='object')\n\n    >>> s.cat.rename_categories(list(\"cba\"))\n    0    c\n    1    b\n    2    b\n    3    a\n    4    a\n    5    a\n    dtype: category\n    Categories (3, object): ['c', 'b', 'a']\n\n    >>> s.cat.reorder_categories(list(\"cba\"))\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['c', 'b', 'a']\n\n    >>> s.cat.add_categories([\"d\", \"e\"])\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n\n    >>> s.cat.remove_categories([\"a\", \"c\"])\n    0    NaN\n    1      b\n    2      b\n    3    NaN\n    4    NaN\n    5    NaN\n    dtype: category\n    Categories (1, object): ['b']\n\n    >>> s1 = s.cat.add_categories([\"d\", \"e\"])\n    >>> s1.cat.remove_unused_categories()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n\n    >>> s.cat.set_categories(list(\"abcde\"))\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n\n    >>> s.cat.as_ordered()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a' < 'b' < 'c']\n\n    >>> s.cat.as_unordered()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n    \"\"\"\n\n    def _delegate_method(self, name: str, *args, **kwargs):\n        from pandas import Series\n        method = getattr(self._parent, name)\n        res = method(*args, **kwargs)\n        if res is not None:\n            return Series(res, index=self._index, name=self._name)\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular object.\n        \"\"\"\n        return object.__repr__(self)\n\n    @staticmethod\n    def _validate(data):\n        if not isinstance(data.dtype, CategoricalDtype):\n            raise AttributeError(\"Can only use .cat accessor with a 'category' dtype\")\n\n    def _delegate_property_get(self, name: str):\n        return getattr(self._parent, name)\n\n    @property\n    def codes(self) -> Series:\n        \"\"\"\n        Return Series of codes as well as the index.\n\n        Examples\n        --------\n        >>> raw_cate = pd.Categorical([\"a\", \"b\", \"c\", \"a\"], categories=[\"a\", \"b\"])\n        >>> ser = pd.Series(raw_cate)\n        >>> ser.cat.codes\n        0   0\n        1   1\n        2  -1\n        3   0\n        dtype: int8\n        \"\"\"\n        from pandas import Series\n        return Series(self._parent.codes, index=self._index)\n\n    def __setattr__(self, key: str, value) -> None:\n        if getattr(self, '__frozen', False) and (not (key == '_cache' or key in type(self).__dict__ or getattr(self, key, None) is not None)):\n            raise AttributeError(f\"You cannot add any new attribute '{key}'\")\n        object.__setattr__(self, key, value)\n\n    def __init__(self, data) -> None:\n        self._validate(data)\n        self._parent = data.values\n        self._index = data.index\n        self._name = data.name\n        self._freeze()\n\n    def _freeze(self) -> None:\n        \"\"\"\n        Prevents setting additional attributes.\n        \"\"\"\n        object.__setattr__(self, '__frozen', True)\n\n    def _delegate_property_set(self, name: str, new_values):\n        return setattr(self._parent, name, new_values)", "class_fn": true, "question_id": "pandas/pandas.core.arrays.categorical/CategoricalAccessor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/datetimelike.py", "fn_id": "", "content": "class TimelikeOps(DatetimeLikeArrayMixin):\n    \"\"\"\n    Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.\n    \"\"\"\n    _default_dtype: np.dtype\n\n    @final\n    @classmethod\n    def _validate_frequency(cls, index, freq: BaseOffset, **kwargs):\n        \"\"\"\n        Validate that a frequency is compatible with the values of a given\n        Datetime Array/Index or Timedelta Array/Index\n\n        Parameters\n        ----------\n        index : DatetimeIndex or TimedeltaIndex\n            The index on which to determine if the given frequency is valid\n        freq : DateOffset\n            The frequency to validate\n        \"\"\"\n        inferred = index.inferred_freq\n        if index.size == 0 or inferred == freq.freqstr:\n            return None\n        try:\n            on_freq = cls._generate_range(start=index[0], end=None, periods=len(index), freq=freq, unit=index.unit, **kwargs)\n            if not np.array_equal(index.asi8, on_freq.asi8):\n                raise ValueError\n        except ValueError as err:\n            if 'non-fixed' in str(err):\n                raise err\n            raise ValueError(f'Inferred frequency {inferred} from passed values does not conform to passed frequency {freq.freqstr}') from err\n\n    def any(self, *, axis: AxisInt | None=None, skipna: bool=True) -> bool:\n        return nanops.nanany(self._ndarray, axis=axis, skipna=skipna, mask=self.isna())\n\n    @property\n    def freq(self):\n        \"\"\"\n        Return the frequency object if it is set, otherwise None.\n        \"\"\"\n        return self._freq\n\n    @freq.setter\n    def freq(self, value) -> None:\n        if value is not None:\n            value = to_offset(value)\n            self._validate_frequency(self, value)\n            if self.dtype.kind == 'm' and (not isinstance(value, Tick)):\n                raise TypeError('TimedeltaArray/Index freq must be a Tick')\n            if self.ndim > 1:\n                raise ValueError('Cannot set freq with ndim > 1')\n        self._freq = value\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n        if ufunc in [np.isnan, np.isinf, np.isfinite] and len(inputs) == 1 and (inputs[0] is self):\n            return getattr(ufunc, method)(self._ndarray, **kwargs)\n        return super().__array_ufunc__(ufunc, method, *inputs, **kwargs)\n\n    def _values_for_json(self) -> np.ndarray:\n        if isinstance(self.dtype, np.dtype):\n            return self._ndarray\n        return super()._values_for_json()\n\n    def all(self, *, axis: AxisInt | None=None, skipna: bool=True) -> bool:\n        return nanops.nanall(self._ndarray, axis=axis, skipna=skipna, mask=self.isna())\n\n    @cache_readonly\n    def _creso(self) -> int:\n        return get_unit_from_dtype(self._ndarray.dtype)\n\n    @cache_readonly\n    def unit(self) -> str:\n        return dtype_to_unit(self.dtype)\n\n    @Appender((_round_doc + _round_example).format(op='round'))\n    def round(self, freq, ambiguous: TimeAmbiguous='raise', nonexistent: TimeNonexistent='raise') -> Self:\n        return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)\n\n    def _maybe_clear_freq(self) -> None:\n        self._freq = None\n\n    def factorize(self, use_na_sentinel: bool=True, sort: bool=False):\n        if self.freq is not None:\n            codes = np.arange(len(self), dtype=np.intp)\n            uniques = self.copy()\n            if sort and self.freq.n < 0:\n                codes = codes[::-1]\n                uniques = uniques[::-1]\n            return (codes, uniques)\n        if sort:\n            raise NotImplementedError(f\"The 'sort' keyword in {type(self).__name__}.factorize is ignored unless arr.freq is not None. To factorize with sort, call pd.factorize(obj, sort=True) instead.\")\n        return super().factorize(use_na_sentinel=use_na_sentinel)\n\n    @classmethod\n    def _validate_dtype(cls, values, dtype):\n        raise AbstractMethodError(cls)\n\n    def copy(self, order: str='C') -> Self:\n        new_obj = super().copy(order=order)\n        new_obj._freq = self.freq\n        return new_obj\n\n    @final\n    def _maybe_pin_freq(self, freq, validate_kwds: dict):\n        \"\"\"\n        Constructor helper to pin the appropriate `freq` attribute.  Assumes\n        that self._freq is currently set to any freq inferred in\n        _from_sequence_not_strict.\n        \"\"\"\n        if freq is None:\n            self._freq = None\n        elif freq == 'infer':\n            if self._freq is None:\n                self._freq = to_offset(self.inferred_freq)\n        elif freq is lib.no_default:\n            pass\n        elif self._freq is None:\n            freq = to_offset(freq)\n            type(self)._validate_frequency(self, freq, **validate_kwds)\n            self._freq = freq\n        else:\n            freq = to_offset(freq)\n            _validate_inferred_freq(freq, self._freq)\n\n    def _with_freq(self, freq) -> Self:\n        \"\"\"\n        Helper to get a view on the same data, with a new freq.\n\n        Parameters\n        ----------\n        freq : DateOffset, None, or \"infer\"\n\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        if freq is None:\n            pass\n        elif len(self) == 0 and isinstance(freq, BaseOffset):\n            if self.dtype.kind == 'm' and (not isinstance(freq, Tick)):\n                raise TypeError('TimedeltaArray/Index freq must be a Tick')\n        else:\n            assert freq == 'infer'\n            freq = to_offset(self.inferred_freq)\n        arr = self.view()\n        arr._freq = freq\n        return arr\n\n    def interpolate(self, *, method: InterpolateOptions, axis: int, index: Index, limit, limit_direction, limit_area, copy: bool, **kwargs) -> Self:\n        \"\"\"\n        See NDFrame.interpolate.__doc__.\n        \"\"\"\n        if method != 'linear':\n            raise NotImplementedError\n        if not copy:\n            out_data = self._ndarray\n        else:\n            out_data = self._ndarray.copy()\n        missing.interpolate_2d_inplace(out_data, method=method, axis=axis, index=index, limit=limit, limit_direction=limit_direction, limit_area=limit_area, **kwargs)\n        if not copy:\n            return self\n        return type(self)._simple_new(out_data, dtype=self.dtype)\n\n    def _ensure_matching_resos(self, other):\n        if self._creso != other._creso:\n            if self._creso < other._creso:\n                self = self.as_unit(other.unit)\n            else:\n                other = other.as_unit(self.unit)\n        return (self, other)\n\n    def __init__(self, values, dtype=None, freq=lib.no_default, copy: bool=False) -> None:\n        warnings.warn(f'{type(self).__name__}.__init__ is deprecated and will be removed in a future version. Use pd.array instead.', FutureWarning, stacklevel=find_stack_level())\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n        values = extract_array(values, extract_numpy=True)\n        if isinstance(values, IntegerArray):\n            values = values.to_numpy('int64', na_value=iNaT)\n        inferred_freq = getattr(values, '_freq', None)\n        explicit_none = freq is None\n        freq = freq if freq is not lib.no_default else None\n        if isinstance(values, type(self)):\n            if explicit_none:\n                pass\n            elif freq is None:\n                freq = values.freq\n            elif freq and values.freq:\n                freq = to_offset(freq)\n                freq = _validate_inferred_freq(freq, values.freq)\n            if dtype is not None and dtype != values.dtype:\n                raise TypeError(f'dtype={dtype} does not match data dtype {values.dtype}')\n            dtype = values.dtype\n            values = values._ndarray\n        elif dtype is None:\n            if isinstance(values, np.ndarray) and values.dtype.kind in 'Mm':\n                dtype = values.dtype\n            else:\n                dtype = self._default_dtype\n                if isinstance(values, np.ndarray) and values.dtype == 'i8':\n                    values = values.view(dtype)\n        if not isinstance(values, np.ndarray):\n            raise ValueError(f\"Unexpected type '{type(values).__name__}'. 'values' must be a {type(self).__name__}, ndarray, or Series or Index containing one of those.\")\n        if values.ndim not in [1, 2]:\n            raise ValueError('Only 1-dimensional input arrays are supported.')\n        if values.dtype == 'i8':\n            if dtype is None:\n                dtype = self._default_dtype\n                values = values.view(self._default_dtype)\n            elif lib.is_np_dtype(dtype, 'mM'):\n                values = values.view(dtype)\n            elif isinstance(dtype, DatetimeTZDtype):\n                kind = self._default_dtype.kind\n                new_dtype = f'{kind}8[{dtype.unit}]'\n                values = values.view(new_dtype)\n        dtype = self._validate_dtype(values, dtype)\n        if freq == 'infer':\n            raise ValueError(f\"Frequency inference not allowed in {type(self).__name__}.__init__. Use 'pd.array()' instead.\")\n        if copy:\n            values = values.copy()\n        if freq:\n            freq = to_offset(freq)\n            if values.dtype.kind == 'm' and (not isinstance(freq, Tick)):\n                raise TypeError('TimedeltaArray/Index freq must be a Tick')\n        NDArrayBacked.__init__(self, values=values, dtype=dtype)\n        self._freq = freq\n        if inferred_freq is None and freq is not None:\n            type(self)._validate_frequency(self, freq)\n\n    @classmethod\n    def _generate_range(cls, start, end, periods: int | None, freq, *args, **kwargs) -> Self:\n        raise AbstractMethodError(cls)\n\n    def _get_getitem_freq(self, key) -> BaseOffset | None:\n        \"\"\"\n        Find the `freq` attribute to assign to the result of a __getitem__ lookup.\n        \"\"\"\n        is_period = isinstance(self.dtype, PeriodDtype)\n        if is_period:\n            freq = self.freq\n        elif self.ndim != 1:\n            freq = None\n        else:\n            key = check_array_indexer(self, key)\n            freq = None\n            if isinstance(key, slice):\n                if self.freq is not None and key.step is not None:\n                    freq = key.step * self.freq\n                else:\n                    freq = self.freq\n            elif key is Ellipsis:\n                freq = self.freq\n            elif com.is_bool_indexer(key):\n                new_key = lib.maybe_booleans_to_slice(key.view(np.uint8))\n                if isinstance(new_key, slice):\n                    return self._get_getitem_freq(new_key)\n        return freq\n\n    def _round(self, freq, mode, ambiguous, nonexistent):\n        if isinstance(self.dtype, DatetimeTZDtype):\n            self = cast('DatetimeArray', self)\n            naive = self.tz_localize(None)\n            result = naive._round(freq, mode, ambiguous, nonexistent)\n            return result.tz_localize(self.tz, ambiguous=ambiguous, nonexistent=nonexistent)\n        values = self.view('i8')\n        values = cast(np.ndarray, values)\n        nanos = get_unit_for_round(freq, self._creso)\n        if nanos == 0:\n            return self.copy()\n        result_i8 = round_nsint64(values, mode, nanos)\n        result = self._maybe_mask_results(result_i8, fill_value=iNaT)\n        result = result.view(self._ndarray.dtype)\n        return self._simple_new(result, dtype=self.dtype)\n\n    def _add_offset(self, offset):\n        raise AbstractMethodError(self)\n\n    @Appender((_round_doc + _floor_example).format(op='floor'))\n    def floor(self, freq, ambiguous: TimeAmbiguous='raise', nonexistent: TimeNonexistent='raise') -> Self:\n        return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)\n\n    @Appender((_round_doc + _ceil_example).format(op='ceil'))\n    def ceil(self, freq, ambiguous: TimeAmbiguous='raise', nonexistent: TimeNonexistent='raise') -> Self:\n        return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)\n\n    @property\n    def _is_dates_only(self) -> bool:\n        \"\"\"\n        Check if we are round times at midnight (and no timezone), which will\n        be given a more compact __repr__ than other cases. For TimedeltaArray\n        we are checking for multiples of 24H.\n        \"\"\"\n        if not lib.is_np_dtype(self.dtype):\n            return False\n        values_int = self.asi8\n        consider_values = values_int != iNaT\n        reso = get_unit_from_dtype(self.dtype)\n        ppd = periods_per_day(reso)\n        even_days = np.logical_and(consider_values, values_int % ppd != 0).sum() == 0\n        return even_days\n\n    def as_unit(self, unit: str, round_ok: bool=True) -> Self:\n        if unit not in ['s', 'ms', 'us', 'ns']:\n            raise ValueError(\"Supported units are 's', 'ms', 'us', 'ns'\")\n        dtype = np.dtype(f'{self.dtype.kind}8[{unit}]')\n        new_values = astype_overflowsafe(self._ndarray, dtype, round_ok=round_ok)\n        if isinstance(self.dtype, np.dtype):\n            new_dtype = new_values.dtype\n        else:\n            tz = cast('DatetimeArray', self).tz\n            new_dtype = DatetimeTZDtype(tz=tz, unit=unit)\n        return type(self)._simple_new(new_values, dtype=new_dtype, freq=self.freq)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat: Sequence[Self], axis: AxisInt=0) -> Self:\n        new_obj = super()._concat_same_type(to_concat, axis)\n        obj = to_concat[0]\n        if axis == 0:\n            to_concat = [x for x in to_concat if len(x)]\n            if obj.freq is not None and all((x.freq == obj.freq for x in to_concat)):\n                pairs = zip(to_concat[:-1], to_concat[1:])\n                if all((pair[0][-1] + obj.freq == pair[1][0] for pair in pairs)):\n                    new_freq = obj.freq\n                    new_obj._freq = new_freq\n        return new_obj\n\n    def __setitem__(self, key: int | Sequence[int] | Sequence[bool] | slice, value: NaTType | Any | Sequence[Any]) -> None:\n        no_op = check_setitem_lengths(key, value, self)\n        super().__setitem__(key, value)\n        if no_op:\n            return\n        self._maybe_clear_freq()", "class_fn": true, "question_id": "pandas/pandas.core.arrays.datetimelike/TimelikeOps", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/masked.py", "fn_id": "", "content": "class BaseMaskedArray(OpsMixin, ExtensionArray):\n    \"\"\"\n    Base class for masked arrays (which use _data and _mask to store the data).\n\n    numpy based\n    \"\"\"\n    _internal_fill_value: Scalar\n    _data: np.ndarray\n    _mask: npt.NDArray[np.bool_]\n    _truthy_value = Scalar\n    _falsey_value = Scalar\n\n    def __invert__(self) -> Self:\n        return self._simple_new(~self._data, self._mask.copy())\n\n    def min(self, *, skipna: bool=True, axis: AxisInt | None=0, **kwargs):\n        nv.validate_min((), kwargs)\n        result = masked_reductions.min(self._data, self._mask, skipna=skipna, axis=axis)\n        return self._wrap_reduction_result('min', result, skipna=skipna, axis=axis)\n\n    def reshape(self, *args, **kwargs) -> Self:\n        data = self._data.reshape(*args, **kwargs)\n        mask = self._mask.reshape(*args, **kwargs)\n        return self._simple_new(data, mask)\n\n    def __neg__(self) -> Self:\n        return self._simple_new(-self._data, self._mask.copy())\n\n    @classmethod\n    def _coerce_to_array(cls, values, *, dtype: DtypeObj, copy: bool=False) -> tuple[np.ndarray, np.ndarray]:\n        raise AbstractMethodError(cls)\n\n    @property\n    def dtype(self) -> BaseMaskedDtype:\n        raise AbstractMethodError(self)\n\n    @overload\n    def __getitem__(self, item: ScalarIndexer) -> Any:\n        ...\n\n    @overload\n    def __getitem__(self, item: SequenceIndexer) -> Self:\n        ...\n\n    def _propagate_mask(self, mask: npt.NDArray[np.bool_] | None, other) -> npt.NDArray[np.bool_]:\n        if mask is None:\n            mask = self._mask.copy()\n            if other is libmissing.NA:\n                mask = mask | True\n            elif is_list_like(other) and len(other) == len(mask):\n                mask = mask | isna(other)\n        else:\n            mask = self._mask | mask\n        return mask\n\n    @doc(ExtensionArray.factorize)\n    def factorize(self, use_na_sentinel: bool=True) -> tuple[np.ndarray, ExtensionArray]:\n        arr = self._data\n        mask = self._mask\n        (codes, uniques) = factorize_array(arr, use_na_sentinel=True, mask=mask)\n        assert uniques.dtype == self.dtype.numpy_dtype, (uniques.dtype, self.dtype)\n        has_na = mask.any()\n        if use_na_sentinel or not has_na:\n            size = len(uniques)\n        else:\n            size = len(uniques) + 1\n        uniques_mask = np.zeros(size, dtype=bool)\n        if not use_na_sentinel and has_na:\n            na_index = mask.argmax()\n            if na_index == 0:\n                na_code = np.intp(0)\n            else:\n                na_code = codes[:na_index].max() + 1\n            codes[codes >= na_code] += 1\n            codes[codes == -1] = na_code\n            uniques = np.insert(uniques, na_code, 0)\n            uniques_mask[na_code] = True\n        uniques_ea = self._simple_new(uniques, uniques_mask)\n        return (codes, uniques_ea)\n\n    def _pad_or_backfill(self, *, method: FillnaOptions, limit: int | None=None, limit_area: Literal['inside', 'outside'] | None=None, copy: bool=True) -> Self:\n        mask = self._mask\n        if mask.any():\n            func = missing.get_fill_func(method, ndim=self.ndim)\n            npvalues = self._data.T\n            new_mask = mask.T\n            if copy:\n                npvalues = npvalues.copy()\n                new_mask = new_mask.copy()\n            elif limit_area is not None:\n                mask = mask.copy()\n            func(npvalues, limit=limit, mask=new_mask)\n            if limit_area is not None and (not mask.all()):\n                mask = mask.T\n                neg_mask = ~mask\n                first = neg_mask.argmax()\n                last = len(neg_mask) - neg_mask[::-1].argmax() - 1\n                if limit_area == 'inside':\n                    new_mask[:first] |= mask[:first]\n                    new_mask[last + 1:] |= mask[last + 1:]\n                elif limit_area == 'outside':\n                    new_mask[first + 1:last] |= mask[first + 1:last]\n            if copy:\n                return self._simple_new(npvalues.T, new_mask.T)\n            else:\n                return self\n        elif copy:\n            new_values = self.copy()\n        else:\n            new_values = self\n        return new_values\n\n    def map(self, mapper, na_action=None):\n        return map_array(self.to_numpy(), mapper, na_action=na_action)\n\n    def _where(self, mask: npt.NDArray[np.bool_], value) -> Self:\n        \"\"\"\n        Analogue to np.where(mask, self, value)\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool]\n        value : scalar or listlike\n\n        Returns\n        -------\n        same type as self\n        \"\"\"\n        result = self.copy()\n        if is_list_like(value):\n            val = value[~mask]\n        else:\n            val = value\n        result[~mask] = val\n        return result\n\n    @classmethod\n    @doc(ExtensionArray._empty)\n    def _empty(cls, shape: Shape, dtype: ExtensionDtype):\n        values = np.empty(shape, dtype=dtype.type)\n        values.fill(cls._internal_fill_value)\n        mask = np.ones(shape, dtype=bool)\n        result = cls(values, mask)\n        if not isinstance(result, cls) or dtype != result.dtype:\n            raise NotImplementedError(f\"Default 'empty' implementation is invalid for dtype='{dtype}'\")\n        return result\n\n    def round(self, decimals: int=0, *args, **kwargs):\n        \"\"\"\n        Round each value in the array a to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int, default 0\n            Number of decimal places to round to. If decimals is negative,\n            it specifies the number of positions to the left of the decimal point.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        NumericArray\n            Rounded values of the NumericArray.\n\n        See Also\n        --------\n        numpy.around : Round values of an np.array.\n        DataFrame.round : Round values of a DataFrame.\n        Series.round : Round values of a Series.\n        \"\"\"\n        if self.dtype.kind == 'b':\n            return self\n        nv.validate_round(args, kwargs)\n        values = np.round(self._data, decimals=decimals, **kwargs)\n        return self._maybe_mask_result(values, self._mask.copy())\n\n    def copy(self) -> Self:\n        data = self._data.copy()\n        mask = self._mask.copy()\n        return self._simple_new(data, mask)\n\n    def _formatter(self, boxed: bool=False) -> Callable[[Any], str | None]:\n        return str\n\n    @property\n    def shape(self) -> Shape:\n        return self._data.shape\n\n    @property\n    def ndim(self) -> int:\n        return self._data.ndim\n\n    def any(self, *, skipna: bool=True, axis: AxisInt | None=0, **kwargs):\n        \"\"\"\n        Return whether any element is truthy.\n\n        Returns False unless there is at least one element that is truthy.\n        By default, NAs are skipped. If ``skipna=False`` is specified and\n        missing values are present, similar :ref:`Kleene logic <boolean.kleene>`\n        is used as for logical operations.\n\n        .. versionchanged:: 1.4.0\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Exclude NA values. If the entire array is NA and `skipna` is\n            True, then the result will be False, as for an empty array.\n            If `skipna` is False, the result will still be True if there is\n            at least one element that is truthy, otherwise NA will be returned\n            if there are NA's present.\n        axis : int, optional, default 0\n        **kwargs : any, default None\n            Additional keywords have no effect but might be accepted for\n            compatibility with NumPy.\n\n        Returns\n        -------\n        bool or :attr:`pandas.NA`\n\n        See Also\n        --------\n        numpy.any : Numpy version of this method.\n        BaseMaskedArray.all : Return whether all elements are truthy.\n\n        Examples\n        --------\n        The result indicates whether any element is truthy (and by default\n        skips NAs):\n\n        >>> pd.array([True, False, True]).any()\n        True\n        >>> pd.array([True, False, pd.NA]).any()\n        True\n        >>> pd.array([False, False, pd.NA]).any()\n        False\n        >>> pd.array([], dtype=\"boolean\").any()\n        False\n        >>> pd.array([pd.NA], dtype=\"boolean\").any()\n        False\n        >>> pd.array([pd.NA], dtype=\"Float64\").any()\n        False\n\n        With ``skipna=False``, the result can be NA if this is logically\n        required (whether ``pd.NA`` is True or False influences the result):\n\n        >>> pd.array([True, False, pd.NA]).any(skipna=False)\n        True\n        >>> pd.array([1, 0, pd.NA]).any(skipna=False)\n        True\n        >>> pd.array([False, False, pd.NA]).any(skipna=False)\n        <NA>\n        >>> pd.array([0, 0, pd.NA]).any(skipna=False)\n        <NA>\n        \"\"\"\n        nv.validate_any((), kwargs)\n        values = self._data.copy()\n        np.putmask(values, self._mask, self._falsey_value)\n        result = values.any()\n        if skipna:\n            return result\n        elif result or len(self) == 0 or (not self._mask.any()):\n            return result\n        else:\n            return self.dtype.na_value\n\n    def _validate_setitem_value(self, value):\n        \"\"\"\n        Check if we have a scalar that we can cast losslessly.\n\n        Raises\n        ------\n        TypeError\n        \"\"\"\n        kind = self.dtype.kind\n        if kind == 'b':\n            if lib.is_bool(value):\n                return value\n        elif kind == 'f':\n            if lib.is_integer(value) or lib.is_float(value):\n                return value\n        elif lib.is_integer(value) or (lib.is_float(value) and value.is_integer()):\n            return value\n        raise TypeError(f\"Invalid value '{str(value)}' for dtype {self.dtype}\")\n\n    def astype(self, dtype: AstypeArg, copy: bool=True) -> ArrayLike:\n        dtype = pandas_dtype(dtype)\n        if dtype == self.dtype:\n            if copy:\n                return self.copy()\n            return self\n        if isinstance(dtype, BaseMaskedDtype):\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore', category=RuntimeWarning)\n                data = self._data.astype(dtype.numpy_dtype, copy=copy)\n            mask = self._mask if data is self._data else self._mask.copy()\n            cls = dtype.construct_array_type()\n            return cls(data, mask, copy=False)\n        if isinstance(dtype, ExtensionDtype):\n            eacls = dtype.construct_array_type()\n            return eacls._from_sequence(self, dtype=dtype, copy=copy)\n        na_value: float | np.datetime64 | lib.NoDefault\n        if dtype.kind == 'f':\n            na_value = np.nan\n        elif dtype.kind == 'M':\n            na_value = np.datetime64('NaT')\n        else:\n            na_value = lib.no_default\n        if dtype.kind in 'iu' and self._hasna:\n            raise ValueError('cannot convert NA to integer')\n        if dtype.kind == 'b' and self._hasna:\n            raise ValueError('cannot convert float NaN to bool')\n        data = self.to_numpy(dtype=dtype, na_value=na_value, copy=copy)\n        return data\n\n    def transpose(self, *axes: int) -> ExtensionArray:\n        \"\"\"\n        Return a transposed view on this array.\n\n        Because ExtensionArrays are always 1D, this is a no-op.  It is included\n        for compatibility with np.ndarray.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3]).transpose()\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        return self[:]\n\n    @property\n    def T(self) -> Self:\n        return self._simple_new(self._data.T, self._mask.T)\n\n    def _wrap_reduction_result(self, name: str, result, *, skipna, axis):\n        if isinstance(result, np.ndarray):\n            if skipna:\n                mask = self._mask.all(axis=axis)\n            else:\n                mask = self._mask.any(axis=axis)\n            return self._maybe_mask_result(result, mask)\n        return result\n\n    def _mode(self, dropna: bool=True) -> Self:\n        if dropna:\n            result = mode(self._data, dropna=dropna, mask=self._mask)\n            res_mask = np.zeros(result.shape, dtype=np.bool_)\n        else:\n            (result, res_mask) = mode(self._data, dropna=dropna, mask=self._mask)\n        result = type(self)(result, res_mask)\n        return result[result.argsort()]\n\n    def mean(self, *, skipna: bool=True, axis: AxisInt | None=0, **kwargs):\n        nv.validate_mean((), kwargs)\n        result = masked_reductions.mean(self._data, self._mask, skipna=skipna, axis=axis)\n        return self._wrap_reduction_result('mean', result, skipna=skipna, axis=axis)\n\n    def all(self, *, skipna: bool=True, axis: AxisInt | None=0, **kwargs):\n        \"\"\"\n        Return whether all elements are truthy.\n\n        Returns True unless there is at least one element that is falsey.\n        By default, NAs are skipped. If ``skipna=False`` is specified and\n        missing values are present, similar :ref:`Kleene logic <boolean.kleene>`\n        is used as for logical operations.\n\n        .. versionchanged:: 1.4.0\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Exclude NA values. If the entire array is NA and `skipna` is\n            True, then the result will be True, as for an empty array.\n            If `skipna` is False, the result will still be False if there is\n            at least one element that is falsey, otherwise NA will be returned\n            if there are NA's present.\n        axis : int, optional, default 0\n        **kwargs : any, default None\n            Additional keywords have no effect but might be accepted for\n            compatibility with NumPy.\n\n        Returns\n        -------\n        bool or :attr:`pandas.NA`\n\n        See Also\n        --------\n        numpy.all : Numpy version of this method.\n        BooleanArray.any : Return whether any element is truthy.\n\n        Examples\n        --------\n        The result indicates whether all elements are truthy (and by default\n        skips NAs):\n\n        >>> pd.array([True, True, pd.NA]).all()\n        True\n        >>> pd.array([1, 1, pd.NA]).all()\n        True\n        >>> pd.array([True, False, pd.NA]).all()\n        False\n        >>> pd.array([], dtype=\"boolean\").all()\n        True\n        >>> pd.array([pd.NA], dtype=\"boolean\").all()\n        True\n        >>> pd.array([pd.NA], dtype=\"Float64\").all()\n        True\n\n        With ``skipna=False``, the result can be NA if this is logically\n        required (whether ``pd.NA`` is True or False influences the result):\n\n        >>> pd.array([True, True, pd.NA]).all(skipna=False)\n        <NA>\n        >>> pd.array([1, 1, pd.NA]).all(skipna=False)\n        <NA>\n        >>> pd.array([True, False, pd.NA]).all(skipna=False)\n        False\n        >>> pd.array([1, 0, pd.NA]).all(skipna=False)\n        False\n        \"\"\"\n        nv.validate_all((), kwargs)\n        values = self._data.copy()\n        np.putmask(values, self._mask, self._truthy_value)\n        result = values.all(axis=axis)\n        if skipna:\n            return result\n        elif not result or len(self) == 0 or (not self._mask.any()):\n            return result\n        else:\n            return self.dtype.na_value\n\n    def ravel(self, *args, **kwargs) -> Self:\n        data = self._data.ravel(*args, **kwargs)\n        mask = self._mask.ravel(*args, **kwargs)\n        return type(self)(data, mask)\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype=None, copy: bool=False) -> Self:\n        (values, mask) = cls._coerce_to_array(scalars, dtype=dtype, copy=copy)\n        return cls(values, mask)\n\n    def __setitem__(self, key, value) -> None:\n        key = check_array_indexer(self, key)\n        if is_scalar(value):\n            if is_valid_na_for_dtype(value, self.dtype):\n                self._mask[key] = True\n            else:\n                value = self._validate_setitem_value(value)\n                self._data[key] = value\n                self._mask[key] = False\n            return\n        (value, mask) = self._coerce_to_array(value, dtype=self.dtype)\n        self._data[key] = value\n        self._mask[key] = mask\n\n    def _wrap_min_count_reduction_result(self, name: str, result, *, skipna, min_count, axis):\n        if min_count == 0 and isinstance(result, np.ndarray):\n            return self._maybe_mask_result(result, np.zeros(result.shape, dtype=bool))\n        return self._wrap_reduction_result(name, result, skipna=skipna, axis=axis)\n\n    @overload\n    def astype(self, dtype: npt.DTypeLike, copy: bool=...) -> np.ndarray:\n        ...\n\n    @overload\n    def astype(self, dtype: ExtensionDtype, copy: bool=...) -> ExtensionArray:\n        ...\n\n    @overload\n    def astype(self, dtype: AstypeArg, copy: bool=...) -> ArrayLike:\n        ...\n\n    def delete(self, loc, axis: AxisInt=0) -> Self:\n        data = np.delete(self._data, loc, axis=axis)\n        mask = np.delete(self._mask, loc, axis=axis)\n        return self._simple_new(data, mask)\n    __array_priority__ = 1000\n\n    def value_counts(self, dropna: bool=True) -> Series:\n        \"\"\"\n        Returns a Series containing counts of each unique value.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include counts of missing values.\n\n        Returns\n        -------\n        counts : Series\n\n        See Also\n        --------\n        Series.value_counts\n        \"\"\"\n        from pandas import Index, Series\n        from pandas.arrays import IntegerArray\n        (keys, value_counts, na_counter) = algos.value_counts_arraylike(self._data, dropna=dropna, mask=self._mask)\n        mask_index = np.zeros((len(value_counts),), dtype=np.bool_)\n        mask = mask_index.copy()\n        if na_counter > 0:\n            mask_index[-1] = True\n        arr = IntegerArray(value_counts, mask)\n        index = Index(self.dtype.construct_array_type()(keys, mask_index))\n        return Series(arr, index=index, name='count', copy=False)\n    _HANDLED_TYPES: tuple[type, ...]\n\n    @doc(ExtensionArray._values_for_argsort)\n    def _values_for_argsort(self) -> np.ndarray:\n        return self._data\n\n    def _values_for_json(self) -> np.ndarray:\n        return np.asarray(self, dtype=object)\n\n    @property\n    def _hasna(self) -> bool:\n        return self._mask.any()\n\n    def __arrow_array__(self, type=None):\n        \"\"\"\n        Convert myself into a pyarrow Array.\n        \"\"\"\n        import pyarrow as pa\n        return pa.array(self._data, mask=self._mask, type=type)\n\n    def __getitem__(self, item: PositionalIndexer) -> Self | Any:\n        item = check_array_indexer(self, item)\n        newmask = self._mask[item]\n        if is_bool(newmask):\n            if newmask:\n                return self.dtype.na_value\n            return self._data[item]\n        return self._simple_new(self._data[item], newmask)\n    _logical_method = _arith_method\n\n    @classmethod\n    def _simple_new(cls, values: np.ndarray, mask: npt.NDArray[np.bool_]) -> Self:\n        result = BaseMaskedArray.__new__(cls)\n        result._data = values\n        result._mask = mask\n        return result\n\n    @doc(ExtensionArray.searchsorted)\n    def searchsorted(self, value: NumpyValueArrayLike | ExtensionArray, side: Literal['left', 'right']='left', sorter: NumpySorter | None=None) -> npt.NDArray[np.intp] | np.intp:\n        if self._hasna:\n            raise ValueError('searchsorted requires array to be sorted, which is impossible with NAs present.')\n        if isinstance(value, ExtensionArray):\n            value = value.astype(object)\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n\n    def std(self, *, skipna: bool=True, axis: AxisInt | None=0, ddof: int=1, **kwargs):\n        nv.validate_stat_ddof_func((), kwargs, fname='std')\n        result = masked_reductions.std(self._data, self._mask, skipna=skipna, axis=axis, ddof=ddof)\n        return self._wrap_reduction_result('std', result, skipna=skipna, axis=axis)\n\n    @property\n    def _na_value(self):\n        return self.dtype.na_value\n\n    @property\n    def nbytes(self) -> int:\n        return self._data.nbytes + self._mask.nbytes\n\n    def _reduce(self, name: str, *, skipna: bool=True, keepdims: bool=False, **kwargs):\n        if name in {'any', 'all', 'min', 'max', 'sum', 'prod', 'mean', 'var', 'std'}:\n            result = getattr(self, name)(skipna=skipna, **kwargs)\n        else:\n            data = self._data\n            mask = self._mask\n            op = getattr(nanops, f'nan{name}')\n            axis = kwargs.pop('axis', None)\n            result = op(data, axis=axis, skipna=skipna, mask=mask, **kwargs)\n        if keepdims:\n            if isna(result):\n                return self._wrap_na_result(name=name, axis=0, mask_size=(1,))\n            else:\n                result = result.reshape(1)\n                mask = np.zeros(1, dtype=bool)\n                return self._maybe_mask_result(result, mask)\n        if isna(result):\n            return libmissing.NA\n        else:\n            return result\n\n    @doc(ExtensionArray.fillna)\n    def fillna(self, value=None, method=None, limit: int | None=None, copy: bool=True) -> Self:\n        (value, method) = validate_fillna_kwargs(value, method)\n        mask = self._mask\n        value = missing.check_value_size(value, mask, len(self))\n        if mask.any():\n            if method is not None:\n                func = missing.get_fill_func(method, ndim=self.ndim)\n                npvalues = self._data.T\n                new_mask = mask.T\n                if copy:\n                    npvalues = npvalues.copy()\n                    new_mask = new_mask.copy()\n                func(npvalues, limit=limit, mask=new_mask)\n                return self._simple_new(npvalues.T, new_mask.T)\n            else:\n                if copy:\n                    new_values = self.copy()\n                else:\n                    new_values = self[:]\n                new_values[mask] = value\n        elif copy:\n            new_values = self.copy()\n        else:\n            new_values = self[:]\n        return new_values\n\n    def to_numpy(self, dtype: npt.DTypeLike | None=None, copy: bool=False, na_value: object=lib.no_default) -> np.ndarray:\n        \"\"\"\n        Convert to a NumPy Array.\n\n        By default converts to an object-dtype NumPy array. Specify the `dtype` and\n        `na_value` keywords to customize the conversion.\n\n        Parameters\n        ----------\n        dtype : dtype, default object\n            The numpy dtype to convert to.\n        copy : bool, default False\n            Whether to ensure that the returned value is a not a view on\n            the array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary. This is typically\n            only possible when no missing values are present and `dtype`\n            is the equivalent numpy dtype.\n        na_value : scalar, optional\n             Scalar missing value indicator to use in numpy array. Defaults\n             to the native missing value indicator of this array (pd.NA).\n\n        Returns\n        -------\n        numpy.ndarray\n\n        Examples\n        --------\n        An object-dtype is the default result\n\n        >>> a = pd.array([True, False, pd.NA], dtype=\"boolean\")\n        >>> a.to_numpy()\n        array([True, False, <NA>], dtype=object)\n\n        When no missing values are present, an equivalent dtype can be used.\n\n        >>> pd.array([True, False], dtype=\"boolean\").to_numpy(dtype=\"bool\")\n        array([ True, False])\n        >>> pd.array([1, 2], dtype=\"Int64\").to_numpy(\"int64\")\n        array([1, 2])\n\n        However, requesting such dtype will raise a ValueError if\n        missing values are present and the default missing value :attr:`NA`\n        is used.\n\n        >>> a = pd.array([True, False, pd.NA], dtype=\"boolean\")\n        >>> a\n        <BooleanArray>\n        [True, False, <NA>]\n        Length: 3, dtype: boolean\n\n        >>> a.to_numpy(dtype=\"bool\")\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot convert to bool numpy array in presence of missing values\n\n        Specify a valid `na_value` instead\n\n        >>> a.to_numpy(dtype=\"bool\", na_value=False)\n        array([ True, False, False])\n        \"\"\"\n        hasna = self._hasna\n        (dtype, na_value) = to_numpy_dtype_inference(self, dtype, na_value, hasna)\n        if dtype is None:\n            dtype = object\n        if hasna:\n            if dtype != object and (not is_string_dtype(dtype)) and (na_value is libmissing.NA):\n                raise ValueError(f\"cannot convert to '{dtype}'-dtype NumPy array with missing values. Specify an appropriate 'na_value' for this dtype.\")\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore', category=RuntimeWarning)\n                data = self._data.astype(dtype)\n            data[self._mask] = na_value\n        else:\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore', category=RuntimeWarning)\n                data = self._data.astype(dtype, copy=copy)\n        return data\n\n    def __init__(self, values: np.ndarray, mask: npt.NDArray[np.bool_], copy: bool=False) -> None:\n        if not (isinstance(mask, np.ndarray) and mask.dtype == np.bool_):\n            raise TypeError(\"mask should be boolean numpy array. Use the 'pd.array' function instead\")\n        if values.shape != mask.shape:\n            raise ValueError('values.shape must match mask.shape')\n        if copy:\n            values = values.copy()\n            mask = mask.copy()\n        self._data = values\n        self._mask = mask\n\n    def _groupby_op(self, *, how: str, has_dropped_na: bool, min_count: int, ngroups: int, ids: npt.NDArray[np.intp], **kwargs):\n        from pandas.core.groupby.ops import WrappedCythonOp\n        kind = WrappedCythonOp.get_kind_from_how(how)\n        op = WrappedCythonOp(how=how, kind=kind, has_dropped_na=has_dropped_na)\n        mask = self._mask\n        if op.kind != 'aggregate':\n            result_mask = mask.copy()\n        else:\n            result_mask = np.zeros(ngroups, dtype=bool)\n        if how == 'rank' and kwargs.get('na_option') in ['top', 'bottom']:\n            result_mask[:] = False\n        res_values = op._cython_op_ndim_compat(self._data, min_count=min_count, ngroups=ngroups, comp_ids=ids, mask=mask, result_mask=result_mask, **kwargs)\n        if op.how == 'ohlc':\n            arity = op._cython_arity.get(op.how, 1)\n            result_mask = np.tile(result_mask, (arity, 1)).T\n        if op.how in ['idxmin', 'idxmax']:\n            return res_values\n        else:\n            return self._maybe_mask_result(res_values, result_mask)\n\n    def _arith_method(self, other, op):\n        op_name = op.__name__\n        omask = None\n        if not hasattr(other, 'dtype') and is_list_like(other) and (len(other) == len(self)):\n            other = pd_array(other)\n            other = extract_array(other, extract_numpy=True)\n        if isinstance(other, BaseMaskedArray):\n            (other, omask) = (other._data, other._mask)\n        elif is_list_like(other):\n            if not isinstance(other, ExtensionArray):\n                other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError('can only perform ops with 1-d structures')\n        other = ops.maybe_prepare_scalar_for_op(other, (len(self),))\n        pd_op = ops.get_array_op(op)\n        other = ensure_wrapped_if_datetimelike(other)\n        if op_name in {'pow', 'rpow'} and isinstance(other, np.bool_):\n            other = bool(other)\n        mask = self._propagate_mask(omask, other)\n        if other is libmissing.NA:\n            result = np.ones_like(self._data)\n            if self.dtype.kind == 'b':\n                if op_name in {'floordiv', 'rfloordiv', 'pow', 'rpow', 'truediv', 'rtruediv'}:\n                    raise NotImplementedError(f\"operator '{op_name}' not implemented for bool dtypes\")\n                if op_name in {'mod', 'rmod'}:\n                    dtype = 'int8'\n                else:\n                    dtype = 'bool'\n                result = result.astype(dtype)\n            elif 'truediv' in op_name and self.dtype.kind != 'f':\n                result = result.astype(np.float64)\n        else:\n            if self.dtype.kind in 'iu' and op_name in ['floordiv', 'mod']:\n                pd_op = op\n            with np.errstate(all='ignore'):\n                result = pd_op(self._data, other)\n        if op_name == 'pow':\n            mask = np.where((self._data == 1) & ~self._mask, False, mask)\n            if omask is not None:\n                mask = np.where((other == 0) & ~omask, False, mask)\n            elif other is not libmissing.NA:\n                mask = np.where(other == 0, False, mask)\n        elif op_name == 'rpow':\n            if omask is not None:\n                mask = np.where((other == 1) & ~omask, False, mask)\n            elif other is not libmissing.NA:\n                mask = np.where(other == 1, False, mask)\n            mask = np.where((self._data == 0) & ~self._mask, False, mask)\n        return self._maybe_mask_result(result, mask)\n\n    def _maybe_mask_result(self, result: np.ndarray | tuple[np.ndarray, np.ndarray], mask: np.ndarray):\n        \"\"\"\n        Parameters\n        ----------\n        result : array-like or tuple[array-like]\n        mask : array-like bool\n        \"\"\"\n        if isinstance(result, tuple):\n            (div, mod) = result\n            return (self._maybe_mask_result(div, mask), self._maybe_mask_result(mod, mask))\n        if result.dtype.kind == 'f':\n            from pandas.core.arrays import FloatingArray\n            return FloatingArray(result, mask, copy=False)\n        elif result.dtype.kind == 'b':\n            from pandas.core.arrays import BooleanArray\n            return BooleanArray(result, mask, copy=False)\n        elif lib.is_np_dtype(result.dtype, 'm') and is_supported_dtype(result.dtype):\n            from pandas.core.arrays import TimedeltaArray\n            result[mask] = result.dtype.type('NaT')\n            if not isinstance(result, TimedeltaArray):\n                return TimedeltaArray._simple_new(result, dtype=result.dtype)\n            return result\n        elif result.dtype.kind in 'iu':\n            from pandas.core.arrays import IntegerArray\n            return IntegerArray(result, mask, copy=False)\n        else:\n            result[mask] = np.nan\n            return result\n\n    def _wrap_na_result(self, *, name, axis, mask_size):\n        mask = np.ones(mask_size, dtype=bool)\n        float_dtyp = 'float32' if self.dtype == 'Float32' else 'float64'\n        if name in ['mean', 'median', 'var', 'std', 'skew', 'kurt']:\n            np_dtype = float_dtyp\n        elif name in ['min', 'max'] or self.dtype.itemsize == 8:\n            np_dtype = self.dtype.numpy_dtype.name\n        else:\n            is_windows_or_32bit = is_platform_windows() or not IS64\n            int_dtyp = 'int32' if is_windows_or_32bit else 'int64'\n            uint_dtyp = 'uint32' if is_windows_or_32bit else 'uint64'\n            np_dtype = {'b': int_dtyp, 'i': int_dtyp, 'u': uint_dtyp, 'f': float_dtyp}[self.dtype.kind]\n        value = np.array([1], dtype=np_dtype)\n        return self._maybe_mask_result(value, mask=mask)\n\n    def isna(self) -> np.ndarray:\n        return self._mask.copy()\n\n    def __abs__(self) -> Self:\n        return self._simple_new(abs(self._data), self._mask.copy())\n\n    def interpolate(self, *, method: InterpolateOptions, axis: int, index, limit, limit_direction, limit_area, copy: bool, **kwargs) -> FloatingArray:\n        \"\"\"\n        See NDFrame.interpolate.__doc__.\n        \"\"\"\n        if self.dtype.kind == 'f':\n            if copy:\n                data = self._data.copy()\n                mask = self._mask.copy()\n            else:\n                data = self._data\n                mask = self._mask\n        elif self.dtype.kind in 'iu':\n            copy = True\n            data = self._data.astype('f8')\n            mask = self._mask.copy()\n        else:\n            raise NotImplementedError(f'interpolate is not implemented for dtype={self.dtype}')\n        missing.interpolate_2d_inplace(data, method=method, axis=0, index=index, limit=limit, limit_direction=limit_direction, limit_area=limit_area, mask=mask, **kwargs)\n        if not copy:\n            return self\n        if self.dtype.kind == 'f':\n            return type(self)._simple_new(data, mask)\n        else:\n            from pandas.core.arrays import FloatingArray\n            return FloatingArray._simple_new(data, mask)\n\n    def __pos__(self) -> Self:\n        return self.copy()\n\n    def unique(self) -> Self:\n        \"\"\"\n        Compute the BaseMaskedArray of unique values.\n\n        Returns\n        -------\n        uniques : BaseMaskedArray\n        \"\"\"\n        (uniques, mask) = algos.unique_with_mask(self._data, self._mask)\n        return self._simple_new(uniques, mask)\n\n    def _putmask(self, mask: npt.NDArray[np.bool_], value) -> None:\n        \"\"\"\n        Analogue to np.putmask(self, mask, value)\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool]\n        value : scalar or listlike\n            If listlike, must be arraylike with same length as self.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        Unlike np.putmask, we do not repeat listlike values with mismatched length.\n        'value' should either be a scalar or an arraylike with the same length\n        as self.\n        \"\"\"\n        if is_list_like(value):\n            val = value[mask]\n        else:\n            val = value\n        self[mask] = val\n\n    @doc(ExtensionArray.equals)\n    def equals(self, other) -> bool:\n        if type(self) != type(other):\n            return False\n        if other.dtype != self.dtype:\n            return False\n        if not np.array_equal(self._mask, other._mask):\n            return False\n        left = self._data[~self._mask]\n        right = other._data[~other._mask]\n        return array_equivalent(left, right, strict_nan=True, dtype_equal=True)\n\n    def __array__(self, dtype: NpDtype | None=None, copy: bool | None=None) -> np.ndarray:\n        \"\"\"\n        the array interface, return my values\n        We return an object array here to preserve our scalar values\n        \"\"\"\n        return self.to_numpy(dtype=dtype)\n\n    def _hash_pandas_object(self, *, encoding: str, hash_key: str, categorize: bool) -> npt.NDArray[np.uint64]:\n        hashed_array = hash_array(self._data, encoding=encoding, hash_key=hash_key, categorize=categorize)\n        hashed_array[self.isna()] = hash(self.dtype.na_value)\n        return hashed_array\n\n    def __contains__(self, key) -> bool:\n        if isna(key) and key is not self.dtype.na_value:\n            if self._data.dtype.kind == 'f' and lib.is_float(key):\n                return bool((np.isnan(self._data) & ~self._mask).any())\n        return bool(super().__contains__(key))\n\n    @doc(ExtensionArray.duplicated)\n    def duplicated(self, keep: Literal['first', 'last', False]='first') -> npt.NDArray[np.bool_]:\n        values = self._data\n        mask = self._mask\n        return algos.duplicated(values, keep=keep, mask=mask)\n\n    def __len__(self) -> int:\n        return len(self._data)\n\n    @doc(ExtensionArray.tolist)\n    def tolist(self):\n        if self.ndim > 1:\n            return [x.tolist() for x in self]\n        dtype = None if self._hasna else self._data.dtype\n        return self.to_numpy(dtype=dtype, na_value=libmissing.NA).tolist()\n\n    def var(self, *, skipna: bool=True, axis: AxisInt | None=0, ddof: int=1, **kwargs):\n        nv.validate_stat_ddof_func((), kwargs, fname='var')\n        result = masked_reductions.var(self._data, self._mask, skipna=skipna, axis=axis, ddof=ddof)\n        return self._wrap_reduction_result('var', result, skipna=skipna, axis=axis)\n\n    def sum(self, *, skipna: bool=True, min_count: int=0, axis: AxisInt | None=0, **kwargs):\n        nv.validate_sum((), kwargs)\n        result = masked_reductions.sum(self._data, self._mask, skipna=skipna, min_count=min_count, axis=axis)\n        return self._wrap_min_count_reduction_result('sum', result, skipna=skipna, min_count=min_count, axis=axis)\n\n    def _accumulate(self, name: str, *, skipna: bool=True, **kwargs) -> BaseMaskedArray:\n        data = self._data\n        mask = self._mask\n        op = getattr(masked_accumulations, name)\n        (data, mask) = op(data, mask, skipna=skipna, **kwargs)\n        return self._simple_new(data, mask)\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n        out = kwargs.get('out', ())\n        for x in inputs + out:\n            if not isinstance(x, self._HANDLED_TYPES + (BaseMaskedArray,)):\n                return NotImplemented\n        result = arraylike.maybe_dispatch_ufunc_to_dunder_op(self, ufunc, method, *inputs, **kwargs)\n        if result is not NotImplemented:\n            return result\n        if 'out' in kwargs:\n            return arraylike.dispatch_ufunc_with_out(self, ufunc, method, *inputs, **kwargs)\n        if method == 'reduce':\n            result = arraylike.dispatch_reduction_ufunc(self, ufunc, method, *inputs, **kwargs)\n            if result is not NotImplemented:\n                return result\n        mask = np.zeros(len(self), dtype=bool)\n        inputs2 = []\n        for x in inputs:\n            if isinstance(x, BaseMaskedArray):\n                mask |= x._mask\n                inputs2.append(x._data)\n            else:\n                inputs2.append(x)\n\n        def reconstruct(x: np.ndarray):\n            from pandas.core.arrays import BooleanArray, FloatingArray, IntegerArray\n            if x.dtype.kind == 'b':\n                m = mask.copy()\n                return BooleanArray(x, m)\n            elif x.dtype.kind in 'iu':\n                m = mask.copy()\n                return IntegerArray(x, m)\n            elif x.dtype.kind == 'f':\n                m = mask.copy()\n                if x.dtype == np.float16:\n                    x = x.astype(np.float32)\n                return FloatingArray(x, m)\n            else:\n                x[mask] = np.nan\n            return x\n        result = getattr(ufunc, method)(*inputs2, **kwargs)\n        if ufunc.nout > 1:\n            return tuple((reconstruct(x) for x in result))\n        elif method == 'reduce':\n            if self._mask.any():\n                return self._na_value\n            return result\n        else:\n            return reconstruct(result)\n\n    def max(self, *, skipna: bool=True, axis: AxisInt | None=0, **kwargs):\n        nv.validate_max((), kwargs)\n        result = masked_reductions.max(self._data, self._mask, skipna=skipna, axis=axis)\n        return self._wrap_reduction_result('max', result, skipna=skipna, axis=axis)\n\n    def __iter__(self) -> Iterator:\n        if self.ndim == 1:\n            if not self._hasna:\n                for val in self._data:\n                    yield val\n            else:\n                na_value = self.dtype.na_value\n                for (isna_, val) in zip(self._mask, self._data):\n                    if isna_:\n                        yield na_value\n                    else:\n                        yield val\n        else:\n            for i in range(len(self)):\n                yield self[i]\n\n    def prod(self, *, skipna: bool=True, min_count: int=0, axis: AxisInt | None=0, **kwargs):\n        nv.validate_prod((), kwargs)\n        result = masked_reductions.prod(self._data, self._mask, skipna=skipna, min_count=min_count, axis=axis)\n        return self._wrap_min_count_reduction_result('prod', result, skipna=skipna, min_count=min_count, axis=axis)\n\n    def _cmp_method(self, other, op) -> BooleanArray:\n        from pandas.core.arrays import BooleanArray\n        mask = None\n        if isinstance(other, BaseMaskedArray):\n            (other, mask) = (other._data, other._mask)\n        elif is_list_like(other):\n            other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError('can only perform ops with 1-d structures')\n            if len(self) != len(other):\n                raise ValueError('Lengths must match to compare')\n        if other is libmissing.NA:\n            result = np.zeros(self._data.shape, dtype='bool')\n            mask = np.ones(self._data.shape, dtype='bool')\n        else:\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore', 'elementwise', FutureWarning)\n                warnings.filterwarnings('ignore', 'elementwise', DeprecationWarning)\n                method = getattr(self._data, f'__{op.__name__}__')\n                result = method(other)\n                if result is NotImplemented:\n                    result = invalid_comparison(self._data, other, op)\n        mask = self._propagate_mask(mask, other)\n        return BooleanArray(result, mask, copy=False)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat: Sequence[Self], axis: AxisInt=0) -> Self:\n        data = np.concatenate([x._data for x in to_concat], axis=axis)\n        mask = np.concatenate([x._mask for x in to_concat], axis=axis)\n        return cls(data, mask)\n\n    def take(self, indexer, *, allow_fill: bool=False, fill_value: Scalar | None=None, axis: AxisInt=0) -> Self:\n        data_fill_value = self._internal_fill_value if isna(fill_value) else fill_value\n        result = take(self._data, indexer, fill_value=data_fill_value, allow_fill=allow_fill, axis=axis)\n        mask = take(self._mask, indexer, fill_value=True, allow_fill=allow_fill, axis=axis)\n        if allow_fill and notna(fill_value):\n            fill_mask = np.asarray(indexer) == -1\n            result[fill_mask] = fill_value\n            mask = mask ^ fill_mask\n        return self._simple_new(result, mask)\n\n    def _quantile(self, qs: npt.NDArray[np.float64], interpolation: str) -> BaseMaskedArray:\n        \"\"\"\n        Dispatch to quantile_with_mask, needed because we do not have\n        _from_factorized.\n\n        Notes\n        -----\n        We assume that all impacted cases are 1D-only.\n        \"\"\"\n        res = quantile_with_mask(self._data, mask=self._mask, fill_value=np.nan, qs=qs, interpolation=interpolation)\n        if self._hasna:\n            if self.ndim == 2:\n                raise NotImplementedError\n            if self.isna().all():\n                out_mask = np.ones(res.shape, dtype=bool)\n                if is_integer_dtype(self.dtype):\n                    res = np.zeros(res.shape, dtype=self.dtype.numpy_dtype)\n            else:\n                out_mask = np.zeros(res.shape, dtype=bool)\n        else:\n            out_mask = np.zeros(res.shape, dtype=bool)\n        return self._maybe_mask_result(res, mask=out_mask)\n\n    def swapaxes(self, axis1, axis2) -> Self:\n        data = self._data.swapaxes(axis1, axis2)\n        mask = self._mask.swapaxes(axis1, axis2)\n        return self._simple_new(data, mask)\n\n    def isin(self, values: ArrayLike) -> BooleanArray:\n        from pandas.core.arrays import BooleanArray\n        values_arr = np.asarray(values)\n        result = isin(self._data, values_arr)\n        if self._hasna:\n            values_have_NA = values_arr.dtype == object and any((val is self.dtype.na_value for val in values_arr))\n            result[self._mask] = values_have_NA\n        mask = np.zeros(self._data.shape, dtype=bool)\n        return BooleanArray(result, mask, copy=False)", "class_fn": true, "question_id": "pandas/pandas.core.arrays.masked/BaseMaskedArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/period.py", "fn_id": "", "content": "class PeriodArray(dtl.DatelikeOps, libperiod.PeriodMixin):\n    \"\"\"\n    Pandas ExtensionArray for storing Period data.\n\n    Users should use :func:`~pandas.array` to create new instances.\n\n    Parameters\n    ----------\n    values : Union[PeriodArray, Series[period], ndarray[int], PeriodIndex]\n        The data to store. These should be arrays that can be directly\n        converted to ordinals without inference or copy (PeriodArray,\n        ndarray[int64]), or a box around such an array (Series[period],\n        PeriodIndex).\n    dtype : PeriodDtype, optional\n        A PeriodDtype instance from which to extract a `freq`. If both\n        `freq` and `dtype` are specified, then the frequencies must match.\n    freq : str or DateOffset\n        The `freq` to use for the array. Mostly applicable when `values`\n        is an ndarray of integers, when `freq` is required. When `values`\n        is a PeriodArray (or box around), it's checked that ``values.freq``\n        matches `freq`.\n    copy : bool, default False\n        Whether to copy the ordinals before storing.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    See Also\n    --------\n    Period: Represents a period of time.\n    PeriodIndex : Immutable Index for period data.\n    period_range: Create a fixed-frequency PeriodArray.\n    array: Construct a pandas array.\n\n    Notes\n    -----\n    There are two components to a PeriodArray\n\n    - ordinals : integer ndarray\n    - freq : pd.tseries.offsets.Offset\n\n    The values are physically stored as a 1-D ndarray of integers. These are\n    called \"ordinals\" and represent some kind of offset from a base.\n\n    The `freq` indicates the span covered by each element of the array.\n    All elements in the PeriodArray have the same `freq`.\n\n    Examples\n    --------\n    >>> pd.arrays.PeriodArray(pd.PeriodIndex(['2023-01-01',\n    ...                                       '2023-01-02'], freq='D'))\n    <PeriodArray>\n    ['2023-01-01', '2023-01-02']\n    Length: 2, dtype: period[D]\n    \"\"\"\n    __array_priority__ = 1000\n    _typ = 'periodarray'\n    _internal_fill_value = np.int64(iNaT)\n    _recognized_scalars = (Period,)\n\n    def __arrow_array__(self, type=None):\n        \"\"\"\n        Convert myself into a pyarrow Array.\n        \"\"\"\n        import pyarrow\n        from pandas.core.arrays.arrow.extension_types import ArrowPeriodType\n        if type is not None:\n            if pyarrow.types.is_integer(type):\n                return pyarrow.array(self._ndarray, mask=self.isna(), type=type)\n            elif isinstance(type, ArrowPeriodType):\n                if self.freqstr != type.freq:\n                    raise TypeError(f\"Not supported to convert PeriodArray to array with different 'freq' ({self.freqstr} vs {type.freq})\")\n            else:\n                raise TypeError(f\"Not supported to convert PeriodArray to '{type}' type\")\n        period_type = ArrowPeriodType(self.freqstr)\n        storage_array = pyarrow.array(self._ndarray, mask=self.isna(), type='int64')\n        return pyarrow.ExtensionArray.from_storage(period_type, storage_array)\n    _infer_matches = ('period',)\n\n    @property\n    def _scalar_type(self) -> type[Period]:\n        return Period\n    _other_ops: list[str] = []\n    _bool_ops: list[str] = ['is_leap_year']\n    _object_ops: list[str] = ['start_time', 'end_time', 'freq']\n    _field_ops: list[str] = ['year', 'month', 'day', 'hour', 'minute', 'second', 'weekofyear', 'weekday', 'week', 'dayofweek', 'day_of_week', 'dayofyear', 'day_of_year', 'quarter', 'qyear', 'days_in_month', 'daysinmonth']\n    _datetimelike_ops: list[str] = _field_ops + _object_ops + _bool_ops\n    _datetimelike_methods: list[str] = ['strftime', 'to_timestamp', 'asfreq']\n    _dtype: PeriodDtype\n\n    @classmethod\n    def _from_datetime64(cls, data, freq, tz=None) -> Self:\n        \"\"\"\n        Construct a PeriodArray from a datetime64 array\n\n        Parameters\n        ----------\n        data : ndarray[datetime64[ns], datetime64[ns, tz]]\n        freq : str or Tick\n        tz : tzinfo, optional\n\n        Returns\n        -------\n        PeriodArray[freq]\n        \"\"\"\n        if isinstance(freq, BaseOffset):\n            freq = freq_to_period_freqstr(freq.n, freq.name)\n        (data, freq) = dt64arr_to_periodarr(data, freq, tz)\n        dtype = PeriodDtype(freq)\n        return cls(data, dtype=dtype)\n\n    def fillna(self, value=None, method=None, limit: int | None=None, copy: bool=True) -> Self:\n        if method is not None:\n            dta = self.view('M8[ns]')\n            result = dta.fillna(value=value, method=method, limit=limit, copy=copy)\n            return result.view(self.dtype)\n        return super().fillna(value=value, method=method, limit=limit, copy=copy)\n\n    def _unbox_scalar(self, value: Period | NaTType) -> np.int64:\n        if value is NaT:\n            return np.int64(value._value)\n        elif isinstance(value, self._scalar_type):\n            self._check_compatible_with(value)\n            return np.int64(value.ordinal)\n        else:\n            raise ValueError(f\"'value' should be a Period. Got '{value}' instead.\")\n\n    @classmethod\n    def _from_fields(cls, *, fields: dict, freq) -> Self:\n        (subarr, freq) = _range_from_fields(freq=freq, **fields)\n        dtype = PeriodDtype(freq)\n        return cls._simple_new(subarr, dtype=dtype)\n\n    def astype(self, dtype, copy: bool=True):\n        dtype = pandas_dtype(dtype)\n        if dtype == self._dtype:\n            if not copy:\n                return self\n            else:\n                return self.copy()\n        if isinstance(dtype, PeriodDtype):\n            return self.asfreq(dtype.freq)\n        if lib.is_np_dtype(dtype, 'M') or isinstance(dtype, DatetimeTZDtype):\n            tz = getattr(dtype, 'tz', None)\n            unit = dtl.dtype_to_unit(dtype)\n            return self.to_timestamp().tz_localize(tz).as_unit(unit)\n        return super().astype(dtype, copy=copy)\n\n    def _scalar_from_string(self, value: str) -> Period:\n        return Period(value, freq=self.freq)\n\n    @classmethod\n    def _generate_range(cls, start, end, periods, freq):\n        periods = dtl.validate_periods(periods)\n        if freq is not None:\n            freq = Period._maybe_convert_freq(freq)\n        if start is not None or end is not None:\n            (subarr, freq) = _get_ordinal_range(start, end, periods, freq)\n        else:\n            raise ValueError('Not enough parameters to construct Period range')\n        return (subarr, freq)\n\n    @classmethod\n    def _from_sequence_of_strings(cls, strings, *, dtype: Dtype | None=None, copy: bool=False) -> Self:\n        return cls._from_sequence(strings, dtype=dtype, copy=copy)\n\n    def _formatter(self, boxed: bool=False):\n        if boxed:\n            return str\n        return \"'{}'\".format\n\n    @doc(**_shared_doc_kwargs, other='PeriodIndex', other_name='PeriodIndex')\n    def asfreq(self, freq=None, how: str='E') -> Self:\n        \"\"\"\n        Convert the {klass} to the specified frequency `freq`.\n\n        Equivalent to applying :meth:`pandas.Period.asfreq` with the given arguments\n        to each :class:`~pandas.Period` in this {klass}.\n\n        Parameters\n        ----------\n        freq : str\n            A frequency.\n        how : str {{'E', 'S'}}, default 'E'\n            Whether the elements should be aligned to the end\n            or start within pa period.\n\n            * 'E', 'END', or 'FINISH' for end,\n            * 'S', 'START', or 'BEGIN' for start.\n\n            January 31st ('END') vs. January 1st ('START') for example.\n\n        Returns\n        -------\n        {klass}\n            The transformed {klass} with the new frequency.\n\n        See Also\n        --------\n        {other}.asfreq: Convert each Period in a {other_name} to the given frequency.\n        Period.asfreq : Convert a :class:`~pandas.Period` object to the given frequency.\n\n        Examples\n        --------\n        >>> pidx = pd.period_range('2010-01-01', '2015-01-01', freq='Y')\n        >>> pidx\n        PeriodIndex(['2010', '2011', '2012', '2013', '2014', '2015'],\n        dtype='period[Y-DEC]')\n\n        >>> pidx.asfreq('M')\n        PeriodIndex(['2010-12', '2011-12', '2012-12', '2013-12', '2014-12',\n        '2015-12'], dtype='period[M]')\n\n        >>> pidx.asfreq('M', how='S')\n        PeriodIndex(['2010-01', '2011-01', '2012-01', '2013-01', '2014-01',\n        '2015-01'], dtype='period[M]')\n        \"\"\"\n        how = libperiod.validate_end_alias(how)\n        if isinstance(freq, BaseOffset) and hasattr(freq, '_period_dtype_code'):\n            freq = PeriodDtype(freq)._freqstr\n        freq = Period._maybe_convert_freq(freq)\n        base1 = self._dtype._dtype_code\n        base2 = freq._period_dtype_code\n        asi8 = self.asi8\n        end = how == 'E'\n        if end:\n            ordinal = asi8 + self.dtype._n - 1\n        else:\n            ordinal = asi8\n        new_data = period_asfreq_arr(ordinal, base1, base2, end)\n        if self._hasna:\n            new_data[self._isnan] = iNaT\n        dtype = PeriodDtype(freq)\n        return type(self)(new_data, dtype=dtype)\n\n    @cache_readonly\n    def dtype(self) -> PeriodDtype:\n        return self._dtype\n\n    @property\n    def freq(self) -> BaseOffset:\n        \"\"\"\n        Return the frequency object for this PeriodArray.\n        \"\"\"\n        return self.dtype.freq\n\n    @property\n    def freqstr(self) -> str:\n        return freq_to_period_freqstr(self.freq.n, self.freq.name)\n\n    def _check_timedeltalike_freq_compat(self, other):\n        \"\"\"\n        Arithmetic operations with timedelta-like scalars or array `other`\n        are only valid if `other` is an integer multiple of `self.freq`.\n        If the operation is valid, find that integer multiple.  Otherwise,\n        raise because the operation is invalid.\n\n        Parameters\n        ----------\n        other : timedelta, np.timedelta64, Tick,\n                ndarray[timedelta64], TimedeltaArray, TimedeltaIndex\n\n        Returns\n        -------\n        multiple : int or ndarray[int64]\n\n        Raises\n        ------\n        IncompatibleFrequency\n        \"\"\"\n        assert self.dtype._is_tick_like()\n        dtype = np.dtype(f'm8[{self.dtype._td64_unit}]')\n        if isinstance(other, (timedelta, np.timedelta64, Tick)):\n            td = np.asarray(Timedelta(other).asm8)\n        else:\n            td = np.asarray(other)\n        try:\n            delta = astype_overflowsafe(td, dtype=dtype, copy=False, round_ok=False)\n        except ValueError as err:\n            raise raise_on_incompatible(self, other) from err\n        delta = delta.view('i8')\n        return lib.item_from_zerodim(delta)\n\n    def searchsorted(self, value: NumpyValueArrayLike | ExtensionArray, side: Literal['left', 'right']='left', sorter: NumpySorter | None=None) -> npt.NDArray[np.intp] | np.intp:\n        npvalue = self._validate_setitem_value(value).view('M8[ns]')\n        m8arr = self._ndarray.view('M8[ns]')\n        return m8arr.searchsorted(npvalue, side=side, sorter=sorter)\n    year = _field_accessor('year', '\\n        The year of the period.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.PeriodIndex([\"2023\", \"2024\", \"2025\"], freq=\"Y\")\\n        >>> idx.year\\n        Index([2023, 2024, 2025], dtype=\\'int64\\')\\n        ')\n    month = _field_accessor('month', '\\n        The month as January=1, December=12.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.PeriodIndex([\"2023-01\", \"2023-02\", \"2023-03\"], freq=\"M\")\\n        >>> idx.month\\n        Index([1, 2, 3], dtype=\\'int64\\')\\n        ')\n    day = _field_accessor('day', \"\\n        The days of the period.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.PeriodIndex(['2020-01-31', '2020-02-28'], freq='D')\\n        >>> idx.day\\n        Index([31, 28], dtype='int64')\\n        \")\n    hour = _field_accessor('hour', '\\n        The hour of the period.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.PeriodIndex([\"2023-01-01 10:00\", \"2023-01-01 11:00\"], freq=\\'h\\')\\n        >>> idx.hour\\n        Index([10, 11], dtype=\\'int64\\')\\n        ')\n    minute = _field_accessor('minute', '\\n        The minute of the period.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.PeriodIndex([\"2023-01-01 10:30:00\",\\n        ...                       \"2023-01-01 11:50:00\"], freq=\\'min\\')\\n        >>> idx.minute\\n        Index([30, 50], dtype=\\'int64\\')\\n        ')\n    second = _field_accessor('second', '\\n        The second of the period.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.PeriodIndex([\"2023-01-01 10:00:30\",\\n        ...                       \"2023-01-01 10:00:31\"], freq=\\'s\\')\\n        >>> idx.second\\n        Index([30, 31], dtype=\\'int64\\')\\n        ')\n    weekofyear = _field_accessor('week', '\\n        The week ordinal of the year.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.PeriodIndex([\"2023-01\", \"2023-02\", \"2023-03\"], freq=\"M\")\\n        >>> idx.week  # It can be written `weekofyear`\\n        Index([5, 9, 13], dtype=\\'int64\\')\\n        ')\n    week = weekofyear\n    day_of_week = _field_accessor('day_of_week', '\\n        The day of the week with Monday=0, Sunday=6.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.PeriodIndex([\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"], freq=\"D\")\\n        >>> idx.weekday\\n        Index([6, 0, 1], dtype=\\'int64\\')\\n        ')\n    dayofweek = day_of_week\n    weekday = dayofweek\n    dayofyear = day_of_year = _field_accessor('day_of_year', '\\n        The ordinal day of the year.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.PeriodIndex([\"2023-01-10\", \"2023-02-01\", \"2023-03-01\"], freq=\"D\")\\n        >>> idx.dayofyear\\n        Index([10, 32, 60], dtype=\\'int64\\')\\n\\n        >>> idx = pd.PeriodIndex([\"2023\", \"2024\", \"2025\"], freq=\"Y\")\\n        >>> idx\\n        PeriodIndex([\\'2023\\', \\'2024\\', \\'2025\\'], dtype=\\'period[Y-DEC]\\')\\n        >>> idx.dayofyear\\n        Index([365, 366, 365], dtype=\\'int64\\')\\n        ')\n    quarter = _field_accessor('quarter', '\\n        The quarter of the date.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.PeriodIndex([\"2023-01\", \"2023-02\", \"2023-03\"], freq=\"M\")\\n        >>> idx.quarter\\n        Index([1, 1, 1], dtype=\\'int64\\')\\n        ')\n    qyear = _field_accessor('qyear')\n    days_in_month = _field_accessor('days_in_month', '\\n        The number of days in the month.\\n\\n        Examples\\n        --------\\n        For Series:\\n\\n        >>> period = pd.period_range(\\'2020-1-1 00:00\\', \\'2020-3-1 00:00\\', freq=\\'M\\')\\n        >>> s = pd.Series(period)\\n        >>> s\\n        0   2020-01\\n        1   2020-02\\n        2   2020-03\\n        dtype: period[M]\\n        >>> s.dt.days_in_month\\n        0    31\\n        1    29\\n        2    31\\n        dtype: int64\\n\\n        For PeriodIndex:\\n\\n        >>> idx = pd.PeriodIndex([\"2023-01\", \"2023-02\", \"2023-03\"], freq=\"M\")\\n        >>> idx.days_in_month   # It can be also entered as `daysinmonth`\\n        Index([31, 28, 31], dtype=\\'int64\\')\\n        ')\n    daysinmonth = days_in_month\n\n    @property\n    def is_leap_year(self) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Logical indicating if the date belongs to a leap year.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023\", \"2024\", \"2025\"], freq=\"Y\")\n        >>> idx.is_leap_year\n        array([False,  True, False])\n        \"\"\"\n        return isleapyear_arr(np.asarray(self.year))\n\n    def _check_compatible_with(self, other: Period | NaTType | PeriodArray) -> None:\n        if other is NaT:\n            return\n        self._require_matching_freq(other.freq)\n\n    def _add_timedelta_arraylike(self, other: TimedeltaArray | npt.NDArray[np.timedelta64]) -> Self:\n        \"\"\"\n        Parameters\n        ----------\n        other : TimedeltaArray or ndarray[timedelta64]\n\n        Returns\n        -------\n        PeriodArray\n        \"\"\"\n        if not self.dtype._is_tick_like():\n            raise TypeError(f'Cannot add or subtract timedelta64[ns] dtype from {self.dtype}')\n        dtype = np.dtype(f'm8[{self.dtype._td64_unit}]')\n        try:\n            delta = astype_overflowsafe(np.asarray(other), dtype=dtype, copy=False, round_ok=False)\n        except ValueError as err:\n            raise IncompatibleFrequency(\"Cannot add/subtract timedelta-like from PeriodArray that is not an integer multiple of the PeriodArray's freq.\") from err\n        res_values = add_overflowsafe(self.asi8, np.asarray(delta.view('i8')))\n        return type(self)(res_values, dtype=self.dtype)\n\n    @classmethod\n    def _simple_new(cls, values: npt.NDArray[np.int64], dtype: PeriodDtype) -> Self:\n        assertion_msg = 'Should be numpy array of type i8'\n        assert isinstance(values, np.ndarray) and values.dtype == 'i8', assertion_msg\n        return cls(values, dtype=dtype)\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype: Dtype | None=None, copy: bool=False) -> Self:\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n        if dtype and isinstance(dtype, PeriodDtype):\n            freq = dtype.freq\n        else:\n            freq = None\n        if isinstance(scalars, cls):\n            validate_dtype_freq(scalars.dtype, freq)\n            if copy:\n                scalars = scalars.copy()\n            return scalars\n        periods = np.asarray(scalars, dtype=object)\n        freq = freq or libperiod.extract_freq(periods)\n        ordinals = libperiod.extract_ordinals(periods, freq)\n        dtype = PeriodDtype(freq)\n        return cls(ordinals, dtype=dtype)\n\n    def _add_offset(self, other: BaseOffset):\n        assert not isinstance(other, Tick)\n        self._require_matching_freq(other, base=True)\n        return self._addsub_int_array_or_scalar(other.n, operator.add)\n\n    def _add_timedeltalike_scalar(self, other):\n        \"\"\"\n        Parameters\n        ----------\n        other : timedelta, Tick, np.timedelta64\n\n        Returns\n        -------\n        PeriodArray\n        \"\"\"\n        if not isinstance(self.freq, Tick):\n            raise raise_on_incompatible(self, other)\n        if isna(other):\n            return super()._add_timedeltalike_scalar(other)\n        td = np.asarray(Timedelta(other).asm8)\n        return self._add_timedelta_arraylike(td)\n\n    def to_timestamp(self, freq=None, how: str='start') -> DatetimeArray:\n        \"\"\"\n        Cast to DatetimeArray/Index.\n\n        Parameters\n        ----------\n        freq : str or DateOffset, optional\n            Target frequency. The default is 'D' for week or longer,\n            's' otherwise.\n        how : {'s', 'e', 'start', 'end'}\n            Whether to use the start or end of the time period being converted.\n\n        Returns\n        -------\n        DatetimeArray/Index\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex([\"2023-01\", \"2023-02\", \"2023-03\"], freq=\"M\")\n        >>> idx.to_timestamp()\n        DatetimeIndex(['2023-01-01', '2023-02-01', '2023-03-01'],\n        dtype='datetime64[ns]', freq='MS')\n        \"\"\"\n        from pandas.core.arrays import DatetimeArray\n        how = libperiod.validate_end_alias(how)\n        end = how == 'E'\n        if end:\n            if freq == 'B' or self.freq == 'B':\n                adjust = Timedelta(1, 'D') - Timedelta(1, 'ns')\n                return self.to_timestamp(how='start') + adjust\n            else:\n                adjust = Timedelta(1, 'ns')\n                return (self + self.freq).to_timestamp(how='start') - adjust\n        if freq is None:\n            freq_code = self._dtype._get_to_timestamp_base()\n            dtype = PeriodDtypeBase(freq_code, 1)\n            freq = dtype._freqstr\n            base = freq_code\n        else:\n            freq = Period._maybe_convert_freq(freq)\n            base = freq._period_dtype_code\n        new_parr = self.asfreq(freq, how=how)\n        new_data = libperiod.periodarr_to_dt64arr(new_parr.asi8, base)\n        dta = DatetimeArray._from_sequence(new_data)\n        if self.freq.name == 'B':\n            diffs = libalgos.unique_deltas(self.asi8)\n            if len(diffs) == 1:\n                diff = diffs[0]\n                if diff == self.dtype._n:\n                    dta._freq = self.freq\n                elif diff == 1:\n                    dta._freq = self.freq.base\n            return dta\n        else:\n            return dta._with_freq('infer')\n\n    @Substitution(URL='https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior')\n    def strftime(self, date_format: str) -> npt.NDArray[np.object_]:\n        \"\"\"\n        Convert to Index using specified date_format.\n\n        Return an Index of formatted strings specified by date_format, which\n        supports the same string format as the python standard library. Details\n        of the string format can be found in `python string format\n        doc <%(URL)s>`__.\n\n        Formats supported by the C `strftime` API but not by the python string format\n        doc (such as `\"%%R\"`, `\"%%r\"`) are not officially supported and should be\n        preferably replaced with their supported equivalents (such as `\"%%H:%%M\"`,\n        `\"%%I:%%M:%%S %%p\"`).\n\n        Note that `PeriodIndex` support additional directives, detailed in\n        `Period.strftime`.\n\n        Parameters\n        ----------\n        date_format : str\n            Date format string (e.g. \"%%Y-%%m-%%d\").\n\n        Returns\n        -------\n        ndarray[object]\n            NumPy ndarray of formatted strings.\n\n        See Also\n        --------\n        to_datetime : Convert the given argument to datetime.\n        DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.\n        DatetimeIndex.round : Round the DatetimeIndex to the specified freq.\n        DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.\n        Timestamp.strftime : Format a single Timestamp.\n        Period.strftime : Format a single Period.\n\n        Examples\n        --------\n        >>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"),\n        ...                     periods=3, freq='s')\n        >>> rng.strftime('%%B %%d, %%Y, %%r')\n        Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\n               'March 10, 2018, 09:00:02 AM'],\n              dtype='object')\n        \"\"\"\n        result = self._format_native_types(date_format=date_format, na_rep=np.nan)\n        return result.astype(object, copy=False)\n\n    def _addsub_int_array_or_scalar(self, other: np.ndarray | int, op: Callable[[Any, Any], Any]) -> Self:\n        \"\"\"\n        Add or subtract array of integers.\n\n        Parameters\n        ----------\n        other : np.ndarray[int64] or int\n        op : {operator.add, operator.sub}\n\n        Returns\n        -------\n        result : PeriodArray\n        \"\"\"\n        assert op in [operator.add, operator.sub]\n        if op is operator.sub:\n            other = -other\n        res_values = add_overflowsafe(self.asi8, np.asarray(other, dtype='i8'))\n        return type(self)(res_values, dtype=self.dtype)\n    _is_recognized_dtype = lambda x: isinstance(x, PeriodDtype)\n\n    def __array__(self, dtype: NpDtype | None=None, copy: bool | None=None) -> np.ndarray:\n        if dtype == 'i8':\n            return self.asi8\n        elif dtype == bool:\n            return ~self._isnan\n        return np.array(list(self), dtype=object)\n\n    def _format_native_types(self, *, na_rep: str | float='NaT', date_format=None, **kwargs) -> npt.NDArray[np.object_]:\n        \"\"\"\n        actually format my specific types\n        \"\"\"\n        return libperiod.period_array_strftime(self.asi8, self.dtype._dtype_code, na_rep, date_format)\n\n    def _pad_or_backfill(self, *, method: FillnaOptions, limit: int | None=None, limit_area: Literal['inside', 'outside'] | None=None, copy: bool=True) -> Self:\n        dta = self.view('M8[ns]')\n        result = dta._pad_or_backfill(method=method, limit=limit, limit_area=limit_area, copy=copy)\n        if copy:\n            return cast('Self', result.view(self.dtype))\n        else:\n            return self\n\n    def __init__(self, values, dtype: Dtype | None=None, freq=None, copy: bool=False) -> None:\n        if freq is not None:\n            warnings.warn(\"The 'freq' keyword in the PeriodArray constructor is deprecated and will be removed in a future version. Pass 'dtype' instead\", FutureWarning, stacklevel=find_stack_level())\n            freq = validate_dtype_freq(dtype, freq)\n            dtype = PeriodDtype(freq)\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n            if not isinstance(dtype, PeriodDtype):\n                raise ValueError(f'Invalid dtype {dtype} for PeriodArray')\n        if isinstance(values, ABCSeries):\n            values = values._values\n            if not isinstance(values, type(self)):\n                raise TypeError('Incorrect dtype')\n        elif isinstance(values, ABCPeriodIndex):\n            values = values._values\n        if isinstance(values, type(self)):\n            if dtype is not None and dtype != values.dtype:\n                raise raise_on_incompatible(values, dtype.freq)\n            (values, dtype) = (values._ndarray, values.dtype)\n        if not copy:\n            values = np.asarray(values, dtype='int64')\n        else:\n            values = np.array(values, dtype='int64', copy=copy)\n        if dtype is None:\n            raise ValueError('dtype is not specified and cannot be inferred')\n        dtype = cast(PeriodDtype, dtype)\n        NDArrayBacked.__init__(self, values, dtype)\n\n    def _box_func(self, x) -> Period | NaTType:\n        return Period._from_ordinal(ordinal=x, freq=self.freq)", "class_fn": true, "question_id": "pandas/pandas.core.arrays.period/PeriodArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/sparse/array.py", "fn_id": "", "content": "class SparseArray(OpsMixin, PandasObject, ExtensionArray):\n    \"\"\"\n    An ExtensionArray for storing sparse data.\n\n    Parameters\n    ----------\n    data : array-like or scalar\n        A dense array of values to store in the SparseArray. This may contain\n        `fill_value`.\n    sparse_index : SparseIndex, optional\n    fill_value : scalar, optional\n        Elements in data that are ``fill_value`` are not stored in the\n        SparseArray. For memory savings, this should be the most common value\n        in `data`. By default, `fill_value` depends on the dtype of `data`:\n\n        =========== ==========\n        data.dtype  na_value\n        =========== ==========\n        float       ``np.nan``\n        int         ``0``\n        bool        False\n        datetime64  ``pd.NaT``\n        timedelta64 ``pd.NaT``\n        =========== ==========\n\n        The fill value is potentially specified in three ways. In order of\n        precedence, these are\n\n        1. The `fill_value` argument\n        2. ``dtype.fill_value`` if `fill_value` is None and `dtype` is\n           a ``SparseDtype``\n        3. ``data.dtype.fill_value`` if `fill_value` is None and `dtype`\n           is not a ``SparseDtype`` and `data` is a ``SparseArray``.\n\n    kind : str\n        Can be 'integer' or 'block', default is 'integer'.\n        The type of storage for sparse locations.\n\n        * 'block': Stores a `block` and `block_length` for each\n          contiguous *span* of sparse values. This is best when\n          sparse data tends to be clumped together, with large\n          regions of ``fill-value`` values between sparse values.\n        * 'integer': uses an integer to store the location of\n          each sparse value.\n\n    dtype : np.dtype or SparseDtype, optional\n        The dtype to use for the SparseArray. For numpy dtypes, this\n        determines the dtype of ``self.sp_values``. For SparseDtype,\n        this determines ``self.sp_values`` and ``self.fill_value``.\n    copy : bool, default False\n        Whether to explicitly copy the incoming `data` array.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Examples\n    --------\n    >>> from pandas.arrays import SparseArray\n    >>> arr = SparseArray([0, 0, 1, 2])\n    >>> arr\n    [0, 0, 1, 2]\n    Fill: 0\n    IntIndex\n    Indices: array([2, 3], dtype=int32)\n    \"\"\"\n    _subtyp = 'sparse_array'\n    _hidden_attrs = PandasObject._hidden_attrs | frozenset([])\n    _sparse_index: SparseIndex\n    _sparse_values: np.ndarray\n    _dtype: SparseDtype\n\n    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\n        if fill_value is None:\n            fill_value = self.dtype.na_value\n        if indices.min() < -1:\n            raise ValueError(\"Invalid value in 'indices'. Must be between -1 and the length of the array.\")\n        if indices.max() >= len(self):\n            raise IndexError(\"out of bounds value in 'indices'.\")\n        if len(self) == 0:\n            if (indices == -1).all():\n                dtype = np.result_type(self.sp_values, type(fill_value))\n                taken = np.empty_like(indices, dtype=dtype)\n                taken.fill(fill_value)\n                return taken\n            else:\n                raise IndexError('cannot do a non-empty take from an empty axes.')\n        sp_indexer = self.sp_index.lookup_array(indices)\n        new_fill_indices = indices == -1\n        old_fill_indices = (sp_indexer == -1) & ~new_fill_indices\n        if self.sp_index.npoints == 0 and old_fill_indices.all():\n            taken = np.full(sp_indexer.shape, fill_value=self.fill_value, dtype=self.dtype.subtype)\n        elif self.sp_index.npoints == 0:\n            _dtype = np.result_type(self.dtype.subtype, type(fill_value))\n            taken = np.full(sp_indexer.shape, fill_value=fill_value, dtype=_dtype)\n            taken[old_fill_indices] = self.fill_value\n        else:\n            taken = self.sp_values.take(sp_indexer)\n            m0 = sp_indexer[old_fill_indices] < 0\n            m1 = sp_indexer[new_fill_indices] < 0\n            result_type = taken.dtype\n            if m0.any():\n                result_type = np.result_type(result_type, type(self.fill_value))\n                taken = taken.astype(result_type)\n                taken[old_fill_indices] = self.fill_value\n            if m1.any():\n                result_type = np.result_type(result_type, type(fill_value))\n                taken = taken.astype(result_type)\n                taken[new_fill_indices] = fill_value\n        return taken\n\n    def _formatter(self, boxed: bool=False):\n        return None\n\n    @doc(ExtensionArray.duplicated)\n    def duplicated(self, keep: Literal['first', 'last', False]='first') -> npt.NDArray[np.bool_]:\n        values = np.asarray(self)\n        mask = np.asarray(self.isna())\n        return algos.duplicated(values, keep=keep, mask=mask)\n\n    def _values_for_factorize(self):\n        return (np.asarray(self), self.fill_value)\n\n    def transpose(self, *axes: int) -> ExtensionArray:\n        \"\"\"\n        Return a transposed view on this array.\n\n        Because ExtensionArrays are always 1D, this is a no-op.  It is included\n        for compatibility with np.ndarray.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Examples\n        --------\n        >>> pd.array([1, 2, 3]).transpose()\n        <IntegerArray>\n        [1, 2, 3]\n        Length: 3, dtype: Int64\n        \"\"\"\n        return self[:]\n\n    def factorize(self, use_na_sentinel: bool=True) -> tuple[np.ndarray, SparseArray]:\n        (codes, uniques) = algos.factorize(np.asarray(self), use_na_sentinel=use_na_sentinel)\n        uniques_sp = SparseArray(uniques, dtype=self.dtype)\n        return (codes, uniques_sp)\n\n    def _get_val_at(self, loc):\n        loc = validate_insert_loc(loc, len(self))\n        sp_loc = self.sp_index.lookup(loc)\n        if sp_loc == -1:\n            return self.fill_value\n        else:\n            val = self.sp_values[sp_loc]\n            val = maybe_box_datetimelike(val, self.sp_values.dtype)\n            return val\n\n    @property\n    def sp_index(self) -> SparseIndex:\n        \"\"\"\n        The SparseIndex containing the location of non- ``fill_value`` points.\n        \"\"\"\n        return self._sparse_index\n\n    @property\n    def sp_values(self) -> np.ndarray:\n        \"\"\"\n        An ndarray containing the non- ``fill_value`` values.\n\n        Examples\n        --------\n        >>> from pandas.arrays import SparseArray\n        >>> s = SparseArray([0, 0, 1, 0, 2], fill_value=0)\n        >>> s.sp_values\n        array([1, 2])\n        \"\"\"\n        return self._sparse_values\n\n    @property\n    def dtype(self) -> SparseDtype:\n        return self._dtype\n\n    @property\n    def fill_value(self):\n        \"\"\"\n        Elements in `data` that are `fill_value` are not stored.\n\n        For memory savings, this should be the most common value in the array.\n\n        Examples\n        --------\n        >>> ser = pd.Series([0, 0, 2, 2, 2], dtype=\"Sparse[int]\")\n        >>> ser.sparse.fill_value\n        0\n        >>> spa_dtype = pd.SparseDtype(dtype=np.int32, fill_value=2)\n        >>> ser = pd.Series([0, 0, 2, 2, 2], dtype=spa_dtype)\n        >>> ser.sparse.fill_value\n        2\n        \"\"\"\n        return self.dtype.fill_value\n\n    @fill_value.setter\n    def fill_value(self, value) -> None:\n        self._dtype = SparseDtype(self.dtype.subtype, value)\n\n    @property\n    def kind(self) -> SparseIndexKind:\n        \"\"\"\n        The kind of sparse index for this array. One of {'integer', 'block'}.\n        \"\"\"\n        if isinstance(self.sp_index, IntIndex):\n            return 'integer'\n        else:\n            return 'block'\n\n    @property\n    def _valid_sp_values(self) -> np.ndarray:\n        sp_vals = self.sp_values\n        mask = notna(sp_vals)\n        return sp_vals[mask]\n\n    def _get_repr_footer(self) -> str:\n        if self.ndim > 1:\n            return f'Shape: {self.shape}, dtype: {self.dtype}'\n        return f'Length: {len(self)}, dtype: {self.dtype}'\n\n    @property\n    def _null_fill_value(self) -> bool:\n        return self._dtype._is_na_fill_value\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n        out = kwargs.get('out', ())\n        for x in inputs + out:\n            if not isinstance(x, self._HANDLED_TYPES + (SparseArray,)):\n                return NotImplemented\n        result = arraylike.maybe_dispatch_ufunc_to_dunder_op(self, ufunc, method, *inputs, **kwargs)\n        if result is not NotImplemented:\n            return result\n        if 'out' in kwargs:\n            res = arraylike.dispatch_ufunc_with_out(self, ufunc, method, *inputs, **kwargs)\n            return res\n        if method == 'reduce':\n            result = arraylike.dispatch_reduction_ufunc(self, ufunc, method, *inputs, **kwargs)\n            if result is not NotImplemented:\n                return result\n        if len(inputs) == 1:\n            sp_values = getattr(ufunc, method)(self.sp_values, **kwargs)\n            fill_value = getattr(ufunc, method)(self.fill_value, **kwargs)\n            if ufunc.nout > 1:\n                arrays = tuple((self._simple_new(sp_value, self.sp_index, SparseDtype(sp_value.dtype, fv)) for (sp_value, fv) in zip(sp_values, fill_value)))\n                return arrays\n            elif method == 'reduce':\n                return sp_values\n            return self._simple_new(sp_values, self.sp_index, SparseDtype(sp_values.dtype, fill_value))\n        new_inputs = tuple((np.asarray(x) for x in inputs))\n        result = getattr(ufunc, method)(*new_inputs, **kwargs)\n        if out:\n            if len(out) == 1:\n                out = out[0]\n            return out\n        if ufunc.nout > 1:\n            return tuple((type(self)(x) for x in result))\n        elif method == 'at':\n            return None\n        else:\n            return type(self)(result)\n\n    @property\n    def nbytes(self) -> int:\n        return self.sp_values.nbytes + self.sp_index.nbytes\n\n    @property\n    def density(self) -> float:\n        \"\"\"\n        The percent of non- ``fill_value`` points, as decimal.\n\n        Examples\n        --------\n        >>> from pandas.arrays import SparseArray\n        >>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\n        >>> s.density\n        0.6\n        \"\"\"\n        return self.sp_index.npoints / self.sp_index.length\n\n    @property\n    def npoints(self) -> int:\n        \"\"\"\n        The number of non- ``fill_value`` points.\n\n        Examples\n        --------\n        >>> from pandas.arrays import SparseArray\n        >>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\n        >>> s.npoints\n        3\n        \"\"\"\n        return self.sp_index.npoints\n\n    def any(self, axis: AxisInt=0, *args, **kwargs) -> bool:\n        \"\"\"\n        Tests whether at least one of elements evaluate True\n\n        Returns\n        -------\n        any : bool\n\n        See Also\n        --------\n        numpy.any\n        \"\"\"\n        nv.validate_any(args, kwargs)\n        values = self.sp_values\n        if len(values) != len(self) and np.any(self.fill_value):\n            return True\n        return values.any().item()\n\n    def isna(self) -> Self:\n        dtype = SparseDtype(bool, self._null_fill_value)\n        if self._null_fill_value:\n            return type(self)._simple_new(isna(self.sp_values), self.sp_index, dtype)\n        mask = np.full(len(self), False, dtype=np.bool_)\n        mask[self.sp_index.indices] = isna(self.sp_values)\n        return type(self)(mask, fill_value=False, dtype=dtype)\n\n    def _repr_2d(self) -> str:\n        from pandas.io.formats.printing import format_object_summary\n        lines = [format_object_summary(x, self._formatter(), indent_for_name=False).rstrip(', \\n') for x in self]\n        data = ',\\n'.join(lines)\n        class_name = f'<{type(self).__name__}>'\n        footer = self._get_repr_footer()\n        return f'{class_name}\\n[\\n{data}\\n]\\n{footer}'\n\n    def _pad_or_backfill(self, *, method: FillnaOptions, limit: int | None=None, limit_area: Literal['inside', 'outside'] | None=None, copy: bool=True) -> Self:\n        return super()._pad_or_backfill(method=method, limit=limit, limit_area=limit_area, copy=copy)\n\n    def __pos__(self) -> SparseArray:\n        return self._unary_method(operator.pos)\n\n    def argmin(self, skipna: bool=True) -> int:\n        validate_bool_kwarg(skipna, 'skipna')\n        if not skipna and self._hasna:\n            raise NotImplementedError\n        return self._argmin_argmax('argmin')\n\n    def map(self, mapper, na_action=None) -> Self:\n        \"\"\"\n        Map categories using an input mapping or function.\n\n        Parameters\n        ----------\n        mapper : dict, Series, callable\n            The correspondence from old values to new.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        SparseArray\n            The output array will have the same density as the input.\n            The output fill value will be the result of applying the\n            mapping to ``self.fill_value``\n\n        Examples\n        --------\n        >>> arr = pd.arrays.SparseArray([0, 1, 2])\n        >>> arr.map(lambda x: x + 10)\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)\n\n        >>> arr.map({0: 10, 1: 11, 2: 12})\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)\n\n        >>> arr.map(pd.Series([10, 11, 12], index=[0, 1, 2]))\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)\n        \"\"\"\n        is_map = isinstance(mapper, (abc.Mapping, ABCSeries))\n        fill_val = self.fill_value\n        if na_action is None or notna(fill_val):\n            fill_val = mapper.get(fill_val, fill_val) if is_map else mapper(fill_val)\n\n        def func(sp_val):\n            new_sp_val = mapper.get(sp_val, None) if is_map else mapper(sp_val)\n            if new_sp_val is fill_val or new_sp_val == fill_val:\n                msg = 'fill value in the sparse values not supported'\n                raise ValueError(msg)\n            return new_sp_val\n        sp_values = [func(x) for x in self.sp_values]\n        return type(self)(sp_values, sparse_index=self.sp_index, fill_value=fill_val)\n\n    def __len__(self) -> int:\n        return self.sp_index.length\n\n    def __setstate__(self, state) -> None:\n        \"\"\"Necessary for making this object picklable\"\"\"\n        if isinstance(state, tuple):\n            (nd_state, (fill_value, sp_index)) = state\n            sparse_values = np.array([])\n            sparse_values.__setstate__(nd_state)\n            self._sparse_values = sparse_values\n            self._sparse_index = sp_index\n            self._dtype = SparseDtype(sparse_values.dtype, fill_value)\n        else:\n            self.__dict__.update(state)\n\n    def take(self, indices, *, allow_fill: bool=False, fill_value=None) -> Self:\n        if is_scalar(indices):\n            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\n        indices = np.asarray(indices, dtype=np.int32)\n        dtype = None\n        if indices.size == 0:\n            result = np.array([], dtype='object')\n            dtype = self.dtype\n        elif allow_fill:\n            result = self._take_with_fill(indices, fill_value=fill_value)\n        else:\n            return self._take_without_fill(indices)\n        return type(self)(result, fill_value=self.fill_value, kind=self.kind, dtype=dtype)\n\n    @overload\n    def __getitem__(self, key: ScalarIndexer) -> Any:\n        ...\n\n    @overload\n    def __getitem__(self, key: SequenceIndexer | tuple[int | ellipsis, ...]) -> Self:\n        ...\n\n    def _where(self, mask, value):\n        naive_implementation = np.where(mask, self, value)\n        dtype = SparseDtype(naive_implementation.dtype, fill_value=self.fill_value)\n        result = type(self)._from_sequence(naive_implementation, dtype=dtype)\n        return result\n\n    def value_counts(self, dropna: bool=True) -> Series:\n        \"\"\"\n        Returns a Series containing counts of unique values.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include counts of NaN, even if NaN is in sp_values.\n\n        Returns\n        -------\n        counts : Series\n        \"\"\"\n        from pandas import Index, Series\n        (keys, counts, _) = algos.value_counts_arraylike(self.sp_values, dropna=dropna)\n        fcounts = self.sp_index.ngaps\n        if fcounts > 0 and (not self._null_fill_value or not dropna):\n            mask = isna(keys) if self._null_fill_value else keys == self.fill_value\n            if mask.any():\n                counts[mask] += fcounts\n            else:\n                keys = np.insert(keys, 0, self.fill_value)\n                counts = np.insert(counts, 0, fcounts)\n        if not isinstance(keys, ABCIndex):\n            index = Index(keys)\n        else:\n            index = keys\n        return Series(counts, index=index, copy=False)\n\n    def mean(self, axis: Axis=0, *args, **kwargs):\n        \"\"\"\n        Mean of non-NA/null values\n\n        Returns\n        -------\n        mean : float\n        \"\"\"\n        nv.validate_mean(args, kwargs)\n        valid_vals = self._valid_sp_values\n        sp_sum = valid_vals.sum()\n        ct = len(valid_vals)\n        if self._null_fill_value:\n            return sp_sum / ct\n        else:\n            nsparse = self.sp_index.ngaps\n            return (sp_sum + self.fill_value * nsparse) / (ct + nsparse)\n\n    def _reduce(self, name: str, *, skipna: bool=True, keepdims: bool=False, **kwargs):\n        method = getattr(self, name, None)\n        if method is None:\n            raise TypeError(f'cannot perform {name} with type {self.dtype}')\n        if skipna:\n            arr = self\n        else:\n            arr = self.dropna()\n        result = getattr(arr, name)(**kwargs)\n        if keepdims:\n            return type(self)([result], dtype=self.dtype)\n        else:\n            return result\n\n    def __repr__(self) -> str:\n        pp_str = printing.pprint_thing(self)\n        pp_fill = printing.pprint_thing(self.fill_value)\n        pp_index = printing.pprint_thing(self.sp_index)\n        return f'{pp_str}\\nFill: {pp_fill}\\n{pp_index}'\n\n    def __invert__(self) -> SparseArray:\n        return self._unary_method(operator.invert)\n\n    @classmethod\n    def from_spmatrix(cls, data: spmatrix) -> Self:\n        \"\"\"\n        Create a SparseArray from a scipy.sparse matrix.\n\n        Parameters\n        ----------\n        data : scipy.sparse.sp_matrix\n            This should be a SciPy sparse matrix where the size\n            of the second dimension is 1. In other words, a\n            sparse matrix with a single column.\n\n        Returns\n        -------\n        SparseArray\n\n        Examples\n        --------\n        >>> import scipy.sparse\n        >>> mat = scipy.sparse.coo_matrix((4, 1))\n        >>> pd.arrays.SparseArray.from_spmatrix(mat)\n        [0.0, 0.0, 0.0, 0.0]\n        Fill: 0.0\n        IntIndex\n        Indices: array([], dtype=int32)\n        \"\"\"\n        (length, ncol) = data.shape\n        if ncol != 1:\n            raise ValueError(f\"'data' must have a single column, not '{ncol}'\")\n        data = data.tocsc()\n        data.sort_indices()\n        arr = data.data\n        idx = data.indices\n        zero = np.array(0, dtype=arr.dtype).item()\n        dtype = SparseDtype(arr.dtype, zero)\n        index = IntIndex(length, idx)\n        return cls._simple_new(arr, index, dtype)\n\n    def _first_fill_value_loc(self):\n        \"\"\"\n        Get the location of the first fill value.\n\n        Returns\n        -------\n        int\n        \"\"\"\n        if len(self) == 0 or self.sp_index.npoints == len(self):\n            return -1\n        indices = self.sp_index.indices\n        if not len(indices) or indices[0] > 0:\n            return 0\n        diff = np.r_[np.diff(indices), 2]\n        return indices[(diff > 1).argmax()] + 1\n\n    def __init__(self, data, sparse_index=None, fill_value=None, kind: SparseIndexKind='integer', dtype: Dtype | None=None, copy: bool=False) -> None:\n        if fill_value is None and isinstance(dtype, SparseDtype):\n            fill_value = dtype.fill_value\n        if isinstance(data, type(self)):\n            if sparse_index is None:\n                sparse_index = data.sp_index\n            if fill_value is None:\n                fill_value = data.fill_value\n            if dtype is None:\n                dtype = data.dtype\n            data = data.sp_values\n        if isinstance(dtype, str):\n            try:\n                dtype = SparseDtype.construct_from_string(dtype)\n            except TypeError:\n                dtype = pandas_dtype(dtype)\n        if isinstance(dtype, SparseDtype):\n            if fill_value is None:\n                fill_value = dtype.fill_value\n            dtype = dtype.subtype\n        if is_scalar(data):\n            warnings.warn(f'Constructing {type(self).__name__} with scalar data is deprecated and will raise in a future version. Pass a sequence instead.', FutureWarning, stacklevel=find_stack_level())\n            if sparse_index is None:\n                npoints = 1\n            else:\n                npoints = sparse_index.length\n            data = construct_1d_arraylike_from_scalar(data, npoints, dtype=None)\n            dtype = data.dtype\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n        if data is None:\n            data = np.array([], dtype=dtype)\n        try:\n            data = sanitize_array(data, index=None)\n        except ValueError:\n            if dtype is None:\n                dtype = np.dtype(object)\n                data = np.atleast_1d(np.asarray(data, dtype=dtype))\n            else:\n                raise\n        if copy:\n            data = data.copy()\n        if fill_value is None:\n            fill_value_dtype = data.dtype if dtype is None else dtype\n            if fill_value_dtype is None:\n                fill_value = np.nan\n            else:\n                fill_value = na_value_for_dtype(fill_value_dtype)\n        if isinstance(data, type(self)) and sparse_index is None:\n            sparse_index = data._sparse_index\n            sparse_values = np.asarray(data.sp_values, dtype=dtype)\n        elif sparse_index is None:\n            data = extract_array(data, extract_numpy=True)\n            if not isinstance(data, np.ndarray):\n                if isinstance(data.dtype, DatetimeTZDtype):\n                    warnings.warn(f'Creating SparseArray from {data.dtype} data loses timezone information. Cast to object before sparse to retain timezone information.', UserWarning, stacklevel=find_stack_level())\n                    data = np.asarray(data, dtype='datetime64[ns]')\n                    if fill_value is NaT:\n                        fill_value = np.datetime64('NaT', 'ns')\n                data = np.asarray(data)\n            (sparse_values, sparse_index, fill_value) = _make_sparse(data, kind=kind, fill_value=fill_value, dtype=dtype)\n        else:\n            sparse_values = np.asarray(data, dtype=dtype)\n            if len(sparse_values) != sparse_index.npoints:\n                raise AssertionError(f'Non array-like type {type(sparse_values)} must have the same length as the index')\n        self._sparse_index = sparse_index\n        self._sparse_values = sparse_values\n        self._dtype = SparseDtype(sparse_values.dtype, fill_value)\n\n    def argmax(self, skipna: bool=True) -> int:\n        validate_bool_kwarg(skipna, 'skipna')\n        if not skipna and self._hasna:\n            raise NotImplementedError\n        return self._argmin_argmax('argmax')\n\n    def fillna(self, value=None, method: FillnaOptions | None=None, limit: int | None=None, copy: bool=True) -> Self:\n        \"\"\"\n        Fill missing values with `value`.\n\n        Parameters\n        ----------\n        value : scalar, optional\n        method : str, optional\n\n            .. warning::\n\n               Using 'method' will result in high memory use,\n               as all `fill_value` methods will be converted to\n               an in-memory ndarray\n\n        limit : int, optional\n\n        copy: bool, default True\n            Ignored for SparseArray.\n\n        Returns\n        -------\n        SparseArray\n\n        Notes\n        -----\n        When `value` is specified, the result's ``fill_value`` depends on\n        ``self.fill_value``. The goal is to maintain low-memory use.\n\n        If ``self.fill_value`` is NA, the result dtype will be\n        ``SparseDtype(self.dtype, fill_value=value)``. This will preserve\n        amount of memory used before and after filling.\n\n        When ``self.fill_value`` is not NA, the result dtype will be\n        ``self.dtype``. Again, this preserves the amount of memory used.\n        \"\"\"\n        if method is None and value is None or (method is not None and value is not None):\n            raise ValueError(\"Must specify one of 'method' or 'value'.\")\n        if method is not None:\n            return super().fillna(method=method, limit=limit)\n        else:\n            new_values = np.where(isna(self.sp_values), value, self.sp_values)\n            if self._null_fill_value:\n                new_dtype = SparseDtype(self.dtype.subtype, fill_value=value)\n            else:\n                new_dtype = self.dtype\n        return self._simple_new(new_values, self._sparse_index, new_dtype)\n\n    def max(self, *, axis: AxisInt | None=None, skipna: bool=True):\n        \"\"\"\n        Max of array values, ignoring NA values if specified.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Not Used. NumPy compatibility.\n        skipna : bool, default True\n            Whether to ignore NA values.\n\n        Returns\n        -------\n        scalar\n        \"\"\"\n        nv.validate_minmax_axis(axis, self.ndim)\n        return self._min_max('max', skipna=skipna)\n\n    def to_dense(self) -> np.ndarray:\n        \"\"\"\n        Convert SparseArray to a NumPy array.\n\n        Returns\n        -------\n        arr : NumPy array\n        \"\"\"\n        return np.asarray(self, dtype=self.sp_values.dtype)\n\n    def __getitem__(self, key: PositionalIndexer | tuple[int | ellipsis, ...]) -> Self | Any:\n        if isinstance(key, tuple):\n            key = unpack_tuple_and_ellipses(key)\n            if key is Ellipsis:\n                raise ValueError('Cannot slice with Ellipsis')\n        if is_integer(key):\n            return self._get_val_at(key)\n        elif isinstance(key, tuple):\n            data_slice = self.to_dense()[key]\n        elif isinstance(key, slice):\n            if key.step is None or key.step == 1:\n                start = 0 if key.start is None else key.start\n                if start < 0:\n                    start += len(self)\n                end = len(self) if key.stop is None else key.stop\n                if end < 0:\n                    end += len(self)\n                indices = self.sp_index.indices\n                keep_inds = np.flatnonzero((indices >= start) & (indices < end))\n                sp_vals = self.sp_values[keep_inds]\n                sp_index = indices[keep_inds].copy()\n                if start > 0:\n                    sp_index -= start\n                new_len = len(range(len(self))[key])\n                new_sp_index = make_sparse_index(new_len, sp_index, self.kind)\n                return type(self)._simple_new(sp_vals, new_sp_index, self.dtype)\n            else:\n                indices = np.arange(len(self), dtype=np.int32)[key]\n                return self.take(indices)\n        elif not is_list_like(key):\n            raise IndexError('only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices')\n        else:\n            if isinstance(key, SparseArray):\n                if is_bool_dtype(key):\n                    if isna(key.fill_value):\n                        return self.take(key.sp_index.indices[key.sp_values])\n                    if not key.fill_value:\n                        return self.take(key.sp_index.indices)\n                    n = len(self)\n                    mask = np.full(n, True, dtype=np.bool_)\n                    mask[key.sp_index.indices] = False\n                    return self.take(np.arange(n)[mask])\n                else:\n                    key = np.asarray(key)\n            key = check_array_indexer(self, key)\n            if com.is_bool_indexer(key):\n                key = cast(np.ndarray, key)\n                return self.take(np.arange(len(key), dtype=np.int32)[key])\n            elif hasattr(key, '__len__'):\n                return self.take(key)\n            else:\n                raise ValueError(f\"Cannot slice with '{key}'\")\n        return type(self)(data_slice, kind=self.kind)\n\n    def _cmp_method(self, other, op) -> SparseArray:\n        if not is_scalar(other) and (not isinstance(other, type(self))):\n            other = np.asarray(other)\n        if isinstance(other, np.ndarray):\n            other = SparseArray(other, fill_value=self.fill_value)\n        if isinstance(other, SparseArray):\n            if len(self) != len(other):\n                raise ValueError(f'operands have mismatched length {len(self)} and {len(other)}')\n            op_name = op.__name__.strip('_')\n            return _sparse_array_op(self, other, op, op_name)\n        else:\n            fill_value = op(self.fill_value, other)\n            result = np.full(len(self), fill_value, dtype=np.bool_)\n            result[self.sp_index.indices] = op(self.sp_values, other)\n            return type(self)(result, fill_value=fill_value, dtype=np.bool_)\n\n    def _min_max(self, kind: Literal['min', 'max'], skipna: bool) -> Scalar:\n        \"\"\"\n        Min/max of non-NA/null values\n\n        Parameters\n        ----------\n        kind : {\"min\", \"max\"}\n        skipna : bool\n\n        Returns\n        -------\n        scalar\n        \"\"\"\n        valid_vals = self._valid_sp_values\n        has_nonnull_fill_vals = not self._null_fill_value and self.sp_index.ngaps > 0\n        if len(valid_vals) > 0:\n            sp_min_max = getattr(valid_vals, kind)()\n            if has_nonnull_fill_vals:\n                func = max if kind == 'max' else min\n                return func(sp_min_max, self.fill_value)\n            elif skipna:\n                return sp_min_max\n            elif self.sp_index.ngaps == 0:\n                return sp_min_max\n            else:\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\n        elif has_nonnull_fill_vals:\n            return self.fill_value\n        else:\n            return na_value_for_dtype(self.dtype.subtype, compat=False)\n\n    def nonzero(self) -> tuple[npt.NDArray[np.int32]]:\n        if self.fill_value == 0:\n            return (self.sp_index.indices,)\n        else:\n            return (self.sp_index.indices[self.sp_values != 0],)\n\n    def astype(self, dtype: AstypeArg | None=None, copy: bool=True):\n        \"\"\"\n        Change the dtype of a SparseArray.\n\n        The output will always be a SparseArray. To convert to a dense\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : np.dtype or ExtensionDtype\n            For SparseDtype, this changes the dtype of\n            ``self.sp_values`` and the ``self.fill_value``.\n\n            For other dtypes, this only changes the dtype of\n            ``self.sp_values``.\n\n        copy : bool, default True\n            Whether to ensure a copy is made, even if not necessary.\n\n        Returns\n        -------\n        SparseArray\n\n        Examples\n        --------\n        >>> arr = pd.arrays.SparseArray([0, 0, 1, 2])\n        >>> arr\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        >>> arr.astype(SparseDtype(np.dtype('int32')))\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\n        just ``self.sp_values``.\n\n        >>> arr.astype(SparseDtype(np.dtype('float64')))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [nan, nan, 1.0, 2.0]\n        Fill: nan\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Using a SparseDtype, you can also change the fill value as well.\n\n        >>> arr.astype(SparseDtype(\"float64\", fill_value=0.0))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [0.0, 0.0, 1.0, 2.0]\n        Fill: 0.0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n        \"\"\"\n        if dtype == self._dtype:\n            if not copy:\n                return self\n            else:\n                return self.copy()\n        future_dtype = pandas_dtype(dtype)\n        if not isinstance(future_dtype, SparseDtype):\n            values = np.asarray(self)\n            values = ensure_wrapped_if_datetimelike(values)\n            return astype_array(values, dtype=future_dtype, copy=False)\n        dtype = self.dtype.update_dtype(dtype)\n        subtype = pandas_dtype(dtype._subtype_with_str)\n        subtype = cast(np.dtype, subtype)\n        values = ensure_wrapped_if_datetimelike(self.sp_values)\n        sp_values = astype_array(values, subtype, copy=copy)\n        sp_values = np.asarray(sp_values)\n        return self._simple_new(sp_values, self.sp_index, dtype)\n\n    def __array__(self, dtype: NpDtype | None=None, copy: bool | None=None) -> np.ndarray:\n        fill_value = self.fill_value\n        if self.sp_index.ngaps == 0:\n            return self.sp_values\n        if dtype is None:\n            if self.sp_values.dtype.kind == 'M':\n                if fill_value is NaT:\n                    fill_value = np.datetime64('NaT')\n            try:\n                dtype = np.result_type(self.sp_values.dtype, type(fill_value))\n            except TypeError:\n                dtype = object\n        out = np.full(self.shape, fill_value, dtype=dtype)\n        out[self.sp_index.indices] = self.sp_values\n        return out\n\n    def cumsum(self, axis: AxisInt=0, *args, **kwargs) -> SparseArray:\n        \"\"\"\n        Cumulative sum of non-NA/null values.\n\n        When performing the cumulative summation, any non-NA/null values will\n        be skipped. The resulting SparseArray will preserve the locations of\n        NaN values, but the fill value will be `np.nan` regardless.\n\n        Parameters\n        ----------\n        axis : int or None\n            Axis over which to perform the cumulative summation. If None,\n            perform cumulative summation over flattened array.\n\n        Returns\n        -------\n        cumsum : SparseArray\n        \"\"\"\n        nv.validate_cumsum(args, kwargs)\n        if axis is not None and axis >= self.ndim:\n            raise ValueError(f'axis(={axis}) out of bounds')\n        if not self._null_fill_value:\n            return SparseArray(self.to_dense()).cumsum()\n        return SparseArray(self.sp_values.cumsum(), sparse_index=self.sp_index, fill_value=self.fill_value)\n\n    def copy(self) -> Self:\n        values = self.sp_values.copy()\n        return self._simple_new(values, self.sp_index, self.dtype)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat: Sequence[Self]) -> Self:\n        fill_value = to_concat[0].fill_value\n        values = []\n        length = 0\n        if to_concat:\n            sp_kind = to_concat[0].kind\n        else:\n            sp_kind = 'integer'\n        sp_index: SparseIndex\n        if sp_kind == 'integer':\n            indices = []\n            for arr in to_concat:\n                int_idx = arr.sp_index.indices.copy()\n                int_idx += length\n                length += arr.sp_index.length\n                values.append(arr.sp_values)\n                indices.append(int_idx)\n            data = np.concatenate(values)\n            indices_arr = np.concatenate(indices)\n            sp_index = IntIndex(length, indices_arr)\n        else:\n            blengths = []\n            blocs = []\n            for arr in to_concat:\n                block_idx = arr.sp_index.to_block_index()\n                values.append(arr.sp_values)\n                blocs.append(block_idx.blocs.copy() + length)\n                blengths.append(block_idx.blengths)\n                length += arr.sp_index.length\n            data = np.concatenate(values)\n            blocs_arr = np.concatenate(blocs)\n            blengths_arr = np.concatenate(blengths)\n            sp_index = BlockIndex(length, blocs_arr, blengths_arr)\n        return cls(data, sparse_index=sp_index, fill_value=fill_value)\n\n    def all(self, axis=None, *args, **kwargs):\n        \"\"\"\n        Tests whether all elements evaluate True\n\n        Returns\n        -------\n        all : bool\n\n        See Also\n        --------\n        numpy.all\n        \"\"\"\n        nv.validate_all(args, kwargs)\n        values = self.sp_values\n        if len(values) != len(self) and (not np.all(self.fill_value)):\n            return False\n        return values.all()\n\n    def _fill_value_matches(self, fill_value) -> bool:\n        if self._null_fill_value:\n            return isna(fill_value)\n        else:\n            return self.fill_value == fill_value\n\n    def _argmin_argmax(self, kind: Literal['argmin', 'argmax']) -> int:\n        values = self._sparse_values\n        index = self._sparse_index.indices\n        mask = np.asarray(isna(values))\n        func = np.argmax if kind == 'argmax' else np.argmin\n        idx = np.arange(values.shape[0])\n        non_nans = values[~mask]\n        non_nan_idx = idx[~mask]\n        _candidate = non_nan_idx[func(non_nans)]\n        candidate = index[_candidate]\n        if isna(self.fill_value):\n            return candidate\n        if kind == 'argmin' and self[candidate] < self.fill_value:\n            return candidate\n        if kind == 'argmax' and self[candidate] > self.fill_value:\n            return candidate\n        _loc = self._first_fill_value_loc()\n        if _loc == -1:\n            return candidate\n        else:\n            return _loc\n\n    def __setitem__(self, key, value) -> None:\n        msg = 'SparseArray does not support item assignment via setitem'\n        raise TypeError(msg)\n    _HANDLED_TYPES = (np.ndarray, numbers.Number)\n\n    def searchsorted(self, v: ArrayLike | object, side: Literal['left', 'right']='left', sorter: NumpySorter | None=None) -> npt.NDArray[np.intp] | np.intp:\n        msg = 'searchsorted requires high memory usage.'\n        warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())\n        v = np.asarray(v)\n        return np.asarray(self, dtype=self.dtype.subtype).searchsorted(v, side, sorter)\n\n    def _unary_method(self, op) -> SparseArray:\n        fill_value = op(np.array(self.fill_value)).item()\n        dtype = SparseDtype(self.dtype.subtype, fill_value)\n        if isna(self.fill_value) or fill_value == self.fill_value:\n            values = op(self.sp_values)\n            return type(self)._simple_new(values, self.sp_index, self.dtype)\n        return type(self)(op(self.to_dense()), dtype=dtype)\n\n    def min(self, *, axis: AxisInt | None=None, skipna: bool=True):\n        \"\"\"\n        Min of array values, ignoring NA values if specified.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Not Used. NumPy compatibility.\n        skipna : bool, default True\n            Whether to ignore NA values.\n\n        Returns\n        -------\n        scalar\n        \"\"\"\n        nv.validate_minmax_axis(axis, self.ndim)\n        return self._min_max('min', skipna=skipna)\n    _logical_method = _cmp_method\n\n    @classmethod\n    def _simple_new(cls, sparse_array: np.ndarray, sparse_index: SparseIndex, dtype: SparseDtype) -> Self:\n        new = object.__new__(cls)\n        new._sparse_index = sparse_index\n        new._sparse_values = sparse_array\n        new._dtype = dtype\n        return new\n\n    def shift(self, periods: int=1, fill_value=None) -> Self:\n        if not len(self) or periods == 0:\n            return self.copy()\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n        subtype = np.result_type(fill_value, self.dtype.subtype)\n        if subtype != self.dtype.subtype:\n            arr = self.astype(SparseDtype(subtype, self.fill_value))\n        else:\n            arr = self\n        empty = self._from_sequence([fill_value] * min(abs(periods), len(self)), dtype=arr.dtype)\n        if periods > 0:\n            a = empty\n            b = arr[:-periods]\n        else:\n            a = arr[abs(periods):]\n            b = empty\n        return arr._concat_same_type([a, b])\n\n    def __neg__(self) -> SparseArray:\n        return self._unary_method(operator.neg)\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype: Dtype | None=None, copy: bool=False):\n        return cls(scalars, dtype=dtype)\n\n    def _arith_method(self, other, op):\n        op_name = op.__name__\n        if isinstance(other, SparseArray):\n            return _sparse_array_op(self, other, op, op_name)\n        elif is_scalar(other):\n            with np.errstate(all='ignore'):\n                fill = op(_get_fill(self), np.asarray(other))\n                result = op(self.sp_values, other)\n            if op_name == 'divmod':\n                (left, right) = result\n                (lfill, rfill) = fill\n                return (_wrap_result(op_name, left, self.sp_index, lfill), _wrap_result(op_name, right, self.sp_index, rfill))\n            return _wrap_result(op_name, result, self.sp_index, fill)\n        else:\n            other = np.asarray(other)\n            with np.errstate(all='ignore'):\n                if len(self) != len(other):\n                    raise AssertionError(f'length mismatch: {len(self)} vs. {len(other)}')\n                if not isinstance(other, SparseArray):\n                    dtype = getattr(other, 'dtype', None)\n                    other = SparseArray(other, fill_value=self.fill_value, dtype=dtype)\n                return _sparse_array_op(self, other, op, op_name)\n\n    def _take_without_fill(self, indices) -> Self:\n        to_shift = indices < 0\n        n = len(self)\n        if indices.max() >= n or indices.min() < -n:\n            if n == 0:\n                raise IndexError('cannot do a non-empty take from an empty axes.')\n            raise IndexError(\"out of bounds value in 'indices'.\")\n        if to_shift.any():\n            indices = indices.copy()\n            indices[to_shift] += n\n        sp_indexer = self.sp_index.lookup_array(indices)\n        value_mask = sp_indexer != -1\n        new_sp_values = self.sp_values[sp_indexer[value_mask]]\n        value_indices = np.flatnonzero(value_mask).astype(np.int32, copy=False)\n        new_sp_index = make_sparse_index(len(indices), value_indices, kind=self.kind)\n        return type(self)._simple_new(new_sp_values, new_sp_index, dtype=self.dtype)\n\n    def __abs__(self) -> SparseArray:\n        return self._unary_method(operator.abs)\n\n    def unique(self) -> Self:\n        uniques = algos.unique(self.sp_values)\n        if len(self.sp_values) != len(self):\n            fill_loc = self._first_fill_value_loc()\n            insert_loc = len(algos.unique(self.sp_values[:fill_loc]))\n            uniques = np.insert(uniques, insert_loc, self.fill_value)\n        return type(self)._from_sequence(uniques, dtype=self.dtype)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return cls(values, dtype=original.dtype)\n\n    def sum(self, axis: AxisInt=0, min_count: int=0, skipna: bool=True, *args, **kwargs) -> Scalar:\n        \"\"\"\n        Sum of non-NA/null values\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Not Used. NumPy compatibility.\n        min_count : int, default 0\n            The required number of valid values to perform the summation. If fewer\n            than ``min_count`` valid values are present, the result will be the missing\n            value indicator for subarray type.\n        *args, **kwargs\n            Not Used. NumPy compatibility.\n\n        Returns\n        -------\n        scalar\n        \"\"\"\n        nv.validate_sum(args, kwargs)\n        valid_vals = self._valid_sp_values\n        sp_sum = valid_vals.sum()\n        has_na = self.sp_index.ngaps > 0 and (not self._null_fill_value)\n        if has_na and (not skipna):\n            return na_value_for_dtype(self.dtype.subtype, compat=False)\n        if self._null_fill_value:\n            if check_below_min_count(valid_vals.shape, None, min_count):\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\n            return sp_sum\n        else:\n            nsparse = self.sp_index.ngaps\n            if check_below_min_count(valid_vals.shape, None, min_count - nsparse):\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\n            return sp_sum + self.fill_value * nsparse", "class_fn": true, "question_id": "pandas/pandas.core.arrays.sparse.array/SparseArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/arrays/string_arrow.py", "fn_id": "", "content": "class ArrowStringArray(ObjectStringArrayMixin, ArrowExtensionArray, BaseStringArray):\n    \"\"\"\n    Extension array for string data in a ``pyarrow.ChunkedArray``.\n\n    .. warning::\n\n       ArrowStringArray is considered experimental. The implementation and\n       parts of the API may change without warning.\n\n    Parameters\n    ----------\n    values : pyarrow.Array or pyarrow.ChunkedArray\n        The array of data.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    See Also\n    --------\n    :func:`pandas.array`\n        The recommended function for creating a ArrowStringArray.\n    Series.str\n        The string methods are available on Series backed by\n        a ArrowStringArray.\n\n    Notes\n    -----\n    ArrowStringArray returns a BooleanArray for comparison methods.\n\n    Examples\n    --------\n    >>> pd.array(['This is', 'some text', None, 'data.'], dtype=\"string[pyarrow]\")\n    <ArrowStringArray>\n    ['This is', 'some text', <NA>, 'data.']\n    Length: 4, dtype: string\n    \"\"\"\n    _dtype: StringDtype\n    _storage = 'pyarrow'\n\n    def __init__(self, values) -> None:\n        _chk_pyarrow_available()\n        if isinstance(values, (pa.Array, pa.ChunkedArray)) and pa.types.is_string(values.type):\n            values = pc.cast(values, pa.large_string())\n        super().__init__(values)\n        self._dtype = StringDtype(storage=self._storage)\n        if not pa.types.is_large_string(self._pa_array.type) and (not (pa.types.is_dictionary(self._pa_array.type) and pa.types.is_large_string(self._pa_array.type.value_type))):\n            raise ValueError('ArrowStringArray requires a PyArrow (chunked) array of large_string type')\n\n    def _maybe_convert_setitem_value(self, value):\n        \"\"\"Maybe convert value to be pyarrow compatible.\"\"\"\n        if is_scalar(value):\n            if isna(value):\n                value = None\n            elif not isinstance(value, str):\n                raise TypeError('Scalar must be NA or str')\n        else:\n            value = np.array(value, dtype=object, copy=True)\n            value[isna(value)] = None\n            for v in value:\n                if not (v is None or isinstance(v, str)):\n                    raise TypeError('Scalar must be NA or str')\n        return super()._maybe_convert_setitem_value(value)\n\n    def _str_upper(self):\n        return type(self)(pc.utf8_upper(self._pa_array))\n\n    def _str_strip(self, to_strip=None):\n        if to_strip is None:\n            result = pc.utf8_trim_whitespace(self._pa_array)\n        else:\n            result = pc.utf8_trim(self._pa_array, characters=to_strip)\n        return type(self)(result)\n\n    def _str_isdecimal(self):\n        result = pc.utf8_is_decimal(self._pa_array)\n        return self._result_converter(result)\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype: Dtype | None=None, copy: bool=False):\n        from pandas.core.arrays.masked import BaseMaskedArray\n        _chk_pyarrow_available()\n        if dtype and (not (isinstance(dtype, str) and dtype == 'string')):\n            dtype = pandas_dtype(dtype)\n            assert isinstance(dtype, StringDtype) and dtype.storage in ('pyarrow', 'pyarrow_numpy')\n        if isinstance(scalars, BaseMaskedArray):\n            na_values = scalars._mask\n            result = scalars._data\n            result = lib.ensure_string_array(result, copy=copy, convert_na_value=False)\n            return cls(pa.array(result, mask=na_values, type=pa.large_string()))\n        elif isinstance(scalars, (pa.Array, pa.ChunkedArray)):\n            return cls(pc.cast(scalars, pa.large_string()))\n        result = lib.ensure_string_array(scalars, copy=copy)\n        return cls(pa.array(result, type=pa.large_string(), from_pandas=True))\n\n    @property\n    def dtype(self) -> StringDtype:\n        \"\"\"\n        An instance of 'string[pyarrow]'.\n        \"\"\"\n        return self._dtype\n\n    def _str_find(self, sub: str, start: int=0, end: int | None=None):\n        if start != 0 and end is not None:\n            slices = pc.utf8_slice_codeunits(self._pa_array, start, stop=end)\n            result = pc.find_substring(slices, sub)\n            not_found = pc.equal(result, -1)\n            offset_result = pc.add(result, end - start)\n            result = pc.if_else(not_found, result, offset_result)\n        elif start == 0 and end is None:\n            slices = self._pa_array\n            result = pc.find_substring(slices, sub)\n        else:\n            return super()._str_find(sub, start, end)\n        return self._convert_int_dtype(result)\n\n    def insert(self, loc: int, item) -> ArrowStringArray:\n        if not isinstance(item, str) and item is not libmissing.NA:\n            raise TypeError('Scalar must be NA or str')\n        return super().insert(loc, item)\n\n    def _str_isnumeric(self):\n        result = pc.utf8_is_numeric(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_fullmatch(self, pat, case: bool=True, flags: int=0, na: Scalar | None=None):\n        if not pat.endswith('$') or pat.endswith('\\\\$'):\n            pat = f'{pat}$'\n        return self._str_match(pat, case, flags, na)\n\n    def _str_istitle(self):\n        result = pc.utf8_is_title(self._pa_array)\n        return self._result_converter(result)\n\n    @property\n    def _data(self):\n        warnings.warn(f'{type(self).__name__}._data is a deprecated and will be removed in a future version, use ._pa_array instead', FutureWarning, stacklevel=find_stack_level())\n        return self._pa_array\n    _str_na_value = libmissing.NA\n\n    @classmethod\n    def _box_pa_scalar(cls, value, pa_type: pa.DataType | None=None) -> pa.Scalar:\n        pa_scalar = super()._box_pa_scalar(value, pa_type)\n        if pa.types.is_string(pa_scalar.type) and pa_type is None:\n            pa_scalar = pc.cast(pa_scalar, pa.large_string())\n        return pa_scalar\n\n    def _str_repeat(self, repeats: int | Sequence[int]):\n        if not isinstance(repeats, int):\n            return super()._str_repeat(repeats)\n        else:\n            return type(self)(pc.binary_repeat(self._pa_array, repeats))\n\n    def _explode(self):\n        \"\"\"\n        See Series.explode.__doc__.\n        \"\"\"\n        if not pa.types.is_list(self.dtype.pyarrow_dtype):\n            return super()._explode()\n        values = self\n        counts = pa.compute.list_value_length(values._pa_array)\n        counts = counts.fill_null(1).to_numpy()\n        fill_value = pa.scalar([None], type=self._pa_array.type)\n        mask = counts == 0\n        if mask.any():\n            values = values.copy()\n            values[mask] = fill_value\n            counts = counts.copy()\n            counts[mask] = 1\n        values = values.fillna(fill_value)\n        values = type(self)(pa.compute.list_flatten(values._pa_array))\n        return (values, counts)\n\n    def _str_isdigit(self):\n        result = pc.utf8_is_digit(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_len(self):\n        result = pc.utf8_length(self._pa_array)\n        return self._convert_int_dtype(result)\n\n    def _str_endswith(self, pat: str | tuple[str, ...], na: Scalar | None=None):\n        if isinstance(pat, str):\n            result = pc.ends_with(self._pa_array, pattern=pat)\n        elif len(pat) == 0:\n            result = pa.array(np.zeros(len(self._pa_array), dtype=bool), mask=isna(self._pa_array))\n        else:\n            result = pc.ends_with(self._pa_array, pattern=pat[0])\n            for p in pat[1:]:\n                result = pc.or_(result, pc.ends_with(self._pa_array, pattern=p))\n        if not isna(na):\n            result = result.fill_null(na)\n        return self._result_converter(result)\n\n    def _rank(self, *, axis: AxisInt=0, method: str='average', na_option: str='keep', ascending: bool=True, pct: bool=False):\n        \"\"\"\n        See Series.rank.__doc__.\n        \"\"\"\n        return self._convert_int_dtype(self._rank_calc(axis=axis, method=method, na_option=na_option, ascending=ascending, pct=pct))\n\n    def _str_rstrip(self, to_strip=None):\n        if to_strip is None:\n            result = pc.utf8_rtrim_whitespace(self._pa_array)\n        else:\n            result = pc.utf8_rtrim(self._pa_array, characters=to_strip)\n        return type(self)(result)\n\n    def _reduce(self, name: str, *, skipna: bool=True, keepdims: bool=False, **kwargs):\n        result = self._reduce_calc(name, skipna=skipna, keepdims=keepdims, **kwargs)\n        if name in ('argmin', 'argmax') and isinstance(result, pa.Array):\n            return self._convert_int_dtype(result)\n        elif isinstance(result, pa.Array):\n            return type(self)(result)\n        else:\n            return result\n\n    @doc(ExtensionArray.tolist)\n    def tolist(self):\n        if self.ndim > 1:\n            return [x.tolist() for x in self]\n        return list(self.to_numpy())\n\n    def _str_isupper(self):\n        result = pc.utf8_is_upper(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_isalpha(self):\n        result = pc.utf8_is_alpha(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_contains(self, pat, case: bool=True, flags: int=0, na=np.nan, regex: bool=True):\n        if flags:\n            fallback_performancewarning()\n            return super()._str_contains(pat, case, flags, na, regex)\n        if regex:\n            result = pc.match_substring_regex(self._pa_array, pat, ignore_case=not case)\n        else:\n            result = pc.match_substring(self._pa_array, pat, ignore_case=not case)\n        result = self._result_converter(result, na=na)\n        if not isna(na):\n            result[isna(result)] = bool(na)\n        return result\n\n    def _str_removesuffix(self, suffix: str):\n        ends_with = pc.ends_with(self._pa_array, pattern=suffix)\n        removed = pc.utf8_slice_codeunits(self._pa_array, 0, stop=-len(suffix))\n        result = pc.if_else(ends_with, removed, self._pa_array)\n        return type(self)(result)\n\n    @classmethod\n    def _from_sequence_of_strings(cls, strings, dtype: Dtype | None=None, copy: bool=False):\n        return cls._from_sequence(strings, dtype=dtype, copy=copy)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Length of this array.\n\n        Returns\n        -------\n        length : int\n        \"\"\"\n        return len(self._pa_array)\n\n    def _str_match(self, pat: str, case: bool=True, flags: int=0, na: Scalar | None=None):\n        if not pat.startswith('^'):\n            pat = f'^{pat}'\n        return self._str_contains(pat, case, flags, na, regex=True)\n\n    def _str_slice(self, start: int | None=None, stop: int | None=None, step: int | None=None):\n        if stop is None:\n            return super()._str_slice(start, stop, step)\n        if start is None:\n            start = 0\n        if step is None:\n            step = 1\n        return type(self)(pc.utf8_slice_codeunits(self._pa_array, start=start, stop=stop, step=step))\n\n    @classmethod\n    def _result_converter(cls, values, na=None):\n        return BooleanDtype().__from_arrow__(values)\n\n    def _str_lower(self):\n        return type(self)(pc.utf8_lower(self._pa_array))\n\n    def _convert_int_dtype(self, result):\n        return Int64Dtype().__from_arrow__(result)\n\n    def _str_get_dummies(self, sep: str='|'):\n        (dummies_pa, labels) = ArrowExtensionArray(self._pa_array)._str_get_dummies(sep)\n        if len(labels) == 0:\n            return (np.empty(shape=(0, 0), dtype=np.int64), labels)\n        dummies = np.vstack(dummies_pa.to_numpy())\n        return (dummies.astype(np.int64, copy=False), labels)\n\n    def __setitem__(self, key, value) -> None:\n        \"\"\"Set one or more values inplace.\n\n        Parameters\n        ----------\n        key : int, ndarray, or slice\n            When called from, e.g. ``Series.__setitem__``, ``key`` will be\n            one of\n\n            * scalar int\n            * ndarray of integers.\n            * boolean ndarray\n            * slice object\n\n        value : ExtensionDtype.type, Sequence[ExtensionDtype.type], or object\n            value or values to be set of ``key``.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if isinstance(key, tuple) and len(key) == 1:\n            key = key[0]\n        key = check_array_indexer(self, key)\n        value = self._maybe_convert_setitem_value(value)\n        if com.is_null_slice(key):\n            data = self._if_else(True, value, self._pa_array)\n        elif is_integer(key):\n            key = cast(int, key)\n            n = len(self)\n            if key < 0:\n                key += n\n            if not 0 <= key < n:\n                raise IndexError(f'index {key} is out of bounds for axis 0 with size {n}')\n            if isinstance(value, pa.Scalar):\n                value = value.as_py()\n            elif is_list_like(value):\n                raise ValueError('Length of indexer and values mismatch')\n            chunks = [*self._pa_array[:key].chunks, pa.array([value], type=self._pa_array.type, from_pandas=True), *self._pa_array[key + 1:].chunks]\n            data = pa.chunked_array(chunks).combine_chunks()\n        elif is_bool_dtype(key):\n            key = np.asarray(key, dtype=np.bool_)\n            data = self._replace_with_mask(self._pa_array, key, value)\n        elif is_scalar(value) or isinstance(value, pa.Scalar):\n            mask = np.zeros(len(self), dtype=np.bool_)\n            mask[key] = True\n            data = self._if_else(mask, value, self._pa_array)\n        else:\n            indices = np.arange(len(self))[key]\n            if len(indices) != len(value):\n                raise ValueError('Length of indexer and values mismatch')\n            if len(indices) == 0:\n                return\n            argsort = np.argsort(indices)\n            indices = indices[argsort]\n            value = value.take(argsort)\n            mask = np.zeros(len(self), dtype=np.bool_)\n            mask[indices] = True\n            data = self._replace_with_mask(self._pa_array, mask, value)\n        if isinstance(data, pa.Array):\n            data = pa.chunked_array([data])\n        self._pa_array = data\n\n    def _str_removeprefix(self, prefix: str):\n        if not pa_version_under13p0:\n            starts_with = pc.starts_with(self._pa_array, pattern=prefix)\n            removed = pc.utf8_slice_codeunits(self._pa_array, len(prefix))\n            result = pc.if_else(starts_with, removed, self._pa_array)\n            return type(self)(result)\n        return super()._str_removeprefix(prefix)\n\n    def astype(self, dtype, copy: bool=True):\n        dtype = pandas_dtype(dtype)\n        if dtype == self.dtype:\n            if copy:\n                return self.copy()\n            return self\n        elif isinstance(dtype, NumericDtype):\n            data = self._pa_array.cast(pa.from_numpy_dtype(dtype.numpy_dtype))\n            return dtype.__from_arrow__(data)\n        elif isinstance(dtype, np.dtype) and np.issubdtype(dtype, np.floating):\n            return self.to_numpy(dtype=dtype, na_value=np.nan)\n        return super().astype(dtype, copy=copy)\n\n    def _str_islower(self):\n        result = pc.utf8_is_lower(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_isalnum(self):\n        result = pc.utf8_is_alnum(self._pa_array)\n        return self._result_converter(result)\n\n    def _str_startswith(self, pat: str | tuple[str, ...], na: Scalar | None=None):\n        if isinstance(pat, str):\n            result = pc.starts_with(self._pa_array, pattern=pat)\n        elif len(pat) == 0:\n            result = pa.array(np.zeros(len(self._pa_array), dtype=bool), mask=isna(self._pa_array))\n        else:\n            result = pc.starts_with(self._pa_array, pattern=pat[0])\n            for p in pat[1:]:\n                result = pc.or_(result, pc.starts_with(self._pa_array, pattern=p))\n        if not isna(na):\n            result = result.fill_null(na)\n        return self._result_converter(result)\n\n    @classmethod\n    def _box_pa_array(cls, value, pa_type: pa.DataType | None=None, copy: bool=False) -> pa.Array | pa.ChunkedArray:\n        pa_array = super()._box_pa_array(value, pa_type)\n        if pa.types.is_string(pa_array.type) and pa_type is None:\n            pa_array = pc.cast(pa_array, pa.large_string())\n        return pa_array\n\n    def _str_count(self, pat: str, flags: int=0):\n        if flags:\n            return super()._str_count(pat, flags)\n        result = pc.count_substring_regex(self._pa_array, pat)\n        return self._convert_int_dtype(result)\n\n    def _str_replace(self, pat: str | re.Pattern, repl: str | Callable, n: int=-1, case: bool=True, flags: int=0, regex: bool=True):\n        if isinstance(pat, re.Pattern) or callable(repl) or (not case) or flags:\n            fallback_performancewarning()\n            return super()._str_replace(pat, repl, n, case, flags, regex)\n        func = pc.replace_substring_regex if regex else pc.replace_substring\n        result = func(self._pa_array, pattern=pat, replacement=repl, max_replacements=n)\n        return type(self)(result)\n\n    def _str_lstrip(self, to_strip=None):\n        if to_strip is None:\n            result = pc.utf8_ltrim_whitespace(self._pa_array)\n        else:\n            result = pc.utf8_ltrim(self._pa_array, characters=to_strip)\n        return type(self)(result)\n\n    def _str_map(self, f, na_value=None, dtype: Dtype | None=None, convert: bool=True):\n        from pandas.arrays import BooleanArray, IntegerArray\n        if dtype is None:\n            dtype = self.dtype\n        if na_value is None:\n            na_value = self.dtype.na_value\n        mask = isna(self)\n        arr = np.asarray(self)\n        if is_integer_dtype(dtype) or is_bool_dtype(dtype):\n            constructor: type[IntegerArray | BooleanArray]\n            if is_integer_dtype(dtype):\n                constructor = IntegerArray\n            else:\n                constructor = BooleanArray\n            na_value_is_na = isna(na_value)\n            if na_value_is_na:\n                na_value = 1\n            result = lib.map_infer_mask(arr, f, mask.view('uint8'), convert=False, na_value=na_value, dtype=np.dtype(dtype))\n            if not na_value_is_na:\n                mask[:] = False\n            return constructor(result, mask)\n        elif is_string_dtype(dtype) and (not is_object_dtype(dtype)):\n            result = lib.map_infer_mask(arr, f, mask.view('uint8'), convert=False, na_value=na_value)\n            result = pa.array(result, mask=mask, type=pa.large_string(), from_pandas=True)\n            return type(self)(result)\n        else:\n            return lib.map_infer_mask(arr, f, mask.view('uint8'))\n\n    def _str_isspace(self):\n        result = pc.utf8_is_space(self._pa_array)\n        return self._result_converter(result)\n\n    def isin(self, values: ArrayLike) -> npt.NDArray[np.bool_]:\n        value_set = [pa_scalar.as_py() for pa_scalar in [pa.scalar(value, from_pandas=True) for value in values] if pa_scalar.type in (pa.string(), pa.null(), pa.large_string())]\n        if not len(value_set):\n            return np.zeros(len(self), dtype=bool)\n        result = pc.is_in(self._pa_array, value_set=pa.array(value_set, type=self._pa_array.type))\n        return np.array(result, dtype=np.bool_)", "class_fn": true, "question_id": "pandas/pandas.core.arrays.string_arrow/ArrowStringArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/base.py", "fn_id": "", "content": "class IndexOpsMixin(OpsMixin):\n    \"\"\"\n    Common ops mixin to support a unified interface / docs for Series / Index\n    \"\"\"\n    __array_priority__ = 1000\n    _hidden_attrs: frozenset[str] = frozenset(['tolist'])\n\n    @property\n    def dtype(self) -> DtypeObj:\n        raise AbstractMethodError(self)\n\n    @property\n    def _values(self) -> ExtensionArray | np.ndarray:\n        raise AbstractMethodError(self)\n\n    @final\n    def _map_values(self, mapper, na_action=None, convert: bool=True):\n        \"\"\"\n        An internal function that maps values using the input\n        correspondence (which can be a dict, Series, or function).\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            The input correspondence object\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mapping function\n        convert : bool, default True\n            Try to find better dtype for elementwise function results. If\n            False, leave as dtype=object. Note that the dtype is always\n            preserved for some extension array dtypes, such as Categorical.\n\n        Returns\n        -------\n        Union[Index, MultiIndex], inferred\n            The output of the mapping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n        arr = self._values\n        if isinstance(arr, ExtensionArray):\n            return arr.map(mapper, na_action=na_action)\n        return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n    T = property(transpose, doc=\"\\n        Return the transpose, which is by definition self.\\n\\n        Examples\\n        --------\\n        For Series:\\n\\n        >>> s = pd.Series(['Ant', 'Bear', 'Cow'])\\n        >>> s\\n        0     Ant\\n        1    Bear\\n        2     Cow\\n        dtype: object\\n        >>> s.T\\n        0     Ant\\n        1    Bear\\n        2     Cow\\n        dtype: object\\n\\n        For Index:\\n\\n        >>> idx = pd.Index([1, 2, 3])\\n        >>> idx.T\\n        Index([1, 2, 3], dtype='int64')\\n        \")\n\n    @property\n    def shape(self) -> Shape:\n        \"\"\"\n        Return a tuple of the shape of the underlying data.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.shape\n        (3,)\n        \"\"\"\n        return self._values.shape\n\n    @doc(op='max', oppose='min', value='largest')\n    def argmax(self, axis: AxisInt | None=None, skipna: bool=True, *args, **kwargs) -> int:\n        \"\"\"\n        Return int position of the {value} value in the Series.\n\n        If the {op}imum is achieved in multiple locations,\n        the first row position is returned.\n\n        Parameters\n        ----------\n        axis : {{None}}\n            Unused. Parameter needed for compatibility with DataFrame.\n        skipna : bool, default True\n            Exclude NA/null values when showing the result.\n        *args, **kwargs\n            Additional arguments and keywords for compatibility with NumPy.\n\n        Returns\n        -------\n        int\n            Row position of the {op}imum value.\n\n        See Also\n        --------\n        Series.arg{op} : Return position of the {op}imum value.\n        Series.arg{oppose} : Return position of the {oppose}imum value.\n        numpy.ndarray.arg{op} : Equivalent method for numpy arrays.\n        Series.idxmax : Return index label of the maximum values.\n        Series.idxmin : Return index label of the minimum values.\n\n        Examples\n        --------\n        Consider dataset containing cereal calories\n\n        >>> s = pd.Series({{'Corn Flakes': 100.0, 'Almond Delight': 110.0,\n        ...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0}})\n        >>> s\n        Corn Flakes              100.0\n        Almond Delight           110.0\n        Cinnamon Toast Crunch    120.0\n        Cocoa Puff               110.0\n        dtype: float64\n\n        >>> s.argmax()\n        2\n        >>> s.argmin()\n        0\n\n        The maximum cereal calories is the third element and\n        the minimum cereal calories is the first element,\n        since series is zero-indexed.\n        \"\"\"\n        delegate = self._values\n        nv.validate_minmax_axis(axis)\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\n        if isinstance(delegate, ExtensionArray):\n            if not skipna and delegate.isna().any():\n                warnings.warn(f'The behavior of {type(self).__name__}.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.', FutureWarning, stacklevel=find_stack_level())\n                return -1\n            else:\n                return delegate.argmax()\n        else:\n            result = nanops.nanargmax(delegate, skipna=skipna)\n            if result == -1:\n                warnings.warn(f'The behavior of {type(self).__name__}.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.', FutureWarning, stacklevel=find_stack_level())\n            return result\n\n    @property\n    def ndim(self) -> Literal[1]:\n        \"\"\"\n        Number of dimensions of the underlying data, by definition 1.\n\n        Examples\n        --------\n        >>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n        >>> s\n        0     Ant\n        1    Bear\n        2     Cow\n        dtype: object\n        >>> s.ndim\n        1\n\n        For Index:\n\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx\n        Index([1, 2, 3], dtype='int64')\n        >>> idx.ndim\n        1\n        \"\"\"\n        return 1\n\n    @final\n    def item(self):\n        \"\"\"\n        Return the first element of the underlying data as a Python scalar.\n\n        Returns\n        -------\n        scalar\n            The first element of Series or Index.\n\n        Raises\n        ------\n        ValueError\n            If the data is not length = 1.\n\n        Examples\n        --------\n        >>> s = pd.Series([1])\n        >>> s.item()\n        1\n\n        For an index:\n\n        >>> s = pd.Series([1], index=['a'])\n        >>> s.index.item()\n        'a'\n        \"\"\"\n        if len(self) == 1:\n            return next(iter(self))\n        raise ValueError('can only convert an array of size 1 to a Python scalar')\n\n    @property\n    def nbytes(self) -> int:\n        \"\"\"\n        Return the number of bytes in the underlying data.\n\n        Examples\n        --------\n        For Series:\n\n        >>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n        >>> s\n        0     Ant\n        1    Bear\n        2     Cow\n        dtype: object\n        >>> s.nbytes\n        24\n\n        For Index:\n\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx\n        Index([1, 2, 3], dtype='int64')\n        >>> idx.nbytes\n        24\n        \"\"\"\n        return self._values.nbytes\n\n    @property\n    def size(self) -> int:\n        \"\"\"\n        Return the number of elements in the underlying data.\n\n        Examples\n        --------\n        For Series:\n\n        >>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n        >>> s\n        0     Ant\n        1    Bear\n        2     Cow\n        dtype: object\n        >>> s.size\n        3\n\n        For Index:\n\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx\n        Index([1, 2, 3], dtype='int64')\n        >>> idx.size\n        3\n        \"\"\"\n        return len(self._values)\n\n    @property\n    def array(self) -> ExtensionArray:\n        \"\"\"\n        The ExtensionArray of the data backing this Series or Index.\n\n        Returns\n        -------\n        ExtensionArray\n            An ExtensionArray of the values stored within. For extension\n            types, this is the actual array. For NumPy native types, this\n            is a thin (no copy) wrapper around :class:`numpy.ndarray`.\n\n            ``.array`` differs from ``.values``, which may require converting\n            the data to a different form.\n\n        See Also\n        --------\n        Index.to_numpy : Similar method that always returns a NumPy array.\n        Series.to_numpy : Similar method that always returns a NumPy array.\n\n        Notes\n        -----\n        This table lays out the different array types for each extension\n        dtype within pandas.\n\n        ================== =============================\n        dtype              array type\n        ================== =============================\n        category           Categorical\n        period             PeriodArray\n        interval           IntervalArray\n        IntegerNA          IntegerArray\n        string             StringArray\n        boolean            BooleanArray\n        datetime64[ns, tz] DatetimeArray\n        ================== =============================\n\n        For any 3rd-party extension types, the array type will be an\n        ExtensionArray.\n\n        For all remaining dtypes ``.array`` will be a\n        :class:`arrays.NumpyExtensionArray` wrapping the actual ndarray\n        stored within. If you absolutely need a NumPy array (possibly with\n        copying / coercing data), then use :meth:`Series.to_numpy` instead.\n\n        Examples\n        --------\n        For regular NumPy types like int, and float, a NumpyExtensionArray\n        is returned.\n\n        >>> pd.Series([1, 2, 3]).array\n        <NumpyExtensionArray>\n        [1, 2, 3]\n        Length: 3, dtype: int64\n\n        For extension types, like Categorical, the actual ExtensionArray\n        is returned\n\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\n        >>> ser.array\n        ['a', 'b', 'a']\n        Categories (2, object): ['a', 'b']\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _arith_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n        rvalues = ops.maybe_prepare_scalar_for_op(rvalues, lvalues.shape)\n        rvalues = ensure_wrapped_if_datetimelike(rvalues)\n        if isinstance(rvalues, range):\n            rvalues = np.arange(rvalues.start, rvalues.stop, rvalues.step)\n        with np.errstate(all='ignore'):\n            result = ops.arithmetic_op(lvalues, rvalues, op)\n        return self._construct_result(result, name=res_name)\n\n    @final\n    @property\n    def empty(self) -> bool:\n        return not self.size\n\n    @doc(_shared_docs['searchsorted'], klass='Index')\n    def searchsorted(self, value: NumpyValueArrayLike | ExtensionArray, side: Literal['left', 'right']='left', sorter: NumpySorter | None=None) -> npt.NDArray[np.intp] | np.intp:\n        if isinstance(value, ABCDataFrame):\n            msg = f'Value must be 1-D array-like or scalar, {type(value).__name__} is not supported'\n            raise ValueError(msg)\n        values = self._values\n        if not isinstance(values, np.ndarray):\n            return values.searchsorted(value, side=side, sorter=sorter)\n        return algorithms.searchsorted(values, value, side=side, sorter=sorter)\n\n    @final\n    def nunique(self, dropna: bool=True) -> int:\n        \"\"\"\n        Return number of unique elements in the object.\n\n        Excludes NA values by default.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include NaN in the count.\n\n        Returns\n        -------\n        int\n\n        See Also\n        --------\n        DataFrame.nunique: Method nunique for DataFrame.\n        Series.count: Count non-NA/null observations in the Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 3, 5, 7, 7])\n        >>> s\n        0    1\n        1    3\n        2    5\n        3    7\n        4    7\n        dtype: int64\n\n        >>> s.nunique()\n        4\n        \"\"\"\n        uniqs = self.unique()\n        if dropna:\n            uniqs = remove_na_arraylike(uniqs)\n        return len(uniqs)\n\n    @doc(argmax, op='min', oppose='max', value='smallest')\n    def argmin(self, axis: AxisInt | None=None, skipna: bool=True, *args, **kwargs) -> int:\n        delegate = self._values\n        nv.validate_minmax_axis(axis)\n        skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)\n        if isinstance(delegate, ExtensionArray):\n            if not skipna and delegate.isna().any():\n                warnings.warn(f'The behavior of {type(self).__name__}.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.', FutureWarning, stacklevel=find_stack_level())\n                return -1\n            else:\n                return delegate.argmin()\n        else:\n            result = nanops.nanargmin(delegate, skipna=skipna)\n            if result == -1:\n                warnings.warn(f'The behavior of {type(self).__name__}.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.', FutureWarning, stacklevel=find_stack_level())\n            return result\n    to_list = tolist\n\n    @doc(algorithms.factorize, values='', order='', size_hint='', sort=textwrap.dedent('            sort : bool, default False\\n                Sort `uniques` and shuffle `codes` to maintain the\\n                relationship.\\n            '))\n    def factorize(self, sort: bool=False, use_na_sentinel: bool=True) -> tuple[npt.NDArray[np.intp], Index]:\n        (codes, uniques) = algorithms.factorize(self._values, sort=sort, use_na_sentinel=use_na_sentinel)\n        if uniques.dtype == np.float16:\n            uniques = uniques.astype(np.float32)\n        if isinstance(self, ABCIndex):\n            uniques = self._constructor(uniques)\n        else:\n            from pandas import Index\n            uniques = Index(uniques)\n        return (codes, uniques)\n\n    @cache_readonly\n    def hasnans(self) -> bool:\n        \"\"\"\n        Return True if there are any NaNs.\n\n        Enables various performance speedups.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, None])\n        >>> s\n        0    1.0\n        1    2.0\n        2    3.0\n        3    NaN\n        dtype: float64\n        >>> s.hasnans\n        True\n        \"\"\"\n        return bool(isna(self).any())\n\n    def _logical_method(self, other, op):\n        return NotImplemented\n\n    def _construct_result(self, result, name):\n        \"\"\"\n        Construct an appropriately-wrapped result from the ArrayLike result\n        of an arithmetic-like operation.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _cmp_method(self, other, op):\n        return NotImplemented\n\n    def tolist(self):\n        \"\"\"\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a pandas scalar\n        (for Timestamp/Timedelta/Interval/Period)\n\n        Returns\n        -------\n        list\n\n        See Also\n        --------\n        numpy.ndarray.tolist : Return the array as an a.ndim-levels deep\n            nested list of Python scalars.\n\n        Examples\n        --------\n        For Series\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.to_list()\n        [1, 2, 3]\n\n        For Index:\n\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx\n        Index([1, 2, 3], dtype='int64')\n\n        >>> idx.to_list()\n        [1, 2, 3]\n        \"\"\"\n        return self._values.tolist()\n\n    @property\n    def is_unique(self) -> bool:\n        \"\"\"\n        Return boolean if values in the object are unique.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.is_unique\n        True\n\n        >>> s = pd.Series([1, 2, 3, 1])\n        >>> s.is_unique\n        False\n        \"\"\"\n        return self.nunique(dropna=False) == len(self)\n\n    @property\n    def is_monotonic_increasing(self) -> bool:\n        \"\"\"\n        Return boolean if values in the object are monotonically increasing.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 2])\n        >>> s.is_monotonic_increasing\n        True\n\n        >>> s = pd.Series([3, 2, 1])\n        >>> s.is_monotonic_increasing\n        False\n        \"\"\"\n        from pandas import Index\n        return Index(self).is_monotonic_increasing\n\n    @property\n    def is_monotonic_decreasing(self) -> bool:\n        \"\"\"\n        Return boolean if values in the object are monotonically decreasing.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> s = pd.Series([3, 2, 2, 1])\n        >>> s.is_monotonic_decreasing\n        True\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.is_monotonic_decreasing\n        False\n        \"\"\"\n        from pandas import Index\n        return Index(self).is_monotonic_decreasing\n\n    def __iter__(self) -> Iterator:\n        \"\"\"\n        Return an iterator of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a pandas scalar\n        (for Timestamp/Timedelta/Interval/Period)\n\n        Returns\n        -------\n        iterator\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> for x in s:\n        ...     print(x)\n        1\n        2\n        3\n        \"\"\"\n        if not isinstance(self._values, np.ndarray):\n            return iter(self._values)\n        else:\n            return map(self._values.item, range(self._values.size))\n\n    @final\n    def _memory_usage(self, deep: bool=False) -> int:\n        \"\"\"\n        Memory usage of the values.\n\n        Parameters\n        ----------\n        deep : bool, default False\n            Introspect the data deeply, interrogate\n            `object` dtypes for system-level memory consumption.\n\n        Returns\n        -------\n        bytes used\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n            array.\n\n        Notes\n        -----\n        Memory usage does not include memory consumed by elements that\n        are not components of the array if deep=False or if used on PyPy\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx.memory_usage()\n        24\n        \"\"\"\n        if hasattr(self.array, 'memory_usage'):\n            return self.array.memory_usage(deep=deep)\n        v = self.array.nbytes\n        if deep and is_object_dtype(self.dtype) and (not PYPY):\n            values = cast(np.ndarray, self._values)\n            v += lib.memory_usage_of_objects(values)\n        return v\n    _shared_docs['searchsorted'] = \"\\n        Find indices where elements should be inserted to maintain order.\\n\\n        Find the indices into a sorted {klass} `self` such that, if the\\n        corresponding elements in `value` were inserted before the indices,\\n        the order of `self` would be preserved.\\n\\n        .. note::\\n\\n            The {klass} *must* be monotonically sorted, otherwise\\n            wrong locations will likely be returned. Pandas does *not*\\n            check this for you.\\n\\n        Parameters\\n        ----------\\n        value : array-like or scalar\\n            Values to insert into `self`.\\n        side : {{'left', 'right'}}, optional\\n            If 'left', the index of the first suitable location found is given.\\n            If 'right', return the last such index.  If there is no suitable\\n            index, return either 0 or N (where N is the length of `self`).\\n        sorter : 1-D array-like, optional\\n            Optional array of integer indices that sort `self` into ascending\\n            order. They are typically the result of ``np.argsort``.\\n\\n        Returns\\n        -------\\n        int or array of int\\n            A scalar or array of insertion points with the\\n            same shape as `value`.\\n\\n        See Also\\n        --------\\n        sort_values : Sort by the values along either axis.\\n        numpy.searchsorted : Similar method from NumPy.\\n\\n        Notes\\n        -----\\n        Binary search is used to find the required insertion points.\\n\\n        Examples\\n        --------\\n        >>> ser = pd.Series([1, 2, 3])\\n        >>> ser\\n        0    1\\n        1    2\\n        2    3\\n        dtype: int64\\n\\n        >>> ser.searchsorted(4)\\n        3\\n\\n        >>> ser.searchsorted([0, 4])\\n        array([0, 3])\\n\\n        >>> ser.searchsorted([1, 3], side='left')\\n        array([0, 2])\\n\\n        >>> ser.searchsorted([1, 3], side='right')\\n        array([1, 3])\\n\\n        >>> ser = pd.Series(pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000']))\\n        >>> ser\\n        0   2000-03-11\\n        1   2000-03-12\\n        2   2000-03-13\\n        dtype: datetime64[ns]\\n\\n        >>> ser.searchsorted('3/14/2000')\\n        3\\n\\n        >>> ser = pd.Categorical(\\n        ...     ['apple', 'bread', 'bread', 'cheese', 'milk'], ordered=True\\n        ... )\\n        >>> ser\\n        ['apple', 'bread', 'bread', 'cheese', 'milk']\\n        Categories (4, object): ['apple' < 'bread' < 'cheese' < 'milk']\\n\\n        >>> ser.searchsorted('bread')\\n        1\\n\\n        >>> ser.searchsorted(['bread'], side='right')\\n        array([3])\\n\\n        If the values are not monotonically sorted, wrong locations\\n        may be returned:\\n\\n        >>> ser = pd.Series([2, 1, 3])\\n        >>> ser\\n        0    2\\n        1    1\\n        2    3\\n        dtype: int64\\n\\n        >>> ser.searchsorted(1)  # doctest: +SKIP\\n        0  # wrong result, correct would be 1\\n        \"\n\n    @overload\n    def searchsorted(self, value: ScalarLike_co, side: Literal['left', 'right']=..., sorter: NumpySorter=...) -> np.intp:\n        ...\n\n    @overload\n    def searchsorted(self, value: npt.ArrayLike | ExtensionArray, side: Literal['left', 'right']=..., sorter: NumpySorter=...) -> npt.NDArray[np.intp]:\n        ...\n\n    @final\n    def value_counts(self, normalize: bool=False, sort: bool=True, ascending: bool=False, bins=None, dropna: bool=True) -> Series:\n        \"\"\"\n        Return a Series containing counts of unique values.\n\n        The resulting object will be in descending order so that the\n        first element is the most frequently-occurring element.\n        Excludes NA values by default.\n\n        Parameters\n        ----------\n        normalize : bool, default False\n            If True then the object returned will contain the relative\n            frequencies of the unique values.\n        sort : bool, default True\n            Sort by frequencies when True. Preserve the order of the data when False.\n        ascending : bool, default False\n            Sort in ascending order.\n        bins : int, optional\n            Rather than count values, group them into half-open bins,\n            a convenience for ``pd.cut``, only works with numeric data.\n        dropna : bool, default True\n            Don't include counts of NaN.\n\n        Returns\n        -------\n        Series\n\n        See Also\n        --------\n        Series.count: Number of non-NA elements in a Series.\n        DataFrame.count: Number of non-NA elements in a DataFrame.\n        DataFrame.value_counts: Equivalent method on DataFrames.\n\n        Examples\n        --------\n        >>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\n        >>> index.value_counts()\n        3.0    2\n        1.0    1\n        2.0    1\n        4.0    1\n        Name: count, dtype: int64\n\n        With `normalize` set to `True`, returns the relative frequency by\n        dividing all values by the sum of values.\n\n        >>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\n        >>> s.value_counts(normalize=True)\n        3.0    0.4\n        1.0    0.2\n        2.0    0.2\n        4.0    0.2\n        Name: proportion, dtype: float64\n\n        **bins**\n\n        Bins can be useful for going from a continuous variable to a\n        categorical variable; instead of counting unique\n        apparitions of values, divide the index in the specified\n        number of half-open bins.\n\n        >>> s.value_counts(bins=3)\n        (0.996, 2.0]    2\n        (2.0, 3.0]      2\n        (3.0, 4.0]      1\n        Name: count, dtype: int64\n\n        **dropna**\n\n        With `dropna` set to `False` we can also see NaN index values.\n\n        >>> s.value_counts(dropna=False)\n        3.0    2\n        1.0    1\n        2.0    1\n        4.0    1\n        NaN    1\n        Name: count, dtype: int64\n        \"\"\"\n        return algorithms.value_counts_internal(self, sort=sort, ascending=ascending, normalize=normalize, bins=bins, dropna=dropna)\n\n    def unique(self):\n        values = self._values\n        if not isinstance(values, np.ndarray):\n            result = values.unique()\n        else:\n            result = algorithms.unique1d(values)\n        return result\n\n    @final\n    def transpose(self, *args, **kwargs) -> Self:\n        \"\"\"\n        Return the transpose, which is by definition self.\n\n        Returns\n        -------\n        %(klass)s\n        \"\"\"\n        nv.validate_transpose(args, kwargs)\n        return self\n\n    def __len__(self) -> int:\n        raise AbstractMethodError(self)\n\n    @final\n    def to_numpy(self, dtype: npt.DTypeLike | None=None, copy: bool=False, na_value: object=lib.no_default, **kwargs) -> np.ndarray:\n        \"\"\"\n        A NumPy ndarray representing the values in this Series or Index.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to pass to :meth:`numpy.asarray`.\n        copy : bool, default False\n            Whether to ensure that the returned value is not a view on\n            another array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary.\n        na_value : Any, optional\n            The value to use for missing values. The default value depends\n            on `dtype` and the type of the array.\n        **kwargs\n            Additional keywords passed through to the ``to_numpy`` method\n            of the underlying array (for extension arrays).\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        Series.array : Get the actual data stored within.\n        Index.array : Get the actual data stored within.\n        DataFrame.to_numpy : Similar method for DataFrame.\n\n        Notes\n        -----\n        The returned array will be the same up to equality (values equal\n        in `self` will be equal in the returned array; likewise for values\n        that are not equal). When `self` contains an ExtensionArray, the\n        dtype may be different. For example, for a category-dtype Series,\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\n        will be lost.\n\n        For NumPy dtypes, this will be a reference to the actual data stored\n        in this Series or Index (assuming ``copy=False``). Modifying the result\n        in place will modify the data stored in the Series or Index (not that\n        we recommend doing that).\n\n        For extension types, ``to_numpy()`` *may* require copying data and\n        coercing the result to a NumPy type (possibly object), which may be\n        expensive. When you need a no-copy reference to the underlying data,\n        :attr:`Series.array` should be used instead.\n\n        This table lays out the different dtypes and default return types of\n        ``to_numpy()`` for various dtypes within pandas.\n\n        ================== ================================\n        dtype              array type\n        ================== ================================\n        category[T]        ndarray[T] (same dtype as input)\n        period             ndarray[object] (Periods)\n        interval           ndarray[object] (Intervals)\n        IntegerNA          ndarray[object]\n        datetime64[ns]     datetime64[ns]\n        datetime64[ns, tz] ndarray[object] (Timestamps)\n        ================== ================================\n\n        Examples\n        --------\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\n        >>> ser.to_numpy()\n        array(['a', 'b', 'a'], dtype=object)\n\n        Specify the `dtype` to control how datetime-aware data is represented.\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\n        objects, each with the correct ``tz``.\n\n        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n        >>> ser.to_numpy(dtype=object)\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\n              dtype=object)\n\n        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\n        datetime64 values. The values are converted to UTC and the timezone\n        info is dropped.\n\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\n        ... # doctest: +ELLIPSIS\n        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\n              dtype='datetime64[ns]')\n        \"\"\"\n        if isinstance(self.dtype, ExtensionDtype):\n            return self.array.to_numpy(dtype, copy=copy, na_value=na_value, **kwargs)\n        elif kwargs:\n            bad_keys = next(iter(kwargs.keys()))\n            raise TypeError(f\"to_numpy() got an unexpected keyword argument '{bad_keys}'\")\n        fillna = na_value is not lib.no_default and (not (na_value is np.nan and np.issubdtype(self.dtype, np.floating)))\n        values = self._values\n        if fillna:\n            if not can_hold_element(values, na_value):\n                values = np.asarray(values, dtype=dtype)\n            else:\n                values = values.copy()\n            values[np.asanyarray(isna(self))] = na_value\n        result = np.asarray(values, dtype=dtype)\n        if copy and (not fillna) or (not copy and using_copy_on_write()):\n            if np.shares_memory(self._values[:2], result[:2]):\n                if using_copy_on_write() and (not copy):\n                    result = result.view()\n                    result.flags.writeable = False\n                else:\n                    result = result.copy()\n        return result\n\n    def drop_duplicates(self, *, keep: DropKeep='first'):\n        duplicated = self._duplicated(keep=keep)\n        return self[~duplicated]\n\n    @final\n    def _duplicated(self, keep: DropKeep='first') -> npt.NDArray[np.bool_]:\n        arr = self._values\n        if isinstance(arr, ExtensionArray):\n            return arr.duplicated(keep=keep)\n        return algorithms.duplicated(arr, keep=keep)", "class_fn": true, "question_id": "pandas/pandas.core.base/IndexOpsMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/ops.py", "fn_id": "", "content": "class BinOp(Op):\n    \"\"\"\n    Hold a binary operator and its operands.\n\n    Parameters\n    ----------\n    op : str\n    lhs : Term or Op\n    rhs : Term or Op\n    \"\"\"\n\n    def convert_values(self) -> None:\n        \"\"\"\n        Convert datetimes to a comparable value in an expression.\n        \"\"\"\n\n        def stringify(value):\n            encoder: Callable\n            if self.encoding is not None:\n                encoder = partial(pprint_thing_encoded, encoding=self.encoding)\n            else:\n                encoder = pprint_thing\n            return encoder(value)\n        (lhs, rhs) = (self.lhs, self.rhs)\n        if is_term(lhs) and lhs.is_datetime and is_term(rhs) and rhs.is_scalar:\n            v = rhs.value\n            if isinstance(v, (int, float)):\n                v = stringify(v)\n            v = Timestamp(ensure_decoded(v))\n            if v.tz is not None:\n                v = v.tz_convert('UTC')\n            self.rhs.update(v)\n        if is_term(rhs) and rhs.is_datetime and is_term(lhs) and lhs.is_scalar:\n            v = lhs.value\n            if isinstance(v, (int, float)):\n                v = stringify(v)\n            v = Timestamp(ensure_decoded(v))\n            if v.tz is not None:\n                v = v.tz_convert('UTC')\n            self.lhs.update(v)\n\n    def __call__(self, env):\n        \"\"\"\n        Recursively evaluate an expression in Python space.\n\n        Parameters\n        ----------\n        env : Scope\n\n        Returns\n        -------\n        object\n            The result of an evaluated expression.\n        \"\"\"\n        left = self.lhs(env)\n        right = self.rhs(env)\n        return self.func(left, right)\n\n    def __iter__(self) -> Iterator:\n        return iter(self.operands)\n\n    def __init__(self, op: str, lhs, rhs) -> None:\n        super().__init__(op, (lhs, rhs))\n        self.lhs = lhs\n        self.rhs = rhs\n        self._disallow_scalar_only_bool_ops()\n        self.convert_values()\n        try:\n            self.func = _binary_ops_dict[op]\n        except KeyError as err:\n            keys = list(_binary_ops_dict.keys())\n            raise ValueError(f'Invalid binary operator {repr(op)}, valid operators are {keys}') from err\n\n    def _disallow_scalar_only_bool_ops(self):\n        rhs = self.rhs\n        lhs = self.lhs\n        rhs_rt = rhs.return_type\n        rhs_rt = getattr(rhs_rt, 'type', rhs_rt)\n        lhs_rt = lhs.return_type\n        lhs_rt = getattr(lhs_rt, 'type', lhs_rt)\n        if (lhs.is_scalar or rhs.is_scalar) and self.op in _bool_ops_dict and (not (issubclass(rhs_rt, (bool, np.bool_)) and issubclass(lhs_rt, (bool, np.bool_)))):\n            raise NotImplementedError('cannot evaluate scalar only bool ops')\n\n    def evaluate(self, env, engine: str, parser, term_type, eval_in_python):\n        \"\"\"\n        Evaluate a binary operation *before* being passed to the engine.\n\n        Parameters\n        ----------\n        env : Scope\n        engine : str\n        parser : str\n        term_type : type\n        eval_in_python : list\n\n        Returns\n        -------\n        term_type\n            The \"pre-evaluated\" expression as an instance of ``term_type``\n        \"\"\"\n        if engine == 'python':\n            res = self(env)\n        else:\n            left = self.lhs.evaluate(env, engine=engine, parser=parser, term_type=term_type, eval_in_python=eval_in_python)\n            right = self.rhs.evaluate(env, engine=engine, parser=parser, term_type=term_type, eval_in_python=eval_in_python)\n            if self.op in eval_in_python:\n                res = self.func(left.value, right.value)\n            else:\n                from pandas.core.computation.eval import eval\n                res = eval(self, local_dict=env, engine=engine, parser=parser)\n        name = env.add_tmp(res)\n        return term_type(name, env=env)\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Print a generic n-ary operator and its operands using infix notation.\n        \"\"\"\n        parened = (f'({pprint_thing(opr)})' for opr in self.operands)\n        return pprint_thing(f' {self.op} '.join(parened))", "class_fn": true, "question_id": "pandas/pandas.core.computation.ops/BinOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/computation/pytables.py", "fn_id": "", "content": "class PyTablesExpr(expr.Expr):\n    \"\"\"\n    Hold a pytables-like expression, comprised of possibly multiple 'terms'.\n\n    Parameters\n    ----------\n    where : string term expression, PyTablesExpr, or list-like of PyTablesExprs\n    queryables : a \"kinds\" map (dict of column name -> kind), or None if column\n        is non-indexable\n    encoding : an encoding that will encode the query terms\n\n    Returns\n    -------\n    a PyTablesExpr object\n\n    Examples\n    --------\n    'index>=date'\n    \"columns=['A', 'D']\"\n    'columns=A'\n    'columns==A'\n    \"~(columns=['A','B'])\"\n    'index>df.index[3] & string=\"bar\"'\n    '(index>df.index[3] & index<=df.index[6]) | string=\"bar\"'\n    \"ts>=Timestamp('2012-02-01')\"\n    \"major_axis>=20130101\"\n    \"\"\"\n    _visitor: PyTablesExprVisitor | None\n    env: PyTablesScope\n    expr: str\n\n    def evaluate(self):\n        \"\"\"create and return the numexpr condition and filter\"\"\"\n        try:\n            self.condition = self.terms.prune(ConditionBinOp)\n        except AttributeError as err:\n            raise ValueError(f'cannot process expression [{self.expr}], [{self}] is not a valid condition') from err\n        try:\n            self.filter = self.terms.prune(FilterBinOp)\n        except AttributeError as err:\n            raise ValueError(f'cannot process expression [{self.expr}], [{self}] is not a valid filter') from err\n        return (self.condition, self.filter)\n\n    def __call__(self):\n        return self.terms(self.env)\n\n    def __repr__(self) -> str:\n        if self.terms is not None:\n            return pprint_thing(self.terms)\n        return pprint_thing(self.expr)\n\n    def parse(self):\n        \"\"\"\n        Parse an expression.\n        \"\"\"\n        return self._visitor.visit(self.expr)\n\n    def __init__(self, where, queryables: dict[str, Any] | None=None, encoding=None, scope_level: int=0) -> None:\n        where = _validate_where(where)\n        self.encoding = encoding\n        self.condition = None\n        self.filter = None\n        self.terms = None\n        self._visitor = None\n        local_dict: _scope.DeepChainMap[Any, Any] | None = None\n        if isinstance(where, PyTablesExpr):\n            local_dict = where.env.scope\n            _where = where.expr\n        elif is_list_like(where):\n            where = list(where)\n            for (idx, w) in enumerate(where):\n                if isinstance(w, PyTablesExpr):\n                    local_dict = w.env.scope\n                else:\n                    where[idx] = _validate_where(w)\n            _where = ' & '.join([f'({w})' for w in com.flatten(where)])\n        else:\n            _where = where\n        self.expr = _where\n        self.env = PyTablesScope(scope_level + 1, local_dict=local_dict)\n        if queryables is not None and isinstance(self.expr, str):\n            self.env.queryables.update(queryables)\n            self._visitor = PyTablesExprVisitor(self.env, queryables=queryables, parser='pytables', engine='pytables', encoding=encoding)\n            self.terms = self.parse()\n\n    def __len__(self) -> int:\n        return len(self.expr)", "class_fn": true, "question_id": "pandas/pandas.core.computation.pytables/PyTablesExpr", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/dtypes/base.py", "fn_id": "", "content": "class ExtensionDtype:\n    \"\"\"\n    A custom data type, to be paired with an ExtensionArray.\n\n    See Also\n    --------\n    extensions.register_extension_dtype: Register an ExtensionType\n        with pandas as class decorator.\n    extensions.ExtensionArray: Abstract base class for custom 1-D array types.\n\n    Notes\n    -----\n    The interface includes the following abstract methods that must\n    be implemented by subclasses:\n\n    * type\n    * name\n    * construct_array_type\n\n    The following attributes and methods influence the behavior of the dtype in\n    pandas operations\n\n    * _is_numeric\n    * _is_boolean\n    * _get_common_dtype\n\n    The `na_value` class attribute can be used to set the default NA value\n    for this type. :attr:`numpy.nan` is used by default.\n\n    ExtensionDtypes are required to be hashable. The base class provides\n    a default implementation, which relies on the ``_metadata`` class\n    attribute. ``_metadata`` should be a tuple containing the strings\n    that define your data type. For example, with ``PeriodDtype`` that's\n    the ``freq`` attribute.\n\n    **If you have a parametrized dtype you should set the ``_metadata``\n    class property**.\n\n    Ideally, the attributes in ``_metadata`` will match the\n    parameters to your ``ExtensionDtype.__init__`` (if any). If any of\n    the attributes in ``_metadata`` don't implement the standard\n    ``__eq__`` or ``__hash__``, the default implementations here will not\n    work.\n\n    Examples\n    --------\n\n    For interaction with Apache Arrow (pyarrow), a ``__from_arrow__`` method\n    can be implemented: this method receives a pyarrow Array or ChunkedArray\n    as only argument and is expected to return the appropriate pandas\n    ExtensionArray for this dtype and the passed values:\n\n    >>> import pyarrow\n    >>> from pandas.api.extensions import ExtensionArray\n    >>> class ExtensionDtype:\n    ...     def __from_arrow__(\n    ...         self,\n    ...         array: pyarrow.Array | pyarrow.ChunkedArray\n    ...     ) -> ExtensionArray:\n    ...         ...\n\n    This class does not inherit from 'abc.ABCMeta' for performance reasons.\n    Methods and properties required by the interface raise\n    ``pandas.errors.AbstractMethodError`` and no ``register`` method is\n    provided for registering virtual subclasses.\n    \"\"\"\n    _metadata: tuple[str, ...] = ()\n\n    def __str__(self) -> str:\n        return self.name\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"\n        Check whether 'other' is equal to self.\n\n        By default, 'other' is considered equal if either\n\n        * it's a string matching 'self.name'.\n        * it's an instance of this type and all of the attributes\n          in ``self._metadata`` are equal between `self` and `other`.\n\n        Parameters\n        ----------\n        other : Any\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if isinstance(other, str):\n            try:\n                other = self.construct_from_string(other)\n            except TypeError:\n                return False\n        if isinstance(other, type(self)):\n            return all((getattr(self, attr) == getattr(other, attr) for attr in self._metadata))\n        return False\n\n    def __hash__(self) -> int:\n        return object_hash(tuple((getattr(self, attr) for attr in self._metadata)))\n\n    def __ne__(self, other: object) -> bool:\n        return not self.__eq__(other)\n\n    @property\n    def na_value(self) -> object:\n        \"\"\"\n        Default NA value to use for this type.\n\n        This is used in e.g. ExtensionArray.take. This should be the\n        user-facing \"boxed\" version of the NA value, not the physical NA value\n        for storage.  e.g. for JSONArray, this is an empty dictionary.\n        \"\"\"\n        return np.nan\n\n    @property\n    def type(self) -> type_t[Any]:\n        \"\"\"\n        The scalar type for the array, e.g. ``int``\n\n        It's expected ``ExtensionArray[item]`` returns an instance\n        of ``ExtensionDtype.type`` for scalar ``item``, assuming\n        that value is valid (not NA). NA values do not need to be\n        instances of `type`.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def kind(self) -> str:\n        \"\"\"\n        A character code (one of 'biufcmMOSUV'), default 'O'\n\n        This should match the NumPy dtype used when the array is\n        converted to an ndarray, which is probably 'O' for object if\n        the extension type cannot be represented as a built-in NumPy\n        type.\n\n        See Also\n        --------\n        numpy.dtype.kind\n        \"\"\"\n        return 'O'\n\n    @property\n    def name(self) -> str:\n        \"\"\"\n        A string identifying the data type.\n\n        Will be used for display in, e.g. ``Series.dtype``\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def names(self) -> list[str] | None:\n        \"\"\"\n        Ordered list of field names, or None if there are no fields.\n\n        This is for compatibility with NumPy arrays, and may be removed in the\n        future.\n        \"\"\"\n        return None\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[ExtensionArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    def empty(self, shape: Shape) -> ExtensionArray:\n        \"\"\"\n        Construct an ExtensionArray of this dtype with the given shape.\n\n        Analogous to numpy.empty.\n\n        Parameters\n        ----------\n        shape : int or tuple[int]\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        cls = self.construct_array_type()\n        return cls._empty(shape, dtype=self)\n\n    @classmethod\n    def construct_from_string(cls, string: str) -> Self:\n        \"\"\"\n        Construct this type from a string.\n\n        This is useful mainly for data types that accept parameters.\n        For example, a period dtype accepts a frequency parameter that\n        can be set as ``period[h]`` (where H means hourly frequency).\n\n        By default, in the abstract class, just the name of the type is\n        expected. But subclasses can overwrite this method to accept\n        parameters.\n\n        Parameters\n        ----------\n        string : str\n            The name of the type, for example ``category``.\n\n        Returns\n        -------\n        ExtensionDtype\n            Instance of the dtype.\n\n        Raises\n        ------\n        TypeError\n            If a class cannot be constructed from this 'string'.\n\n        Examples\n        --------\n        For extension dtypes with arguments the following may be an\n        adequate implementation.\n\n        >>> import re\n        >>> @classmethod\n        ... def construct_from_string(cls, string):\n        ...     pattern = re.compile(r\"^my_type\\\\[(?P<arg_name>.+)\\\\]$\")\n        ...     match = pattern.match(string)\n        ...     if match:\n        ...         return cls(**match.groupdict())\n        ...     else:\n        ...         raise TypeError(\n        ...             f\"Cannot construct a '{cls.__name__}' from '{string}'\"\n        ...         )\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(f\"'construct_from_string' expects a string, got {type(string)}\")\n        assert isinstance(cls.name, str), (cls, type(cls.name))\n        if string != cls.name:\n            raise TypeError(f\"Cannot construct a '{cls.__name__}' from '{string}'\")\n        return cls()\n\n    @classmethod\n    def is_dtype(cls, dtype: object) -> bool:\n        \"\"\"\n        Check if we match 'dtype'.\n\n        Parameters\n        ----------\n        dtype : object\n            The object to check.\n\n        Returns\n        -------\n        bool\n\n        Notes\n        -----\n        The default implementation is True if\n\n        1. ``cls.construct_from_string(dtype)`` is an instance\n           of ``cls``.\n        2. ``dtype`` is an object and is an instance of ``cls``\n        3. ``dtype`` has a ``dtype`` attribute, and any of the above\n           conditions is true for ``dtype.dtype``.\n        \"\"\"\n        dtype = getattr(dtype, 'dtype', dtype)\n        if isinstance(dtype, (ABCSeries, ABCIndex, ABCDataFrame, np.dtype)):\n            return False\n        elif dtype is None:\n            return False\n        elif isinstance(dtype, cls):\n            return True\n        if isinstance(dtype, str):\n            try:\n                return cls.construct_from_string(dtype) is not None\n            except TypeError:\n                return False\n        return False\n\n    @property\n    def _is_numeric(self) -> bool:\n        \"\"\"\n        Whether columns with this dtype should be considered numeric.\n\n        By default ExtensionDtypes are assumed to be non-numeric.\n        They'll be excluded from operations that exclude non-numeric\n        columns, like (groupby) reductions, plotting, etc.\n        \"\"\"\n        return False\n\n    @property\n    def _is_boolean(self) -> bool:\n        \"\"\"\n        Whether this dtype should be considered boolean.\n\n        By default, ExtensionDtypes are assumed to be non-numeric.\n        Setting this to True will affect the behavior of several places,\n        e.g.\n\n        * is_bool\n        * boolean indexing\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return False\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        \"\"\"\n        Return the common dtype, if one exists.\n\n        Used in `find_common_type` implementation. This is for example used\n        to determine the resulting dtype in a concat operation.\n\n        If no common dtype exists, return None (which gives the other dtypes\n        the chance to determine a common dtype). If all dtypes in the list\n        return None, then the common dtype will be \"object\" dtype (this means\n        it is never needed to return \"object\" dtype from this method itself).\n\n        Parameters\n        ----------\n        dtypes : list of dtypes\n            The dtypes for which to determine a common dtype. This is a list\n            of np.dtype or ExtensionDtype instances.\n\n        Returns\n        -------\n        Common dtype (np.dtype or ExtensionDtype) or None\n        \"\"\"\n        if len(set(dtypes)) == 1:\n            return self\n        else:\n            return None\n\n    @property\n    def _can_hold_na(self) -> bool:\n        \"\"\"\n        Can arrays of this dtype hold NA values?\n        \"\"\"\n        return True\n\n    @property\n    def _is_immutable(self) -> bool:\n        \"\"\"\n        Can arrays with this dtype be modified with __setitem__? If not, return\n        True.\n\n        Immutable arrays are expected to raise TypeError on __setitem__ calls.\n        \"\"\"\n        return False\n\n    @cache_readonly\n    def index_class(self) -> type_t[Index]:\n        \"\"\"\n        The Index subclass to return from Index.__new__ when this dtype is\n        encountered.\n        \"\"\"\n        from pandas import Index\n        return Index\n\n    @property\n    def _supports_2d(self) -> bool:\n        \"\"\"\n        Do ExtensionArrays with this dtype support 2D arrays?\n\n        Historically ExtensionArrays were limited to 1D. By returning True here,\n        authors can indicate that their arrays support 2D instances. This can\n        improve performance in some cases, particularly operations with `axis=1`.\n\n        Arrays that support 2D values should:\n\n            - implement Array.reshape\n            - subclass the Dim2CompatTests in tests.extension.base\n            - _concat_same_type should support `axis` keyword\n            - _reduce and reductions should support `axis` keyword\n        \"\"\"\n        return False\n\n    @property\n    def _can_fast_transpose(self) -> bool:\n        \"\"\"\n        Is transposing an array with this dtype zero-copy?\n\n        Only relevant for cases where _supports_2d is True.\n        \"\"\"\n        return False", "class_fn": true, "question_id": "pandas/pandas.core.dtypes.base/ExtensionDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/dtypes/dtypes.py", "fn_id": "", "content": "class BaseMaskedDtype(ExtensionDtype):\n    \"\"\"\n    Base class for dtypes for BaseMaskedArray subclasses.\n    \"\"\"\n    base = None\n    type: type\n\n    @property\n    def na_value(self) -> libmissing.NAType:\n        return libmissing.NA\n\n    @cache_readonly\n    def numpy_dtype(self) -> np.dtype:\n        \"\"\"Return an instance of our numpy dtype\"\"\"\n        return np.dtype(self.type)\n\n    @cache_readonly\n    def kind(self) -> str:\n        return self.numpy_dtype.kind\n\n    @cache_readonly\n    def itemsize(self) -> int:\n        \"\"\"Return the number of bytes in this dtype\"\"\"\n        return self.numpy_dtype.itemsize\n\n    def __str__(self) -> str:\n        return self.name\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        from pandas.core.dtypes.cast import find_common_type\n        new_dtype = find_common_type([dtype.numpy_dtype if isinstance(dtype, BaseMaskedDtype) else dtype for dtype in dtypes])\n        if not isinstance(new_dtype, np.dtype):\n            return None\n        try:\n            return type(self).from_numpy_dtype(new_dtype)\n        except (KeyError, NotImplementedError):\n            return None\n\n    def __ne__(self, other: object) -> bool:\n        return not self.__eq__(other)\n\n    @classmethod\n    def from_numpy_dtype(cls, dtype: np.dtype) -> BaseMaskedDtype:\n        \"\"\"\n        Construct the MaskedDtype corresponding to the given numpy dtype.\n        \"\"\"\n        if dtype.kind == 'b':\n            from pandas.core.arrays.boolean import BooleanDtype\n            return BooleanDtype()\n        elif dtype.kind in 'iu':\n            from pandas.core.arrays.integer import NUMPY_INT_TO_DTYPE\n            return NUMPY_INT_TO_DTYPE[dtype]\n        elif dtype.kind == 'f':\n            from pandas.core.arrays.floating import NUMPY_FLOAT_TO_DTYPE\n            return NUMPY_FLOAT_TO_DTYPE[dtype]\n        else:\n            raise NotImplementedError(dtype)\n\n    def __hash__(self) -> int:\n        return object_hash(tuple((getattr(self, attr) for attr in self._metadata)))\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[BaseMaskedArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        raise NotImplementedError", "class_fn": true, "question_id": "pandas/pandas.core.dtypes.dtypes/BaseMaskedDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/dtypes/dtypes.py", "fn_id": "", "content": "@register_extension_dtype\nclass DatetimeTZDtype(PandasExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for timezone-aware datetime data.\n\n    **This is not an actual numpy dtype**, but a duck type.\n\n    Parameters\n    ----------\n    unit : str, default \"ns\"\n        The precision of the datetime data. Currently limited\n        to ``\"ns\"``.\n    tz : str, int, or datetime.tzinfo\n        The timezone.\n\n    Attributes\n    ----------\n    unit\n    tz\n\n    Methods\n    -------\n    None\n\n    Raises\n    ------\n    ZoneInfoNotFoundError\n        When the requested timezone cannot be found.\n\n    Examples\n    --------\n    >>> from zoneinfo import ZoneInfo\n    >>> pd.DatetimeTZDtype(tz=ZoneInfo('UTC'))\n    datetime64[ns, UTC]\n\n    >>> pd.DatetimeTZDtype(tz=ZoneInfo('Europe/Paris'))\n    datetime64[ns, Europe/Paris]\n    \"\"\"\n    type: type[Timestamp] = Timestamp\n    kind: str_type = 'M'\n    num = 101\n    _metadata = ('unit', 'tz')\n    _match = re.compile('(datetime64|M8)\\\\[(?P<unit>.+), (?P<tz>.+)\\\\]')\n    _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}\n    _supports_2d = True\n    _can_fast_transpose = True\n\n    @property\n    def na_value(self) -> NaTType:\n        return NaT\n\n    @cache_readonly\n    def base(self) -> DtypeObj:\n        return np.dtype(f'M8[{self.unit}]')\n\n    @cache_readonly\n    def str(self) -> str:\n        return f'|M8[{self.unit}]'\n\n    @classmethod\n    def construct_from_string(cls, string: str_type) -> DatetimeTZDtype:\n        \"\"\"\n        Construct a DatetimeTZDtype from a string.\n\n        Parameters\n        ----------\n        string : str\n            The string alias for this DatetimeTZDtype.\n            Should be formatted like ``datetime64[ns, <tz>]``,\n            where ``<tz>`` is the timezone name.\n\n        Examples\n        --------\n        >>> DatetimeTZDtype.construct_from_string('datetime64[ns, UTC]')\n        datetime64[ns, UTC]\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(f\"'construct_from_string' expects a string, got {type(string)}\")\n        msg = f\"Cannot construct a 'DatetimeTZDtype' from '{string}'\"\n        match = cls._match.match(string)\n        if match:\n            d = match.groupdict()\n            try:\n                return cls(unit=d['unit'], tz=d['tz'])\n            except (KeyError, TypeError, ValueError) as err:\n                raise TypeError(msg) from err\n        raise TypeError(msg)\n\n    @cache_readonly\n    def _creso(self) -> int:\n        \"\"\"\n        The NPY_DATETIMEUNIT corresponding to this dtype's resolution.\n        \"\"\"\n        return abbrev_to_npy_unit(self.unit)\n\n    @property\n    def unit(self) -> str_type:\n        \"\"\"\n        The precision of the datetime data.\n\n        Examples\n        --------\n        >>> from zoneinfo import ZoneInfo\n        >>> dtype = pd.DatetimeTZDtype(tz=ZoneInfo('America/Los_Angeles'))\n        >>> dtype.unit\n        'ns'\n        \"\"\"\n        return self._unit\n\n    @property\n    def tz(self) -> tzinfo:\n        \"\"\"\n        The timezone.\n\n        Examples\n        --------\n        >>> from zoneinfo import ZoneInfo\n        >>> dtype = pd.DatetimeTZDtype(tz=ZoneInfo('America/Los_Angeles'))\n        >>> dtype.tz\n        zoneinfo.ZoneInfo(key='America/Los_Angeles')\n        \"\"\"\n        return self._tz\n\n    def __repr__(self) -> str_type:\n        \"\"\"\n        Return a string representation for a particular object.\n        \"\"\"\n        return str(self)\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        if all((isinstance(t, DatetimeTZDtype) and t.tz == self.tz for t in dtypes)):\n            np_dtype = np.max([cast(DatetimeTZDtype, t).base for t in [self, *dtypes]])\n            unit = np.datetime_data(np_dtype)[0]\n            return type(self)(unit=unit, tz=self.tz)\n        return super()._get_common_dtype(dtypes)\n\n    def __setstate__(self, state) -> None:\n        self._tz = state['tz']\n        self._unit = state['unit']\n\n    @property\n    def name(self) -> str_type:\n        \"\"\"A string representation of the dtype.\"\"\"\n        return str(self)\n\n    def __str__(self) -> str_type:\n        return f'datetime64[{self.unit}, {self.tz}]'\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[DatetimeArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays import DatetimeArray\n        return DatetimeArray\n\n    def __hash__(self) -> int:\n        return hash(str(self))\n\n    def __from_arrow__(self, array: pa.Array | pa.ChunkedArray) -> DatetimeArray:\n        \"\"\"\n        Construct DatetimeArray from pyarrow Array/ChunkedArray.\n\n        Note: If the units in the pyarrow Array are the same as this\n        DatetimeDtype, then values corresponding to the integer representation\n        of ``NaT`` (e.g. one nanosecond before :attr:`pandas.Timestamp.min`)\n        are converted to ``NaT``, regardless of the null indicator in the\n        pyarrow array.\n\n        Parameters\n        ----------\n        array : pyarrow.Array or pyarrow.ChunkedArray\n            The Arrow array to convert to DatetimeArray.\n\n        Returns\n        -------\n        extension array : DatetimeArray\n        \"\"\"\n        import pyarrow\n        from pandas.core.arrays import DatetimeArray\n        array = array.cast(pyarrow.timestamp(unit=self._unit), safe=True)\n        if isinstance(array, pyarrow.Array):\n            np_arr = array.to_numpy(zero_copy_only=False)\n        else:\n            np_arr = array.to_numpy()\n        return DatetimeArray._simple_new(np_arr, dtype=self)\n\n    def __getstate__(self) -> dict[str_type, Any]:\n        return {k: getattr(self, k, None) for k in self._metadata}\n\n    @cache_readonly\n    def index_class(self) -> type_t[DatetimeIndex]:\n        from pandas import DatetimeIndex\n        return DatetimeIndex\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, str):\n            if other.startswith('M8['):\n                other = f'datetime64[{other[3:]}'\n            return other == self.name\n        return isinstance(other, DatetimeTZDtype) and self.unit == other.unit and tz_compare(self.tz, other.tz)\n\n    def __init__(self, unit: str_type | DatetimeTZDtype='ns', tz=None) -> None:\n        if isinstance(unit, DatetimeTZDtype):\n            (unit, tz) = (unit.unit, unit.tz)\n        if unit != 'ns':\n            if isinstance(unit, str) and tz is None:\n                result = type(self).construct_from_string(unit)\n                unit = result.unit\n                tz = result.tz\n                msg = f\"Passing a dtype alias like 'datetime64[ns, {tz}]' to DatetimeTZDtype is no longer supported. Use 'DatetimeTZDtype.construct_from_string()' instead.\"\n                raise ValueError(msg)\n            if unit not in ['s', 'ms', 'us', 'ns']:\n                raise ValueError('DatetimeTZDtype only supports s, ms, us, ns units')\n        if tz:\n            tz = timezones.maybe_get_tz(tz)\n            tz = timezones.tz_standardize(tz)\n        elif tz is not None:\n            raise pytz.UnknownTimeZoneError(tz)\n        if tz is None:\n            raise TypeError(\"A 'tz' is required.\")\n        self._unit = unit\n        self._tz = tz", "class_fn": true, "question_id": "pandas/pandas.core.dtypes.dtypes/DatetimeTZDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/dtypes/dtypes.py", "fn_id": "", "content": "@register_extension_dtype\nclass PeriodDtype(PeriodDtypeBase, PandasExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for Period data.\n\n    **This is not an actual numpy dtype**, but a duck type.\n\n    Parameters\n    ----------\n    freq : str or DateOffset\n        The frequency of this PeriodDtype.\n\n    Attributes\n    ----------\n    freq\n\n    Methods\n    -------\n    None\n\n    Examples\n    --------\n    >>> pd.PeriodDtype(freq='D')\n    period[D]\n\n    >>> pd.PeriodDtype(freq=pd.offsets.MonthEnd())\n    period[M]\n    \"\"\"\n    type: type[Period] = Period\n    kind: str_type = 'O'\n    str = '|O08'\n    base = np.dtype('O')\n    num = 102\n    _metadata = ('freq',)\n    _match = re.compile('(P|p)eriod\\\\[(?P<freq>.+)\\\\]')\n    _cache_dtypes: dict[BaseOffset, int] = {}\n    __hash__ = PeriodDtypeBase.__hash__\n    _freq: BaseOffset\n    _supports_2d = True\n    _can_fast_transpose = True\n\n    def __ne__(self, other: object) -> bool:\n        return not self.__eq__(other)\n\n    def __new__(cls, freq) -> PeriodDtype:\n        \"\"\"\n        Parameters\n        ----------\n        freq : PeriodDtype, BaseOffset, or string\n        \"\"\"\n        if isinstance(freq, PeriodDtype):\n            return freq\n        if not isinstance(freq, BaseOffset):\n            freq = cls._parse_dtype_strict(freq)\n        if isinstance(freq, BDay):\n            warnings.warn(\"PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\", FutureWarning, stacklevel=find_stack_level())\n        try:\n            dtype_code = cls._cache_dtypes[freq]\n        except KeyError:\n            dtype_code = freq._period_dtype_code\n            cls._cache_dtypes[freq] = dtype_code\n        u = PeriodDtypeBase.__new__(cls, dtype_code, freq.n)\n        u._freq = freq\n        return u\n\n    @property\n    def freq(self) -> BaseOffset:\n        \"\"\"\n        The frequency object of this PeriodDtype.\n\n        Examples\n        --------\n        >>> dtype = pd.PeriodDtype(freq='D')\n        >>> dtype.freq\n        <Day>\n        \"\"\"\n        return self._freq\n\n    def __reduce__(self) -> tuple[type_t[Self], tuple[str_type]]:\n        return (type(self), (self.name,))\n\n    def __from_arrow__(self, array: pa.Array | pa.ChunkedArray) -> PeriodArray:\n        \"\"\"\n        Construct PeriodArray from pyarrow Array/ChunkedArray.\n        \"\"\"\n        import pyarrow\n        from pandas.core.arrays import PeriodArray\n        from pandas.core.arrays.arrow._arrow_utils import pyarrow_array_to_numpy_and_mask\n        if isinstance(array, pyarrow.Array):\n            chunks = [array]\n        else:\n            chunks = array.chunks\n        results = []\n        for arr in chunks:\n            (data, mask) = pyarrow_array_to_numpy_and_mask(arr, dtype=np.dtype(np.int64))\n            parr = PeriodArray(data.copy(), dtype=self, copy=False)\n            parr[~mask] = NaT\n            results.append(parr)\n        if not results:\n            return PeriodArray(np.array([], dtype='int64'), dtype=self, copy=False)\n        return PeriodArray._concat_same_type(results)\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[PeriodArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays import PeriodArray\n        return PeriodArray\n\n    @property\n    def name(self) -> str_type:\n        return f'period[{self._freqstr}]'\n\n    @property\n    def na_value(self) -> NaTType:\n        return NaT\n\n    def __getstate__(self) -> dict[str_type, Any]:\n        return {k: getattr(self, k, None) for k in self._metadata}\n\n    def __str__(self) -> str_type:\n        return self.name\n\n    @classmethod\n    def is_dtype(cls, dtype: object) -> bool:\n        \"\"\"\n        Return a boolean if we if the passed type is an actual dtype that we\n        can match (via string or type)\n        \"\"\"\n        if isinstance(dtype, str):\n            if dtype.startswith(('period[', 'Period[')):\n                try:\n                    return cls._parse_dtype_strict(dtype) is not None\n                except ValueError:\n                    return False\n            else:\n                return False\n        return super().is_dtype(dtype)\n\n    @classmethod\n    def _parse_dtype_strict(cls, freq: str_type) -> BaseOffset:\n        if isinstance(freq, str):\n            if freq.startswith(('Period[', 'period[')):\n                m = cls._match.search(freq)\n                if m is not None:\n                    freq = m.group('freq')\n            freq_offset = to_offset(freq, is_period=True)\n            if freq_offset is not None:\n                return freq_offset\n        raise TypeError(f'PeriodDtype argument should be string or BaseOffset, got {type(freq).__name__}')\n\n    @classmethod\n    def construct_from_string(cls, string: str_type) -> PeriodDtype:\n        \"\"\"\n        Strict construction from a string, raise a TypeError if not\n        possible\n        \"\"\"\n        if isinstance(string, str) and string.startswith(('period[', 'Period[')) or isinstance(string, BaseOffset):\n            try:\n                return cls(freq=string)\n            except ValueError:\n                pass\n        if isinstance(string, str):\n            msg = f\"Cannot construct a 'PeriodDtype' from '{string}'\"\n        else:\n            msg = f\"'construct_from_string' expects a string, got {type(string)}\"\n        raise TypeError(msg)\n\n    @cache_readonly\n    def index_class(self) -> type_t[PeriodIndex]:\n        from pandas import PeriodIndex\n        return PeriodIndex\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, str):\n            return other in [self.name, capitalize_first_letter(self.name)]\n        return super().__eq__(other)\n\n    def __repr__(self) -> str_type:\n        \"\"\"\n        Return a string representation for a particular object.\n        \"\"\"\n        return str(self)", "class_fn": true, "question_id": "pandas/pandas.core.dtypes.dtypes/PeriodDtype", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/frame.py", "fn_id": "", "content": "class DataFrame(NDFrame, OpsMixin):\n    \"\"\"\n    Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).\n    Arithmetic operations align on both row and column labels. Can be\n    thought of as a dict-like container for Series objects. The primary\n    pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n        data is a dict, column order follows insertion-order. If a dict contains Series\n        which have an index defined, it is aligned by its index. This alignment also\n        occurs if data is a Series or a DataFrame itself. Alignment is done on\n        Series/DataFrame inputs.\n\n        If data is a list of dicts, column order follows insertion-order.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided.\n    columns : Index or array-like\n        Column labels to use for resulting frame when data does not have them,\n        defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n        will perform column selection instead.\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer.\n    copy : bool or None, default None\n        Copy data from inputs.\n        For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n        or 2d ndarray input, the default of None behaves like ``copy=False``.\n        If data is a dict containing one or more Series (possibly of different dtypes),\n        ``copy=False`` will ensure that these inputs are not copied.\n\n        .. versionchanged:: 1.3.0\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    read_csv : Read a comma-separated values (csv) file into DataFrame.\n    read_table : Read general delimited file into DataFrame.\n    read_clipboard : Read text from clipboard into DataFrame.\n\n    Notes\n    -----\n    Please reference the :ref:`User Guide <basics.dataframe>` for more information.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from a dictionary including Series:\n\n    >>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n    >>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n       col1  col2\n    0     0   NaN\n    1     1   NaN\n    2     2   2.0\n    3     3   3.0\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n\n    Constructing DataFrame from a numpy ndarray that has labeled columns:\n\n    >>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n    ...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n    >>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n    ...\n    >>> df3\n       c  a\n    0  3  1\n    1  6  4\n    2  9  7\n\n    Constructing DataFrame from dataclass:\n\n    >>> from dataclasses import make_dataclass\n    >>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n    >>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n       x  y\n    0  0  0\n    1  0  3\n    2  2  3\n\n    Constructing DataFrame from Series/DataFrame:\n\n    >>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n    >>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n    >>> df\n       0\n    a  1\n    c  3\n\n    >>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n    >>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n    >>> df2\n       x\n    a  1\n    c  3\n    \"\"\"\n    _internal_names_set = {'columns', 'index'} | NDFrame._internal_names_set\n    _typ = 'dataframe'\n    _HANDLED_TYPES = (Series, Index, ExtensionArray, np.ndarray)\n    _accessors: set[str] = {'sparse'}\n    _hidden_attrs: frozenset[str] = NDFrame._hidden_attrs | frozenset([])\n    _mgr: BlockManager | ArrayManager\n    __pandas_priority__ = 4000\n\n    @property\n    def _constructor(self) -> Callable[..., DataFrame]:\n        return DataFrame\n\n    def __len__(self) -> int:\n        \"\"\"\n        Returns length of info axis, but here we use the index.\n        \"\"\"\n        return len(self.index)\n    _constructor_sliced: Callable[..., Series] = Series\n\n    @Appender(dedent('\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'Animal\\': [\\'Falcon\\', \\'Falcon\\',\\n        ...                               \\'Parrot\\', \\'Parrot\\'],\\n        ...                    \\'Max Speed\\': [380., 370., 24., 26.]})\\n        >>> df\\n           Animal  Max Speed\\n        0  Falcon      380.0\\n        1  Falcon      370.0\\n        2  Parrot       24.0\\n        3  Parrot       26.0\\n        >>> df.groupby([\\'Animal\\']).mean()\\n                Max Speed\\n        Animal\\n        Falcon      375.0\\n        Parrot       25.0\\n\\n        **Hierarchical Indexes**\\n\\n        We can groupby different levels of a hierarchical index\\n        using the `level` parameter:\\n\\n        >>> arrays = [[\\'Falcon\\', \\'Falcon\\', \\'Parrot\\', \\'Parrot\\'],\\n        ...           [\\'Captive\\', \\'Wild\\', \\'Captive\\', \\'Wild\\']]\\n        >>> index = pd.MultiIndex.from_arrays(arrays, names=(\\'Animal\\', \\'Type\\'))\\n        >>> df = pd.DataFrame({\\'Max Speed\\': [390., 350., 30., 20.]},\\n        ...                   index=index)\\n        >>> df\\n                        Max Speed\\n        Animal Type\\n        Falcon Captive      390.0\\n               Wild         350.0\\n        Parrot Captive       30.0\\n               Wild          20.0\\n        >>> df.groupby(level=0).mean()\\n                Max Speed\\n        Animal\\n        Falcon      370.0\\n        Parrot       25.0\\n        >>> df.groupby(level=\"Type\").mean()\\n                 Max Speed\\n        Type\\n        Captive      210.0\\n        Wild         185.0\\n\\n        We can also choose to include NA in group keys or not by setting\\n        `dropna` parameter, the default setting is `True`.\\n\\n        >>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\\n        >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\\n\\n        >>> df.groupby(by=[\"b\"]).sum()\\n            a   c\\n        b\\n        1.0 2   3\\n        2.0 2   5\\n\\n        >>> df.groupby(by=[\"b\"], dropna=False).sum()\\n            a   c\\n        b\\n        1.0 2   3\\n        2.0 2   5\\n        NaN 1   4\\n\\n        >>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\\n        >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\\n\\n        >>> df.groupby(by=\"a\").sum()\\n            b     c\\n        a\\n        a   13.0   13.0\\n        b   12.3  123.0\\n\\n        >>> df.groupby(by=\"a\", dropna=False).sum()\\n            b     c\\n        a\\n        a   13.0   13.0\\n        b   12.3  123.0\\n        NaN 12.3   33.0\\n\\n        When using ``.apply()``, use ``group_keys`` to include or exclude the\\n        group keys. The ``group_keys`` argument defaults to ``True`` (include).\\n\\n        >>> df = pd.DataFrame({\\'Animal\\': [\\'Falcon\\', \\'Falcon\\',\\n        ...                               \\'Parrot\\', \\'Parrot\\'],\\n        ...                    \\'Max Speed\\': [380., 370., 24., 26.]})\\n        >>> df.groupby(\"Animal\", group_keys=True)[[\\'Max Speed\\']].apply(lambda x: x)\\n                  Max Speed\\n        Animal\\n        Falcon 0      380.0\\n               1      370.0\\n        Parrot 2       24.0\\n               3       26.0\\n\\n        >>> df.groupby(\"Animal\", group_keys=False)[[\\'Max Speed\\']].apply(lambda x: x)\\n           Max Speed\\n        0      380.0\\n        1      370.0\\n        2       24.0\\n        3       26.0\\n        '))\n    @Appender(_shared_docs['groupby'] % _shared_doc_kwargs)\n    def groupby(self, by=None, axis: Axis | lib.NoDefault=lib.no_default, level: IndexLabel | None=None, as_index: bool=True, sort: bool=True, group_keys: bool=True, observed: bool | lib.NoDefault=lib.no_default, dropna: bool=True) -> DataFrameGroupBy:\n        if axis is not lib.no_default:\n            axis = self._get_axis_number(axis)\n            if axis == 1:\n                warnings.warn('DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.', FutureWarning, stacklevel=find_stack_level())\n            else:\n                warnings.warn(\"The 'axis' keyword in DataFrame.groupby is deprecated and will be removed in a future version.\", FutureWarning, stacklevel=find_stack_level())\n        else:\n            axis = 0\n        from pandas.core.groupby.generic import DataFrameGroupBy\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n        return DataFrameGroupBy(obj=self, keys=by, axis=axis, level=level, as_index=as_index, sort=sort, group_keys=group_keys, observed=observed, dropna=dropna)\n\n    @doc(make_doc('prod', ndim=2))\n    def prod(self, axis: Axis | None=0, skipna: bool=True, numeric_only: bool=False, min_count: int=0, **kwargs):\n        result = super().prod(axis, skipna, numeric_only, min_count, **kwargs)\n        return result.__finalize__(self, method='prod')\n\n    def _construct_result(self, result) -> DataFrame:\n        \"\"\"\n        Wrap the result of an arithmetic, comparison, or logical operation.\n\n        Parameters\n        ----------\n        result : DataFrame\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        out = self._constructor(result, copy=False).__finalize__(self)\n        out.columns = self.columns\n        out.index = self.index\n        return out\n\n    @Appender(ops.make_flex_doc('rpow', 'dataframe'))\n    def rpow(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, roperator.rpow, level=level, fill_value=fill_value, axis=axis)\n\n    def to_records(self, index: bool=True, column_dtypes=None, index_dtypes=None) -> np.rec.recarray:\n        \"\"\"\n        Convert DataFrame to a NumPy record array.\n\n        Index will be included as the first field of the record array if\n        requested.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Include index in resulting record array, stored in 'index'\n            field or using the index label, if set.\n        column_dtypes : str, type, dict, default None\n            If a string or type, the data type to store all columns. If\n            a dictionary, a mapping of column names and indices (zero-indexed)\n            to specific data types.\n        index_dtypes : str, type, dict, default None\n            If a string or type, the data type to store all index levels. If\n            a dictionary, a mapping of index level names and indices\n            (zero-indexed) to specific data types.\n\n            This mapping is applied only if `index=True`.\n\n        Returns\n        -------\n        numpy.rec.recarray\n            NumPy ndarray with the DataFrame labels as fields and each row\n            of the DataFrame as entries.\n\n        See Also\n        --------\n        DataFrame.from_records: Convert structured or record ndarray\n            to DataFrame.\n        numpy.rec.recarray: An ndarray that allows field access using\n            attributes, analogous to typed columns in a\n            spreadsheet.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n        ...                   index=['a', 'b'])\n        >>> df\n           A     B\n        a  1  0.50\n        b  2  0.75\n        >>> df.to_records()\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])\n\n        If the DataFrame index has no label then the recarray field name\n        is set to 'index'. If the index has a label then this is used as the\n        field name:\n\n        >>> df.index = df.index.rename(\"I\")\n        >>> df.to_records()\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')])\n\n        The index can be excluded from the record array:\n\n        >>> df.to_records(index=False)\n        rec.array([(1, 0.5 ), (2, 0.75)],\n                  dtype=[('A', '<i8'), ('B', '<f8')])\n\n        Data types can be specified for the columns:\n\n        >>> df.to_records(column_dtypes={\"A\": \"int32\"})\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')])\n\n        As well as for the index:\n\n        >>> df.to_records(index_dtypes=\"<S2\")\n        rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n                  dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')])\n\n        >>> index_dtypes = f\"<S{df.index.str.len().max()}\"\n        >>> df.to_records(index_dtypes=index_dtypes)\n        rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n                  dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')])\n        \"\"\"\n        if index:\n            ix_vals = [np.asarray(self.index.get_level_values(i)) for i in range(self.index.nlevels)]\n            arrays = ix_vals + [np.asarray(self.iloc[:, i]) for i in range(len(self.columns))]\n            index_names = list(self.index.names)\n            if isinstance(self.index, MultiIndex):\n                index_names = com.fill_missing_names(index_names)\n            elif index_names[0] is None:\n                index_names = ['index']\n            names = [str(name) for name in itertools.chain(index_names, self.columns)]\n        else:\n            arrays = [np.asarray(self.iloc[:, i]) for i in range(len(self.columns))]\n            names = [str(c) for c in self.columns]\n            index_names = []\n        index_len = len(index_names)\n        formats = []\n        for (i, v) in enumerate(arrays):\n            index_int = i\n            if index_int < index_len:\n                dtype_mapping = index_dtypes\n                name = index_names[index_int]\n            else:\n                index_int -= index_len\n                dtype_mapping = column_dtypes\n                name = self.columns[index_int]\n            if is_dict_like(dtype_mapping):\n                if name in dtype_mapping:\n                    dtype_mapping = dtype_mapping[name]\n                elif index_int in dtype_mapping:\n                    dtype_mapping = dtype_mapping[index_int]\n                else:\n                    dtype_mapping = None\n            if dtype_mapping is None:\n                formats.append(v.dtype)\n            elif isinstance(dtype_mapping, (type, np.dtype, str)):\n                formats.append(dtype_mapping)\n            else:\n                element = 'row' if i < index_len else 'column'\n                msg = f'Invalid dtype {dtype_mapping} specified for {element} {name}'\n                raise ValueError(msg)\n        return np.rec.fromarrays(arrays, dtype={'names': names, 'formats': formats})\n\n    @property\n    def axes(self) -> list[Index]:\n        \"\"\"\n        Return a list representing the axes of the DataFrame.\n\n        It has the row axis labels and column axis labels as the only members.\n        They are returned in that order.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.axes\n        [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'],\n        dtype='object')]\n        \"\"\"\n        return [self.index, self.columns]\n\n    @property\n    def shape(self) -> tuple[int, int]:\n        \"\"\"\n        Return a tuple representing the dimensionality of the DataFrame.\n\n        See Also\n        --------\n        ndarray.shape : Tuple of array dimensions.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.shape\n        (2, 2)\n\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4],\n        ...                    'col3': [5, 6]})\n        >>> df.shape\n        (2, 3)\n        \"\"\"\n        return (len(self.index), len(self.columns))\n\n    @property\n    def _is_homogeneous_type(self) -> bool:\n        \"\"\"\n        Whether all the columns in a DataFrame have the same type.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3, 4]})._is_homogeneous_type\n        True\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.0]})._is_homogeneous_type\n        False\n\n        Items with the same type but different sizes are considered\n        different types.\n\n        >>> DataFrame({\n        ...    \"A\": np.array([1, 2], dtype=np.int32),\n        ...    \"B\": np.array([1, 2], dtype=np.int64)})._is_homogeneous_type\n        False\n        \"\"\"\n        return len({arr.dtype for arr in self._mgr.arrays}) <= 1\n\n    @property\n    def _can_fast_transpose(self) -> bool:\n        \"\"\"\n        Can we transpose this DataFrame without creating any new array objects.\n        \"\"\"\n        if isinstance(self._mgr, ArrayManager):\n            return False\n        blocks = self._mgr.blocks\n        if len(blocks) != 1:\n            return False\n        dtype = blocks[0].dtype\n        return not is_1d_only_ea_dtype(dtype)\n\n    @property\n    def _values(self) -> np.ndarray | DatetimeArray | TimedeltaArray | PeriodArray:\n        \"\"\"\n        Analogue to ._values that may return a 2D ExtensionArray.\n        \"\"\"\n        mgr = self._mgr\n        if isinstance(mgr, ArrayManager):\n            if len(mgr.arrays) == 1 and (not is_1d_only_ea_dtype(mgr.arrays[0].dtype)):\n                return mgr.arrays[0].reshape(-1, 1)\n            return ensure_wrapped_if_datetimelike(self.values)\n        blocks = mgr.blocks\n        if len(blocks) != 1:\n            return ensure_wrapped_if_datetimelike(self.values)\n        arr = blocks[0].values\n        if arr.ndim == 1:\n            return self.values\n        arr = cast('np.ndarray | DatetimeArray | TimedeltaArray | PeriodArray', arr)\n        return arr.T\n\n    def _set_item_mgr(self, key, value: ArrayLike, refs: BlockValuesRefs | None=None) -> None:\n        try:\n            loc = self._info_axis.get_loc(key)\n        except KeyError:\n            self._mgr.insert(len(self._info_axis), key, value, refs)\n        else:\n            self._iset_item_mgr(loc, value, refs=refs)\n        if len(self):\n            self._check_setitem_copy()\n\n    @Appender(ops.make_flex_doc('sub', 'dataframe'))\n    def sub(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, operator.sub, level=level, fill_value=fill_value, axis=axis)\n\n    def drop(self, labels: IndexLabel | None=None, *, axis: Axis=0, index: IndexLabel | None=None, columns: IndexLabel | None=None, level: Level | None=None, inplace: bool=False, errors: IgnoreRaise='raise') -> DataFrame | None:\n        \"\"\"\n        Drop specified labels from rows or columns.\n\n        Remove rows or columns by specifying label names and corresponding\n        axis, or by directly specifying index or column names. When using a\n        multi-index, labels on different levels can be removed by specifying\n        the level. See the :ref:`user guide <advanced.shown_levels>`\n        for more information about the now unused levels.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index or column labels to drop. A tuple will be used as a single\n            label and not treated as a list-like.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Whether to drop labels from the index (0 or 'index') or\n            columns (1 or 'columns').\n        index : single label or list-like\n            Alternative to specifying axis (``labels, axis=0``\n            is equivalent to ``index=labels``).\n        columns : single label or list-like\n            Alternative to specifying axis (``labels, axis=1``\n            is equivalent to ``columns=labels``).\n        level : int or level name, optional\n            For MultiIndex, level from which the labels will be removed.\n        inplace : bool, default False\n            If False, return a copy. Otherwise, do operation\n            in place and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are\n            dropped.\n\n        Returns\n        -------\n        DataFrame or None\n            Returns DataFrame or None DataFrame with the specified\n            index or column labels removed or None if inplace=True.\n\n        Raises\n        ------\n        KeyError\n            If any of the labels is not found in the selected axis.\n\n        See Also\n        --------\n        DataFrame.loc : Label-location based indexer for selection by label.\n        DataFrame.dropna : Return DataFrame with labels on given axis omitted\n            where (all or any) data are missing.\n        DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n            removed, optionally only considering certain columns.\n        Series.drop : Return Series with specified index labels removed.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n        ...                   columns=['A', 'B', 'C', 'D'])\n        >>> df\n           A  B   C   D\n        0  0  1   2   3\n        1  4  5   6   7\n        2  8  9  10  11\n\n        Drop columns\n\n        >>> df.drop(['B', 'C'], axis=1)\n           A   D\n        0  0   3\n        1  4   7\n        2  8  11\n\n        >>> df.drop(columns=['B', 'C'])\n           A   D\n        0  0   3\n        1  4   7\n        2  8  11\n\n        Drop a row by index\n\n        >>> df.drop([0, 1])\n           A  B   C   D\n        2  8  9  10  11\n\n        Drop columns and/or rows of MultiIndex DataFrame\n\n        >>> midx = pd.MultiIndex(levels=[['llama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n        ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n        ...                         [250, 150], [1.5, 0.8], [320, 250],\n        ...                         [1, 0.8], [0.3, 0.2]])\n        >>> df\n                        big     small\n        llama   speed   45.0    30.0\n                weight  200.0   100.0\n                length  1.5     1.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n                length  1.5     0.8\n        falcon  speed   320.0   250.0\n                weight  1.0     0.8\n                length  0.3     0.2\n\n        Drop a specific index combination from the MultiIndex\n        DataFrame, i.e., drop the combination ``'falcon'`` and\n        ``'weight'``, which deletes only the corresponding row\n\n        >>> df.drop(index=('falcon', 'weight'))\n                        big     small\n        llama   speed   45.0    30.0\n                weight  200.0   100.0\n                length  1.5     1.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n                length  1.5     0.8\n        falcon  speed   320.0   250.0\n                length  0.3     0.2\n\n        >>> df.drop(index='cow', columns='small')\n                        big\n        llama   speed   45.0\n                weight  200.0\n                length  1.5\n        falcon  speed   320.0\n                weight  1.0\n                length  0.3\n\n        >>> df.drop(index='length', level=1)\n                        big     small\n        llama   speed   45.0    30.0\n                weight  200.0   100.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n        falcon  speed   320.0   250.0\n                weight  1.0     0.8\n        \"\"\"\n        return super().drop(labels=labels, axis=axis, index=index, columns=columns, level=level, inplace=inplace, errors=errors)\n\n    @Appender(ops.make_flex_doc('eq', 'dataframe'))\n    def eq(self, other, axis: Axis='columns', level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.eq, axis=axis, level=level)\n\n    def _box_col_values(self, values: SingleDataManager, loc: int) -> Series:\n        \"\"\"\n        Provide boxed values for a column.\n        \"\"\"\n        name = self.columns[loc]\n        obj = self._constructor_sliced_from_mgr(values, axes=values.axes)\n        obj._name = name\n        return obj.__finalize__(self)\n\n    @overload\n    def to_string(self, buf: None=..., columns: Axes | None=..., col_space: int | list[int] | dict[Hashable, int] | None=..., header: bool | SequenceNotStr[str]=..., index: bool=..., na_rep: str=..., formatters: fmt.FormattersType | None=..., float_format: fmt.FloatFormatType | None=..., sparsify: bool | None=..., index_names: bool=..., justify: str | None=..., max_rows: int | None=..., max_cols: int | None=..., show_dimensions: bool=..., decimal: str=..., line_width: int | None=..., min_rows: int | None=..., max_colwidth: int | None=..., encoding: str | None=...) -> str:\n        ...\n\n    @overload\n    def to_string(self, buf: FilePath | WriteBuffer[str], columns: Axes | None=..., col_space: int | list[int] | dict[Hashable, int] | None=..., header: bool | SequenceNotStr[str]=..., index: bool=..., na_rep: str=..., formatters: fmt.FormattersType | None=..., float_format: fmt.FloatFormatType | None=..., sparsify: bool | None=..., index_names: bool=..., justify: str | None=..., max_rows: int | None=..., max_cols: int | None=..., show_dimensions: bool=..., decimal: str=..., line_width: int | None=..., min_rows: int | None=..., max_colwidth: int | None=..., encoding: str | None=...) -> None:\n        ...\n\n    @deprecate_nonkeyword_arguments(version='3.0', allowed_args=['self', 'buf'], name='to_string')\n    @Substitution(header_type='bool or list of str', header='Write out the column names. If a list of columns is given, it is assumed to be aliases for the column names', col_space_type='int, list or dict of int', col_space='The minimum width of each column. If a list of ints is given every integers corresponds with one column. If a dict is given, the key references the column, while the value defines the space to use.')\n    @Substitution(shared_params=fmt.common_docstring, returns=fmt.return_docstring)\n    def to_string(self, buf: FilePath | WriteBuffer[str] | None=None, columns: Axes | None=None, col_space: int | list[int] | dict[Hashable, int] | None=None, header: bool | SequenceNotStr[str]=True, index: bool=True, na_rep: str='NaN', formatters: fmt.FormattersType | None=None, float_format: fmt.FloatFormatType | None=None, sparsify: bool | None=None, index_names: bool=True, justify: str | None=None, max_rows: int | None=None, max_cols: int | None=None, show_dimensions: bool=False, decimal: str='.', line_width: int | None=None, min_rows: int | None=None, max_colwidth: int | None=None, encoding: str | None=None) -> str | None:\n        \"\"\"\n        Render a DataFrame to a console-friendly tabular output.\n        %(shared_params)s\n        line_width : int, optional\n            Width to wrap a line in characters.\n        min_rows : int, optional\n            The number of rows to display in the console in a truncated repr\n            (when number of rows is above `max_rows`).\n        max_colwidth : int, optional\n            Max width to truncate each column in characters. By default, no limit.\n        encoding : str, default \"utf-8\"\n            Set character encoding.\n        %(returns)s\n        See Also\n        --------\n        to_html : Convert DataFrame to HTML.\n\n        Examples\n        --------\n        >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n        >>> df = pd.DataFrame(d)\n        >>> print(df.to_string())\n           col1  col2\n        0     1     4\n        1     2     5\n        2     3     6\n        \"\"\"\n        from pandas import option_context\n        with option_context('display.max_colwidth', max_colwidth):\n            formatter = fmt.DataFrameFormatter(self, columns=columns, col_space=col_space, na_rep=na_rep, formatters=formatters, float_format=float_format, sparsify=sparsify, justify=justify, index_names=index_names, header=header, index=index, min_rows=min_rows, max_rows=max_rows, max_cols=max_cols, show_dimensions=show_dimensions, decimal=decimal)\n            return fmt.DataFrameRenderer(formatter).to_string(buf=buf, encoding=encoding, line_width=line_width)\n\n    @Appender(ops.make_flex_doc('floordiv', 'dataframe'))\n    def floordiv(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, operator.floordiv, level=level, fill_value=fill_value, axis=axis)\n\n    @property\n    def style(self) -> Styler:\n        \"\"\"\n        Returns a Styler object.\n\n        Contains methods for building a styled HTML representation of the DataFrame.\n\n        See Also\n        --------\n        io.formats.style.Styler : Helps style a DataFrame or Series according to the\n            data with HTML and CSS.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3]})\n        >>> df.style  # doctest: +SKIP\n\n        Please see\n        `Table Visualization <../../user_guide/style.ipynb>`_ for more examples.\n        \"\"\"\n        from pandas.io.formats.style import Styler\n        return Styler(self)\n    _shared_docs['items'] = \"\\n        Iterate over (column name, Series) pairs.\\n\\n        Iterates over the DataFrame columns, returning a tuple with\\n        the column name and the content as a Series.\\n\\n        Yields\\n        ------\\n        label : object\\n            The column names for the DataFrame being iterated over.\\n        content : Series\\n            The column entries belonging to each label, as a Series.\\n\\n        See Also\\n        --------\\n        DataFrame.iterrows : Iterate over DataFrame rows as\\n            (index, Series) pairs.\\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples\\n            of the values.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\\n        ...                   'population': [1864, 22000, 80000]},\\n        ...                   index=['panda', 'polar', 'koala'])\\n        >>> df\\n                species   population\\n        panda   bear      1864\\n        polar   bear      22000\\n        koala   marsupial 80000\\n        >>> for label, content in df.items():\\n        ...     print(f'label: {label}')\\n        ...     print(f'content: {content}', sep='\\\\n')\\n        ...\\n        label: species\\n        content:\\n        panda         bear\\n        polar         bear\\n        koala    marsupial\\n        Name: species, dtype: object\\n        label: population\\n        content:\\n        panda     1864\\n        polar    22000\\n        koala    80000\\n        Name: population, dtype: int64\\n        \"\n\n    def __getitem__(self, key):\n        check_dict_or_set_indexers(key)\n        key = lib.item_from_zerodim(key)\n        key = com.apply_if_callable(key, self)\n        if is_hashable(key) and (not is_iterator(key)):\n            is_mi = isinstance(self.columns, MultiIndex)\n            if not is_mi and (self.columns.is_unique and key in self.columns or key in self.columns.drop_duplicates(keep=False)):\n                return self._get_item_cache(key)\n            elif is_mi and self.columns.is_unique and (key in self.columns):\n                return self._getitem_multilevel(key)\n        if isinstance(key, slice):\n            return self._getitem_slice(key)\n        if isinstance(key, DataFrame):\n            return self.where(key)\n        if com.is_bool_indexer(key):\n            return self._getitem_bool_array(key)\n        is_single_key = isinstance(key, tuple) or not is_list_like(key)\n        if is_single_key:\n            if self.columns.nlevels > 1:\n                return self._getitem_multilevel(key)\n            indexer = self.columns.get_loc(key)\n            if is_integer(indexer):\n                indexer = [indexer]\n        else:\n            if is_iterator(key):\n                key = list(key)\n            indexer = self.columns._get_indexer_strict(key, 'columns')[1]\n        if getattr(indexer, 'dtype', None) == bool:\n            indexer = np.where(indexer)[0]\n        if isinstance(indexer, slice):\n            return self._slice(indexer, axis=1)\n        data = self._take_with_is_copy(indexer, axis=1)\n        if is_single_key:\n            if data.shape[1] == 1 and (not isinstance(self.columns, MultiIndex)):\n                return data._get_item_cache(key)\n        return data\n\n    def nlargest(self, n: int, columns: IndexLabel, keep: NsmallestNlargestKeep='first') -> DataFrame:\n        \"\"\"\n        Return the first `n` rows ordered by `columns` in descending order.\n\n        Return the first `n` rows with the largest values in `columns`, in\n        descending order. The columns that are not specified are returned as\n        well, but not used for ordering.\n\n        This method is equivalent to\n        ``df.sort_values(columns, ascending=False).head(n)``, but more\n        performant.\n\n        Parameters\n        ----------\n        n : int\n            Number of rows to return.\n        columns : label or list of labels\n            Column label(s) to order by.\n        keep : {'first', 'last', 'all'}, default 'first'\n            Where there are duplicate values:\n\n            - ``first`` : prioritize the first occurrence(s)\n            - ``last`` : prioritize the last occurrence(s)\n            - ``all`` : keep all the ties of the smallest item even if it means\n              selecting more than ``n`` items.\n\n        Returns\n        -------\n        DataFrame\n            The first `n` rows ordered by the given columns in descending\n            order.\n\n        See Also\n        --------\n        DataFrame.nsmallest : Return the first `n` rows ordered by `columns` in\n            ascending order.\n        DataFrame.sort_values : Sort DataFrame by the values.\n        DataFrame.head : Return the first `n` rows without re-ordering.\n\n        Notes\n        -----\n        This function cannot be used with all column types. For example, when\n        specifying columns with `object` or `category` dtypes, ``TypeError`` is\n        raised.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n        ...                                   434000, 434000, 337000, 11300,\n        ...                                   11300, 11300],\n        ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n        ...                            17036, 182, 38, 311],\n        ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n        >>> df\n                  population      GDP alpha-2\n        Italy       59000000  1937894      IT\n        France      65000000  2583560      FR\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n        Iceland       337000    17036      IS\n        Nauru          11300      182      NR\n        Tuvalu         11300       38      TV\n        Anguilla       11300      311      AI\n\n        In the following example, we will use ``nlargest`` to select the three\n        rows having the largest values in column \"population\".\n\n        >>> df.nlargest(3, 'population')\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Malta       434000    12011      MT\n\n        When using ``keep='last'``, ties are resolved in reverse order:\n\n        >>> df.nlargest(3, 'population', keep='last')\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Brunei      434000    12128      BN\n\n        When using ``keep='all'``, the number of element kept can go beyond ``n``\n        if there are duplicate values for the smallest element, all the\n        ties are kept:\n\n        >>> df.nlargest(3, 'population', keep='all')\n                  population      GDP alpha-2\n        France      65000000  2583560      FR\n        Italy       59000000  1937894      IT\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n\n        However, ``nlargest`` does not keep ``n`` distinct largest elements:\n\n        >>> df.nlargest(5, 'population', keep='all')\n                  population      GDP alpha-2\n        France      65000000  2583560      FR\n        Italy       59000000  1937894      IT\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n\n        To order by the largest values in column \"population\" and then \"GDP\",\n        we can specify multiple columns like in the next example.\n\n        >>> df.nlargest(3, ['population', 'GDP'])\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Brunei      434000    12128      BN\n        \"\"\"\n        return selectn.SelectNFrame(self, n=n, keep=keep, columns=columns).nlargest()\n\n    def query(self, expr: str, *, inplace: bool=False, **kwargs) -> DataFrame | None:\n        \"\"\"\n        Query the columns of a DataFrame with a boolean expression.\n\n        Parameters\n        ----------\n        expr : str\n            The query string to evaluate.\n\n            You can refer to variables\n            in the environment by prefixing them with an '@' character like\n            ``@a + b``.\n\n            You can refer to column names that are not valid Python variable names\n            by surrounding them in backticks. Thus, column names containing spaces\n            or punctuations (besides underscores) or starting with digits must be\n            surrounded by backticks. (For example, a column named \"Area (cm^2)\" would\n            be referenced as ```Area (cm^2)```). Column names which are Python keywords\n            (like \"list\", \"for\", \"import\", etc) cannot be used.\n\n            For example, if one of your columns is called ``a a`` and you want\n            to sum it with ``b``, your query should be ```a a` + b``.\n\n        inplace : bool\n            Whether to modify the DataFrame rather than creating a new one.\n        **kwargs\n            See the documentation for :func:`eval` for complete details\n            on the keyword arguments accepted by :meth:`DataFrame.query`.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame resulting from the provided query expression or\n            None if ``inplace=True``.\n\n        See Also\n        --------\n        eval : Evaluate a string describing operations on\n            DataFrame columns.\n        DataFrame.eval : Evaluate a string describing operations on\n            DataFrame columns.\n\n        Notes\n        -----\n        The result of the evaluation of this expression is first passed to\n        :attr:`DataFrame.loc` and if that fails because of a\n        multidimensional key (e.g., a DataFrame) then the result will be passed\n        to :meth:`DataFrame.__getitem__`.\n\n        This method uses the top-level :func:`eval` function to\n        evaluate the passed query.\n\n        The :meth:`~pandas.DataFrame.query` method uses a slightly\n        modified Python syntax by default. For example, the ``&`` and ``|``\n        (bitwise) operators have the precedence of their boolean cousins,\n        :keyword:`and` and :keyword:`or`. This *is* syntactically valid Python,\n        however the semantics are different.\n\n        You can change the semantics of the expression by passing the keyword\n        argument ``parser='python'``. This enforces the same semantics as\n        evaluation in Python space. Likewise, you can pass ``engine='python'``\n        to evaluate an expression using Python itself as a backend. This is not\n        recommended as it is inefficient compared to using ``numexpr`` as the\n        engine.\n\n        The :attr:`DataFrame.index` and\n        :attr:`DataFrame.columns` attributes of the\n        :class:`~pandas.DataFrame` instance are placed in the query namespace\n        by default, which allows you to treat both the index and columns of the\n        frame as a column in the frame.\n        The identifier ``index`` is used for the frame index; you can also\n        use the name of the index to identify it in a query. Please note that\n        Python keywords may not be used as identifiers.\n\n        For further details and examples see the ``query`` documentation in\n        :ref:`indexing <indexing.query>`.\n\n        *Backtick quoted variables*\n\n        Backtick quoted variables are parsed as literal Python code and\n        are converted internally to a Python valid identifier.\n        This can lead to the following problems.\n\n        During parsing a number of disallowed characters inside the backtick\n        quoted string are replaced by strings that are allowed as a Python identifier.\n        These characters include all operators in Python, the space character, the\n        question mark, the exclamation mark, the dollar sign, and the euro sign.\n        For other characters that fall outside the ASCII range (U+0001..U+007F)\n        and those that are not further specified in PEP 3131,\n        the query parser will raise an error.\n        This excludes whitespace different than the space character,\n        but also the hashtag (as it is used for comments) and the backtick\n        itself (backtick can also not be escaped).\n\n        In a special case, quotes that make a pair around a backtick can\n        confuse the parser.\n        For example, ```it's` > `that's``` will raise an error,\n        as it forms a quoted string (``'s > `that'``) with a backtick inside.\n\n        See also the Python documentation about lexical analysis\n        (https://docs.python.org/3/reference/lexical_analysis.html)\n        in combination with the source code in :mod:`pandas.core.computation.parsing`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': range(1, 6),\n        ...                    'B': range(10, 0, -2),\n        ...                    'C C': range(10, 5, -1)})\n        >>> df\n           A   B  C C\n        0  1  10   10\n        1  2   8    9\n        2  3   6    8\n        3  4   4    7\n        4  5   2    6\n        >>> df.query('A > B')\n           A  B  C C\n        4  5  2    6\n\n        The previous expression is equivalent to\n\n        >>> df[df.A > df.B]\n           A  B  C C\n        4  5  2    6\n\n        For columns with spaces in their name, you can use backtick quoting.\n\n        >>> df.query('B == `C C`')\n           A   B  C C\n        0  1  10   10\n\n        The previous expression is equivalent to\n\n        >>> df[df.B == df['C C']]\n           A   B  C C\n        0  1  10   10\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        if not isinstance(expr, str):\n            msg = f'expr must be a string to be evaluated, {type(expr)} given'\n            raise ValueError(msg)\n        kwargs['level'] = kwargs.pop('level', 0) + 1\n        kwargs['target'] = None\n        res = self.eval(expr, **kwargs)\n        try:\n            result = self.loc[res]\n        except ValueError:\n            result = self[res]\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs['klass'])\n    def notnull(self) -> DataFrame:\n        \"\"\"\n        DataFrame.notnull is an alias for DataFrame.notna.\n        \"\"\"\n        return ~self.isna()\n\n    @overload\n    def dot(self, other: Series) -> Series:\n        ...\n\n    @overload\n    def dot(self, other: DataFrame | Index | ArrayLike) -> DataFrame:\n        ...\n\n    def _repr_fits_horizontal_(self) -> bool:\n        \"\"\"\n        Check if full repr fits in horizontal boundaries imposed by the display\n        options width and max_columns.\n        \"\"\"\n        (width, height) = console.get_console_size()\n        max_columns = get_option('display.max_columns')\n        nb_columns = len(self.columns)\n        if max_columns and nb_columns > max_columns or (width and nb_columns > width // 2):\n            return False\n        if width is None or not console.in_interactive_session():\n            return True\n        if get_option('display.width') is not None or console.in_ipython_frontend():\n            max_rows = 1\n        else:\n            max_rows = get_option('display.max_rows')\n        buf = StringIO()\n        d = self\n        if max_rows is not None:\n            d = d.iloc[:min(max_rows, len(d))]\n        else:\n            return True\n        d.to_string(buf=buf)\n        value = buf.getvalue()\n        repr_width = max((len(line) for line in value.split('\\n')))\n        return repr_width < width\n\n    @overload\n    def __matmul__(self, other: Series) -> Series:\n        ...\n\n    @overload\n    def __matmul__(self, other: AnyArrayLike | DataFrame) -> DataFrame | Series:\n        ...\n\n    def _setitem_array(self, key, value):\n        if com.is_bool_indexer(key):\n            if len(key) != len(self.index):\n                raise ValueError(f'Item wrong length {len(key)} instead of {len(self.index)}!')\n            key = check_bool_indexer(self.index, key)\n            indexer = key.nonzero()[0]\n            self._check_setitem_copy()\n            if isinstance(value, DataFrame):\n                value = value.reindex(self.index.take(indexer))\n            self.iloc[indexer] = value\n        elif isinstance(value, DataFrame):\n            check_key_length(self.columns, key, value)\n            for (k1, k2) in zip(key, value.columns):\n                self[k1] = value[k2]\n        elif not is_list_like(value):\n            for col in key:\n                self[col] = value\n        elif isinstance(value, np.ndarray) and value.ndim == 2:\n            self._iset_not_inplace(key, value)\n        elif np.ndim(value) > 1:\n            value = DataFrame(value).values\n            return self._setitem_array(key, value)\n        else:\n            self._iset_not_inplace(key, value)\n\n    def join(self, other: DataFrame | Series | Iterable[DataFrame | Series], on: IndexLabel | None=None, how: MergeHow='left', lsuffix: str='', rsuffix: str='', sort: bool=False, validate: JoinValidate | None=None) -> DataFrame:\n        \"\"\"\n        Join columns of another DataFrame.\n\n        Join columns with `other` DataFrame either on index or on a key\n        column. Efficiently join multiple DataFrame objects by index at once by\n        passing a list.\n\n        Parameters\n        ----------\n        other : DataFrame, Series, or a list containing any combination of them\n            Index should be similar to one of the columns in this one. If a\n            Series is passed, its name attribute must be set, and that will be\n            used as the column name in the resulting joined DataFrame.\n        on : str, list of str, or array-like, optional\n            Column or index level name(s) in the caller to join on the index\n            in `other`, otherwise joins index-on-index. If multiple\n            values given, the `other` DataFrame must have a MultiIndex. Can\n            pass an array as the join key if it is not already contained in\n            the calling DataFrame. Like an Excel VLOOKUP operation.\n        how : {'left', 'right', 'outer', 'inner', 'cross'}, default 'left'\n            How to handle the operation of the two objects.\n\n            * left: use calling frame's index (or column if on is specified)\n            * right: use `other`'s index.\n            * outer: form union of calling frame's index (or column if on is\n              specified) with `other`'s index, and sort it lexicographically.\n            * inner: form intersection of calling frame's index (or column if\n              on is specified) with `other`'s index, preserving the order\n              of the calling's one.\n            * cross: creates the cartesian product from both frames, preserves the order\n              of the left keys.\n        lsuffix : str, default ''\n            Suffix to use from left frame's overlapping columns.\n        rsuffix : str, default ''\n            Suffix to use from right frame's overlapping columns.\n        sort : bool, default False\n            Order result DataFrame lexicographically by the join key. If False,\n            the order of the join key depends on the join type (how keyword).\n        validate : str, optional\n            If specified, checks if join is of specified type.\n\n            * \"one_to_one\" or \"1:1\": check if join keys are unique in both left\n              and right datasets.\n            * \"one_to_many\" or \"1:m\": check if join keys are unique in left dataset.\n            * \"many_to_one\" or \"m:1\": check if join keys are unique in right dataset.\n            * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n\n            .. versionadded:: 1.5.0\n\n        Returns\n        -------\n        DataFrame\n            A dataframe containing columns from both the caller and `other`.\n\n        See Also\n        --------\n        DataFrame.merge : For column(s)-on-column(s) operations.\n\n        Notes\n        -----\n        Parameters `on`, `lsuffix`, and `rsuffix` are not supported when\n        passing a list of `DataFrame` objects.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n        ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n\n        >>> df\n          key   A\n        0  K0  A0\n        1  K1  A1\n        2  K2  A2\n        3  K3  A3\n        4  K4  A4\n        5  K5  A5\n\n        >>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n        ...                       'B': ['B0', 'B1', 'B2']})\n\n        >>> other\n          key   B\n        0  K0  B0\n        1  K1  B1\n        2  K2  B2\n\n        Join DataFrames using their indexes.\n\n        >>> df.join(other, lsuffix='_caller', rsuffix='_other')\n          key_caller   A key_other    B\n        0         K0  A0        K0   B0\n        1         K1  A1        K1   B1\n        2         K2  A2        K2   B2\n        3         K3  A3       NaN  NaN\n        4         K4  A4       NaN  NaN\n        5         K5  A5       NaN  NaN\n\n        If we want to join using the key columns, we need to set key to be\n        the index in both `df` and `other`. The joined DataFrame will have\n        key as its index.\n\n        >>> df.set_index('key').join(other.set_index('key'))\n              A    B\n        key\n        K0   A0   B0\n        K1   A1   B1\n        K2   A2   B2\n        K3   A3  NaN\n        K4   A4  NaN\n        K5   A5  NaN\n\n        Another option to join using the key columns is to use the `on`\n        parameter. DataFrame.join always uses `other`'s index but we can use\n        any column in `df`. This method preserves the original DataFrame's\n        index in the result.\n\n        >>> df.join(other.set_index('key'), on='key')\n          key   A    B\n        0  K0  A0   B0\n        1  K1  A1   B1\n        2  K2  A2   B2\n        3  K3  A3  NaN\n        4  K4  A4  NaN\n        5  K5  A5  NaN\n\n        Using non-unique key values shows how they are matched.\n\n        >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K0', 'K1'],\n        ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n\n        >>> df\n          key   A\n        0  K0  A0\n        1  K1  A1\n        2  K1  A2\n        3  K3  A3\n        4  K0  A4\n        5  K1  A5\n\n        >>> df.join(other.set_index('key'), on='key', validate='m:1')\n          key   A    B\n        0  K0  A0   B0\n        1  K1  A1   B1\n        2  K1  A2   B1\n        3  K3  A3  NaN\n        4  K0  A4   B0\n        5  K1  A5   B1\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n        from pandas.core.reshape.merge import merge\n        if isinstance(other, Series):\n            if other.name is None:\n                raise ValueError('Other Series must have a name')\n            other = DataFrame({other.name: other})\n        if isinstance(other, DataFrame):\n            if how == 'cross':\n                return merge(self, other, how=how, on=on, suffixes=(lsuffix, rsuffix), sort=sort, validate=validate)\n            return merge(self, other, left_on=on, how=how, left_index=on is None, right_index=True, suffixes=(lsuffix, rsuffix), sort=sort, validate=validate)\n        else:\n            if on is not None:\n                raise ValueError('Joining multiple DataFrames only supported for joining on index')\n            if rsuffix or lsuffix:\n                raise ValueError('Suffixes not supported when joining multiple DataFrames')\n            frames = [cast('DataFrame | Series', self)] + list(other)\n            can_concat = all((df.index.is_unique for df in frames))\n            if can_concat:\n                if how == 'left':\n                    res = concat(frames, axis=1, join='outer', verify_integrity=True, sort=sort)\n                    return res.reindex(self.index, copy=False)\n                else:\n                    return concat(frames, axis=1, join=how, verify_integrity=True, sort=sort)\n            joined = frames[0]\n            for frame in frames[1:]:\n                joined = merge(joined, frame, how=how, left_index=True, right_index=True, validate=validate)\n            return joined\n\n    @doc(make_doc('sum', ndim=2))\n    def sum(self, axis: Axis | None=0, skipna: bool=True, numeric_only: bool=False, min_count: int=0, **kwargs):\n        result = super().sum(axis, skipna, numeric_only, min_count, **kwargs)\n        return result.__finalize__(self, method='sum')\n\n    def mode(self, axis: Axis=0, numeric_only: bool=False, dropna: bool=True) -> DataFrame:\n        \"\"\"\n        Get the mode(s) of each element along the selected axis.\n\n        The mode of a set of values is the value that appears most often.\n        It can be multiple values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to iterate over while searching for the mode:\n\n            * 0 or 'index' : get mode of each column\n            * 1 or 'columns' : get mode of each row.\n\n        numeric_only : bool, default False\n            If True, only apply to numeric columns.\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n        Returns\n        -------\n        DataFrame\n            The modes of each column or row.\n\n        See Also\n        --------\n        Series.mode : Return the highest frequency value in a Series.\n        Series.value_counts : Return the counts of values in a Series.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('bird', 2, 2),\n        ...                    ('mammal', 4, np.nan),\n        ...                    ('arthropod', 8, 0),\n        ...                    ('bird', 2, np.nan)],\n        ...                   index=('falcon', 'horse', 'spider', 'ostrich'),\n        ...                   columns=('species', 'legs', 'wings'))\n        >>> df\n                   species  legs  wings\n        falcon        bird     2    2.0\n        horse       mammal     4    NaN\n        spider   arthropod     8    0.0\n        ostrich       bird     2    NaN\n\n        By default, missing values are not considered, and the mode of wings\n        are both 0 and 2. Because the resulting DataFrame has two rows,\n        the second row of ``species`` and ``legs`` contains ``NaN``.\n\n        >>> df.mode()\n          species  legs  wings\n        0    bird   2.0    0.0\n        1     NaN   NaN    2.0\n\n        Setting ``dropna=False`` ``NaN`` values are considered and they can be\n        the mode (like for wings).\n\n        >>> df.mode(dropna=False)\n          species  legs  wings\n        0    bird     2    NaN\n\n        Setting ``numeric_only=True``, only the mode of numeric columns is\n        computed, and columns of other types are ignored.\n\n        >>> df.mode(numeric_only=True)\n           legs  wings\n        0   2.0    0.0\n        1   NaN    2.0\n\n        To compute the mode over columns and not rows, use the axis parameter:\n\n        >>> df.mode(axis='columns', numeric_only=True)\n                   0    1\n        falcon   2.0  NaN\n        horse    4.0  NaN\n        spider   0.0  8.0\n        ostrich  2.0  NaN\n        \"\"\"\n        data = self if not numeric_only else self._get_numeric_data()\n\n        def f(s):\n            return s.mode(dropna=dropna)\n        data = data.apply(f, axis=axis)\n        if data.empty:\n            data.index = default_index(0)\n        return data\n\n    def _get_value(self, index, col, takeable: bool=False) -> Scalar:\n        \"\"\"\n        Quickly retrieve single value at passed column and index.\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        takeable : interpret the index/col as indexers, default False\n\n        Returns\n        -------\n        scalar\n\n        Notes\n        -----\n        Assumes that both `self.index._index_as_unique` and\n        `self.columns._index_as_unique`; Caller is responsible for checking.\n        \"\"\"\n        if takeable:\n            series = self._ixs(col, axis=1)\n            return series._values[index]\n        series = self._get_item_cache(col)\n        engine = self.index._engine\n        if not isinstance(self.index, MultiIndex):\n            row = self.index.get_loc(index)\n            return series._values[row]\n        loc = engine.get_loc(index)\n        return series._values[loc]\n\n    @overload\n    def to_dict(self, orient: Literal['dict', 'list', 'series', 'split', 'tight', 'index']=..., *, into: type[MutableMappingT] | MutableMappingT, index: bool=...) -> MutableMappingT:\n        ...\n\n    @overload\n    def to_dict(self, orient: Literal['records'], *, into: type[MutableMappingT] | MutableMappingT, index: bool=...) -> list[MutableMappingT]:\n        ...\n\n    @overload\n    def to_dict(self, orient: Literal['dict', 'list', 'series', 'split', 'tight', 'index']=..., *, into: type[dict]=..., index: bool=...) -> dict:\n        ...\n\n    @overload\n    def to_dict(self, orient: Literal['records'], *, into: type[dict]=..., index: bool=...) -> list[dict]:\n        ...\n\n    @deprecate_nonkeyword_arguments(version='3.0', allowed_args=['self', 'orient'], name='to_dict')\n    def to_dict(self, orient: Literal['dict', 'list', 'series', 'split', 'tight', 'records', 'index']='dict', into: type[MutableMappingT] | MutableMappingT=dict, index: bool=True) -> MutableMappingT | list[MutableMappingT]:\n        \"\"\"\n        Convert the DataFrame to a dictionary.\n\n        The type of the key-value pairs can be customized with the parameters\n        (see below).\n\n        Parameters\n        ----------\n        orient : str {'dict', 'list', 'series', 'split', 'tight', 'records', 'index'}\n            Determines the type of the values of the dictionary.\n\n            - 'dict' (default) : dict like {column -> {index -> value}}\n            - 'list' : dict like {column -> [values]}\n            - 'series' : dict like {column -> Series(values)}\n            - 'split' : dict like\n              {'index' -> [index], 'columns' -> [columns], 'data' -> [values]}\n            - 'tight' : dict like\n              {'index' -> [index], 'columns' -> [columns], 'data' -> [values],\n              'index_names' -> [index.names], 'column_names' -> [column.names]}\n            - 'records' : list like\n              [{column -> value}, ... , {column -> value}]\n            - 'index' : dict like {index -> {column -> value}}\n\n            .. versionadded:: 1.4.0\n                'tight' as an allowed value for the ``orient`` argument\n\n        into : class, default dict\n            The collections.abc.MutableMapping subclass used for all Mappings\n            in the return value.  Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        index : bool, default True\n            Whether to include the index item (and index_names item if `orient`\n            is 'tight') in the returned dictionary. Can only be ``False``\n            when `orient` is 'split' or 'tight'.\n\n            .. versionadded:: 2.0.0\n\n        Returns\n        -------\n        dict, list or collections.abc.MutableMapping\n            Return a collections.abc.MutableMapping object representing the\n            DataFrame. The resulting transformation depends on the `orient`\n            parameter.\n\n        See Also\n        --------\n        DataFrame.from_dict: Create a DataFrame from a dictionary.\n        DataFrame.to_json: Convert a DataFrame to JSON format.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2],\n        ...                    'col2': [0.5, 0.75]},\n        ...                   index=['row1', 'row2'])\n        >>> df\n              col1  col2\n        row1     1  0.50\n        row2     2  0.75\n        >>> df.to_dict()\n        {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}}\n\n        You can specify the return orientation.\n\n        >>> df.to_dict('series')\n        {'col1': row1    1\n                 row2    2\n        Name: col1, dtype: int64,\n        'col2': row1    0.50\n                row2    0.75\n        Name: col2, dtype: float64}\n\n        >>> df.to_dict('split')\n        {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n         'data': [[1, 0.5], [2, 0.75]]}\n\n        >>> df.to_dict('records')\n        [{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}]\n\n        >>> df.to_dict('index')\n        {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}\n\n        >>> df.to_dict('tight')\n        {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n         'data': [[1, 0.5], [2, 0.75]], 'index_names': [None], 'column_names': [None]}\n\n        You can also specify the mapping type.\n\n        >>> from collections import OrderedDict, defaultdict\n        >>> df.to_dict(into=OrderedDict)\n        OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])),\n                     ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])\n\n        If you want a `defaultdict`, you need to initialize it:\n\n        >>> dd = defaultdict(list)\n        >>> df.to_dict('records', into=dd)\n        [defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}),\n         defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]\n        \"\"\"\n        from pandas.core.methods.to_dict import to_dict\n        return to_dict(self, orient, into=into, index=index)\n\n    @deprecate_nonkeyword_arguments(version='3.0', allowed_args=['self', 'destination_table'], name='to_gbq')\n    def to_gbq(self, destination_table: str, project_id: str | None=None, chunksize: int | None=None, reauth: bool=False, if_exists: ToGbqIfexist='fail', auth_local_webserver: bool=True, table_schema: list[dict[str, str]] | None=None, location: str | None=None, progress_bar: bool=True, credentials=None) -> None:\n        \"\"\"\n        Write a DataFrame to a Google BigQuery table.\n\n        .. deprecated:: 2.2.0\n\n           Please use ``pandas_gbq.to_gbq`` instead.\n\n        This function requires the `pandas-gbq package\n        <https://pandas-gbq.readthedocs.io>`__.\n\n        See the `How to authenticate with Google BigQuery\n        <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__\n        guide for authentication instructions.\n\n        Parameters\n        ----------\n        destination_table : str\n            Name of table to be written, in the form ``dataset.tablename``.\n        project_id : str, optional\n            Google BigQuery Account project ID. Optional when available from\n            the environment.\n        chunksize : int, optional\n            Number of rows to be inserted in each chunk from the dataframe.\n            Set to ``None`` to load the whole dataframe at once.\n        reauth : bool, default False\n            Force Google BigQuery to re-authenticate the user. This is useful\n            if multiple accounts are used.\n        if_exists : str, default 'fail'\n            Behavior when the destination table exists. Value can be one of:\n\n            ``'fail'``\n                If table exists raise pandas_gbq.gbq.TableCreationError.\n            ``'replace'``\n                If table exists, drop it, recreate it, and insert data.\n            ``'append'``\n                If table exists, insert data. Create if does not exist.\n        auth_local_webserver : bool, default True\n            Use the `local webserver flow`_ instead of the `console flow`_\n            when getting user credentials.\n\n            .. _local webserver flow:\n                https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server\n            .. _console flow:\n                https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console\n\n            *New in version 0.2.0 of pandas-gbq*.\n\n            .. versionchanged:: 1.5.0\n               Default value is changed to ``True``. Google has deprecated the\n               ``auth_local_webserver = False`` `\"out of band\" (copy-paste)\n               flow\n               <https://developers.googleblog.com/2022/02/making-oauth-flows-safer.html?m=1#disallowed-oob>`_.\n        table_schema : list of dicts, optional\n            List of BigQuery table fields to which according DataFrame\n            columns conform to, e.g. ``[{'name': 'col1', 'type':\n            'STRING'},...]``. If schema is not provided, it will be\n            generated according to dtypes of DataFrame columns. See\n            BigQuery API documentation on available names of a field.\n\n            *New in version 0.3.1 of pandas-gbq*.\n        location : str, optional\n            Location where the load job should run. See the `BigQuery locations\n            documentation\n            <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a\n            list of available locations. The location must match that of the\n            target dataset.\n\n            *New in version 0.5.0 of pandas-gbq*.\n        progress_bar : bool, default True\n            Use the library `tqdm` to show the progress bar for the upload,\n            chunk by chunk.\n\n            *New in version 0.5.0 of pandas-gbq*.\n        credentials : google.auth.credentials.Credentials, optional\n            Credentials for accessing Google APIs. Use this parameter to\n            override default credentials, such as to use Compute Engine\n            :class:`google.auth.compute_engine.Credentials` or Service\n            Account :class:`google.oauth2.service_account.Credentials`\n            directly.\n\n            *New in version 0.8.0 of pandas-gbq*.\n\n        See Also\n        --------\n        pandas_gbq.to_gbq : This function in the pandas-gbq library.\n        read_gbq : Read a DataFrame from Google BigQuery.\n\n        Examples\n        --------\n        Example taken from `Google BigQuery documentation\n        <https://cloud.google.com/bigquery/docs/samples/bigquery-pandas-gbq-to-gbq-simple>`_\n\n        >>> project_id = \"my-project\"\n        >>> table_id = 'my_dataset.my_table'\n        >>> df = pd.DataFrame({\n        ...                   \"my_string\": [\"a\", \"b\", \"c\"],\n        ...                   \"my_int64\": [1, 2, 3],\n        ...                   \"my_float64\": [4.0, 5.0, 6.0],\n        ...                   \"my_bool1\": [True, False, True],\n        ...                   \"my_bool2\": [False, True, False],\n        ...                   \"my_dates\": pd.date_range(\"now\", periods=3),\n        ...                   }\n        ...                   )\n\n        >>> df.to_gbq(table_id, project_id=project_id)  # doctest: +SKIP\n        \"\"\"\n        from pandas.io import gbq\n        gbq.to_gbq(self, destination_table, project_id=project_id, chunksize=chunksize, reauth=reauth, if_exists=if_exists, auth_local_webserver=auth_local_webserver, table_schema=table_schema, location=location, progress_bar=progress_bar, credentials=credentials)\n\n    def __dataframe_consortium_standard__(self, *, api_version: str | None=None) -> Any:\n        \"\"\"\n        Provide entry point to the Consortium DataFrame Standard API.\n\n        This is developed and maintained outside of pandas.\n        Please report any issues to https://github.com/data-apis/dataframe-api-compat.\n        \"\"\"\n        dataframe_api_compat = import_optional_dependency('dataframe_api_compat')\n        convert_to_standard_compliant_dataframe = dataframe_api_compat.pandas_standard.convert_to_standard_compliant_dataframe\n        return convert_to_standard_compliant_dataframe(self, api_version=api_version)\n\n    @doc(make_doc('cummin', ndim=2))\n    def cummin(self, axis: Axis | None=None, skipna: bool=True, *args, **kwargs):\n        return NDFrame.cummin(self, axis, skipna, *args, **kwargs)\n\n    def _constructor_from_mgr(self, mgr, axes) -> DataFrame:\n        df = DataFrame._from_mgr(mgr, axes=axes)\n        if type(self) is DataFrame:\n            return df\n        elif type(self).__name__ == 'GeoDataFrame':\n            return self._constructor(mgr)\n        return self._constructor(df)\n\n    def pop(self, item: Hashable) -> Series:\n        \"\"\"\n        Return item and drop from frame. Raise KeyError if not found.\n\n        Parameters\n        ----------\n        item : label\n            Label of column to be popped.\n\n        Returns\n        -------\n        Series\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n        ...                    ('parrot', 'bird', 24.0),\n        ...                    ('lion', 'mammal', 80.5),\n        ...                    ('monkey', 'mammal', np.nan)],\n        ...                   columns=('name', 'class', 'max_speed'))\n        >>> df\n             name   class  max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        >>> df.pop('class')\n        0      bird\n        1      bird\n        2    mammal\n        3    mammal\n        Name: class, dtype: object\n\n        >>> df\n             name  max_speed\n        0  falcon      389.0\n        1  parrot       24.0\n        2    lion       80.5\n        3  monkey        NaN\n        \"\"\"\n        return super().pop(item=item)\n\n    def _maybe_cache_changed(self, item, value: Series, inplace: bool) -> None:\n        \"\"\"\n        The object has called back to us saying maybe it has changed.\n        \"\"\"\n        loc = self._info_axis.get_loc(item)\n        arraylike = value._values\n        old = self._ixs(loc, axis=1)\n        if old._values is value._values and inplace:\n            return\n        self._mgr.iset(loc, arraylike, inplace=inplace)\n\n    @deprecate_nonkeyword_arguments(version='3.0', allowed_args=['self', 'buf'], name='to_markdown')\n    @doc(Series.to_markdown, klass=_shared_doc_kwargs['klass'], storage_options=_shared_docs['storage_options'], examples='Examples\\n        --------\\n        >>> df = pd.DataFrame(\\n        ...     data={\"animal_1\": [\"elk\", \"pig\"], \"animal_2\": [\"dog\", \"quetzal\"]}\\n        ... )\\n        >>> print(df.to_markdown())\\n        |    | animal_1   | animal_2   |\\n        |---:|:-----------|:-----------|\\n        |  0 | elk        | dog        |\\n        |  1 | pig        | quetzal    |\\n\\n        Output markdown with a tabulate option.\\n\\n        >>> print(df.to_markdown(tablefmt=\"grid\"))\\n        +----+------------+------------+\\n        |    | animal_1   | animal_2   |\\n        +====+============+============+\\n        |  0 | elk        | dog        |\\n        +----+------------+------------+\\n        |  1 | pig        | quetzal    |\\n        +----+------------+------------+')\n    def to_markdown(self, buf: FilePath | WriteBuffer[str] | None=None, mode: str='wt', index: bool=True, storage_options: StorageOptions | None=None, **kwargs) -> str | None:\n        if 'showindex' in kwargs:\n            raise ValueError(\"Pass 'index' instead of 'showindex\")\n        kwargs.setdefault('headers', 'keys')\n        kwargs.setdefault('tablefmt', 'pipe')\n        kwargs.setdefault('showindex', index)\n        tabulate = import_optional_dependency('tabulate')\n        result = tabulate.tabulate(self, **kwargs)\n        if buf is None:\n            return result\n        with get_handle(buf, mode, storage_options=storage_options) as handles:\n            handles.handle.write(result)\n        return None\n\n    @overload\n    def to_parquet(self, path: None=..., engine: Literal['auto', 'pyarrow', 'fastparquet']=..., compression: str | None=..., index: bool | None=..., partition_cols: list[str] | None=..., storage_options: StorageOptions=..., **kwargs) -> bytes:\n        ...\n\n    @overload\n    def to_parquet(self, path: FilePath | WriteBuffer[bytes], engine: Literal['auto', 'pyarrow', 'fastparquet']=..., compression: str | None=..., index: bool | None=..., partition_cols: list[str] | None=..., storage_options: StorageOptions=..., **kwargs) -> None:\n        ...\n\n    @deprecate_nonkeyword_arguments(version='3.0', allowed_args=['self', 'path'], name='to_parquet')\n    @doc(storage_options=_shared_docs['storage_options'])\n    def to_parquet(self, path: FilePath | WriteBuffer[bytes] | None=None, engine: Literal['auto', 'pyarrow', 'fastparquet']='auto', compression: str | None='snappy', index: bool | None=None, partition_cols: list[str] | None=None, storage_options: StorageOptions | None=None, **kwargs) -> bytes | None:\n        \"\"\"\n        Write a DataFrame to the binary parquet format.\n\n        This function writes the dataframe as a `parquet file\n        <https://parquet.apache.org/>`_. You can choose different parquet\n        backends, and have the option of compression. See\n        :ref:`the user guide <io.parquet>` for more details.\n\n        Parameters\n        ----------\n        path : str, path object, file-like object, or None, default None\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a binary ``write()`` function. If None, the result is\n            returned as bytes. If a string or path, it will be used as Root Directory\n            path when writing a partitioned dataset.\n        engine : {{'auto', 'pyarrow', 'fastparquet'}}, default 'auto'\n            Parquet library to use. If 'auto', then the option\n            ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n            behavior is to try 'pyarrow', falling back to 'fastparquet' if\n            'pyarrow' is unavailable.\n        compression : str or None, default 'snappy'\n            Name of the compression to use. Use ``None`` for no compression.\n            Supported options: 'snappy', 'gzip', 'brotli', 'lz4', 'zstd'.\n        index : bool, default None\n            If ``True``, include the dataframe's index(es) in the file output.\n            If ``False``, they will not be written to the file.\n            If ``None``, similar to ``True`` the dataframe's index(es)\n            will be saved. However, instead of being saved as values,\n            the RangeIndex will be stored as a range in the metadata so it\n            doesn't require much space and is faster. Other indexes will\n            be included as columns in the file output.\n        partition_cols : list, optional, default None\n            Column names by which to partition the dataset.\n            Columns are partitioned in the order they are given.\n            Must be None if path is not a string.\n        {storage_options}\n\n        **kwargs\n            Additional arguments passed to the parquet library. See\n            :ref:`pandas io <io.parquet>` for more details.\n\n        Returns\n        -------\n        bytes if no path argument is provided else None\n\n        See Also\n        --------\n        read_parquet : Read a parquet file.\n        DataFrame.to_orc : Write an orc file.\n        DataFrame.to_csv : Write a csv file.\n        DataFrame.to_sql : Write to a sql table.\n        DataFrame.to_hdf : Write to hdf.\n\n        Notes\n        -----\n        This function requires either the `fastparquet\n        <https://pypi.org/project/fastparquet>`_ or `pyarrow\n        <https://arrow.apache.org/docs/python/>`_ library.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={{'col1': [1, 2], 'col2': [3, 4]}})\n        >>> df.to_parquet('df.parquet.gzip',\n        ...               compression='gzip')  # doctest: +SKIP\n        >>> pd.read_parquet('df.parquet.gzip')  # doctest: +SKIP\n           col1  col2\n        0     1     3\n        1     2     4\n\n        If you want to get a buffer to the parquet content you can use a io.BytesIO\n        object, as long as you don't use partition_cols, which creates multiple files.\n\n        >>> import io\n        >>> f = io.BytesIO()\n        >>> df.to_parquet(f)\n        >>> f.seek(0)\n        0\n        >>> content = f.read()\n        \"\"\"\n        from pandas.io.parquet import to_parquet\n        return to_parquet(self, path, engine, compression=compression, index=index, partition_cols=partition_cols, storage_options=storage_options, **kwargs)\n\n    def reorder_levels(self, order: Sequence[int | str], axis: Axis=0) -> DataFrame:\n        \"\"\"\n        Rearrange index levels using input order. May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int or list of str\n            List representing new level order. Reference level by number\n            (position) or by key (label).\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Where to reorder levels.\n\n        Returns\n        -------\n        DataFrame\n\n        Examples\n        --------\n        >>> data = {\n        ...     \"class\": [\"Mammals\", \"Mammals\", \"Reptiles\"],\n        ...     \"diet\": [\"Omnivore\", \"Carnivore\", \"Carnivore\"],\n        ...     \"species\": [\"Humans\", \"Dogs\", \"Snakes\"],\n        ... }\n        >>> df = pd.DataFrame(data, columns=[\"class\", \"diet\", \"species\"])\n        >>> df = df.set_index([\"class\", \"diet\"])\n        >>> df\n                                          species\n        class      diet\n        Mammals    Omnivore                Humans\n                   Carnivore                 Dogs\n        Reptiles   Carnivore               Snakes\n\n        Let's reorder the levels of the index:\n\n        >>> df.reorder_levels([\"diet\", \"class\"])\n                                          species\n        diet      class\n        Omnivore  Mammals                  Humans\n        Carnivore Mammals                    Dogs\n                  Reptiles                 Snakes\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        if not isinstance(self._get_axis(axis), MultiIndex):\n            raise TypeError('Can only reorder levels on a hierarchical axis.')\n        result = self.copy(deep=None)\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.reorder_levels(order)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.reorder_levels(order)\n        return result\n\n    @overload\n    def to_html(self, buf: FilePath | WriteBuffer[str], columns: Axes | None=..., col_space: ColspaceArgType | None=..., header: bool=..., index: bool=..., na_rep: str=..., formatters: FormattersType | None=..., float_format: FloatFormatType | None=..., sparsify: bool | None=..., index_names: bool=..., justify: str | None=..., max_rows: int | None=..., max_cols: int | None=..., show_dimensions: bool | str=..., decimal: str=..., bold_rows: bool=..., classes: str | list | tuple | None=..., escape: bool=..., notebook: bool=..., border: int | bool | None=..., table_id: str | None=..., render_links: bool=..., encoding: str | None=...) -> None:\n        ...\n\n    @overload\n    def to_html(self, buf: None=..., columns: Axes | None=..., col_space: ColspaceArgType | None=..., header: bool=..., index: bool=..., na_rep: str=..., formatters: FormattersType | None=..., float_format: FloatFormatType | None=..., sparsify: bool | None=..., index_names: bool=..., justify: str | None=..., max_rows: int | None=..., max_cols: int | None=..., show_dimensions: bool | str=..., decimal: str=..., bold_rows: bool=..., classes: str | list | tuple | None=..., escape: bool=..., notebook: bool=..., border: int | bool | None=..., table_id: str | None=..., render_links: bool=..., encoding: str | None=...) -> str:\n        ...\n\n    @deprecate_nonkeyword_arguments(version='3.0', allowed_args=['self', 'buf'], name='to_html')\n    @Substitution(header_type='bool', header='Whether to print column labels, default True', col_space_type='str or int, list or dict of int or str', col_space='The minimum width of each column in CSS length units.  An int is assumed to be px units.')\n    @Substitution(shared_params=fmt.common_docstring, returns=fmt.return_docstring)\n    def to_html(self, buf: FilePath | WriteBuffer[str] | None=None, columns: Axes | None=None, col_space: ColspaceArgType | None=None, header: bool=True, index: bool=True, na_rep: str='NaN', formatters: FormattersType | None=None, float_format: FloatFormatType | None=None, sparsify: bool | None=None, index_names: bool=True, justify: str | None=None, max_rows: int | None=None, max_cols: int | None=None, show_dimensions: bool | str=False, decimal: str='.', bold_rows: bool=True, classes: str | list | tuple | None=None, escape: bool=True, notebook: bool=False, border: int | bool | None=None, table_id: str | None=None, render_links: bool=False, encoding: str | None=None) -> str | None:\n        \"\"\"\n        Render a DataFrame as an HTML table.\n        %(shared_params)s\n        bold_rows : bool, default True\n            Make the row labels bold in the output.\n        classes : str or list or tuple, default None\n            CSS class(es) to apply to the resulting html table.\n        escape : bool, default True\n            Convert the characters <, >, and & to HTML-safe sequences.\n        notebook : {True, False}, default False\n            Whether the generated HTML is for IPython Notebook.\n        border : int\n            A ``border=border`` attribute is included in the opening\n            `<table>` tag. Default ``pd.options.display.html.border``.\n        table_id : str, optional\n            A css id is included in the opening `<table>` tag if specified.\n        render_links : bool, default False\n            Convert URLs to HTML links.\n        encoding : str, default \"utf-8\"\n            Set character encoding.\n        %(returns)s\n        See Also\n        --------\n        to_string : Convert DataFrame to a string.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [4, 3]})\n        >>> html_string = '''<table border=\"1\" class=\"dataframe\">\n        ...   <thead>\n        ...     <tr style=\"text-align: right;\">\n        ...       <th></th>\n        ...       <th>col1</th>\n        ...       <th>col2</th>\n        ...     </tr>\n        ...   </thead>\n        ...   <tbody>\n        ...     <tr>\n        ...       <th>0</th>\n        ...       <td>1</td>\n        ...       <td>4</td>\n        ...     </tr>\n        ...     <tr>\n        ...       <th>1</th>\n        ...       <td>2</td>\n        ...       <td>3</td>\n        ...     </tr>\n        ...   </tbody>\n        ... </table>'''\n        >>> assert html_string == df.to_html()\n        \"\"\"\n        if justify is not None and justify not in fmt.VALID_JUSTIFY_PARAMETERS:\n            raise ValueError('Invalid value for justify parameter')\n        formatter = fmt.DataFrameFormatter(self, columns=columns, col_space=col_space, na_rep=na_rep, header=header, index=index, formatters=formatters, float_format=float_format, bold_rows=bold_rows, sparsify=sparsify, justify=justify, index_names=index_names, escape=escape, decimal=decimal, max_rows=max_rows, max_cols=max_cols, show_dimensions=show_dimensions)\n        return fmt.DataFrameRenderer(formatter).to_html(buf=buf, classes=classes, notebook=notebook, border=border, encoding=encoding, table_id=table_id, render_links=render_links)\n\n    @overload\n    def to_xml(self, path_or_buffer: None=..., *, index: bool=..., root_name: str | None=..., row_name: str | None=..., na_rep: str | None=..., attr_cols: list[str] | None=..., elem_cols: list[str] | None=..., namespaces: dict[str | None, str] | None=..., prefix: str | None=..., encoding: str=..., xml_declaration: bool | None=..., pretty_print: bool | None=..., parser: XMLParsers | None=..., stylesheet: FilePath | ReadBuffer[str] | ReadBuffer[bytes] | None=..., compression: CompressionOptions=..., storage_options: StorageOptions | None=...) -> str:\n        ...\n\n    @overload\n    def to_xml(self, path_or_buffer: FilePath | WriteBuffer[bytes] | WriteBuffer[str], *, index: bool=..., root_name: str | None=..., row_name: str | None=..., na_rep: str | None=..., attr_cols: list[str] | None=..., elem_cols: list[str] | None=..., namespaces: dict[str | None, str] | None=..., prefix: str | None=..., encoding: str=..., xml_declaration: bool | None=..., pretty_print: bool | None=..., parser: XMLParsers | None=..., stylesheet: FilePath | ReadBuffer[str] | ReadBuffer[bytes] | None=..., compression: CompressionOptions=..., storage_options: StorageOptions | None=...) -> None:\n        ...\n\n    @deprecate_nonkeyword_arguments(version='3.0', allowed_args=['self', 'path_or_buffer'], name='to_xml')\n    @doc(storage_options=_shared_docs['storage_options'], compression_options=_shared_docs['compression_options'] % 'path_or_buffer')\n    def to_xml(self, path_or_buffer: FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None=None, index: bool=True, root_name: str | None='data', row_name: str | None='row', na_rep: str | None=None, attr_cols: list[str] | None=None, elem_cols: list[str] | None=None, namespaces: dict[str | None, str] | None=None, prefix: str | None=None, encoding: str='utf-8', xml_declaration: bool | None=True, pretty_print: bool | None=True, parser: XMLParsers | None='lxml', stylesheet: FilePath | ReadBuffer[str] | ReadBuffer[bytes] | None=None, compression: CompressionOptions='infer', storage_options: StorageOptions | None=None) -> str | None:\n        \"\"\"\n        Render a DataFrame to an XML document.\n\n        .. versionadded:: 1.3.0\n\n        Parameters\n        ----------\n        path_or_buffer : str, path object, file-like object, or None, default None\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a ``write()`` function. If None, the result is returned\n            as a string.\n        index : bool, default True\n            Whether to include index in XML document.\n        root_name : str, default 'data'\n            The name of root element in XML document.\n        row_name : str, default 'row'\n            The name of row element in XML document.\n        na_rep : str, optional\n            Missing data representation.\n        attr_cols : list-like, optional\n            List of columns to write as attributes in row element.\n            Hierarchical columns will be flattened with underscore\n            delimiting the different levels.\n        elem_cols : list-like, optional\n            List of columns to write as children in row element. By default,\n            all columns output as children of row element. Hierarchical\n            columns will be flattened with underscore delimiting the\n            different levels.\n        namespaces : dict, optional\n            All namespaces to be defined in root element. Keys of dict\n            should be prefix names and values of dict corresponding URIs.\n            Default namespaces should be given empty string key. For\n            example, ::\n\n                namespaces = {{\"\": \"https://example.com\"}}\n\n        prefix : str, optional\n            Namespace prefix to be used for every element and/or attribute\n            in document. This should be one of the keys in ``namespaces``\n            dict.\n        encoding : str, default 'utf-8'\n            Encoding of the resulting document.\n        xml_declaration : bool, default True\n            Whether to include the XML declaration at start of document.\n        pretty_print : bool, default True\n            Whether output should be pretty printed with indentation and\n            line breaks.\n        parser : {{'lxml','etree'}}, default 'lxml'\n            Parser module to use for building of tree. Only 'lxml' and\n            'etree' are supported. With 'lxml', the ability to use XSLT\n            stylesheet is supported.\n        stylesheet : str, path object or file-like object, optional\n            A URL, file-like object, or a raw string containing an XSLT\n            script used to transform the raw XML output. Script should use\n            layout of elements and attributes from original output. This\n            argument requires ``lxml`` to be installed. Only XSLT 1.0\n            scripts and not later versions is currently supported.\n        {compression_options}\n\n            .. versionchanged:: 1.4.0 Zstandard support.\n\n        {storage_options}\n\n        Returns\n        -------\n        None or str\n            If ``io`` is None, returns the resulting XML format as a\n            string. Otherwise returns None.\n\n        See Also\n        --------\n        to_json : Convert the pandas object to a JSON string.\n        to_html : Convert DataFrame to a html.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({{'shape': ['square', 'circle', 'triangle'],\n        ...                    'degrees': [360, 360, 180],\n        ...                    'sides': [4, np.nan, 3]}})\n\n        >>> df.to_xml()  # doctest: +SKIP\n        <?xml version='1.0' encoding='utf-8'?>\n        <data>\n          <row>\n            <index>0</index>\n            <shape>square</shape>\n            <degrees>360</degrees>\n            <sides>4.0</sides>\n          </row>\n          <row>\n            <index>1</index>\n            <shape>circle</shape>\n            <degrees>360</degrees>\n            <sides/>\n          </row>\n          <row>\n            <index>2</index>\n            <shape>triangle</shape>\n            <degrees>180</degrees>\n            <sides>3.0</sides>\n          </row>\n        </data>\n\n        >>> df.to_xml(attr_cols=[\n        ...           'index', 'shape', 'degrees', 'sides'\n        ...           ])  # doctest: +SKIP\n        <?xml version='1.0' encoding='utf-8'?>\n        <data>\n          <row index=\"0\" shape=\"square\" degrees=\"360\" sides=\"4.0\"/>\n          <row index=\"1\" shape=\"circle\" degrees=\"360\"/>\n          <row index=\"2\" shape=\"triangle\" degrees=\"180\" sides=\"3.0\"/>\n        </data>\n\n        >>> df.to_xml(namespaces={{\"doc\": \"https://example.com\"}},\n        ...           prefix=\"doc\")  # doctest: +SKIP\n        <?xml version='1.0' encoding='utf-8'?>\n        <doc:data xmlns:doc=\"https://example.com\">\n          <doc:row>\n            <doc:index>0</doc:index>\n            <doc:shape>square</doc:shape>\n            <doc:degrees>360</doc:degrees>\n            <doc:sides>4.0</doc:sides>\n          </doc:row>\n          <doc:row>\n            <doc:index>1</doc:index>\n            <doc:shape>circle</doc:shape>\n            <doc:degrees>360</doc:degrees>\n            <doc:sides/>\n          </doc:row>\n          <doc:row>\n            <doc:index>2</doc:index>\n            <doc:shape>triangle</doc:shape>\n            <doc:degrees>180</doc:degrees>\n            <doc:sides>3.0</doc:sides>\n          </doc:row>\n        </doc:data>\n        \"\"\"\n        from pandas.io.formats.xml import EtreeXMLFormatter, LxmlXMLFormatter\n        lxml = import_optional_dependency('lxml.etree', errors='ignore')\n        TreeBuilder: type[EtreeXMLFormatter | LxmlXMLFormatter]\n        if parser == 'lxml':\n            if lxml is not None:\n                TreeBuilder = LxmlXMLFormatter\n            else:\n                raise ImportError('lxml not found, please install or use the etree parser.')\n        elif parser == 'etree':\n            TreeBuilder = EtreeXMLFormatter\n        else:\n            raise ValueError('Values for parser can only be lxml or etree.')\n        xml_formatter = TreeBuilder(self, path_or_buffer=path_or_buffer, index=index, root_name=root_name, row_name=row_name, na_rep=na_rep, attr_cols=attr_cols, elem_cols=elem_cols, namespaces=namespaces, prefix=prefix, encoding=encoding, xml_declaration=xml_declaration, pretty_print=pretty_print, stylesheet=stylesheet, compression=compression, storage_options=storage_options)\n        return xml_formatter.write_output()\n\n    def _reindex_multi(self, axes: dict[str, Index], copy: bool, fill_value) -> DataFrame:\n        \"\"\"\n        We are guaranteed non-Nones in the axes.\n        \"\"\"\n        (new_index, row_indexer) = self.index.reindex(axes['index'])\n        (new_columns, col_indexer) = self.columns.reindex(axes['columns'])\n        if row_indexer is not None and col_indexer is not None:\n            indexer = (row_indexer, col_indexer)\n            new_values = take_2d_multi(self.values, indexer, fill_value=fill_value)\n            return self._constructor(new_values, index=new_index, columns=new_columns, copy=False)\n        else:\n            return self._reindex_with_indexers({0: [new_index, row_indexer], 1: [new_columns, col_indexer]}, copy=copy, fill_value=fill_value)\n\n    def _get_values_for_csv(self, *, float_format: FloatFormatType | None, date_format: str | None, decimal: str, na_rep: str, quoting) -> Self:\n        mgr = self._mgr.get_values_for_csv(float_format=float_format, date_format=date_format, decimal=decimal, na_rep=na_rep, quoting=quoting)\n        return self._constructor_from_mgr(mgr, axes=mgr.axes)\n\n    def _cmp_method(self, other, op):\n        axis: Literal[1] = 1\n        (self, other) = self._align_for_op(other, axis, flex=False, level=None)\n        new_data = self._dispatch_frame_op(other, op, axis=axis)\n        return self._construct_result(new_data)\n\n    @property\n    def T(self) -> DataFrame:\n        \"\"\"\n        The transpose of the DataFrame.\n\n        Returns\n        -------\n        DataFrame\n            The transposed DataFrame.\n\n        See Also\n        --------\n        DataFrame.transpose : Transpose index and columns.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df\n           col1  col2\n        0     1     3\n        1     2     4\n\n        >>> df.T\n              0  1\n        col1  1  2\n        col2  3  4\n        \"\"\"\n        return self.transpose()\n\n    def combine_first(self, other: DataFrame) -> DataFrame:\n        \"\"\"\n        Update null elements with value in the same location in `other`.\n\n        Combine two DataFrame objects by filling null values in one DataFrame\n        with non-null values from other DataFrame. The row and column indexes\n        of the resulting DataFrame will be the union of the two. The resulting\n        dataframe contains the 'first' dataframe values and overrides the\n        second one values where both first.loc[index, col] and\n        second.loc[index, col] are not missing values, upon calling\n        first.combine_first(second).\n\n        Parameters\n        ----------\n        other : DataFrame\n            Provided DataFrame to use to fill null values.\n\n        Returns\n        -------\n        DataFrame\n            The result of combining the provided DataFrame with the other object.\n\n        See Also\n        --------\n        DataFrame.combine : Perform series-wise operation on two DataFrames\n            using a given function.\n\n        Examples\n        --------\n        >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine_first(df2)\n             A    B\n        0  1.0  3.0\n        1  0.0  4.0\n\n        Null values still persist if the location of that null value\n        does not exist in `other`\n\n        >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])\n        >>> df1.combine_first(df2)\n             A    B    C\n        0  NaN  4.0  NaN\n        1  0.0  3.0  1.0\n        2  NaN  3.0  1.0\n        \"\"\"\n        from pandas.core.computation import expressions\n\n        def combiner(x: Series, y: Series):\n            mask = x.isna()._values\n            x_values = x._values\n            y_values = y._values\n            if y.name not in self.columns:\n                return y_values\n            return expressions.where(mask, y_values, x_values)\n        if len(other) == 0:\n            combined = self.reindex(self.columns.append(other.columns.difference(self.columns)), axis=1)\n            combined = combined.astype(other.dtypes)\n        else:\n            combined = self.combine(other, combiner, overwrite=False)\n        dtypes = {col: find_common_type([self.dtypes[col], other.dtypes[col]]) for col in self.columns.intersection(other.columns) if combined.dtypes[col] != self.dtypes[col]}\n        if dtypes:\n            combined = combined.astype(dtypes)\n        return combined.__finalize__(self, method='combine_first')\n\n    @doc(make_doc('cummax', ndim=2))\n    def cummax(self, axis: Axis | None=None, skipna: bool=True, *args, **kwargs):\n        return NDFrame.cummax(self, axis, skipna, *args, **kwargs)\n\n    def __divmod__(self, other) -> tuple[DataFrame, DataFrame]:\n        div = self // other\n        mod = self - div * other\n        return (div, mod)\n\n    @Substitution('')\n    @Appender(_shared_docs['pivot'])\n    def pivot(self, *, columns, index=lib.no_default, values=lib.no_default) -> DataFrame:\n        from pandas.core.reshape.pivot import pivot\n        return pivot(self, index=index, columns=columns, values=values)\n\n    def to_period(self, freq: Frequency | None=None, axis: Axis=0, copy: bool | None=None) -> DataFrame:\n        \"\"\"\n        Convert DataFrame from DatetimeIndex to PeriodIndex.\n\n        Convert DataFrame from DatetimeIndex to PeriodIndex with desired\n        frequency (inferred from index if not passed).\n\n        Parameters\n        ----------\n        freq : str, default\n            Frequency of the PeriodIndex.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to convert (the index by default).\n        copy : bool, default True\n            If False then underlying input data is not copied.\n\n            .. note::\n                The `copy` keyword will change behavior in pandas 3.0.\n                `Copy-on-Write\n                <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n                will be enabled by default, which means that all methods with a\n                `copy` keyword will use a lazy copy mechanism to defer the copy and\n                ignore the `copy` keyword. The `copy` keyword will be removed in a\n                future version of pandas.\n\n                You can already get the future behavior and improvements through\n                enabling copy on write ``pd.options.mode.copy_on_write = True``\n\n        Returns\n        -------\n        DataFrame\n            The DataFrame has a PeriodIndex.\n\n        Examples\n        --------\n        >>> idx = pd.to_datetime(\n        ...     [\n        ...         \"2001-03-31 00:00:00\",\n        ...         \"2002-05-31 00:00:00\",\n        ...         \"2003-08-31 00:00:00\",\n        ...     ]\n        ... )\n\n        >>> idx\n        DatetimeIndex(['2001-03-31', '2002-05-31', '2003-08-31'],\n        dtype='datetime64[ns]', freq=None)\n\n        >>> idx.to_period(\"M\")\n        PeriodIndex(['2001-03', '2002-05', '2003-08'], dtype='period[M]')\n\n        For the yearly frequency\n\n        >>> idx.to_period(\"Y\")\n        PeriodIndex(['2001', '2002', '2003'], dtype='period[Y-DEC]')\n        \"\"\"\n        new_obj = self.copy(deep=copy and (not using_copy_on_write()))\n        axis_name = self._get_axis_name(axis)\n        old_ax = getattr(self, axis_name)\n        if not isinstance(old_ax, DatetimeIndex):\n            raise TypeError(f'unsupported Type {type(old_ax).__name__}')\n        new_ax = old_ax.to_period(freq=freq)\n        setattr(new_obj, axis_name, new_ax)\n        return new_obj\n\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs['klass'])\n    def notna(self) -> DataFrame:\n        return ~self.isna()\n\n    @doc(make_doc('sem', ndim=2))\n    def sem(self, axis: Axis | None=0, skipna: bool=True, ddof: int=1, numeric_only: bool=False, **kwargs):\n        result = super().sem(axis, skipna, ddof, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method='sem')\n        return result\n\n    @doc(NDFrame.shift, klass=_shared_doc_kwargs['klass'])\n    def shift(self, periods: int | Sequence[int]=1, freq: Frequency | None=None, axis: Axis=0, fill_value: Hashable=lib.no_default, suffix: str | None=None) -> DataFrame:\n        if freq is not None and fill_value is not lib.no_default:\n            warnings.warn(\"Passing a 'freq' together with a 'fill_value' silently ignores the fill_value and is deprecated. This will raise in a future version.\", FutureWarning, stacklevel=find_stack_level())\n            fill_value = lib.no_default\n        if self.empty:\n            return self.copy()\n        axis = self._get_axis_number(axis)\n        if is_list_like(periods):\n            periods = cast(Sequence, periods)\n            if axis == 1:\n                raise ValueError('If `periods` contains multiple shifts, `axis` cannot be 1.')\n            if len(periods) == 0:\n                raise ValueError('If `periods` is an iterable, it cannot be empty.')\n            from pandas.core.reshape.concat import concat\n            shifted_dataframes = []\n            for period in periods:\n                if not is_integer(period):\n                    raise TypeError(f'Periods must be integer, but {period} is {type(period)}.')\n                period = cast(int, period)\n                shifted_dataframes.append(super().shift(periods=period, freq=freq, axis=axis, fill_value=fill_value).add_suffix(f'{suffix}_{period}' if suffix else f'_{period}'))\n            return concat(shifted_dataframes, axis=1)\n        elif suffix:\n            raise ValueError('Cannot specify `suffix` if `periods` is an int.')\n        periods = cast(int, periods)\n        ncols = len(self.columns)\n        arrays = self._mgr.arrays\n        if axis == 1 and periods != 0 and (ncols > 0) and (freq is None):\n            if fill_value is lib.no_default:\n                label = self.columns[0]\n                if periods > 0:\n                    result = self.iloc[:, :-periods]\n                    for col in range(min(ncols, abs(periods))):\n                        filler = self.iloc[:, 0].shift(len(self))\n                        result.insert(0, label, filler, allow_duplicates=True)\n                else:\n                    result = self.iloc[:, -periods:]\n                    for col in range(min(ncols, abs(periods))):\n                        filler = self.iloc[:, -1].shift(len(self))\n                        result.insert(len(result.columns), label, filler, allow_duplicates=True)\n                result.columns = self.columns.copy()\n                return result\n            elif len(arrays) > 1 or not can_hold_element(arrays[0], fill_value):\n                nper = abs(periods)\n                nper = min(nper, ncols)\n                if periods > 0:\n                    indexer = np.array([-1] * nper + list(range(ncols - periods)), dtype=np.intp)\n                else:\n                    indexer = np.array(list(range(nper, ncols)) + [-1] * nper, dtype=np.intp)\n                mgr = self._mgr.reindex_indexer(self.columns, indexer, axis=0, fill_value=fill_value, allow_dups=True)\n                res_df = self._constructor_from_mgr(mgr, axes=mgr.axes)\n                return res_df.__finalize__(self, method='shift')\n            else:\n                return self.T.shift(periods=periods, fill_value=fill_value).T\n        return super().shift(periods=periods, freq=freq, axis=axis, fill_value=fill_value)\n\n    def explode(self, column: IndexLabel, ignore_index: bool=False) -> DataFrame:\n        \"\"\"\n        Transform each element of a list-like to a row, replicating index values.\n\n        Parameters\n        ----------\n        column : IndexLabel\n            Column(s) to explode.\n            For multiple columns, specify a non-empty list with each element\n            be str or tuple, and all specified columns their list-like data\n            on same row of the frame must have matching length.\n\n            .. versionadded:: 1.3.0\n                Multi-column explode\n\n        ignore_index : bool, default False\n            If True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\n        Returns\n        -------\n        DataFrame\n            Exploded lists to rows of the subset columns;\n            index will be duplicated for these rows.\n\n        Raises\n        ------\n        ValueError :\n            * If columns of the frame are not unique.\n            * If specified columns to explode is empty list.\n            * If specified columns to explode have not matching count of\n              elements rowwise in the frame.\n\n        See Also\n        --------\n        DataFrame.unstack : Pivot a level of the (necessarily hierarchical)\n            index labels.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        Series.explode : Explode a DataFrame from list-like columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples, sets,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged, and empty list-likes will\n        result in a np.nan for that row. In addition, the ordering of rows in the\n        output will be non-deterministic when exploding sets.\n\n        Reference :ref:`the user guide <reshaping.explode>` for more examples.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [[0, 1, 2], 'foo', [], [3, 4]],\n        ...                    'B': 1,\n        ...                    'C': [['a', 'b', 'c'], np.nan, [], ['d', 'e']]})\n        >>> df\n                   A  B          C\n        0  [0, 1, 2]  1  [a, b, c]\n        1        foo  1        NaN\n        2         []  1         []\n        3     [3, 4]  1     [d, e]\n\n        Single-column explode.\n\n        >>> df.explode('A')\n             A  B          C\n        0    0  1  [a, b, c]\n        0    1  1  [a, b, c]\n        0    2  1  [a, b, c]\n        1  foo  1        NaN\n        2  NaN  1         []\n        3    3  1     [d, e]\n        3    4  1     [d, e]\n\n        Multi-column explode.\n\n        >>> df.explode(list('AC'))\n             A  B    C\n        0    0  1    a\n        0    1  1    b\n        0    2  1    c\n        1  foo  1  NaN\n        2  NaN  1  NaN\n        3    3  1    d\n        3    4  1    e\n        \"\"\"\n        if not self.columns.is_unique:\n            duplicate_cols = self.columns[self.columns.duplicated()].tolist()\n            raise ValueError(f'DataFrame columns must be unique. Duplicate columns: {duplicate_cols}')\n        columns: list[Hashable]\n        if is_scalar(column) or isinstance(column, tuple):\n            columns = [column]\n        elif isinstance(column, list) and all((is_scalar(c) or isinstance(c, tuple) for c in column)):\n            if not column:\n                raise ValueError('column must be nonempty')\n            if len(column) > len(set(column)):\n                raise ValueError('column must be unique')\n            columns = column\n        else:\n            raise ValueError('column must be a scalar, tuple, or list thereof')\n        df = self.reset_index(drop=True)\n        if len(columns) == 1:\n            result = df[columns[0]].explode()\n        else:\n            mylen = lambda x: len(x) if is_list_like(x) and len(x) > 0 else 1\n            counts0 = self[columns[0]].apply(mylen)\n            for c in columns[1:]:\n                if not all(counts0 == self[c].apply(mylen)):\n                    raise ValueError('columns must have matching element counts')\n            result = DataFrame({c: df[c].explode() for c in columns})\n        result = df.drop(columns, axis=1).join(result)\n        if ignore_index:\n            result.index = default_index(len(result))\n        else:\n            result.index = self.index.take(result.index)\n        result = result.reindex(columns=self.columns, copy=False)\n        return result.__finalize__(self, method='explode')\n\n    def __rdivmod__(self, other) -> tuple[DataFrame, DataFrame]:\n        div = other // self\n        mod = other - div * self\n        return (div, mod)\n\n    def nsmallest(self, n: int, columns: IndexLabel, keep: NsmallestNlargestKeep='first') -> DataFrame:\n        \"\"\"\n        Return the first `n` rows ordered by `columns` in ascending order.\n\n        Return the first `n` rows with the smallest values in `columns`, in\n        ascending order. The columns that are not specified are returned as\n        well, but not used for ordering.\n\n        This method is equivalent to\n        ``df.sort_values(columns, ascending=True).head(n)``, but more\n        performant.\n\n        Parameters\n        ----------\n        n : int\n            Number of items to retrieve.\n        columns : list or str\n            Column name or names to order by.\n        keep : {'first', 'last', 'all'}, default 'first'\n            Where there are duplicate values:\n\n            - ``first`` : take the first occurrence.\n            - ``last`` : take the last occurrence.\n            - ``all`` : keep all the ties of the largest item even if it means\n              selecting more than ``n`` items.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.nlargest : Return the first `n` rows ordered by `columns` in\n            descending order.\n        DataFrame.sort_values : Sort DataFrame by the values.\n        DataFrame.head : Return the first `n` rows without re-ordering.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n        ...                                   434000, 434000, 337000, 337000,\n        ...                                   11300, 11300],\n        ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n        ...                            17036, 182, 38, 311],\n        ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n        >>> df\n                  population      GDP alpha-2\n        Italy       59000000  1937894      IT\n        France      65000000  2583560      FR\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n        Iceland       337000    17036      IS\n        Nauru         337000      182      NR\n        Tuvalu         11300       38      TV\n        Anguilla       11300      311      AI\n\n        In the following example, we will use ``nsmallest`` to select the\n        three rows having the smallest values in column \"population\".\n\n        >>> df.nsmallest(3, 'population')\n                  population    GDP alpha-2\n        Tuvalu         11300     38      TV\n        Anguilla       11300    311      AI\n        Iceland       337000  17036      IS\n\n        When using ``keep='last'``, ties are resolved in reverse order:\n\n        >>> df.nsmallest(3, 'population', keep='last')\n                  population  GDP alpha-2\n        Anguilla       11300  311      AI\n        Tuvalu         11300   38      TV\n        Nauru         337000  182      NR\n\n        When using ``keep='all'``, the number of element kept can go beyond ``n``\n        if there are duplicate values for the largest element, all the\n        ties are kept.\n\n        >>> df.nsmallest(3, 'population', keep='all')\n                  population    GDP alpha-2\n        Tuvalu         11300     38      TV\n        Anguilla       11300    311      AI\n        Iceland       337000  17036      IS\n        Nauru         337000    182      NR\n\n        However, ``nsmallest`` does not keep ``n`` distinct\n        smallest elements:\n\n        >>> df.nsmallest(4, 'population', keep='all')\n                  population    GDP alpha-2\n        Tuvalu         11300     38      TV\n        Anguilla       11300    311      AI\n        Iceland       337000  17036      IS\n        Nauru         337000    182      NR\n\n        To order by the smallest values in column \"population\" and then \"GDP\", we can\n        specify multiple columns like in the next example.\n\n        >>> df.nsmallest(3, ['population', 'GDP'])\n                  population  GDP alpha-2\n        Tuvalu         11300   38      TV\n        Anguilla       11300  311      AI\n        Nauru         337000  182      NR\n        \"\"\"\n        return selectn.SelectNFrame(self, n=n, keep=keep, columns=columns).nsmallest()\n\n    @classmethod\n    def from_dict(cls, data: dict, orient: FromDictOrient='columns', dtype: Dtype | None=None, columns: Axes | None=None) -> DataFrame:\n        \"\"\"\n        Construct DataFrame from dict of array-like or dicts.\n\n        Creates DataFrame object from dictionary by columns or by index\n        allowing dtype specification.\n\n        Parameters\n        ----------\n        data : dict\n            Of the form {field : array-like} or {field : dict}.\n        orient : {'columns', 'index', 'tight'}, default 'columns'\n            The \"orientation\" of the data. If the keys of the passed dict\n            should be the columns of the resulting DataFrame, pass 'columns'\n            (default). Otherwise if the keys should be rows, pass 'index'.\n            If 'tight', assume a dict with keys ['index', 'columns', 'data',\n            'index_names', 'column_names'].\n\n            .. versionadded:: 1.4.0\n               'tight' as an allowed value for the ``orient`` argument\n\n        dtype : dtype, default None\n            Data type to force after DataFrame construction, otherwise infer.\n        columns : list, default None\n            Column labels to use when ``orient='index'``. Raises a ValueError\n            if used with ``orient='columns'`` or ``orient='tight'``.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.from_records : DataFrame from structured ndarray, sequence\n            of tuples or dicts, or DataFrame.\n        DataFrame : DataFrame object creation using constructor.\n        DataFrame.to_dict : Convert the DataFrame to a dictionary.\n\n        Examples\n        --------\n        By default the keys of the dict become the DataFrame columns:\n\n        >>> data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\n        >>> pd.DataFrame.from_dict(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Specify ``orient='index'`` to create the DataFrame using dictionary\n        keys as rows:\n\n        >>> data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n        >>> pd.DataFrame.from_dict(data, orient='index')\n               0  1  2  3\n        row_1  3  2  1  0\n        row_2  a  b  c  d\n\n        When using the 'index' orientation, the column names can be\n        specified manually:\n\n        >>> pd.DataFrame.from_dict(data, orient='index',\n        ...                        columns=['A', 'B', 'C', 'D'])\n               A  B  C  D\n        row_1  3  2  1  0\n        row_2  a  b  c  d\n\n        Specify ``orient='tight'`` to create the DataFrame using a 'tight'\n        format:\n\n        >>> data = {'index': [('a', 'b'), ('a', 'c')],\n        ...         'columns': [('x', 1), ('y', 2)],\n        ...         'data': [[1, 3], [2, 4]],\n        ...         'index_names': ['n1', 'n2'],\n        ...         'column_names': ['z1', 'z2']}\n        >>> pd.DataFrame.from_dict(data, orient='tight')\n        z1     x  y\n        z2     1  2\n        n1 n2\n        a  b   1  3\n           c   2  4\n        \"\"\"\n        index = None\n        orient = orient.lower()\n        if orient == 'index':\n            if len(data) > 0:\n                if isinstance(next(iter(data.values())), (Series, dict)):\n                    data = _from_nested_dict(data)\n                else:\n                    index = list(data.keys())\n                    data = list(data.values())\n        elif orient in ('columns', 'tight'):\n            if columns is not None:\n                raise ValueError(f\"cannot use columns parameter with orient='{orient}'\")\n        else:\n            raise ValueError(f\"Expected 'index', 'columns' or 'tight' for orient parameter. Got '{orient}' instead\")\n        if orient != 'tight':\n            return cls(data, index=index, columns=columns, dtype=dtype)\n        else:\n            realdata = data['data']\n\n            def create_index(indexlist, namelist):\n                index: Index\n                if len(namelist) > 1:\n                    index = MultiIndex.from_tuples(indexlist, names=namelist)\n                else:\n                    index = Index(indexlist, name=namelist[0])\n                return index\n            index = create_index(data['index'], data['index_names'])\n            columns = create_index(data['columns'], data['column_names'])\n            return cls(realdata, index=index, columns=columns, dtype=dtype)\n\n    def _clear_item_cache(self) -> None:\n        self._item_cache.clear()\n\n    def round(self, decimals: int | dict[IndexLabel, int] | Series=0, *args, **kwargs) -> DataFrame:\n        \"\"\"\n        Round a DataFrame to a variable number of decimal places.\n\n        Parameters\n        ----------\n        decimals : int, dict, Series\n            Number of decimal places to round each column to. If an int is\n            given, round each column to the same number of places.\n            Otherwise dict and Series round to variable numbers of places.\n            Column names should be in the keys if `decimals` is a\n            dict-like, or in the index if `decimals` is a Series. Any\n            columns not included in `decimals` will be left as is. Elements\n            of `decimals` which are not columns of the input will be\n            ignored.\n        *args\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n        **kwargs\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n\n        Returns\n        -------\n        DataFrame\n            A DataFrame with the affected columns rounded to the specified\n            number of decimal places.\n\n        See Also\n        --------\n        numpy.around : Round a numpy array to the given number of decimals.\n        Series.round : Round a Series to the given number of decimals.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df\n            dogs  cats\n        0  0.21  0.32\n        1  0.01  0.67\n        2  0.66  0.03\n        3  0.21  0.18\n\n        By providing an integer each column is rounded to the same number\n        of decimal places\n\n        >>> df.round(1)\n            dogs  cats\n        0   0.2   0.3\n        1   0.0   0.7\n        2   0.7   0.0\n        3   0.2   0.2\n\n        With a dict, the number of places for specific columns can be\n        specified with the column names as key and the number of decimal\n        places as value\n\n        >>> df.round({'dogs': 1, 'cats': 0})\n            dogs  cats\n        0   0.2   0.0\n        1   0.0   1.0\n        2   0.7   0.0\n        3   0.2   0.0\n\n        Using a Series, the number of places for specific columns can be\n        specified with the column names as index and the number of\n        decimal places as value\n\n        >>> decimals = pd.Series([0, 1], index=['cats', 'dogs'])\n        >>> df.round(decimals)\n            dogs  cats\n        0   0.2   0.0\n        1   0.0   1.0\n        2   0.7   0.0\n        3   0.2   0.0\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        def _dict_round(df: DataFrame, decimals):\n            for (col, vals) in df.items():\n                try:\n                    yield _series_round(vals, decimals[col])\n                except KeyError:\n                    yield vals\n\n        def _series_round(ser: Series, decimals: int) -> Series:\n            if is_integer_dtype(ser.dtype) or is_float_dtype(ser.dtype):\n                return ser.round(decimals)\n            return ser\n        nv.validate_round(args, kwargs)\n        if isinstance(decimals, (dict, Series)):\n            if isinstance(decimals, Series) and (not decimals.index.is_unique):\n                raise ValueError('Index of decimals must be unique')\n            if is_dict_like(decimals) and (not all((is_integer(value) for (_, value) in decimals.items()))):\n                raise TypeError('Values in decimals must be integers')\n            new_cols = list(_dict_round(self, decimals))\n        elif is_integer(decimals):\n            new_mgr = self._mgr.round(decimals=decimals, using_cow=using_copy_on_write())\n            return self._constructor_from_mgr(new_mgr, axes=new_mgr.axes).__finalize__(self, method='round')\n        else:\n            raise TypeError('decimals must be an integer, a dict-like or a Series')\n        if new_cols is not None and len(new_cols) > 0:\n            return self._constructor(concat(new_cols, axis=1), index=self.index, columns=self.columns).__finalize__(self, method='round')\n        else:\n            return self.copy(deep=False)\n\n    def apply(self, func: AggFuncType, axis: Axis=0, raw: bool=False, result_type: Literal['expand', 'reduce', 'broadcast'] | None=None, args=(), by_row: Literal[False, 'compat']='compat', engine: Literal['python', 'numba']='python', engine_kwargs: dict[str, bool] | None=None, **kwargs):\n        \"\"\"\n        Apply a function along an axis of the DataFrame.\n\n        Objects passed to the function are Series objects whose index is\n        either the DataFrame's index (``axis=0``) or the DataFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to apply to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': apply function to each column.\n            * 1 or 'columns': apply function to each row.\n\n        raw : bool, default False\n            Determines if row or column is passed as a Series or ndarray object:\n\n            * ``False`` : passes each row or column as a Series to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just applying a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Series if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the DataFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Series\n            of those. However if the apply function returns a Series these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/series.\n        by_row : False or \"compat\", default \"compat\"\n            Only has an effect when ``func`` is a listlike or dictlike of funcs\n            and the func isn't a string.\n            If \"compat\", will if possible first translate the func into pandas\n            methods (e.g. ``Series().apply(np.sum)`` will be translated to\n            ``Series().sum()``). If that doesn't work, will try call to apply again with\n            ``by_row=True`` and if that fails, will call apply again with\n            ``by_row=False`` (backward compatible).\n            If False, the funcs will be passed the whole Series at once.\n\n            .. versionadded:: 2.1.0\n\n        engine : {'python', 'numba'}, default 'python'\n            Choose between the python (default) engine or the numba engine in apply.\n\n            The numba engine will attempt to JIT compile the passed function,\n            which may result in speedups for large DataFrames.\n            It also supports the following engine_kwargs :\n\n            - nopython (compile the function in nopython mode)\n            - nogil (release the GIL inside the JIT compiled function)\n            - parallel (try to apply the function in parallel over the DataFrame)\n\n              Note: Due to limitations within numba/how pandas interfaces with numba,\n              you should only use this if raw=True\n\n            Note: The numba compiler only supports a subset of\n            valid Python/numpy operations.\n\n            Please read more about the `supported python features\n            <https://numba.pydata.org/numba-doc/dev/reference/pysupported.html>`_\n            and `supported numpy features\n            <https://numba.pydata.org/numba-doc/dev/reference/numpysupported.html>`_\n            in numba to learn what you can or cannot use in the passed function.\n\n            .. versionadded:: 2.2.0\n\n        engine_kwargs : dict\n            Pass keyword arguments to the engine.\n            This is currently only used by the numba engine,\n            see the documentation for the engine argument for more information.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Series or DataFrame\n            Result of applying ``func`` along the given axis of the\n            DataFrame.\n\n        See Also\n        --------\n        DataFrame.map: For elementwise operations.\n        DataFrame.aggregate: Only perform aggregating type operations.\n        DataFrame.transform: Only perform transforming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n        for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> df\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(df)``):\n\n        >>> df.apply(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> df.apply(np.sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> df.apply(np.sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Series\n\n        >>> df.apply(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Series inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Series index.\n\n        >>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        \"\"\"\n        from pandas.core.apply import frame_apply\n        op = frame_apply(self, func=func, axis=axis, raw=raw, result_type=result_type, by_row=by_row, engine=engine, engine_kwargs=engine_kwargs, args=args, kwargs=kwargs)\n        return op.apply().__finalize__(self, method='apply')\n\n    def nunique(self, axis: Axis=0, dropna: bool=True) -> Series:\n        \"\"\"\n        Count number of distinct elements in specified axis.\n\n        Return Series with number of distinct elements. Can ignore NaN\n        values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for\n            column-wise.\n        dropna : bool, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        Series\n\n        See Also\n        --------\n        Series.nunique: Method nunique for Series.\n        DataFrame.count: Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [4, 5, 6], 'B': [4, 1, 1]})\n        >>> df.nunique()\n        A    3\n        B    2\n        dtype: int64\n\n        >>> df.nunique(axis=1)\n        0    1\n        1    2\n        2    2\n        dtype: int64\n        \"\"\"\n        return self.apply(Series.nunique, axis=axis, dropna=dropna)\n\n    @Substitution('')\n    @Appender(_merge_doc, indents=2)\n    def merge(self, right: DataFrame | Series, how: MergeHow='inner', on: IndexLabel | AnyArrayLike | None=None, left_on: IndexLabel | AnyArrayLike | None=None, right_on: IndexLabel | AnyArrayLike | None=None, left_index: bool=False, right_index: bool=False, sort: bool=False, suffixes: Suffixes=('_x', '_y'), copy: bool | None=None, indicator: str | bool=False, validate: MergeValidate | None=None) -> DataFrame:\n        from pandas.core.reshape.merge import merge\n        return merge(self, right, how=how, on=on, left_on=left_on, right_on=right_on, left_index=left_index, right_index=right_index, sort=sort, suffixes=suffixes, copy=copy, indicator=indicator, validate=validate)\n\n    @doc(storage_options=_shared_docs['storage_options'], compression_options=_shared_docs['compression_options'] % 'path')\n    def to_stata(self, path: FilePath | WriteBuffer[bytes], *, convert_dates: dict[Hashable, str] | None=None, write_index: bool=True, byteorder: ToStataByteorder | None=None, time_stamp: datetime.datetime | None=None, data_label: str | None=None, variable_labels: dict[Hashable, str] | None=None, version: int | None=114, convert_strl: Sequence[Hashable] | None=None, compression: CompressionOptions='infer', storage_options: StorageOptions | None=None, value_labels: dict[Hashable, dict[float, str]] | None=None) -> None:\n        \"\"\"\n        Export DataFrame object to Stata dta format.\n\n        Writes the DataFrame to a Stata dataset file.\n        \"dta\" files contain a Stata dataset.\n\n        Parameters\n        ----------\n        path : str, path object, or buffer\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a binary ``write()`` function.\n\n        convert_dates : dict\n            Dictionary mapping columns containing datetime types to stata\n            internal format to use when writing the dates. Options are 'tc',\n            'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer\n            or a name. Datetime columns that do not have a conversion type\n            specified will be converted to 'tc'. Raises NotImplementedError if\n            a datetime column has timezone information.\n        write_index : bool\n            Write the index to Stata dataset.\n        byteorder : str\n            Can be \">\", \"<\", \"little\", or \"big\". default is `sys.byteorder`.\n        time_stamp : datetime\n            A datetime to use as file creation date.  Default is the current\n            time.\n        data_label : str, optional\n            A label for the data set.  Must be 80 characters or smaller.\n        variable_labels : dict\n            Dictionary containing columns as keys and variable labels as\n            values. Each label must be 80 characters or smaller.\n        version : {{114, 117, 118, 119, None}}, default 114\n            Version to use in the output dta file. Set to None to let pandas\n            decide between 118 or 119 formats depending on the number of\n            columns in the frame. Version 114 can be read by Stata 10 and\n            later. Version 117 can be read by Stata 13 or later. Version 118\n            is supported in Stata 14 and later. Version 119 is supported in\n            Stata 15 and later. Version 114 limits string variables to 244\n            characters or fewer while versions 117 and later allow strings\n            with lengths up to 2,000,000 characters. Versions 118 and 119\n            support Unicode characters, and version 119 supports more than\n            32,767 variables.\n\n            Version 119 should usually only be used when the number of\n            variables exceeds the capacity of dta format 118. Exporting\n            smaller datasets in format 119 may have unintended consequences,\n            and, as of November 2020, Stata SE cannot read version 119 files.\n\n        convert_strl : list, optional\n            List of column names to convert to string columns to Stata StrL\n            format. Only available if version is 117.  Storing strings in the\n            StrL format can produce smaller dta files if strings have more than\n            8 characters and values are repeated.\n        {compression_options}\n\n            .. versionchanged:: 1.4.0 Zstandard support.\n\n        {storage_options}\n\n        value_labels : dict of dicts\n            Dictionary containing columns as keys and dictionaries of column value\n            to labels as values. Labels for a single variable must be 32,000\n            characters or smaller.\n\n            .. versionadded:: 1.4.0\n\n        Raises\n        ------\n        NotImplementedError\n            * If datetimes contain timezone information\n            * Column dtype is not representable in Stata\n        ValueError\n            * Columns listed in convert_dates are neither datetime64[ns]\n              or datetime.datetime\n            * Column listed in convert_dates is not in DataFrame\n            * Categorical label contains more than 32,000 characters\n\n        See Also\n        --------\n        read_stata : Import Stata data files.\n        io.stata.StataWriter : Low-level writer for Stata data files.\n        io.stata.StataWriter117 : Low-level writer for version 117 files.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({{'animal': ['falcon', 'parrot', 'falcon',\n        ...                               'parrot'],\n        ...                    'speed': [350, 18, 361, 15]}})\n        >>> df.to_stata('animals.dta')  # doctest: +SKIP\n        \"\"\"\n        if version not in (114, 117, 118, 119, None):\n            raise ValueError('Only formats 114, 117, 118 and 119 are supported.')\n        if version == 114:\n            if convert_strl is not None:\n                raise ValueError('strl is not supported in format 114')\n            from pandas.io.stata import StataWriter as statawriter\n        elif version == 117:\n            from pandas.io.stata import StataWriter117 as statawriter\n        else:\n            from pandas.io.stata import StataWriterUTF8 as statawriter\n        kwargs: dict[str, Any] = {}\n        if version is None or version >= 117:\n            kwargs['convert_strl'] = convert_strl\n        if version is None or version >= 118:\n            kwargs['version'] = version\n        writer = statawriter(path, self, convert_dates=convert_dates, byteorder=byteorder, time_stamp=time_stamp, data_label=data_label, write_index=write_index, variable_labels=variable_labels, compression=compression, storage_options=storage_options, value_labels=value_labels, **kwargs)\n        writer.write_file()\n\n    @doc(make_doc('kurt', ndim=2))\n    def kurt(self, axis: Axis | None=0, skipna: bool=True, numeric_only: bool=False, **kwargs):\n        result = super().kurt(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method='kurt')\n        return result\n\n    def _ensure_valid_index(self, value) -> None:\n        \"\"\"\n        Ensure that if we don't have an index, that we can create one from the\n        passed value.\n        \"\"\"\n        if not len(self.index) and is_list_like(value) and len(value):\n            if not isinstance(value, DataFrame):\n                try:\n                    value = Series(value)\n                except (ValueError, NotImplementedError, TypeError) as err:\n                    raise ValueError('Cannot set a frame with no defined index and a value that cannot be converted to a Series') from err\n            index_copy = value.index.copy()\n            if self.index.name is not None:\n                index_copy.name = self.index.name\n            self._mgr = self._mgr.reindex_axis(index_copy, axis=1, fill_value=np.nan)\n\n    def _constructor_sliced_from_mgr(self, mgr, axes) -> Series:\n        ser = Series._from_mgr(mgr, axes)\n        ser._name = None\n        if type(self) is DataFrame:\n            return ser\n        return self._constructor_sliced(ser)\n\n    def quantile(self, q: float | AnyArrayLike | Sequence[float]=0.5, axis: Axis=0, numeric_only: bool=False, interpolation: QuantileInterpolation='linear', method: Literal['single', 'table']='single') -> Series | DataFrame:\n        \"\"\"\n        Return values at the given quantile over requested axis.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            Value between 0 <= q <= 1, the quantile(s) to compute.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionchanged:: 2.0.0\n                The default value of ``numeric_only`` is now ``False``.\n\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n            * linear: `i + (j - i) * fraction`, where `fraction` is the\n              fractional part of the index surrounded by `i` and `j`.\n            * lower: `i`.\n            * higher: `j`.\n            * nearest: `i` or `j` whichever is nearest.\n            * midpoint: (`i` + `j`) / 2.\n        method : {'single', 'table'}, default 'single'\n            Whether to compute quantiles per-column ('single') or over all columns\n            ('table'). When 'table', the only allowed interpolation methods are\n            'nearest', 'lower', and 'higher'.\n\n        Returns\n        -------\n        Series or DataFrame\n\n            If ``q`` is an array, a DataFrame will be returned where the\n              index is ``q``, the columns are the columns of self, and the\n              values are the quantiles.\n            If ``q`` is a float, a Series will be returned where the\n              index is the columns of self and the values are the quantiles.\n\n        See Also\n        --------\n        core.window.rolling.Rolling.quantile: Rolling quantile.\n        numpy.percentile: Numpy function to compute the percentile.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),\n        ...                   columns=['a', 'b'])\n        >>> df.quantile(.1)\n        a    1.3\n        b    3.7\n        Name: 0.1, dtype: float64\n        >>> df.quantile([.1, .5])\n               a     b\n        0.1  1.3   3.7\n        0.5  2.5  55.0\n\n        Specifying `method='table'` will compute the quantile over all columns.\n\n        >>> df.quantile(.1, method=\"table\", interpolation=\"nearest\")\n        a    1\n        b    1\n        Name: 0.1, dtype: int64\n        >>> df.quantile([.1, .5], method=\"table\", interpolation=\"nearest\")\n             a    b\n        0.1  1    1\n        0.5  3  100\n\n        Specifying `numeric_only=False` will also compute the quantile of\n        datetime and timedelta data.\n\n        >>> df = pd.DataFrame({'A': [1, 2],\n        ...                    'B': [pd.Timestamp('2010'),\n        ...                          pd.Timestamp('2011')],\n        ...                    'C': [pd.Timedelta('1 days'),\n        ...                          pd.Timedelta('2 days')]})\n        >>> df.quantile(0.5, numeric_only=False)\n        A                    1.5\n        B    2010-07-02 12:00:00\n        C        1 days 12:00:00\n        Name: 0.5, dtype: object\n        \"\"\"\n        validate_percentile(q)\n        axis = self._get_axis_number(axis)\n        if not is_list_like(q):\n            res_df = self.quantile([q], axis=axis, numeric_only=numeric_only, interpolation=interpolation, method=method)\n            if method == 'single':\n                res = res_df.iloc[0]\n            else:\n                res = res_df.T.iloc[:, 0]\n            if axis == 1 and len(self) == 0:\n                dtype = find_common_type(list(self.dtypes))\n                if needs_i8_conversion(dtype):\n                    return res.astype(dtype)\n            return res\n        q = Index(q, dtype=np.float64)\n        data = self._get_numeric_data() if numeric_only else self\n        if axis == 1:\n            data = data.T\n        if len(data.columns) == 0:\n            cols = Index([], name=self.columns.name)\n            dtype = np.float64\n            if axis == 1:\n                cdtype = find_common_type(list(self.dtypes))\n                if needs_i8_conversion(cdtype):\n                    dtype = cdtype\n            res = self._constructor([], index=q, columns=cols, dtype=dtype)\n            return res.__finalize__(self, method='quantile')\n        valid_method = {'single', 'table'}\n        if method not in valid_method:\n            raise ValueError(f'Invalid method: {method}. Method must be in {valid_method}.')\n        if method == 'single':\n            res = data._mgr.quantile(qs=q, interpolation=interpolation)\n        elif method == 'table':\n            valid_interpolation = {'nearest', 'lower', 'higher'}\n            if interpolation not in valid_interpolation:\n                raise ValueError(f'Invalid interpolation: {interpolation}. Interpolation must be in {valid_interpolation}')\n            if len(data) == 0:\n                if data.ndim == 2:\n                    dtype = find_common_type(list(self.dtypes))\n                else:\n                    dtype = self.dtype\n                return self._constructor([], index=q, columns=data.columns, dtype=dtype)\n            q_idx = np.quantile(np.arange(len(data)), q, method=interpolation)\n            by = data.columns\n            if len(by) > 1:\n                keys = [data._get_label_or_level_values(x) for x in by]\n                indexer = lexsort_indexer(keys)\n            else:\n                k = data._get_label_or_level_values(by[0])\n                indexer = nargsort(k)\n            res = data._mgr.take(indexer[q_idx], verify=False)\n            res.axes[1] = q\n        result = self._constructor_from_mgr(res, axes=res.axes)\n        return result.__finalize__(self, method='quantile')\n\n    @Appender(ops.make_flex_doc('radd', 'dataframe'))\n    def radd(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, roperator.radd, level=level, fill_value=fill_value, axis=axis)\n\n    def transpose(self, *args, copy: bool=False) -> DataFrame:\n        \"\"\"\n        Transpose index and columns.\n\n        Reflect the DataFrame over its main diagonal by writing rows as columns\n        and vice-versa. The property :attr:`.T` is an accessor to the method\n        :meth:`transpose`.\n\n        Parameters\n        ----------\n        *args : tuple, optional\n            Accepted for compatibility with NumPy.\n        copy : bool, default False\n            Whether to copy the data after transposing, even for DataFrames\n            with a single dtype.\n\n            Note that a copy is always required for mixed dtype DataFrames,\n            or for DataFrames with any extension types.\n\n            .. note::\n                The `copy` keyword will change behavior in pandas 3.0.\n                `Copy-on-Write\n                <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n                will be enabled by default, which means that all methods with a\n                `copy` keyword will use a lazy copy mechanism to defer the copy and\n                ignore the `copy` keyword. The `copy` keyword will be removed in a\n                future version of pandas.\n\n                You can already get the future behavior and improvements through\n                enabling copy on write ``pd.options.mode.copy_on_write = True``\n\n        Returns\n        -------\n        DataFrame\n            The transposed DataFrame.\n\n        See Also\n        --------\n        numpy.transpose : Permute the dimensions of a given array.\n\n        Notes\n        -----\n        Transposing a DataFrame with mixed dtypes will result in a homogeneous\n        DataFrame with the `object` dtype. In such a case, a copy of the data\n        is always made.\n\n        Examples\n        --------\n        **Square DataFrame with homogeneous dtype**\n\n        >>> d1 = {'col1': [1, 2], 'col2': [3, 4]}\n        >>> df1 = pd.DataFrame(data=d1)\n        >>> df1\n           col1  col2\n        0     1     3\n        1     2     4\n\n        >>> df1_transposed = df1.T  # or df1.transpose()\n        >>> df1_transposed\n              0  1\n        col1  1  2\n        col2  3  4\n\n        When the dtype is homogeneous in the original DataFrame, we get a\n        transposed DataFrame with the same dtype:\n\n        >>> df1.dtypes\n        col1    int64\n        col2    int64\n        dtype: object\n        >>> df1_transposed.dtypes\n        0    int64\n        1    int64\n        dtype: object\n\n        **Non-square DataFrame with mixed dtypes**\n\n        >>> d2 = {'name': ['Alice', 'Bob'],\n        ...       'score': [9.5, 8],\n        ...       'employed': [False, True],\n        ...       'kids': [0, 0]}\n        >>> df2 = pd.DataFrame(data=d2)\n        >>> df2\n            name  score  employed  kids\n        0  Alice    9.5     False     0\n        1    Bob    8.0      True     0\n\n        >>> df2_transposed = df2.T  # or df2.transpose()\n        >>> df2_transposed\n                      0     1\n        name      Alice   Bob\n        score       9.5   8.0\n        employed  False  True\n        kids          0     0\n\n        When the DataFrame has mixed dtypes, we get a transposed DataFrame with\n        the `object` dtype:\n\n        >>> df2.dtypes\n        name         object\n        score       float64\n        employed       bool\n        kids          int64\n        dtype: object\n        >>> df2_transposed.dtypes\n        0    object\n        1    object\n        dtype: object\n        \"\"\"\n        nv.validate_transpose(args, {})\n        dtypes = list(self.dtypes)\n        if self._can_fast_transpose:\n            new_vals = self._values.T\n            if copy and (not using_copy_on_write()):\n                new_vals = new_vals.copy()\n            result = self._constructor(new_vals, index=self.columns, columns=self.index, copy=False, dtype=new_vals.dtype)\n            if using_copy_on_write() and len(self) > 0:\n                result._mgr.add_references(self._mgr)\n        elif self._is_homogeneous_type and dtypes and isinstance(dtypes[0], ExtensionDtype):\n            new_values: list\n            if isinstance(dtypes[0], BaseMaskedDtype):\n                from pandas.core.arrays.masked import transpose_homogeneous_masked_arrays\n                new_values = transpose_homogeneous_masked_arrays(cast(Sequence[BaseMaskedArray], self._iter_column_arrays()))\n            elif isinstance(dtypes[0], ArrowDtype):\n                from pandas.core.arrays.arrow.array import ArrowExtensionArray, transpose_homogeneous_pyarrow\n                new_values = transpose_homogeneous_pyarrow(cast(Sequence[ArrowExtensionArray], self._iter_column_arrays()))\n            else:\n                dtyp = dtypes[0]\n                arr_typ = dtyp.construct_array_type()\n                values = self.values\n                new_values = [arr_typ._from_sequence(row, dtype=dtyp) for row in values]\n            result = type(self)._from_arrays(new_values, index=self.columns, columns=self.index, verify_integrity=False)\n        else:\n            new_arr = self.values.T\n            if copy and (not using_copy_on_write()):\n                new_arr = new_arr.copy()\n            result = self._constructor(new_arr, index=self.columns, columns=self.index, dtype=new_arr.dtype, copy=False)\n        return result.__finalize__(self, method='transpose')\n\n    @Appender(ops.make_flex_doc('ne', 'dataframe'))\n    def ne(self, other, axis: Axis='columns', level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.ne, axis=axis, level=level)\n\n    def _reduce_axis1(self, name: str, func, skipna: bool) -> Series:\n        \"\"\"\n        Special case for _reduce to try to avoid a potentially-expensive transpose.\n\n        Apply the reduction block-wise along axis=1 and then reduce the resulting\n        1D arrays.\n        \"\"\"\n        if name == 'all':\n            result = np.ones(len(self), dtype=bool)\n            ufunc = np.logical_and\n        elif name == 'any':\n            result = np.zeros(len(self), dtype=bool)\n            ufunc = np.logical_or\n        else:\n            raise NotImplementedError(name)\n        for arr in self._mgr.arrays:\n            middle = func(arr, axis=0, skipna=skipna)\n            result = ufunc(result, middle)\n        res_ser = self._constructor_sliced(result, index=self.index, copy=False)\n        return res_ser\n\n    @overload\n    def query(self, expr: str, *, inplace: Literal[False]=..., **kwargs) -> DataFrame:\n        ...\n\n    @overload\n    def query(self, expr: str, *, inplace: Literal[True], **kwargs) -> None:\n        ...\n\n    @overload\n    def query(self, expr: str, *, inplace: bool=..., **kwargs) -> DataFrame | None:\n        ...\n\n    @Appender(ops.make_flex_doc('le', 'dataframe'))\n    def le(self, other, axis: Axis='columns', level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.le, axis=axis, level=level)\n\n    @overload\n    def eval(self, expr: str, *, inplace: Literal[False]=..., **kwargs) -> Any:\n        ...\n\n    @overload\n    def eval(self, expr: str, *, inplace: Literal[True], **kwargs) -> None:\n        ...\n\n    def _iset_not_inplace(self, key, value):\n\n        def igetitem(obj, i: int):\n            if isinstance(obj, np.ndarray):\n                return obj[..., i]\n            else:\n                return obj[i]\n        if self.columns.is_unique:\n            if np.shape(value)[-1] != len(key):\n                raise ValueError('Columns must be same length as key')\n            for (i, col) in enumerate(key):\n                self[col] = igetitem(value, i)\n        else:\n            ilocs = self.columns.get_indexer_non_unique(key)[0]\n            if (ilocs < 0).any():\n                raise NotImplementedError\n            if np.shape(value)[-1] != len(ilocs):\n                raise ValueError('Columns must be same length as key')\n            assert np.ndim(value) <= 2\n            orig_columns = self.columns\n            try:\n                self.columns = Index(range(len(self.columns)))\n                for (i, iloc) in enumerate(ilocs):\n                    self[iloc] = igetitem(value, i)\n            finally:\n                self.columns = orig_columns\n\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs['klass'])\n    def isnull(self) -> DataFrame:\n        \"\"\"\n        DataFrame.isnull is an alias for DataFrame.isna.\n        \"\"\"\n        return self.isna()\n\n    def to_numpy(self, dtype: npt.DTypeLike | None=None, copy: bool=False, na_value: object=lib.no_default) -> np.ndarray:\n        \"\"\"\n        Convert the DataFrame to a NumPy array.\n\n        By default, the dtype of the returned array will be the common NumPy\n        dtype of all types in the DataFrame. For example, if the dtypes are\n        ``float16`` and ``float32``, the results dtype will be ``float32``.\n        This may require copying data and coercing values, which may be\n        expensive.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to pass to :meth:`numpy.asarray`.\n        copy : bool, default False\n            Whether to ensure that the returned value is not a view on\n            another array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary.\n        na_value : Any, optional\n            The value to use for missing values. The default value depends\n            on `dtype` and the dtypes of the DataFrame columns.\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        Series.to_numpy : Similar method for Series.\n\n        Examples\n        --------\n        >>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\n        array([[1, 3],\n               [2, 4]])\n\n        With heterogeneous data, the lowest common type will have to\n        be used.\n\n        >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\n        >>> df.to_numpy()\n        array([[1. , 3. ],\n               [2. , 4.5]])\n\n        For a mix of numeric and non-numeric types, the output array will\n        have object dtype.\n\n        >>> df['C'] = pd.date_range('2000', periods=2)\n        >>> df.to_numpy()\n        array([[1, 3.0, Timestamp('2000-01-01 00:00:00')],\n               [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)\n        \"\"\"\n        if dtype is not None:\n            dtype = np.dtype(dtype)\n        result = self._mgr.as_array(dtype=dtype, copy=copy, na_value=na_value)\n        if result.dtype is not dtype:\n            result = np.asarray(result, dtype=dtype)\n        return result\n\n    @doc(make_doc('skew', ndim=2))\n    def skew(self, axis: Axis | None=0, skipna: bool=True, numeric_only: bool=False, **kwargs):\n        result = super().skew(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method='skew')\n        return result\n\n    @doc(make_doc('all', ndim=2))\n    def all(self, axis: Axis | None=0, bool_only: bool=False, skipna: bool=True, **kwargs) -> Series | bool:\n        result = self._logical_func('all', nanops.nanall, axis, bool_only, skipna, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method='all')\n        return result\n\n    @property\n    def _series(self):\n        return {item: self._ixs(idx, axis=1) for (idx, item) in enumerate(self.columns)}\n\n    @Appender(ops.make_flex_doc('rfloordiv', 'dataframe'))\n    def rfloordiv(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, roperator.rfloordiv, level=level, fill_value=fill_value, axis=axis)\n\n    @Appender('\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\\n\\n        Change the row labels.\\n\\n        >>> df.set_axis([\\'a\\', \\'b\\', \\'c\\'], axis=\\'index\\')\\n           A  B\\n        a  1  4\\n        b  2  5\\n        c  3  6\\n\\n        Change the column labels.\\n\\n        >>> df.set_axis([\\'I\\', \\'II\\'], axis=\\'columns\\')\\n           I  II\\n        0  1   4\\n        1  2   5\\n        2  3   6\\n        ')\n    @Substitution(klass=_shared_doc_kwargs['klass'], axes_single_arg=_shared_doc_kwargs['axes_single_arg'], extended_summary_sub=' column or', axis_description_sub=', and 1 identifies the columns', see_also_sub=' or columns')\n    @Appender(NDFrame.set_axis.__doc__)\n    def set_axis(self, labels, *, axis: Axis=0, copy: bool | None=None) -> DataFrame:\n        return super().set_axis(labels, axis=axis, copy=copy)\n\n    def to_orc(self, path: FilePath | WriteBuffer[bytes] | None=None, *, engine: Literal['pyarrow']='pyarrow', index: bool | None=None, engine_kwargs: dict[str, Any] | None=None) -> bytes | None:\n        \"\"\"\n        Write a DataFrame to the ORC format.\n\n        .. versionadded:: 1.5.0\n\n        Parameters\n        ----------\n        path : str, file-like object or None, default None\n            If a string, it will be used as Root Directory path\n            when writing a partitioned dataset. By file-like object,\n            we refer to objects with a write() method, such as a file handle\n            (e.g. via builtin open function). If path is None,\n            a bytes object is returned.\n        engine : {'pyarrow'}, default 'pyarrow'\n            ORC library to use.\n        index : bool, optional\n            If ``True``, include the dataframe's index(es) in the file output.\n            If ``False``, they will not be written to the file.\n            If ``None``, similar to ``infer`` the dataframe's index(es)\n            will be saved. However, instead of being saved as values,\n            the RangeIndex will be stored as a range in the metadata so it\n            doesn't require much space and is faster. Other indexes will\n            be included as columns in the file output.\n        engine_kwargs : dict[str, Any] or None, default None\n            Additional keyword arguments passed to :func:`pyarrow.orc.write_table`.\n\n        Returns\n        -------\n        bytes if no path argument is provided else None\n\n        Raises\n        ------\n        NotImplementedError\n            Dtype of one or more columns is category, unsigned integers, interval,\n            period or sparse.\n        ValueError\n            engine is not pyarrow.\n\n        See Also\n        --------\n        read_orc : Read a ORC file.\n        DataFrame.to_parquet : Write a parquet file.\n        DataFrame.to_csv : Write a csv file.\n        DataFrame.to_sql : Write to a sql table.\n        DataFrame.to_hdf : Write to hdf.\n\n        Notes\n        -----\n        * Before using this function you should read the :ref:`user guide about\n          ORC <io.orc>` and :ref:`install optional dependencies <install.warn_orc>`.\n        * This function requires `pyarrow <https://arrow.apache.org/docs/python/>`_\n          library.\n        * For supported dtypes please refer to `supported ORC features in Arrow\n          <https://arrow.apache.org/docs/cpp/orc.html#data-types>`__.\n        * Currently timezones in datetime columns are not preserved when a\n          dataframe is converted into ORC files.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [4, 3]})\n        >>> df.to_orc('df.orc')  # doctest: +SKIP\n        >>> pd.read_orc('df.orc')  # doctest: +SKIP\n           col1  col2\n        0     1     4\n        1     2     3\n\n        If you want to get a buffer to the orc content you can write it to io.BytesIO\n\n        >>> import io\n        >>> b = io.BytesIO(df.to_orc())  # doctest: +SKIP\n        >>> b.seek(0)  # doctest: +SKIP\n        0\n        >>> content = b.read()  # doctest: +SKIP\n        \"\"\"\n        from pandas.io.orc import to_orc\n        return to_orc(self, path, engine=engine, index=index, engine_kwargs=engine_kwargs)\n\n    @overload\n    def drop(self, labels: IndexLabel=..., *, axis: Axis=..., index: IndexLabel=..., columns: IndexLabel=..., level: Level=..., inplace: Literal[True], errors: IgnoreRaise=...) -> None:\n        ...\n\n    @overload\n    def drop(self, labels: IndexLabel=..., *, axis: Axis=..., index: IndexLabel=..., columns: IndexLabel=..., level: Level=..., inplace: Literal[False]=..., errors: IgnoreRaise=...) -> DataFrame:\n        ...\n\n    @overload\n    def drop(self, labels: IndexLabel=..., *, axis: Axis=..., index: IndexLabel=..., columns: IndexLabel=..., level: Level=..., inplace: bool=..., errors: IgnoreRaise=...) -> DataFrame | None:\n        ...\n\n    @doc(_shared_docs['idxmax'], numeric_only_default='False')\n    def idxmax(self, axis: Axis=0, skipna: bool=True, numeric_only: bool=False) -> Series:\n        axis = self._get_axis_number(axis)\n        if self.empty and len(self.axes[axis]):\n            axis_dtype = self.axes[axis].dtype\n            return self._constructor_sliced(dtype=axis_dtype)\n        if numeric_only:\n            data = self._get_numeric_data()\n        else:\n            data = self\n        res = data._reduce(nanops.nanargmax, 'argmax', axis=axis, skipna=skipna, numeric_only=False)\n        indices = res._values\n        if (indices == -1).any():\n            warnings.warn(f'The behavior of {type(self).__name__}.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError', FutureWarning, stacklevel=find_stack_level())\n        index = data._get_axis(axis)\n        result = algorithms.take(index._values, indices, allow_fill=True, fill_value=index._na_value)\n        final_result = data._constructor_sliced(result, index=data._get_agg_axis(axis))\n        return final_result.__finalize__(self, method='idxmax')\n\n    @overload\n    def rename(self, mapper: Renamer | None=..., *, index: Renamer | None=..., columns: Renamer | None=..., axis: Axis | None=..., copy: bool | None=..., inplace: Literal[True], level: Level=..., errors: IgnoreRaise=...) -> None:\n        ...\n\n    @overload\n    def rename(self, mapper: Renamer | None=..., *, index: Renamer | None=..., columns: Renamer | None=..., axis: Axis | None=..., copy: bool | None=..., inplace: Literal[False]=..., level: Level=..., errors: IgnoreRaise=...) -> DataFrame:\n        ...\n\n    @overload\n    def rename(self, mapper: Renamer | None=..., *, index: Renamer | None=..., columns: Renamer | None=..., axis: Axis | None=..., copy: bool | None=..., inplace: bool=..., level: Level=..., errors: IgnoreRaise=...) -> DataFrame | None:\n        ...\n\n    @Appender(ops.make_flex_doc('truediv', 'dataframe'))\n    def truediv(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, operator.truediv, level=level, fill_value=fill_value, axis=axis)\n\n    def _getitem_multilevel(self, key):\n        loc = self.columns.get_loc(key)\n        if isinstance(loc, (slice, np.ndarray)):\n            new_columns = self.columns[loc]\n            result_columns = maybe_droplevels(new_columns, key)\n            result = self.iloc[:, loc]\n            result.columns = result_columns\n            if len(result.columns) == 1:\n                top = result.columns[0]\n                if isinstance(top, tuple):\n                    top = top[0]\n                if top == '':\n                    result = result['']\n                    if isinstance(result, Series):\n                        result = self._constructor_sliced(result, index=self.index, name=key)\n            result._set_is_copy(self)\n            return result\n        else:\n            return self._ixs(loc, axis=1)\n\n    @Appender(ops.make_flex_doc('ge', 'dataframe'))\n    def ge(self, other, axis: Axis='columns', level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.ge, axis=axis, level=level)\n\n    def __arrow_c_stream__(self, requested_schema=None):\n        \"\"\"\n        Export the pandas DataFrame as an Arrow C stream PyCapsule.\n\n        This relies on pyarrow to convert the pandas DataFrame to the Arrow\n        format (and follows the default behaviour of ``pyarrow.Table.from_pandas``\n        in its handling of the index, i.e. store the index as a column except\n        for RangeIndex).\n        This conversion is not necessarily zero-copy.\n\n        Parameters\n        ----------\n        requested_schema : PyCapsule, default None\n            The schema to which the dataframe should be casted, passed as a\n            PyCapsule containing a C ArrowSchema representation of the\n            requested schema.\n\n        Returns\n        -------\n        PyCapsule\n        \"\"\"\n        pa = import_optional_dependency('pyarrow', min_version='14.0.0')\n        if requested_schema is not None:\n            requested_schema = pa.Schema._import_from_c_capsule(requested_schema)\n        table = pa.Table.from_pandas(self, schema=requested_schema)\n        return table.__arrow_c_stream__()\n\n    @overload\n    def set_index(self, keys, *, drop: bool=..., append: bool=..., inplace: Literal[False]=..., verify_integrity: bool=...) -> DataFrame:\n        ...\n\n    @overload\n    def set_index(self, keys, *, drop: bool=..., append: bool=..., inplace: Literal[True], verify_integrity: bool=...) -> None:\n        ...\n\n    @doc(make_doc('cumprod', 2))\n    def cumprod(self, axis: Axis | None=None, skipna: bool=True, *args, **kwargs):\n        return NDFrame.cumprod(self, axis, skipna, *args, **kwargs)\n\n    @overload\n    def reset_index(self, level: IndexLabel=..., *, drop: bool=..., inplace: Literal[False]=..., col_level: Hashable=..., col_fill: Hashable=..., allow_duplicates: bool | lib.NoDefault=..., names: Hashable | Sequence[Hashable] | None=None) -> DataFrame:\n        ...\n\n    @overload\n    def reset_index(self, level: IndexLabel=..., *, drop: bool=..., inplace: Literal[True], col_level: Hashable=..., col_fill: Hashable=..., allow_duplicates: bool | lib.NoDefault=..., names: Hashable | Sequence[Hashable] | None=None) -> None:\n        ...\n\n    @overload\n    def reset_index(self, level: IndexLabel=..., *, drop: bool=..., inplace: bool=..., col_level: Hashable=..., col_fill: Hashable=..., allow_duplicates: bool | lib.NoDefault=..., names: Hashable | Sequence[Hashable] | None=None) -> DataFrame | None:\n        ...\n\n    @doc(make_doc('var', ndim=2))\n    def var(self, axis: Axis | None=0, skipna: bool=True, ddof: int=1, numeric_only: bool=False, **kwargs):\n        result = super().var(axis, skipna, ddof, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method='var')\n        return result\n\n    def count(self, axis: Axis=0, numeric_only: bool=False):\n        \"\"\"\n        Count non-NA cells for each column or row.\n\n        The values `None`, `NaN`, `NaT`, ``pandas.NA`` are considered NA.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            If 0 or 'index' counts are generated for each column.\n            If 1 or 'columns' counts are generated for each row.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n        Returns\n        -------\n        Series\n            For each column/row the number of non-NA/null entries.\n\n        See Also\n        --------\n        Series.count: Number of non-NA elements in a Series.\n        DataFrame.value_counts: Count unique combinations of columns.\n        DataFrame.shape: Number of DataFrame rows and columns (including NA\n            elements).\n        DataFrame.isna: Boolean same-sized DataFrame showing places of NA\n            elements.\n\n        Examples\n        --------\n        Constructing DataFrame from a dictionary:\n\n        >>> df = pd.DataFrame({\"Person\":\n        ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n        ...                    \"Age\": [24., np.nan, 21., 33, 26],\n        ...                    \"Single\": [False, True, True, True, False]})\n        >>> df\n           Person   Age  Single\n        0    John  24.0   False\n        1    Myla   NaN    True\n        2   Lewis  21.0    True\n        3    John  33.0    True\n        4    Myla  26.0   False\n\n        Notice the uncounted NA values:\n\n        >>> df.count()\n        Person    5\n        Age       4\n        Single    5\n        dtype: int64\n\n        Counts for each **row**:\n\n        >>> df.count(axis='columns')\n        0    3\n        1    2\n        2    3\n        3    3\n        4    3\n        dtype: int64\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        if numeric_only:\n            frame = self._get_numeric_data()\n        else:\n            frame = self\n        if len(frame._get_axis(axis)) == 0:\n            result = self._constructor_sliced(0, index=frame._get_agg_axis(axis))\n        else:\n            result = notna(frame).sum(axis=axis)\n        return result.astype('int64', copy=False).__finalize__(self, method='count')\n\n    def _flex_arith_method(self, other, op, *, axis: Axis='columns', level=None, fill_value=None):\n        axis = self._get_axis_number(axis) if axis is not None else 1\n        if self._should_reindex_frame_op(other, op, axis, fill_value, level):\n            return self._arith_method_with_reindex(other, op)\n        if isinstance(other, Series) and fill_value is not None:\n            raise NotImplementedError(f'fill_value {fill_value} not supported.')\n        other = ops.maybe_prepare_scalar_for_op(other, self.shape)\n        (self, other) = self._align_for_op(other, axis, flex=True, level=level)\n        with np.errstate(all='ignore'):\n            if isinstance(other, DataFrame):\n                new_data = self._combine_frame(other, op, fill_value)\n            elif isinstance(other, Series):\n                new_data = self._dispatch_frame_op(other, op, axis=axis)\n            else:\n                if fill_value is not None:\n                    self = self.fillna(fill_value)\n                new_data = self._dispatch_frame_op(other, op)\n        return self._construct_result(new_data)\n\n    def combine(self, other: DataFrame, func: Callable[[Series, Series], Series | Hashable], fill_value=None, overwrite: bool=True) -> DataFrame:\n        \"\"\"\n        Perform column-wise combine with another DataFrame.\n\n        Combines a DataFrame with `other` DataFrame using `func`\n        to element-wise combine columns. The row and column indexes of the\n        resulting DataFrame will be the union of the two.\n\n        Parameters\n        ----------\n        other : DataFrame\n            The DataFrame to merge column-wise.\n        func : function\n            Function that takes two series as inputs and return a Series or a\n            scalar. Used to merge the two dataframes column by columns.\n        fill_value : scalar value, default None\n            The value to fill NaNs with prior to passing any column to the\n            merge func.\n        overwrite : bool, default True\n            If True, columns in `self` that do not exist in `other` will be\n            overwritten with NaNs.\n\n        Returns\n        -------\n        DataFrame\n            Combination of the provided DataFrames.\n\n        See Also\n        --------\n        DataFrame.combine_first : Combine two DataFrame objects and default to\n            non-null values in frame calling the method.\n\n        Examples\n        --------\n        Combine using a simple function that chooses the smaller column.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2\n        >>> df1.combine(df2, take_smaller)\n           A  B\n        0  0  3\n        1  0  3\n\n        Example using a true element-wise combine function.\n\n        >>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine(df2, np.minimum)\n           A  B\n        0  1  2\n        1  0  3\n\n        Using `fill_value` fills Nones prior to passing the column to the\n        merge function.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\n           A    B\n        0  0 -5.0\n        1  0  4.0\n\n        However, if the same element in both dataframes is None, that None\n        is preserved\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\n            A    B\n        0  0 -5.0\n        1  0  3.0\n\n        Example that demonstrates the use of `overwrite` and behavior when\n        the axis differ between the dataframes.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2])\n        >>> df1.combine(df2, take_smaller)\n             A    B     C\n        0  NaN  NaN   NaN\n        1  NaN  3.0 -10.0\n        2  NaN  3.0   1.0\n\n        >>> df1.combine(df2, take_smaller, overwrite=False)\n             A    B     C\n        0  0.0  NaN   NaN\n        1  0.0  3.0 -10.0\n        2  NaN  3.0   1.0\n\n        Demonstrating the preference of the passed in dataframe.\n\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2])\n        >>> df2.combine(df1, take_smaller)\n           A    B   C\n        0  0.0  NaN NaN\n        1  0.0  3.0 NaN\n        2  NaN  3.0 NaN\n\n        >>> df2.combine(df1, take_smaller, overwrite=False)\n             A    B   C\n        0  0.0  NaN NaN\n        1  0.0  3.0 1.0\n        2  NaN  3.0 1.0\n        \"\"\"\n        other_idxlen = len(other.index)\n        (this, other) = self.align(other, copy=False)\n        new_index = this.index\n        if other.empty and len(new_index) == len(self.index):\n            return self.copy()\n        if self.empty and len(other) == other_idxlen:\n            return other.copy()\n        new_columns = this.columns.union(other.columns)\n        do_fill = fill_value is not None\n        result = {}\n        for col in new_columns:\n            series = this[col]\n            other_series = other[col]\n            this_dtype = series.dtype\n            other_dtype = other_series.dtype\n            this_mask = isna(series)\n            other_mask = isna(other_series)\n            if not overwrite and other_mask.all():\n                result[col] = this[col].copy()\n                continue\n            if do_fill:\n                series = series.copy()\n                other_series = other_series.copy()\n                series[this_mask] = fill_value\n                other_series[other_mask] = fill_value\n            if col not in self.columns:\n                new_dtype = other_dtype\n                try:\n                    series = series.astype(new_dtype, copy=False)\n                except ValueError:\n                    pass\n            else:\n                new_dtype = find_common_type([this_dtype, other_dtype])\n                series = series.astype(new_dtype, copy=False)\n                other_series = other_series.astype(new_dtype, copy=False)\n            arr = func(series, other_series)\n            if isinstance(new_dtype, np.dtype):\n                arr = maybe_downcast_to_dtype(arr, new_dtype)\n            result[col] = arr\n        frame_result = self._constructor(result, index=new_index, columns=new_columns)\n        return frame_result.__finalize__(self, method='combine')\n\n    @Appender(_shared_docs['items'])\n    def items(self) -> Iterable[tuple[Hashable, Series]]:\n        if self.columns.is_unique and hasattr(self, '_item_cache'):\n            for k in self.columns:\n                yield (k, self._get_item_cache(k))\n        else:\n            for (i, k) in enumerate(self.columns):\n                yield (k, self._ixs(i, axis=1))\n\n    @overload\n    def dropna(self, *, axis: Axis=..., how: AnyAll | lib.NoDefault=..., thresh: int | lib.NoDefault=..., subset: IndexLabel=..., inplace: Literal[False]=..., ignore_index: bool=...) -> DataFrame:\n        ...\n\n    @overload\n    def dropna(self, *, axis: Axis=..., how: AnyAll | lib.NoDefault=..., thresh: int | lib.NoDefault=..., subset: IndexLabel=..., inplace: Literal[True], ignore_index: bool=...) -> None:\n        ...\n\n    def value_counts(self, subset: IndexLabel | None=None, normalize: bool=False, sort: bool=True, ascending: bool=False, dropna: bool=True) -> Series:\n        \"\"\"\n        Return a Series containing the frequency of each distinct row in the Dataframe.\n\n        Parameters\n        ----------\n        subset : label or list of labels, optional\n            Columns to use when counting unique combinations.\n        normalize : bool, default False\n            Return proportions rather than frequencies.\n        sort : bool, default True\n            Sort by frequencies when True. Sort by DataFrame column values when False.\n        ascending : bool, default False\n            Sort in ascending order.\n        dropna : bool, default True\n            Don't include counts of rows that contain NA values.\n\n            .. versionadded:: 1.3.0\n\n        Returns\n        -------\n        Series\n\n        See Also\n        --------\n        Series.value_counts: Equivalent method on Series.\n\n        Notes\n        -----\n        The returned Series will have a MultiIndex with one level per input\n        column but an Index (non-multi) for a single label. By default, rows\n        that contain any NA values are omitted from the result. By default,\n        the resulting Series will be in descending order so that the first\n        element is the most frequently-occurring row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6],\n        ...                    'num_wings': [2, 0, 0, 0]},\n        ...                   index=['falcon', 'dog', 'cat', 'ant'])\n        >>> df\n                num_legs  num_wings\n        falcon         2          2\n        dog            4          0\n        cat            4          0\n        ant            6          0\n\n        >>> df.value_counts()\n        num_legs  num_wings\n        4         0            2\n        2         2            1\n        6         0            1\n        Name: count, dtype: int64\n\n        >>> df.value_counts(sort=False)\n        num_legs  num_wings\n        2         2            1\n        4         0            2\n        6         0            1\n        Name: count, dtype: int64\n\n        >>> df.value_counts(ascending=True)\n        num_legs  num_wings\n        2         2            1\n        6         0            1\n        4         0            2\n        Name: count, dtype: int64\n\n        >>> df.value_counts(normalize=True)\n        num_legs  num_wings\n        4         0            0.50\n        2         2            0.25\n        6         0            0.25\n        Name: proportion, dtype: float64\n\n        With `dropna` set to `False` we can also count rows with NA values.\n\n        >>> df = pd.DataFrame({'first_name': ['John', 'Anne', 'John', 'Beth'],\n        ...                    'middle_name': ['Smith', pd.NA, pd.NA, 'Louise']})\n        >>> df\n          first_name middle_name\n        0       John       Smith\n        1       Anne        <NA>\n        2       John        <NA>\n        3       Beth      Louise\n\n        >>> df.value_counts()\n        first_name  middle_name\n        Beth        Louise         1\n        John        Smith          1\n        Name: count, dtype: int64\n\n        >>> df.value_counts(dropna=False)\n        first_name  middle_name\n        Anne        NaN            1\n        Beth        Louise         1\n        John        Smith          1\n                    NaN            1\n        Name: count, dtype: int64\n\n        >>> df.value_counts(\"first_name\")\n        first_name\n        John    2\n        Anne    1\n        Beth    1\n        Name: count, dtype: int64\n        \"\"\"\n        if subset is None:\n            subset = self.columns.tolist()\n        name = 'proportion' if normalize else 'count'\n        counts = self.groupby(subset, dropna=dropna, observed=False)._grouper.size()\n        counts.name = name\n        if sort:\n            counts = counts.sort_values(ascending=ascending)\n        if normalize:\n            counts /= counts.sum()\n        if is_list_like(subset) and len(subset) == 1:\n            counts.index = MultiIndex.from_arrays([counts.index], names=[counts.index.name])\n        return counts\n\n    @overload\n    def drop_duplicates(self, subset: Hashable | Sequence[Hashable] | None=..., *, keep: DropKeep=..., inplace: Literal[True], ignore_index: bool=...) -> None:\n        ...\n\n    @overload\n    def drop_duplicates(self, subset: Hashable | Sequence[Hashable] | None=..., *, keep: DropKeep=..., inplace: Literal[False]=..., ignore_index: bool=...) -> DataFrame:\n        ...\n\n    @overload\n    def drop_duplicates(self, subset: Hashable | Sequence[Hashable] | None=..., *, keep: DropKeep=..., inplace: bool=..., ignore_index: bool=...) -> DataFrame | None:\n        ...\n\n    def _set_item_frame_value(self, key, value: DataFrame) -> None:\n        self._ensure_valid_index(value)\n        if key in self.columns:\n            loc = self.columns.get_loc(key)\n            cols = self.columns[loc]\n            len_cols = 1 if is_scalar(cols) or isinstance(cols, tuple) else len(cols)\n            if len_cols != len(value.columns):\n                raise ValueError('Columns must be same length as key')\n            if isinstance(self.columns, MultiIndex) and isinstance(loc, (slice, Series, np.ndarray, Index)):\n                cols_droplevel = maybe_droplevels(cols, key)\n                if len(cols_droplevel) and (not cols_droplevel.equals(value.columns)):\n                    value = value.reindex(cols_droplevel, axis=1)\n                for (col, col_droplevel) in zip(cols, cols_droplevel):\n                    self[col] = value[col_droplevel]\n                return\n            if is_scalar(cols):\n                self[cols] = value[value.columns[0]]\n                return\n            locs: np.ndarray | list\n            if isinstance(loc, slice):\n                locs = np.arange(loc.start, loc.stop, loc.step)\n            elif is_scalar(loc):\n                locs = [loc]\n            else:\n                locs = loc.nonzero()[0]\n            return self.isetitem(locs, value)\n        if len(value.columns) > 1:\n            raise ValueError(f'Cannot set a DataFrame with multiple columns to the single column {key}')\n        elif len(value.columns) == 0:\n            raise ValueError(f'Cannot set a DataFrame without columns to the column {key}')\n        self[key] = value[value.columns[0]]\n\n    def sort_index(self, *, axis: Axis=0, level: IndexLabel | None=None, ascending: bool | Sequence[bool]=True, inplace: bool=False, kind: SortKind='quicksort', na_position: NaPosition='last', sort_remaining: bool=True, ignore_index: bool=False, key: IndexKeyFunc | None=None) -> DataFrame | None:\n        \"\"\"\n        Sort object by labels (along an axis).\n\n        Returns a new DataFrame sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original DataFrame and returns None.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis along which to sort.  The value 0 identifies the rows,\n            and 1 identifies the columns.\n        level : int or level name or list of ints or list of level names\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list-like of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            Whether to modify the DataFrame rather than creating a new one.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. `mergesort` and `stable` are the only stable algorithms. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            Puts NaNs at the beginning if `first`; `last` puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape. For MultiIndex\n            inputs, the key is applied *per level*.\n\n        Returns\n        -------\n        DataFrame or None\n            The original DataFrame sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.sort_index : Sort Series by the index.\n        DataFrame.sort_values : Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n        ...                   columns=['A'])\n        >>> df.sort_index()\n             A\n        1    4\n        29   2\n        100  1\n        150  5\n        234  3\n\n        By default, it sorts in ascending order, to sort in descending order,\n        use ``ascending=False``\n\n        >>> df.sort_index(ascending=False)\n             A\n        234  3\n        150  5\n        100  1\n        29   2\n        1    4\n\n        A key function can be specified which is applied to the index before\n        sorting. For a ``MultiIndex`` this is applied to each level separately.\n\n        >>> df = pd.DataFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n        >>> df.sort_index(key=lambda x: x.str.lower())\n           a\n        A  1\n        b  2\n        C  3\n        d  4\n        \"\"\"\n        return super().sort_index(axis=axis, level=level, ascending=ascending, inplace=inplace, kind=kind, na_position=na_position, sort_remaining=sort_remaining, ignore_index=ignore_index, key=key)\n\n    @overload\n    def sort_values(self, by: IndexLabel, *, axis: Axis=..., ascending=..., inplace: Literal[False]=..., kind: SortKind=..., na_position: NaPosition=..., ignore_index: bool=..., key: ValueKeyFunc=...) -> DataFrame:\n        ...\n\n    @overload\n    def sort_values(self, by: IndexLabel, *, axis: Axis=..., ascending=..., inplace: Literal[True], kind: SortKind=..., na_position: str=..., ignore_index: bool=..., key: ValueKeyFunc=...) -> None:\n        ...\n\n    def _combine_frame(self, other: DataFrame, func, fill_value=None):\n        if fill_value is None:\n            _arith_op = func\n        else:\n\n            def _arith_op(left, right):\n                (left, right) = ops.fill_binop(left, right, fill_value)\n                return func(left, right)\n        new_data = self._dispatch_frame_op(other, _arith_op)\n        return new_data\n\n    @overload\n    def sort_index(self, *, axis: Axis=..., level: IndexLabel=..., ascending: bool | Sequence[bool]=..., inplace: Literal[True], kind: SortKind=..., na_position: NaPosition=..., sort_remaining: bool=..., ignore_index: bool=..., key: IndexKeyFunc=...) -> None:\n        ...\n\n    @overload\n    def sort_index(self, *, axis: Axis=..., level: IndexLabel=..., ascending: bool | Sequence[bool]=..., inplace: Literal[False]=..., kind: SortKind=..., na_position: NaPosition=..., sort_remaining: bool=..., ignore_index: bool=..., key: IndexKeyFunc=...) -> DataFrame:\n        ...\n\n    @overload\n    def sort_index(self, *, axis: Axis=..., level: IndexLabel=..., ascending: bool | Sequence[bool]=..., inplace: bool=..., kind: SortKind=..., na_position: NaPosition=..., sort_remaining: bool=..., ignore_index: bool=..., key: IndexKeyFunc=...) -> DataFrame | None:\n        ...\n\n    @Substitution('')\n    @Appender(_shared_docs['pivot_table'])\n    def pivot_table(self, values=None, index=None, columns=None, aggfunc: AggFuncType='mean', fill_value=None, margins: bool=False, dropna: bool=True, margins_name: Level='All', observed: bool | lib.NoDefault=lib.no_default, sort: bool=True) -> DataFrame:\n        from pandas.core.reshape.pivot import pivot_table\n        return pivot_table(self, values=values, index=index, columns=columns, aggfunc=aggfunc, fill_value=fill_value, margins=margins, dropna=dropna, margins_name=margins_name, observed=observed, sort=sort)\n\n    @Appender(ops.make_flex_doc('rmul', 'dataframe'))\n    def rmul(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, roperator.rmul, level=level, fill_value=fill_value, axis=axis)\n\n    def __dataframe__(self, nan_as_null: bool=False, allow_copy: bool=True) -> DataFrameXchg:\n        \"\"\"\n        Return the dataframe interchange object implementing the interchange protocol.\n\n        Parameters\n        ----------\n        nan_as_null : bool, default False\n            `nan_as_null` is DEPRECATED and has no effect. Please avoid using\n            it; it will be removed in a future release.\n        allow_copy : bool, default True\n            Whether to allow memory copying when exporting. If set to False\n            it would cause non-zero-copy exports to fail.\n\n        Returns\n        -------\n        DataFrame interchange object\n            The object which consuming library can use to ingress the dataframe.\n\n        Notes\n        -----\n        Details on the interchange protocol:\n        https://data-apis.org/dataframe-protocol/latest/index.html\n\n        Examples\n        --------\n        >>> df_not_necessarily_pandas = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n        >>> interchange_object = df_not_necessarily_pandas.__dataframe__()\n        >>> interchange_object.column_names()\n        Index(['A', 'B'], dtype='object')\n        >>> df_pandas = (pd.api.interchange.from_dataframe\n        ...              (interchange_object.select_columns_by_name(['A'])))\n        >>> df_pandas\n             A\n        0    1\n        1    2\n\n        These methods (``column_names``, ``select_columns_by_name``) should work\n        for any dataframe library which implements the interchange protocol.\n        \"\"\"\n        from pandas.core.interchange.dataframe import PandasDataFrameXchg\n        return PandasDataFrameXchg(self, allow_copy=allow_copy)\n\n    def _setitem_frame(self, key, value):\n        if isinstance(key, np.ndarray):\n            if key.shape != self.shape:\n                raise ValueError('Array conditional must be same shape as self')\n            key = self._constructor(key, **self._construct_axes_dict(), copy=False)\n        if key.size and (not all((is_bool_dtype(dtype) for dtype in key.dtypes))):\n            raise TypeError('Must pass DataFrame or 2-d ndarray with boolean values only')\n        self._check_setitem_copy()\n        self._where(-key, value, inplace=True)\n\n    @Appender(ops.make_flex_doc('add', 'dataframe'))\n    def add(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, operator.add, level=level, fill_value=fill_value, axis=axis)\n\n    def __init__(self, data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -> None:\n        allow_mgr = False\n        if dtype is not None:\n            dtype = self._validate_dtype(dtype)\n        if isinstance(data, DataFrame):\n            data = data._mgr\n            allow_mgr = True\n            if not copy:\n                data = data.copy(deep=False)\n        if isinstance(data, (BlockManager, ArrayManager)):\n            if not allow_mgr:\n                warnings.warn(f'Passing a {type(data).__name__} to {type(self).__name__} is deprecated and will raise in a future version. Use public APIs instead.', DeprecationWarning, stacklevel=1)\n            if using_copy_on_write():\n                data = data.copy(deep=False)\n            if index is None and columns is None and (dtype is None) and (not copy):\n                NDFrame.__init__(self, data)\n                return\n        manager = _get_option('mode.data_manager', silent=True)\n        is_pandas_object = isinstance(data, (Series, Index, ExtensionArray))\n        data_dtype = getattr(data, 'dtype', None)\n        original_dtype = dtype\n        if isinstance(index, set):\n            raise ValueError('index cannot be a set')\n        if isinstance(columns, set):\n            raise ValueError('columns cannot be a set')\n        if copy is None:\n            if isinstance(data, dict):\n                copy = True\n            elif manager == 'array' and isinstance(data, (np.ndarray, ExtensionArray)) and (data.ndim == 2):\n                copy = True\n            elif using_copy_on_write() and (not isinstance(data, (Index, DataFrame, Series))):\n                copy = True\n            else:\n                copy = False\n        if data is None:\n            index = index if index is not None else default_index(0)\n            columns = columns if columns is not None else default_index(0)\n            dtype = dtype if dtype is not None else pandas_dtype(object)\n            data = []\n        if isinstance(data, (BlockManager, ArrayManager)):\n            mgr = self._init_mgr(data, axes={'index': index, 'columns': columns}, dtype=dtype, copy=copy)\n        elif isinstance(data, dict):\n            mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n        elif isinstance(data, ma.MaskedArray):\n            from numpy.ma import mrecords\n            if isinstance(data, mrecords.MaskedRecords):\n                raise TypeError('MaskedRecords are not supported. Pass {name: data[name] for name in data.dtype.names} instead')\n            data = sanitize_masked_array(data)\n            mgr = ndarray_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n        elif isinstance(data, (np.ndarray, Series, Index, ExtensionArray)):\n            if data.dtype.names:\n                data = cast(np.ndarray, data)\n                mgr = rec_array_to_mgr(data, index, columns, dtype, copy, typ=manager)\n            elif getattr(data, 'name', None) is not None:\n                _copy = copy if using_copy_on_write() else True\n                mgr = dict_to_mgr({data.name: data}, index, columns, dtype=dtype, typ=manager, copy=_copy)\n            else:\n                mgr = ndarray_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n        elif is_list_like(data):\n            if not isinstance(data, abc.Sequence):\n                if hasattr(data, '__array__'):\n                    data = np.asarray(data)\n                else:\n                    data = list(data)\n            if len(data) > 0:\n                if is_dataclass(data[0]):\n                    data = dataclasses_to_dicts(data)\n                if not isinstance(data, np.ndarray) and treat_as_nested(data):\n                    if columns is not None:\n                        columns = ensure_index(columns)\n                    (arrays, columns, index) = nested_data_to_arrays(data, columns, index, dtype)\n                    mgr = arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=manager)\n                else:\n                    mgr = ndarray_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n            else:\n                mgr = dict_to_mgr({}, index, columns if columns is not None else default_index(0), dtype=dtype, typ=manager)\n        else:\n            if index is None or columns is None:\n                raise ValueError('DataFrame constructor not properly called!')\n            index = ensure_index(index)\n            columns = ensure_index(columns)\n            if not dtype:\n                (dtype, _) = infer_dtype_from_scalar(data)\n            if isinstance(dtype, ExtensionDtype):\n                values = [construct_1d_arraylike_from_scalar(data, len(index), dtype) for _ in range(len(columns))]\n                mgr = arrays_to_mgr(values, columns, index, dtype=None, typ=manager)\n            else:\n                arr2d = construct_2d_arraylike_from_scalar(data, len(index), len(columns), dtype, copy)\n                mgr = ndarray_to_mgr(arr2d, index, columns, dtype=arr2d.dtype, copy=False, typ=manager)\n        mgr = mgr_to_mgr(mgr, typ=manager)\n        NDFrame.__init__(self, mgr)\n        if original_dtype is None and is_pandas_object and (data_dtype == np.object_):\n            if self.dtypes.iloc[0] != data_dtype:\n                warnings.warn('Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The DataFrame constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.', FutureWarning, stacklevel=2)\n\n    def __matmul__(self, other: AnyArrayLike | DataFrame) -> DataFrame | Series:\n        \"\"\"\n        Matrix multiplication using binary `@` operator.\n        \"\"\"\n        return self.dot(other)\n\n    @doc(_shared_docs['aggregate'], klass=_shared_doc_kwargs['klass'], axis=_shared_doc_kwargs['axis'], see_also=_agg_see_also_doc, examples=_agg_examples_doc)\n    def aggregate(self, func=None, axis: Axis=0, *args, **kwargs):\n        from pandas.core.apply import frame_apply\n        axis = self._get_axis_number(axis)\n        op = frame_apply(self, func=func, axis=axis, args=args, kwargs=kwargs)\n        result = op.agg()\n        result = reconstruct_and_relabel_result(result, func, **kwargs)\n        return result\n    _logical_method = _arith_method\n\n    def _sanitize_column(self, value) -> tuple[ArrayLike, BlockValuesRefs | None]:\n        \"\"\"\n        Ensures new columns (which go into the BlockManager as new blocks) are\n        always copied (or a reference is being tracked to them under CoW)\n        and converted into an array.\n\n        Parameters\n        ----------\n        value : scalar, Series, or array-like\n\n        Returns\n        -------\n        tuple of numpy.ndarray or ExtensionArray and optional BlockValuesRefs\n        \"\"\"\n        self._ensure_valid_index(value)\n        assert not isinstance(value, DataFrame)\n        if is_dict_like(value):\n            if not isinstance(value, Series):\n                value = Series(value)\n            return _reindex_for_setitem(value, self.index)\n        if is_list_like(value):\n            com.require_length_match(value, self.index)\n        arr = sanitize_array(value, self.index, copy=True, allow_2d=True)\n        if isinstance(value, Index) and value.dtype == 'object' and (arr.dtype != value.dtype):\n            warnings.warn('Setting an Index with object dtype into a DataFrame will stop inferring another dtype in a future version. Cast the Index explicitly before setting it into the DataFrame.', FutureWarning, stacklevel=find_stack_level())\n        return (arr, None)\n\n    def _align_for_op(self, other, axis: AxisInt, flex: bool | None=False, level: Level | None=None):\n        \"\"\"\n        Convert rhs to meet lhs dims if input is list, tuple or np.ndarray.\n\n        Parameters\n        ----------\n        left : DataFrame\n        right : Any\n        axis : int\n        flex : bool or None, default False\n            Whether this is a flex op, in which case we reindex.\n            None indicates not to check for alignment.\n        level : int or level name, default None\n\n        Returns\n        -------\n        left : DataFrame\n        right : Any\n        \"\"\"\n        (left, right) = (self, other)\n\n        def to_series(right):\n            msg = 'Unable to coerce to Series, length must be {req_len}: given {given_len}'\n            dtype = None\n            if getattr(right, 'dtype', None) == object:\n                dtype = object\n            if axis == 0:\n                if len(left.index) != len(right):\n                    raise ValueError(msg.format(req_len=len(left.index), given_len=len(right)))\n                right = left._constructor_sliced(right, index=left.index, dtype=dtype)\n            else:\n                if len(left.columns) != len(right):\n                    raise ValueError(msg.format(req_len=len(left.columns), given_len=len(right)))\n                right = left._constructor_sliced(right, index=left.columns, dtype=dtype)\n            return right\n        if isinstance(right, np.ndarray):\n            if right.ndim == 1:\n                right = to_series(right)\n            elif right.ndim == 2:\n                dtype = None\n                if right.dtype == object:\n                    dtype = object\n                if right.shape == left.shape:\n                    right = left._constructor(right, index=left.index, columns=left.columns, dtype=dtype)\n                elif right.shape[0] == left.shape[0] and right.shape[1] == 1:\n                    right = np.broadcast_to(right, left.shape)\n                    right = left._constructor(right, index=left.index, columns=left.columns, dtype=dtype)\n                elif right.shape[1] == left.shape[1] and right.shape[0] == 1:\n                    right = to_series(right[0, :])\n                else:\n                    raise ValueError(f'Unable to coerce to DataFrame, shape must be {left.shape}: given {right.shape}')\n            elif right.ndim > 2:\n                raise ValueError(f'Unable to coerce to Series/DataFrame, dimension must be <= 2: {right.shape}')\n        elif is_list_like(right) and (not isinstance(right, (Series, DataFrame))):\n            if any((is_array_like(el) for el in right)):\n                raise ValueError(f'Unable to coerce list of {type(right[0])} to Series/DataFrame')\n            right = to_series(right)\n        if flex is not None and isinstance(right, DataFrame):\n            if not left._indexed_same(right):\n                if flex:\n                    (left, right) = left.align(right, join='outer', level=level, copy=False)\n                else:\n                    raise ValueError('Can only compare identically-labeled (both index and columns) DataFrame objects')\n        elif isinstance(right, Series):\n            axis = axis if axis is not None else 1\n            if not flex:\n                if not left.axes[axis].equals(right.index):\n                    raise ValueError('Operands are not aligned. Do `left, right = left.align(right, axis=1, copy=False)` before operating.')\n            (left, right) = left.align(right, join='outer', axis=axis, level=level, copy=False)\n            right = left._maybe_align_series_as_frame(right, axis)\n        return (left, right)\n\n    def duplicated(self, subset: Hashable | Sequence[Hashable] | None=None, keep: DropKeep='first') -> Series:\n        \"\"\"\n        Return boolean Series denoting duplicate rows.\n\n        Considering certain columns is optional.\n\n        Parameters\n        ----------\n        subset : column label or sequence of labels, optional\n            Only consider certain columns for identifying duplicates, by\n            default use all of the columns.\n        keep : {'first', 'last', False}, default 'first'\n            Determines which duplicates (if any) to mark.\n\n            - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n            - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n            - False : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series\n            Boolean series for each duplicated rows.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on index.\n        Series.duplicated : Equivalent method on Series.\n        Series.drop_duplicates : Remove duplicate values from Series.\n        DataFrame.drop_duplicates : Remove duplicate values from DataFrame.\n\n        Examples\n        --------\n        Consider dataset containing ramen rating.\n\n        >>> df = pd.DataFrame({\n        ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n        ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n        ...     'rating': [4, 4, 3.5, 15, 5]\n        ... })\n        >>> df\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        By default, for each set of duplicated values, the first occurrence\n        is set on False and all others on True.\n\n        >>> df.duplicated()\n        0    False\n        1     True\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True.\n\n        >>> df.duplicated(keep='last')\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        By setting ``keep`` on False, all duplicates are True.\n\n        >>> df.duplicated(keep=False)\n        0     True\n        1     True\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        To find duplicates on specific column(s), use ``subset``.\n\n        >>> df.duplicated(subset=['brand'])\n        0    False\n        1     True\n        2    False\n        3     True\n        4     True\n        dtype: bool\n        \"\"\"\n        if self.empty:\n            return self._constructor_sliced(dtype=bool)\n\n        def f(vals) -> tuple[np.ndarray, int]:\n            (labels, shape) = algorithms.factorize(vals, size_hint=len(self))\n            return (labels.astype('i8', copy=False), len(shape))\n        if subset is None:\n            subset = self.columns\n        elif not np.iterable(subset) or isinstance(subset, str) or (isinstance(subset, tuple) and subset in self.columns):\n            subset = (subset,)\n        subset = cast(Sequence, subset)\n        diff = set(subset) - set(self.columns)\n        if diff:\n            raise KeyError(Index(diff))\n        if len(subset) == 1 and self.columns.is_unique:\n            result = self[subset[0]].duplicated(keep)\n            result.name = None\n        else:\n            vals = (col.values for (name, col) in self.items() if name in subset)\n            (labels, shape) = map(list, zip(*map(f, vals)))\n            ids = get_group_index(labels, tuple(shape), sort=False, xnull=False)\n            result = self._constructor_sliced(duplicated(ids, keep), index=self.index)\n        return result.__finalize__(self, method='duplicated')\n\n    @Appender(ops.make_flex_doc('pow', 'dataframe'))\n    def pow(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, operator.pow, level=level, fill_value=fill_value, axis=axis)\n\n    def map(self, func: PythonFuncType, na_action: str | None=None, **kwargs) -> DataFrame:\n        \"\"\"\n        Apply a function to a Dataframe elementwise.\n\n        .. versionadded:: 2.1.0\n\n           DataFrame.applymap was deprecated and renamed to DataFrame.map.\n\n        This method applies a function that accepts and returns a scalar\n        to every element of a DataFrame.\n\n        Parameters\n        ----------\n        func : callable\n            Python function, returns a single value from a single value.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to func.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        DataFrame\n            Transformed DataFrame.\n\n        See Also\n        --------\n        DataFrame.apply : Apply a function along input axis of DataFrame.\n        DataFrame.replace: Replace values given in `to_replace` with `value`.\n        Series.map : Apply a function elementwise on a Series.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n        >>> df\n               0      1\n        0  1.000  2.120\n        1  3.356  4.567\n\n        >>> df.map(lambda x: len(str(x)))\n           0  1\n        0  3  4\n        1  5  5\n\n        Like Series.map, NA values can be ignored:\n\n        >>> df_copy = df.copy()\n        >>> df_copy.iloc[0, 0] = pd.NA\n        >>> df_copy.map(lambda x: len(str(x)), na_action='ignore')\n             0  1\n        0  NaN  4\n        1  5.0  5\n\n        It is also possible to use `map` with functions that are not\n        `lambda` functions:\n\n        >>> df.map(round, ndigits=1)\n             0    1\n        0  1.0  2.1\n        1  3.4  4.6\n\n        Note that a vectorized version of `func` often exists, which will\n        be much faster. You could square each number elementwise.\n\n        >>> df.map(lambda x: x**2)\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n\n        But it's better to avoid map in that case.\n\n        >>> df ** 2\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n        \"\"\"\n        if na_action not in {'ignore', None}:\n            raise ValueError(f\"na_action must be 'ignore' or None. Got {repr(na_action)}\")\n        if self.empty:\n            return self.copy()\n        func = functools.partial(func, **kwargs)\n\n        def infer(x):\n            return x._map_values(func, na_action=na_action)\n        return self.apply(infer).__finalize__(self, 'map')\n\n    def drop_duplicates(self, subset: Hashable | Sequence[Hashable] | None=None, *, keep: DropKeep='first', inplace: bool=False, ignore_index: bool=False) -> DataFrame | None:\n        \"\"\"\n        Return DataFrame with duplicate rows removed.\n\n        Considering certain columns is optional. Indexes, including time indexes\n        are ignored.\n\n        Parameters\n        ----------\n        subset : column label or sequence of labels, optional\n            Only consider certain columns for identifying duplicates, by\n            default use all of the columns.\n        keep : {'first', 'last', ``False``}, default 'first'\n            Determines which duplicates (if any) to keep.\n\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        inplace : bool, default ``False``\n            Whether to modify the DataFrame rather than creating a new one.\n        ignore_index : bool, default ``False``\n            If ``True``, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with duplicates removed or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.value_counts: Count unique combinations of columns.\n\n        Examples\n        --------\n        Consider dataset containing ramen rating.\n\n        >>> df = pd.DataFrame({\n        ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n        ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n        ...     'rating': [4, 4, 3.5, 15, 5]\n        ... })\n        >>> df\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        By default, it removes duplicate rows based on all columns.\n\n        >>> df.drop_duplicates()\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        To remove duplicates on specific column(s), use ``subset``.\n\n        >>> df.drop_duplicates(subset=['brand'])\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n\n        To remove duplicates and keep last occurrences, use ``keep``.\n\n        >>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n            brand style  rating\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        4  Indomie  pack     5.0\n        \"\"\"\n        if self.empty:\n            return self.copy(deep=None)\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        ignore_index = validate_bool_kwarg(ignore_index, 'ignore_index')\n        result = self[-self.duplicated(subset, keep=keep)]\n        if ignore_index:\n            result.index = default_index(len(result))\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def applymap(self, func: PythonFuncType, na_action: NaAction | None=None, **kwargs) -> DataFrame:\n        \"\"\"\n        Apply a function to a Dataframe elementwise.\n\n        .. deprecated:: 2.1.0\n\n           DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n        This method applies a function that accepts and returns a scalar\n        to every element of a DataFrame.\n\n        Parameters\n        ----------\n        func : callable\n            Python function, returns a single value from a single value.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to func.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        DataFrame\n            Transformed DataFrame.\n\n        See Also\n        --------\n        DataFrame.apply : Apply a function along input axis of DataFrame.\n        DataFrame.map : Apply a function along input axis of DataFrame.\n        DataFrame.replace: Replace values given in `to_replace` with `value`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n        >>> df\n               0      1\n        0  1.000  2.120\n        1  3.356  4.567\n\n        >>> df.map(lambda x: len(str(x)))\n           0  1\n        0  3  4\n        1  5  5\n        \"\"\"\n        warnings.warn('DataFrame.applymap has been deprecated. Use DataFrame.map instead.', FutureWarning, stacklevel=find_stack_level())\n        return self.map(func, na_action=na_action, **kwargs)\n\n    def memory_usage(self, index: bool=True, deep: bool=False) -> Series:\n        \"\"\"\n        Return the memory usage of each column in bytes.\n\n        The memory usage can optionally include the contribution of\n        the index and elements of `object` dtype.\n\n        This value is displayed in `DataFrame.info` by default. This can be\n        suppressed by setting ``pandas.options.display.memory_usage`` to False.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the DataFrame's\n            index in returned Series. If ``index=True``, the memory usage of\n            the index is the first item in the output.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned values.\n\n        Returns\n        -------\n        Series\n            A Series whose index is the original column names and whose values\n            is the memory usage of each column in bytes.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of an\n            ndarray.\n        Series.memory_usage : Bytes consumed by a Series.\n        Categorical : Memory-efficient array for string values with\n            many repeated values.\n        DataFrame.info : Concise summary of a DataFrame.\n\n        Notes\n        -----\n        See the :ref:`Frequently Asked Questions <df-memory-usage>` for more\n        details.\n\n        Examples\n        --------\n        >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']\n        >>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t))\n        ...              for t in dtypes])\n        >>> df = pd.DataFrame(data)\n        >>> df.head()\n           int64  float64            complex128  object  bool\n        0      1      1.0              1.0+0.0j       1  True\n        1      1      1.0              1.0+0.0j       1  True\n        2      1      1.0              1.0+0.0j       1  True\n        3      1      1.0              1.0+0.0j       1  True\n        4      1      1.0              1.0+0.0j       1  True\n\n        >>> df.memory_usage()\n        Index           128\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        >>> df.memory_usage(index=False)\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        The memory footprint of `object` dtype columns is ignored by default:\n\n        >>> df.memory_usage(deep=True)\n        Index            128\n        int64          40000\n        float64        40000\n        complex128     80000\n        object        180000\n        bool            5000\n        dtype: int64\n\n        Use a Categorical for efficient storage of an object-dtype column with\n        many repeated values.\n\n        >>> df['object'].astype('category').memory_usage(deep=True)\n        5244\n        \"\"\"\n        result = self._constructor_sliced([c.memory_usage(index=False, deep=deep) for (col, c) in self.items()], index=self.columns, dtype=np.intp)\n        if index:\n            index_memory_usage = self._constructor_sliced(self.index.memory_usage(deep=deep), index=['Index'])\n            result = index_memory_usage._append(result)\n        return result\n\n    def to_feather(self, path: FilePath | WriteBuffer[bytes], **kwargs) -> None:\n        \"\"\"\n        Write a DataFrame to the binary Feather format.\n\n        Parameters\n        ----------\n        path : str, path object, file-like object\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a binary ``write()`` function. If a string or a path,\n            it will be used as Root Directory path when writing a partitioned dataset.\n        **kwargs :\n            Additional keywords passed to :func:`pyarrow.feather.write_feather`.\n            This includes the `compression`, `compression_level`, `chunksize`\n            and `version` keywords.\n\n        Notes\n        -----\n        This function writes the dataframe as a `feather file\n        <https://arrow.apache.org/docs/python/feather.html>`_. Requires a default\n        index. For saving the DataFrame with your custom index use a method that\n        supports custom indices e.g. `to_parquet`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]])\n        >>> df.to_feather(\"file.feather\")  # doctest: +SKIP\n        \"\"\"\n        from pandas.io.feather_format import to_feather\n        to_feather(self, path, **kwargs)\n\n    @Appender(ops.make_flex_doc('lt', 'dataframe'))\n    def lt(self, other, axis: Axis='columns', level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.lt, axis=axis, level=level)\n\n    def _repr_html_(self) -> str | None:\n        \"\"\"\n        Return a html representation for a particular DataFrame.\n\n        Mainly for IPython notebook.\n        \"\"\"\n        if self._info_repr():\n            buf = StringIO()\n            self.info(buf=buf)\n            val = buf.getvalue().replace('<', '&lt;', 1)\n            val = val.replace('>', '&gt;', 1)\n            return f'<pre>{val}</pre>'\n        if get_option('display.notebook_repr_html'):\n            max_rows = get_option('display.max_rows')\n            min_rows = get_option('display.min_rows')\n            max_cols = get_option('display.max_columns')\n            show_dimensions = get_option('display.show_dimensions')\n            formatter = fmt.DataFrameFormatter(self, columns=None, col_space=None, na_rep='NaN', formatters=None, float_format=None, sparsify=None, justify=None, index_names=True, header=True, index=True, bold_rows=True, escape=True, max_rows=max_rows, min_rows=min_rows, max_cols=max_cols, show_dimensions=show_dimensions, decimal='.')\n            return fmt.DataFrameRenderer(formatter).to_html(notebook=True)\n        else:\n            return None\n\n    def to_timestamp(self, freq: Frequency | None=None, how: ToTimestampHow='start', axis: Axis=0, copy: bool | None=None) -> DataFrame:\n        \"\"\"\n        Cast to DatetimeIndex of timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to convert (the index by default).\n        copy : bool, default True\n            If False then underlying input data is not copied.\n\n            .. note::\n                The `copy` keyword will change behavior in pandas 3.0.\n                `Copy-on-Write\n                <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n                will be enabled by default, which means that all methods with a\n                `copy` keyword will use a lazy copy mechanism to defer the copy and\n                ignore the `copy` keyword. The `copy` keyword will be removed in a\n                future version of pandas.\n\n                You can already get the future behavior and improvements through\n                enabling copy on write ``pd.options.mode.copy_on_write = True``\n\n        Returns\n        -------\n        DataFrame\n            The DataFrame has a DatetimeIndex.\n\n        Examples\n        --------\n        >>> idx = pd.PeriodIndex(['2023', '2024'], freq='Y')\n        >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n        >>> df1 = pd.DataFrame(data=d, index=idx)\n        >>> df1\n              col1   col2\n        2023     1      3\n        2024\t 2      4\n\n        The resulting timestamps will be at the beginning of the year in this case\n\n        >>> df1 = df1.to_timestamp()\n        >>> df1\n                    col1   col2\n        2023-01-01     1      3\n        2024-01-01     2      4\n        >>> df1.index\n        DatetimeIndex(['2023-01-01', '2024-01-01'], dtype='datetime64[ns]', freq=None)\n\n        Using `freq` which is the offset that the Timestamps will have\n\n        >>> df2 = pd.DataFrame(data=d, index=idx)\n        >>> df2 = df2.to_timestamp(freq='M')\n        >>> df2\n                    col1   col2\n        2023-01-31     1      3\n        2024-01-31     2      4\n        >>> df2.index\n        DatetimeIndex(['2023-01-31', '2024-01-31'], dtype='datetime64[ns]', freq=None)\n        \"\"\"\n        new_obj = self.copy(deep=copy and (not using_copy_on_write()))\n        axis_name = self._get_axis_name(axis)\n        old_ax = getattr(self, axis_name)\n        if not isinstance(old_ax, PeriodIndex):\n            raise TypeError(f'unsupported Type {type(old_ax).__name__}')\n        new_ax = old_ax.to_timestamp(freq=freq, how=how)\n        setattr(new_obj, axis_name, new_ax)\n        return new_obj\n\n    def _arith_method(self, other, op):\n        if self._should_reindex_frame_op(other, op, 1, None, None):\n            return self._arith_method_with_reindex(other, op)\n        axis: Literal[1] = 1\n        other = ops.maybe_prepare_scalar_for_op(other, (self.shape[axis],))\n        (self, other) = self._align_for_op(other, axis, flex=True, level=None)\n        with np.errstate(all='ignore'):\n            new_data = self._dispatch_frame_op(other, op, axis=axis)\n        return self._construct_result(new_data)\n\n    @doc(_shared_docs['compare'], dedent('\\n        Returns\\n        -------\\n        DataFrame\\n            DataFrame that shows the differences stacked side by side.\\n\\n            The resulting index will be a MultiIndex with \\'self\\' and \\'other\\'\\n            stacked alternately at the inner level.\\n\\n        Raises\\n        ------\\n        ValueError\\n            When the two DataFrames don\\'t have identical labels or shape.\\n\\n        See Also\\n        --------\\n        Series.compare : Compare with another Series and show differences.\\n        DataFrame.equals : Test whether two objects contain the same elements.\\n\\n        Notes\\n        -----\\n        Matching NaNs will not appear as a difference.\\n\\n        Can only compare identically-labeled\\n        (i.e. same shape, identical row and column labels) DataFrames\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame(\\n        ...     {{\\n        ...         \"col1\": [\"a\", \"a\", \"b\", \"b\", \"a\"],\\n        ...         \"col2\": [1.0, 2.0, 3.0, np.nan, 5.0],\\n        ...         \"col3\": [1.0, 2.0, 3.0, 4.0, 5.0]\\n        ...     }},\\n        ...     columns=[\"col1\", \"col2\", \"col3\"],\\n        ... )\\n        >>> df\\n          col1  col2  col3\\n        0    a   1.0   1.0\\n        1    a   2.0   2.0\\n        2    b   3.0   3.0\\n        3    b   NaN   4.0\\n        4    a   5.0   5.0\\n\\n        >>> df2 = df.copy()\\n        >>> df2.loc[0, \\'col1\\'] = \\'c\\'\\n        >>> df2.loc[2, \\'col3\\'] = 4.0\\n        >>> df2\\n          col1  col2  col3\\n        0    c   1.0   1.0\\n        1    a   2.0   2.0\\n        2    b   3.0   4.0\\n        3    b   NaN   4.0\\n        4    a   5.0   5.0\\n\\n        Align the differences on columns\\n\\n        >>> df.compare(df2)\\n          col1       col3\\n          self other self other\\n        0    a     c  NaN   NaN\\n        2  NaN   NaN  3.0   4.0\\n\\n        Assign result_names\\n\\n        >>> df.compare(df2, result_names=(\"left\", \"right\"))\\n          col1       col3\\n          left right left right\\n        0    a     c  NaN   NaN\\n        2  NaN   NaN  3.0   4.0\\n\\n        Stack the differences on rows\\n\\n        >>> df.compare(df2, align_axis=0)\\n                col1  col3\\n        0 self     a   NaN\\n          other    c   NaN\\n        2 self   NaN   3.0\\n          other  NaN   4.0\\n\\n        Keep the equal values\\n\\n        >>> df.compare(df2, keep_equal=True)\\n          col1       col3\\n          self other self other\\n        0    a     c  1.0   1.0\\n        2    b     b  3.0   4.0\\n\\n        Keep all original rows and columns\\n\\n        >>> df.compare(df2, keep_shape=True)\\n          col1       col2       col3\\n          self other self other self other\\n        0    a     c  NaN   NaN  NaN   NaN\\n        1  NaN   NaN  NaN   NaN  NaN   NaN\\n        2  NaN   NaN  NaN   NaN  3.0   4.0\\n        3  NaN   NaN  NaN   NaN  NaN   NaN\\n        4  NaN   NaN  NaN   NaN  NaN   NaN\\n\\n        Keep all original rows and columns and also all original values\\n\\n        >>> df.compare(df2, keep_shape=True, keep_equal=True)\\n          col1       col2       col3\\n          self other self other self other\\n        0    a     c  1.0   1.0  1.0   1.0\\n        1    a     a  2.0   2.0  2.0   2.0\\n        2    b     b  3.0   3.0  3.0   4.0\\n        3    b     b  NaN   NaN  4.0   4.0\\n        4    a     a  5.0   5.0  5.0   5.0\\n        '), klass=_shared_doc_kwargs['klass'])\n    def compare(self, other: DataFrame, align_axis: Axis=1, keep_shape: bool=False, keep_equal: bool=False, result_names: Suffixes=('self', 'other')) -> DataFrame:\n        return super().compare(other=other, align_axis=align_axis, keep_shape=keep_shape, keep_equal=keep_equal, result_names=result_names)\n\n    @final\n    @doc(Rolling)\n    def rolling(self, window: int | dt.timedelta | str | BaseOffset | BaseIndexer, min_periods: int | None=None, center: bool_t=False, win_type: str | None=None, on: str | None=None, axis: Axis | lib.NoDefault=lib.no_default, closed: IntervalClosedType | None=None, step: int | None=None, method: str='single') -> Window | Rolling:\n        if axis is not lib.no_default:\n            axis = self._get_axis_number(axis)\n            name = 'rolling'\n            if axis == 1:\n                warnings.warn(f'Support for axis=1 in {type(self).__name__}.{name} is deprecated and will be removed in a future version. Use obj.T.{name}(...) instead', FutureWarning, stacklevel=find_stack_level())\n            else:\n                warnings.warn(f\"The 'axis' keyword in {type(self).__name__}.{name} is deprecated and will be removed in a future version. Call the method without the axis keyword instead.\", FutureWarning, stacklevel=find_stack_level())\n        else:\n            axis = 0\n        if win_type is not None:\n            return Window(self, window=window, min_periods=min_periods, center=center, win_type=win_type, on=on, axis=axis, closed=closed, step=step, method=method)\n        return Rolling(self, window=window, min_periods=min_periods, center=center, win_type=win_type, on=on, axis=axis, closed=closed, step=step, method=method)\n\n    def _setitem_slice(self, key: slice, value) -> None:\n        self._check_setitem_copy()\n        self.iloc[key] = value\n\n    def _append(self, other, ignore_index: bool=False, verify_integrity: bool=False, sort: bool=False) -> DataFrame:\n        if isinstance(other, (Series, dict)):\n            if isinstance(other, dict):\n                if not ignore_index:\n                    raise TypeError('Can only append a dict if ignore_index=True')\n                other = Series(other)\n            if other.name is None and (not ignore_index):\n                raise TypeError('Can only append a Series if ignore_index=True or if the Series has a name')\n            index = Index([other.name], name=self.index.names if isinstance(self.index, MultiIndex) else self.index.name)\n            row_df = other.to_frame().T\n            other = row_df.infer_objects(copy=False).rename_axis(index.names, copy=False)\n        elif isinstance(other, list):\n            if not other:\n                pass\n            elif not isinstance(other[0], DataFrame):\n                other = DataFrame(other)\n                if self.index.name is not None and (not ignore_index):\n                    other.index.name = self.index.name\n        from pandas.core.reshape.concat import concat\n        if isinstance(other, (list, tuple)):\n            to_concat = [self, *other]\n        else:\n            to_concat = [self, other]\n        result = concat(to_concat, ignore_index=ignore_index, verify_integrity=verify_integrity, sort=sort)\n        return result.__finalize__(self, method='append')\n\n    def _iset_item_mgr(self, loc: int | slice | np.ndarray, value, inplace: bool=False, refs: BlockValuesRefs | None=None) -> None:\n        self._mgr.iset(loc, value, inplace=inplace, refs=refs)\n        self._clear_item_cache()\n\n    def _set_value(self, index: IndexLabel, col, value: Scalar, takeable: bool=False) -> None:\n        \"\"\"\n        Put single value at passed column and index.\n\n        Parameters\n        ----------\n        index : Label\n            row label\n        col : Label\n            column label\n        value : scalar\n        takeable : bool, default False\n            Sets whether or not index/col interpreted as indexers\n        \"\"\"\n        try:\n            if takeable:\n                icol = col\n                iindex = cast(int, index)\n            else:\n                icol = self.columns.get_loc(col)\n                iindex = self.index.get_loc(index)\n            self._mgr.column_setitem(icol, iindex, value, inplace_only=True)\n            self._clear_item_cache()\n        except (KeyError, TypeError, ValueError, LossySetitemError):\n            if takeable:\n                self.iloc[index, col] = value\n            else:\n                self.loc[index, col] = value\n            self._item_cache.pop(col, None)\n        except InvalidIndexError as ii_err:\n            raise InvalidIndexError(f'You can only assign a scalar value not a {type(value)}') from ii_err\n\n    def _getitem_bool_array(self, key):\n        if isinstance(key, Series) and (not key.index.equals(self.index)):\n            warnings.warn('Boolean Series key will be reindexed to match DataFrame index.', UserWarning, stacklevel=find_stack_level())\n        elif len(key) != len(self.index):\n            raise ValueError(f'Item wrong length {len(key)} instead of {len(self.index)}.')\n        key = check_bool_indexer(self.index, key)\n        if key.all():\n            return self.copy(deep=None)\n        indexer = key.nonzero()[0]\n        return self._take_with_is_copy(indexer, axis=0)\n    subtract = sub\n\n    def _info_repr(self) -> bool:\n        \"\"\"\n        True if the repr should show the info view.\n        \"\"\"\n        info_repr_option = get_option('display.large_repr') == 'info'\n        return info_repr_option and (not (self._repr_fits_horizontal_() and self._repr_fits_vertical_()))\n\n    def _get_column_array(self, i: int) -> ArrayLike:\n        \"\"\"\n        Get the values of the i'th column (ndarray or ExtensionArray, as stored\n        in the Block)\n\n        Warning! The returned array is a view but doesn't handle Copy-on-Write,\n        so this should be used with caution (for read-only purposes).\n        \"\"\"\n        return self._mgr.iget_values(i)\n    multiply = mul\n\n    @doc(make_doc('min', ndim=2))\n    def min(self, axis: Axis | None=0, skipna: bool=True, numeric_only: bool=False, **kwargs):\n        result = super().min(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method='min')\n        return result\n\n    @classmethod\n    def from_records(cls, data, index=None, exclude=None, columns=None, coerce_float: bool=False, nrows: int | None=None) -> DataFrame:\n        \"\"\"\n        Convert structured or record ndarray to DataFrame.\n\n        Creates a DataFrame object from a structured ndarray, sequence of\n        tuples or dicts, or DataFrame.\n\n        Parameters\n        ----------\n        data : structured ndarray, sequence of tuples or dicts, or DataFrame\n            Structured input data.\n\n            .. deprecated:: 2.1.0\n                Passing a DataFrame is deprecated.\n        index : str, list of fields, array-like\n            Field of array to use as the index, alternately a specific set of\n            input labels to use.\n        exclude : sequence, default None\n            Columns or fields to exclude.\n        columns : sequence, default None\n            Column names to use. If the passed data do not have names\n            associated with them, this argument provides names for the\n            columns. Otherwise this argument indicates the order of the columns\n            in the result (any names not found in the data will become all-NA\n            columns).\n        coerce_float : bool, default False\n            Attempt to convert values of non-string, non-numeric objects (like\n            decimal.Decimal) to floating point, useful for SQL result sets.\n        nrows : int, default None\n            Number of rows to read if data is an iterator.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.from_dict : DataFrame from dict of array-like or dicts.\n        DataFrame : DataFrame object creation using constructor.\n\n        Examples\n        --------\n        Data can be provided as a structured ndarray:\n\n        >>> data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')],\n        ...                 dtype=[('col_1', 'i4'), ('col_2', 'U1')])\n        >>> pd.DataFrame.from_records(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Data can be provided as a list of dicts:\n\n        >>> data = [{'col_1': 3, 'col_2': 'a'},\n        ...         {'col_1': 2, 'col_2': 'b'},\n        ...         {'col_1': 1, 'col_2': 'c'},\n        ...         {'col_1': 0, 'col_2': 'd'}]\n        >>> pd.DataFrame.from_records(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Data can be provided as a list of tuples with corresponding columns:\n\n        >>> data = [(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')]\n        >>> pd.DataFrame.from_records(data, columns=['col_1', 'col_2'])\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n        \"\"\"\n        if isinstance(data, DataFrame):\n            warnings.warn('Passing a DataFrame to DataFrame.from_records is deprecated. Use set_index and/or drop to modify the DataFrame instead.', FutureWarning, stacklevel=find_stack_level())\n            if columns is not None:\n                if is_scalar(columns):\n                    columns = [columns]\n                data = data[columns]\n            if index is not None:\n                data = data.set_index(index)\n            if exclude is not None:\n                data = data.drop(columns=exclude)\n            return data.copy(deep=False)\n        result_index = None\n        if columns is not None:\n            columns = ensure_index(columns)\n\n        def maybe_reorder(arrays: list[ArrayLike], arr_columns: Index, columns: Index, index) -> tuple[list[ArrayLike], Index, Index | None]:\n            \"\"\"\n            If our desired 'columns' do not match the data's pre-existing 'arr_columns',\n            we re-order our arrays.  This is like a pre-emptive (cheap) reindex.\n            \"\"\"\n            if len(arrays):\n                length = len(arrays[0])\n            else:\n                length = 0\n            result_index = None\n            if len(arrays) == 0 and index is None and (length == 0):\n                result_index = default_index(0)\n            (arrays, arr_columns) = reorder_arrays(arrays, arr_columns, columns, length)\n            return (arrays, arr_columns, result_index)\n        if is_iterator(data):\n            if nrows == 0:\n                return cls()\n            try:\n                first_row = next(data)\n            except StopIteration:\n                return cls(index=index, columns=columns)\n            dtype = None\n            if hasattr(first_row, 'dtype') and first_row.dtype.names:\n                dtype = first_row.dtype\n            values = [first_row]\n            if nrows is None:\n                values += data\n            else:\n                values.extend(itertools.islice(data, nrows - 1))\n            if dtype is not None:\n                data = np.array(values, dtype=dtype)\n            else:\n                data = values\n        if isinstance(data, dict):\n            if columns is None:\n                columns = arr_columns = ensure_index(sorted(data))\n                arrays = [data[k] for k in columns]\n            else:\n                arrays = []\n                arr_columns_list = []\n                for (k, v) in data.items():\n                    if k in columns:\n                        arr_columns_list.append(k)\n                        arrays.append(v)\n                arr_columns = Index(arr_columns_list)\n                (arrays, arr_columns, result_index) = maybe_reorder(arrays, arr_columns, columns, index)\n        elif isinstance(data, np.ndarray):\n            (arrays, columns) = to_arrays(data, columns)\n            arr_columns = columns\n        else:\n            (arrays, arr_columns) = to_arrays(data, columns)\n            if coerce_float:\n                for (i, arr) in enumerate(arrays):\n                    if arr.dtype == object:\n                        arrays[i] = lib.maybe_convert_objects(arr, try_float=True)\n            arr_columns = ensure_index(arr_columns)\n            if columns is None:\n                columns = arr_columns\n            else:\n                (arrays, arr_columns, result_index) = maybe_reorder(arrays, arr_columns, columns, index)\n        if exclude is None:\n            exclude = set()\n        else:\n            exclude = set(exclude)\n        if index is not None:\n            if isinstance(index, str) or not hasattr(index, '__iter__'):\n                i = columns.get_loc(index)\n                exclude.add(index)\n                if len(arrays) > 0:\n                    result_index = Index(arrays[i], name=index)\n                else:\n                    result_index = Index([], name=index)\n            else:\n                try:\n                    index_data = [arrays[arr_columns.get_loc(field)] for field in index]\n                except (KeyError, TypeError):\n                    result_index = index\n                else:\n                    result_index = ensure_index_from_sequences(index_data, names=index)\n                    exclude.update(index)\n        if any(exclude):\n            arr_exclude = [x for x in exclude if x in arr_columns]\n            to_remove = [arr_columns.get_loc(col) for col in arr_exclude]\n            arrays = [v for (i, v) in enumerate(arrays) if i not in to_remove]\n            columns = columns.drop(exclude)\n        manager = _get_option('mode.data_manager', silent=True)\n        mgr = arrays_to_mgr(arrays, columns, result_index, typ=manager)\n        return cls._from_mgr(mgr, axes=mgr.axes)\n    div = truediv\n    divide = truediv\n\n    @doc(make_doc('any', ndim=2))\n    def any(self, *, axis: Axis | None=0, bool_only: bool=False, skipna: bool=True, **kwargs) -> Series | bool:\n        result = self._logical_func('any', nanops.nanany, axis, bool_only, skipna, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method='any')\n        return result\n    rdiv = rtruediv\n\n    def _repr_fits_vertical_(self) -> bool:\n        \"\"\"\n        Check length against max_rows.\n        \"\"\"\n        max_rows = get_option('display.max_rows')\n        return len(self) <= max_rows\n\n    def _ixs(self, i: int, axis: AxisInt=0) -> Series:\n        \"\"\"\n        Parameters\n        ----------\n        i : int\n        axis : int\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        if axis == 0:\n            new_mgr = self._mgr.fast_xs(i)\n            copy = isinstance(new_mgr.array, np.ndarray) and new_mgr.array.base is None\n            result = self._constructor_sliced_from_mgr(new_mgr, axes=new_mgr.axes)\n            result._name = self.index[i]\n            result = result.__finalize__(self)\n            result._set_is_copy(self, copy=copy)\n            return result\n        else:\n            label = self.columns[i]\n            col_mgr = self._mgr.iget(i)\n            result = self._box_col_values(col_mgr, i)\n            result._set_as_cached(label, self)\n            return result\n\n    def assign(self, **kwargs) -> DataFrame:\n        \"\"\"\n        Assign new columns to a DataFrame.\n\n        Returns a new object with all original columns in addition to new ones.\n        Existing columns that are re-assigned will be overwritten.\n\n        Parameters\n        ----------\n        **kwargs : dict of {str: callable or Series}\n            The column names are keywords. If the values are\n            callable, they are computed on the DataFrame and\n            assigned to the new columns. The callable must not\n            change input DataFrame (though pandas doesn't check it).\n            If the values are not callable, (e.g. a Series, scalar, or array),\n            they are simply assigned.\n\n        Returns\n        -------\n        DataFrame\n            A new DataFrame with the new columns in addition to\n            all the existing columns.\n\n        Notes\n        -----\n        Assigning multiple columns within the same ``assign`` is possible.\n        Later items in '\\\\*\\\\*kwargs' may refer to newly created or modified\n        columns in 'df'; items are computed and assigned into 'df' in order.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'temp_c': [17.0, 25.0]},\n        ...                   index=['Portland', 'Berkeley'])\n        >>> df\n                  temp_c\n        Portland    17.0\n        Berkeley    25.0\n\n        Where the value is a callable, evaluated on `df`:\n\n        >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        Alternatively, the same behavior can be achieved by directly\n        referencing an existing Series or sequence:\n\n        >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        You can create multiple columns within the same assign where one\n        of the columns depends on another one defined within the same assign:\n\n        >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,\n        ...           temp_k=lambda x: (x['temp_f'] + 459.67) * 5 / 9)\n                  temp_c  temp_f  temp_k\n        Portland    17.0    62.6  290.15\n        Berkeley    25.0    77.0  298.15\n        \"\"\"\n        data = self.copy(deep=None)\n        for (k, v) in kwargs.items():\n            data[k] = com.apply_if_callable(v, data)\n        return data\n\n    @doc(Series.diff, klass='DataFrame', extra_params=\"axis : {0 or 'index', 1 or 'columns'}, default 0\\n    Take difference over rows (0) or columns (1).\\n\", other_klass='Series', examples=dedent(\"\\n        Difference with previous row\\n\\n        >>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6],\\n        ...                    'b': [1, 1, 2, 3, 5, 8],\\n        ...                    'c': [1, 4, 9, 16, 25, 36]})\\n        >>> df\\n           a  b   c\\n        0  1  1   1\\n        1  2  1   4\\n        2  3  2   9\\n        3  4  3  16\\n        4  5  5  25\\n        5  6  8  36\\n\\n        >>> df.diff()\\n             a    b     c\\n        0  NaN  NaN   NaN\\n        1  1.0  0.0   3.0\\n        2  1.0  1.0   5.0\\n        3  1.0  1.0   7.0\\n        4  1.0  2.0   9.0\\n        5  1.0  3.0  11.0\\n\\n        Difference with previous column\\n\\n        >>> df.diff(axis=1)\\n            a  b   c\\n        0 NaN  0   0\\n        1 NaN -1   3\\n        2 NaN -1   7\\n        3 NaN -1  13\\n        4 NaN  0  20\\n        5 NaN  2  28\\n\\n        Difference with 3rd previous row\\n\\n        >>> df.diff(periods=3)\\n             a    b     c\\n        0  NaN  NaN   NaN\\n        1  NaN  NaN   NaN\\n        2  NaN  NaN   NaN\\n        3  3.0  2.0  15.0\\n        4  3.0  4.0  21.0\\n        5  3.0  6.0  27.0\\n\\n        Difference with following row\\n\\n        >>> df.diff(periods=-1)\\n             a    b     c\\n        0 -1.0  0.0  -3.0\\n        1 -1.0 -1.0  -5.0\\n        2 -1.0 -1.0  -7.0\\n        3 -1.0 -2.0  -9.0\\n        4 -1.0 -3.0 -11.0\\n        5  NaN  NaN   NaN\\n\\n        Overflow in input dtype\\n\\n        >>> df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8)\\n        >>> df.diff()\\n               a\\n        0    NaN\\n        1  255.0\"))\n    def diff(self, periods: int=1, axis: Axis=0) -> DataFrame:\n        if not lib.is_integer(periods):\n            if not (is_float(periods) and periods.is_integer()):\n                raise ValueError('periods must be an integer')\n            periods = int(periods)\n        axis = self._get_axis_number(axis)\n        if axis == 1:\n            if periods != 0:\n                return self - self.shift(periods, axis=axis)\n            axis = 0\n        new_data = self._mgr.diff(n=periods)\n        res_df = self._constructor_from_mgr(new_data, axes=new_data.axes)\n        return res_df.__finalize__(self, 'diff')\n\n    @Appender(_shared_docs['melt'] % {'caller': 'df.melt(', 'other': 'melt'})\n    def melt(self, id_vars=None, value_vars=None, var_name=None, value_name: Hashable='value', col_level: Level | None=None, ignore_index: bool=True) -> DataFrame:\n        return melt(self, id_vars=id_vars, value_vars=value_vars, var_name=var_name, value_name=value_name, col_level=col_level, ignore_index=ignore_index).__finalize__(self, method='melt')\n\n    @Appender(ops.make_flex_doc('gt', 'dataframe'))\n    def gt(self, other, axis: Axis='columns', level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.gt, axis=axis, level=level)\n\n    def _should_reindex_frame_op(self, right, op, axis: int, fill_value, level) -> bool:\n        \"\"\"\n        Check if this is an operation between DataFrames that will need to reindex.\n        \"\"\"\n        if op is operator.pow or op is roperator.rpow:\n            return False\n        if not isinstance(right, DataFrame):\n            return False\n        if fill_value is None and level is None and (axis == 1):\n            left_uniques = self.columns.unique()\n            right_uniques = right.columns.unique()\n            cols = left_uniques.intersection(right_uniques)\n            if len(cols) and (not (len(cols) == len(left_uniques) and len(cols) == len(right_uniques))):\n                return True\n        return False\n\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs['klass'])\n    def isna(self) -> DataFrame:\n        res_mgr = self._mgr.isna(func=isna)\n        result = self._constructor_from_mgr(res_mgr, axes=res_mgr.axes)\n        return result.__finalize__(self, method='isna')\n\n    def _get_item_cache(self, item: Hashable) -> Series:\n        \"\"\"Return the cached item, item represents a label indexer.\"\"\"\n        if using_copy_on_write() or warn_copy_on_write():\n            loc = self.columns.get_loc(item)\n            return self._ixs(loc, axis=1)\n        cache = self._item_cache\n        res = cache.get(item)\n        if res is None:\n            loc = self.columns.get_loc(item)\n            res = self._ixs(loc, axis=1)\n            cache[item] = res\n            res._is_copy = self._is_copy\n        return res\n\n    @classmethod\n    def _from_arrays(cls, arrays, columns, index, dtype: Dtype | None=None, verify_integrity: bool=True) -> Self:\n        \"\"\"\n        Create DataFrame from a list of arrays corresponding to the columns.\n\n        Parameters\n        ----------\n        arrays : list-like of arrays\n            Each array in the list corresponds to one column, in order.\n        columns : list-like, Index\n            The column names for the resulting DataFrame.\n        index : list-like, Index\n            The rows labels for the resulting DataFrame.\n        dtype : dtype, optional\n            Optional dtype to enforce for all arrays.\n        verify_integrity : bool, default True\n            Validate and homogenize all input. If set to False, it is assumed\n            that all elements of `arrays` are actual arrays how they will be\n            stored in a block (numpy ndarray or ExtensionArray), have the same\n            length as and are aligned with the index, and that `columns` and\n            `index` are ensured to be an Index object.\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n        manager = _get_option('mode.data_manager', silent=True)\n        columns = ensure_index(columns)\n        if len(columns) != len(arrays):\n            raise ValueError('len(columns) must match len(arrays)')\n        mgr = arrays_to_mgr(arrays, columns, index, dtype=dtype, verify_integrity=verify_integrity, typ=manager)\n        return cls._from_mgr(mgr, axes=mgr.axes)\n\n    def rename(self, mapper: Renamer | None=None, *, index: Renamer | None=None, columns: Renamer | None=None, axis: Axis | None=None, copy: bool | None=None, inplace: bool=False, level: Level | None=None, errors: IgnoreRaise='ignore') -> DataFrame | None:\n        \"\"\"\n        Rename columns or index labels.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        mapper : dict-like or function\n            Dict-like or function transformations to apply to\n            that axis' values. Use either ``mapper`` and ``axis`` to\n            specify the axis to target with ``mapper``, or ``index`` and\n            ``columns``.\n        index : dict-like or function\n            Alternative to specifying axis (``mapper, axis=0``\n            is equivalent to ``index=mapper``).\n        columns : dict-like or function\n            Alternative to specifying axis (``mapper, axis=1``\n            is equivalent to ``columns=mapper``).\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis to target with ``mapper``. Can be either the axis name\n            ('index', 'columns') or number (0, 1). The default is 'index'.\n        copy : bool, default True\n            Also copy underlying data.\n\n            .. note::\n                The `copy` keyword will change behavior in pandas 3.0.\n                `Copy-on-Write\n                <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n                will be enabled by default, which means that all methods with a\n                `copy` keyword will use a lazy copy mechanism to defer the copy and\n                ignore the `copy` keyword. The `copy` keyword will be removed in a\n                future version of pandas.\n\n                You can already get the future behavior and improvements through\n                enabling copy on write ``pd.options.mode.copy_on_write = True``\n        inplace : bool, default False\n            Whether to modify the DataFrame rather than creating a new one.\n            If True then value of copy is ignored.\n        level : int or level name, default None\n            In case of a MultiIndex, only rename labels in the specified\n            level.\n        errors : {'ignore', 'raise'}, default 'ignore'\n            If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`,\n            or `columns` contains labels that are not present in the Index\n            being transformed.\n            If 'ignore', existing keys will be renamed and extra keys will be\n            ignored.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with the renamed axis labels or None if ``inplace=True``.\n\n        Raises\n        ------\n        KeyError\n            If any of the labels is not found in the selected axis and\n            \"errors='raise'\".\n\n        See Also\n        --------\n        DataFrame.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        ``DataFrame.rename`` supports two calling conventions\n\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\n        * ``(mapper, axis={'index', 'columns'}, ...)``\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        Rename columns using a mapping:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n        >>> df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n           a  c\n        0  1  4\n        1  2  5\n        2  3  6\n\n        Rename index using a mapping:\n\n        >>> df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"})\n           A  B\n        x  1  4\n        y  2  5\n        z  3  6\n\n        Cast index labels to a different type:\n\n        >>> df.index\n        RangeIndex(start=0, stop=3, step=1)\n        >>> df.rename(index=str).index\n        Index(['0', '1', '2'], dtype='object')\n\n        >>> df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\")\n        Traceback (most recent call last):\n        KeyError: ['C'] not found in axis\n\n        Using axis-style parameters:\n\n        >>> df.rename(str.lower, axis='columns')\n           a  b\n        0  1  4\n        1  2  5\n        2  3  6\n\n        >>> df.rename({1: 2, 2: 4}, axis='index')\n           A  B\n        0  1  4\n        2  2  5\n        4  3  6\n        \"\"\"\n        return super()._rename(mapper=mapper, index=index, columns=columns, axis=axis, copy=copy, inplace=inplace, level=level, errors=errors)\n    _shared_docs['pivot'] = '\\n        Return reshaped DataFrame organized by given index / column values.\\n\\n        Reshape data (produce a \"pivot\" table) based on column values. Uses\\n        unique values from specified `index` / `columns` to form axes of the\\n        resulting DataFrame. This function does not support data\\n        aggregation, multiple values will result in a MultiIndex in the\\n        columns. See the :ref:`User Guide <reshaping>` for more on reshaping.\\n\\n        Parameters\\n        ----------%s\\n        columns : str or object or a list of str\\n            Column to use to make new frame\\'s columns.\\n        index : str or object or a list of str, optional\\n            Column to use to make new frame\\'s index. If not given, uses existing index.\\n        values : str, object or a list of the previous, optional\\n            Column(s) to use for populating new frame\\'s values. If not\\n            specified, all remaining columns will be used and the result will\\n            have hierarchically indexed columns.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            Returns reshaped DataFrame.\\n\\n        Raises\\n        ------\\n        ValueError:\\n            When there are any `index`, `columns` combinations with multiple\\n            values. `DataFrame.pivot_table` when you need to aggregate.\\n\\n        See Also\\n        --------\\n        DataFrame.pivot_table : Generalization of pivot that can handle\\n            duplicate values for one index/column pair.\\n        DataFrame.unstack : Pivot based on the index values instead of a\\n            column.\\n        wide_to_long : Wide panel to long format. Less flexible but more\\n            user-friendly than melt.\\n\\n        Notes\\n        -----\\n        For finer-tuned control, see hierarchical indexing documentation along\\n        with the related stack/unstack methods.\\n\\n        Reference :ref:`the user guide <reshaping.pivot>` for more examples.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'foo\\': [\\'one\\', \\'one\\', \\'one\\', \\'two\\', \\'two\\',\\n        ...                            \\'two\\'],\\n        ...                    \\'bar\\': [\\'A\\', \\'B\\', \\'C\\', \\'A\\', \\'B\\', \\'C\\'],\\n        ...                    \\'baz\\': [1, 2, 3, 4, 5, 6],\\n        ...                    \\'zoo\\': [\\'x\\', \\'y\\', \\'z\\', \\'q\\', \\'w\\', \\'t\\']})\\n        >>> df\\n            foo   bar  baz  zoo\\n        0   one   A    1    x\\n        1   one   B    2    y\\n        2   one   C    3    z\\n        3   two   A    4    q\\n        4   two   B    5    w\\n        5   two   C    6    t\\n\\n        >>> df.pivot(index=\\'foo\\', columns=\\'bar\\', values=\\'baz\\')\\n        bar  A   B   C\\n        foo\\n        one  1   2   3\\n        two  4   5   6\\n\\n        >>> df.pivot(index=\\'foo\\', columns=\\'bar\\')[\\'baz\\']\\n        bar  A   B   C\\n        foo\\n        one  1   2   3\\n        two  4   5   6\\n\\n        >>> df.pivot(index=\\'foo\\', columns=\\'bar\\', values=[\\'baz\\', \\'zoo\\'])\\n              baz       zoo\\n        bar   A  B  C   A  B  C\\n        foo\\n        one   1  2  3   x  y  z\\n        two   4  5  6   q  w  t\\n\\n        You could also assign a list of column names or a list of index names.\\n\\n        >>> df = pd.DataFrame({\\n        ...        \"lev1\": [1, 1, 1, 2, 2, 2],\\n        ...        \"lev2\": [1, 1, 2, 1, 1, 2],\\n        ...        \"lev3\": [1, 2, 1, 2, 1, 2],\\n        ...        \"lev4\": [1, 2, 3, 4, 5, 6],\\n        ...        \"values\": [0, 1, 2, 3, 4, 5]})\\n        >>> df\\n            lev1 lev2 lev3 lev4 values\\n        0   1    1    1    1    0\\n        1   1    1    2    2    1\\n        2   1    2    1    3    2\\n        3   2    1    2    4    3\\n        4   2    1    1    5    4\\n        5   2    2    2    6    5\\n\\n        >>> df.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"], values=\"values\")\\n        lev2    1         2\\n        lev3    1    2    1    2\\n        lev1\\n        1     0.0  1.0  2.0  NaN\\n        2     4.0  3.0  NaN  5.0\\n\\n        >>> df.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"], values=\"values\")\\n              lev3    1    2\\n        lev1  lev2\\n           1     1  0.0  1.0\\n                 2  2.0  NaN\\n           2     1  4.0  3.0\\n                 2  NaN  5.0\\n\\n        A ValueError is raised if there are any duplicates.\\n\\n        >>> df = pd.DataFrame({\"foo\": [\\'one\\', \\'one\\', \\'two\\', \\'two\\'],\\n        ...                    \"bar\": [\\'A\\', \\'A\\', \\'B\\', \\'C\\'],\\n        ...                    \"baz\": [1, 2, 3, 4]})\\n        >>> df\\n           foo bar  baz\\n        0  one   A    1\\n        1  one   A    2\\n        2  two   B    3\\n        3  two   C    4\\n\\n        Notice that the first two rows are the same for our `index`\\n        and `columns` arguments.\\n\\n        >>> df.pivot(index=\\'foo\\', columns=\\'bar\\', values=\\'baz\\')\\n        Traceback (most recent call last):\\n           ...\\n        ValueError: Index contains duplicate entries, cannot reshape\\n        '\n\n    @doc(Series.swaplevel, klass=_shared_doc_kwargs['klass'], extra_params=dedent(\"axis : {0 or 'index', 1 or 'columns'}, default 0\\n            The axis to swap levels on. 0 or 'index' for row-wise, 1 or\\n            'columns' for column-wise.\"), examples=dedent('        Examples\\n        --------\\n        >>> df = pd.DataFrame(\\n        ...     {\"Grade\": [\"A\", \"B\", \"A\", \"C\"]},\\n        ...     index=[\\n        ...         [\"Final exam\", \"Final exam\", \"Coursework\", \"Coursework\"],\\n        ...         [\"History\", \"Geography\", \"History\", \"Geography\"],\\n        ...         [\"January\", \"February\", \"March\", \"April\"],\\n        ...     ],\\n        ... )\\n        >>> df\\n                                            Grade\\n        Final exam  History     January      A\\n                    Geography   February     B\\n        Coursework  History     March        A\\n                    Geography   April        C\\n\\n        In the following example, we will swap the levels of the indices.\\n        Here, we will swap the levels column-wise, but levels can be swapped row-wise\\n        in a similar manner. Note that column-wise is the default behaviour.\\n        By not supplying any arguments for i and j, we swap the last and second to\\n        last indices.\\n\\n        >>> df.swaplevel()\\n                                            Grade\\n        Final exam  January     History         A\\n                    February    Geography       B\\n        Coursework  March       History         A\\n                    April       Geography       C\\n\\n        By supplying one argument, we can choose which index to swap the last\\n        index with. We can for example swap the first index with the last one as\\n        follows.\\n\\n        >>> df.swaplevel(0)\\n                                            Grade\\n        January     History     Final exam      A\\n        February    Geography   Final exam      B\\n        March       History     Coursework      A\\n        April       Geography   Coursework      C\\n\\n        We can also define explicitly which indices we want to swap by supplying values\\n        for both i and j. Here, we for example swap the first and second indices.\\n\\n        >>> df.swaplevel(0, 1)\\n                                            Grade\\n        History     Final exam  January         A\\n        Geography   Final exam  February        B\\n        History     Coursework  March           A\\n        Geography   Coursework  April           C'))\n    def swaplevel(self, i: Axis=-2, j: Axis=-1, axis: Axis=0) -> DataFrame:\n        result = self.copy(deep=None)\n        axis = self._get_axis_number(axis)\n        if not isinstance(result._get_axis(axis), MultiIndex):\n            raise TypeError('Can only swap levels on a hierarchical axis.')\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.swaplevel(i, j)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.swaplevel(i, j)\n        return result\n    _shared_docs['pivot_table'] = '\\n        Create a spreadsheet-style pivot table as a DataFrame.\\n\\n        The levels in the pivot table will be stored in MultiIndex objects\\n        (hierarchical indexes) on the index and columns of the result DataFrame.\\n\\n        Parameters\\n        ----------%s\\n        values : list-like or scalar, optional\\n            Column or columns to aggregate.\\n        index : column, Grouper, array, or list of the previous\\n            Keys to group by on the pivot table index. If a list is passed,\\n            it can contain any of the other types (except list). If an array is\\n            passed, it must be the same length as the data and will be used in\\n            the same manner as column values.\\n        columns : column, Grouper, array, or list of the previous\\n            Keys to group by on the pivot table column. If a list is passed,\\n            it can contain any of the other types (except list). If an array is\\n            passed, it must be the same length as the data and will be used in\\n            the same manner as column values.\\n        aggfunc : function, list of functions, dict, default \"mean\"\\n            If a list of functions is passed, the resulting pivot table will have\\n            hierarchical columns whose top level are the function names\\n            (inferred from the function objects themselves).\\n            If a dict is passed, the key is column to aggregate and the value is\\n            function or list of functions. If ``margin=True``, aggfunc will be\\n            used to calculate the partial aggregates.\\n        fill_value : scalar, default None\\n            Value to replace missing values with (in the resulting pivot table,\\n            after aggregation).\\n        margins : bool, default False\\n            If ``margins=True``, special ``All`` columns and rows\\n            will be added with partial group aggregates across the categories\\n            on the rows and columns.\\n        dropna : bool, default True\\n            Do not include columns whose entries are all NaN. If True,\\n            rows with a NaN value in any column will be omitted before\\n            computing margins.\\n        margins_name : str, default \\'All\\'\\n            Name of the row / column that will contain the totals\\n            when margins is True.\\n        observed : bool, default False\\n            This only applies if any of the groupers are Categoricals.\\n            If True: only show observed values for categorical groupers.\\n            If False: show all values for categorical groupers.\\n\\n            .. deprecated:: 2.2.0\\n\\n                The default value of ``False`` is deprecated and will change to\\n                ``True`` in a future version of pandas.\\n\\n        sort : bool, default True\\n            Specifies if the result should be sorted.\\n\\n            .. versionadded:: 1.3.0\\n\\n        Returns\\n        -------\\n        DataFrame\\n            An Excel style pivot table.\\n\\n        See Also\\n        --------\\n        DataFrame.pivot : Pivot without aggregation that can handle\\n            non-numeric data.\\n        DataFrame.melt: Unpivot a DataFrame from wide to long format,\\n            optionally leaving identifiers set.\\n        wide_to_long : Wide panel to long format. Less flexible but more\\n            user-friendly than melt.\\n\\n        Notes\\n        -----\\n        Reference :ref:`the user guide <reshaping.pivot>` for more examples.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\\n        ...                          \"bar\", \"bar\", \"bar\", \"bar\"],\\n        ...                    \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\\n        ...                          \"one\", \"one\", \"two\", \"two\"],\\n        ...                    \"C\": [\"small\", \"large\", \"large\", \"small\",\\n        ...                          \"small\", \"large\", \"small\", \"small\",\\n        ...                          \"large\"],\\n        ...                    \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\\n        ...                    \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\\n        >>> df\\n             A    B      C  D  E\\n        0  foo  one  small  1  2\\n        1  foo  one  large  2  4\\n        2  foo  one  large  2  5\\n        3  foo  two  small  3  5\\n        4  foo  two  small  3  6\\n        5  bar  one  large  4  6\\n        6  bar  one  small  5  8\\n        7  bar  two  small  6  9\\n        8  bar  two  large  7  9\\n\\n        This first example aggregates values by taking the sum.\\n\\n        >>> table = pd.pivot_table(df, values=\\'D\\', index=[\\'A\\', \\'B\\'],\\n        ...                        columns=[\\'C\\'], aggfunc=\"sum\")\\n        >>> table\\n        C        large  small\\n        A   B\\n        bar one    4.0    5.0\\n            two    7.0    6.0\\n        foo one    4.0    1.0\\n            two    NaN    6.0\\n\\n        We can also fill missing values using the `fill_value` parameter.\\n\\n        >>> table = pd.pivot_table(df, values=\\'D\\', index=[\\'A\\', \\'B\\'],\\n        ...                        columns=[\\'C\\'], aggfunc=\"sum\", fill_value=0)\\n        >>> table\\n        C        large  small\\n        A   B\\n        bar one      4      5\\n            two      7      6\\n        foo one      4      1\\n            two      0      6\\n\\n        The next example aggregates by taking the mean across multiple columns.\\n\\n        >>> table = pd.pivot_table(df, values=[\\'D\\', \\'E\\'], index=[\\'A\\', \\'C\\'],\\n        ...                        aggfunc={\\'D\\': \"mean\", \\'E\\': \"mean\"})\\n        >>> table\\n                        D         E\\n        A   C\\n        bar large  5.500000  7.500000\\n            small  5.500000  8.500000\\n        foo large  2.000000  4.500000\\n            small  2.333333  4.333333\\n\\n        We can also calculate multiple types of aggregations for any given\\n        value column.\\n\\n        >>> table = pd.pivot_table(df, values=[\\'D\\', \\'E\\'], index=[\\'A\\', \\'C\\'],\\n        ...                        aggfunc={\\'D\\': \"mean\",\\n        ...                                 \\'E\\': [\"min\", \"max\", \"mean\"]})\\n        >>> table\\n                          D   E\\n                       mean max      mean  min\\n        A   C\\n        bar large  5.500000   9  7.500000    6\\n            small  5.500000   9  8.500000    8\\n        foo large  2.000000   5  4.500000    4\\n            small  2.333333   6  4.333333    2\\n        '\n\n    @Appender(ops.make_flex_doc('mod', 'dataframe'))\n    def mod(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, operator.mod, level=level, fill_value=fill_value, axis=axis)\n\n    @Appender(ops.make_flex_doc('rmod', 'dataframe'))\n    def rmod(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, roperator.rmod, level=level, fill_value=fill_value, axis=axis)\n\n    def dropna(self, *, axis: Axis=0, how: AnyAll | lib.NoDefault=lib.no_default, thresh: int | lib.NoDefault=lib.no_default, subset: IndexLabel | None=None, inplace: bool=False, ignore_index: bool=False) -> DataFrame | None:\n        \"\"\"\n        Remove missing values.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Determine if rows or columns which contain missing values are\n            removed.\n\n            * 0, or 'index' : Drop rows which contain missing values.\n            * 1, or 'columns' : Drop columns which contain missing value.\n\n            Only a single axis is allowed.\n\n        how : {'any', 'all'}, default 'any'\n            Determine if row or column is removed from DataFrame, when we have\n            at least one NA or all NA.\n\n            * 'any' : If any NA values are present, drop that row or column.\n            * 'all' : If all values are NA, drop that row or column.\n\n        thresh : int, optional\n            Require that many non-NA values. Cannot be combined with how.\n        subset : column label or sequence of labels, optional\n            Labels along other axis to consider, e.g. if you are dropping rows\n            these would be a list of columns to include.\n        inplace : bool, default False\n            Whether to modify the DataFrame rather than creating a new one.\n        ignore_index : bool, default ``False``\n            If ``True``, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 2.0.0\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with NA entries dropped from it or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.isna: Indicate missing values.\n        DataFrame.notna : Indicate existing (non-missing) values.\n        DataFrame.fillna : Replace missing values.\n        Series.dropna : Drop missing values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n        ...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n        ...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n        ...                             pd.NaT]})\n        >>> df\n               name        toy       born\n        0    Alfred        NaN        NaT\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Drop the rows where at least one element is missing.\n\n        >>> df.dropna()\n             name        toy       born\n        1  Batman  Batmobile 1940-04-25\n\n        Drop the columns where at least one element is missing.\n\n        >>> df.dropna(axis='columns')\n               name\n        0    Alfred\n        1    Batman\n        2  Catwoman\n\n        Drop the rows where all elements are missing.\n\n        >>> df.dropna(how='all')\n               name        toy       born\n        0    Alfred        NaN        NaT\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Keep only the rows with at least 2 non-NA values.\n\n        >>> df.dropna(thresh=2)\n               name        toy       born\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Define in which columns to look for missing values.\n\n        >>> df.dropna(subset=['name', 'toy'])\n               name        toy       born\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n        \"\"\"\n        if how is not lib.no_default and thresh is not lib.no_default:\n            raise TypeError('You cannot set both the how and thresh arguments at the same time.')\n        if how is lib.no_default:\n            how = 'any'\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        if isinstance(axis, (tuple, list)):\n            raise TypeError('supplying multiple axes to axis is no longer supported.')\n        axis = self._get_axis_number(axis)\n        agg_axis = 1 - axis\n        agg_obj = self\n        if subset is not None:\n            if not is_list_like(subset):\n                subset = [subset]\n            ax = self._get_axis(agg_axis)\n            indices = ax.get_indexer_for(subset)\n            check = indices == -1\n            if check.any():\n                raise KeyError(np.array(subset)[check].tolist())\n            agg_obj = self.take(indices, axis=agg_axis)\n        if thresh is not lib.no_default:\n            count = agg_obj.count(axis=agg_axis)\n            mask = count >= thresh\n        elif how == 'any':\n            mask = notna(agg_obj).all(axis=agg_axis, bool_only=False)\n        elif how == 'all':\n            mask = notna(agg_obj).any(axis=agg_axis, bool_only=False)\n        else:\n            raise ValueError(f'invalid how option: {how}')\n        if np.all(mask):\n            result = self.copy(deep=None)\n        else:\n            result = self.loc(axis=axis)[mask]\n        if ignore_index:\n            result.index = default_index(len(result))\n        if not inplace:\n            return result\n        self._update_inplace(result)\n        return None\n\n    def _maybe_align_series_as_frame(self, series: Series, axis: AxisInt):\n        \"\"\"\n        If the Series operand is not EA-dtype, we can broadcast to 2D and operate\n        blockwise.\n        \"\"\"\n        rvalues = series._values\n        if not isinstance(rvalues, np.ndarray):\n            if rvalues.dtype in ('datetime64[ns]', 'timedelta64[ns]'):\n                rvalues = np.asarray(rvalues)\n            else:\n                return series\n        if axis == 0:\n            rvalues = rvalues.reshape(-1, 1)\n        else:\n            rvalues = rvalues.reshape(1, -1)\n        rvalues = np.broadcast_to(rvalues, self.shape)\n        return self._constructor(rvalues, index=self.index, columns=self.columns, dtype=rvalues.dtype)\n\n    def _reset_cacher(self) -> None:\n        pass\n\n    def _get_agg_axis(self, axis_num: int) -> Index:\n        \"\"\"\n        Let's be explicit about this.\n        \"\"\"\n        if axis_num == 0:\n            return self.columns\n        elif axis_num == 1:\n            return self.index\n        else:\n            raise ValueError(f'Axis must be 0 or 1 (got {repr(axis_num)})')\n\n    def corrwith(self, other: DataFrame | Series, axis: Axis=0, drop: bool=False, method: CorrelationMethod='pearson', numeric_only: bool=False) -> Series:\n        \"\"\"\n        Compute pairwise correlation.\n\n        Pairwise correlation is computed between rows or columns of\n        DataFrame with rows or columns of Series or DataFrame. DataFrames\n        are first aligned along both axes before computing the\n        correlations.\n\n        Parameters\n        ----------\n        other : DataFrame, Series\n            Object with which to compute correlations.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' to compute row-wise, 1 or 'columns' for\n            column-wise.\n        drop : bool, default False\n            Drop missing indices from result.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method of correlation:\n\n            * pearson : standard correlation coefficient\n            * kendall : Kendall Tau correlation coefficient\n            * spearman : Spearman rank correlation\n            * callable: callable with input two 1d ndarrays\n                and returning a float.\n\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n                The default value of ``numeric_only`` is now ``False``.\n\n        Returns\n        -------\n        Series\n            Pairwise correlations.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation of columns.\n\n        Examples\n        --------\n        >>> index = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n        >>> columns = [\"one\", \"two\", \"three\", \"four\"]\n        >>> df1 = pd.DataFrame(np.arange(20).reshape(5, 4), index=index, columns=columns)\n        >>> df2 = pd.DataFrame(np.arange(16).reshape(4, 4), index=index[:4], columns=columns)\n        >>> df1.corrwith(df2)\n        one      1.0\n        two      1.0\n        three    1.0\n        four     1.0\n        dtype: float64\n\n        >>> df2.corrwith(df1, axis=1)\n        a    1.0\n        b    1.0\n        c    1.0\n        d    1.0\n        e    NaN\n        dtype: float64\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        this = self._get_numeric_data() if numeric_only else self\n        if isinstance(other, Series):\n            return this.apply(lambda x: other.corr(x, method=method), axis=axis)\n        if numeric_only:\n            other = other._get_numeric_data()\n        (left, right) = this.align(other, join='inner', copy=False)\n        if axis == 1:\n            left = left.T\n            right = right.T\n        if method == 'pearson':\n            left = left + right * 0\n            right = right + left * 0\n            ldem = left - left.mean(numeric_only=numeric_only)\n            rdem = right - right.mean(numeric_only=numeric_only)\n            num = (ldem * rdem).sum()\n            dom = (left.count() - 1) * left.std(numeric_only=numeric_only) * right.std(numeric_only=numeric_only)\n            correl = num / dom\n        elif method in ['kendall', 'spearman'] or callable(method):\n\n            def c(x):\n                return nanops.nancorr(x[0], x[1], method=method)\n            correl = self._constructor_sliced(map(c, zip(left.values.T, right.values.T)), index=left.columns, copy=False)\n        else:\n            raise ValueError(f\"Invalid method {method} was passed, valid methods are: 'pearson', 'kendall', 'spearman', or callable\")\n        if not drop:\n            raxis: AxisInt = 1 if axis == 0 else 0\n            result_index = this._get_axis(raxis).union(other._get_axis(raxis))\n            idx_diff = result_index.difference(correl.index)\n            if len(idx_diff) > 0:\n                correl = correl._append(Series([np.nan] * len(idx_diff), index=idx_diff))\n        return correl\n    _agg_see_also_doc = dedent('\\n    See Also\\n    --------\\n    DataFrame.apply : Perform any type of operations.\\n    DataFrame.transform : Perform transformation type operations.\\n    pandas.DataFrame.groupby : Perform operations over groups.\\n    pandas.DataFrame.resample : Perform operations over resampled bins.\\n    pandas.DataFrame.rolling : Perform operations over rolling window.\\n    pandas.DataFrame.expanding : Perform operations over expanding window.\\n    pandas.core.window.ewm.ExponentialMovingWindow : Perform operation over exponential\\n        weighted window.\\n    ')\n    _agg_examples_doc = dedent('\\n    Examples\\n    --------\\n    >>> df = pd.DataFrame([[1, 2, 3],\\n    ...                    [4, 5, 6],\\n    ...                    [7, 8, 9],\\n    ...                    [np.nan, np.nan, np.nan]],\\n    ...                   columns=[\\'A\\', \\'B\\', \\'C\\'])\\n\\n    Aggregate these functions over the rows.\\n\\n    >>> df.agg([\\'sum\\', \\'min\\'])\\n            A     B     C\\n    sum  12.0  15.0  18.0\\n    min   1.0   2.0   3.0\\n\\n    Different aggregations per column.\\n\\n    >>> df.agg({\\'A\\' : [\\'sum\\', \\'min\\'], \\'B\\' : [\\'min\\', \\'max\\']})\\n            A    B\\n    sum  12.0  NaN\\n    min   1.0  2.0\\n    max   NaN  8.0\\n\\n    Aggregate different functions over the columns and rename the index of the resulting\\n    DataFrame.\\n\\n    >>> df.agg(x=(\\'A\\', \\'max\\'), y=(\\'B\\', \\'min\\'), z=(\\'C\\', \\'mean\\'))\\n         A    B    C\\n    x  7.0  NaN  NaN\\n    y  NaN  2.0  NaN\\n    z  NaN  NaN  6.0\\n\\n    Aggregate over the columns.\\n\\n    >>> df.agg(\"mean\", axis=\"columns\")\\n    0    2.0\\n    1    5.0\\n    2    8.0\\n    3    NaN\\n    dtype: float64\\n    ')\n\n    def insert(self, loc: int, column: Hashable, value: Scalar | AnyArrayLike, allow_duplicates: bool | lib.NoDefault=lib.no_default) -> None:\n        \"\"\"\n        Insert column into DataFrame at specified location.\n\n        Raises a ValueError if `column` is already contained in the DataFrame,\n        unless `allow_duplicates` is set to True.\n\n        Parameters\n        ----------\n        loc : int\n            Insertion index. Must verify 0 <= loc <= len(columns).\n        column : str, number, or hashable object\n            Label of the inserted column.\n        value : Scalar, Series, or array-like\n            Content of the inserted column.\n        allow_duplicates : bool, optional, default lib.no_default\n            Allow duplicate column labels to be created.\n\n        See Also\n        --------\n        Index.insert : Insert new item by index.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df\n           col1  col2\n        0     1     3\n        1     2     4\n        >>> df.insert(1, \"newcol\", [99, 99])\n        >>> df\n           col1  newcol  col2\n        0     1      99     3\n        1     2      99     4\n        >>> df.insert(0, \"col1\", [100, 100], allow_duplicates=True)\n        >>> df\n           col1  col1  newcol  col2\n        0   100     1      99     3\n        1   100     2      99     4\n\n        Notice that pandas uses index alignment in case of `value` from type `Series`:\n\n        >>> df.insert(0, \"col0\", pd.Series([5, 6], index=[1, 2]))\n        >>> df\n           col0  col1  col1  newcol  col2\n        0   NaN   100     1      99     3\n        1   5.0   100     2      99     4\n        \"\"\"\n        if allow_duplicates is lib.no_default:\n            allow_duplicates = False\n        if allow_duplicates and (not self.flags.allows_duplicate_labels):\n            raise ValueError(\"Cannot specify 'allow_duplicates=True' when 'self.flags.allows_duplicate_labels' is False.\")\n        if not allow_duplicates and column in self.columns:\n            raise ValueError(f'cannot insert {column}, already exists')\n        if not is_integer(loc):\n            raise TypeError('loc must be int')\n        loc = int(loc)\n        if isinstance(value, DataFrame) and len(value.columns) > 1:\n            raise ValueError(f'Expected a one-dimensional object, got a DataFrame with {len(value.columns)} columns instead.')\n        elif isinstance(value, DataFrame):\n            value = value.iloc[:, 0]\n        (value, refs) = self._sanitize_column(value)\n        self._mgr.insert(loc, column, value, refs=refs)\n    agg = aggregate\n\n    def isin(self, values: Series | DataFrame | Sequence | Mapping) -> DataFrame:\n        \"\"\"\n        Whether each element in the DataFrame is contained in values.\n\n        Parameters\n        ----------\n        values : iterable, Series, DataFrame or dict\n            The result will only be true at a location if all the\n            labels match. If `values` is a Series, that's the index. If\n            `values` is a dict, the keys must be the column names,\n            which must match. If `values` is a DataFrame,\n            then both the index and column labels must match.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame of booleans showing whether each element in the DataFrame\n            is contained in values.\n\n        See Also\n        --------\n        DataFrame.eq: Equality test for DataFrame.\n        Series.isin: Equivalent method on Series.\n        Series.str.contains: Test if pattern or regex is contained within a\n            string of a Series or Index.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n        ...                   index=['falcon', 'dog'])\n        >>> df\n                num_legs  num_wings\n        falcon         2          2\n        dog            4          0\n\n        When ``values`` is a list check whether every value in the DataFrame\n        is present in the list (which animals have 0 or 2 legs or wings)\n\n        >>> df.isin([0, 2])\n                num_legs  num_wings\n        falcon      True       True\n        dog        False       True\n\n        To check if ``values`` is *not* in the DataFrame, use the ``~`` operator:\n\n        >>> ~df.isin([0, 2])\n                num_legs  num_wings\n        falcon     False      False\n        dog         True      False\n\n        When ``values`` is a dict, we can pass values to check for each\n        column separately:\n\n        >>> df.isin({'num_wings': [0, 3]})\n                num_legs  num_wings\n        falcon     False      False\n        dog        False       True\n\n        When ``values`` is a Series or DataFrame the index and column must\n        match. Note that 'falcon' does not match based on the number of legs\n        in other.\n\n        >>> other = pd.DataFrame({'num_legs': [8, 3], 'num_wings': [0, 2]},\n        ...                      index=['spider', 'falcon'])\n        >>> df.isin(other)\n                num_legs  num_wings\n        falcon     False       True\n        dog        False      False\n        \"\"\"\n        if isinstance(values, dict):\n            from pandas.core.reshape.concat import concat\n            values = collections.defaultdict(list, values)\n            result = concat((self.iloc[:, [i]].isin(values[col]) for (i, col) in enumerate(self.columns)), axis=1)\n        elif isinstance(values, Series):\n            if not values.index.is_unique:\n                raise ValueError('cannot compute isin with a duplicate axis.')\n            result = self.eq(values.reindex_like(self), axis='index')\n        elif isinstance(values, DataFrame):\n            if not (values.columns.is_unique and values.index.is_unique):\n                raise ValueError('cannot compute isin with a duplicate axis.')\n            result = self.eq(values.reindex_like(self))\n        else:\n            if not is_list_like(values):\n                raise TypeError(f\"only list-like or dict-like objects are allowed to be passed to DataFrame.isin(), you passed a '{type(values).__name__}'\")\n\n            def isin_(x):\n                result = algorithms.isin(x.ravel(), values)\n                return result.reshape(x.shape)\n            res_mgr = self._mgr.apply(isin_)\n            result = self._constructor_from_mgr(res_mgr, axes=res_mgr.axes)\n        return result.__finalize__(self, method='isin')\n\n    def _getitem_nocopy(self, key: list):\n        \"\"\"\n        Behaves like __getitem__, but returns a view in cases where __getitem__\n        would make a copy.\n        \"\"\"\n        indexer = self.columns._get_indexer_strict(key, 'columns')[1]\n        new_axis = self.columns[indexer]\n        new_mgr = self._mgr.reindex_indexer(new_axis, indexer, axis=0, allow_dups=True, copy=False, only_slice=True)\n        result = self._constructor_from_mgr(new_mgr, axes=new_mgr.axes)\n        result = result.__finalize__(self)\n        return result\n\n    def _to_dict_of_blocks(self):\n        \"\"\"\n        Return a dict of dtype -> Constructor Types that\n        each is a homogeneous dtype.\n\n        Internal ONLY - only works for BlockManager\n        \"\"\"\n        mgr = self._mgr\n        mgr = cast(BlockManager, mgr_to_mgr(mgr, 'block'))\n        return {k: self._constructor_from_mgr(v, axes=v.axes).__finalize__(self) for (k, v) in mgr.to_dict().items()}\n\n    @doc(_shared_docs['transform'], klass=_shared_doc_kwargs['klass'], axis=_shared_doc_kwargs['axis'])\n    def transform(self, func: AggFuncType, axis: Axis=0, *args, **kwargs) -> DataFrame:\n        from pandas.core.apply import frame_apply\n        op = frame_apply(self, func=func, axis=axis, args=args, kwargs=kwargs)\n        result = op.transform()\n        assert isinstance(result, DataFrame)\n        return result\n\n    def unstack(self, level: IndexLabel=-1, fill_value=None, sort: bool=True):\n        \"\"\"\n        Pivot a level of the (necessarily hierarchical) index labels.\n\n        Returns a DataFrame having a new level of column labels whose inner-most level\n        consists of the pivoted index labels.\n\n        If the index is not a MultiIndex, the output will be a Series\n        (the analogue of stack when the columns are not a MultiIndex).\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default -1 (last level)\n            Level(s) of index to unstack, can pass level name.\n        fill_value : int, str or dict\n            Replace NaN with this value if the unstack produces missing values.\n        sort : bool, default True\n            Sort the level(s) in the resulting MultiIndex columns.\n\n        Returns\n        -------\n        Series or DataFrame\n\n        See Also\n        --------\n        DataFrame.pivot : Pivot a table based on column values.\n        DataFrame.stack : Pivot a level of the column labels (inverse operation\n            from `unstack`).\n\n        Notes\n        -----\n        Reference :ref:`the user guide <reshaping.stacking>` for more examples.\n\n        Examples\n        --------\n        >>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\n        ...                                    ('two', 'a'), ('two', 'b')])\n        >>> s = pd.Series(np.arange(1.0, 5.0), index=index)\n        >>> s\n        one  a   1.0\n             b   2.0\n        two  a   3.0\n             b   4.0\n        dtype: float64\n\n        >>> s.unstack(level=-1)\n             a   b\n        one  1.0  2.0\n        two  3.0  4.0\n\n        >>> s.unstack(level=0)\n           one  two\n        a  1.0   3.0\n        b  2.0   4.0\n\n        >>> df = s.unstack(level=0)\n        >>> df.unstack()\n        one  a  1.0\n             b  2.0\n        two  a  3.0\n             b  4.0\n        dtype: float64\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n        result = unstack(self, level, fill_value, sort)\n        return result.__finalize__(self, method='unstack')\n\n    def sort_values(self, by: IndexLabel, *, axis: Axis=0, ascending: bool | list[bool] | tuple[bool, ...]=True, inplace: bool=False, kind: SortKind='quicksort', na_position: str='last', ignore_index: bool=False, key: ValueKeyFunc | None=None) -> DataFrame | None:\n        \"\"\"\n        Sort by the values along either axis.\n\n        Parameters\n        ----------\n        by : str or list of str\n            Name or list of names to sort by.\n\n            - if `axis` is 0 or `'index'` then `by` may contain index\n              levels and/or column labels.\n            - if `axis` is 1 or `'columns'` then `by` may contain column\n              levels and/or index labels.\n        axis : \"{0 or 'index', 1 or 'columns'}\", default 0\n             Axis to be sorted.\n        ascending : bool or list of bool, default True\n             Sort ascending vs. descending. Specify list for multiple sort\n             orders.  If this is a list of bools, must match the length of\n             the by.\n        inplace : bool, default False\n             If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n             Choice of sorting algorithm. See also :func:`numpy.sort` for more\n             information. `mergesort` and `stable` are the only stable algorithms. For\n             DataFrames, this option is only applied when sorting on a single\n             column or label.\n        na_position : {'first', 'last'}, default 'last'\n             Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n             end.\n        ignore_index : bool, default False\n             If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n        key : callable, optional\n            Apply the key function to the values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect a\n            ``Series`` and return a Series with the same shape as the input.\n            It will be applied to each column in `by` independently.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with sorted values or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.sort_index : Sort a DataFrame by the index.\n        Series.sort_values : Similar method for a Series.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\n        ...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n        ...     'col2': [2, 1, 9, 8, 7, 4],\n        ...     'col3': [0, 1, 9, 4, 2, 3],\n        ...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n        ... })\n        >>> df\n          col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n\n        Sort by col1\n\n        >>> df.sort_values(by=['col1'])\n          col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        5    C     4     3    F\n        4    D     7     2    e\n        3  NaN     8     4    D\n\n        Sort by multiple columns\n\n        >>> df.sort_values(by=['col1', 'col2'])\n          col1  col2  col3 col4\n        1    A     1     1    B\n        0    A     2     0    a\n        2    B     9     9    c\n        5    C     4     3    F\n        4    D     7     2    e\n        3  NaN     8     4    D\n\n        Sort Descending\n\n        >>> df.sort_values(by='col1', ascending=False)\n          col1  col2  col3 col4\n        4    D     7     2    e\n        5    C     4     3    F\n        2    B     9     9    c\n        0    A     2     0    a\n        1    A     1     1    B\n        3  NaN     8     4    D\n\n        Putting NAs first\n\n        >>> df.sort_values(by='col1', ascending=False, na_position='first')\n          col1  col2  col3 col4\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n        2    B     9     9    c\n        0    A     2     0    a\n        1    A     1     1    B\n\n        Sorting with a key function\n\n        >>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n           col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n\n        Natural sort with the key argument,\n        using the `natsort <https://github.com/SethMMorton/natsort>` package.\n\n        >>> df = pd.DataFrame({\n        ...    \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n        ...    \"value\": [10, 20, 30, 40, 50]\n        ... })\n        >>> df\n            time  value\n        0    0hr     10\n        1  128hr     20\n        2   72hr     30\n        3   48hr     40\n        4   96hr     50\n        >>> from natsort import index_natsorted\n        >>> df.sort_values(\n        ...     by=\"time\",\n        ...     key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n        ... )\n            time  value\n        0    0hr     10\n        3   48hr     40\n        2   72hr     30\n        4   96hr     50\n        1  128hr     20\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        axis = self._get_axis_number(axis)\n        ascending = validate_ascending(ascending)\n        if not isinstance(by, list):\n            by = [by]\n        if is_sequence(ascending) and len(by) != len(ascending):\n            raise ValueError(f'Length of ascending ({len(ascending)}) != length of by ({len(by)})')\n        if len(by) > 1:\n            keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n            if key is not None:\n                keys = [Series(k, name=name) for (k, name) in zip(keys, by)]\n            indexer = lexsort_indexer(keys, orders=ascending, na_position=na_position, key=key)\n        elif len(by):\n            k = self._get_label_or_level_values(by[0], axis=axis)\n            if key is not None:\n                k = Series(k, name=by[0])\n            if isinstance(ascending, (tuple, list)):\n                ascending = ascending[0]\n            indexer = nargsort(k, kind=kind, ascending=ascending, na_position=na_position, key=key)\n        elif inplace:\n            return self._update_inplace(self)\n        else:\n            return self.copy(deep=None)\n        if is_range_indexer(indexer, len(indexer)):\n            result = self.copy(deep=not inplace and (not using_copy_on_write()))\n            if ignore_index:\n                result.index = default_index(len(result))\n            if inplace:\n                return self._update_inplace(result)\n            else:\n                return result\n        new_data = self._mgr.take(indexer, axis=self._get_block_manager_axis(axis), verify=False)\n        if ignore_index:\n            new_data.set_axis(self._get_block_manager_axis(axis), default_index(len(indexer)))\n        result = self._constructor_from_mgr(new_data, axes=new_data.axes)\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method='sort_values')\n\n    @final\n    def _indexed_same(self, other) -> bool_t:\n        return all((self._get_axis(a).equals(other._get_axis(a)) for a in self._AXIS_ORDERS))\n\n    def _gotitem(self, key: IndexLabel, ndim: int, subset: DataFrame | Series | None=None) -> DataFrame | Series:\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        if subset is None:\n            subset = self\n        elif subset.ndim == 1:\n            return subset\n        return subset[key]\n\n    def _create_data_for_split_and_tight_to_dict(self, are_all_object_dtype_cols: bool, object_dtype_indices: list[int]) -> list:\n        \"\"\"\n        Simple helper method to create data for to ``to_dict(orient=\"split\")`` and\n        ``to_dict(orient=\"tight\")`` to create the main output data\n        \"\"\"\n        if are_all_object_dtype_cols:\n            data = [list(map(maybe_box_native, t)) for t in self.itertuples(index=False, name=None)]\n        else:\n            data = [list(t) for t in self.itertuples(index=False, name=None)]\n            if object_dtype_indices:\n                for row in data:\n                    for i in object_dtype_indices:\n                        row[i] = maybe_box_native(row[i])\n        return data\n\n    @Appender(ops.make_flex_doc('rtruediv', 'dataframe'))\n    def rtruediv(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, roperator.rtruediv, level=level, fill_value=fill_value, axis=axis)\n\n    def corr(self, method: CorrelationMethod='pearson', min_periods: int=1, numeric_only: bool=False) -> DataFrame:\n        \"\"\"\n        Compute pairwise correlation of columns, excluding NA/null values.\n\n        Parameters\n        ----------\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method of correlation:\n\n            * pearson : standard correlation coefficient\n            * kendall : Kendall Tau correlation coefficient\n            * spearman : Spearman rank correlation\n            * callable: callable with input two 1d ndarrays\n                and returning a float. Note that the returned matrix from corr\n                will have 1 along the diagonals and will be symmetric\n                regardless of the callable's behavior.\n        min_periods : int, optional\n            Minimum number of observations required per pair of columns\n            to have a valid result. Currently only available for Pearson\n            and Spearman correlation.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n                The default value of ``numeric_only`` is now ``False``.\n\n        Returns\n        -------\n        DataFrame\n            Correlation matrix.\n\n        See Also\n        --------\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n        Series.corr : Compute the correlation between two Series.\n\n        Notes\n        -----\n        Pearson, Kendall and Spearman correlation are currently computed using pairwise complete observations.\n\n        * `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`_\n        * `Kendall rank correlation coefficient <https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient>`_\n        * `Spearman's rank correlation coefficient <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>`_\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.corr(method=histogram_intersection)\n              dogs  cats\n        dogs   1.0   0.3\n        cats   0.3   1.0\n\n        >>> df = pd.DataFrame([(1, 1), (2, np.nan), (np.nan, 3), (4, 4)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.corr(min_periods=3)\n              dogs  cats\n        dogs   1.0   NaN\n        cats   NaN   1.0\n        \"\"\"\n        data = self._get_numeric_data() if numeric_only else self\n        cols = data.columns\n        idx = cols.copy()\n        mat = data.to_numpy(dtype=float, na_value=np.nan, copy=False)\n        if method == 'pearson':\n            correl = libalgos.nancorr(mat, minp=min_periods)\n        elif method == 'spearman':\n            correl = libalgos.nancorr_spearman(mat, minp=min_periods)\n        elif method == 'kendall' or callable(method):\n            if min_periods is None:\n                min_periods = 1\n            mat = mat.T\n            corrf = nanops.get_corr_func(method)\n            K = len(cols)\n            correl = np.empty((K, K), dtype=float)\n            mask = np.isfinite(mat)\n            for (i, ac) in enumerate(mat):\n                for (j, bc) in enumerate(mat):\n                    if i > j:\n                        continue\n                    valid = mask[i] & mask[j]\n                    if valid.sum() < min_periods:\n                        c = np.nan\n                    elif i == j:\n                        c = 1.0\n                    elif not valid.all():\n                        c = corrf(ac[valid], bc[valid])\n                    else:\n                        c = corrf(ac, bc)\n                    correl[i, j] = c\n                    correl[j, i] = c\n        else:\n            raise ValueError(f\"method must be either 'pearson', 'spearman', 'kendall', or a callable, '{method}' was supplied\")\n        result = self._constructor(correl, index=idx, columns=cols, copy=False)\n        return result.__finalize__(self, method='corr')\n\n    @doc(make_doc('cumsum', ndim=2))\n    def cumsum(self, axis: Axis | None=None, skipna: bool=True, *args, **kwargs):\n        return NDFrame.cumsum(self, axis, skipna, *args, **kwargs)\n\n    @Appender(ops.make_flex_doc('mul', 'dataframe'))\n    def mul(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, operator.mul, level=level, fill_value=fill_value, axis=axis)\n\n    def set_index(self, keys, *, drop: bool=True, append: bool=False, inplace: bool=False, verify_integrity: bool=False) -> DataFrame | None:\n        \"\"\"\n        Set the DataFrame index using existing columns.\n\n        Set the DataFrame index (row labels) using one or more existing\n        columns or arrays (of the correct length). The index can replace the\n        existing index or expand on it.\n\n        Parameters\n        ----------\n        keys : label or array-like or list of labels/arrays\n            This parameter can be either a single column key, a single array of\n            the same length as the calling DataFrame, or a list containing an\n            arbitrary combination of column keys and arrays. Here, \"array\"\n            encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and\n            instances of :class:`~collections.abc.Iterator`.\n        drop : bool, default True\n            Delete columns to be used as the new index.\n        append : bool, default False\n            Whether to append columns to existing index.\n        inplace : bool, default False\n            Whether to modify the DataFrame rather than creating a new one.\n        verify_integrity : bool, default False\n            Check the new index for duplicates. Otherwise defer the check until\n            necessary. Setting to False will improve the performance of this\n            method.\n\n        Returns\n        -------\n        DataFrame or None\n            Changed row labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.reset_index : Opposite of set_index.\n        DataFrame.reindex : Change to new indices or expand indices.\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'month': [1, 4, 7, 10],\n        ...                    'year': [2012, 2014, 2013, 2014],\n        ...                    'sale': [55, 40, 84, 31]})\n        >>> df\n           month  year  sale\n        0      1  2012    55\n        1      4  2014    40\n        2      7  2013    84\n        3     10  2014    31\n\n        Set the index to become the 'month' column:\n\n        >>> df.set_index('month')\n               year  sale\n        month\n        1      2012    55\n        4      2014    40\n        7      2013    84\n        10     2014    31\n\n        Create a MultiIndex using columns 'year' and 'month':\n\n        >>> df.set_index(['year', 'month'])\n                    sale\n        year  month\n        2012  1     55\n        2014  4     40\n        2013  7     84\n        2014  10    31\n\n        Create a MultiIndex using an Index and a column:\n\n        >>> df.set_index([pd.Index([1, 2, 3, 4]), 'year'])\n                 month  sale\n           year\n        1  2012  1      55\n        2  2014  4      40\n        3  2013  7      84\n        4  2014  10     31\n\n        Create a MultiIndex using two Series:\n\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> df.set_index([s, s**2])\n              month  year  sale\n        1 1       1  2012    55\n        2 4       4  2014    40\n        3 9       7  2013    84\n        4 16     10  2014    31\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        if not isinstance(keys, list):\n            keys = [keys]\n        err_msg = 'The parameter \"keys\" may be a column key, one-dimensional array, or a list containing only valid column keys and one-dimensional arrays.'\n        missing: list[Hashable] = []\n        for col in keys:\n            if isinstance(col, (Index, Series, np.ndarray, list, abc.Iterator)):\n                if getattr(col, 'ndim', 1) != 1:\n                    raise ValueError(err_msg)\n            else:\n                try:\n                    found = col in self.columns\n                except TypeError as err:\n                    raise TypeError(f'{err_msg}. Received column of type {type(col)}') from err\n                else:\n                    if not found:\n                        missing.append(col)\n        if missing:\n            raise KeyError(f'None of {missing} are in the columns')\n        if inplace:\n            frame = self\n        else:\n            frame = self.copy(deep=None)\n        arrays: list[Index] = []\n        names: list[Hashable] = []\n        if append:\n            names = list(self.index.names)\n            if isinstance(self.index, MultiIndex):\n                arrays.extend((self.index._get_level_values(i) for i in range(self.index.nlevels)))\n            else:\n                arrays.append(self.index)\n        to_remove: list[Hashable] = []\n        for col in keys:\n            if isinstance(col, MultiIndex):\n                arrays.extend((col._get_level_values(n) for n in range(col.nlevels)))\n                names.extend(col.names)\n            elif isinstance(col, (Index, Series)):\n                arrays.append(col)\n                names.append(col.name)\n            elif isinstance(col, (list, np.ndarray)):\n                arrays.append(col)\n                names.append(None)\n            elif isinstance(col, abc.Iterator):\n                arrays.append(list(col))\n                names.append(None)\n            else:\n                arrays.append(frame[col])\n                names.append(col)\n                if drop:\n                    to_remove.append(col)\n            if len(arrays[-1]) != len(self):\n                raise ValueError(f'Length mismatch: Expected {len(self)} rows, received array of length {len(arrays[-1])}')\n        index = ensure_index_from_sequences(arrays, names)\n        if verify_integrity and (not index.is_unique):\n            duplicates = index[index.duplicated()].unique()\n            raise ValueError(f'Index has duplicate keys: {duplicates}')\n        for c in set(to_remove):\n            del frame[c]\n        index._cleanup()\n        frame.index = index\n        if not inplace:\n            return frame\n        return None\n\n    def _iter_column_arrays(self) -> Iterator[ArrayLike]:\n        \"\"\"\n        Iterate over the arrays of all columns in order.\n        This returns the values as stored in the Block (ndarray or ExtensionArray).\n\n        Warning! The returned array is a view but doesn't handle Copy-on-Write,\n        so this should be used with caution (for read-only purposes).\n        \"\"\"\n        if isinstance(self._mgr, ArrayManager):\n            yield from self._mgr.arrays\n        else:\n            for i in range(len(self.columns)):\n                yield self._get_column_array(i)\n\n    @doc(INFO_DOCSTRING, **frame_sub_kwargs)\n    def info(self, verbose: bool | None=None, buf: WriteBuffer[str] | None=None, max_cols: int | None=None, memory_usage: bool | str | None=None, show_counts: bool | None=None) -> None:\n        info = DataFrameInfo(data=self, memory_usage=memory_usage)\n        info.render(buf=buf, max_cols=max_cols, verbose=verbose, show_counts=show_counts)\n\n    def _iset_item(self, loc: int, value: Series, inplace: bool=True) -> None:\n        if using_copy_on_write():\n            self._iset_item_mgr(loc, value._values, inplace=inplace, refs=value._references)\n        else:\n            self._iset_item_mgr(loc, value._values.copy(), inplace=True)\n        if len(self):\n            self._check_setitem_copy()\n\n    def isetitem(self, loc, value) -> None:\n        \"\"\"\n        Set the given value in the column with position `loc`.\n\n        This is a positional analogue to ``__setitem__``.\n\n        Parameters\n        ----------\n        loc : int or sequence of ints\n            Index position for the column.\n        value : scalar or arraylike\n            Value(s) for the column.\n\n        Notes\n        -----\n        ``frame.isetitem(loc, value)`` is an in-place method as it will\n        modify the DataFrame in place (not returning a new object). In contrast to\n        ``frame.iloc[:, i] = value`` which will try to update the existing values in\n        place, ``frame.isetitem(loc, value)`` will not update the values of the column\n        itself in place, it will instead insert a new array.\n\n        In cases where ``frame.columns`` is unique, this is equivalent to\n        ``frame[frame.columns[i]] = value``.\n        \"\"\"\n        if isinstance(value, DataFrame):\n            if is_integer(loc):\n                loc = [loc]\n            if len(loc) != len(value.columns):\n                raise ValueError(f'Got {len(loc)} positions but value has {len(value.columns)} columns.')\n            for (i, idx) in enumerate(loc):\n                (arraylike, refs) = self._sanitize_column(value.iloc[:, i])\n                self._iset_item_mgr(idx, arraylike, inplace=False, refs=refs)\n            return\n        (arraylike, refs) = self._sanitize_column(value)\n        self._iset_item_mgr(loc, arraylike, inplace=False, refs=refs)\n\n    @doc(make_doc('std', ndim=2))\n    def std(self, axis: Axis | None=0, skipna: bool=True, ddof: int=1, numeric_only: bool=False, **kwargs):\n        result = super().std(axis, skipna, ddof, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method='std')\n        return result\n\n    def stack(self, level: IndexLabel=-1, dropna: bool | lib.NoDefault=lib.no_default, sort: bool | lib.NoDefault=lib.no_default, future_stack: bool=False):\n        \"\"\"\n        Stack the prescribed level(s) from columns to index.\n\n        Return a reshaped DataFrame or Series having a multi-level\n        index with one or more new inner-most levels compared to the current\n        DataFrame. The new inner-most levels are created by pivoting the\n        columns of the current dataframe:\n\n          - if the columns have a single level, the output is a Series;\n          - if the columns have multiple levels, the new index\n            level(s) is (are) taken from the prescribed level(s) and\n            the output is a DataFrame.\n\n        Parameters\n        ----------\n        level : int, str, list, default -1\n            Level(s) to stack from the column axis onto the index\n            axis, defined as one index or label, or a list of indices\n            or labels.\n        dropna : bool, default True\n            Whether to drop rows in the resulting Frame/Series with\n            missing values. Stacking a column level onto the index\n            axis can create combinations of index and column values\n            that are missing from the original dataframe. See Examples\n            section.\n        sort : bool, default True\n            Whether to sort the levels of the resulting MultiIndex.\n        future_stack : bool, default False\n            Whether to use the new implementation that will replace the current\n            implementation in pandas 3.0. When True, dropna and sort have no impact\n            on the result and must remain unspecified. See :ref:`pandas 2.1.0 Release\n            notes <whatsnew_210.enhancements.new_stack>` for more details.\n\n        Returns\n        -------\n        DataFrame or Series\n            Stacked dataframe or series.\n\n        See Also\n        --------\n        DataFrame.unstack : Unstack prescribed level(s) from index axis\n             onto column axis.\n        DataFrame.pivot : Reshape dataframe from long format to wide\n             format.\n        DataFrame.pivot_table : Create a spreadsheet-style pivot table\n             as a DataFrame.\n\n        Notes\n        -----\n        The function is named by analogy with a collection of books\n        being reorganized from being side by side on a horizontal\n        position (the columns of the dataframe) to being stacked\n        vertically on top of each other (in the index of the\n        dataframe).\n\n        Reference :ref:`the user guide <reshaping.stacking>` for more examples.\n\n        Examples\n        --------\n        **Single level columns**\n\n        >>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=['weight', 'height'])\n\n        Stacking a dataframe with a single level column axis returns a Series:\n\n        >>> df_single_level_cols\n             weight height\n        cat       0      1\n        dog       2      3\n        >>> df_single_level_cols.stack(future_stack=True)\n        cat  weight    0\n             height    1\n        dog  weight    2\n             height    3\n        dtype: int64\n\n        **Multi level columns: simple case**\n\n        >>> multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n        ...                                        ('weight', 'pounds')])\n        >>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol1)\n\n        Stacking a dataframe with a multi-level column axis:\n\n        >>> df_multi_level_cols1\n             weight\n                 kg    pounds\n        cat       1        2\n        dog       2        4\n        >>> df_multi_level_cols1.stack(future_stack=True)\n                    weight\n        cat kg           1\n            pounds       2\n        dog kg           2\n            pounds       4\n\n        **Missing values**\n\n        >>> multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n        ...                                        ('height', 'm')])\n        >>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol2)\n\n        It is common to have missing values when stacking a dataframe\n        with multi-level columns, as the stacked dataframe typically\n        has more values than the original dataframe. Missing values\n        are filled with NaNs:\n\n        >>> df_multi_level_cols2\n            weight height\n                kg      m\n        cat    1.0    2.0\n        dog    3.0    4.0\n        >>> df_multi_level_cols2.stack(future_stack=True)\n                weight  height\n        cat kg     1.0     NaN\n            m      NaN     2.0\n        dog kg     3.0     NaN\n            m      NaN     4.0\n\n        **Prescribing the level(s) to be stacked**\n\n        The first parameter controls which level or levels are stacked:\n\n        >>> df_multi_level_cols2.stack(0, future_stack=True)\n                     kg    m\n        cat weight  1.0  NaN\n            height  NaN  2.0\n        dog weight  3.0  NaN\n            height  NaN  4.0\n        >>> df_multi_level_cols2.stack([0, 1], future_stack=True)\n        cat  weight  kg    1.0\n             height  m     2.0\n        dog  weight  kg    3.0\n             height  m     4.0\n        dtype: float64\n        \"\"\"\n        if not future_stack:\n            from pandas.core.reshape.reshape import stack, stack_multiple\n            if dropna is not lib.no_default or sort is not lib.no_default or self.columns.nlevels > 1:\n                warnings.warn(\"The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\", FutureWarning, stacklevel=find_stack_level())\n            if dropna is lib.no_default:\n                dropna = True\n            if sort is lib.no_default:\n                sort = True\n            if isinstance(level, (tuple, list)):\n                result = stack_multiple(self, level, dropna=dropna, sort=sort)\n            else:\n                result = stack(self, level, dropna=dropna, sort=sort)\n        else:\n            from pandas.core.reshape.reshape import stack_v3\n            if dropna is not lib.no_default:\n                raise ValueError('dropna must be unspecified with future_stack=True as the new implementation does not introduce rows of NA values. This argument will be removed in a future version of pandas.')\n            if sort is not lib.no_default:\n                raise ValueError('Cannot specify sort with future_stack=True, this argument will be removed in a future version of pandas. Sort the result using .sort_index instead.')\n            if isinstance(level, (tuple, list)) and (not all((lev in self.columns.names for lev in level))) and (not all((isinstance(lev, int) for lev in level))):\n                raise ValueError('level should contain all level names or all level numbers, not a mixture of the two.')\n            if not isinstance(level, (tuple, list)):\n                level = [level]\n            level = [self.columns._get_level_number(lev) for lev in level]\n            result = stack_v3(self, level)\n        return result.__finalize__(self, method='stack')\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular DataFrame.\n        \"\"\"\n        if self._info_repr():\n            buf = StringIO()\n            self.info(buf=buf)\n            return buf.getvalue()\n        repr_params = fmt.get_dataframe_repr_params()\n        return self.to_string(**repr_params)\n\n    def _arith_method_with_reindex(self, right: DataFrame, op) -> DataFrame:\n        \"\"\"\n        For DataFrame-with-DataFrame operations that require reindexing,\n        operate only on shared columns, then reindex.\n\n        Parameters\n        ----------\n        right : DataFrame\n        op : binary operator\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        left = self\n        (cols, lcols, rcols) = left.columns.join(right.columns, how='inner', level=None, return_indexers=True)\n        new_left = left.iloc[:, lcols]\n        new_right = right.iloc[:, rcols]\n        result = op(new_left, new_right)\n        (join_columns, _, _) = left.columns.join(right.columns, how='outer', level=None, return_indexers=True)\n        if result.columns.has_duplicates:\n            (indexer, _) = result.columns.get_indexer_non_unique(join_columns)\n            indexer = algorithms.unique1d(indexer)\n            result = result._reindex_with_indexers({1: [join_columns, indexer]}, allow_dups=True)\n        else:\n            result = result.reindex(join_columns, axis=1)\n        return result\n\n    def __rmatmul__(self, other) -> DataFrame:\n        \"\"\"\n        Matrix multiplication using binary `@` operator.\n        \"\"\"\n        try:\n            return self.T.dot(np.transpose(other)).T\n        except ValueError as err:\n            if 'shape mismatch' not in str(err):\n                raise\n            msg = f'shapes {np.shape(other)} and {self.shape} not aligned'\n            raise ValueError(msg) from err\n\n    @doc(make_doc('median', ndim=2))\n    def median(self, axis: Axis | None=0, skipna: bool=True, numeric_only: bool=False, **kwargs):\n        result = super().median(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method='median')\n        return result\n\n    def _dispatch_frame_op(self, right, func: Callable, axis: AxisInt | None=None) -> DataFrame:\n        \"\"\"\n        Evaluate the frame operation func(left, right) by evaluating\n        column-by-column, dispatching to the Series implementation.\n\n        Parameters\n        ----------\n        right : scalar, Series, or DataFrame\n        func : arithmetic or comparison operator\n        axis : {None, 0, 1}\n\n        Returns\n        -------\n        DataFrame\n\n        Notes\n        -----\n        Caller is responsible for setting np.errstate where relevant.\n        \"\"\"\n        array_op = ops.get_array_op(func)\n        right = lib.item_from_zerodim(right)\n        if not is_list_like(right):\n            bm = self._mgr.apply(array_op, right=right)\n            return self._constructor_from_mgr(bm, axes=bm.axes)\n        elif isinstance(right, DataFrame):\n            assert self.index.equals(right.index)\n            assert self.columns.equals(right.columns)\n            bm = self._mgr.operate_blockwise(right._mgr, array_op)\n            return self._constructor_from_mgr(bm, axes=bm.axes)\n        elif isinstance(right, Series) and axis == 1:\n            assert right.index.equals(self.columns)\n            right = right._values\n            assert not isinstance(right, np.ndarray)\n            arrays = [array_op(_left, _right) for (_left, _right) in zip(self._iter_column_arrays(), right)]\n        elif isinstance(right, Series):\n            assert right.index.equals(self.index)\n            right = right._values\n            arrays = [array_op(left, right) for left in self._iter_column_arrays()]\n        else:\n            raise NotImplementedError(right)\n        return type(self)._from_arrays(arrays, self.columns, self.index, verify_integrity=False)\n\n    def _reduce(self, op, name: str, *, axis: Axis=0, skipna: bool=True, numeric_only: bool=False, filter_type=None, **kwds):\n        assert filter_type is None or filter_type == 'bool', filter_type\n        out_dtype = 'bool' if filter_type == 'bool' else None\n        if axis is not None:\n            axis = self._get_axis_number(axis)\n\n        def func(values: np.ndarray):\n            return op(values, axis=axis, skipna=skipna, **kwds)\n        dtype_has_keepdims: dict[ExtensionDtype, bool] = {}\n\n        def blk_func(values, axis: Axis=1):\n            if isinstance(values, ExtensionArray):\n                if not is_1d_only_ea_dtype(values.dtype) and (not isinstance(self._mgr, ArrayManager)):\n                    return values._reduce(name, axis=1, skipna=skipna, **kwds)\n                has_keepdims = dtype_has_keepdims.get(values.dtype)\n                if has_keepdims is None:\n                    sign = signature(values._reduce)\n                    has_keepdims = 'keepdims' in sign.parameters\n                    dtype_has_keepdims[values.dtype] = has_keepdims\n                if has_keepdims:\n                    return values._reduce(name, skipna=skipna, keepdims=True, **kwds)\n                else:\n                    warnings.warn(f'{type(values)}._reduce will require a `keepdims` parameter in the future', FutureWarning, stacklevel=find_stack_level())\n                    result = values._reduce(name, skipna=skipna, **kwds)\n                    return np.array([result])\n            else:\n                return op(values, axis=axis, skipna=skipna, **kwds)\n\n        def _get_data() -> DataFrame:\n            if filter_type is None:\n                data = self._get_numeric_data()\n            else:\n                assert filter_type == 'bool'\n                data = self._get_bool_data()\n            return data\n        df = self\n        if numeric_only:\n            df = _get_data()\n        if axis is None:\n            dtype = find_common_type([arr.dtype for arr in df._mgr.arrays])\n            if isinstance(dtype, ExtensionDtype):\n                df = df.astype(dtype, copy=False)\n                arr = concat_compat(list(df._iter_column_arrays()))\n                return arr._reduce(name, skipna=skipna, keepdims=False, **kwds)\n            return func(df.values)\n        elif axis == 1:\n            if len(df.index) == 0:\n                result = df._reduce(op, name, axis=0, skipna=skipna, numeric_only=False, filter_type=filter_type, **kwds).iloc[:0]\n                result.index = df.index\n                return result\n            if df.shape[1] and name != 'kurt':\n                dtype = find_common_type([arr.dtype for arr in df._mgr.arrays])\n                if isinstance(dtype, ExtensionDtype):\n                    name = {'argmax': 'idxmax', 'argmin': 'idxmin'}.get(name, name)\n                    df = df.astype(dtype, copy=False)\n                    arr = concat_compat(list(df._iter_column_arrays()))\n                    (nrows, ncols) = df.shape\n                    row_index = np.tile(np.arange(nrows), ncols)\n                    col_index = np.repeat(np.arange(ncols), nrows)\n                    ser = Series(arr, index=col_index, copy=False)\n                    with rewrite_warning(target_message=f'The behavior of SeriesGroupBy.{name} with all-NA values', target_category=FutureWarning, new_message=f'The behavior of {type(self).__name__}.{name} with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError'):\n                        result = ser.groupby(row_index).agg(name, **kwds)\n                    result.index = df.index\n                    if not skipna and name not in ('any', 'all'):\n                        mask = df.isna().to_numpy(dtype=np.bool_).any(axis=1)\n                        other = -1 if name in ('idxmax', 'idxmin') else lib.no_default\n                        result = result.mask(mask, other)\n                    return result\n            df = df.T\n        res = df._mgr.reduce(blk_func)\n        out = df._constructor_from_mgr(res, axes=res.axes).iloc[0]\n        if out_dtype is not None and out.dtype != 'boolean':\n            out = out.astype(out_dtype)\n        elif (df._mgr.get_dtypes() == object).any() and name not in ['any', 'all']:\n            out = out.astype(object)\n        elif len(self) == 0 and out.dtype == object and (name in ('sum', 'prod')):\n            out = out.astype(np.float64)\n        return out\n\n    def update(self, other, join: UpdateJoin='left', overwrite: bool=True, filter_func=None, errors: IgnoreRaise='ignore') -> None:\n        \"\"\"\n        Modify in place using non-NA values from another DataFrame.\n\n        Aligns on indices. There is no return value.\n\n        Parameters\n        ----------\n        other : DataFrame, or object coercible into a DataFrame\n            Should have at least one matching index/column label\n            with the original DataFrame. If a Series is passed,\n            its name attribute must be set, and that will be\n            used as the column name to align with the original DataFrame.\n        join : {'left'}, default 'left'\n            Only left join is implemented, keeping the index and columns of the\n            original object.\n        overwrite : bool, default True\n            How to handle non-NA values for overlapping keys:\n\n            * True: overwrite original DataFrame's values\n              with values from `other`.\n            * False: only update values that are NA in\n              the original DataFrame.\n\n        filter_func : callable(1d-array) -> bool 1d-array, optional\n            Can choose to replace values other than NA. Return True for values\n            that should be updated.\n        errors : {'raise', 'ignore'}, default 'ignore'\n            If 'raise', will raise a ValueError if the DataFrame and `other`\n            both contain non-NA data in the same place.\n\n        Returns\n        -------\n        None\n            This method directly changes calling object.\n\n        Raises\n        ------\n        ValueError\n            * When `errors='raise'` and there's overlapping non-NA data.\n            * When `errors` is not either `'ignore'` or `'raise'`\n        NotImplementedError\n            * If `join != 'left'`\n\n        See Also\n        --------\n        dict.update : Similar method for dictionaries.\n        DataFrame.merge : For column(s)-on-column(s) operations.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3],\n        ...                    'B': [400, 500, 600]})\n        >>> new_df = pd.DataFrame({'B': [4, 5, 6],\n        ...                        'C': [7, 8, 9]})\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  1  4\n        1  2  5\n        2  3  6\n\n        The DataFrame's length does not increase as a result of the update,\n        only values at matching index/column labels are updated.\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_df = pd.DataFrame({'B': ['d', 'e', 'f', 'g', 'h', 'i']})\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  a  d\n        1  b  e\n        2  c  f\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_df = pd.DataFrame({'B': ['d', 'f']}, index=[0, 2])\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  a  d\n        1  b  y\n        2  c  f\n\n        For Series, its name attribute must be set.\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_column = pd.Series(['d', 'e', 'f'], name='B')\n        >>> df.update(new_column)\n        >>> df\n           A  B\n        0  a  d\n        1  b  e\n        2  c  f\n\n        If `other` contains NaNs the corresponding values are not updated\n        in the original dataframe.\n\n        >>> df = pd.DataFrame({'A': [1, 2, 3],\n        ...                    'B': [400., 500., 600.]})\n        >>> new_df = pd.DataFrame({'B': [4, np.nan, 6]})\n        >>> df.update(new_df)\n        >>> df\n           A      B\n        0  1    4.0\n        1  2  500.0\n        2  3    6.0\n        \"\"\"\n        if not PYPY and using_copy_on_write():\n            if sys.getrefcount(self) <= REF_COUNT:\n                warnings.warn(_chained_assignment_method_msg, ChainedAssignmentError, stacklevel=2)\n        elif not PYPY and (not using_copy_on_write()) and self._is_view_after_cow_rules():\n            if sys.getrefcount(self) <= REF_COUNT:\n                warnings.warn(_chained_assignment_warning_method_msg, FutureWarning, stacklevel=2)\n        if join != 'left':\n            raise NotImplementedError('Only left join is supported')\n        if errors not in ['ignore', 'raise']:\n            raise ValueError(\"The parameter errors must be either 'ignore' or 'raise'\")\n        if not isinstance(other, DataFrame):\n            other = DataFrame(other)\n        other = other.reindex(self.index)\n        for col in self.columns.intersection(other.columns):\n            this = self[col]._values\n            that = other[col]._values\n            if filter_func is not None:\n                mask = ~filter_func(this) | isna(that)\n            else:\n                if errors == 'raise':\n                    mask_this = notna(that)\n                    mask_that = notna(this)\n                    if any(mask_this & mask_that):\n                        raise ValueError('Data overlaps.')\n                if overwrite:\n                    mask = isna(that)\n                else:\n                    mask = notna(this)\n            if mask.all():\n                continue\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore', message='Downcasting behavior', category=FutureWarning)\n                self.loc[:, col] = self[col].where(mask, that)\n    kurtosis = kurt\n    product = prod\n\n    def _replace_columnwise(self, mapping: dict[Hashable, tuple[Any, Any]], inplace: bool, regex):\n        \"\"\"\n        Dispatch to Series.replace column-wise.\n\n        Parameters\n        ----------\n        mapping : dict\n            of the form {col: (target, value)}\n        inplace : bool\n        regex : bool or same types as `to_replace` in DataFrame.replace\n\n        Returns\n        -------\n        DataFrame or None\n        \"\"\"\n        res = self if inplace else self.copy(deep=None)\n        ax = self.columns\n        for (i, ax_value) in enumerate(ax):\n            if ax_value in mapping:\n                ser = self.iloc[:, i]\n                (target, value) = mapping[ax_value]\n                newobj = ser.replace(target, value, regex=regex)\n                res._iset_item(i, newobj, inplace=inplace)\n        if inplace:\n            return\n        return res.__finalize__(self)\n\n    @final\n    @doc(klass=_shared_doc_kwargs['klass'], axes_single_arg=_shared_doc_kwargs['axes_single_arg'])\n    def ffill(self, *, axis: None | Axis=None, inplace: bool_t=False, limit: None | int=None, limit_area: Literal['inside', 'outside'] | None=None, downcast: dict | None | lib.NoDefault=lib.no_default) -> Self | None:\n        \"\"\"\n        Fill NA/NaN values by propagating the last valid observation to next valid.\n\n        Parameters\n        ----------\n        axis : {axes_single_arg}\n            Axis along which to fill missing values. For `Series`\n            this parameter is unused and defaults to 0.\n        inplace : bool, default False\n            If True, fill in-place. Note: this will modify any\n            other views on this object (e.g., a no-copy slice for a column in a\n            DataFrame).\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled. Must be greater than 0 if not None.\n        limit_area : {{`None`, 'inside', 'outside'}}, default None\n            If limit is specified, consecutive NaNs will be filled with this\n            restriction.\n\n            * ``None``: No fill restriction.\n            * 'inside': Only fill NaNs surrounded by valid values\n              (interpolate).\n            * 'outside': Only fill NaNs outside valid values (extrapolate).\n\n            .. versionadded:: 2.2.0\n\n        downcast : dict, default is None\n            A dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible).\n\n            .. deprecated:: 2.2.0\n\n        Returns\n        -------\n        {klass} or None\n            Object with missing values filled or None if ``inplace=True``.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n        ...                    [3, 4, np.nan, 1],\n        ...                    [np.nan, np.nan, np.nan, np.nan],\n        ...                    [np.nan, 3, np.nan, 4]],\n        ...                   columns=list(\"ABCD\"))\n        >>> df\n             A    B   C    D\n        0  NaN  2.0 NaN  0.0\n        1  3.0  4.0 NaN  1.0\n        2  NaN  NaN NaN  NaN\n        3  NaN  3.0 NaN  4.0\n\n        >>> df.ffill()\n             A    B   C    D\n        0  NaN  2.0 NaN  0.0\n        1  3.0  4.0 NaN  1.0\n        2  3.0  4.0 NaN  1.0\n        3  3.0  3.0 NaN  4.0\n\n        >>> ser = pd.Series([1, np.nan, 2, 3])\n        >>> ser.ffill()\n        0   1.0\n        1   1.0\n        2   2.0\n        3   3.0\n        dtype: float64\n        \"\"\"\n        downcast = self._deprecate_downcast(downcast, 'ffill')\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        if inplace:\n            if not PYPY and using_copy_on_write():\n                if sys.getrefcount(self) <= REF_COUNT:\n                    warnings.warn(_chained_assignment_method_msg, ChainedAssignmentError, stacklevel=2)\n            elif not PYPY and (not using_copy_on_write()) and self._is_view_after_cow_rules():\n                ctr = sys.getrefcount(self)\n                ref_count = REF_COUNT\n                if isinstance(self, ABCSeries) and _check_cacher(self):\n                    ref_count += 1\n                if ctr <= ref_count:\n                    warnings.warn(_chained_assignment_warning_method_msg, FutureWarning, stacklevel=2)\n        return self._pad_or_backfill('ffill', axis=axis, inplace=inplace, limit=limit, limit_area=limit_area, downcast=downcast)\n\n    def itertuples(self, index: bool=True, name: str | None='Pandas') -> Iterable[tuple[Any, ...]]:\n        \"\"\"\n        Iterate over DataFrame rows as namedtuples.\n\n        Parameters\n        ----------\n        index : bool, default True\n            If True, return the index as the first element of the tuple.\n        name : str or None, default \"Pandas\"\n            The name of the returned namedtuples or None to return regular\n            tuples.\n\n        Returns\n        -------\n        iterator\n            An object to iterate over namedtuples for each row in the\n            DataFrame with the first field possibly being the index and\n            following fields being the column values.\n\n        See Also\n        --------\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series)\n            pairs.\n        DataFrame.items : Iterate over (column name, Series) pairs.\n\n        Notes\n        -----\n        The column names will be renamed to positional names if they are\n        invalid Python identifiers, repeated, or start with an underscore.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]},\n        ...                   index=['dog', 'hawk'])\n        >>> df\n              num_legs  num_wings\n        dog          4          0\n        hawk         2          2\n        >>> for row in df.itertuples():\n        ...     print(row)\n        ...\n        Pandas(Index='dog', num_legs=4, num_wings=0)\n        Pandas(Index='hawk', num_legs=2, num_wings=2)\n\n        By setting the `index` parameter to False we can remove the index\n        as the first element of the tuple:\n\n        >>> for row in df.itertuples(index=False):\n        ...     print(row)\n        ...\n        Pandas(num_legs=4, num_wings=0)\n        Pandas(num_legs=2, num_wings=2)\n\n        With the `name` parameter set we set a custom name for the yielded\n        namedtuples:\n\n        >>> for row in df.itertuples(name='Animal'):\n        ...     print(row)\n        ...\n        Animal(Index='dog', num_legs=4, num_wings=0)\n        Animal(Index='hawk', num_legs=2, num_wings=2)\n        \"\"\"\n        arrays = []\n        fields = list(self.columns)\n        if index:\n            arrays.append(self.index)\n            fields.insert(0, 'Index')\n        arrays.extend((self.iloc[:, k] for k in range(len(self.columns))))\n        if name is not None:\n            itertuple = collections.namedtuple(name, fields, rename=True)\n            return map(itertuple._make, zip(*arrays))\n        return zip(*arrays)\n\n    def dot(self, other: AnyArrayLike | DataFrame) -> DataFrame | Series:\n        \"\"\"\n        Compute the matrix multiplication between the DataFrame and other.\n\n        This method computes the matrix product between the DataFrame and the\n        values of an other Series, DataFrame or a numpy array.\n\n        It can also be called using ``self @ other``.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the matrix product with.\n\n        Returns\n        -------\n        Series or DataFrame\n            If other is a Series, return the matrix product between self and\n            other as a Series. If other is a DataFrame or a numpy.array, return\n            the matrix product of self and other in a DataFrame of a np.array.\n\n        See Also\n        --------\n        Series.dot: Similar method for Series.\n\n        Notes\n        -----\n        The dimensions of DataFrame and other must be compatible in order to\n        compute the matrix multiplication. In addition, the column names of\n        DataFrame and the index of other must contain the same values, as they\n        will be aligned prior to the multiplication.\n\n        The dot method for Series computes the inner product, instead of the\n        matrix product here.\n\n        Examples\n        --------\n        Here we multiply a DataFrame with a Series.\n\n        >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\n        >>> s = pd.Series([1, 1, 2, 1])\n        >>> df.dot(s)\n        0    -4\n        1     5\n        dtype: int64\n\n        Here we multiply a DataFrame with another DataFrame.\n\n        >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(other)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note that the dot method give the same result as @\n\n        >>> df @ other\n            0   1\n        0   1   4\n        1   2   2\n\n        The dot method works also if other is an np.array.\n\n        >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(arr)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note how shuffling of the objects does not change the result.\n\n        >>> s2 = s.reindex([1, 0, 2, 3])\n        >>> df.dot(s2)\n        0    -4\n        1     5\n        dtype: int64\n        \"\"\"\n        if isinstance(other, (Series, DataFrame)):\n            common = self.columns.union(other.index)\n            if len(common) > len(self.columns) or len(common) > len(other.index):\n                raise ValueError('matrices are not aligned')\n            left = self.reindex(columns=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right._values\n        else:\n            left = self\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[1] != rvals.shape[0]:\n                raise ValueError(f'Dot product shape mismatch, {lvals.shape} vs {rvals.shape}')\n        if isinstance(other, DataFrame):\n            common_type = find_common_type(list(self.dtypes) + list(other.dtypes))\n            return self._constructor(np.dot(lvals, rvals), index=left.index, columns=other.columns, copy=False, dtype=common_type)\n        elif isinstance(other, Series):\n            common_type = find_common_type(list(self.dtypes) + [other.dtypes])\n            return self._constructor_sliced(np.dot(lvals, rvals), index=left.index, copy=False, dtype=common_type)\n        elif isinstance(rvals, (np.ndarray, Index)):\n            result = np.dot(lvals, rvals)\n            if result.ndim == 2:\n                return self._constructor(result, index=left.index, copy=False)\n            else:\n                return self._constructor_sliced(result, index=left.index, copy=False)\n        else:\n            raise TypeError(f'unsupported type: {type(other)}')\n\n    def reset_index(self, level: IndexLabel | None=None, *, drop: bool=False, inplace: bool=False, col_level: Hashable=0, col_fill: Hashable='', allow_duplicates: bool | lib.NoDefault=lib.no_default, names: Hashable | Sequence[Hashable] | None=None) -> DataFrame | None:\n        \"\"\"\n        Reset the index, or a level of it.\n\n        Reset the index of the DataFrame, and use the default one instead.\n        If the DataFrame has a MultiIndex, this method can remove one or more\n        levels.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes all levels by\n            default.\n        drop : bool, default False\n            Do not try to insert index into dataframe columns. This resets\n            the index to the default integer index.\n        inplace : bool, default False\n            Whether to modify the DataFrame rather than creating a new one.\n        col_level : int or str, default 0\n            If the columns have multiple levels, determines which level the\n            labels are inserted into. By default it is inserted into the first\n            level.\n        col_fill : object, default ''\n            If the columns have multiple levels, determines how the other\n            levels are named. If None then the index name is repeated.\n        allow_duplicates : bool, optional, default lib.no_default\n            Allow duplicate column labels to be created.\n\n            .. versionadded:: 1.5.0\n\n        names : int, str or 1-dimensional list, default None\n            Using the given string, rename the DataFrame column which contains the\n            index data. If the DataFrame has a MultiIndex, this has to be a list or\n            tuple with length equal to the number of levels.\n\n            .. versionadded:: 1.5.0\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with the new index or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.set_index : Opposite of reset_index.\n        DataFrame.reindex : Change to new indices or expand indices.\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('bird', 389.0),\n        ...                    ('bird', 24.0),\n        ...                    ('mammal', 80.5),\n        ...                    ('mammal', np.nan)],\n        ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n        ...                   columns=('class', 'max_speed'))\n        >>> df\n                 class  max_speed\n        falcon    bird      389.0\n        parrot    bird       24.0\n        lion    mammal       80.5\n        monkey  mammal        NaN\n\n        When we reset the index, the old index is added as a column, and a\n        new sequential index is used:\n\n        >>> df.reset_index()\n            index   class  max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        We can use the `drop` parameter to avoid the old index being added as\n        a column:\n\n        >>> df.reset_index(drop=True)\n            class  max_speed\n        0    bird      389.0\n        1    bird       24.0\n        2  mammal       80.5\n        3  mammal        NaN\n\n        You can also use `reset_index` with `MultiIndex`.\n\n        >>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),\n        ...                                    ('bird', 'parrot'),\n        ...                                    ('mammal', 'lion'),\n        ...                                    ('mammal', 'monkey')],\n        ...                                   names=['class', 'name'])\n        >>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),\n        ...                                      ('species', 'type')])\n        >>> df = pd.DataFrame([(389.0, 'fly'),\n        ...                    (24.0, 'fly'),\n        ...                    (80.5, 'run'),\n        ...                    (np.nan, 'jump')],\n        ...                   index=index,\n        ...                   columns=columns)\n        >>> df\n                       speed species\n                         max    type\n        class  name\n        bird   falcon  389.0     fly\n               parrot   24.0     fly\n        mammal lion     80.5     run\n               monkey    NaN    jump\n\n        Using the `names` parameter, choose a name for the index column:\n\n        >>> df.reset_index(names=['classes', 'names'])\n          classes   names  speed species\n                             max    type\n        0    bird  falcon  389.0     fly\n        1    bird  parrot   24.0     fly\n        2  mammal    lion   80.5     run\n        3  mammal  monkey    NaN    jump\n\n        If the index has multiple levels, we can reset a subset of them:\n\n        >>> df.reset_index(level='class')\n                 class  speed species\n                          max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        If we are not dropping the index, by default, it is placed in the top\n        level. We can place it in another level:\n\n        >>> df.reset_index(level='class', col_level=1)\n                        speed species\n                 class    max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        When the index is inserted under another level, we can specify under\n        which one with the parameter `col_fill`:\n\n        >>> df.reset_index(level='class', col_level=1, col_fill='species')\n                      species  speed species\n                        class    max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n\n        If we specify a nonexistent level for `col_fill`, it is created:\n\n        >>> df.reset_index(level='class', col_level=1, col_fill='genus')\n                        genus  speed species\n                        class    max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        if inplace:\n            new_obj = self\n        else:\n            new_obj = self.copy(deep=None)\n        if allow_duplicates is not lib.no_default:\n            allow_duplicates = validate_bool_kwarg(allow_duplicates, 'allow_duplicates')\n        new_index = default_index(len(new_obj))\n        if level is not None:\n            if not isinstance(level, (tuple, list)):\n                level = [level]\n            level = [self.index._get_level_number(lev) for lev in level]\n            if len(level) < self.index.nlevels:\n                new_index = self.index.droplevel(level)\n        if not drop:\n            to_insert: Iterable[tuple[Any, Any | None]]\n            default = 'index' if 'index' not in self else 'level_0'\n            names = self.index._get_default_index_names(names, default)\n            if isinstance(self.index, MultiIndex):\n                to_insert = zip(self.index.levels, self.index.codes)\n            else:\n                to_insert = ((self.index, None),)\n            multi_col = isinstance(self.columns, MultiIndex)\n            for (i, (lev, lab)) in reversed(list(enumerate(to_insert))):\n                if level is not None and i not in level:\n                    continue\n                name = names[i]\n                if multi_col:\n                    col_name = list(name) if isinstance(name, tuple) else [name]\n                    if col_fill is None:\n                        if len(col_name) not in (1, self.columns.nlevels):\n                            raise ValueError(f'col_fill=None is incompatible with incomplete column name {name}')\n                        col_fill = col_name[0]\n                    lev_num = self.columns._get_level_number(col_level)\n                    name_lst = [col_fill] * lev_num + col_name\n                    missing = self.columns.nlevels - len(name_lst)\n                    name_lst += [col_fill] * missing\n                    name = tuple(name_lst)\n                level_values = lev._values\n                if level_values.dtype == np.object_:\n                    level_values = lib.maybe_convert_objects(level_values)\n                if lab is not None:\n                    level_values = algorithms.take(level_values, lab, allow_fill=True, fill_value=lev._na_value)\n                new_obj.insert(0, name, level_values, allow_duplicates=allow_duplicates)\n        new_obj.index = new_index\n        if not inplace:\n            return new_obj\n        return None\n\n    @doc(make_doc('mean', ndim=2))\n    def mean(self, axis: Axis | None=0, skipna: bool=True, numeric_only: bool=False, **kwargs):\n        result = super().mean(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method='mean')\n        return result\n\n    def iterrows(self) -> Iterable[tuple[Hashable, Series]]:\n        \"\"\"\n        Iterate over DataFrame rows as (index, Series) pairs.\n\n        Yields\n        ------\n        index : label or tuple of label\n            The index of the row. A tuple for a `MultiIndex`.\n        data : Series\n            The data of the row as a Series.\n\n        See Also\n        --------\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.\n        DataFrame.items : Iterate over (column name, Series) pairs.\n\n        Notes\n        -----\n        1. Because ``iterrows`` returns a Series for each row,\n           it does **not** preserve dtypes across the rows (dtypes are\n           preserved across columns for DataFrames).\n\n           To preserve dtypes while iterating over the rows, it is better\n           to use :meth:`itertuples` which returns namedtuples of the values\n           and which is generally faster than ``iterrows``.\n\n        2. You should **never modify** something you are iterating over.\n           This is not guaranteed to work in all cases. Depending on the\n           data types, the iterator returns a copy and not a view, and writing\n           to it will have no effect.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])\n        >>> row = next(df.iterrows())[1]\n        >>> row\n        int      1.0\n        float    1.5\n        Name: 0, dtype: float64\n        >>> print(row['int'].dtype)\n        float64\n        >>> print(df['int'].dtype)\n        int64\n        \"\"\"\n        columns = self.columns\n        klass = self._constructor_sliced\n        using_cow = using_copy_on_write()\n        for (k, v) in zip(self.index, self.values):\n            s = klass(v, index=columns, name=k).__finalize__(self)\n            if using_cow and self._mgr.is_single_block:\n                s._mgr.add_references(self._mgr)\n            yield (k, s)\n\n    @doc(NDFrame.reindex, klass=_shared_doc_kwargs['klass'], optional_reindex=_shared_doc_kwargs['optional_reindex'])\n    def reindex(self, labels=None, *, index=None, columns=None, axis: Axis | None=None, method: ReindexMethod | None=None, copy: bool | None=None, level: Level | None=None, fill_value: Scalar | None=np.nan, limit: int | None=None, tolerance=None) -> DataFrame:\n        return super().reindex(labels=labels, index=index, columns=columns, axis=axis, method=method, copy=copy, level=level, fill_value=fill_value, limit=limit, tolerance=tolerance)\n\n    def _flex_cmp_method(self, other, op, *, axis: Axis='columns', level=None):\n        axis = self._get_axis_number(axis) if axis is not None else 1\n        (self, other) = self._align_for_op(other, axis, flex=True, level=level)\n        new_data = self._dispatch_frame_op(other, op, axis=axis)\n        return self._construct_result(new_data)\n\n    @overload\n    def quantile(self, q: float=..., axis: Axis=..., numeric_only: bool=..., interpolation: QuantileInterpolation=..., method: Literal['single', 'table']=...) -> Series:\n        ...\n\n    @overload\n    def quantile(self, q: AnyArrayLike | Sequence[float], axis: Axis=..., numeric_only: bool=..., interpolation: QuantileInterpolation=..., method: Literal['single', 'table']=...) -> Series | DataFrame:\n        ...\n\n    @overload\n    def quantile(self, q: float | AnyArrayLike | Sequence[float]=..., axis: Axis=..., numeric_only: bool=..., interpolation: QuantileInterpolation=..., method: Literal['single', 'table']=...) -> Series | DataFrame:\n        ...\n\n    @Appender(ops.make_flex_doc('rsub', 'dataframe'))\n    def rsub(self, other, axis: Axis='columns', level=None, fill_value=None) -> DataFrame:\n        return self._flex_arith_method(other, roperator.rsub, level=level, fill_value=fill_value, axis=axis)\n\n    def cov(self, min_periods: int | None=None, ddof: int | None=1, numeric_only: bool=False) -> DataFrame:\n        \"\"\"\n        Compute pairwise covariance of columns, excluding NA/null values.\n\n        Compute the pairwise covariance among the series of a DataFrame.\n        The returned data frame is the `covariance matrix\n        <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns\n        of the DataFrame.\n\n        Both NA and null values are automatically excluded from the\n        calculation. (See the note below about bias from missing values.)\n        A threshold can be set for the minimum number of\n        observations for each value created. Comparisons with observations\n        below this threshold will be returned as ``NaN``.\n\n        This method is generally used for the analysis of time series data to\n        understand the relationship between different measures\n        across time.\n\n        Parameters\n        ----------\n        min_periods : int, optional\n            Minimum number of observations required per pair of columns\n            to have a valid result.\n\n        ddof : int, default 1\n            Delta degrees of freedom.  The divisor used in calculations\n            is ``N - ddof``, where ``N`` represents the number of elements.\n            This argument is applicable only when no ``nan`` is in the dataframe.\n\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n                The default value of ``numeric_only`` is now ``False``.\n\n        Returns\n        -------\n        DataFrame\n            The covariance matrix of the series of the DataFrame.\n\n        See Also\n        --------\n        Series.cov : Compute covariance with another Series.\n        core.window.ewm.ExponentialMovingWindow.cov : Exponential weighted sample\n            covariance.\n        core.window.expanding.Expanding.cov : Expanding sample covariance.\n        core.window.rolling.Rolling.cov : Rolling sample covariance.\n\n        Notes\n        -----\n        Returns the covariance matrix of the DataFrame's time series.\n        The covariance is normalized by N-ddof.\n\n        For DataFrames that have Series that are missing data (assuming that\n        data is `missing at random\n        <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__)\n        the returned covariance matrix will be an unbiased estimate\n        of the variance and covariance between the member Series.\n\n        However, for many applications this estimate may not be acceptable\n        because the estimate covariance matrix is not guaranteed to be positive\n        semi-definite. This could lead to estimate correlations having\n        absolute values which are greater than one, and/or a non-invertible\n        covariance matrix. See `Estimation of covariance matrices\n        <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_\n        matrices>`__ for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.cov()\n                  dogs      cats\n        dogs  0.666667 -1.000000\n        cats -1.000000  1.666667\n\n        >>> np.random.seed(42)\n        >>> df = pd.DataFrame(np.random.randn(1000, 5),\n        ...                   columns=['a', 'b', 'c', 'd', 'e'])\n        >>> df.cov()\n                  a         b         c         d         e\n        a  0.998438 -0.020161  0.059277 -0.008943  0.014144\n        b -0.020161  1.059352 -0.008543 -0.024738  0.009826\n        c  0.059277 -0.008543  1.010670 -0.001486 -0.000271\n        d -0.008943 -0.024738 -0.001486  0.921297 -0.013692\n        e  0.014144  0.009826 -0.000271 -0.013692  0.977795\n\n        **Minimum number of periods**\n\n        This method also supports an optional ``min_periods`` keyword\n        that specifies the required minimum number of non-NA observations for\n        each column pair in order to have a valid result:\n\n        >>> np.random.seed(42)\n        >>> df = pd.DataFrame(np.random.randn(20, 3),\n        ...                   columns=['a', 'b', 'c'])\n        >>> df.loc[df.index[:5], 'a'] = np.nan\n        >>> df.loc[df.index[5:10], 'b'] = np.nan\n        >>> df.cov(min_periods=12)\n                  a         b         c\n        a  0.316741       NaN -0.150812\n        b       NaN  1.248003  0.191417\n        c -0.150812  0.191417  0.895202\n        \"\"\"\n        data = self._get_numeric_data() if numeric_only else self\n        cols = data.columns\n        idx = cols.copy()\n        mat = data.to_numpy(dtype=float, na_value=np.nan, copy=False)\n        if notna(mat).all():\n            if min_periods is not None and min_periods > len(mat):\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n            else:\n                base_cov = np.cov(mat.T, ddof=ddof)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n        else:\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n        result = self._constructor(base_cov, index=idx, columns=cols, copy=False)\n        return result.__finalize__(self, method='cov')\n\n    def __setitem__(self, key, value) -> None:\n        if not PYPY and using_copy_on_write():\n            if sys.getrefcount(self) <= 3:\n                warnings.warn(_chained_assignment_msg, ChainedAssignmentError, stacklevel=2)\n        elif not PYPY and (not using_copy_on_write()):\n            if sys.getrefcount(self) <= 3 and (warn_copy_on_write() or (not warn_copy_on_write() and any((b.refs.has_reference() for b in self._mgr.blocks)))):\n                warnings.warn(_chained_assignment_warning_msg, FutureWarning, stacklevel=2)\n        key = com.apply_if_callable(key, self)\n        if isinstance(key, slice):\n            slc = self.index._convert_slice_indexer(key, kind='getitem')\n            return self._setitem_slice(slc, value)\n        if isinstance(key, DataFrame) or getattr(key, 'ndim', None) == 2:\n            self._setitem_frame(key, value)\n        elif isinstance(key, (Series, np.ndarray, list, Index)):\n            self._setitem_array(key, value)\n        elif isinstance(value, DataFrame):\n            self._set_item_frame_value(key, value)\n        elif is_list_like(value) and (not self.columns.is_unique) and (1 < len(self.columns.get_indexer_for([key])) == len(value)):\n            self._setitem_array([key], value)\n        else:\n            self._set_item(key, value)\n\n    @doc(make_doc('max', ndim=2))\n    def max(self, axis: Axis | None=0, skipna: bool=True, numeric_only: bool=False, **kwargs):\n        result = super().max(axis, skipna, numeric_only, **kwargs)\n        if isinstance(result, Series):\n            result = result.__finalize__(self, method='max')\n        return result\n    _AXIS_ORDERS: list[Literal['index', 'columns']] = ['index', 'columns']\n    _AXIS_TO_AXIS_NUMBER: dict[Axis, int] = {**NDFrame._AXIS_TO_AXIS_NUMBER, 1: 1, 'columns': 1}\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number: Literal[1] = 1\n    _info_axis_name: Literal['columns'] = 'columns'\n    index = properties.AxisProperty(axis=1, doc=\"\\n        The index (row labels) of the DataFrame.\\n\\n        The index of a DataFrame is a series of labels that identify each row.\\n        The labels can be integers, strings, or any other hashable type. The index\\n        is used for label-based access and alignment, and can be accessed or\\n        modified using this attribute.\\n\\n        Returns\\n        -------\\n        pandas.Index\\n            The index labels of the DataFrame.\\n\\n        See Also\\n        --------\\n        DataFrame.columns : The column labels of the DataFrame.\\n        DataFrame.to_numpy : Convert the DataFrame to a NumPy array.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({'Name': ['Alice', 'Bob', 'Aritra'],\\n        ...                    'Age': [25, 30, 35],\\n        ...                    'Location': ['Seattle', 'New York', 'Kona']},\\n        ...                   index=([10, 20, 30]))\\n        >>> df.index\\n        Index([10, 20, 30], dtype='int64')\\n\\n        In this example, we create a DataFrame with 3 rows and 3 columns,\\n        including Name, Age, and Location information. We set the index labels to\\n        be the integers 10, 20, and 30. We then access the `index` attribute of the\\n        DataFrame, which returns an `Index` object containing the index labels.\\n\\n        >>> df.index = [100, 200, 300]\\n        >>> df\\n            Name  Age Location\\n        100  Alice   25  Seattle\\n        200    Bob   30 New York\\n        300  Aritra  35    Kona\\n\\n        In this example, we modify the index labels of the DataFrame by assigning\\n        a new list of labels to the `index` attribute. The DataFrame is then\\n        updated with the new labels, and the output shows the modified DataFrame.\\n        \")\n    columns = properties.AxisProperty(axis=0, doc=dedent(\"\\n                The column labels of the DataFrame.\\n\\n                Examples\\n                --------\\n                >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\\n                >>> df\\n                     A  B\\n                0    1  3\\n                1    2  4\\n                >>> df.columns\\n                Index(['A', 'B'], dtype='object')\\n                \"))\n    plot = CachedAccessor('plot', pandas.plotting.PlotAccessor)\n    hist = pandas.plotting.hist_frame\n    boxplot = pandas.plotting.boxplot_frame\n    sparse = CachedAccessor('sparse', SparseFrameAccessor)\n\n    def _set_item(self, key, value) -> None:\n        \"\"\"\n        Add series to DataFrame in specified column.\n\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\n        same length as the DataFrames index or an error will be thrown.\n\n        Series/TimeSeries will be conformed to the DataFrames index to\n        ensure homogeneity.\n        \"\"\"\n        (value, refs) = self._sanitize_column(value)\n        if key in self.columns and value.ndim == 1 and (not isinstance(value.dtype, ExtensionDtype)):\n            if not self.columns.is_unique or isinstance(self.columns, MultiIndex):\n                existing_piece = self[key]\n                if isinstance(existing_piece, DataFrame):\n                    value = np.tile(value, (len(existing_piece.columns), 1)).T\n                    refs = None\n        self._set_item_mgr(key, value, refs)\n\n    @property\n    def values(self) -> np.ndarray:\n        \"\"\"\n        Return a Numpy representation of the DataFrame.\n\n        .. warning::\n\n           We recommend using :meth:`DataFrame.to_numpy` instead.\n\n        Only the values in the DataFrame will be returned, the axes labels\n        will be removed.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values of the DataFrame.\n\n        See Also\n        --------\n        DataFrame.to_numpy : Recommended alternative to this method.\n        DataFrame.index : Retrieve the index labels.\n        DataFrame.columns : Retrieving the column names.\n\n        Notes\n        -----\n        The dtype will be a lower-common-denominator dtype (implicit\n        upcasting); that is to say if the dtypes (even of numeric types)\n        are mixed, the one that accommodates all will be chosen. Use this\n        with care if you are not dealing with the blocks.\n\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\n        float32.  If dtypes are int32 and uint8, dtype will be upcast to\n        int32. By :func:`numpy.find_common_type` convention, mixing int64\n        and uint64 will result in a float64 dtype.\n\n        Examples\n        --------\n        A DataFrame where all columns are the same type (e.g., int64) results\n        in an array of the same type.\n\n        >>> df = pd.DataFrame({'age':    [ 3,  29],\n        ...                    'height': [94, 170],\n        ...                    'weight': [31, 115]})\n        >>> df\n           age  height  weight\n        0    3      94      31\n        1   29     170     115\n        >>> df.dtypes\n        age       int64\n        height    int64\n        weight    int64\n        dtype: object\n        >>> df.values\n        array([[  3,  94,  31],\n               [ 29, 170, 115]])\n\n        A DataFrame with mixed type columns(e.g., str/object, int64, float32)\n        results in an ndarray of the broadest type that accommodates these\n        mixed types (e.g., object).\n\n        >>> df2 = pd.DataFrame([('parrot',   24.0, 'second'),\n        ...                     ('lion',     80.5, 1),\n        ...                     ('monkey', np.nan, None)],\n        ...                   columns=('name', 'max_speed', 'rank'))\n        >>> df2.dtypes\n        name          object\n        max_speed    float64\n        rank          object\n        dtype: object\n        >>> df2.values\n        array([['parrot', 24.0, 'second'],\n               ['lion', 80.5, 1],\n               ['monkey', nan, None]], dtype=object)\n        \"\"\"\n        return self._mgr.as_array()\n\n    def eval(self, expr: str, *, inplace: bool=False, **kwargs) -> Any | None:\n        \"\"\"\n        Evaluate a string describing operations on DataFrame columns.\n\n        Operates on columns only, not specific rows or elements.  This allows\n        `eval` to run arbitrary code, which can make you vulnerable to code\n        injection if you pass user input to this function.\n\n        Parameters\n        ----------\n        expr : str\n            The expression string to evaluate.\n        inplace : bool, default False\n            If the expression contains an assignment, whether to perform the\n            operation inplace and mutate the existing DataFrame. Otherwise,\n            a new DataFrame is returned.\n        **kwargs\n            See the documentation for :func:`eval` for complete details\n            on the keyword arguments accepted by\n            :meth:`~pandas.DataFrame.query`.\n\n        Returns\n        -------\n        ndarray, scalar, pandas object, or None\n            The result of the evaluation or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.query : Evaluates a boolean expression to query the columns\n            of a frame.\n        DataFrame.assign : Can evaluate an expression or function to create new\n            values for a column.\n        eval : Evaluate a Python expression as a string using various\n            backends.\n\n        Notes\n        -----\n        For more details see the API documentation for :func:`~eval`.\n        For detailed examples see :ref:`enhancing performance with eval\n        <enhancingperf.eval>`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)})\n        >>> df\n           A   B\n        0  1  10\n        1  2   8\n        2  3   6\n        3  4   4\n        4  5   2\n        >>> df.eval('A + B')\n        0    11\n        1    10\n        2     9\n        3     8\n        4     7\n        dtype: int64\n\n        Assignment is allowed though by default the original DataFrame is not\n        modified.\n\n        >>> df.eval('C = A + B')\n           A   B   C\n        0  1  10  11\n        1  2   8  10\n        2  3   6   9\n        3  4   4   8\n        4  5   2   7\n        >>> df\n           A   B\n        0  1  10\n        1  2   8\n        2  3   6\n        3  4   4\n        4  5   2\n\n        Multiple columns can be assigned to using multi-line expressions:\n\n        >>> df.eval(\n        ...     '''\n        ... C = A + B\n        ... D = A - B\n        ... '''\n        ... )\n           A   B   C  D\n        0  1  10  11 -9\n        1  2   8  10 -6\n        2  3   6   9 -3\n        3  4   4   8  0\n        4  5   2   7  3\n        \"\"\"\n        from pandas.core.computation.eval import eval as _eval\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        kwargs['level'] = kwargs.pop('level', 0) + 1\n        index_resolvers = self._get_index_resolvers()\n        column_resolvers = self._get_cleaned_column_resolvers()\n        resolvers = (column_resolvers, index_resolvers)\n        if 'target' not in kwargs:\n            kwargs['target'] = self\n        kwargs['resolvers'] = tuple(kwargs.get('resolvers', ())) + resolvers\n        return _eval(expr, inplace=inplace, **kwargs)\n\n    def select_dtypes(self, include=None, exclude=None) -> Self:\n        \"\"\"\n        Return a subset of the DataFrame's columns based on the column dtypes.\n\n        Parameters\n        ----------\n        include, exclude : scalar or list-like\n            A selection of dtypes or strings to be included/excluded. At least\n            one of these parameters must be supplied.\n\n        Returns\n        -------\n        DataFrame\n            The subset of the frame including the dtypes in ``include`` and\n            excluding the dtypes in ``exclude``.\n\n        Raises\n        ------\n        ValueError\n            * If both of ``include`` and ``exclude`` are empty\n            * If ``include`` and ``exclude`` have overlapping elements\n            * If any kind of string dtype is passed in.\n\n        See Also\n        --------\n        DataFrame.dtypes: Return Series with the data type of each column.\n\n        Notes\n        -----\n        * To select all *numeric* types, use ``np.number`` or ``'number'``\n        * To select strings you must use the ``object`` dtype, but note that\n          this will return *all* object dtype columns\n        * See the `numpy dtype hierarchy\n          <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__\n        * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n          ``'datetime64'``\n        * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n          ``'timedelta64'``\n        * To select Pandas categorical dtypes, use ``'category'``\n        * To select Pandas datetimetz dtypes, use ``'datetimetz'``\n          or ``'datetime64[ns, tz]'``\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'a': [1, 2] * 3,\n        ...                    'b': [True, False] * 3,\n        ...                    'c': [1.0, 2.0] * 3})\n        >>> df\n                a      b  c\n        0       1   True  1.0\n        1       2  False  2.0\n        2       1   True  1.0\n        3       2  False  2.0\n        4       1   True  1.0\n        5       2  False  2.0\n\n        >>> df.select_dtypes(include='bool')\n           b\n        0  True\n        1  False\n        2  True\n        3  False\n        4  True\n        5  False\n\n        >>> df.select_dtypes(include=['float64'])\n           c\n        0  1.0\n        1  2.0\n        2  1.0\n        3  2.0\n        4  1.0\n        5  2.0\n\n        >>> df.select_dtypes(exclude=['int64'])\n               b    c\n        0   True  1.0\n        1  False  2.0\n        2   True  1.0\n        3  False  2.0\n        4   True  1.0\n        5  False  2.0\n        \"\"\"\n        if not is_list_like(include):\n            include = (include,) if include is not None else ()\n        if not is_list_like(exclude):\n            exclude = (exclude,) if exclude is not None else ()\n        selection = (frozenset(include), frozenset(exclude))\n        if not any(selection):\n            raise ValueError('at least one of include or exclude must be nonempty')\n\n        def check_int_infer_dtype(dtypes):\n            converted_dtypes: list[type] = []\n            for dtype in dtypes:\n                if isinstance(dtype, str) and dtype == 'int' or dtype is int:\n                    converted_dtypes.append(np.int32)\n                    converted_dtypes.append(np.int64)\n                elif dtype == 'float' or dtype is float:\n                    converted_dtypes.extend([np.float64, np.float32])\n                else:\n                    converted_dtypes.append(infer_dtype_from_object(dtype))\n            return frozenset(converted_dtypes)\n        include = check_int_infer_dtype(include)\n        exclude = check_int_infer_dtype(exclude)\n        for dtypes in (include, exclude):\n            invalidate_string_dtypes(dtypes)\n        if not include.isdisjoint(exclude):\n            raise ValueError(f'include and exclude overlap on {include & exclude}')\n\n        def dtype_predicate(dtype: DtypeObj, dtypes_set) -> bool:\n            dtype = dtype if not isinstance(dtype, ArrowDtype) else dtype.numpy_dtype\n            return issubclass(dtype.type, tuple(dtypes_set)) or (np.number in dtypes_set and getattr(dtype, '_is_numeric', False) and (not is_bool_dtype(dtype)))\n\n        def predicate(arr: ArrayLike) -> bool:\n            dtype = arr.dtype\n            if include:\n                if not dtype_predicate(dtype, include):\n                    return False\n            if exclude:\n                if dtype_predicate(dtype, exclude):\n                    return False\n            return True\n        mgr = self._mgr._get_data_subset(predicate).copy(deep=None)\n        return self._constructor_from_mgr(mgr, axes=mgr.axes).__finalize__(self)\n\n    @doc(_shared_docs['idxmin'], numeric_only_default='False')\n    def idxmin(self, axis: Axis=0, skipna: bool=True, numeric_only: bool=False) -> Series:\n        axis = self._get_axis_number(axis)\n        if self.empty and len(self.axes[axis]):\n            axis_dtype = self.axes[axis].dtype\n            return self._constructor_sliced(dtype=axis_dtype)\n        if numeric_only:\n            data = self._get_numeric_data()\n        else:\n            data = self\n        res = data._reduce(nanops.nanargmin, 'argmin', axis=axis, skipna=skipna, numeric_only=False)\n        indices = res._values\n        if (indices == -1).any():\n            warnings.warn(f'The behavior of {type(self).__name__}.idxmin with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError', FutureWarning, stacklevel=find_stack_level())\n        index = data._get_axis(axis)\n        result = algorithms.take(index._values, indices, allow_fill=True, fill_value=index._na_value)\n        final_result = data._constructor_sliced(result, index=data._get_agg_axis(axis))\n        return final_result.__finalize__(self, method='idxmin')", "class_fn": true, "question_id": "pandas/pandas.core.frame/DataFrame", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/generic.py", "fn_id": "", "content": "class DataFrameGroupBy(GroupBy[DataFrame]):\n    _agg_examples_doc = dedent('\\n    Examples\\n    --------\\n    >>> data = {\"A\": [1, 1, 2, 2],\\n    ...         \"B\": [1, 2, 3, 4],\\n    ...         \"C\": [0.362838, 0.227877, 1.267767, -0.562860]}\\n    >>> df = pd.DataFrame(data)\\n    >>> df\\n       A  B         C\\n    0  1  1  0.362838\\n    1  1  2  0.227877\\n    2  2  3  1.267767\\n    3  2  4 -0.562860\\n\\n    The aggregation is for each column.\\n\\n    >>> df.groupby(\\'A\\').agg(\\'min\\')\\n       B         C\\n    A\\n    1  1  0.227877\\n    2  3 -0.562860\\n\\n    Multiple aggregations\\n\\n    >>> df.groupby(\\'A\\').agg([\\'min\\', \\'max\\'])\\n        B             C\\n      min max       min       max\\n    A\\n    1   1   2  0.227877  0.362838\\n    2   3   4 -0.562860  1.267767\\n\\n    Select a column for aggregation\\n\\n    >>> df.groupby(\\'A\\').B.agg([\\'min\\', \\'max\\'])\\n       min  max\\n    A\\n    1    1    2\\n    2    3    4\\n\\n    User-defined function for aggregation\\n\\n    >>> df.groupby(\\'A\\').agg(lambda x: sum(x) + 2)\\n        B\\t       C\\n    A\\n    1\\t5\\t2.590715\\n    2\\t9\\t2.704907\\n\\n    Different aggregations per column\\n\\n    >>> df.groupby(\\'A\\').agg({\\'B\\': [\\'min\\', \\'max\\'], \\'C\\': \\'sum\\'})\\n        B             C\\n      min max       sum\\n    A\\n    1   1   2  0.590715\\n    2   3   4  0.704907\\n\\n    To control the output names with different aggregations per column,\\n    pandas supports \"named aggregation\"\\n\\n    >>> df.groupby(\"A\").agg(\\n    ...     b_min=pd.NamedAgg(column=\"B\", aggfunc=\"min\"),\\n    ...     c_sum=pd.NamedAgg(column=\"C\", aggfunc=\"sum\")\\n    ... )\\n       b_min     c_sum\\n    A\\n    1      1  0.590715\\n    2      3  0.704907\\n\\n    - The keywords are the *output* column names\\n    - The values are tuples whose first element is the column to select\\n      and the second element is the aggregation to apply to that column.\\n      Pandas provides the ``pandas.NamedAgg`` namedtuple with the fields\\n      ``[\\'column\\', \\'aggfunc\\']`` to make it clearer what the arguments are.\\n      As usual, the aggregation can be a callable or a string alias.\\n\\n    See :ref:`groupby.aggregate.named` for more.\\n\\n    .. versionchanged:: 1.3.0\\n\\n        The resulting dtype will reflect the return value of the aggregating function.\\n\\n    >>> df.groupby(\"A\")[[\"B\"]].agg(lambda x: x.astype(float).min())\\n          B\\n    A\\n    1   1.0\\n    2   3.0\\n    ')\n\n    def _apply_to_column_groupbys(self, func) -> DataFrame:\n        from pandas.core.reshape.concat import concat\n        obj = self._obj_with_exclusions\n        columns = obj.columns\n        sgbs = [SeriesGroupBy(obj.iloc[:, i], selection=colname, grouper=self._grouper, exclusions=self.exclusions, observed=self.observed) for (i, colname) in enumerate(obj.columns)]\n        results = [func(sgb) for sgb in sgbs]\n        if not len(results):\n            res_df = DataFrame([], columns=columns, index=self._grouper.result_index)\n        else:\n            res_df = concat(results, keys=columns, axis=1)\n        if not self.as_index:\n            res_df.index = default_index(len(res_df))\n            res_df = self._insert_inaxis_grouper(res_df)\n        return res_df\n    agg = aggregate\n\n    def _wrap_applied_output_series(self, values: list[Series], not_indexed_same: bool, first_not_none, key_index: Index | None, is_transform: bool) -> DataFrame | Series:\n        kwargs = first_not_none._construct_axes_dict()\n        backup = Series(**kwargs)\n        values = [x if x is not None else backup for x in values]\n        all_indexed_same = all_indexes_same((x.index for x in values))\n        if not all_indexed_same:\n            return self._concat_objects(values, not_indexed_same=True, is_transform=is_transform)\n        stacked_values = np.vstack([np.asarray(v) for v in values])\n        if self.axis == 0:\n            index = key_index\n            columns = first_not_none.index.copy()\n            if columns.name is None:\n                names = {v.name for v in values}\n                if len(names) == 1:\n                    columns.name = next(iter(names))\n        else:\n            index = first_not_none.index\n            columns = key_index\n            stacked_values = stacked_values.T\n        if stacked_values.dtype == object:\n            stacked_values = stacked_values.tolist()\n        result = self.obj._constructor(stacked_values, index=index, columns=columns)\n        if not self.as_index:\n            result = self._insert_inaxis_grouper(result)\n        return self._reindex_output(result)\n\n    def idxmax(self, axis: Axis | None | lib.NoDefault=lib.no_default, skipna: bool=True, numeric_only: bool=False) -> DataFrame:\n        \"\"\"\n        Return index of first occurrence of maximum over requested axis.\n\n        NA/null values are excluded.\n\n        Parameters\n        ----------\n        axis : {{0 or 'index', 1 or 'columns'}}, default None\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n            If axis is not provided, grouper's axis is used.\n\n            .. versionchanged:: 2.0.0\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n        Returns\n        -------\n        Series\n            Indexes of maxima along the specified axis.\n\n        Raises\n        ------\n        ValueError\n            * If the row/column is empty\n\n        See Also\n        --------\n        Series.idxmax : Return index of the maximum element.\n\n        Notes\n        -----\n        This method is the DataFrame version of ``ndarray.argmax``.\n\n        Examples\n        --------\n        Consider a dataset containing food consumption in Argentina.\n\n        >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n        ...                    'co2_emissions': [37.2, 19.66, 1712]},\n        ...                   index=['Pork', 'Wheat Products', 'Beef'])\n\n        >>> df\n                        consumption  co2_emissions\n        Pork                  10.51         37.20\n        Wheat Products       103.11         19.66\n        Beef                  55.48       1712.00\n\n        By default, it returns the index for the maximum value in each column.\n\n        >>> df.idxmax()\n        consumption     Wheat Products\n        co2_emissions             Beef\n        dtype: object\n\n        To return the index for the maximum value in each row, use ``axis=\"columns\"``.\n\n        >>> df.idxmax(axis=\"columns\")\n        Pork              co2_emissions\n        Wheat Products     consumption\n        Beef              co2_emissions\n        dtype: object\n        \"\"\"\n        return self._idxmax_idxmin('idxmax', axis=axis, numeric_only=numeric_only, skipna=skipna)\n\n    @final\n    def _python_apply_general(self, f: Callable, data: DataFrame | Series, not_indexed_same: bool | None=None, is_transform: bool=False, is_agg: bool=False) -> NDFrameT:\n        \"\"\"\n        Apply function f in python space\n\n        Parameters\n        ----------\n        f : callable\n            Function to apply\n        data : Series or DataFrame\n            Data to apply f to\n        not_indexed_same: bool, optional\n            When specified, overrides the value of not_indexed_same. Apply behaves\n            differently when the result index is equal to the input index, but\n            this can be coincidental leading to value-dependent behavior.\n        is_transform : bool, default False\n            Indicator for whether the function is actually a transform\n            and should not have group keys prepended.\n        is_agg : bool, default False\n            Indicator for whether the function is an aggregation. When the\n            result is empty, we don't want to warn for this case.\n            See _GroupBy._python_agg_general.\n\n        Returns\n        -------\n        Series or DataFrame\n            data after applying f\n        \"\"\"\n        (values, mutated) = self._grouper.apply_groupwise(f, data, self.axis)\n        if not_indexed_same is None:\n            not_indexed_same = mutated\n        return self._wrap_applied_output(data, values, not_indexed_same, is_transform)\n\n    def filter(self, func, dropna: bool=True, *args, **kwargs):\n        \"\"\"\n        Filter elements from groups that don't satisfy a criterion.\n\n        Elements from groups are filtered if they do not satisfy the\n        boolean criterion specified by func.\n\n        Parameters\n        ----------\n        func : function\n            Criterion to apply to each group. Should return True or False.\n        dropna : bool\n            Drop groups that do not pass the filter. True by default; if False,\n            groups that evaluate False are filled with NaNs.\n\n        Returns\n        -------\n        DataFrame\n\n        Notes\n        -----\n        Each subframe is endowed the attribute 'name' in case you need to know\n        which group you are working on.\n\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n        for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n        ...                           'foo', 'bar'],\n        ...                    'B' : [1, 2, 3, 4, 5, 6],\n        ...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n        >>> grouped = df.groupby('A')\n        >>> grouped.filter(lambda x: x['B'].mean() > 3.)\n             A  B    C\n        1  bar  2  5.0\n        3  bar  4  1.0\n        5  bar  6  9.0\n        \"\"\"\n        indices = []\n        obj = self._selected_obj\n        gen = self._grouper.get_iterator(obj, axis=self.axis)\n        for (name, group) in gen:\n            object.__setattr__(group, 'name', name)\n            res = func(group, *args, **kwargs)\n            try:\n                res = res.squeeze()\n            except AttributeError:\n                pass\n            if is_bool(res) or (is_scalar(res) and isna(res)):\n                if notna(res) and res:\n                    indices.append(self._get_index(name))\n            else:\n                raise TypeError(f'filter function returned a {type(res).__name__}, but expected a scalar bool')\n        return self._apply_filter(indices, dropna)\n\n    def _wrap_agged_manager(self, mgr: Manager2D) -> DataFrame:\n        return self.obj._constructor_from_mgr(mgr, axes=mgr.axes)\n\n    @doc(DataFrame.cov.__doc__)\n    def cov(self, min_periods: int | None=None, ddof: int | None=1, numeric_only: bool=False) -> DataFrame:\n        result = self._op_via_apply('cov', min_periods=min_periods, ddof=ddof, numeric_only=numeric_only)\n        return result\n    __examples_dataframe_doc = dedent('\\n    >>> df = pd.DataFrame({\\'A\\' : [\\'foo\\', \\'bar\\', \\'foo\\', \\'bar\\',\\n    ...                           \\'foo\\', \\'bar\\'],\\n    ...                    \\'B\\' : [\\'one\\', \\'one\\', \\'two\\', \\'three\\',\\n    ...                           \\'two\\', \\'two\\'],\\n    ...                    \\'C\\' : [1, 5, 5, 2, 5, 5],\\n    ...                    \\'D\\' : [2.0, 5., 8., 1., 2., 9.]})\\n    >>> grouped = df.groupby(\\'A\\')[[\\'C\\', \\'D\\']]\\n    >>> grouped.transform(lambda x: (x - x.mean()) / x.std())\\n            C         D\\n    0 -1.154701 -0.577350\\n    1  0.577350  0.000000\\n    2  0.577350  1.154701\\n    3 -1.154701 -1.000000\\n    4  0.577350 -0.577350\\n    5  0.577350  1.000000\\n\\n    Broadcast result of the transformation\\n\\n    >>> grouped.transform(lambda x: x.max() - x.min())\\n        C    D\\n    0  4.0  6.0\\n    1  3.0  8.0\\n    2  4.0  6.0\\n    3  3.0  8.0\\n    4  4.0  6.0\\n    5  3.0  8.0\\n\\n    >>> grouped.transform(\"mean\")\\n        C    D\\n    0  3.666667  4.0\\n    1  4.000000  5.0\\n    2  3.666667  4.0\\n    3  4.000000  5.0\\n    4  3.666667  4.0\\n    5  4.000000  5.0\\n\\n    .. versionchanged:: 1.3.0\\n\\n    The resulting dtype will reflect the return value of the passed ``func``,\\n    for example:\\n\\n    >>> grouped.transform(lambda x: x.astype(int).max())\\n    C  D\\n    0  5  8\\n    1  5  9\\n    2  5  8\\n    3  5  9\\n    4  5  8\\n    5  5  9\\n    ')\n\n    @doc(DataFrame.corrwith.__doc__)\n    def corrwith(self, other: DataFrame | Series, axis: Axis | lib.NoDefault=lib.no_default, drop: bool=False, method: CorrelationMethod='pearson', numeric_only: bool=False) -> DataFrame:\n        result = self._op_via_apply('corrwith', other=other, axis=axis, drop=drop, method=method, numeric_only=numeric_only)\n        return result\n\n    def nunique(self, dropna: bool=True) -> DataFrame:\n        \"\"\"\n        Return DataFrame with counts of unique elements in each position.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        nunique: DataFrame\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',\n        ...                           'ham', 'ham'],\n        ...                    'value1': [1, 5, 5, 2, 5, 5],\n        ...                    'value2': list('abbaxy')})\n        >>> df\n             id  value1 value2\n        0  spam       1      a\n        1   egg       5      b\n        2   egg       5      b\n        3  spam       2      a\n        4   ham       5      x\n        5   ham       5      y\n\n        >>> df.groupby('id').nunique()\n              value1  value2\n        id\n        egg        1       1\n        ham        1       2\n        spam       2       1\n\n        Check for rows with the same id but conflicting values:\n\n        >>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())\n             id  value1 value2\n        0  spam       1      a\n        3  spam       2      a\n        4   ham       5      x\n        5   ham       5      y\n        \"\"\"\n        if self.axis != 0:\n            return self._python_apply_general(lambda sgb: sgb.nunique(dropna), self._obj_with_exclusions, is_agg=True)\n        return self._apply_to_column_groupbys(lambda sgb: sgb.nunique(dropna))\n\n    def idxmin(self, axis: Axis | None | lib.NoDefault=lib.no_default, skipna: bool=True, numeric_only: bool=False) -> DataFrame:\n        \"\"\"\n        Return index of first occurrence of minimum over requested axis.\n\n        NA/null values are excluded.\n\n        Parameters\n        ----------\n        axis : {{0 or 'index', 1 or 'columns'}}, default None\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n            If axis is not provided, grouper's axis is used.\n\n            .. versionchanged:: 2.0.0\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n        Returns\n        -------\n        Series\n            Indexes of minima along the specified axis.\n\n        Raises\n        ------\n        ValueError\n            * If the row/column is empty\n\n        See Also\n        --------\n        Series.idxmin : Return index of the minimum element.\n\n        Notes\n        -----\n        This method is the DataFrame version of ``ndarray.argmin``.\n\n        Examples\n        --------\n        Consider a dataset containing food consumption in Argentina.\n\n        >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n        ...                    'co2_emissions': [37.2, 19.66, 1712]},\n        ...                   index=['Pork', 'Wheat Products', 'Beef'])\n\n        >>> df\n                        consumption  co2_emissions\n        Pork                  10.51         37.20\n        Wheat Products       103.11         19.66\n        Beef                  55.48       1712.00\n\n        By default, it returns the index for the minimum value in each column.\n\n        >>> df.idxmin()\n        consumption                Pork\n        co2_emissions    Wheat Products\n        dtype: object\n\n        To return the index for the minimum value in each row, use ``axis=\"columns\"``.\n\n        >>> df.idxmin(axis=\"columns\")\n        Pork                consumption\n        Wheat Products    co2_emissions\n        Beef                consumption\n        dtype: object\n        \"\"\"\n        return self._idxmax_idxmin('idxmin', axis=axis, numeric_only=numeric_only, skipna=skipna)\n\n    def _define_paths(self, func, *args, **kwargs):\n        if isinstance(func, str):\n            fast_path = lambda group: getattr(group, func)(*args, **kwargs)\n            slow_path = lambda group: group.apply(lambda x: getattr(x, func)(*args, **kwargs), axis=self.axis)\n        else:\n            fast_path = lambda group: func(group, *args, **kwargs)\n            slow_path = lambda group: group.apply(lambda x: func(x, *args, **kwargs), axis=self.axis)\n        return (fast_path, slow_path)\n\n    def _cython_transform(self, how: str, numeric_only: bool=False, axis: AxisInt=0, **kwargs) -> DataFrame:\n        assert axis == 0\n        mgr: Manager2D = self._get_data_to_aggregate(numeric_only=numeric_only, name=how)\n\n        def arr_func(bvalues: ArrayLike) -> ArrayLike:\n            return self._grouper._cython_operation('transform', bvalues, how, 1, **kwargs)\n        res_mgr = mgr.grouped_reduce(arr_func)\n        res_mgr.set_axis(1, mgr.axes[1])\n        res_df = self.obj._constructor_from_mgr(res_mgr, axes=res_mgr.axes)\n        res_df = self._maybe_transpose_result(res_df)\n        return res_df\n\n    @Substitution(klass='DataFrame', example=__examples_dataframe_doc)\n    @Appender(_transform_template)\n    def transform(self, func, *args, engine=None, engine_kwargs=None, **kwargs):\n        return self._transform(func, *args, engine=engine, engine_kwargs=engine_kwargs, **kwargs)\n\n    def _wrap_applied_output(self, data: DataFrame, values: list, not_indexed_same: bool=False, is_transform: bool=False):\n        if len(values) == 0:\n            if is_transform:\n                res_index = data.index\n            else:\n                res_index = self._grouper.result_index\n            result = self.obj._constructor(index=res_index, columns=data.columns)\n            result = result.astype(data.dtypes, copy=False)\n            return result\n        first_not_none = next(com.not_none(*values), None)\n        if first_not_none is None:\n            return self.obj._constructor()\n        elif isinstance(first_not_none, DataFrame):\n            return self._concat_objects(values, not_indexed_same=not_indexed_same, is_transform=is_transform)\n        key_index = self._grouper.result_index if self.as_index else None\n        if isinstance(first_not_none, (np.ndarray, Index)):\n            if not is_hashable(self._selection):\n                name = tuple(self._selection)\n            else:\n                name = self._selection\n            return self.obj._constructor_sliced(values, index=key_index, name=name)\n        elif not isinstance(first_not_none, Series):\n            if self.as_index:\n                return self.obj._constructor_sliced(values, index=key_index)\n            else:\n                result = self.obj._constructor(values, columns=[self._selection])\n                result = self._insert_inaxis_grouper(result)\n                return result\n        else:\n            return self._wrap_applied_output_series(values, not_indexed_same, first_not_none, key_index, is_transform)\n\n    def take(self, indices: TakeIndexer, axis: Axis | None | lib.NoDefault=lib.no_default, **kwargs) -> DataFrame:\n        \"\"\"\n        Return the elements in the given *positional* indices in each group.\n\n        This means that we are not indexing according to actual values in\n        the index attribute of the object. We are indexing according to the\n        actual position of the element in the object.\n\n        If a requested index does not exist for some group, this method will raise.\n        To get similar behavior that ignores indices that don't exist, see\n        :meth:`.DataFrameGroupBy.nth`.\n\n        Parameters\n        ----------\n        indices : array-like\n            An array of ints indicating which positions to take.\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            The axis on which to select elements. ``0`` means that we are\n            selecting rows, ``1`` means that we are selecting columns.\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        **kwargs\n            For compatibility with :meth:`numpy.take`. Has no effect on the\n            output.\n\n        Returns\n        -------\n        DataFrame\n            An DataFrame containing the elements taken from each group.\n\n        See Also\n        --------\n        DataFrame.take : Take elements from a Series along an axis.\n        DataFrame.loc : Select a subset of a DataFrame by labels.\n        DataFrame.iloc : Select a subset of a DataFrame by positions.\n        numpy.take : Take elements from an array along an axis.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n        ...                    ('parrot', 'bird', 24.0),\n        ...                    ('lion', 'mammal', 80.5),\n        ...                    ('monkey', 'mammal', np.nan),\n        ...                    ('rabbit', 'mammal', 15.0)],\n        ...                   columns=['name', 'class', 'max_speed'],\n        ...                   index=[4, 3, 2, 1, 0])\n        >>> df\n             name   class  max_speed\n        4  falcon    bird      389.0\n        3  parrot    bird       24.0\n        2    lion  mammal       80.5\n        1  monkey  mammal        NaN\n        0  rabbit  mammal       15.0\n        >>> gb = df.groupby([1, 1, 2, 2, 2])\n\n        Take elements at positions 0 and 1 along the axis 0 (default).\n\n        Note how the indices selected in the result do not correspond to\n        our input indices 0 and 1. That's because we are selecting the 0th\n        and 1st rows, not rows whose indices equal 0 and 1.\n\n        >>> gb.take([0, 1])\n               name   class  max_speed\n        1 4  falcon    bird      389.0\n          3  parrot    bird       24.0\n        2 2    lion  mammal       80.5\n          1  monkey  mammal        NaN\n\n        The order of the specified indices influences the order in the result.\n        Here, the order is swapped from the previous example.\n\n        >>> gb.take([1, 0])\n               name   class  max_speed\n        1 3  parrot    bird       24.0\n          4  falcon    bird      389.0\n        2 1  monkey  mammal        NaN\n          2    lion  mammal       80.5\n\n        Take elements at indices 1 and 2 along the axis 1 (column selection).\n\n        We may take elements using negative integers for positive indices,\n        starting from the end of the object, just like with Python lists.\n\n        >>> gb.take([-1, -2])\n               name   class  max_speed\n        1 3  parrot    bird       24.0\n          4  falcon    bird      389.0\n        2 0  rabbit  mammal       15.0\n          1  monkey  mammal        NaN\n        \"\"\"\n        result = self._op_via_apply('take', indices=indices, axis=axis, **kwargs)\n        return result\n\n    def fillna(self, value: Hashable | Mapping | Series | DataFrame | None=None, method: FillnaOptions | None=None, axis: Axis | None | lib.NoDefault=lib.no_default, inplace: bool=False, limit: int | None=None, downcast=lib.no_default) -> DataFrame | None:\n        \"\"\"\n        Fill NA/NaN values using the specified method within groups.\n\n        .. deprecated:: 2.2.0\n            This method is deprecated and will be removed in a future version.\n            Use the :meth:`.DataFrameGroupBy.ffill` or :meth:`.DataFrameGroupBy.bfill`\n            for forward or backward filling instead. If you want to fill with a\n            single value, use :meth:`DataFrame.fillna` instead.\n\n        Parameters\n        ----------\n        value : scalar, dict, Series, or DataFrame\n            Value to use to fill holes (e.g. 0), alternately a\n            dict/Series/DataFrame of values specifying which value to use for\n            each index (for a Series) or column (for a DataFrame).  Values not\n            in the dict/Series/DataFrame will not be filled. This value cannot\n            be a list. Users wanting to use the ``value`` argument and not ``method``\n            should prefer :meth:`.DataFrame.fillna` as this\n            will produce the same result and be more performant.\n        method : {{'bfill', 'ffill', None}}, default None\n            Method to use for filling holes. ``'ffill'`` will propagate\n            the last valid observation forward within a group.\n            ``'bfill'`` will use next valid observation to fill the gap.\n        axis : {0 or 'index', 1 or 'columns'}\n            Axis along which to fill missing values. When the :class:`DataFrameGroupBy`\n            ``axis`` argument is ``0``, using ``axis=1`` here will produce\n            the same results as :meth:`.DataFrame.fillna`. When the\n            :class:`DataFrameGroupBy` ``axis`` argument is ``1``, using ``axis=0``\n            or ``axis=1`` here will produce the same results.\n        inplace : bool, default False\n            Broken. Do not set to True.\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill within a group. In other words,\n            if there is a gap with more than this number of consecutive NaNs,\n            it will only be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled. Must be greater than 0 if not None.\n        downcast : dict, default is None\n            A dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible).\n\n        Returns\n        -------\n        DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        ffill : Forward fill values within a group.\n        bfill : Backward fill values within a group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"key\": [0, 0, 1, 1, 1],\n        ...         \"A\": [np.nan, 2, np.nan, 3, np.nan],\n        ...         \"B\": [2, 3, np.nan, np.nan, np.nan],\n        ...         \"C\": [np.nan, np.nan, 2, np.nan, np.nan],\n        ...     }\n        ... )\n        >>> df\n           key    A    B   C\n        0    0  NaN  2.0 NaN\n        1    0  2.0  3.0 NaN\n        2    1  NaN  NaN 2.0\n        3    1  3.0  NaN NaN\n        4    1  NaN  NaN NaN\n\n        Propagate non-null values forward or backward within each group along columns.\n\n        >>> df.groupby(\"key\").fillna(method=\"ffill\")\n             A    B   C\n        0  NaN  2.0 NaN\n        1  2.0  3.0 NaN\n        2  NaN  NaN 2.0\n        3  3.0  NaN 2.0\n        4  3.0  NaN 2.0\n\n        >>> df.groupby(\"key\").fillna(method=\"bfill\")\n             A    B   C\n        0  2.0  2.0 NaN\n        1  2.0  3.0 NaN\n        2  3.0  NaN 2.0\n        3  3.0  NaN NaN\n        4  NaN  NaN NaN\n\n        Propagate non-null values forward or backward within each group along rows.\n\n        >>> df.T.groupby(np.array([0, 0, 1, 1])).fillna(method=\"ffill\").T\n           key    A    B    C\n        0  0.0  0.0  2.0  2.0\n        1  0.0  2.0  3.0  3.0\n        2  1.0  1.0  NaN  2.0\n        3  1.0  3.0  NaN  NaN\n        4  1.0  1.0  NaN  NaN\n\n        >>> df.T.groupby(np.array([0, 0, 1, 1])).fillna(method=\"bfill\").T\n           key    A    B    C\n        0  0.0  NaN  2.0  NaN\n        1  0.0  2.0  3.0  NaN\n        2  1.0  NaN  2.0  2.0\n        3  1.0  3.0  NaN  NaN\n        4  1.0  NaN  NaN  NaN\n\n        Only replace the first NaN element within a group along rows.\n\n        >>> df.groupby(\"key\").fillna(method=\"ffill\", limit=1)\n             A    B    C\n        0  NaN  2.0  NaN\n        1  2.0  3.0  NaN\n        2  NaN  NaN  2.0\n        3  3.0  NaN  2.0\n        4  3.0  NaN  NaN\n        \"\"\"\n        warnings.warn(f'{type(self).__name__}.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use {type(self.obj).__name__}.fillna instead', FutureWarning, stacklevel=find_stack_level())\n        result = self._op_via_apply('fillna', value=value, method=method, axis=axis, inplace=inplace, limit=limit, downcast=downcast)\n        return result\n\n    def __getitem__(self, key) -> DataFrameGroupBy | SeriesGroupBy:\n        if self.axis == 1:\n            raise ValueError('Cannot subset columns when using axis=1')\n        if isinstance(key, tuple) and len(key) > 1:\n            raise ValueError('Cannot subset columns with a tuple with more than one element. Use a list instead.')\n        return super().__getitem__(key)\n\n    def _python_agg_general(self, func, *args, **kwargs):\n        orig_func = func\n        func = com.is_builtin_func(func)\n        if orig_func != func:\n            alias = com._builtin_table_alias[func]\n            warn_alias_replacement(self, orig_func, alias)\n        f = lambda x: func(x, *args, **kwargs)\n        if self.ngroups == 0:\n            return self._python_apply_general(f, self._selected_obj, is_agg=True)\n        obj = self._obj_with_exclusions\n        if self.axis == 1:\n            obj = obj.T\n        if not len(obj.columns):\n            return self._python_apply_general(f, self._selected_obj)\n        output: dict[int, ArrayLike] = {}\n        for (idx, (name, ser)) in enumerate(obj.items()):\n            result = self._grouper.agg_series(ser, f)\n            output[idx] = result\n        res = self.obj._constructor(output)\n        res.columns = obj.columns.copy(deep=False)\n        return self._wrap_aggregated_output(res)\n\n    def skew(self, axis: Axis | None | lib.NoDefault=lib.no_default, skipna: bool=True, numeric_only: bool=False, **kwargs) -> DataFrame:\n        \"\"\"\n        Return unbiased skew within groups.\n\n        Normalized by N-1.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            Axis for the function to be applied on.\n\n            Specifying ``axis=None`` will apply the aggregation across both axes.\n\n            .. versionadded:: 2.0.0\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        skipna : bool, default True\n            Exclude NA/null values when computing the result.\n\n        numeric_only : bool, default False\n            Include only float, int, boolean columns.\n\n        **kwargs\n            Additional keyword arguments to be passed to the function.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.skew : Return unbiased skew over requested axis.\n\n        Examples\n        --------\n        >>> arrays = [['falcon', 'parrot', 'cockatoo', 'kiwi',\n        ...            'lion', 'monkey', 'rabbit'],\n        ...           ['bird', 'bird', 'bird', 'bird',\n        ...            'mammal', 'mammal', 'mammal']]\n        >>> index = pd.MultiIndex.from_arrays(arrays, names=('name', 'class'))\n        >>> df = pd.DataFrame({'max_speed': [389.0, 24.0, 70.0, np.nan,\n        ...                                  80.5, 21.5, 15.0]},\n        ...                   index=index)\n        >>> df\n                        max_speed\n        name     class\n        falcon   bird        389.0\n        parrot   bird         24.0\n        cockatoo bird         70.0\n        kiwi     bird          NaN\n        lion     mammal       80.5\n        monkey   mammal       21.5\n        rabbit   mammal       15.0\n        >>> gb = df.groupby([\"class\"])\n        >>> gb.skew()\n                max_speed\n        class\n        bird     1.628296\n        mammal   1.669046\n        >>> gb.skew(skipna=False)\n                max_speed\n        class\n        bird          NaN\n        mammal   1.669046\n        \"\"\"\n        if axis is lib.no_default:\n            axis = 0\n        if axis != 0:\n            result = self._op_via_apply('skew', axis=axis, skipna=skipna, numeric_only=numeric_only, **kwargs)\n            return result\n\n        def alt(obj):\n            raise TypeError(f\"'skew' is not supported for dtype={obj.dtype}\")\n        return self._cython_agg_general('skew', alt=alt, skipna=skipna, numeric_only=numeric_only, **kwargs)\n    boxplot = boxplot_frame_groupby\n\n    @doc(DataFrame.describe)\n    def describe(self, percentiles=None, include=None, exclude=None) -> NDFrameT:\n        obj = self._obj_with_exclusions\n        if len(obj) == 0:\n            described = obj.describe(percentiles=percentiles, include=include, exclude=exclude)\n            if obj.ndim == 1:\n                result = described\n            else:\n                result = described.unstack()\n            return result.to_frame().T.iloc[:0]\n        with com.temp_setattr(self, 'as_index', True):\n            result = self._python_apply_general(lambda x: x.describe(percentiles=percentiles, include=include, exclude=exclude), obj, not_indexed_same=True)\n        if self.axis == 1:\n            return result.T\n        result = result.unstack()\n        if not self.as_index:\n            result = self._insert_inaxis_grouper(result)\n            result.index = default_index(len(result))\n        return result\n\n    def value_counts(self, subset: Sequence[Hashable] | None=None, normalize: bool=False, sort: bool=True, ascending: bool=False, dropna: bool=True) -> DataFrame | Series:\n        \"\"\"\n        Return a Series or DataFrame containing counts of unique rows.\n\n        .. versionadded:: 1.4.0\n\n        Parameters\n        ----------\n        subset : list-like, optional\n            Columns to use when counting unique combinations.\n        normalize : bool, default False\n            Return proportions rather than frequencies.\n        sort : bool, default True\n            Sort by frequencies.\n        ascending : bool, default False\n            Sort in ascending order.\n        dropna : bool, default True\n            Don't include counts of rows that contain NA values.\n\n        Returns\n        -------\n        Series or DataFrame\n            Series if the groupby as_index is True, otherwise DataFrame.\n\n        See Also\n        --------\n        Series.value_counts: Equivalent method on Series.\n        DataFrame.value_counts: Equivalent method on DataFrame.\n        SeriesGroupBy.value_counts: Equivalent method on SeriesGroupBy.\n\n        Notes\n        -----\n        - If the groupby as_index is True then the returned Series will have a\n          MultiIndex with one level per input column.\n        - If the groupby as_index is False then the returned DataFrame will have an\n          additional column with the value_counts. The column is labelled 'count' or\n          'proportion', depending on the ``normalize`` parameter.\n\n        By default, rows that contain any NA values are omitted from\n        the result.\n\n        By default, the result will be in descending order so that the\n        first element of each group is the most frequently-occurring row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\n        ...     'gender': ['male', 'male', 'female', 'male', 'female', 'male'],\n        ...     'education': ['low', 'medium', 'high', 'low', 'high', 'low'],\n        ...     'country': ['US', 'FR', 'US', 'FR', 'FR', 'FR']\n        ... })\n\n        >>> df\n                gender  education   country\n        0       male    low         US\n        1       male    medium      FR\n        2       female  high        US\n        3       male    low         FR\n        4       female  high        FR\n        5       male    low         FR\n\n        >>> df.groupby('gender').value_counts()\n        gender  education  country\n        female  high       FR         1\n                           US         1\n        male    low        FR         2\n                           US         1\n                medium     FR         1\n        Name: count, dtype: int64\n\n        >>> df.groupby('gender').value_counts(ascending=True)\n        gender  education  country\n        female  high       FR         1\n                           US         1\n        male    low        US         1\n                medium     FR         1\n                low        FR         2\n        Name: count, dtype: int64\n\n        >>> df.groupby('gender').value_counts(normalize=True)\n        gender  education  country\n        female  high       FR         0.50\n                           US         0.50\n        male    low        FR         0.50\n                           US         0.25\n                medium     FR         0.25\n        Name: proportion, dtype: float64\n\n        >>> df.groupby('gender', as_index=False).value_counts()\n           gender education country  count\n        0  female      high      FR      1\n        1  female      high      US      1\n        2    male       low      FR      2\n        3    male       low      US      1\n        4    male    medium      FR      1\n\n        >>> df.groupby('gender', as_index=False).value_counts(normalize=True)\n           gender education country  proportion\n        0  female      high      FR        0.50\n        1  female      high      US        0.50\n        2    male       low      FR        0.50\n        3    male       low      US        0.25\n        4    male    medium      FR        0.25\n        \"\"\"\n        return self._value_counts(subset, normalize, sort, ascending, dropna)\n\n    def _transform_general(self, func, engine, engine_kwargs, *args, **kwargs):\n        if maybe_use_numba(engine):\n            return self._transform_with_numba(func, *args, engine_kwargs=engine_kwargs, **kwargs)\n        from pandas.core.reshape.concat import concat\n        applied = []\n        obj = self._obj_with_exclusions\n        gen = self._grouper.get_iterator(obj, axis=self.axis)\n        (fast_path, slow_path) = self._define_paths(func, *args, **kwargs)\n        try:\n            (name, group) = next(gen)\n        except StopIteration:\n            pass\n        else:\n            object.__setattr__(group, 'name', name)\n            try:\n                (path, res) = self._choose_path(fast_path, slow_path, group)\n            except ValueError as err:\n                msg = 'transform must return a scalar value for each group'\n                raise ValueError(msg) from err\n            if group.size > 0:\n                res = _wrap_transform_general_frame(self.obj, group, res)\n                applied.append(res)\n        for (name, group) in gen:\n            if group.size == 0:\n                continue\n            object.__setattr__(group, 'name', name)\n            res = path(group)\n            res = _wrap_transform_general_frame(self.obj, group, res)\n            applied.append(res)\n        concat_index = obj.columns if self.axis == 0 else obj.index\n        other_axis = 1 if self.axis == 0 else 0\n        concatenated = concat(applied, axis=self.axis, verify_integrity=False)\n        concatenated = concatenated.reindex(concat_index, axis=other_axis, copy=False)\n        return self._set_result_index_ordered(concatenated)\n\n    def _gotitem(self, key, ndim: int, subset=None):\n        \"\"\"\n        sub-classes to define\n        return a sliced object\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        if ndim == 2:\n            if subset is None:\n                subset = self.obj\n            return DataFrameGroupBy(subset, self.keys, axis=self.axis, level=self.level, grouper=self._grouper, exclusions=self.exclusions, selection=key, as_index=self.as_index, sort=self.sort, group_keys=self.group_keys, observed=self.observed, dropna=self.dropna)\n        elif ndim == 1:\n            if subset is None:\n                subset = self.obj[key]\n            return SeriesGroupBy(subset, self.keys, level=self.level, grouper=self._grouper, exclusions=self.exclusions, selection=key, as_index=self.as_index, sort=self.sort, group_keys=self.group_keys, observed=self.observed, dropna=self.dropna)\n        raise AssertionError('invalid ndim for _gotitem')\n\n    @property\n    @doc(DataFrame.plot.__doc__)\n    def plot(self) -> GroupByPlot:\n        result = GroupByPlot(self)\n        return result\n\n    @doc(_agg_template_frame, examples=_agg_examples_doc, klass='DataFrame')\n    def aggregate(self, func=None, *args, engine=None, engine_kwargs=None, **kwargs):\n        (relabeling, func, columns, order) = reconstruct_func(func, **kwargs)\n        func = maybe_mangle_lambdas(func)\n        if maybe_use_numba(engine):\n            kwargs['engine'] = engine\n            kwargs['engine_kwargs'] = engine_kwargs\n        op = GroupByApply(self, func, args=args, kwargs=kwargs)\n        result = op.agg()\n        if not is_dict_like(func) and result is not None:\n            if not self.as_index and is_list_like(func):\n                return result.reset_index()\n            else:\n                return result\n        elif relabeling:\n            result = cast(DataFrame, result)\n            result = result.iloc[:, order]\n            result = cast(DataFrame, result)\n            result.columns = columns\n        if result is None:\n            if 'engine' in kwargs:\n                del kwargs['engine']\n                del kwargs['engine_kwargs']\n            if maybe_use_numba(engine):\n                return self._aggregate_with_numba(func, *args, engine_kwargs=engine_kwargs, **kwargs)\n            if self._grouper.nkeys > 1:\n                return self._python_agg_general(func, *args, **kwargs)\n            elif args or kwargs:\n                result = self._aggregate_frame(func, *args, **kwargs)\n            elif self.axis == 1:\n                result = self._aggregate_frame(func)\n                return result\n            else:\n                gba = GroupByApply(self, [func], args=(), kwargs={})\n                try:\n                    result = gba.agg()\n                except ValueError as err:\n                    if 'No objects to concatenate' not in str(err):\n                        raise\n                    result = self._aggregate_frame(func)\n                else:\n                    result = cast(DataFrame, result)\n                    result.columns = self._obj_with_exclusions.columns.copy()\n        if not self.as_index:\n            result = self._insert_inaxis_grouper(result)\n            result.index = default_index(len(result))\n        return result\n\n    def _aggregate_frame(self, func, *args, **kwargs) -> DataFrame:\n        if self._grouper.nkeys != 1:\n            raise AssertionError('Number of keys must be 1')\n        obj = self._obj_with_exclusions\n        result: dict[Hashable, NDFrame | np.ndarray] = {}\n        for (name, grp_df) in self._grouper.get_iterator(obj, self.axis):\n            fres = func(grp_df, *args, **kwargs)\n            result[name] = fres\n        result_index = self._grouper.result_index\n        other_ax = obj.axes[1 - self.axis]\n        out = self.obj._constructor(result, index=other_ax, columns=result_index)\n        if self.axis == 0:\n            out = out.T\n        return out\n\n    def _choose_path(self, fast_path: Callable, slow_path: Callable, group: DataFrame):\n        path = slow_path\n        res = slow_path(group)\n        if self.ngroups == 1:\n            return (path, res)\n        try:\n            res_fast = fast_path(group)\n        except AssertionError:\n            raise\n        except Exception:\n            return (path, res)\n        if isinstance(res_fast, DataFrame):\n            if not res_fast.columns.equals(group.columns):\n                return (path, res)\n        elif isinstance(res_fast, Series):\n            if not res_fast.index.equals(group.columns):\n                return (path, res)\n        else:\n            return (path, res)\n        if res_fast.equals(res):\n            path = fast_path\n        return (path, res)\n\n    @property\n    @doc(DataFrame.dtypes.__doc__)\n    def dtypes(self) -> Series:\n        warnings.warn(f'{type(self).__name__}.dtypes is deprecated and will be removed in a future version. Check the dtypes on the base object instead', FutureWarning, stacklevel=find_stack_level())\n        return self._python_apply_general(lambda df: df.dtypes, self._selected_obj)\n\n    def _get_data_to_aggregate(self, *, numeric_only: bool=False, name: str | None=None) -> Manager2D:\n        obj = self._obj_with_exclusions\n        if self.axis == 1:\n            mgr = obj.T._mgr\n        else:\n            mgr = obj._mgr\n        if numeric_only:\n            mgr = mgr.get_numeric_data()\n        return mgr\n\n    @doc(DataFrame.hist.__doc__)\n    def hist(self, column: IndexLabel | None=None, by=None, grid: bool=True, xlabelsize: int | None=None, xrot: float | None=None, ylabelsize: int | None=None, yrot: float | None=None, ax=None, sharex: bool=False, sharey: bool=False, figsize: tuple[int, int] | None=None, layout: tuple[int, int] | None=None, bins: int | Sequence[int]=10, backend: str | None=None, legend: bool=False, **kwargs):\n        result = self._op_via_apply('hist', column=column, by=by, grid=grid, xlabelsize=xlabelsize, xrot=xrot, ylabelsize=ylabelsize, yrot=yrot, ax=ax, sharex=sharex, sharey=sharey, figsize=figsize, layout=layout, bins=bins, backend=backend, legend=legend, **kwargs)\n        return result\n\n    @doc(DataFrame.corr.__doc__)\n    def corr(self, method: str | Callable[[np.ndarray, np.ndarray], float]='pearson', min_periods: int=1, numeric_only: bool=False) -> DataFrame:\n        result = self._op_via_apply('corr', method=method, min_periods=min_periods, numeric_only=numeric_only)\n        return result\n\n    @final\n    def resample(self, rule, *args, include_groups: bool=True, **kwargs) -> Resampler:\n        \"\"\"\n        Provide resampling when using a TimeGrouper.\n\n        Given a grouper, the function resamples it according to a string\n        \"string\" -> \"frequency\".\n\n        See the :ref:`frequency aliases <timeseries.offset_aliases>`\n        documentation for more details.\n\n        Parameters\n        ----------\n        rule : str or DateOffset\n            The offset string or object representing target grouper conversion.\n        *args\n            Possible arguments are `how`, `fill_method`, `limit`, `kind` and\n            `on`, and other arguments of `TimeGrouper`.\n        include_groups : bool, default True\n            When True, will attempt to include the groupings in the operation in\n            the case that they are columns of the DataFrame. If this raises a\n            TypeError, the result will be computed with the groupings excluded.\n            When False, the groupings will be excluded when applying ``func``.\n\n            .. versionadded:: 2.2.0\n\n            .. deprecated:: 2.2.0\n\n               Setting include_groups to True is deprecated. Only the value\n               False will be allowed in a future version of pandas.\n\n        **kwargs\n            Possible arguments are `how`, `fill_method`, `limit`, `kind` and\n            `on`, and other arguments of `TimeGrouper`.\n\n        Returns\n        -------\n        pandas.api.typing.DatetimeIndexResamplerGroupby,\n        pandas.api.typing.PeriodIndexResamplerGroupby, or\n        pandas.api.typing.TimedeltaIndexResamplerGroupby\n            Return a new groupby object, with type depending on the data\n            being resampled.\n\n        See Also\n        --------\n        Grouper : Specify a frequency to resample with when\n            grouping by a key.\n        DatetimeIndex.resample : Frequency conversion and resampling of\n            time series.\n\n        Examples\n        --------\n        >>> idx = pd.date_range('1/1/2000', periods=4, freq='min')\n        >>> df = pd.DataFrame(data=4 * [range(2)],\n        ...                   index=idx,\n        ...                   columns=['a', 'b'])\n        >>> df.iloc[2, 0] = 5\n        >>> df\n                            a  b\n        2000-01-01 00:00:00  0  1\n        2000-01-01 00:01:00  0  1\n        2000-01-01 00:02:00  5  1\n        2000-01-01 00:03:00  0  1\n\n        Downsample the DataFrame into 3 minute bins and sum the values of\n        the timestamps falling into a bin.\n\n        >>> df.groupby('a').resample('3min', include_groups=False).sum()\n                                 b\n        a\n        0   2000-01-01 00:00:00  2\n            2000-01-01 00:03:00  1\n        5   2000-01-01 00:00:00  1\n\n        Upsample the series into 30 second bins.\n\n        >>> df.groupby('a').resample('30s', include_groups=False).sum()\n                            b\n        a\n        0   2000-01-01 00:00:00  1\n            2000-01-01 00:00:30  0\n            2000-01-01 00:01:00  1\n            2000-01-01 00:01:30  0\n            2000-01-01 00:02:00  0\n            2000-01-01 00:02:30  0\n            2000-01-01 00:03:00  1\n        5   2000-01-01 00:02:00  1\n\n        Resample by month. Values are assigned to the month of the period.\n\n        >>> df.groupby('a').resample('ME', include_groups=False).sum()\n                    b\n        a\n        0   2000-01-31  3\n        5   2000-01-31  1\n\n        Downsample the series into 3 minute bins as above, but close the right\n        side of the bin interval.\n\n        >>> (\n        ...     df.groupby('a')\n        ...     .resample('3min', closed='right', include_groups=False)\n        ...     .sum()\n        ... )\n                                 b\n        a\n        0   1999-12-31 23:57:00  1\n            2000-01-01 00:00:00  2\n        5   2000-01-01 00:00:00  1\n\n        Downsample the series into 3 minute bins and close the right side of\n        the bin interval, but label each bin using the right edge instead of\n        the left.\n\n        >>> (\n        ...     df.groupby('a')\n        ...     .resample('3min', closed='right', label='right', include_groups=False)\n        ...     .sum()\n        ... )\n                                 b\n        a\n        0   2000-01-01 00:00:00  1\n            2000-01-01 00:03:00  2\n        5   2000-01-01 00:03:00  1\n        \"\"\"\n        from pandas.core.resample import get_resampler_for_grouping\n        return get_resampler_for_grouping(self, rule, *args, include_groups=include_groups, **kwargs)", "class_fn": true, "question_id": "pandas/pandas.core.groupby.generic/DataFrameGroupBy", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/groupby.py", "fn_id": "", "content": "class GroupBy(BaseGroupBy[NDFrameT]):\n    \"\"\"\n    Class for grouping and aggregating relational data.\n\n    See aggregate, transform, and apply functions on this object.\n\n    It's easiest to use obj.groupby(...) to use GroupBy, but you can also do:\n\n    ::\n\n        grouped = groupby(obj, ...)\n\n    Parameters\n    ----------\n    obj : pandas object\n    axis : int, default 0\n    level : int, default None\n        Level of MultiIndex\n    groupings : list of Grouping objects\n        Most users should ignore this\n    exclusions : array-like, optional\n        List of columns to exclude\n    name : str\n        Most users should ignore this\n\n    Returns\n    -------\n    **Attributes**\n    groups : dict\n        {group name -> group labels}\n    len(grouped) : int\n        Number of groups\n\n    Notes\n    -----\n    After grouping, see aggregate, apply, and transform functions. Here are\n    some other brief notes about usage. When grouping by multiple groups, the\n    result index will be a MultiIndex (hierarchical) by default.\n\n    Iteration produces (key, group) tuples, i.e. chunking the data by group. So\n    you can write code like:\n\n    ::\n\n        grouped = obj.groupby(keys, axis=axis)\n        for key, group in grouped:\n            # do something with the data\n\n    Function calls on GroupBy, if not specially implemented, \"dispatch\" to the\n    grouped data. So if you group a DataFrame and wish to invoke the std()\n    method on each group, you can simply do:\n\n    ::\n\n        df.groupby(mapper).std()\n\n    rather than\n\n    ::\n\n        df.groupby(mapper).aggregate(np.std)\n\n    You can pass arguments to these \"wrapped\" functions, too.\n\n    See the online documentation for full exposition on these topics and much\n    more\n    \"\"\"\n    _grouper: ops.BaseGrouper\n    as_index: bool\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def diff(self, periods: int=1, axis: AxisInt | lib.NoDefault=lib.no_default) -> NDFrameT:\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of each element compared with another\n        element in the group (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative values.\n        axis : axis to shift, default 0\n            Take difference over rows (0) or columns (1).\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        Returns\n        -------\n        Series or DataFrame\n            First differences.\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n        >>> ser\n        a     7\n        a     2\n        a     8\n        b     4\n        b     3\n        b     3\n        dtype: int64\n        >>> ser.groupby(level=0).diff()\n        a    NaN\n        a   -5.0\n        a    6.0\n        b    NaN\n        b   -1.0\n        b    0.0\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n        >>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n        ...                   'mouse', 'mouse', 'mouse', 'mouse'])\n        >>> df\n                 a  b\n          dog    1  1\n          dog    3  4\n          dog    5  8\n        mouse    7  4\n        mouse    7  4\n        mouse    8  2\n        mouse    3  1\n        >>> df.groupby(level=0).diff()\n                 a    b\n          dog  NaN  NaN\n          dog  2.0  3.0\n          dog  2.0  4.0\n        mouse  NaN  NaN\n        mouse  0.0  0.0\n        mouse  1.0 -2.0\n        mouse -5.0 -1.0\n        \"\"\"\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, 'diff')\n        else:\n            axis = 0\n        if axis != 0:\n            return self.apply(lambda x: x.diff(periods=periods, axis=axis))\n        obj = self._obj_with_exclusions\n        shifted = self.shift(periods=periods)\n        dtypes_to_f32 = ['int8', 'int16']\n        if obj.ndim == 1:\n            if obj.dtype in dtypes_to_f32:\n                shifted = shifted.astype('float32')\n        else:\n            to_coerce = [c for (c, dtype) in obj.dtypes.items() if dtype in dtypes_to_f32]\n            if len(to_coerce):\n                shifted = shifted.astype({c: 'float32' for c in to_coerce})\n        return obj - shifted\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def cumprod(self, axis: Axis | lib.NoDefault=lib.no_default, *args, **kwargs) -> NDFrameT:\n        \"\"\"\n        Cumulative product for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([6, 2, 0], index=lst)\n        >>> ser\n        a    6\n        a    2\n        b    0\n        dtype: int64\n        >>> ser.groupby(level=0).cumprod()\n        a    6\n        a   12\n        b    0\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"cow\", \"horse\", \"bull\"])\n        >>> df\n                a   b   c\n        cow     1   8   2\n        horse   1   2   5\n        bull    2   6   9\n        >>> df.groupby(\"a\").groups\n        {1: ['cow', 'horse'], 2: ['bull']}\n        >>> df.groupby(\"a\").cumprod()\n                b   c\n        cow     8   2\n        horse  16  10\n        bull    6   9\n        \"\"\"\n        nv.validate_groupby_func('cumprod', args, kwargs, ['numeric_only', 'skipna'])\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, 'cumprod')\n        else:\n            axis = 0\n        if axis != 0:\n            f = lambda x: x.cumprod(axis=axis, **kwargs)\n            return self._python_apply_general(f, self._selected_obj, is_transform=True)\n        return self._cython_transform('cumprod', **kwargs)\n\n    @final\n    def _get_indices(self, names):\n        \"\"\"\n        Safe get multiple indices, translate keys for\n        datelike to underlying repr.\n        \"\"\"\n\n        def get_converter(s):\n            if isinstance(s, datetime.datetime):\n                return lambda key: Timestamp(key)\n            elif isinstance(s, np.datetime64):\n                return lambda key: Timestamp(key).asm8\n            else:\n                return lambda key: key\n        if len(names) == 0:\n            return []\n        if len(self.indices) > 0:\n            index_sample = next(iter(self.indices))\n        else:\n            index_sample = None\n        name_sample = names[0]\n        if isinstance(index_sample, tuple):\n            if not isinstance(name_sample, tuple):\n                msg = 'must supply a tuple to get_group with multiple grouping keys'\n                raise ValueError(msg)\n            if not len(name_sample) == len(index_sample):\n                try:\n                    return [self.indices[name] for name in names]\n                except KeyError as err:\n                    msg = 'must supply a same-length tuple to get_group with multiple grouping keys'\n                    raise ValueError(msg) from err\n            converters = [get_converter(s) for s in index_sample]\n            names = (tuple((f(n) for (f, n) in zip(converters, name))) for name in names)\n        else:\n            converter = get_converter(index_sample)\n            names = (converter(name) for name in names)\n        return [self.indices.get(name, []) for name in names]\n\n    def _nth(self, n: PositionalIndexer | tuple, dropna: Literal['any', 'all', None]=None) -> NDFrameT:\n        if not dropna:\n            mask = self._make_mask_from_positional_indexer(n)\n            (ids, _, _) = self._grouper.group_info\n            mask = mask & (ids != -1)\n            out = self._mask_selected_obj(mask)\n            return out\n        if not is_integer(n):\n            raise ValueError('dropna option only supported for an integer argument')\n        if dropna not in ['any', 'all']:\n            raise ValueError(f\"For a DataFrame or Series groupby.nth, dropna must be either None, 'any' or 'all', (was passed {dropna}).\")\n        n = cast(int, n)\n        dropped = self._selected_obj.dropna(how=dropna, axis=self.axis)\n        grouper: np.ndarray | Index | ops.BaseGrouper\n        if len(dropped) == len(self._selected_obj):\n            grouper = self._grouper\n        else:\n            axis = self._grouper.axis\n            grouper = self._grouper.codes_info[axis.isin(dropped.index)]\n            if self._grouper.has_dropped_na:\n                nulls = grouper == -1\n                values = np.where(nulls, NA, grouper)\n                grouper = Index(values, dtype='Int64')\n        if self.axis == 1:\n            grb = dropped.T.groupby(grouper, as_index=self.as_index, sort=self.sort)\n        else:\n            grb = dropped.groupby(grouper, as_index=self.as_index, sort=self.sort)\n        return grb.nth(n)\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def any(self, skipna: bool=True) -> NDFrameT:\n        \"\"\"\n        Return True if any value in the group is truthful, else False.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing.\n\n        Returns\n        -------\n        Series or DataFrame\n            DataFrame or Series of boolean values, where a value is True if any element\n            is True within its respective group, False otherwise.\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([1, 2, 0], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    0\n        dtype: int64\n        >>> ser.groupby(level=0).any()\n        a     True\n        b    False\n        dtype: bool\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 0, 3], [1, 0, 6], [7, 1, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"ostrich\", \"penguin\", \"parrot\"])\n        >>> df\n                 a  b  c\n        ostrich  1  0  3\n        penguin  1  0  6\n        parrot   7  1  9\n        >>> df.groupby(by=[\"a\"]).any()\n               b      c\n        a\n        1  False   True\n        7   True   True\n        \"\"\"\n        return self._cython_agg_general('any', alt=lambda x: Series(x, copy=False).any(skipna=skipna), skipna=skipna)\n\n    @final\n    def _value_counts(self, subset: Sequence[Hashable] | None=None, normalize: bool=False, sort: bool=True, ascending: bool=False, dropna: bool=True) -> DataFrame | Series:\n        \"\"\"\n        Shared implementation of value_counts for SeriesGroupBy and DataFrameGroupBy.\n\n        SeriesGroupBy additionally supports a bins argument. See the docstring of\n        DataFrameGroupBy.value_counts for a description of arguments.\n        \"\"\"\n        if self.axis == 1:\n            raise NotImplementedError('DataFrameGroupBy.value_counts only handles axis=0')\n        name = 'proportion' if normalize else 'count'\n        df = self.obj\n        obj = self._obj_with_exclusions\n        in_axis_names = {grouping.name for grouping in self._grouper.groupings if grouping.in_axis}\n        if isinstance(obj, Series):\n            _name = obj.name\n            keys = [] if _name in in_axis_names else [obj]\n        else:\n            unique_cols = set(obj.columns)\n            if subset is not None:\n                subsetted = set(subset)\n                clashing = subsetted & set(in_axis_names)\n                if clashing:\n                    raise ValueError(f'Keys {clashing} in subset cannot be in the groupby column keys.')\n                doesnt_exist = subsetted - unique_cols\n                if doesnt_exist:\n                    raise ValueError(f'Keys {doesnt_exist} in subset do not exist in the DataFrame.')\n            else:\n                subsetted = unique_cols\n            keys = [obj.iloc[:, idx] for (idx, _name) in enumerate(obj.columns) if _name not in in_axis_names and _name in subsetted]\n        groupings = list(self._grouper.groupings)\n        for key in keys:\n            (grouper, _, _) = get_grouper(df, key=key, axis=self.axis, sort=self.sort, observed=False, dropna=dropna)\n            groupings += list(grouper.groupings)\n        gb = df.groupby(groupings, sort=self.sort, observed=self.observed, dropna=self.dropna)\n        result_series = cast(Series, gb.size())\n        result_series.name = name\n        if any((isinstance(grouping.grouping_vector, (Categorical, CategoricalIndex)) and (not grouping._observed) for grouping in groupings)):\n            levels_list = [ping._result_index for ping in groupings]\n            multi_index = MultiIndex.from_product(levels_list, names=[ping.name for ping in groupings])\n            result_series = result_series.reindex(multi_index, fill_value=0)\n        if sort:\n            result_series = result_series.sort_values(ascending=ascending, kind='stable')\n        if self.sort:\n            names = result_series.index.names\n            result_series.index.names = range(len(names))\n            index_level = list(range(len(self._grouper.groupings)))\n            result_series = result_series.sort_index(level=index_level, sort_remaining=False)\n            result_series.index.names = names\n        if normalize:\n            levels = list(range(len(self._grouper.groupings), result_series.index.nlevels))\n            indexed_group_size = result_series.groupby(result_series.index.droplevel(levels), sort=self.sort, dropna=self.dropna, observed=False).transform('sum')\n            result_series /= indexed_group_size\n            result_series = result_series.fillna(0.0)\n        result: Series | DataFrame\n        if self.as_index:\n            result = result_series\n        else:\n            index = result_series.index\n            columns = com.fill_missing_names(index.names)\n            if name in columns:\n                raise ValueError(f\"Column label '{name}' is duplicate of result column\")\n            result_series.name = name\n            result_series.index = index.set_names(range(len(columns)))\n            result_frame = result_series.reset_index()\n            orig_dtype = self._grouper.groupings[0].obj.columns.dtype\n            cols = Index(columns, dtype=orig_dtype).insert(len(columns), name)\n            result_frame.columns = cols\n            result = result_frame\n        return result.__finalize__(self.obj, method='value_counts')\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def all(self, skipna: bool=True) -> NDFrameT:\n        \"\"\"\n        Return True if all values in the group are truthful, else False.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing.\n\n        Returns\n        -------\n        Series or DataFrame\n            DataFrame or Series of boolean values, where a value is True if all elements\n            are True within its respective group, False otherwise.\n        %(see_also)s\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([1, 2, 0], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    0\n        dtype: int64\n        >>> ser.groupby(level=0).all()\n        a     True\n        b    False\n        dtype: bool\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 0, 3], [1, 5, 6], [7, 8, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"ostrich\", \"penguin\", \"parrot\"])\n        >>> df\n                 a  b  c\n        ostrich  1  0  3\n        penguin  1  5  6\n        parrot   7  8  9\n        >>> df.groupby(by=[\"a\"]).all()\n               b      c\n        a\n        1  False   True\n        7   True   True\n        \"\"\"\n        return self._cython_agg_general('all', alt=lambda x: Series(x, copy=False).all(skipna=skipna), skipna=skipna)\n\n    @Appender(_apply_docs['template'].format(input='dataframe', examples=_apply_docs['dataframe_examples']))\n    def apply(self, func, *args, include_groups: bool=True, **kwargs) -> NDFrameT:\n        orig_func = func\n        func = com.is_builtin_func(func)\n        if orig_func != func:\n            alias = com._builtin_table_alias[orig_func]\n            warn_alias_replacement(self, orig_func, alias)\n        if isinstance(func, str):\n            if hasattr(self, func):\n                res = getattr(self, func)\n                if callable(res):\n                    return res(*args, **kwargs)\n                elif args or kwargs:\n                    raise ValueError(f'Cannot pass arguments to property {func}')\n                return res\n            else:\n                raise TypeError(f\"apply func should be callable, not '{func}'\")\n        elif args or kwargs:\n            if callable(func):\n\n                @wraps(func)\n                def f(g):\n                    return func(g, *args, **kwargs)\n            else:\n                raise ValueError('func must be a callable if args or kwargs are supplied')\n        else:\n            f = func\n        if not include_groups:\n            return self._python_apply_general(f, self._obj_with_exclusions)\n        with option_context('mode.chained_assignment', None):\n            try:\n                result = self._python_apply_general(f, self._selected_obj)\n                if not isinstance(self.obj, Series) and self._selection is None and (self._selected_obj.shape != self._obj_with_exclusions.shape):\n                    warnings.warn(message=_apply_groupings_depr.format(type(self).__name__, 'apply'), category=DeprecationWarning, stacklevel=find_stack_level())\n            except TypeError:\n                return self._python_apply_general(f, self._obj_with_exclusions)\n        return result\n\n    @final\n    def __len__(self) -> int:\n        return len(self.groups)\n\n    @final\n    def _cumcount_array(self, ascending: bool=True) -> np.ndarray:\n        \"\"\"\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Notes\n        -----\n        this is currently implementing sort=False\n        (though the default is sort=True) for groupby in general\n        \"\"\"\n        (ids, _, ngroups) = self._grouper.group_info\n        sorter = get_group_index_sorter(ids, ngroups)\n        (ids, count) = (ids[sorter], len(ids))\n        if count == 0:\n            return np.empty(0, dtype=np.int64)\n        run = np.r_[True, ids[:-1] != ids[1:]]\n        rep = np.diff(np.r_[np.nonzero(run)[0], count])\n        out = (~run).cumsum()\n        if ascending:\n            out -= np.repeat(out[run], rep)\n        else:\n            out = np.repeat(out[np.r_[run[1:], True]], rep) - out\n        if self._grouper.has_dropped_na:\n            out = np.where(ids == -1, np.nan, out.astype(np.float64, copy=False))\n        else:\n            out = out.astype(np.int64, copy=False)\n        rev = np.empty(count, dtype=np.intp)\n        rev[sorter] = np.arange(count, dtype=np.intp)\n        return out[rev]\n\n    @final\n    def _insert_inaxis_grouper(self, result: Series | DataFrame) -> DataFrame:\n        if isinstance(result, Series):\n            result = result.to_frame()\n        columns = result.columns\n        for (name, lev, in_axis) in zip(reversed(self._grouper.names), reversed(self._grouper.get_group_levels()), reversed([grp.in_axis for grp in self._grouper.groupings])):\n            if name not in columns:\n                if in_axis:\n                    result.insert(0, name, lev)\n                else:\n                    msg = 'A grouping was used that is not in the columns of the DataFrame and so was excluded from the result. This grouping will be included in a future version of pandas. Add the grouping as a column of the DataFrame to silence this warning.'\n                    warnings.warn(message=msg, category=FutureWarning, stacklevel=find_stack_level())\n        return result\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def tail(self, n: int=5) -> NDFrameT:\n        \"\"\"\n        Return last n rows of each group.\n\n        Similar to ``.apply(lambda x: x.tail(n))``, but it returns a subset of rows\n        from the original DataFrame with original index and order preserved\n        (``as_index`` flag is ignored).\n\n        Parameters\n        ----------\n        n : int\n            If positive: number of entries to include from end of each group.\n            If negative: number of entries to exclude from start of each group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Subset of original Series or DataFrame as determined by n.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A').tail(1)\n           A  B\n        1  a  2\n        3  b  2\n        >>> df.groupby('A').tail(-1)\n           A  B\n        1  a  2\n        3  b  2\n        \"\"\"\n        if n:\n            mask = self._make_mask_from_positional_indexer(slice(-n, None))\n        else:\n            mask = self._make_mask_from_positional_indexer([])\n        return self._mask_selected_obj(mask)\n\n    @final\n    def resample(self, rule, *args, include_groups: bool=True, **kwargs) -> Resampler:\n        \"\"\"\n        Provide resampling when using a TimeGrouper.\n\n        Given a grouper, the function resamples it according to a string\n        \"string\" -> \"frequency\".\n\n        See the :ref:`frequency aliases <timeseries.offset_aliases>`\n        documentation for more details.\n\n        Parameters\n        ----------\n        rule : str or DateOffset\n            The offset string or object representing target grouper conversion.\n        *args\n            Possible arguments are `how`, `fill_method`, `limit`, `kind` and\n            `on`, and other arguments of `TimeGrouper`.\n        include_groups : bool, default True\n            When True, will attempt to include the groupings in the operation in\n            the case that they are columns of the DataFrame. If this raises a\n            TypeError, the result will be computed with the groupings excluded.\n            When False, the groupings will be excluded when applying ``func``.\n\n            .. versionadded:: 2.2.0\n\n            .. deprecated:: 2.2.0\n\n               Setting include_groups to True is deprecated. Only the value\n               False will be allowed in a future version of pandas.\n\n        **kwargs\n            Possible arguments are `how`, `fill_method`, `limit`, `kind` and\n            `on`, and other arguments of `TimeGrouper`.\n\n        Returns\n        -------\n        pandas.api.typing.DatetimeIndexResamplerGroupby,\n        pandas.api.typing.PeriodIndexResamplerGroupby, or\n        pandas.api.typing.TimedeltaIndexResamplerGroupby\n            Return a new groupby object, with type depending on the data\n            being resampled.\n\n        See Also\n        --------\n        Grouper : Specify a frequency to resample with when\n            grouping by a key.\n        DatetimeIndex.resample : Frequency conversion and resampling of\n            time series.\n\n        Examples\n        --------\n        >>> idx = pd.date_range('1/1/2000', periods=4, freq='min')\n        >>> df = pd.DataFrame(data=4 * [range(2)],\n        ...                   index=idx,\n        ...                   columns=['a', 'b'])\n        >>> df.iloc[2, 0] = 5\n        >>> df\n                            a  b\n        2000-01-01 00:00:00  0  1\n        2000-01-01 00:01:00  0  1\n        2000-01-01 00:02:00  5  1\n        2000-01-01 00:03:00  0  1\n\n        Downsample the DataFrame into 3 minute bins and sum the values of\n        the timestamps falling into a bin.\n\n        >>> df.groupby('a').resample('3min', include_groups=False).sum()\n                                 b\n        a\n        0   2000-01-01 00:00:00  2\n            2000-01-01 00:03:00  1\n        5   2000-01-01 00:00:00  1\n\n        Upsample the series into 30 second bins.\n\n        >>> df.groupby('a').resample('30s', include_groups=False).sum()\n                            b\n        a\n        0   2000-01-01 00:00:00  1\n            2000-01-01 00:00:30  0\n            2000-01-01 00:01:00  1\n            2000-01-01 00:01:30  0\n            2000-01-01 00:02:00  0\n            2000-01-01 00:02:30  0\n            2000-01-01 00:03:00  1\n        5   2000-01-01 00:02:00  1\n\n        Resample by month. Values are assigned to the month of the period.\n\n        >>> df.groupby('a').resample('ME', include_groups=False).sum()\n                    b\n        a\n        0   2000-01-31  3\n        5   2000-01-31  1\n\n        Downsample the series into 3 minute bins as above, but close the right\n        side of the bin interval.\n\n        >>> (\n        ...     df.groupby('a')\n        ...     .resample('3min', closed='right', include_groups=False)\n        ...     .sum()\n        ... )\n                                 b\n        a\n        0   1999-12-31 23:57:00  1\n            2000-01-01 00:00:00  2\n        5   2000-01-01 00:00:00  1\n\n        Downsample the series into 3 minute bins and close the right side of\n        the bin interval, but label each bin using the right edge instead of\n        the left.\n\n        >>> (\n        ...     df.groupby('a')\n        ...     .resample('3min', closed='right', label='right', include_groups=False)\n        ...     .sum()\n        ... )\n                                 b\n        a\n        0   2000-01-01 00:00:00  1\n            2000-01-01 00:03:00  2\n        5   2000-01-01 00:03:00  1\n        \"\"\"\n        from pandas.core.resample import get_resampler_for_grouping\n        return get_resampler_for_grouping(self, rule, *args, include_groups=include_groups, **kwargs)\n\n    @final\n    def _wrap_transform_fast_result(self, result: NDFrameT) -> NDFrameT:\n        \"\"\"\n        Fast transform path for aggregations.\n        \"\"\"\n        obj = self._obj_with_exclusions\n        (ids, _, _) = self._grouper.group_info\n        result = result.reindex(self._grouper.result_index, axis=self.axis, copy=False)\n        if self.obj.ndim == 1:\n            out = algorithms.take_nd(result._values, ids)\n            output = obj._constructor(out, index=obj.index, name=obj.name)\n        else:\n            axis = 0 if result.ndim == 1 else self.axis\n            new_ax = result.axes[axis].take(ids)\n            output = result._reindex_with_indexers({axis: (new_ax, ids)}, allow_dups=True, copy=False)\n            output = output.set_axis(obj._get_axis(self.axis), axis=axis)\n        return output\n\n    @final\n    def _agg_general(self, numeric_only: bool=False, min_count: int=-1, *, alias: str, npfunc: Callable | None=None, **kwargs):\n        result = self._cython_agg_general(how=alias, alt=npfunc, numeric_only=numeric_only, min_count=min_count, **kwargs)\n        return result.__finalize__(self.obj, method='groupby')\n\n    @final\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def expanding(self, *args, **kwargs) -> ExpandingGroupby:\n        \"\"\"\n        Return an expanding grouper, providing expanding\n        functionality per group.\n\n        Returns\n        -------\n        pandas.api.typing.ExpandingGroupby\n        \"\"\"\n        from pandas.core.window import ExpandingGroupby\n        return ExpandingGroupby(self._selected_obj, *args, _grouper=self._grouper, **kwargs)\n\n    @final\n    def quantile(self, q: float | AnyArrayLike=0.5, interpolation: str='linear', numeric_only: bool=False):\n        \"\"\"\n        Return group values at the given quantile, a la numpy.percentile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            Value(s) between 0 and 1 providing the quantile(s) to compute.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            Method to use when the desired quantile falls between two points.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only now defaults to ``False``.\n\n        Returns\n        -------\n        Series or DataFrame\n            Return type determined by caller of GroupBy object.\n\n        See Also\n        --------\n        Series.quantile : Similar method for Series.\n        DataFrame.quantile : Similar method for DataFrame.\n        numpy.percentile : NumPy method to compute qth percentile.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([\n        ...     ['a', 1], ['a', 2], ['a', 3],\n        ...     ['b', 1], ['b', 3], ['b', 5]\n        ... ], columns=['key', 'val'])\n        >>> df.groupby('key').quantile()\n            val\n        key\n        a    2.0\n        b    3.0\n        \"\"\"\n        mgr = self._get_data_to_aggregate(numeric_only=numeric_only, name='quantile')\n        obj = self._wrap_agged_manager(mgr)\n        if self.axis == 1:\n            splitter = self._grouper._get_splitter(obj.T, axis=self.axis)\n            sdata = splitter._sorted_data.T\n        else:\n            splitter = self._grouper._get_splitter(obj, axis=self.axis)\n            sdata = splitter._sorted_data\n        (starts, ends) = lib.generate_slices(splitter._slabels, splitter.ngroups)\n\n        def pre_processor(vals: ArrayLike) -> tuple[np.ndarray, DtypeObj | None]:\n            if is_object_dtype(vals.dtype):\n                raise TypeError(\"'quantile' cannot be performed against 'object' dtypes!\")\n            inference: DtypeObj | None = None\n            if isinstance(vals, BaseMaskedArray) and is_numeric_dtype(vals.dtype):\n                out = vals.to_numpy(dtype=float, na_value=np.nan)\n                inference = vals.dtype\n            elif is_integer_dtype(vals.dtype):\n                if isinstance(vals, ExtensionArray):\n                    out = vals.to_numpy(dtype=float, na_value=np.nan)\n                else:\n                    out = vals\n                inference = np.dtype(np.int64)\n            elif is_bool_dtype(vals.dtype) and isinstance(vals, ExtensionArray):\n                out = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_bool_dtype(vals.dtype):\n                warnings.warn(f'Allowing bool dtype in {type(self).__name__}.quantile is deprecated and will raise in a future version, matching the Series/DataFrame behavior. Cast to uint8 dtype before calling quantile instead.', FutureWarning, stacklevel=find_stack_level())\n                out = np.asarray(vals)\n            elif needs_i8_conversion(vals.dtype):\n                inference = vals.dtype\n                return (vals, inference)\n            elif isinstance(vals, ExtensionArray) and is_float_dtype(vals.dtype):\n                inference = np.dtype(np.float64)\n                out = vals.to_numpy(dtype=float, na_value=np.nan)\n            else:\n                out = np.asarray(vals)\n            return (out, inference)\n\n        def post_processor(vals: np.ndarray, inference: DtypeObj | None, result_mask: np.ndarray | None, orig_vals: ArrayLike) -> ArrayLike:\n            if inference:\n                if isinstance(orig_vals, BaseMaskedArray):\n                    assert result_mask is not None\n                    if interpolation in {'linear', 'midpoint'} and (not is_float_dtype(orig_vals)):\n                        return FloatingArray(vals, result_mask)\n                    else:\n                        with warnings.catch_warnings():\n                            warnings.filterwarnings('ignore', category=RuntimeWarning)\n                            return type(orig_vals)(vals.astype(inference.numpy_dtype), result_mask)\n                elif not (is_integer_dtype(inference) and interpolation in {'linear', 'midpoint'}):\n                    if needs_i8_conversion(inference):\n                        vals = vals.astype('i8').view(orig_vals._ndarray.dtype)\n                        return orig_vals._from_backing_data(vals)\n                    assert isinstance(inference, np.dtype)\n                    return vals.astype(inference)\n            return vals\n        qs = np.array(q, dtype=np.float64)\n        pass_qs: np.ndarray | None = qs\n        if is_scalar(q):\n            qs = np.array([q], dtype=np.float64)\n            pass_qs = None\n        (ids, _, ngroups) = self._grouper.group_info\n        nqs = len(qs)\n        func = partial(libgroupby.group_quantile, labels=ids, qs=qs, interpolation=interpolation, starts=starts, ends=ends)\n\n        def blk_func(values: ArrayLike) -> ArrayLike:\n            orig_vals = values\n            if isinstance(values, BaseMaskedArray):\n                mask = values._mask\n                result_mask = np.zeros((ngroups, nqs), dtype=np.bool_)\n            else:\n                mask = isna(values)\n                result_mask = None\n            is_datetimelike = needs_i8_conversion(values.dtype)\n            (vals, inference) = pre_processor(values)\n            ncols = 1\n            if vals.ndim == 2:\n                ncols = vals.shape[0]\n            out = np.empty((ncols, ngroups, nqs), dtype=np.float64)\n            if is_datetimelike:\n                vals = vals.view('i8')\n            if vals.ndim == 1:\n                func(out[0], values=vals, mask=mask, result_mask=result_mask, is_datetimelike=is_datetimelike)\n            else:\n                for i in range(ncols):\n                    func(out[i], values=vals[i], mask=mask[i], result_mask=None, is_datetimelike=is_datetimelike)\n            if vals.ndim == 1:\n                out = out.ravel('K')\n                if result_mask is not None:\n                    result_mask = result_mask.ravel('K')\n            else:\n                out = out.reshape(ncols, ngroups * nqs)\n            return post_processor(out, inference, result_mask, orig_vals)\n        res_mgr = sdata._mgr.grouped_reduce(blk_func)\n        res = self._wrap_agged_manager(res_mgr)\n        return self._wrap_aggregated_output(res, qs=pass_qs)\n\n    def _cython_transform(self, how: str, numeric_only: bool=False, axis: AxisInt=0, **kwargs):\n        raise AbstractMethodError(self)\n\n    def _wrap_applied_output(self, data, values: list, not_indexed_same: bool=False, is_transform: bool=False):\n        raise AbstractMethodError(self)\n\n    @final\n    def _deprecate_axis(self, axis: int, name: str) -> None:\n        if axis == 1:\n            warnings.warn(f'{type(self).__name__}.{name} with axis=1 is deprecated and will be removed in a future version. Operate on the un-grouped DataFrame instead', FutureWarning, stacklevel=find_stack_level())\n        else:\n            warnings.warn(f\"The 'axis' keyword in {type(self).__name__}.{name} is deprecated and will be removed in a future version. Call without passing 'axis' instead.\", FutureWarning, stacklevel=find_stack_level())\n\n    @final\n    def _set_result_index_ordered(self, result: OutputFrameOrSeries) -> OutputFrameOrSeries:\n        obj_axis = self.obj._get_axis(self.axis)\n        if self._grouper.is_monotonic and (not self._grouper.has_dropped_na):\n            result = result.set_axis(obj_axis, axis=self.axis, copy=False)\n            return result\n        original_positions = Index(self._grouper.result_ilocs())\n        result = result.set_axis(original_positions, axis=self.axis, copy=False)\n        result = result.sort_index(axis=self.axis)\n        if self._grouper.has_dropped_na:\n            result = result.reindex(RangeIndex(len(obj_axis)), axis=self.axis)\n        result = result.set_axis(obj_axis, axis=self.axis, copy=False)\n        return result\n\n    @final\n    @doc(_groupby_agg_method_engine_template, fname='sum', no=False, mc=0, e=None, ek=None, example=dedent('        For SeriesGroupBy:\\n\\n        >>> lst = [\\'a\\', \\'a\\', \\'b\\', \\'b\\']\\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\\n        >>> ser\\n        a    1\\n        a    2\\n        b    3\\n        b    4\\n        dtype: int64\\n        >>> ser.groupby(level=0).sum()\\n        a    3\\n        b    7\\n        dtype: int64\\n\\n        For DataFrameGroupBy:\\n\\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\\n        ...                   index=[\"tiger\", \"leopard\", \"cheetah\", \"lion\"])\\n        >>> df\\n                  a  b  c\\n          tiger   1  8  2\\n        leopard   1  2  5\\n        cheetah   2  5  8\\n           lion   2  6  9\\n        >>> df.groupby(\"a\").sum()\\n             b   c\\n        a\\n        1   10   7\\n        2   11  17'))\n    def sum(self, numeric_only: bool=False, min_count: int=0, engine: Literal['cython', 'numba'] | None=None, engine_kwargs: dict[str, bool] | None=None):\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_sum\n            return self._numba_agg_general(grouped_sum, executor.default_dtype_mapping, engine_kwargs, min_periods=min_count)\n        else:\n            with com.temp_setattr(self, 'observed', True):\n                result = self._agg_general(numeric_only=numeric_only, min_count=min_count, alias='sum', npfunc=np.sum)\n            return self._reindex_output(result, fill_value=0)\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def head(self, n: int=5) -> NDFrameT:\n        \"\"\"\n        Return first n rows of each group.\n\n        Similar to ``.apply(lambda x: x.head(n))``, but it returns a subset of rows\n        from the original DataFrame with original index and order preserved\n        (``as_index`` flag is ignored).\n\n        Parameters\n        ----------\n        n : int\n            If positive: number of entries to include from start of each group.\n            If negative: number of entries to exclude from end of each group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Subset of original Series or DataFrame as determined by n.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A').head(1)\n           A  B\n        0  1  2\n        2  5  6\n        >>> df.groupby('A').head(-1)\n           A  B\n        0  1  2\n        \"\"\"\n        mask = self._make_mask_from_positional_indexer(slice(None, n))\n        return self._mask_selected_obj(mask)\n\n    @final\n    def _fill(self, direction: Literal['ffill', 'bfill'], limit: int | None=None):\n        \"\"\"\n        Shared function for `pad` and `backfill` to call Cython method.\n\n        Parameters\n        ----------\n        direction : {'ffill', 'bfill'}\n            Direction passed to underlying Cython function. `bfill` will cause\n            values to be filled backwards. `ffill` and any other values will\n            default to a forward fill\n        limit : int, default None\n            Maximum number of consecutive values to fill. If `None`, this\n            method will convert to -1 prior to passing to Cython\n\n        Returns\n        -------\n        `Series` or `DataFrame` with filled values\n\n        See Also\n        --------\n        pad : Returns Series with minimum number of char in object.\n        backfill : Backward fill the missing values in the dataset.\n        \"\"\"\n        if limit is None:\n            limit = -1\n        (ids, _, _) = self._grouper.group_info\n        sorted_labels = np.argsort(ids, kind='mergesort').astype(np.intp, copy=False)\n        if direction == 'bfill':\n            sorted_labels = sorted_labels[::-1]\n        col_func = partial(libgroupby.group_fillna_indexer, labels=ids, sorted_labels=sorted_labels, limit=limit, dropna=self.dropna)\n\n        def blk_func(values: ArrayLike) -> ArrayLike:\n            mask = isna(values)\n            if values.ndim == 1:\n                indexer = np.empty(values.shape, dtype=np.intp)\n                col_func(out=indexer, mask=mask)\n                return algorithms.take_nd(values, indexer)\n            else:\n                if isinstance(values, np.ndarray):\n                    dtype = values.dtype\n                    if self._grouper.has_dropped_na:\n                        dtype = ensure_dtype_can_hold_na(values.dtype)\n                    out = np.empty(values.shape, dtype=dtype)\n                else:\n                    out = type(values)._empty(values.shape, dtype=values.dtype)\n                for (i, value_element) in enumerate(values):\n                    indexer = np.empty(values.shape[1], dtype=np.intp)\n                    col_func(out=indexer, mask=mask[i])\n                    out[i, :] = algorithms.take_nd(value_element, indexer)\n                return out\n        mgr = self._get_data_to_aggregate()\n        res_mgr = mgr.apply(blk_func)\n        new_obj = self._wrap_agged_manager(res_mgr)\n        if self.axis == 1:\n            new_obj = new_obj.T\n            new_obj.columns = self.obj.columns\n        new_obj.index = self.obj.index\n        return new_obj\n\n    @final\n    @property\n    def _obj_1d_constructor(self) -> Callable:\n        if isinstance(self.obj, DataFrame):\n            return self.obj._constructor_sliced\n        assert isinstance(self.obj, Series)\n        return self.obj._constructor\n\n    @final\n    def _reindex_output(self, output: OutputFrameOrSeries, fill_value: Scalar=np.nan, qs: npt.NDArray[np.float64] | None=None) -> OutputFrameOrSeries:\n        \"\"\"\n        If we have categorical groupers, then we might want to make sure that\n        we have a fully re-indexed output to the levels. This means expanding\n        the output space to accommodate all values in the cartesian product of\n        our groups, regardless of whether they were observed in the data or\n        not. This will expand the output space if there are missing groups.\n\n        The method returns early without modifying the input if the number of\n        groupings is less than 2, self.observed == True or none of the groupers\n        are categorical.\n\n        Parameters\n        ----------\n        output : Series or DataFrame\n            Object resulting from grouping and applying an operation.\n        fill_value : scalar, default np.nan\n            Value to use for unobserved categories if self.observed is False.\n        qs : np.ndarray[float64] or None, default None\n            quantile values, only relevant for quantile.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object (potentially) re-indexed to include all possible groups.\n        \"\"\"\n        groupings = self._grouper.groupings\n        if len(groupings) == 1:\n            return output\n        elif self.observed:\n            return output\n        elif not any((isinstance(ping.grouping_vector, (Categorical, CategoricalIndex)) for ping in groupings)):\n            return output\n        levels_list = [ping._group_index for ping in groupings]\n        names = self._grouper.names\n        if qs is not None:\n            levels_list.append(qs)\n            names = names + [None]\n        index = MultiIndex.from_product(levels_list, names=names)\n        if self.sort:\n            index = index.sort_values()\n        if self.as_index:\n            d = {self.obj._get_axis_name(self.axis): index, 'copy': False, 'fill_value': fill_value}\n            return output.reindex(**d)\n        in_axis_grps = [(i, ping.name) for (i, ping) in enumerate(groupings) if ping.in_axis]\n        if len(in_axis_grps) > 0:\n            (g_nums, g_names) = zip(*in_axis_grps)\n            output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self._grouper.result_index).reindex(index, copy=False, fill_value=fill_value)\n        if len(in_axis_grps) > 0:\n            output = output.reset_index(level=g_nums)\n        return output.reset_index(drop=True)\n\n    @final\n    @doc(_groupby_agg_method_engine_template, fname='max', no=False, mc=-1, e=None, ek=None, example=dedent('        For SeriesGroupBy:\\n\\n        >>> lst = [\\'a\\', \\'a\\', \\'b\\', \\'b\\']\\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\\n        >>> ser\\n        a    1\\n        a    2\\n        b    3\\n        b    4\\n        dtype: int64\\n        >>> ser.groupby(level=0).max()\\n        a    2\\n        b    4\\n        dtype: int64\\n\\n        For DataFrameGroupBy:\\n\\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\\n        ...                   index=[\"tiger\", \"leopard\", \"cheetah\", \"lion\"])\\n        >>> df\\n                  a  b  c\\n          tiger   1  8  2\\n        leopard   1  2  5\\n        cheetah   2  5  8\\n           lion   2  6  9\\n        >>> df.groupby(\"a\").max()\\n            b  c\\n        a\\n        1   8  5\\n        2   6  9'))\n    def max(self, numeric_only: bool=False, min_count: int=-1, engine: Literal['cython', 'numba'] | None=None, engine_kwargs: dict[str, bool] | None=None):\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_min_max\n            return self._numba_agg_general(grouped_min_max, executor.identity_dtype_mapping, engine_kwargs, min_periods=min_count, is_max=True)\n        else:\n            return self._agg_general(numeric_only=numeric_only, min_count=min_count, alias='max', npfunc=np.max)\n\n    @final\n    def _transform(self, func, *args, engine=None, engine_kwargs=None, **kwargs):\n        orig_func = func\n        func = com.get_cython_func(func) or func\n        if orig_func != func:\n            warn_alias_replacement(self, orig_func, func)\n        if not isinstance(func, str):\n            return self._transform_general(func, engine, engine_kwargs, *args, **kwargs)\n        elif func not in base.transform_kernel_allowlist:\n            msg = f\"'{func}' is not a valid function name for transform(name)\"\n            raise ValueError(msg)\n        elif func in base.cythonized_kernels or func in base.transformation_kernels:\n            if engine is not None:\n                kwargs['engine'] = engine\n                kwargs['engine_kwargs'] = engine_kwargs\n            return getattr(self, func)(*args, **kwargs)\n        else:\n            with com.temp_setattr(self, 'as_index', True):\n                if func in ['idxmin', 'idxmax']:\n                    func = cast(Literal['idxmin', 'idxmax'], func)\n                    result = self._idxmax_idxmin(func, True, *args, **kwargs)\n                else:\n                    if engine is not None:\n                        kwargs['engine'] = engine\n                        kwargs['engine_kwargs'] = engine_kwargs\n                    result = getattr(self, func)(*args, **kwargs)\n            return self._wrap_transform_fast_result(result)\n\n    @final\n    def _wrap_aggregated_output(self, result: Series | DataFrame, qs: npt.NDArray[np.float64] | None=None):\n        \"\"\"\n        Wraps the output of GroupBy aggregations into the expected result.\n\n        Parameters\n        ----------\n        result : Series, DataFrame\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        if not self.as_index:\n            result = self._insert_inaxis_grouper(result)\n            result = result._consolidate()\n            index = Index(range(self._grouper.ngroups))\n        else:\n            index = self._grouper.result_index\n        if qs is not None:\n            index = _insert_quantile_level(index, qs)\n        result.index = index\n        res = self._maybe_transpose_result(result)\n        return self._reindex_output(res, qs=qs)\n\n    @final\n    @Substitution(name='groupby')\n    def ffill(self, limit: int | None=None):\n        \"\"\"\n        Forward fill the values.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        Series.ffill: Returns Series with minimum number of char in object.\n        DataFrame.ffill: Object with missing values filled or None if inplace=True.\n        Series.fillna: Fill NaN values of a Series.\n        DataFrame.fillna: Fill NaN values of a DataFrame.\n\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> key = [0, 0, 1, 1]\n        >>> ser = pd.Series([np.nan, 2, 3, np.nan], index=key)\n        >>> ser\n        0    NaN\n        0    2.0\n        1    3.0\n        1    NaN\n        dtype: float64\n        >>> ser.groupby(level=0).ffill()\n        0    NaN\n        0    2.0\n        1    3.0\n        1    3.0\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"key\": [0, 0, 1, 1, 1],\n        ...         \"A\": [np.nan, 2, np.nan, 3, np.nan],\n        ...         \"B\": [2, 3, np.nan, np.nan, np.nan],\n        ...         \"C\": [np.nan, np.nan, 2, np.nan, np.nan],\n        ...     }\n        ... )\n        >>> df\n           key    A    B   C\n        0    0  NaN  2.0 NaN\n        1    0  2.0  3.0 NaN\n        2    1  NaN  NaN 2.0\n        3    1  3.0  NaN NaN\n        4    1  NaN  NaN NaN\n\n        Propagate non-null values forward or backward within each group along columns.\n\n        >>> df.groupby(\"key\").ffill()\n             A    B   C\n        0  NaN  2.0 NaN\n        1  2.0  3.0 NaN\n        2  NaN  NaN 2.0\n        3  3.0  NaN 2.0\n        4  3.0  NaN 2.0\n\n        Propagate non-null values forward or backward within each group along rows.\n\n        >>> df.T.groupby(np.array([0, 0, 1, 1])).ffill().T\n           key    A    B    C\n        0  0.0  0.0  2.0  2.0\n        1  0.0  2.0  3.0  3.0\n        2  1.0  1.0  NaN  2.0\n        3  1.0  3.0  NaN  NaN\n        4  1.0  1.0  NaN  NaN\n\n        Only replace the first NaN element within a group along rows.\n\n        >>> df.groupby(\"key\").ffill(limit=1)\n             A    B    C\n        0  NaN  2.0  NaN\n        1  2.0  3.0  NaN\n        2  NaN  NaN  2.0\n        3  3.0  NaN  2.0\n        4  3.0  NaN  NaN\n        \"\"\"\n        return self._fill('ffill', limit=limit)\n\n    @final\n    def _python_apply_general(self, f: Callable, data: DataFrame | Series, not_indexed_same: bool | None=None, is_transform: bool=False, is_agg: bool=False) -> NDFrameT:\n        \"\"\"\n        Apply function f in python space\n\n        Parameters\n        ----------\n        f : callable\n            Function to apply\n        data : Series or DataFrame\n            Data to apply f to\n        not_indexed_same: bool, optional\n            When specified, overrides the value of not_indexed_same. Apply behaves\n            differently when the result index is equal to the input index, but\n            this can be coincidental leading to value-dependent behavior.\n        is_transform : bool, default False\n            Indicator for whether the function is actually a transform\n            and should not have group keys prepended.\n        is_agg : bool, default False\n            Indicator for whether the function is an aggregation. When the\n            result is empty, we don't want to warn for this case.\n            See _GroupBy._python_agg_general.\n\n        Returns\n        -------\n        Series or DataFrame\n            data after applying f\n        \"\"\"\n        (values, mutated) = self._grouper.apply_groupwise(f, data, self.axis)\n        if not_indexed_same is None:\n            not_indexed_same = mutated\n        return self._wrap_applied_output(data, values, not_indexed_same, is_transform)\n\n    @final\n    @Substitution(name='groupby')\n    def cumcount(self, ascending: bool=True):\n        \"\"\"\n        Number each item in each group from 0 to the length of that group - 1.\n\n        Essentially this is equivalent to\n\n        .. code-block:: python\n\n            self.apply(lambda x: pd.Series(np.arange(len(x)), x.index))\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Returns\n        -------\n        Series\n            Sequence number of each element within each group.\n\n        See Also\n        --------\n        .ngroup : Number the groups themselves.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],\n        ...                   columns=['A'])\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').cumcount()\n        0    0\n        1    1\n        2    2\n        3    0\n        4    1\n        5    3\n        dtype: int64\n        >>> df.groupby('A').cumcount(ascending=False)\n        0    3\n        1    2\n        2    1\n        3    1\n        4    0\n        5    0\n        dtype: int64\n        \"\"\"\n        index = self._obj_with_exclusions._get_axis(self.axis)\n        cumcounts = self._cumcount_array(ascending=ascending)\n        return self._obj_1d_constructor(cumcounts, index)\n\n    @final\n    def rolling(self, *args, **kwargs) -> RollingGroupby:\n        \"\"\"\n        Return a rolling grouper, providing rolling functionality per group.\n\n        Parameters\n        ----------\n        window : int, timedelta, str, offset, or BaseIndexer subclass\n            Size of the moving window.\n\n            If an integer, the fixed number of observations used for\n            each window.\n\n            If a timedelta, str, or offset, the time period of each window. Each\n            window will be a variable sized based on the observations included in\n            the time-period. This is only valid for datetimelike indexes.\n            To learn more about the offsets & frequency strings, please see `this link\n            <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\n            If a BaseIndexer subclass, the window boundaries\n            based on the defined ``get_window_bounds`` method. Additional rolling\n            keyword arguments, namely ``min_periods``, ``center``, ``closed`` and\n            ``step`` will be passed to ``get_window_bounds``.\n\n        min_periods : int, default None\n            Minimum number of observations in window required to have a value;\n            otherwise, result is ``np.nan``.\n\n            For a window that is specified by an offset,\n            ``min_periods`` will default to 1.\n\n            For a window that is specified by an integer, ``min_periods`` will default\n            to the size of the window.\n\n        center : bool, default False\n            If False, set the window labels as the right edge of the window index.\n\n            If True, set the window labels as the center of the window index.\n\n        win_type : str, default None\n            If ``None``, all points are evenly weighted.\n\n            If a string, it must be a valid `scipy.signal window function\n            <https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows>`__.\n\n            Certain Scipy window types require additional parameters to be passed\n            in the aggregation function. The additional parameters must match\n            the keywords specified in the Scipy window type method signature.\n\n        on : str, optional\n            For a DataFrame, a column label or Index level on which\n            to calculate the rolling window, rather than the DataFrame's index.\n\n            Provided integer column is ignored and excluded from result since\n            an integer index is not used to calculate the rolling window.\n\n        axis : int or str, default 0\n            If ``0`` or ``'index'``, roll across the rows.\n\n            If ``1`` or ``'columns'``, roll across the columns.\n\n            For `Series` this parameter is unused and defaults to 0.\n\n        closed : str, default None\n            If ``'right'``, the first point in the window is excluded from calculations.\n\n            If ``'left'``, the last point in the window is excluded from calculations.\n\n            If ``'both'``, no points in the window are excluded from calculations.\n\n            If ``'neither'``, the first and last points in the window are excluded\n            from calculations.\n\n            Default ``None`` (``'right'``).\n\n        method : str {'single', 'table'}, default 'single'\n            Execute the rolling operation per single column or row (``'single'``)\n            or over the entire object (``'table'``).\n\n            This argument is only implemented when specifying ``engine='numba'``\n            in the method call.\n\n        Returns\n        -------\n        pandas.api.typing.RollingGroupby\n            Return a new grouper with our rolling appended.\n\n        See Also\n        --------\n        Series.rolling : Calling object with Series data.\n        DataFrame.rolling : Calling object with DataFrames.\n        Series.groupby : Apply a function groupby to a Series.\n        DataFrame.groupby : Apply a function groupby.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 2],\n        ...                    'B': [1, 2, 3, 4],\n        ...                    'C': [0.362, 0.227, 1.267, -0.562]})\n        >>> df\n              A  B      C\n        0     1  1  0.362\n        1     1  2  0.227\n        2     2  3  1.267\n        3     2  4 -0.562\n\n        >>> df.groupby('A').rolling(2).sum()\n            B      C\n        A\n        1 0  NaN    NaN\n          1  3.0  0.589\n        2 2  NaN    NaN\n          3  7.0  0.705\n\n        >>> df.groupby('A').rolling(2, min_periods=1).sum()\n            B      C\n        A\n        1 0  1.0  0.362\n          1  3.0  0.589\n        2 2  3.0  1.267\n          3  7.0  0.705\n\n        >>> df.groupby('A').rolling(2, on='B').sum()\n            B      C\n        A\n        1 0  1    NaN\n          1  2  0.589\n        2 2  3    NaN\n          3  4  0.705\n        \"\"\"\n        from pandas.core.window import RollingGroupby\n        return RollingGroupby(self._selected_obj, *args, _grouper=self._grouper, _as_index=self.as_index, **kwargs)\n\n    @final\n    def _mask_selected_obj(self, mask: npt.NDArray[np.bool_]) -> NDFrameT:\n        \"\"\"\n        Return _selected_obj with mask applied to the correct axis.\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool]\n            Boolean mask to apply.\n\n        Returns\n        -------\n        Series or DataFrame\n            Filtered _selected_obj.\n        \"\"\"\n        ids = self._grouper.group_info[0]\n        mask = mask & (ids != -1)\n        if self.axis == 0:\n            return self._selected_obj[mask]\n        else:\n            return self._selected_obj.iloc[:, mask]\n\n    @final\n    def _get_index(self, name):\n        \"\"\"\n        Safe get index, translate keys for datelike to underlying repr.\n        \"\"\"\n        return self._get_indices([name])[0]\n\n    def __getattr__(self, attr: str):\n        if attr in self._internal_names_set:\n            return object.__getattribute__(self, attr)\n        if attr in self.obj:\n            return self[attr]\n        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{attr}'\")\n\n    @final\n    def sample(self, n: int | None=None, frac: float | None=None, replace: bool=False, weights: Sequence | Series | None=None, random_state: RandomState | None=None):\n        \"\"\"\n        Return a random sample of items from each group.\n\n        You can use `random_state` for reproducibility.\n\n        Parameters\n        ----------\n        n : int, optional\n            Number of items to return for each group. Cannot be used with\n            `frac` and must be no larger than the smallest group unless\n            `replace` is True. Default is one if `frac` is None.\n        frac : float, optional\n            Fraction of items to return. Cannot be used with `n`.\n        replace : bool, default False\n            Allow or disallow sampling of the same row more than once.\n        weights : list-like, optional\n            Default None results in equal probability weighting.\n            If passed a list-like then values must have the same length as\n            the underlying DataFrame or Series object and will be used as\n            sampling probabilities after normalization within each group.\n            Values must be non-negative with at least one positive element\n            within each group.\n        random_state : int, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optional\n            If int, array-like, or BitGenerator, seed for random number generator.\n            If np.random.RandomState or np.random.Generator, use as given.\n\n            .. versionchanged:: 1.4.0\n\n                np.random.Generator objects now accepted\n\n        Returns\n        -------\n        Series or DataFrame\n            A new object of same type as caller containing items randomly\n            sampled within each group from the caller object.\n\n        See Also\n        --------\n        DataFrame.sample: Generate random samples from a DataFrame object.\n        numpy.random.choice: Generate a random sample from a given 1-D numpy\n            array.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"a\": [\"red\"] * 2 + [\"blue\"] * 2 + [\"black\"] * 2, \"b\": range(6)}\n        ... )\n        >>> df\n               a  b\n        0    red  0\n        1    red  1\n        2   blue  2\n        3   blue  3\n        4  black  4\n        5  black  5\n\n        Select one row at random for each distinct value in column a. The\n        `random_state` argument can be used to guarantee reproducibility:\n\n        >>> df.groupby(\"a\").sample(n=1, random_state=1)\n               a  b\n        4  black  4\n        2   blue  2\n        1    red  1\n\n        Set `frac` to sample fixed proportions rather than counts:\n\n        >>> df.groupby(\"a\")[\"b\"].sample(frac=0.5, random_state=2)\n        5    5\n        2    2\n        0    0\n        Name: b, dtype: int64\n\n        Control sample probabilities within groups by setting weights:\n\n        >>> df.groupby(\"a\").sample(\n        ...     n=1,\n        ...     weights=[1, 1, 1, 0, 0, 1],\n        ...     random_state=1,\n        ... )\n               a  b\n        5  black  5\n        2   blue  2\n        0    red  0\n        \"\"\"\n        if self._selected_obj.empty:\n            return self._selected_obj\n        size = sample.process_sampling_size(n, frac, replace)\n        if weights is not None:\n            weights_arr = sample.preprocess_weights(self._selected_obj, weights, axis=self.axis)\n        random_state = com.random_state(random_state)\n        group_iterator = self._grouper.get_iterator(self._selected_obj, self.axis)\n        sampled_indices = []\n        for (labels, obj) in group_iterator:\n            grp_indices = self.indices[labels]\n            group_size = len(grp_indices)\n            if size is not None:\n                sample_size = size\n            else:\n                assert frac is not None\n                sample_size = round(frac * group_size)\n            grp_sample = sample.sample(group_size, size=sample_size, replace=replace, weights=None if weights is None else weights_arr[grp_indices], random_state=random_state)\n            sampled_indices.append(grp_indices[grp_sample])\n        sampled_indices = np.concatenate(sampled_indices)\n        return self._selected_obj.take(sampled_indices, axis=self.axis)\n\n    @final\n    def _aggregate_with_numba(self, func, *args, engine_kwargs=None, **kwargs):\n        \"\"\"\n        Perform groupby aggregation routine with the numba engine.\n\n        This routine mimics the data splitting routine of the DataSplitter class\n        to generate the indices of each group in the sorted data and then passes the\n        data and indices into a Numba jitted function.\n        \"\"\"\n        data = self._obj_with_exclusions\n        df = data if data.ndim == 2 else data.to_frame()\n        (starts, ends, sorted_index, sorted_data) = self._numba_prep(df)\n        numba_.validate_udf(func)\n        numba_agg_func = numba_.generate_numba_agg_func(func, **get_jit_arguments(engine_kwargs, kwargs))\n        result = numba_agg_func(sorted_data, sorted_index, starts, ends, len(df.columns), *args)\n        index = self._grouper.result_index\n        if data.ndim == 1:\n            result_kwargs = {'name': data.name}\n            result = result.ravel()\n        else:\n            result_kwargs = {'columns': data.columns}\n        res = data._constructor(result, index=index, **result_kwargs)\n        if not self.as_index:\n            res = self._insert_inaxis_grouper(res)\n            res.index = default_index(len(res))\n        return res\n\n    @final\n    @doc(_groupby_agg_method_engine_template, fname='min', no=False, mc=-1, e=None, ek=None, example=dedent('        For SeriesGroupBy:\\n\\n        >>> lst = [\\'a\\', \\'a\\', \\'b\\', \\'b\\']\\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\\n        >>> ser\\n        a    1\\n        a    2\\n        b    3\\n        b    4\\n        dtype: int64\\n        >>> ser.groupby(level=0).min()\\n        a    1\\n        b    3\\n        dtype: int64\\n\\n        For DataFrameGroupBy:\\n\\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\\n        ...                   index=[\"tiger\", \"leopard\", \"cheetah\", \"lion\"])\\n        >>> df\\n                  a  b  c\\n          tiger   1  8  2\\n        leopard   1  2  5\\n        cheetah   2  5  8\\n           lion   2  6  9\\n        >>> df.groupby(\"a\").min()\\n            b  c\\n        a\\n        1   2  2\\n        2   5  8'))\n    def min(self, numeric_only: bool=False, min_count: int=-1, engine: Literal['cython', 'numba'] | None=None, engine_kwargs: dict[str, bool] | None=None):\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_min_max\n            return self._numba_agg_general(grouped_min_max, executor.identity_dtype_mapping, engine_kwargs, min_periods=min_count, is_max=False)\n        else:\n            return self._agg_general(numeric_only=numeric_only, min_count=min_count, alias='min', npfunc=np.min)\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def size(self) -> DataFrame | Series:\n        \"\"\"\n        Compute group sizes.\n\n        Returns\n        -------\n        DataFrame or Series\n            Number of rows in each group as a Series if as_index is True\n            or a DataFrame if as_index is False.\n        %(see_also)s\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([1, 2, 3], index=lst)\n        >>> ser\n        a     1\n        a     2\n        b     3\n        dtype: int64\n        >>> ser.groupby(level=0).size()\n        a    2\n        b    1\n        dtype: int64\n\n        >>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"owl\", \"toucan\", \"eagle\"])\n        >>> df\n                a  b  c\n        owl     1  2  3\n        toucan  1  5  6\n        eagle   7  8  9\n        >>> df.groupby(\"a\").size()\n        a\n        1    2\n        7    1\n        dtype: int64\n\n        For Resampler:\n\n        >>> ser = pd.Series([1, 2, 3], index=pd.DatetimeIndex(\n        ...                 ['2023-01-01', '2023-01-15', '2023-02-01']))\n        >>> ser\n        2023-01-01    1\n        2023-01-15    2\n        2023-02-01    3\n        dtype: int64\n        >>> ser.resample('MS').size()\n        2023-01-01    2\n        2023-02-01    1\n        Freq: MS, dtype: int64\n        \"\"\"\n        result = self._grouper.size()\n        dtype_backend: None | Literal['pyarrow', 'numpy_nullable'] = None\n        if isinstance(self.obj, Series):\n            if isinstance(self.obj.array, ArrowExtensionArray):\n                if isinstance(self.obj.array, ArrowStringArrayNumpySemantics):\n                    dtype_backend = None\n                elif isinstance(self.obj.array, ArrowStringArray):\n                    dtype_backend = 'numpy_nullable'\n                else:\n                    dtype_backend = 'pyarrow'\n            elif isinstance(self.obj.array, BaseMaskedArray):\n                dtype_backend = 'numpy_nullable'\n        if isinstance(self.obj, Series):\n            result = self._obj_1d_constructor(result, name=self.obj.name)\n        else:\n            result = self._obj_1d_constructor(result)\n        if dtype_backend is not None:\n            result = result.convert_dtypes(infer_objects=False, convert_string=False, convert_boolean=False, convert_floating=False, dtype_backend=dtype_backend)\n        with com.temp_setattr(self, 'as_index', True):\n            result = self._reindex_output(result, fill_value=0)\n        if not self.as_index:\n            result = result.rename('size').reset_index()\n        return result\n\n    @final\n    def sem(self, ddof: int=1, numeric_only: bool=False) -> NDFrameT:\n        \"\"\"\n        Compute standard error of the mean of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only now defaults to ``False``.\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard error of the mean of values within each group.\n\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b', 'b']\n        >>> ser = pd.Series([5, 10, 8, 14], index=lst)\n        >>> ser\n        a     5\n        a    10\n        b     8\n        b    14\n        dtype: int64\n        >>> ser.groupby(level=0).sem()\n        a    2.5\n        b    3.0\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 12, 11], [1, 15, 2], [2, 5, 8], [2, 6, 12]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"tuna\", \"salmon\", \"catfish\", \"goldfish\"])\n        >>> df\n                   a   b   c\n            tuna   1  12  11\n          salmon   1  15   2\n         catfish   2   5   8\n        goldfish   2   6  12\n        >>> df.groupby(\"a\").sem()\n              b  c\n        a\n        1    1.5  4.5\n        2    0.5  2.0\n\n        For Resampler:\n\n        >>> ser = pd.Series([1, 3, 2, 4, 3, 8],\n        ...                 index=pd.DatetimeIndex(['2023-01-01',\n        ...                                         '2023-01-10',\n        ...                                         '2023-01-15',\n        ...                                         '2023-02-01',\n        ...                                         '2023-02-10',\n        ...                                         '2023-02-15']))\n        >>> ser.resample('MS').sem()\n        2023-01-01    0.577350\n        2023-02-01    1.527525\n        Freq: MS, dtype: float64\n        \"\"\"\n        if numeric_only and self.obj.ndim == 1 and (not is_numeric_dtype(self.obj.dtype)):\n            raise TypeError(f'{type(self).__name__}.sem called with numeric_only={numeric_only} and dtype {self.obj.dtype}')\n        return self._cython_agg_general('sem', alt=lambda x: Series(x, copy=False).sem(ddof=ddof), numeric_only=numeric_only, ddof=ddof)\n\n    @final\n    def first(self, numeric_only: bool=False, min_count: int=-1, skipna: bool=True) -> NDFrameT:\n        \"\"\"\n        Compute the first entry of each column within each group.\n\n        Defaults to skipping NA elements.\n\n        Parameters\n        ----------\n        numeric_only : bool, default False\n            Include only float, int, boolean columns.\n        min_count : int, default -1\n            The required number of valid values to perform the operation. If fewer\n            than ``min_count`` valid values are present the result will be NA.\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n\n            .. versionadded:: 2.2.1\n\n        Returns\n        -------\n        Series or DataFrame\n            First values within each group.\n\n        See Also\n        --------\n        DataFrame.groupby : Apply a function groupby to each row or column of a\n            DataFrame.\n        pandas.core.groupby.DataFrameGroupBy.last : Compute the last non-null entry\n            of each column.\n        pandas.core.groupby.DataFrameGroupBy.nth : Take the nth row from each group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(dict(A=[1, 1, 3], B=[None, 5, 6], C=[1, 2, 3],\n        ...                        D=['3/11/2000', '3/12/2000', '3/13/2000']))\n        >>> df['D'] = pd.to_datetime(df['D'])\n        >>> df.groupby(\"A\").first()\n             B  C          D\n        A\n        1  5.0  1 2000-03-11\n        3  6.0  3 2000-03-13\n        >>> df.groupby(\"A\").first(min_count=2)\n            B    C          D\n        A\n        1 NaN  1.0 2000-03-11\n        3 NaN  NaN        NaT\n        >>> df.groupby(\"A\").first(numeric_only=True)\n             B  C\n        A\n        1  5.0  1\n        3  6.0  3\n        \"\"\"\n\n        def first_compat(obj: NDFrameT, axis: AxisInt=0):\n\n            def first(x: Series):\n                \"\"\"Helper function for first item that isn't NA.\"\"\"\n                arr = x.array[notna(x.array)]\n                if not len(arr):\n                    return x.array.dtype.na_value\n                return arr[0]\n            if isinstance(obj, DataFrame):\n                return obj.apply(first, axis=axis)\n            elif isinstance(obj, Series):\n                return first(obj)\n            else:\n                raise TypeError(type(obj))\n        return self._agg_general(numeric_only=numeric_only, min_count=min_count, alias='first', npfunc=first_compat, skipna=skipna)\n\n    @final\n    def _transform_with_numba(self, func, *args, engine_kwargs=None, **kwargs):\n        \"\"\"\n        Perform groupby transform routine with the numba engine.\n\n        This routine mimics the data splitting routine of the DataSplitter class\n        to generate the indices of each group in the sorted data and then passes the\n        data and indices into a Numba jitted function.\n        \"\"\"\n        data = self._obj_with_exclusions\n        df = data if data.ndim == 2 else data.to_frame()\n        (starts, ends, sorted_index, sorted_data) = self._numba_prep(df)\n        numba_.validate_udf(func)\n        numba_transform_func = numba_.generate_numba_transform_func(func, **get_jit_arguments(engine_kwargs, kwargs))\n        result = numba_transform_func(sorted_data, sorted_index, starts, ends, len(df.columns), *args)\n        result = result.take(np.argsort(sorted_index), axis=0)\n        index = data.index\n        if data.ndim == 1:\n            result_kwargs = {'name': data.name}\n            result = result.ravel()\n        else:\n            result_kwargs = {'columns': data.columns}\n        return data._constructor(result, index=index, **result_kwargs)\n\n    @final\n    def _op_via_apply(self, name: str, *args, **kwargs):\n        \"\"\"Compute the result of an operation by using GroupBy's apply.\"\"\"\n        f = getattr(type(self._obj_with_exclusions), name)\n        sig = inspect.signature(f)\n        if 'axis' in kwargs and kwargs['axis'] is not lib.no_default:\n            axis = self.obj._get_axis_number(kwargs['axis'])\n            self._deprecate_axis(axis, name)\n        elif 'axis' in kwargs:\n            if name == 'skew':\n                pass\n            elif name == 'fillna':\n                kwargs['axis'] = None\n            else:\n                kwargs['axis'] = 0\n        if 'axis' in sig.parameters:\n            if kwargs.get('axis', None) is None or kwargs.get('axis') is lib.no_default:\n                kwargs['axis'] = self.axis\n\n        def curried(x):\n            return f(x, *args, **kwargs)\n        curried.__name__ = name\n        if name in base.plotting_methods:\n            return self._python_apply_general(curried, self._selected_obj)\n        is_transform = name in base.transformation_kernels\n        result = self._python_apply_general(curried, self._obj_with_exclusions, is_transform=is_transform, not_indexed_same=not is_transform)\n        if self._grouper.has_dropped_na and is_transform:\n            result = self._set_result_index_ordered(result)\n        return result\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def std(self, ddof: int=1, engine: Literal['cython', 'numba'] | None=None, engine_kwargs: dict[str, bool] | None=None, numeric_only: bool=False):\n        \"\"\"\n        Compute standard deviation of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        engine : str, default None\n            * ``'cython'`` : Runs the operation through C-extensions from cython.\n            * ``'numba'`` : Runs the operation through JIT compiled code from numba.\n            * ``None`` : Defaults to ``'cython'`` or globally setting\n              ``compute.use_numba``\n\n            .. versionadded:: 1.4.0\n\n        engine_kwargs : dict, default None\n            * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n            * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n              and ``parallel`` dictionary keys. The values must either be ``True`` or\n              ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n              ``{{'nopython': True, 'nogil': False, 'parallel': False}}``\n\n            .. versionadded:: 1.4.0\n\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only now defaults to ``False``.\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard deviation of values within each group.\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n        >>> ser\n        a     7\n        a     2\n        a     8\n        b     4\n        b     3\n        b     3\n        dtype: int64\n        >>> ser.groupby(level=0).std()\n        a    3.21455\n        b    0.57735\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n        >>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n        ...                   'mouse', 'mouse', 'mouse', 'mouse'])\n        >>> df\n                 a  b\n          dog    1  1\n          dog    3  4\n          dog    5  8\n        mouse    7  4\n        mouse    7  4\n        mouse    8  2\n        mouse    3  1\n        >>> df.groupby(level=0).std()\n                      a         b\n        dog    2.000000  3.511885\n        mouse  2.217356  1.500000\n        \"\"\"\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_var\n            return np.sqrt(self._numba_agg_general(grouped_var, executor.float_dtype_mapping, engine_kwargs, min_periods=0, ddof=ddof))\n        else:\n            return self._cython_agg_general('std', alt=lambda x: Series(x, copy=False).std(ddof=ddof), numeric_only=numeric_only, ddof=ddof)\n\n    @final\n    def ohlc(self) -> DataFrame:\n        \"\"\"\n        Compute open, high, low and close values of a group, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Returns\n        -------\n        DataFrame\n            Open, high, low and close values within each group.\n\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> lst = ['SPX', 'CAC', 'SPX', 'CAC', 'SPX', 'CAC', 'SPX', 'CAC',]\n        >>> ser = pd.Series([3.4, 9.0, 7.2, 5.2, 8.8, 9.4, 0.1, 0.5], index=lst)\n        >>> ser\n        SPX     3.4\n        CAC     9.0\n        SPX     7.2\n        CAC     5.2\n        SPX     8.8\n        CAC     9.4\n        SPX     0.1\n        CAC     0.5\n        dtype: float64\n        >>> ser.groupby(level=0).ohlc()\n             open  high  low  close\n        CAC   9.0   9.4  0.5    0.5\n        SPX   3.4   8.8  0.1    0.1\n\n        For DataFrameGroupBy:\n\n        >>> data = {2022: [1.2, 2.3, 8.9, 4.5, 4.4, 3, 2 , 1],\n        ...         2023: [3.4, 9.0, 7.2, 5.2, 8.8, 9.4, 8.2, 1.0]}\n        >>> df = pd.DataFrame(data, index=['SPX', 'CAC', 'SPX', 'CAC',\n        ...                   'SPX', 'CAC', 'SPX', 'CAC'])\n        >>> df\n             2022  2023\n        SPX   1.2   3.4\n        CAC   2.3   9.0\n        SPX   8.9   7.2\n        CAC   4.5   5.2\n        SPX   4.4   8.8\n        CAC   3.0   9.4\n        SPX   2.0   8.2\n        CAC   1.0   1.0\n        >>> df.groupby(level=0).ohlc()\n            2022                 2023\n            open high  low close open high  low close\n        CAC  2.3  4.5  1.0   1.0  9.0  9.4  1.0   1.0\n        SPX  1.2  8.9  1.2   2.0  3.4  8.8  3.4   8.2\n\n        For Resampler:\n\n        >>> ser = pd.Series([1, 3, 2, 4, 3, 5],\n        ...                 index=pd.DatetimeIndex(['2023-01-01',\n        ...                                         '2023-01-10',\n        ...                                         '2023-01-15',\n        ...                                         '2023-02-01',\n        ...                                         '2023-02-10',\n        ...                                         '2023-02-15']))\n        >>> ser.resample('MS').ohlc()\n                    open  high  low  close\n        2023-01-01     1     3    1      2\n        2023-02-01     4     5    3      5\n        \"\"\"\n        if self.obj.ndim == 1:\n            obj = self._selected_obj\n            is_numeric = is_numeric_dtype(obj.dtype)\n            if not is_numeric:\n                raise DataError('No numeric types to aggregate')\n            res_values = self._grouper._cython_operation('aggregate', obj._values, 'ohlc', axis=0, min_count=-1)\n            agg_names = ['open', 'high', 'low', 'close']\n            result = self.obj._constructor_expanddim(res_values, index=self._grouper.result_index, columns=agg_names)\n            return self._reindex_output(result)\n        result = self._apply_to_column_groupbys(lambda sgb: sgb.ohlc())\n        return result\n\n    @final\n    def median(self, numeric_only: bool=False) -> NDFrameT:\n        \"\"\"\n        Compute median of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Parameters\n        ----------\n        numeric_only : bool, default False\n            Include only float, int, boolean columns.\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only no longer accepts ``None`` and defaults to False.\n\n        Returns\n        -------\n        Series or DataFrame\n            Median of values within each group.\n\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n        >>> ser\n        a     7\n        a     2\n        a     8\n        b     4\n        b     3\n        b     3\n        dtype: int64\n        >>> ser.groupby(level=0).median()\n        a    7.0\n        b    3.0\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n        >>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n        ...                   'mouse', 'mouse', 'mouse', 'mouse'])\n        >>> df\n                 a  b\n          dog    1  1\n          dog    3  4\n          dog    5  8\n        mouse    7  4\n        mouse    7  4\n        mouse    8  2\n        mouse    3  1\n        >>> df.groupby(level=0).median()\n                 a    b\n        dog    3.0  4.0\n        mouse  7.0  3.0\n\n        For Resampler:\n\n        >>> ser = pd.Series([1, 2, 3, 3, 4, 5],\n        ...                 index=pd.DatetimeIndex(['2023-01-01',\n        ...                                         '2023-01-10',\n        ...                                         '2023-01-15',\n        ...                                         '2023-02-01',\n        ...                                         '2023-02-10',\n        ...                                         '2023-02-15']))\n        >>> ser.resample('MS').median()\n        2023-01-01    2.0\n        2023-02-01    4.0\n        Freq: MS, dtype: float64\n        \"\"\"\n        result = self._cython_agg_general('median', alt=lambda x: Series(x, copy=False).median(numeric_only=numeric_only), numeric_only=numeric_only)\n        return result.__finalize__(self.obj, method='groupby')\n\n    @final\n    def _cython_agg_general(self, how: str, alt: Callable | None=None, numeric_only: bool=False, min_count: int=-1, **kwargs):\n        data = self._get_data_to_aggregate(numeric_only=numeric_only, name=how)\n\n        def array_func(values: ArrayLike) -> ArrayLike:\n            try:\n                result = self._grouper._cython_operation('aggregate', values, how, axis=data.ndim - 1, min_count=min_count, **kwargs)\n            except NotImplementedError:\n                if how in ['any', 'all'] and isinstance(values, SparseArray):\n                    pass\n                elif alt is None or how in ['any', 'all', 'std', 'sem']:\n                    raise\n            else:\n                return result\n            assert alt is not None\n            result = self._agg_py_fallback(how, values, ndim=data.ndim, alt=alt)\n            return result\n        new_mgr = data.grouped_reduce(array_func)\n        res = self._wrap_agged_manager(new_mgr)\n        if how in ['idxmin', 'idxmax']:\n            res = self._wrap_idxmax_idxmin(res)\n        out = self._wrap_aggregated_output(res)\n        if self.axis == 1:\n            out = out.infer_objects(copy=False)\n        return out\n\n    @final\n    @doc(_groupby_agg_method_template, fname='prod', no=False, mc=0, example=dedent('        For SeriesGroupBy:\\n\\n        >>> lst = [\\'a\\', \\'a\\', \\'b\\', \\'b\\']\\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\\n        >>> ser\\n        a    1\\n        a    2\\n        b    3\\n        b    4\\n        dtype: int64\\n        >>> ser.groupby(level=0).prod()\\n        a    2\\n        b   12\\n        dtype: int64\\n\\n        For DataFrameGroupBy:\\n\\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\\n        ...                   index=[\"tiger\", \"leopard\", \"cheetah\", \"lion\"])\\n        >>> df\\n                  a  b  c\\n          tiger   1  8  2\\n        leopard   1  2  5\\n        cheetah   2  5  8\\n           lion   2  6  9\\n        >>> df.groupby(\"a\").prod()\\n             b    c\\n        a\\n        1   16   10\\n        2   30   72'))\n    def prod(self, numeric_only: bool=False, min_count: int=0) -> NDFrameT:\n        return self._agg_general(numeric_only=numeric_only, min_count=min_count, alias='prod', npfunc=np.prod)\n\n    @final\n    def _apply_filter(self, indices, dropna):\n        if len(indices) == 0:\n            indices = np.array([], dtype='int64')\n        else:\n            indices = np.sort(np.concatenate(indices))\n        if dropna:\n            filtered = self._selected_obj.take(indices, axis=self.axis)\n        else:\n            mask = np.empty(len(self._selected_obj.index), dtype=bool)\n            mask.fill(False)\n            mask[indices.astype(int)] = True\n            mask = np.tile(mask, list(self._selected_obj.shape[1:]) + [1]).T\n            filtered = self._selected_obj.where(mask)\n        return filtered\n\n    @final\n    @property\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def nth(self) -> GroupByNthSelector:\n        \"\"\"\n        Take the nth row from each group if n is an int, otherwise a subset of rows.\n\n        Can be either a call or an index. dropna is not available with index notation.\n        Index notation accepts a comma separated list of integers and slices.\n\n        If dropna, will take the nth non-null row, dropna is either\n        'all' or 'any'; this is equivalent to calling dropna(how=dropna)\n        before the groupby.\n\n        Parameters\n        ----------\n        n : int, slice or list of ints and slices\n            A single nth value for the row or a list of nth values or slices.\n\n            .. versionchanged:: 1.4.0\n                Added slice and lists containing slices.\n                Added index notation.\n\n        dropna : {'any', 'all', None}, default None\n            Apply the specified dropna operation before counting which row is\n            the nth row. Only supported if n is an int.\n\n        Returns\n        -------\n        Series or DataFrame\n            N-th value within each group.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5]}, columns=['A', 'B'])\n        >>> g = df.groupby('A')\n        >>> g.nth(0)\n           A   B\n        0  1 NaN\n        2  2 3.0\n        >>> g.nth(1)\n           A   B\n        1  1 2.0\n        4  2 5.0\n        >>> g.nth(-1)\n           A   B\n        3  1 4.0\n        4  2 5.0\n        >>> g.nth([0, 1])\n           A   B\n        0  1 NaN\n        1  1 2.0\n        2  2 3.0\n        4  2 5.0\n        >>> g.nth(slice(None, -1))\n           A   B\n        0  1 NaN\n        1  1 2.0\n        2  2 3.0\n\n        Index notation may also be used\n\n        >>> g.nth[0, 1]\n           A   B\n        0  1 NaN\n        1  1 2.0\n        2  2 3.0\n        4  2 5.0\n        >>> g.nth[:-1]\n           A   B\n        0  1 NaN\n        1  1 2.0\n        2  2 3.0\n\n        Specifying `dropna` allows ignoring ``NaN`` values\n\n        >>> g.nth(0, dropna='any')\n           A   B\n        1  1 2.0\n        2  2 3.0\n\n        When the specified ``n`` is larger than any of the groups, an\n        empty DataFrame is returned\n\n        >>> g.nth(3, dropna='any')\n        Empty DataFrame\n        Columns: [A, B]\n        Index: []\n        \"\"\"\n        return GroupByNthSelector(self)\n\n    @final\n    def _numba_prep(self, data: DataFrame):\n        (ids, _, ngroups) = self._grouper.group_info\n        sorted_index = self._grouper._sort_idx\n        sorted_ids = self._grouper._sorted_ids\n        sorted_data = data.take(sorted_index, axis=self.axis).to_numpy()\n        index_data = data.index\n        if isinstance(index_data, MultiIndex):\n            if len(self._grouper.groupings) > 1:\n                raise NotImplementedError(\"Grouping with more than 1 grouping labels and a MultiIndex is not supported with engine='numba'\")\n            group_key = self._grouper.groupings[0].name\n            index_data = index_data.get_level_values(group_key)\n        sorted_index_data = index_data.take(sorted_index).to_numpy()\n        (starts, ends) = lib.generate_slices(sorted_ids, ngroups)\n        return (starts, ends, sorted_index_data, sorted_data)\n\n    def _agg_py_fallback(self, how: str, values: ArrayLike, ndim: int, alt: Callable) -> ArrayLike:\n        \"\"\"\n        Fallback to pure-python aggregation if _cython_operation raises\n        NotImplementedError.\n        \"\"\"\n        assert alt is not None\n        if values.ndim == 1:\n            ser = Series(values, copy=False)\n        else:\n            df = DataFrame(values.T, dtype=values.dtype)\n            assert df.shape[1] == 1\n            ser = df.iloc[:, 0]\n        try:\n            res_values = self._grouper.agg_series(ser, alt, preserve_dtype=True)\n        except Exception as err:\n            msg = f'agg function failed [how->{how},dtype->{ser.dtype}]'\n            raise type(err)(msg) from err\n        if ser.dtype == object:\n            res_values = res_values.astype(object, copy=False)\n        return ensure_block_shape(res_values, ndim=ndim)\n\n    def _numba_agg_general(self, func: Callable, dtype_mapping: dict[np.dtype, Any], engine_kwargs: dict[str, bool] | None, **aggregator_kwargs):\n        \"\"\"\n        Perform groupby with a standard numerical aggregation function (e.g. mean)\n        with Numba.\n        \"\"\"\n        if not self.as_index:\n            raise NotImplementedError('as_index=False is not supported. Use .reset_index() instead.')\n        if self.axis == 1:\n            raise NotImplementedError('axis=1 is not supported.')\n        data = self._obj_with_exclusions\n        df = data if data.ndim == 2 else data.to_frame()\n        aggregator = executor.generate_shared_aggregator(func, dtype_mapping, True, **get_jit_arguments(engine_kwargs))\n        (ids, _, _) = self._grouper.group_info\n        ngroups = self._grouper.ngroups\n        res_mgr = df._mgr.apply(aggregator, labels=ids, ngroups=ngroups, **aggregator_kwargs)\n        res_mgr.axes[1] = self._grouper.result_index\n        result = df._constructor_from_mgr(res_mgr, axes=res_mgr.axes)\n        if data.ndim == 1:\n            result = result.squeeze('columns')\n            result.name = data.name\n        else:\n            result.columns = data.columns\n        return result\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def rank(self, method: str='average', ascending: bool=True, na_option: str='keep', pct: bool=False, axis: AxisInt | lib.NoDefault=lib.no_default) -> NDFrameT:\n        \"\"\"\n        Provide the rank of values within each group.\n\n        Parameters\n        ----------\n        method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n            * average: average rank of group.\n            * min: lowest rank in group.\n            * max: highest rank in group.\n            * first: ranks assigned in order they appear in the array.\n            * dense: like 'min', but rank always increases by 1 between groups.\n        ascending : bool, default True\n            False for ranks by high (1) to low (N).\n        na_option : {'keep', 'top', 'bottom'}, default 'keep'\n            * keep: leave NA values where they are.\n            * top: smallest rank if ascending.\n            * bottom: smallest rank if descending.\n        pct : bool, default False\n            Compute percentage rank of data within each group.\n        axis : int, default 0\n            The axis of the object over which to compute the rank.\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        Returns\n        -------\n        DataFrame with ranking of values within each group\n        %(see_also)s\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"group\": [\"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\"],\n        ...         \"value\": [2, 4, 2, 3, 5, 1, 2, 4, 1, 5],\n        ...     }\n        ... )\n        >>> df\n          group  value\n        0     a      2\n        1     a      4\n        2     a      2\n        3     a      3\n        4     a      5\n        5     b      1\n        6     b      2\n        7     b      4\n        8     b      1\n        9     b      5\n        >>> for method in ['average', 'min', 'max', 'dense', 'first']:\n        ...     df[f'{method}_rank'] = df.groupby('group')['value'].rank(method)\n        >>> df\n          group  value  average_rank  min_rank  max_rank  dense_rank  first_rank\n        0     a      2           1.5       1.0       2.0         1.0         1.0\n        1     a      4           4.0       4.0       4.0         3.0         4.0\n        2     a      2           1.5       1.0       2.0         1.0         2.0\n        3     a      3           3.0       3.0       3.0         2.0         3.0\n        4     a      5           5.0       5.0       5.0         4.0         5.0\n        5     b      1           1.5       1.0       2.0         1.0         1.0\n        6     b      2           3.0       3.0       3.0         2.0         3.0\n        7     b      4           4.0       4.0       4.0         3.0         4.0\n        8     b      1           1.5       1.0       2.0         1.0         2.0\n        9     b      5           5.0       5.0       5.0         4.0         5.0\n        \"\"\"\n        if na_option not in {'keep', 'top', 'bottom'}:\n            msg = \"na_option must be one of 'keep', 'top', or 'bottom'\"\n            raise ValueError(msg)\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, 'rank')\n        else:\n            axis = 0\n        kwargs = {'ties_method': method, 'ascending': ascending, 'na_option': na_option, 'pct': pct}\n        if axis != 0:\n            kwargs['method'] = kwargs.pop('ties_method')\n            f = lambda x: x.rank(axis=axis, numeric_only=False, **kwargs)\n            result = self._python_apply_general(f, self._selected_obj, is_transform=True)\n            return result\n        return self._cython_transform('rank', numeric_only=False, axis=axis, **kwargs)\n\n    @final\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def ewm(self, *args, **kwargs) -> ExponentialMovingWindowGroupby:\n        \"\"\"\n        Return an ewm grouper, providing ewm functionality per group.\n\n        Returns\n        -------\n        pandas.api.typing.ExponentialMovingWindowGroupby\n        \"\"\"\n        from pandas.core.window import ExponentialMovingWindowGroupby\n        return ExponentialMovingWindowGroupby(self._selected_obj, *args, _grouper=self._grouper, **kwargs)\n\n    def _wrap_idxmax_idxmin(self, res: NDFrameT) -> NDFrameT:\n        index = self.obj._get_axis(self.axis)\n        if res.size == 0:\n            result = res.astype(index.dtype)\n        else:\n            if isinstance(index, MultiIndex):\n                index = index.to_flat_index()\n            values = res._values\n            assert isinstance(values, np.ndarray)\n            na_value = na_value_for_dtype(index.dtype, compat=False)\n            if isinstance(res, Series):\n                result = res._constructor(index.array.take(values, allow_fill=True, fill_value=na_value), index=res.index, name=res.name)\n            else:\n                data = {}\n                for (k, column_values) in enumerate(values.T):\n                    data[k] = index.array.take(column_values, allow_fill=True, fill_value=na_value)\n                result = self.obj._constructor(data, index=res.index)\n                result.columns = res.columns\n        return result\n\n    @final\n    @Substitution(name='groupby')\n    def bfill(self, limit: int | None=None):\n        \"\"\"\n        Backward fill the values.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        Series.bfill :  Backward fill the missing values in the dataset.\n        DataFrame.bfill:  Backward fill the missing values in the dataset.\n        Series.fillna: Fill NaN values of a Series.\n        DataFrame.fillna: Fill NaN values of a DataFrame.\n\n        Examples\n        --------\n\n        With Series:\n\n        >>> index = ['Falcon', 'Falcon', 'Parrot', 'Parrot', 'Parrot']\n        >>> s = pd.Series([None, 1, None, None, 3], index=index)\n        >>> s\n        Falcon    NaN\n        Falcon    1.0\n        Parrot    NaN\n        Parrot    NaN\n        Parrot    3.0\n        dtype: float64\n        >>> s.groupby(level=0).bfill()\n        Falcon    1.0\n        Falcon    1.0\n        Parrot    3.0\n        Parrot    3.0\n        Parrot    3.0\n        dtype: float64\n        >>> s.groupby(level=0).bfill(limit=1)\n        Falcon    1.0\n        Falcon    1.0\n        Parrot    NaN\n        Parrot    3.0\n        Parrot    3.0\n        dtype: float64\n\n        With DataFrame:\n\n        >>> df = pd.DataFrame({'A': [1, None, None, None, 4],\n        ...                    'B': [None, None, 5, None, 7]}, index=index)\n        >>> df\n                  A\t    B\n        Falcon\t1.0\t  NaN\n        Falcon\tNaN\t  NaN\n        Parrot\tNaN\t  5.0\n        Parrot\tNaN\t  NaN\n        Parrot\t4.0\t  7.0\n        >>> df.groupby(level=0).bfill()\n                  A\t    B\n        Falcon\t1.0\t  NaN\n        Falcon\tNaN\t  NaN\n        Parrot\t4.0\t  5.0\n        Parrot\t4.0\t  7.0\n        Parrot\t4.0\t  7.0\n        >>> df.groupby(level=0).bfill(limit=1)\n                  A\t    B\n        Falcon\t1.0\t  NaN\n        Falcon\tNaN\t  NaN\n        Parrot\tNaN\t  5.0\n        Parrot\t4.0\t  7.0\n        Parrot\t4.0\t  7.0\n        \"\"\"\n        return self._fill('bfill', limit=limit)\n\n    @final\n    @Substitution(name='groupby')\n    def ngroup(self, ascending: bool=True):\n        \"\"\"\n        Number each group from 0 to the number of groups - 1.\n\n        This is the enumerative complement of cumcount.  Note that the\n        numbers given to the groups match the order in which the groups\n        would be seen when iterating over the groupby object, not the\n        order they are first observed.\n\n        Groups with missing keys (where `pd.isna()` is True) will be labeled with `NaN`\n        and will be skipped from the count.\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from number of group - 1 to 0.\n\n        Returns\n        -------\n        Series\n            Unique numbers for each group.\n\n        See Also\n        --------\n        .cumcount : Number the rows in each group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"color\": [\"red\", None, \"red\", \"blue\", \"blue\", \"red\"]})\n        >>> df\n           color\n        0    red\n        1   None\n        2    red\n        3   blue\n        4   blue\n        5    red\n        >>> df.groupby(\"color\").ngroup()\n        0    1.0\n        1    NaN\n        2    1.0\n        3    0.0\n        4    0.0\n        5    1.0\n        dtype: float64\n        >>> df.groupby(\"color\", dropna=False).ngroup()\n        0    1\n        1    2\n        2    1\n        3    0\n        4    0\n        5    1\n        dtype: int64\n        >>> df.groupby(\"color\", dropna=False).ngroup(ascending=False)\n        0    1\n        1    0\n        2    1\n        3    2\n        4    2\n        5    1\n        dtype: int64\n        \"\"\"\n        obj = self._obj_with_exclusions\n        index = obj._get_axis(self.axis)\n        comp_ids = self._grouper.group_info[0]\n        dtype: type\n        if self._grouper.has_dropped_na:\n            comp_ids = np.where(comp_ids == -1, np.nan, comp_ids)\n            dtype = np.float64\n        else:\n            dtype = np.int64\n        if any((ping._passed_categorical for ping in self._grouper.groupings)):\n            comp_ids = rank_1d(comp_ids, ties_method='dense') - 1\n        result = self._obj_1d_constructor(comp_ids, index, dtype=dtype)\n        if not ascending:\n            result = self.ngroups - 1 - result\n        return result\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def cummax(self, axis: AxisInt | lib.NoDefault=lib.no_default, numeric_only: bool=False, **kwargs) -> NDFrameT:\n        \"\"\"\n        Cumulative max for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([1, 6, 2, 3, 1, 4], index=lst)\n        >>> ser\n        a    1\n        a    6\n        a    2\n        b    3\n        b    1\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).cummax()\n        a    1\n        a    6\n        a    6\n        b    3\n        b    3\n        b    4\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 8, 2], [1, 1, 0], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"cow\", \"horse\", \"bull\"])\n        >>> df\n                a   b   c\n        cow     1   8   2\n        horse   1   1   0\n        bull    2   6   9\n        >>> df.groupby(\"a\").groups\n        {1: ['cow', 'horse'], 2: ['bull']}\n        >>> df.groupby(\"a\").cummax()\n                b   c\n        cow     8   2\n        horse   8   2\n        bull    6   9\n        \"\"\"\n        skipna = kwargs.get('skipna', True)\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, 'cummax')\n        else:\n            axis = 0\n        if axis != 0:\n            f = lambda x: np.maximum.accumulate(x, axis)\n            obj = self._selected_obj\n            if numeric_only:\n                obj = obj._get_numeric_data()\n            return self._python_apply_general(f, obj, is_transform=True)\n        return self._cython_transform('cummax', numeric_only=numeric_only, skipna=skipna)\n\n    @final\n    def last(self, numeric_only: bool=False, min_count: int=-1, skipna: bool=True) -> NDFrameT:\n        \"\"\"\n        Compute the last entry of each column within each group.\n\n        Defaults to skipping NA elements.\n\n        Parameters\n        ----------\n        numeric_only : bool, default False\n            Include only float, int, boolean columns. If None, will attempt to use\n            everything, then use only numeric data.\n        min_count : int, default -1\n            The required number of valid values to perform the operation. If fewer\n            than ``min_count`` valid values are present the result will be NA.\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n\n            .. versionadded:: 2.2.1\n\n        Returns\n        -------\n        Series or DataFrame\n            Last of values within each group.\n\n        See Also\n        --------\n        DataFrame.groupby : Apply a function groupby to each row or column of a\n            DataFrame.\n        pandas.core.groupby.DataFrameGroupBy.first : Compute the first non-null entry\n            of each column.\n        pandas.core.groupby.DataFrameGroupBy.nth : Take the nth row from each group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(dict(A=[1, 1, 3], B=[5, None, 6], C=[1, 2, 3]))\n        >>> df.groupby(\"A\").last()\n             B  C\n        A\n        1  5.0  2\n        3  6.0  3\n        \"\"\"\n\n        def last_compat(obj: NDFrameT, axis: AxisInt=0):\n\n            def last(x: Series):\n                \"\"\"Helper function for last item that isn't NA.\"\"\"\n                arr = x.array[notna(x.array)]\n                if not len(arr):\n                    return x.array.dtype.na_value\n                return arr[-1]\n            if isinstance(obj, DataFrame):\n                return obj.apply(last, axis=axis)\n            elif isinstance(obj, Series):\n                return last(obj)\n            else:\n                raise TypeError(type(obj))\n        return self._agg_general(numeric_only=numeric_only, min_count=min_count, alias='last', npfunc=last_compat, skipna=skipna)\n\n    @final\n    def _concat_objects(self, values, not_indexed_same: bool=False, is_transform: bool=False):\n        from pandas.core.reshape.concat import concat\n        if self.group_keys and (not is_transform):\n            if self.as_index:\n                group_keys = self._grouper.result_index\n                group_levels = self._grouper.levels\n                group_names = self._grouper.names\n                result = concat(values, axis=self.axis, keys=group_keys, levels=group_levels, names=group_names, sort=False)\n            else:\n                keys = list(range(len(values)))\n                result = concat(values, axis=self.axis, keys=keys)\n        elif not not_indexed_same:\n            result = concat(values, axis=self.axis)\n            ax = self._selected_obj._get_axis(self.axis)\n            if self.dropna:\n                labels = self._grouper.group_info[0]\n                mask = labels != -1\n                ax = ax[mask]\n            if ax.has_duplicates and (not result.axes[self.axis].equals(ax)):\n                target = algorithms.unique1d(ax._values)\n                (indexer, _) = result.index.get_indexer_non_unique(target)\n                result = result.take(indexer, axis=self.axis)\n            else:\n                result = result.reindex(ax, axis=self.axis, copy=False)\n        else:\n            result = concat(values, axis=self.axis)\n        if self.obj.ndim == 1:\n            name = self.obj.name\n        elif is_hashable(self._selection):\n            name = self._selection\n        else:\n            name = None\n        if isinstance(result, Series) and name is not None:\n            result.name = name\n        return result\n\n    @final\n    def __init__(self, obj: NDFrameT, keys: _KeysArgType | None=None, axis: Axis=0, level: IndexLabel | None=None, grouper: ops.BaseGrouper | None=None, exclusions: frozenset[Hashable] | None=None, selection: IndexLabel | None=None, as_index: bool=True, sort: bool=True, group_keys: bool=True, observed: bool | lib.NoDefault=lib.no_default, dropna: bool=True) -> None:\n        self._selection = selection\n        assert isinstance(obj, NDFrame), type(obj)\n        self.level = level\n        if not as_index:\n            if axis != 0:\n                raise ValueError('as_index=False only valid for axis=0')\n        self.as_index = as_index\n        self.keys = keys\n        self.sort = sort\n        self.group_keys = group_keys\n        self.dropna = dropna\n        if grouper is None:\n            (grouper, exclusions, obj) = get_grouper(obj, keys, axis=axis, level=level, sort=sort, observed=False if observed is lib.no_default else observed, dropna=self.dropna)\n        if observed is lib.no_default:\n            if any((ping._passed_categorical for ping in grouper.groupings)):\n                warnings.warn('The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.', FutureWarning, stacklevel=find_stack_level())\n            observed = False\n        self.observed = observed\n        self.obj = obj\n        self.axis = obj._get_axis_number(axis)\n        self._grouper = grouper\n        self.exclusions = frozenset(exclusions) if exclusions else frozenset()\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def cummin(self, axis: AxisInt | lib.NoDefault=lib.no_default, numeric_only: bool=False, **kwargs) -> NDFrameT:\n        \"\"\"\n        Cumulative min for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([1, 6, 2, 3, 0, 4], index=lst)\n        >>> ser\n        a    1\n        a    6\n        a    2\n        b    3\n        b    0\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).cummin()\n        a    1\n        a    1\n        a    1\n        b    3\n        b    0\n        b    0\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 0, 2], [1, 1, 5], [6, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"snake\", \"rabbit\", \"turtle\"])\n        >>> df\n                a   b   c\n        snake   1   0   2\n        rabbit  1   1   5\n        turtle  6   6   9\n        >>> df.groupby(\"a\").groups\n        {1: ['snake', 'rabbit'], 6: ['turtle']}\n        >>> df.groupby(\"a\").cummin()\n                b   c\n        snake   0   2\n        rabbit  0   2\n        turtle  6   9\n        \"\"\"\n        skipna = kwargs.get('skipna', True)\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, 'cummin')\n        else:\n            axis = 0\n        if axis != 0:\n            f = lambda x: np.minimum.accumulate(x, axis)\n            obj = self._selected_obj\n            if numeric_only:\n                obj = obj._get_numeric_data()\n            return self._python_apply_general(f, obj, is_transform=True)\n        return self._cython_transform('cummin', numeric_only=numeric_only, skipna=skipna)\n\n    @final\n    @Substitution(name='groupby')\n    def shift(self, periods: int | Sequence[int]=1, freq=None, axis: Axis | lib.NoDefault=lib.no_default, fill_value=lib.no_default, suffix: str | None=None):\n        \"\"\"\n        Shift each group by periods observations.\n\n        If freq is passed, the index will be increased using the periods and the freq.\n\n        Parameters\n        ----------\n        periods : int | Sequence[int], default 1\n            Number of periods to shift. If a list of values, shift each group by\n            each period.\n        freq : str, optional\n            Frequency string.\n        axis : axis to shift, default 0\n            Shift direction.\n\n            .. deprecated:: 2.1.0\n                For axis=1, operate on the underlying object instead. Otherwise\n                the axis keyword is not necessary.\n\n        fill_value : optional\n            The scalar value to use for newly introduced missing values.\n\n            .. versionchanged:: 2.1.0\n                Will raise a ``ValueError`` if ``freq`` is provided too.\n\n        suffix : str, optional\n            A string to add to each shifted column if there are multiple periods.\n            Ignored otherwise.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object shifted within each group.\n\n        See Also\n        --------\n        Index.shift : Shift values of Index.\n\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b', 'b']\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    3\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).shift(1)\n        a    NaN\n        a    1.0\n        b    NaN\n        b    3.0\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 2, 3], [1, 5, 6], [2, 5, 8], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"tuna\", \"salmon\", \"catfish\", \"goldfish\"])\n        >>> df\n                   a  b  c\n            tuna   1  2  3\n          salmon   1  5  6\n         catfish   2  5  8\n        goldfish   2  6  9\n        >>> df.groupby(\"a\").shift(1)\n                      b    c\n            tuna    NaN  NaN\n          salmon    2.0  3.0\n         catfish    NaN  NaN\n        goldfish    5.0  8.0\n        \"\"\"\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, 'shift')\n        else:\n            axis = 0\n        if is_list_like(periods):\n            if axis == 1:\n                raise ValueError('If `periods` contains multiple shifts, `axis` cannot be 1.')\n            periods = cast(Sequence, periods)\n            if len(periods) == 0:\n                raise ValueError('If `periods` is an iterable, it cannot be empty.')\n            from pandas.core.reshape.concat import concat\n            add_suffix = True\n        else:\n            if not is_integer(periods):\n                raise TypeError(f'Periods must be integer, but {periods} is {type(periods)}.')\n            if suffix:\n                raise ValueError('Cannot specify `suffix` if `periods` is an int.')\n            periods = [cast(int, periods)]\n            add_suffix = False\n        shifted_dataframes = []\n        for period in periods:\n            if not is_integer(period):\n                raise TypeError(f'Periods must be integer, but {period} is {type(period)}.')\n            period = cast(int, period)\n            if freq is not None or axis != 0:\n                f = lambda x: x.shift(period, freq, axis, fill_value)\n                shifted = self._python_apply_general(f, self._selected_obj, is_transform=True)\n            else:\n                if fill_value is lib.no_default:\n                    fill_value = None\n                (ids, _, ngroups) = self._grouper.group_info\n                res_indexer = np.zeros(len(ids), dtype=np.int64)\n                libgroupby.group_shift_indexer(res_indexer, ids, ngroups, period)\n                obj = self._obj_with_exclusions\n                shifted = obj._reindex_with_indexers({self.axis: (obj.axes[self.axis], res_indexer)}, fill_value=fill_value, allow_dups=True)\n            if add_suffix:\n                if isinstance(shifted, Series):\n                    shifted = cast(NDFrameT, shifted.to_frame())\n                shifted = shifted.add_suffix(f'{suffix}_{period}' if suffix else f'_{period}')\n            shifted_dataframes.append(cast(Union[Series, DataFrame], shifted))\n        return shifted_dataframes[0] if len(shifted_dataframes) == 1 else concat(shifted_dataframes, axis=1)\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def cumsum(self, axis: Axis | lib.NoDefault=lib.no_default, *args, **kwargs) -> NDFrameT:\n        \"\"\"\n        Cumulative sum for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([6, 2, 0], index=lst)\n        >>> ser\n        a    6\n        a    2\n        b    0\n        dtype: int64\n        >>> ser.groupby(level=0).cumsum()\n        a    6\n        a    8\n        b    0\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 8, 2], [1, 2, 5], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"fox\", \"gorilla\", \"lion\"])\n        >>> df\n                  a   b   c\n        fox       1   8   2\n        gorilla   1   2   5\n        lion      2   6   9\n        >>> df.groupby(\"a\").groups\n        {1: ['fox', 'gorilla'], 2: ['lion']}\n        >>> df.groupby(\"a\").cumsum()\n                  b   c\n        fox       8   2\n        gorilla  10   7\n        lion      6   9\n        \"\"\"\n        nv.validate_groupby_func('cumsum', args, kwargs, ['numeric_only', 'skipna'])\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, 'cumsum')\n        else:\n            axis = 0\n        if axis != 0:\n            f = lambda x: x.cumsum(axis=axis, **kwargs)\n            return self._python_apply_general(f, self._selected_obj, is_transform=True)\n        return self._cython_transform('cumsum', **kwargs)\n\n    def _idxmax_idxmin(self, how: Literal['idxmax', 'idxmin'], ignore_unobserved: bool=False, axis: Axis | None | lib.NoDefault=lib.no_default, skipna: bool=True, numeric_only: bool=False) -> NDFrameT:\n        \"\"\"Compute idxmax/idxmin.\n\n        Parameters\n        ----------\n        how : {'idxmin', 'idxmax'}\n            Whether to compute idxmin or idxmax.\n        axis : {{0 or 'index', 1 or 'columns'}}, default None\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n            If axis is not provided, grouper's axis is used.\n        numeric_only : bool, default False\n            Include only float, int, boolean columns.\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n        ignore_unobserved : bool, default False\n            When True and an unobserved group is encountered, do not raise. This used\n            for transform where unobserved groups do not play an impact on the result.\n\n        Returns\n        -------\n        Series or DataFrame\n            idxmax or idxmin for the groupby operation.\n        \"\"\"\n        if axis is not lib.no_default:\n            if axis is None:\n                axis = self.axis\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, how)\n        else:\n            axis = self.axis\n        if not self.observed and any((ping._passed_categorical for ping in self._grouper.groupings)):\n            expected_len = np.prod([len(ping._group_index) for ping in self._grouper.groupings])\n            if len(self._grouper.groupings) == 1:\n                result_len = len(self._grouper.groupings[0].grouping_vector.unique())\n            else:\n                result_len = len(self._grouper.result_index)\n            assert result_len <= expected_len\n            has_unobserved = result_len < expected_len\n            raise_err: bool | np.bool_ = not ignore_unobserved and has_unobserved\n            data = self._obj_with_exclusions\n            if raise_err and isinstance(data, DataFrame):\n                if numeric_only:\n                    data = data._get_numeric_data()\n                raise_err = len(data.columns) > 0\n            if raise_err:\n                raise ValueError(f\"Can't get {how} of an empty group due to unobserved categories. Specify observed=True in groupby instead.\")\n        elif not skipna:\n            if self._obj_with_exclusions.isna().any(axis=None):\n                warnings.warn(f'The behavior of {type(self).__name__}.{how} with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError', FutureWarning, stacklevel=find_stack_level())\n        if axis == 1:\n            try:\n\n                def func(df):\n                    method = getattr(df, how)\n                    return method(axis=axis, skipna=skipna, numeric_only=numeric_only)\n                func.__name__ = how\n                result = self._python_apply_general(func, self._obj_with_exclusions, not_indexed_same=True)\n            except ValueError as err:\n                name = 'argmax' if how == 'idxmax' else 'argmin'\n                if f'attempt to get {name} of an empty sequence' in str(err):\n                    raise ValueError(f\"Can't get {how} of an empty group due to unobserved categories. Specify observed=True in groupby instead.\") from None\n                raise\n            return result\n        result = self._agg_general(numeric_only=numeric_only, min_count=1, alias=how, skipna=skipna)\n        return result\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def pct_change(self, periods: int=1, fill_method: FillnaOptions | None | lib.NoDefault=lib.no_default, limit: int | None | lib.NoDefault=lib.no_default, freq=None, axis: Axis | lib.NoDefault=lib.no_default):\n        \"\"\"\n        Calculate pct_change of each value to previous entry in group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Percentage changes within each group.\n        %(see_also)s\n        Examples\n        --------\n\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b', 'b']\n        >>> ser = pd.Series([1, 2, 3, 4], index=lst)\n        >>> ser\n        a    1\n        a    2\n        b    3\n        b    4\n        dtype: int64\n        >>> ser.groupby(level=0).pct_change()\n        a         NaN\n        a    1.000000\n        b         NaN\n        b    0.333333\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, 2, 3], [1, 5, 6], [2, 5, 8], [2, 6, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"tuna\", \"salmon\", \"catfish\", \"goldfish\"])\n        >>> df\n                   a  b  c\n            tuna   1  2  3\n          salmon   1  5  6\n         catfish   2  5  8\n        goldfish   2  6  9\n        >>> df.groupby(\"a\").pct_change()\n                    b  c\n            tuna    NaN    NaN\n          salmon    1.5  1.000\n         catfish    NaN    NaN\n        goldfish    0.2  0.125\n        \"\"\"\n        if fill_method not in (lib.no_default, None) or limit is not lib.no_default:\n            warnings.warn(f\"The 'fill_method' keyword being not None and the 'limit' keyword in {type(self).__name__}.pct_change are deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\", FutureWarning, stacklevel=find_stack_level())\n        if fill_method is lib.no_default:\n            if limit is lib.no_default and any((grp.isna().values.any() for (_, grp) in self)):\n                warnings.warn(f\"The default fill_method='ffill' in {type(self).__name__}.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\", FutureWarning, stacklevel=find_stack_level())\n            fill_method = 'ffill'\n        if limit is lib.no_default:\n            limit = None\n        if axis is not lib.no_default:\n            axis = self.obj._get_axis_number(axis)\n            self._deprecate_axis(axis, 'pct_change')\n        else:\n            axis = 0\n        if freq is not None or axis != 0:\n            f = lambda x: x.pct_change(periods=periods, fill_method=fill_method, limit=limit, freq=freq, axis=axis)\n            return self._python_apply_general(f, self._selected_obj, is_transform=True)\n        if fill_method is None:\n            fill_method = 'ffill'\n            limit = 0\n        filled = getattr(self, fill_method)(limit=limit)\n        if self.axis == 0:\n            fill_grp = filled.groupby(self._grouper.codes, group_keys=self.group_keys)\n        else:\n            fill_grp = filled.T.groupby(self._grouper.codes, group_keys=self.group_keys)\n        shifted = fill_grp.shift(periods=periods, freq=freq)\n        if self.axis == 1:\n            shifted = shifted.T\n        return filled / shifted - 1\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def var(self, ddof: int=1, engine: Literal['cython', 'numba'] | None=None, engine_kwargs: dict[str, bool] | None=None, numeric_only: bool=False):\n        \"\"\"\n        Compute variance of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        engine : str, default None\n            * ``'cython'`` : Runs the operation through C-extensions from cython.\n            * ``'numba'`` : Runs the operation through JIT compiled code from numba.\n            * ``None`` : Defaults to ``'cython'`` or globally setting\n              ``compute.use_numba``\n\n            .. versionadded:: 1.4.0\n\n        engine_kwargs : dict, default None\n            * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n            * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n              and ``parallel`` dictionary keys. The values must either be ``True`` or\n              ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n              ``{{'nopython': True, 'nogil': False, 'parallel': False}}``\n\n            .. versionadded:: 1.4.0\n\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n            .. versionadded:: 1.5.0\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only now defaults to ``False``.\n\n        Returns\n        -------\n        Series or DataFrame\n            Variance of values within each group.\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n        >>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n        >>> ser\n        a     7\n        a     2\n        a     8\n        b     4\n        b     3\n        b     3\n        dtype: int64\n        >>> ser.groupby(level=0).var()\n        a    10.333333\n        b     0.333333\n        dtype: float64\n\n        For DataFrameGroupBy:\n\n        >>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n        >>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n        ...                   'mouse', 'mouse', 'mouse', 'mouse'])\n        >>> df\n                 a  b\n          dog    1  1\n          dog    3  4\n          dog    5  8\n        mouse    7  4\n        mouse    7  4\n        mouse    8  2\n        mouse    3  1\n        >>> df.groupby(level=0).var()\n                      a          b\n        dog    4.000000  12.333333\n        mouse  4.916667   2.250000\n        \"\"\"\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_var\n            return self._numba_agg_general(grouped_var, executor.float_dtype_mapping, engine_kwargs, min_periods=0, ddof=ddof)\n        else:\n            return self._cython_agg_general('var', alt=lambda x: Series(x, copy=False).var(ddof=ddof), numeric_only=numeric_only, ddof=ddof)\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def mean(self, numeric_only: bool=False, engine: Literal['cython', 'numba'] | None=None, engine_kwargs: dict[str, bool] | None=None):\n        \"\"\"\n        Compute mean of groups, excluding missing values.\n\n        Parameters\n        ----------\n        numeric_only : bool, default False\n            Include only float, int, boolean columns.\n\n            .. versionchanged:: 2.0.0\n\n                numeric_only no longer accepts ``None`` and defaults to ``False``.\n\n        engine : str, default None\n            * ``'cython'`` : Runs the operation through C-extensions from cython.\n            * ``'numba'`` : Runs the operation through JIT compiled code from numba.\n            * ``None`` : Defaults to ``'cython'`` or globally setting\n              ``compute.use_numba``\n\n            .. versionadded:: 1.4.0\n\n        engine_kwargs : dict, default None\n            * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n            * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n              and ``parallel`` dictionary keys. The values must either be ``True`` or\n              ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n              ``{{'nopython': True, 'nogil': False, 'parallel': False}}``\n\n            .. versionadded:: 1.4.0\n\n        Returns\n        -------\n        pandas.Series or pandas.DataFrame\n        %(see_also)s\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5],\n        ...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])\n\n        Groupby one column and return the mean of the remaining columns in\n        each group.\n\n        >>> df.groupby('A').mean()\n             B         C\n        A\n        1  3.0  1.333333\n        2  4.0  1.500000\n\n        Groupby two columns and return the mean of the remaining column.\n\n        >>> df.groupby(['A', 'B']).mean()\n                 C\n        A B\n        1 2.0  2.0\n          4.0  1.0\n        2 3.0  1.0\n          5.0  2.0\n\n        Groupby one column and return the mean of only particular column in\n        the group.\n\n        >>> df.groupby('A')['B'].mean()\n        A\n        1    3.0\n        2    4.0\n        Name: B, dtype: float64\n        \"\"\"\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import grouped_mean\n            return self._numba_agg_general(grouped_mean, executor.float_dtype_mapping, engine_kwargs, min_periods=0)\n        else:\n            result = self._cython_agg_general('mean', alt=lambda x: Series(x, copy=False).mean(numeric_only=numeric_only), numeric_only=numeric_only)\n            return result.__finalize__(self.obj, method='groupby')\n\n    @final\n    def _maybe_transpose_result(self, result: NDFrameT) -> NDFrameT:\n        if self.axis == 1:\n            result = result.T\n            if result.index.equals(self.obj.index):\n                result.index = self.obj.index.copy()\n        return result\n\n    @doc(DataFrame.describe)\n    def describe(self, percentiles=None, include=None, exclude=None) -> NDFrameT:\n        obj = self._obj_with_exclusions\n        if len(obj) == 0:\n            described = obj.describe(percentiles=percentiles, include=include, exclude=exclude)\n            if obj.ndim == 1:\n                result = described\n            else:\n                result = described.unstack()\n            return result.to_frame().T.iloc[:0]\n        with com.temp_setattr(self, 'as_index', True):\n            result = self._python_apply_general(lambda x: x.describe(percentiles=percentiles, include=include, exclude=exclude), obj, not_indexed_same=True)\n        if self.axis == 1:\n            return result.T\n        result = result.unstack()\n        if not self.as_index:\n            result = self._insert_inaxis_grouper(result)\n            result.index = default_index(len(result))\n        return result\n\n    @final\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def count(self) -> NDFrameT:\n        \"\"\"\n        Compute count of group, excluding missing values.\n\n        Returns\n        -------\n        Series or DataFrame\n            Count of values within each group.\n        %(see_also)s\n        Examples\n        --------\n        For SeriesGroupBy:\n\n        >>> lst = ['a', 'a', 'b']\n        >>> ser = pd.Series([1, 2, np.nan], index=lst)\n        >>> ser\n        a    1.0\n        a    2.0\n        b    NaN\n        dtype: float64\n        >>> ser.groupby(level=0).count()\n        a    2\n        b    0\n        dtype: int64\n\n        For DataFrameGroupBy:\n\n        >>> data = [[1, np.nan, 3], [1, np.nan, 6], [7, 8, 9]]\n        >>> df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n        ...                   index=[\"cow\", \"horse\", \"bull\"])\n        >>> df\n                a\t  b\tc\n        cow     1\tNaN\t3\n        horse\t1\tNaN\t6\n        bull\t7\t8.0\t9\n        >>> df.groupby(\"a\").count()\n            b   c\n        a\n        1   0   2\n        7   1   1\n\n        For Resampler:\n\n        >>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n        ...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n        >>> ser\n        2023-01-01    1\n        2023-01-15    2\n        2023-02-01    3\n        2023-02-15    4\n        dtype: int64\n        >>> ser.resample('MS').count()\n        2023-01-01    2\n        2023-02-01    2\n        Freq: MS, dtype: int64\n        \"\"\"\n        data = self._get_data_to_aggregate()\n        (ids, _, ngroups) = self._grouper.group_info\n        mask = ids != -1\n        is_series = data.ndim == 1\n\n        def hfunc(bvalues: ArrayLike) -> ArrayLike:\n            if bvalues.ndim == 1:\n                masked = mask & ~isna(bvalues).reshape(1, -1)\n            else:\n                masked = mask & ~isna(bvalues)\n            counted = lib.count_level_2d(masked, labels=ids, max_bin=ngroups)\n            if isinstance(bvalues, BaseMaskedArray):\n                return IntegerArray(counted[0], mask=np.zeros(counted.shape[1], dtype=np.bool_))\n            elif isinstance(bvalues, ArrowExtensionArray) and (not isinstance(bvalues.dtype, StringDtype)):\n                dtype = pandas_dtype('int64[pyarrow]')\n                return type(bvalues)._from_sequence(counted[0], dtype=dtype)\n            if is_series:\n                assert counted.ndim == 2\n                assert counted.shape[0] == 1\n                return counted[0]\n            return counted\n        new_mgr = data.grouped_reduce(hfunc)\n        new_obj = self._wrap_agged_manager(new_mgr)\n        with com.temp_setattr(self, 'observed', True):\n            result = self._wrap_aggregated_output(new_obj)\n        return self._reindex_output(result, fill_value=0)", "class_fn": true, "question_id": "pandas/pandas.core.groupby.groupby/GroupBy", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/indexing.py", "fn_id": "", "content": "class GroupByIndexingMixin:\n    \"\"\"\n    Mixin for adding ._positional_selector to GroupBy.\n    \"\"\"\n\n    @cache_readonly\n    def _positional_selector(self) -> GroupByPositionalSelector:\n        \"\"\"\n        Return positional selection for each group.\n\n        ``groupby._positional_selector[i:j]`` is similar to\n        ``groupby.apply(lambda x: x.iloc[i:j])``\n        but much faster and preserves the original index and order.\n\n        ``_positional_selector[]`` is compatible with and extends :meth:`~GroupBy.head`\n        and :meth:`~GroupBy.tail`. For example:\n\n        - ``head(5)``\n        - ``_positional_selector[5:-5]``\n        - ``tail(5)``\n\n        together return all the rows.\n\n        Allowed inputs for the index are:\n\n        - An integer valued iterable, e.g. ``range(2, 4)``.\n        - A comma separated list of integers and slices, e.g. ``5``, ``2, 4``, ``2:4``.\n\n        The output format is the same as :meth:`~GroupBy.head` and\n        :meth:`~GroupBy.tail`, namely\n        a subset of the ``DataFrame`` or ``Series`` with the index and order preserved.\n\n        Returns\n        -------\n        Series\n            The filtered subset of the original Series.\n        DataFrame\n            The filtered subset of the original DataFrame.\n\n        See Also\n        --------\n        DataFrame.iloc : Purely integer-location based indexing for selection by\n            position.\n        GroupBy.head : Return first n rows of each group.\n        GroupBy.tail : Return last n rows of each group.\n        GroupBy.nth : Take the nth row from each group if n is an int, or a\n            subset of rows, if n is a list of ints.\n\n        Notes\n        -----\n        - The slice step cannot be negative.\n        - If the index specification results in overlaps, the item is not duplicated.\n        - If the index specification changes the order of items, then\n          they are returned in their original order.\n          By contrast, ``DataFrame.iloc`` can change the row order.\n        - ``groupby()`` parameters such as as_index and dropna are ignored.\n\n        The differences between ``_positional_selector[]`` and :meth:`~GroupBy.nth`\n        with ``as_index=False`` are:\n\n        - Input to ``_positional_selector`` can include\n          one or more slices whereas ``nth``\n          just handles an integer or a list of integers.\n        - ``_positional_selector`` can  accept a slice relative to the\n          last row of each group.\n        - ``_positional_selector`` does not have an equivalent to the\n          ``nth()`` ``dropna`` parameter.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[\"a\", 1], [\"a\", 2], [\"a\", 3], [\"b\", 4], [\"b\", 5]],\n        ...                   columns=[\"A\", \"B\"])\n        >>> df.groupby(\"A\")._positional_selector[1:2]\n           A  B\n        1  a  2\n        4  b  5\n\n        >>> df.groupby(\"A\")._positional_selector[1, -1]\n           A  B\n        1  a  2\n        2  a  3\n        4  b  5\n        \"\"\"\n        if TYPE_CHECKING:\n            groupby_self = cast(groupby.GroupBy, self)\n        else:\n            groupby_self = self\n        return GroupByPositionalSelector(groupby_self)\n\n    def _make_mask_from_positional_indexer(self, arg: PositionalIndexer | tuple) -> np.ndarray:\n        if is_list_like(arg):\n            if all((is_integer(i) for i in cast(Iterable, arg))):\n                mask = self._make_mask_from_list(cast(Iterable[int], arg))\n            else:\n                mask = self._make_mask_from_tuple(cast(tuple, arg))\n        elif isinstance(arg, slice):\n            mask = self._make_mask_from_slice(arg)\n        elif is_integer(arg):\n            mask = self._make_mask_from_int(cast(int, arg))\n        else:\n            raise TypeError(f'Invalid index {type(arg)}. Must be integer, list-like, slice or a tuple of integers and slices')\n        if isinstance(mask, bool):\n            if mask:\n                mask = self._ascending_count >= 0\n            else:\n                mask = self._ascending_count < 0\n        return cast(np.ndarray, mask)\n\n    def _make_mask_from_int(self, arg: int) -> np.ndarray:\n        if arg >= 0:\n            return self._ascending_count == arg\n        else:\n            return self._descending_count == -arg - 1\n\n    def _make_mask_from_list(self, args: Iterable[int]) -> bool | np.ndarray:\n        positive = [arg for arg in args if arg >= 0]\n        negative = [-arg - 1 for arg in args if arg < 0]\n        mask: bool | np.ndarray = False\n        if positive:\n            mask |= np.isin(self._ascending_count, positive)\n        if negative:\n            mask |= np.isin(self._descending_count, negative)\n        return mask\n\n    def _make_mask_from_tuple(self, args: tuple) -> bool | np.ndarray:\n        mask: bool | np.ndarray = False\n        for arg in args:\n            if is_integer(arg):\n                mask |= self._make_mask_from_int(cast(int, arg))\n            elif isinstance(arg, slice):\n                mask |= self._make_mask_from_slice(arg)\n            else:\n                raise ValueError(f'Invalid argument {type(arg)}. Should be int or slice.')\n        return mask\n\n    def _make_mask_from_slice(self, arg: slice) -> bool | np.ndarray:\n        start = arg.start\n        stop = arg.stop\n        step = arg.step\n        if step is not None and step < 0:\n            raise ValueError(f'Invalid step {step}. Must be non-negative')\n        mask: bool | np.ndarray = True\n        if step is None:\n            step = 1\n        if start is None:\n            if step > 1:\n                mask &= self._ascending_count % step == 0\n        elif start >= 0:\n            mask &= self._ascending_count >= start\n            if step > 1:\n                mask &= (self._ascending_count - start) % step == 0\n        else:\n            mask &= self._descending_count < -start\n            offset_array = self._descending_count + start + 1\n            limit_array = self._ascending_count + self._descending_count + (start + 1) < 0\n            offset_array = np.where(limit_array, self._ascending_count, offset_array)\n            mask &= offset_array % step == 0\n        if stop is not None:\n            if stop >= 0:\n                mask &= self._ascending_count < stop\n            else:\n                mask &= self._descending_count >= -stop\n        return mask\n\n    @cache_readonly\n    def _ascending_count(self) -> np.ndarray:\n        if TYPE_CHECKING:\n            groupby_self = cast(groupby.GroupBy, self)\n        else:\n            groupby_self = self\n        return groupby_self._cumcount_array()\n\n    @cache_readonly\n    def _descending_count(self) -> np.ndarray:\n        if TYPE_CHECKING:\n            groupby_self = cast(groupby.GroupBy, self)\n        else:\n            groupby_self = self\n        return groupby_self._cumcount_array(ascending=False)", "class_fn": true, "question_id": "pandas/pandas.core.groupby.indexing/GroupByIndexingMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/groupby/ops.py", "fn_id": "", "content": "class WrappedCythonOp:\n    \"\"\"\n    Dispatch logic for functions defined in _libs.groupby\n\n    Parameters\n    ----------\n    kind: str\n        Whether the operation is an aggregate or transform.\n    how: str\n        Operation name, e.g. \"mean\".\n    has_dropped_na: bool\n        True precisely when dropna=True and the grouper contains a null value.\n    \"\"\"\n    cast_blocklist = frozenset(['any', 'all', 'rank', 'count', 'size', 'idxmin', 'idxmax'])\n\n    def __init__(self, kind: str, how: str, has_dropped_na: bool) -> None:\n        self.kind = kind\n        self.how = how\n        self.has_dropped_na = has_dropped_na\n    _CYTHON_FUNCTIONS: dict[str, dict] = {'aggregate': {'any': functools.partial(libgroupby.group_any_all, val_test='any'), 'all': functools.partial(libgroupby.group_any_all, val_test='all'), 'sum': 'group_sum', 'prod': 'group_prod', 'idxmin': functools.partial(libgroupby.group_idxmin_idxmax, name='idxmin'), 'idxmax': functools.partial(libgroupby.group_idxmin_idxmax, name='idxmax'), 'min': 'group_min', 'max': 'group_max', 'mean': 'group_mean', 'median': 'group_median_float64', 'var': 'group_var', 'std': functools.partial(libgroupby.group_var, name='std'), 'sem': functools.partial(libgroupby.group_var, name='sem'), 'skew': 'group_skew', 'first': 'group_nth', 'last': 'group_last', 'ohlc': 'group_ohlc'}, 'transform': {'cumprod': 'group_cumprod', 'cumsum': 'group_cumsum', 'cummin': 'group_cummin', 'cummax': 'group_cummax', 'rank': 'group_rank'}}\n    _cython_arity = {'ohlc': 4}\n\n    @classmethod\n    def get_kind_from_how(cls, how: str) -> str:\n        if how in cls._CYTHON_FUNCTIONS['aggregate']:\n            return 'aggregate'\n        return 'transform'\n\n    @classmethod\n    @functools.cache\n    def _get_cython_function(cls, kind: str, how: str, dtype: np.dtype, is_numeric: bool):\n        dtype_str = dtype.name\n        ftype = cls._CYTHON_FUNCTIONS[kind][how]\n        if callable(ftype):\n            f = ftype\n        else:\n            f = getattr(libgroupby, ftype)\n        if is_numeric:\n            return f\n        elif dtype == np.dtype(object):\n            if how in ['median', 'cumprod']:\n                raise NotImplementedError(f'function is not implemented for this dtype: [how->{how},dtype->{dtype_str}]')\n            elif how in ['std', 'sem', 'idxmin', 'idxmax']:\n                return f\n            elif how == 'skew':\n                pass\n            elif 'object' not in f.__signatures__:\n                raise NotImplementedError(f'function is not implemented for this dtype: [how->{how},dtype->{dtype_str}]')\n            return f\n        else:\n            raise NotImplementedError('This should not be reached. Please report a bug at github.com/pandas-dev/pandas/', dtype)\n\n    def _get_cython_vals(self, values: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Cast numeric dtypes to float64 for functions that only support that.\n\n        Parameters\n        ----------\n        values : np.ndarray\n\n        Returns\n        -------\n        values : np.ndarray\n        \"\"\"\n        how = self.how\n        if how in ['median', 'std', 'sem', 'skew']:\n            values = ensure_float64(values)\n        elif values.dtype.kind in 'iu':\n            if how in ['var', 'mean'] or (self.kind == 'transform' and self.has_dropped_na):\n                values = ensure_float64(values)\n            elif how in ['sum', 'ohlc', 'prod', 'cumsum', 'cumprod']:\n                if values.dtype.kind == 'i':\n                    values = ensure_int64(values)\n                else:\n                    values = ensure_uint64(values)\n        return values\n\n    def _get_output_shape(self, ngroups: int, values: np.ndarray) -> Shape:\n        how = self.how\n        kind = self.kind\n        arity = self._cython_arity.get(how, 1)\n        out_shape: Shape\n        if how == 'ohlc':\n            out_shape = (ngroups, arity)\n        elif arity > 1:\n            raise NotImplementedError(\"arity of more than 1 is not supported for the 'how' argument\")\n        elif kind == 'transform':\n            out_shape = values.shape\n        else:\n            out_shape = (ngroups,) + values.shape[1:]\n        return out_shape\n\n    def _get_out_dtype(self, dtype: np.dtype) -> np.dtype:\n        how = self.how\n        if how == 'rank':\n            out_dtype = 'float64'\n        elif how in ['idxmin', 'idxmax']:\n            out_dtype = 'intp'\n        elif dtype.kind in 'iufcb':\n            out_dtype = f'{dtype.kind}{dtype.itemsize}'\n        else:\n            out_dtype = 'object'\n        return np.dtype(out_dtype)\n\n    def _get_result_dtype(self, dtype: np.dtype) -> np.dtype:\n        \"\"\"\n        Get the desired dtype of a result based on the\n        input dtype and how it was computed.\n\n        Parameters\n        ----------\n        dtype : np.dtype\n\n        Returns\n        -------\n        np.dtype\n            The desired dtype of the result.\n        \"\"\"\n        how = self.how\n        if how in ['sum', 'cumsum', 'sum', 'prod', 'cumprod']:\n            if dtype == np.dtype(bool):\n                return np.dtype(np.int64)\n        elif how in ['mean', 'median', 'var', 'std', 'sem']:\n            if dtype.kind in 'fc':\n                return dtype\n            elif dtype.kind in 'iub':\n                return np.dtype(np.float64)\n        return dtype\n\n    @final\n    def _cython_op_ndim_compat(self, values: np.ndarray, *, min_count: int, ngroups: int, comp_ids: np.ndarray, mask: npt.NDArray[np.bool_] | None=None, result_mask: npt.NDArray[np.bool_] | None=None, **kwargs) -> np.ndarray:\n        if values.ndim == 1:\n            values2d = values[None, :]\n            if mask is not None:\n                mask = mask[None, :]\n            if result_mask is not None:\n                result_mask = result_mask[None, :]\n            res = self._call_cython_op(values2d, min_count=min_count, ngroups=ngroups, comp_ids=comp_ids, mask=mask, result_mask=result_mask, **kwargs)\n            if res.shape[0] == 1:\n                return res[0]\n            return res.T\n        return self._call_cython_op(values, min_count=min_count, ngroups=ngroups, comp_ids=comp_ids, mask=mask, result_mask=result_mask, **kwargs)\n\n    @final\n    def _call_cython_op(self, values: np.ndarray, *, min_count: int, ngroups: int, comp_ids: np.ndarray, mask: npt.NDArray[np.bool_] | None, result_mask: npt.NDArray[np.bool_] | None, **kwargs) -> np.ndarray:\n        orig_values = values\n        dtype = values.dtype\n        is_numeric = dtype.kind in 'iufcb'\n        is_datetimelike = dtype.kind in 'mM'\n        if is_datetimelike:\n            values = values.view('int64')\n            is_numeric = True\n        elif dtype.kind == 'b':\n            values = values.view('uint8')\n        if values.dtype == 'float16':\n            values = values.astype(np.float32)\n        if self.how in ['any', 'all']:\n            if mask is None:\n                mask = isna(values)\n            if dtype == object:\n                if kwargs['skipna']:\n                    if mask.any():\n                        values = values.copy()\n                        values[mask] = True\n            values = values.astype(bool, copy=False).view(np.int8)\n            is_numeric = True\n        values = values.T\n        if mask is not None:\n            mask = mask.T\n            if result_mask is not None:\n                result_mask = result_mask.T\n        out_shape = self._get_output_shape(ngroups, values)\n        func = self._get_cython_function(self.kind, self.how, values.dtype, is_numeric)\n        values = self._get_cython_vals(values)\n        out_dtype = self._get_out_dtype(values.dtype)\n        result = maybe_fill(np.empty(out_shape, dtype=out_dtype))\n        if self.kind == 'aggregate':\n            counts = np.zeros(ngroups, dtype=np.int64)\n            if self.how in ['idxmin', 'idxmax', 'min', 'max', 'mean', 'last', 'first', 'sum']:\n                func(out=result, counts=counts, values=values, labels=comp_ids, min_count=min_count, mask=mask, result_mask=result_mask, is_datetimelike=is_datetimelike, **kwargs)\n            elif self.how in ['sem', 'std', 'var', 'ohlc', 'prod', 'median']:\n                if self.how in ['std', 'sem']:\n                    kwargs['is_datetimelike'] = is_datetimelike\n                func(result, counts, values, comp_ids, min_count=min_count, mask=mask, result_mask=result_mask, **kwargs)\n            elif self.how in ['any', 'all']:\n                func(out=result, values=values, labels=comp_ids, mask=mask, result_mask=result_mask, **kwargs)\n                result = result.astype(bool, copy=False)\n            elif self.how in ['skew']:\n                func(out=result, counts=counts, values=values, labels=comp_ids, mask=mask, result_mask=result_mask, **kwargs)\n                if dtype == object:\n                    result = result.astype(object)\n            else:\n                raise NotImplementedError(f'{self.how} is not implemented')\n        else:\n            if self.how != 'rank':\n                kwargs['result_mask'] = result_mask\n            func(out=result, values=values, labels=comp_ids, ngroups=ngroups, is_datetimelike=is_datetimelike, mask=mask, **kwargs)\n        if self.kind == 'aggregate' and self.how not in ['idxmin', 'idxmax']:\n            if result.dtype.kind in 'iu' and (not is_datetimelike):\n                cutoff = max(0 if self.how in ['sum', 'prod'] else 1, min_count)\n                empty_groups = counts < cutoff\n                if empty_groups.any():\n                    if result_mask is not None:\n                        assert result_mask[empty_groups].all()\n                    else:\n                        result = result.astype('float64')\n                        result[empty_groups] = np.nan\n        result = result.T\n        if self.how not in self.cast_blocklist:\n            res_dtype = self._get_result_dtype(orig_values.dtype)\n            op_result = maybe_downcast_to_dtype(result, res_dtype)\n        else:\n            op_result = result\n        return op_result\n\n    @final\n    def _validate_axis(self, axis: AxisInt, values: ArrayLike) -> None:\n        if values.ndim > 2:\n            raise NotImplementedError('number of dimensions is currently limited to 2')\n        if values.ndim == 2:\n            assert axis == 1, axis\n        elif not is_1d_only_ea_dtype(values.dtype):\n            assert axis == 0\n\n    @final\n    def cython_operation(self, *, values: ArrayLike, axis: AxisInt, min_count: int=-1, comp_ids: np.ndarray, ngroups: int, **kwargs) -> ArrayLike:\n        \"\"\"\n        Call our cython function, with appropriate pre- and post- processing.\n        \"\"\"\n        self._validate_axis(axis, values)\n        if not isinstance(values, np.ndarray):\n            return values._groupby_op(how=self.how, has_dropped_na=self.has_dropped_na, min_count=min_count, ngroups=ngroups, ids=comp_ids, **kwargs)\n        return self._cython_op_ndim_compat(values, min_count=min_count, ngroups=ngroups, comp_ids=comp_ids, mask=None, **kwargs)", "class_fn": true, "question_id": "pandas/pandas.core.groupby.ops/WrappedCythonOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/accessors.py", "fn_id": "", "content": "@delegate_names(delegate=ArrowExtensionArray, accessors=TimedeltaArray._datetimelike_ops, typ='property', accessor_mapping=lambda x: f'_dt_{x}', raise_on_missing=False)\n@delegate_names(delegate=ArrowExtensionArray, accessors=TimedeltaArray._datetimelike_methods, typ='method', accessor_mapping=lambda x: f'_dt_{x}', raise_on_missing=False)\n@delegate_names(delegate=ArrowExtensionArray, accessors=DatetimeArray._datetimelike_ops, typ='property', accessor_mapping=lambda x: f'_dt_{x}', raise_on_missing=False)\n@delegate_names(delegate=ArrowExtensionArray, accessors=DatetimeArray._datetimelike_methods, typ='method', accessor_mapping=lambda x: f'_dt_{x}', raise_on_missing=False)\nclass ArrowTemporalProperties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n\n    def isocalendar(self) -> DataFrame:\n        from pandas import DataFrame\n        result = cast(ArrowExtensionArray, self._parent.array)._dt_isocalendar()._pa_array.combine_chunks()\n        iso_calendar_df = DataFrame({col: type(self._parent.array)(result.field(i)) for (i, col) in enumerate(['year', 'week', 'day'])})\n        return iso_calendar_df\n\n    def _freeze(self) -> None:\n        \"\"\"\n        Prevents setting additional attributes.\n        \"\"\"\n        object.__setattr__(self, '__frozen', True)\n\n    def to_pytimedelta(self):\n        return cast(ArrowExtensionArray, self._parent.array)._dt_to_pytimedelta()\n\n    def to_pydatetime(self):\n        warnings.warn(f'The behavior of {type(self).__name__}.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result', FutureWarning, stacklevel=find_stack_level())\n        return cast(ArrowExtensionArray, self._parent.array)._dt_to_pydatetime()\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular object.\n        \"\"\"\n        return object.__repr__(self)\n\n    def _delegate_method(self, name: str, *args, **kwargs):\n        if not hasattr(self._parent.array, f'_dt_{name}'):\n            raise NotImplementedError(f'dt.{name} is not supported for {self._parent.dtype}')\n        result = getattr(self._parent.array, f'_dt_{name}')(*args, **kwargs)\n        if self._orig is not None:\n            index = self._orig.index\n        else:\n            index = self._parent.index\n        result = type(self._parent)(result, index=index, name=self._parent.name).__finalize__(self._parent)\n        return result\n\n    @property\n    def components(self) -> DataFrame:\n        from pandas import DataFrame\n        components_df = DataFrame({col: getattr(self._parent.array, f'_dt_{col}') for col in ['days', 'hours', 'minutes', 'seconds', 'milliseconds', 'microseconds', 'nanoseconds']})\n        return components_df\n\n    def __setattr__(self, key: str, value) -> None:\n        if getattr(self, '__frozen', False) and (not (key == '_cache' or key in type(self).__dict__ or getattr(self, key, None) is not None)):\n            raise AttributeError(f\"You cannot add any new attribute '{key}'\")\n        object.__setattr__(self, key, value)\n\n    def _delegate_property_get(self, name: str):\n        if not hasattr(self._parent.array, f'_dt_{name}'):\n            raise NotImplementedError(f'dt.{name} is not supported for {self._parent.dtype}')\n        result = getattr(self._parent.array, f'_dt_{name}')\n        if not is_list_like(result):\n            return result\n        if self._orig is not None:\n            index = self._orig.index\n        else:\n            index = self._parent.index\n        result = type(self._parent)(result, index=index, name=self._parent.name).__finalize__(self._parent)\n        return result\n\n    def __init__(self, data: Series, orig) -> None:\n        if not isinstance(data, ABCSeries):\n            raise TypeError(f'cannot convert an object of type {type(data)} to a datetimelike index')\n        self._parent = data\n        self._orig = orig\n        self._freeze()", "class_fn": true, "question_id": "pandas/pandas.core.indexes.accessors/ArrowTemporalProperties", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/accessors.py", "fn_id": "", "content": "@delegate_names(delegate=TimedeltaArray, accessors=TimedeltaArray._datetimelike_ops, typ='property')\n@delegate_names(delegate=TimedeltaArray, accessors=TimedeltaArray._datetimelike_methods, typ='method')\nclass TimedeltaProperties(Properties):\n    \"\"\"\n    Accessor object for datetimelike properties of the Series values.\n\n    Returns a Series indexed like the original Series.\n    Raises TypeError if the Series does not contain datetimelike values.\n\n    Examples\n    --------\n    >>> seconds_series = pd.Series(\n    ...     pd.timedelta_range(start=\"1 second\", periods=3, freq=\"s\")\n    ... )\n    >>> seconds_series\n    0   0 days 00:00:01\n    1   0 days 00:00:02\n    2   0 days 00:00:03\n    dtype: timedelta64[ns]\n    >>> seconds_series.dt.seconds\n    0    1\n    1    2\n    2    3\n    dtype: int32\n    \"\"\"\n\n    def _get_values(self):\n        data = self._parent\n        if lib.is_np_dtype(data.dtype, 'M'):\n            return DatetimeIndex(data, copy=False, name=self.name)\n        elif isinstance(data.dtype, DatetimeTZDtype):\n            return DatetimeIndex(data, copy=False, name=self.name)\n        elif lib.is_np_dtype(data.dtype, 'm'):\n            return TimedeltaIndex(data, copy=False, name=self.name)\n        elif isinstance(data.dtype, PeriodDtype):\n            return PeriodArray(data, copy=False)\n        raise TypeError(f'cannot convert an object of type {type(data)} to a datetimelike index')\n\n    @property\n    def components(self):\n        \"\"\"\n        Return a Dataframe of the components of the Timedeltas.\n\n        Returns\n        -------\n        DataFrame\n\n        Examples\n        --------\n        >>> s = pd.Series(pd.to_timedelta(np.arange(5), unit='s'))\n        >>> s\n        0   0 days 00:00:00\n        1   0 days 00:00:01\n        2   0 days 00:00:02\n        3   0 days 00:00:03\n        4   0 days 00:00:04\n        dtype: timedelta64[ns]\n        >>> s.dt.components\n           days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\n        0     0      0        0        0             0             0            0\n        1     0      0        0        1             0             0            0\n        2     0      0        0        2             0             0            0\n        3     0      0        0        3             0             0            0\n        4     0      0        0        4             0             0            0\n        \"\"\"\n        return self._get_values().components.set_index(self._parent.index).__finalize__(self._parent)\n\n    @property\n    def freq(self):\n        return self._get_values().inferred_freq\n\n    def _delegate_property_get(self, name: str):\n        from pandas import Series\n        values = self._get_values()\n        result = getattr(values, name)\n        if isinstance(result, np.ndarray):\n            if is_integer_dtype(result):\n                result = result.astype('int64')\n        elif not is_list_like(result):\n            return result\n        result = np.asarray(result)\n        if self.orig is not None:\n            index = self.orig.index\n        else:\n            index = self._parent.index\n        result = Series(result, index=index, name=self.name).__finalize__(self._parent)\n        result._is_copy = 'modifications to a property of a datetimelike object are not supported and are discarded. Change values on the original.'\n        return result\n\n    def to_pytimedelta(self) -> np.ndarray:\n        \"\"\"\n        Return an array of native :class:`datetime.timedelta` objects.\n\n        Python's standard `datetime` library uses a different representation\n        timedelta's. This method converts a Series of pandas Timedeltas\n        to `datetime.timedelta` format with the same length as the original\n        Series.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of 1D containing data with `datetime.timedelta` type.\n\n        See Also\n        --------\n        datetime.timedelta : A duration expressing the difference\n            between two date, time, or datetime.\n\n        Examples\n        --------\n        >>> s = pd.Series(pd.to_timedelta(np.arange(5), unit=\"d\"))\n        >>> s\n        0   0 days\n        1   1 days\n        2   2 days\n        3   3 days\n        4   4 days\n        dtype: timedelta64[ns]\n\n        >>> s.dt.to_pytimedelta()\n        array([datetime.timedelta(0), datetime.timedelta(days=1),\n        datetime.timedelta(days=2), datetime.timedelta(days=3),\n        datetime.timedelta(days=4)], dtype=object)\n        \"\"\"\n        return self._get_values().to_pytimedelta()\n\n    def _delegate_property_set(self, name: str, value, *args, **kwargs):\n        raise ValueError('modifications to a property of a datetimelike object are not supported. Change values on the original.')", "class_fn": true, "question_id": "pandas/pandas.core.indexes.accessors/TimedeltaProperties", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/datetimelike.py", "fn_id": "", "content": "class DatetimeIndexOpsMixin(NDArrayBackedExtensionIndex, ABC):\n    \"\"\"\n    Common ops mixin to support a unified interface datetimelike Index.\n    \"\"\"\n    _can_hold_strings = False\n    _data: DatetimeArray | TimedeltaArray | PeriodArray\n\n    @Appender(Index.__contains__.__doc__)\n    def __contains__(self, key: Any) -> bool:\n        hash(key)\n        try:\n            self.get_loc(key)\n        except (KeyError, TypeError, ValueError, InvalidIndexError):\n            return False\n        return True\n\n    @property\n    def freq(self) -> BaseOffset | None:\n        return self._data.freq\n\n    @freq.setter\n    def freq(self, value) -> None:\n        self._data.freq = value\n\n    @property\n    def asi8(self) -> npt.NDArray[np.int64]:\n        return self._data.asi8\n\n    @property\n    @doc(DatetimeLikeArrayMixin.freqstr)\n    def freqstr(self) -> str:\n        from pandas import PeriodIndex\n        if self._data.freqstr is not None and isinstance(self._data, (PeriodArray, PeriodIndex)):\n            freq = freq_to_period_freqstr(self._data.freq.n, self._data.freq.name)\n            return freq\n        else:\n            return self._data.freqstr\n\n    @cache_readonly\n    @abstractmethod\n    def _resolution_obj(self) -> Resolution:\n        ...\n\n    @cache_readonly\n    @doc(DatetimeLikeArrayMixin.resolution)\n    def resolution(self) -> str:\n        return self._data.resolution\n\n    @cache_readonly\n    def hasnans(self) -> bool:\n        return self._data._hasna\n\n    def _parsed_string_to_bounds(self, reso: Resolution, parsed):\n        raise NotImplementedError\n\n    @doc(Index._maybe_cast_listlike_indexer)\n    def _maybe_cast_listlike_indexer(self, keyarr):\n        try:\n            res = self._data._validate_listlike(keyarr, allow_object=True)\n        except (ValueError, TypeError):\n            if not isinstance(keyarr, ExtensionArray):\n                res = com.asarray_tuplesafe(keyarr)\n            else:\n                res = keyarr\n        return Index(res, dtype=res.dtype)\n\n    def format(self, name: bool=False, formatter: Callable | None=None, na_rep: str='NaT', date_format: str | None=None) -> list[str]:\n        \"\"\"\n        Render a string representation of the Index.\n        \"\"\"\n        warnings.warn(f'{type(self).__name__}.format is deprecated and will be removed in a future version. Convert using index.astype(str) or index.map(formatter) instead.', FutureWarning, stacklevel=find_stack_level())\n        header = []\n        if name:\n            header.append(ibase.pprint_thing(self.name, escape_chars=('\\t', '\\r', '\\n')) if self.name is not None else '')\n        if formatter is not None:\n            return header + list(self.map(formatter))\n        return self._format_with_header(header=header, na_rep=na_rep, date_format=date_format)\n    _default_na_rep = 'NaT'\n\n    def _from_join_target(self, result: np.ndarray) -> ArrayLike:\n        assert result.dtype == self._data._ndarray.dtype\n        return self._data._from_backing_data(result)\n\n    @final\n    def _can_partial_date_slice(self, reso: Resolution) -> bool:\n        return reso > self._resolution_obj\n\n    @property\n    def _formatter_func(self):\n        return self._data._formatter()\n\n    def equals(self, other: Any) -> bool:\n        \"\"\"\n        Determines if two Index objects contain the same elements.\n        \"\"\"\n        if self.is_(other):\n            return True\n        if not isinstance(other, Index):\n            return False\n        elif other.dtype.kind in 'iufc':\n            return False\n        elif not isinstance(other, type(self)):\n            should_try = False\n            inferable = self._data._infer_matches\n            if other.dtype == object:\n                should_try = other.inferred_type in inferable\n            elif isinstance(other.dtype, CategoricalDtype):\n                other = cast('CategoricalIndex', other)\n                should_try = other.categories.inferred_type in inferable\n            if should_try:\n                try:\n                    other = type(self)(other)\n                except (ValueError, TypeError, OverflowError):\n                    return False\n        if self.dtype != other.dtype:\n            return False\n        return np.array_equal(self.asi8, other.asi8)\n\n    @doc(DatetimeLikeArrayMixin.mean)\n    def mean(self, *, skipna: bool=True, axis: int | None=0):\n        return self._data.mean(skipna=skipna, axis=axis)\n\n    def _format_with_header(self, *, header: list[str], na_rep: str, date_format: str | None=None) -> list[str]:\n        return header + list(self._get_values_for_csv(na_rep=na_rep, date_format=date_format))\n\n    @final\n    def _partial_date_slice(self, reso: Resolution, parsed: datetime) -> slice | npt.NDArray[np.intp]:\n        \"\"\"\n        Parameters\n        ----------\n        reso : Resolution\n        parsed : datetime\n\n        Returns\n        -------\n        slice or ndarray[intp]\n        \"\"\"\n        if not self._can_partial_date_slice(reso):\n            raise ValueError\n        (t1, t2) = self._parsed_string_to_bounds(reso, parsed)\n        vals = self._data._ndarray\n        unbox = self._data._unbox\n        if self.is_monotonic_increasing:\n            if len(self) and (t1 < self[0] and t2 < self[0] or (t1 > self[-1] and t2 > self[-1])):\n                raise KeyError\n            left = vals.searchsorted(unbox(t1), side='left')\n            right = vals.searchsorted(unbox(t2), side='right')\n            return slice(left, right)\n        else:\n            lhs_mask = vals >= unbox(t1)\n            rhs_mask = vals <= unbox(t2)\n            return (lhs_mask & rhs_mask).nonzero()[0]\n\n    def _convert_tolerance(self, tolerance, target):\n        tolerance = np.asarray(to_timedelta(tolerance).to_numpy())\n        return super()._convert_tolerance(tolerance, target)\n\n    def _get_string_slice(self, key: str):\n        (parsed, reso) = self._parse_with_reso(key)\n        try:\n            return self._partial_date_slice(reso, parsed)\n        except KeyError as err:\n            raise KeyError(key) from err\n\n    @Appender(Index._summary.__doc__)\n    def _summary(self, name=None) -> str:\n        result = super()._summary(name=name)\n        if self.freq:\n            result += f'\\nFreq: {self.freqstr}'\n        return result\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value).\n        \"\"\"\n        attrs = super()._format_attrs()\n        for attrib in self._attributes:\n            if attrib == 'freq':\n                freq = self.freqstr\n                if freq is not None:\n                    freq = repr(freq)\n                attrs.append(('freq', freq))\n        return attrs\n\n    def shift(self, periods: int=1, freq=None) -> Self:\n        \"\"\"\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.DatetimeIndex\n            Shifted index.\n\n        See Also\n        --------\n        Index.shift : Shift values of Index.\n        PeriodIndex.shift : Shift values of PeriodIndex.\n        \"\"\"\n        raise NotImplementedError\n\n    def _maybe_cast_slice_bound(self, label, side: str):\n        \"\"\"\n        If label is a string, cast it to scalar type according to resolution.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n\n        Returns\n        -------\n        label : object\n\n        Notes\n        -----\n        Value of `side` parameter should be validated in caller.\n        \"\"\"\n        if isinstance(label, str):\n            try:\n                (parsed, reso) = self._parse_with_reso(label)\n            except ValueError as err:\n                self._raise_invalid_indexer('slice', label, err)\n            (lower, upper) = self._parsed_string_to_bounds(reso, parsed)\n            return lower if side == 'left' else upper\n        elif not isinstance(label, self._data._recognized_scalars):\n            self._raise_invalid_indexer('slice', label)\n        return label\n\n    def _get_engine_target(self) -> np.ndarray:\n        return self._data._ndarray\n\n    def _parse_with_reso(self, label: str):\n        try:\n            if self.freq is None or hasattr(self.freq, 'rule_code'):\n                freq = self.freq\n        except NotImplementedError:\n            freq = getattr(self, 'freqstr', getattr(self, 'inferred_freq', None))\n        freqstr: str | None\n        if freq is not None and (not isinstance(freq, str)):\n            freqstr = freq.rule_code\n        else:\n            freqstr = freq\n        if isinstance(label, np.str_):\n            label = str(label)\n        (parsed, reso_str) = parsing.parse_datetime_string_with_reso(label, freqstr)\n        reso = Resolution.from_attrname(reso_str)\n        return (parsed, reso)", "class_fn": true, "question_id": "pandas/pandas.core.indexes.datetimelike/DatetimeIndexOpsMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/frozen.py", "fn_id": "", "content": "class FrozenList(PandasObject, list):\n    \"\"\"\n    Container that doesn't allow setting item *but*\n    because it's technically hashable, will be used\n    for lookups, appropriately, etc.\n    \"\"\"\n\n    def __hash__(self) -> int:\n        return hash(tuple(self))\n\n    def __reduce__(self):\n        return (type(self), (list(self),))\n    __add__ = __iadd__ = union\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, (tuple, FrozenList)):\n            other = list(other)\n        return super().__eq__(other)\n\n    def __sizeof__(self) -> int:\n        \"\"\"\n        Generates the total memory usage for an object that returns\n        either a value or Series of values\n        \"\"\"\n        memory_usage = getattr(self, 'memory_usage', None)\n        if memory_usage:\n            mem = memory_usage(deep=True)\n            return int(mem if is_scalar(mem) else mem.sum())\n        return super().__sizeof__()\n\n    def _disabled(self, *args, **kwargs) -> NoReturn:\n        \"\"\"\n        This method will not function because object is immutable.\n        \"\"\"\n        raise TypeError(f\"'{type(self).__name__}' does not support mutable operations.\")\n    __req__ = __eq__\n\n    def union(self, other) -> FrozenList:\n        \"\"\"\n        Returns a FrozenList with other concatenated to the end of self.\n\n        Parameters\n        ----------\n        other : array-like\n            The array-like whose elements we are concatenating.\n\n        Returns\n        -------\n        FrozenList\n            The collection difference between self and other.\n        \"\"\"\n        if isinstance(other, tuple):\n            other = list(other)\n        return type(self)(super().__add__(other))\n    __imul__ = __mul__\n\n    def __radd__(self, other) -> Self:\n        if isinstance(other, tuple):\n            other = list(other)\n        return type(self)(other + list(self))\n\n    def difference(self, other) -> FrozenList:\n        \"\"\"\n        Returns a FrozenList with elements from other removed from self.\n\n        Parameters\n        ----------\n        other : array-like\n            The array-like whose elements we are removing self.\n\n        Returns\n        -------\n        FrozenList\n            The collection difference between self and other.\n        \"\"\"\n        other = set(other)\n        temp = [x for x in self if x not in other]\n        return type(self)(temp)\n\n    def __repr__(self) -> str:\n        return f'{type(self).__name__}({str(self)})'\n\n    def __getitem__(self, n):\n        if isinstance(n, slice):\n            return type(self)(super().__getitem__(n))\n        return super().__getitem__(n)\n\n    def __mul__(self, other) -> Self:\n        return type(self)(super().__mul__(other))\n    __setitem__ = __setslice__ = _disabled\n    __delitem__ = __delslice__ = _disabled\n    pop = append = extend = _disabled\n    remove = sort = insert = _disabled\n\n    def __str__(self) -> str:\n        return pprint_thing(self, quote_strings=True, escape_chars=('\\t', '\\r', '\\n'))\n\n    def _reset_cache(self, key: str | None=None) -> None:\n        \"\"\"\n        Reset cached properties. If ``key`` is passed, only clears that key.\n        \"\"\"\n        if not hasattr(self, '_cache'):\n            return\n        if key is None:\n            self._cache.clear()\n        else:\n            self._cache.pop(key, None)", "class_fn": true, "question_id": "pandas/pandas.core.indexes.frozen/FrozenList", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexes/period.py", "fn_id": "", "content": "@inherit_names(['strftime', 'start_time', 'end_time'] + PeriodArray._field_ops, PeriodArray, wrap=True)\n@inherit_names(['is_leap_year'], PeriodArray)\nclass PeriodIndex(DatetimeIndexOpsMixin):\n    \"\"\"\n    Immutable ndarray holding ordinal values indicating regular periods in time.\n\n    Index keys are boxed to Period objects which carries the metadata (eg,\n    frequency information).\n\n    Parameters\n    ----------\n    data : array-like (1d int np.ndarray or PeriodArray), optional\n        Optional period-like data to construct index with.\n    copy : bool\n        Make a copy of input ndarray.\n    freq : str or period object, optional\n        One of pandas period strings or corresponding objects.\n    year : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    month : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    quarter : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    day : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    hour : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    minute : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    second : int, array, or Series, default None\n\n        .. deprecated:: 2.2.0\n           Use PeriodIndex.from_fields instead.\n    dtype : str or PeriodDtype, default None\n\n    Attributes\n    ----------\n    day\n    dayofweek\n    day_of_week\n    dayofyear\n    day_of_year\n    days_in_month\n    daysinmonth\n    end_time\n    freq\n    freqstr\n    hour\n    is_leap_year\n    minute\n    month\n    quarter\n    qyear\n    second\n    start_time\n    week\n    weekday\n    weekofyear\n    year\n\n    Methods\n    -------\n    asfreq\n    strftime\n    to_timestamp\n    from_fields\n    from_ordinals\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n    Period : Represents a period of time.\n    DatetimeIndex : Index with datetime64 data.\n    TimedeltaIndex : Index of timedelta64 data.\n    period_range : Create a fixed-frequency PeriodIndex.\n\n    Examples\n    --------\n    >>> idx = pd.PeriodIndex.from_fields(year=[2000, 2002], quarter=[1, 3])\n    >>> idx\n    PeriodIndex(['2000Q1', '2002Q3'], dtype='period[Q-DEC]')\n    \"\"\"\n    _typ = 'periodindex'\n    _data: PeriodArray\n    freq: BaseOffset\n    dtype: PeriodDtype\n    _data_cls = PeriodArray\n    _supports_partial_string_indexing = True\n\n    @property\n    def _engine_type(self) -> type[libindex.PeriodEngine]:\n        return libindex.PeriodEngine\n\n    @cache_readonly\n    def _resolution_obj(self) -> Resolution:\n        return self.dtype._resolution_obj\n\n    def _parsed_string_to_bounds(self, reso: Resolution, parsed: datetime):\n        freq = OFFSET_TO_PERIOD_FREQSTR.get(reso.attr_abbrev, reso.attr_abbrev)\n        iv = Period(parsed, freq=freq)\n        return (iv.asfreq(self.freq, how='start'), iv.asfreq(self.freq, how='end'))\n\n    @doc(PeriodArray.to_timestamp)\n    def to_timestamp(self, freq=None, how: str='start') -> DatetimeIndex:\n        arr = self._data.to_timestamp(freq, how)\n        return DatetimeIndex._simple_new(arr, name=self.name)\n\n    @property\n    @doc(PeriodArray.hour.fget)\n    def hour(self) -> Index:\n        return Index(self._data.hour, name=self.name)\n\n    @property\n    @doc(PeriodArray.minute.fget)\n    def minute(self) -> Index:\n        return Index(self._data.minute, name=self.name)\n\n    @property\n    @doc(PeriodArray.second.fget)\n    def second(self) -> Index:\n        return Index(self._data.second, name=self.name)\n\n    def __new__(cls, data=None, ordinal=None, freq=None, dtype: Dtype | None=None, copy: bool=False, name: Hashable | None=None, **fields) -> Self:\n        valid_field_set = {'year', 'month', 'day', 'quarter', 'hour', 'minute', 'second'}\n        refs = None\n        if not copy and isinstance(data, (Index, ABCSeries)):\n            refs = data._references\n        if not set(fields).issubset(valid_field_set):\n            argument = next(iter(set(fields) - valid_field_set))\n            raise TypeError(f'__new__() got an unexpected keyword argument {argument}')\n        elif len(fields):\n            warnings.warn('Constructing PeriodIndex from fields is deprecated. Use PeriodIndex.from_fields instead.', FutureWarning, stacklevel=find_stack_level())\n        if ordinal is not None:\n            warnings.warn(\"The 'ordinal' keyword in PeriodIndex is deprecated and will be removed in a future version. Use PeriodIndex.from_ordinals instead.\", FutureWarning, stacklevel=find_stack_level())\n        name = maybe_extract_name(name, data, cls)\n        if data is None and ordinal is None:\n            if not fields:\n                cls._raise_scalar_data_error(None)\n            data = cls.from_fields(**fields, freq=freq)._data\n            copy = False\n        elif fields:\n            if data is not None:\n                raise ValueError('Cannot pass both data and fields')\n            raise ValueError('Cannot pass both ordinal and fields')\n        else:\n            freq = validate_dtype_freq(dtype, freq)\n            if freq and isinstance(data, cls) and (data.freq != freq):\n                data = data.asfreq(freq)\n            if data is None and ordinal is not None:\n                ordinal = np.asarray(ordinal, dtype=np.int64)\n                dtype = PeriodDtype(freq)\n                data = PeriodArray(ordinal, dtype=dtype)\n            elif data is not None and ordinal is not None:\n                raise ValueError('Cannot pass both data and ordinal')\n            else:\n                data = period_array(data=data, freq=freq)\n        if copy:\n            data = data.copy()\n        return cls._simple_new(data, name=name, refs=refs)\n\n    @final\n    def _partial_date_slice(self, reso: Resolution, parsed: datetime) -> slice | npt.NDArray[np.intp]:\n        \"\"\"\n        Parameters\n        ----------\n        reso : Resolution\n        parsed : datetime\n\n        Returns\n        -------\n        slice or ndarray[intp]\n        \"\"\"\n        if not self._can_partial_date_slice(reso):\n            raise ValueError\n        (t1, t2) = self._parsed_string_to_bounds(reso, parsed)\n        vals = self._data._ndarray\n        unbox = self._data._unbox\n        if self.is_monotonic_increasing:\n            if len(self) and (t1 < self[0] and t2 < self[0] or (t1 > self[-1] and t2 > self[-1])):\n                raise KeyError\n            left = vals.searchsorted(unbox(t1), side='left')\n            right = vals.searchsorted(unbox(t2), side='right')\n            return slice(left, right)\n        else:\n            lhs_mask = vals >= unbox(t1)\n            rhs_mask = vals <= unbox(t2)\n            return (lhs_mask & rhs_mask).nonzero()[0]\n\n    @doc(PeriodArray.asfreq, other='pandas.arrays.PeriodArray', other_name='PeriodArray', **_shared_doc_kwargs)\n    def asfreq(self, freq=None, how: str='E') -> Self:\n        arr = self._data.asfreq(freq, how)\n        return type(self)._simple_new(arr, name=self.name)\n\n    @property\n    def values(self) -> npt.NDArray[np.object_]:\n        return np.asarray(self, dtype=object)\n\n    def _disallow_mismatched_indexing(self, key: Period) -> None:\n        if key._dtype != self.dtype:\n            raise KeyError(key)\n\n    @doc(DatetimeIndexOpsMixin.shift)\n    def shift(self, periods: int=1, freq=None) -> Self:\n        if freq is not None:\n            raise TypeError(f'`freq` argument is not supported for {type(self).__name__}.shift')\n        return self + periods\n\n    def _convert_tolerance(self, tolerance, target):\n        tolerance = super()._convert_tolerance(tolerance, target)\n        if self.dtype == target.dtype:\n            tolerance = self._maybe_convert_timedelta(tolerance)\n        return tolerance\n\n    @property\n    def is_full(self) -> bool:\n        \"\"\"\n        Returns True if this PeriodIndex is range-like in that all Periods\n        between start and end are present, in order.\n        \"\"\"\n        if len(self) == 0:\n            return True\n        if not self.is_monotonic_increasing:\n            raise ValueError('Index is not monotonic')\n        values = self.asi8\n        return bool((values[1:] - values[:-1] < 2).all())\n\n    @property\n    def inferred_type(self) -> str:\n        return 'period'\n\n    def get_loc(self, key):\n        \"\"\"\n        Get integer location for requested label.\n\n        Parameters\n        ----------\n        key : Period, NaT, str, or datetime\n            String or datetime key must be parsable as Period.\n\n        Returns\n        -------\n        loc : int or ndarray[int64]\n\n        Raises\n        ------\n        KeyError\n            Key is not present in the index.\n        TypeError\n            If key is listlike or otherwise not hashable.\n        \"\"\"\n        orig_key = key\n        self._check_indexing_error(key)\n        if is_valid_na_for_dtype(key, self.dtype):\n            key = NaT\n        elif isinstance(key, str):\n            try:\n                (parsed, reso) = self._parse_with_reso(key)\n            except ValueError as err:\n                raise KeyError(f\"Cannot interpret '{key}' as period\") from err\n            if self._can_partial_date_slice(reso):\n                try:\n                    return self._partial_date_slice(reso, parsed)\n                except KeyError as err:\n                    raise KeyError(key) from err\n            if reso == self._resolution_obj:\n                key = self._cast_partial_indexing_scalar(parsed)\n            else:\n                raise KeyError(key)\n        elif isinstance(key, Period):\n            self._disallow_mismatched_indexing(key)\n        elif isinstance(key, datetime):\n            key = self._cast_partial_indexing_scalar(key)\n        else:\n            raise KeyError(key)\n        try:\n            return Index.get_loc(self, key)\n        except KeyError as err:\n            raise KeyError(orig_key) from err\n\n    @classmethod\n    def from_fields(cls, *, year=None, quarter=None, month=None, day=None, hour=None, minute=None, second=None, freq=None) -> Self:\n        fields = {'year': year, 'quarter': quarter, 'month': month, 'day': day, 'hour': hour, 'minute': minute, 'second': second}\n        fields = {key: value for (key, value) in fields.items() if value is not None}\n        arr = PeriodArray._from_fields(fields=fields, freq=freq)\n        return cls._simple_new(arr)\n\n    @doc(Index._maybe_cast_listlike_indexer)\n    def _maybe_cast_listlike_indexer(self, keyarr):\n        try:\n            res = self._data._validate_listlike(keyarr, allow_object=True)\n        except (ValueError, TypeError):\n            if not isinstance(keyarr, ExtensionArray):\n                res = com.asarray_tuplesafe(keyarr)\n            else:\n                res = keyarr\n        return Index(res, dtype=res.dtype)\n\n    @doc(DatetimeLikeArrayMixin.mean)\n    def mean(self, *, skipna: bool=True, axis: int | None=0):\n        return self._data.mean(skipna=skipna, axis=axis)\n\n    def asof_locs(self, where: Index, mask: npt.NDArray[np.bool_]) -> np.ndarray:\n        \"\"\"\n        where : array of timestamps\n        mask : np.ndarray[bool]\n            Array of booleans where data is not NA.\n        \"\"\"\n        if isinstance(where, DatetimeIndex):\n            where = PeriodIndex(where._values, freq=self.freq)\n        elif not isinstance(where, PeriodIndex):\n            raise TypeError('asof_locs `where` must be DatetimeIndex or PeriodIndex')\n        return super().asof_locs(where, mask)\n\n    def _maybe_convert_timedelta(self, other) -> int | npt.NDArray[np.int64]:\n        \"\"\"\n        Convert timedelta-like input to an integer multiple of self.freq\n\n        Parameters\n        ----------\n        other : timedelta, np.timedelta64, DateOffset, int, np.ndarray\n\n        Returns\n        -------\n        converted : int, np.ndarray[int64]\n\n        Raises\n        ------\n        IncompatibleFrequency : if the input cannot be written as a multiple\n            of self.freq.  Note IncompatibleFrequency subclasses ValueError.\n        \"\"\"\n        if isinstance(other, (timedelta, np.timedelta64, Tick, np.ndarray)):\n            if isinstance(self.freq, Tick):\n                delta = self._data._check_timedeltalike_freq_compat(other)\n                return delta\n        elif isinstance(other, BaseOffset):\n            if other.base == self.freq.base:\n                return other.n\n            raise raise_on_incompatible(self, other)\n        elif is_integer(other):\n            assert isinstance(other, int)\n            return other\n        raise raise_on_incompatible(self, None)\n\n    def _cast_partial_indexing_scalar(self, label: datetime) -> Period:\n        try:\n            period = Period(label, freq=self.freq)\n        except ValueError as err:\n            raise KeyError(label) from err\n        return period\n\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        \"\"\"\n        Can we compare values of the given dtype to our own?\n        \"\"\"\n        return self.dtype == dtype\n\n    @doc(DatetimeIndexOpsMixin._maybe_cast_slice_bound)\n    def _maybe_cast_slice_bound(self, label, side: str):\n        if isinstance(label, datetime):\n            label = self._cast_partial_indexing_scalar(label)\n        return super()._maybe_cast_slice_bound(label, side)\n\n    @classmethod\n    def from_ordinals(cls, ordinals, *, freq, name=None) -> Self:\n        ordinals = np.asarray(ordinals, dtype=np.int64)\n        dtype = PeriodDtype(freq)\n        data = PeriodArray._simple_new(ordinals, dtype=dtype)\n        return cls._simple_new(data, name=name)", "class_fn": true, "question_id": "pandas/pandas.core.indexes.period/PeriodIndex", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "pandas", "url": "pandas==2.2.3", "last_update_at": "", "file": "pandas/core/indexing.py", "fn_id": "", "content": "class IndexingMixin:\n    \"\"\"\n    Mixin for adding .loc/.iloc/.at/.iat to Dataframes and Series.\n    \"\"\"\n\n    @property\n    def iloc(self) -> _iLocIndexer:\n        \"\"\"\n        Purely integer-location based indexing for selection by position.\n\n        .. deprecated:: 2.2.0\n\n           Returning a tuple from a callable is deprecated.\n\n        ``.iloc[]`` is primarily integer position based (from ``0`` to\n        ``length-1`` of the axis), but may also be used with a boolean\n        array.\n\n        Allowed inputs are:\n\n        - An integer, e.g. ``5``.\n        - A list or array of integers, e.g. ``[4, 3, 0]``.\n        - A slice object with ints, e.g. ``1:7``.\n        - A boolean array.\n        - A ``callable`` function with one argument (the calling Series or\n          DataFrame) and that returns valid output for indexing (one of the above).\n          This is useful in method chains, when you don't have a reference to the\n          calling object, but would like to base your selection on\n          some value.\n        - A tuple of row and column indexes. The tuple elements consist of one of the\n          above inputs, e.g. ``(0, 1)``.\n\n        ``.iloc`` will raise ``IndexError`` if a requested indexer is\n        out-of-bounds, except *slice* indexers which allow out-of-bounds\n        indexing (this conforms with python/numpy *slice* semantics).\n\n        See more at :ref:`Selection by Position <indexing.integer>`.\n\n        See Also\n        --------\n        DataFrame.iat : Fast integer location scalar accessor.\n        DataFrame.loc : Purely label-location based indexer for selection by label.\n        Series.iloc : Purely integer-location based indexing for\n                       selection by position.\n\n        Examples\n        --------\n        >>> mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n        ...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n        ...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000}]\n        >>> df = pd.DataFrame(mydict)\n        >>> df\n              a     b     c     d\n        0     1     2     3     4\n        1   100   200   300   400\n        2  1000  2000  3000  4000\n\n        **Indexing just the rows**\n\n        With a scalar integer.\n\n        >>> type(df.iloc[0])\n        <class 'pandas.core.series.Series'>\n        >>> df.iloc[0]\n        a    1\n        b    2\n        c    3\n        d    4\n        Name: 0, dtype: int64\n\n        With a list of integers.\n\n        >>> df.iloc[[0]]\n           a  b  c  d\n        0  1  2  3  4\n        >>> type(df.iloc[[0]])\n        <class 'pandas.core.frame.DataFrame'>\n\n        >>> df.iloc[[0, 1]]\n             a    b    c    d\n        0    1    2    3    4\n        1  100  200  300  400\n\n        With a `slice` object.\n\n        >>> df.iloc[:3]\n              a     b     c     d\n        0     1     2     3     4\n        1   100   200   300   400\n        2  1000  2000  3000  4000\n\n        With a boolean mask the same length as the index.\n\n        >>> df.iloc[[True, False, True]]\n              a     b     c     d\n        0     1     2     3     4\n        2  1000  2000  3000  4000\n\n        With a callable, useful in method chains. The `x` passed\n        to the ``lambda`` is the DataFrame being sliced. This selects\n        the rows whose index label even.\n\n        >>> df.iloc[lambda x: x.index % 2 == 0]\n              a     b     c     d\n        0     1     2     3     4\n        2  1000  2000  3000  4000\n\n        **Indexing both axes**\n\n        You can mix the indexer types for the index and columns. Use ``:`` to\n        select the entire axis.\n\n        With scalar integers.\n\n        >>> df.iloc[0, 1]\n        2\n\n        With lists of integers.\n\n        >>> df.iloc[[0, 2], [1, 3]]\n              b     d\n        0     2     4\n        2  2000  4000\n\n        With `slice` objects.\n\n        >>> df.iloc[1:3, 0:3]\n              a     b     c\n        1   100   200   300\n        2  1000  2000  3000\n\n        With a boolean array whose length matches the columns.\n\n        >>> df.iloc[:, [True, False, True, False]]\n              a     c\n        0     1     3\n        1   100   300\n        2  1000  3000\n\n        With a callable function that expects the Series or DataFrame.\n\n        >>> df.iloc[:, lambda df: [0, 2]]\n              a     c\n        0     1     3\n        1   100   300\n        2  1000  3000\n        \"\"\"\n        return _iLocIndexer('iloc', self)\n\n    @property\n    def loc(self) -> _LocIndexer:\n        \"\"\"\n        Access a group of rows and columns by label(s) or a boolean array.\n\n        ``.loc[]`` is primarily label based, but may also be used with a\n        boolean array.\n\n        Allowed inputs are:\n\n        - A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n          interpreted as a *label* of the index, and **never** as an\n          integer position along the index).\n        - A list or array of labels, e.g. ``['a', 'b', 'c']``.\n        - A slice object with labels, e.g. ``'a':'f'``.\n\n          .. warning:: Note that contrary to usual python slices, **both** the\n              start and the stop are included\n\n        - A boolean array of the same length as the axis being sliced,\n          e.g. ``[True, False, True]``.\n        - An alignable boolean Series. The index of the key will be aligned before\n          masking.\n        - An alignable Index. The Index of the returned selection will be the input.\n        - A ``callable`` function with one argument (the calling Series or\n          DataFrame) and that returns valid output for indexing (one of the above)\n\n        See more at :ref:`Selection by Label <indexing.label>`.\n\n        Raises\n        ------\n        KeyError\n            If any items are not found.\n        IndexingError\n            If an indexed key is passed and its index is unalignable to the frame index.\n\n        See Also\n        --------\n        DataFrame.at : Access a single value for a row/column label pair.\n        DataFrame.iloc : Access group of rows and columns by integer position(s).\n        DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n                       Series/DataFrame.\n        Series.loc : Access group of values using labels.\n\n        Examples\n        --------\n        **Getting values**\n\n        >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n        ...                   index=['cobra', 'viper', 'sidewinder'],\n        ...                   columns=['max_speed', 'shield'])\n        >>> df\n                    max_speed  shield\n        cobra               1       2\n        viper               4       5\n        sidewinder          7       8\n\n        Single label. Note this returns the row as a Series.\n\n        >>> df.loc['viper']\n        max_speed    4\n        shield       5\n        Name: viper, dtype: int64\n\n        List of labels. Note using ``[[]]`` returns a DataFrame.\n\n        >>> df.loc[['viper', 'sidewinder']]\n                    max_speed  shield\n        viper               4       5\n        sidewinder          7       8\n\n        Single label for row and column\n\n        >>> df.loc['cobra', 'shield']\n        2\n\n        Slice with labels for row and single label for column. As mentioned\n        above, note that both the start and stop of the slice are included.\n\n        >>> df.loc['cobra':'viper', 'max_speed']\n        cobra    1\n        viper    4\n        Name: max_speed, dtype: int64\n\n        Boolean list with the same length as the row axis\n\n        >>> df.loc[[False, False, True]]\n                    max_speed  shield\n        sidewinder          7       8\n\n        Alignable boolean Series:\n\n        >>> df.loc[pd.Series([False, True, False],\n        ...                  index=['viper', 'sidewinder', 'cobra'])]\n                             max_speed  shield\n        sidewinder          7       8\n\n        Index (same behavior as ``df.reindex``)\n\n        >>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n               max_speed  shield\n        foo\n        cobra          1       2\n        viper          4       5\n\n        Conditional that returns a boolean Series\n\n        >>> df.loc[df['shield'] > 6]\n                    max_speed  shield\n        sidewinder          7       8\n\n        Conditional that returns a boolean Series with column labels specified\n\n        >>> df.loc[df['shield'] > 6, ['max_speed']]\n                    max_speed\n        sidewinder          7\n\n        Multiple conditional using ``&`` that returns a boolean Series\n\n        >>> df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)]\n                    max_speed  shield\n        viper          4       5\n\n        Multiple conditional using ``|`` that returns a boolean Series\n\n        >>> df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)]\n                    max_speed  shield\n        cobra               1       2\n        sidewinder          7       8\n\n        Please ensure that each condition is wrapped in parentheses ``()``.\n        See the :ref:`user guide<indexing.boolean>`\n        for more details and explanations of Boolean indexing.\n\n        .. note::\n            If you find yourself using 3 or more conditionals in ``.loc[]``,\n            consider using :ref:`advanced indexing<advanced.advanced_hierarchical>`.\n\n            See below for using ``.loc[]`` on MultiIndex DataFrames.\n\n        Callable that returns a boolean Series\n\n        >>> df.loc[lambda df: df['shield'] == 8]\n                    max_speed  shield\n        sidewinder          7       8\n\n        **Setting values**\n\n        Set value for all items matching the list of labels\n\n        >>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n        >>> df\n                    max_speed  shield\n        cobra               1       2\n        viper               4      50\n        sidewinder          7      50\n\n        Set value for an entire row\n\n        >>> df.loc['cobra'] = 10\n        >>> df\n                    max_speed  shield\n        cobra              10      10\n        viper               4      50\n        sidewinder          7      50\n\n        Set value for an entire column\n\n        >>> df.loc[:, 'max_speed'] = 30\n        >>> df\n                    max_speed  shield\n        cobra              30      10\n        viper              30      50\n        sidewinder         30      50\n\n        Set value for rows matching callable condition\n\n        >>> df.loc[df['shield'] > 35] = 0\n        >>> df\n                    max_speed  shield\n        cobra              30      10\n        viper               0       0\n        sidewinder          0       0\n\n        Add value matching location\n\n        >>> df.loc[\"viper\", \"shield\"] += 5\n        >>> df\n                    max_speed  shield\n        cobra              30      10\n        viper               0       5\n        sidewinder          0       0\n\n        Setting using a ``Series`` or a ``DataFrame`` sets the values matching the\n        index labels, not the index positions.\n\n        >>> shuffled_df = df.loc[[\"viper\", \"cobra\", \"sidewinder\"]]\n        >>> df.loc[:] += shuffled_df\n        >>> df\n                    max_speed  shield\n        cobra              60      20\n        viper               0      10\n        sidewinder          0       0\n\n        **Getting values on a DataFrame with an index that has integer labels**\n\n        Another example using integers for the index\n\n        >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n        ...                   index=[7, 8, 9], columns=['max_speed', 'shield'])\n        >>> df\n           max_speed  shield\n        7          1       2\n        8          4       5\n        9          7       8\n\n        Slice with integer labels for rows. As mentioned above, note that both\n        the start and stop of the slice are included.\n\n        >>> df.loc[7:9]\n           max_speed  shield\n        7          1       2\n        8          4       5\n        9          7       8\n\n        **Getting values with a MultiIndex**\n\n        A number of examples using a DataFrame with a MultiIndex\n\n        >>> tuples = [\n        ...     ('cobra', 'mark i'), ('cobra', 'mark ii'),\n        ...     ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n        ...     ('viper', 'mark ii'), ('viper', 'mark iii')\n        ... ]\n        >>> index = pd.MultiIndex.from_tuples(tuples)\n        >>> values = [[12, 2], [0, 4], [10, 20],\n        ...           [1, 4], [7, 1], [16, 36]]\n        >>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n        >>> df\n                             max_speed  shield\n        cobra      mark i           12       2\n                   mark ii           0       4\n        sidewinder mark i           10      20\n                   mark ii           1       4\n        viper      mark ii           7       1\n                   mark iii         16      36\n\n        Single label. Note this returns a DataFrame with a single index.\n\n        >>> df.loc['cobra']\n                 max_speed  shield\n        mark i          12       2\n        mark ii          0       4\n\n        Single index tuple. Note this returns a Series.\n\n        >>> df.loc[('cobra', 'mark ii')]\n        max_speed    0\n        shield       4\n        Name: (cobra, mark ii), dtype: int64\n\n        Single label for row and column. Similar to passing in a tuple, this\n        returns a Series.\n\n        >>> df.loc['cobra', 'mark i']\n        max_speed    12\n        shield        2\n        Name: (cobra, mark i), dtype: int64\n\n        Single tuple. Note using ``[[]]`` returns a DataFrame.\n\n        >>> df.loc[[('cobra', 'mark ii')]]\n                       max_speed  shield\n        cobra mark ii          0       4\n\n        Single tuple for the index with a single label for the column\n\n        >>> df.loc[('cobra', 'mark i'), 'shield']\n        2\n\n        Slice from index tuple to single label\n\n        >>> df.loc[('cobra', 'mark i'):'viper']\n                             max_speed  shield\n        cobra      mark i           12       2\n                   mark ii           0       4\n        sidewinder mark i           10      20\n                   mark ii           1       4\n        viper      mark ii           7       1\n                   mark iii         16      36\n\n        Slice from index tuple to index tuple\n\n        >>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                            max_speed  shield\n        cobra      mark i          12       2\n                   mark ii          0       4\n        sidewinder mark i          10      20\n                   mark ii          1       4\n        viper      mark ii          7       1\n\n        Please see the :ref:`user guide<advanced.advanced_hierarchical>`\n        for more details and explanations of advanced indexing.\n        \"\"\"\n        return _LocIndexer('loc', self)\n\n    @property\n    def at(self) -> _AtIndexer:\n        \"\"\"\n        Access a single value for a row/column label pair.\n\n        Similar to ``loc``, in that both provide label-based lookups. Use\n        ``at`` if you only need to get or set a single value in a DataFrame\n        or Series.\n\n        Raises\n        ------\n        KeyError\n            If getting a value and 'label' does not exist in a DataFrame or Series.\n\n        ValueError\n            If row/column label pair is not a tuple or if any label\n            from the pair is not a scalar for DataFrame.\n            If label is list-like (*excluding* NamedTuple) for Series.\n\n        See Also\n        --------\n        DataFrame.at : Access a single value for a row/column pair by label.\n        DataFrame.iat : Access a single value for a row/column pair by integer\n            position.\n        DataFrame.loc : Access a group of rows and columns by label(s).\n        DataFrame.iloc : Access a group of rows and columns by integer\n            position(s).\n        Series.at : Access a single value by label.\n        Series.iat : Access a single value by integer position.\n        Series.loc : Access a group of rows by label(s).\n        Series.iloc : Access a group of rows by integer position(s).\n\n        Notes\n        -----\n        See :ref:`Fast scalar value getting and setting <indexing.basics.get_value>`\n        for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n        ...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n        >>> df\n            A   B   C\n        4   0   2   3\n        5   0   4   1\n        6  10  20  30\n\n        Get value at specified row/column pair\n\n        >>> df.at[4, 'B']\n        2\n\n        Set value at specified row/column pair\n\n        >>> df.at[4, 'B'] = 10\n        >>> df.at[4, 'B']\n        10\n\n        Get value within a Series\n\n        >>> df.loc[5].at['B']\n        4\n        \"\"\"\n        return _AtIndexer('at', self)\n\n    @property\n    def iat(self) -> _iAtIndexer:\n        \"\"\"\n        Access a single value for a row/column pair by integer position.\n\n        Similar to ``iloc``, in that both provide integer-based lookups. Use\n        ``iat`` if you only need to get or set a single value in a DataFrame\n        or Series.\n\n        Raises\n        ------\n        IndexError\n            When integer position is out of bounds.\n\n        See Also\n        --------\n        DataFrame.at : Access a single value for a row/column label pair.\n        DataFrame.loc : Access a group of rows and columns by label(s).\n        DataFrame.iloc : Access a group of rows and columns by integer position(s).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n        ...                   columns=['A', 'B', 'C'])\n        >>> df\n            A   B   C\n        0   0   2   3\n        1   0   4   1\n        2  10  20  30\n\n        Get value at specified row/column pair\n\n        >>> df.iat[1, 2]\n        1\n\n        Set value at specified row/column pair\n\n        >>> df.iat[1, 2] = 10\n        >>> df.iat[1, 2]\n        10\n\n        Get value within a series\n\n        >>> df.loc[0].iat[1]\n        2\n        \"\"\"\n        return _iAtIndexer('iat', self)", "class_fn": true, "question_id": "pandas/pandas.core.indexing/IndexingMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/link.py", "fn_id": "", "content": "class HalfLogitLink(BaseLink):\n    \"\"\"Half the logit link function g(x)=1/2 * logit(x).\n\n    Used for the exponential loss.\n    \"\"\"\n    interval_y_pred = Interval(0, 1, False, False)\n\n    def link(self, y_pred, out=None):\n        out = logit(y_pred, out=out)\n        out *= 0.5\n        return out\n\n    def inverse(self, raw_prediction, out=None):\n        return expit(2 * raw_prediction, out)", "class_fn": true, "question_id": "sklearn/sklearn._loss.link/HalfLogitLink", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/link.py", "fn_id": "", "content": "class LogLink(BaseLink):\n    \"\"\"The log link function g(x)=log(x).\"\"\"\n    interval_y_pred = Interval(0, np.inf, False, False)\n\n    def link(self, y_pred, out=None):\n        return np.log(y_pred, out=out)\n\n    def inverse(self, raw_prediction, out=None):\n        return np.exp(raw_prediction, out=out)", "class_fn": true, "question_id": "sklearn/sklearn._loss.link/LogLink", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/loss.py", "fn_id": "", "content": "class HalfGammaLoss(BaseLoss):\n    \"\"\"Half Gamma deviance loss with log-link, for regression.\n\n    Domain:\n    y_true and y_pred in positive real numbers\n\n    Link:\n    y_pred = exp(raw_prediction)\n\n    For a given sample x_i, half Gamma deviance loss is defined as::\n\n        loss(x_i) = log(exp(raw_prediction_i)/y_true_i)\n                    + y_true/exp(raw_prediction_i) - 1\n\n    Half the Gamma deviance is actually proportional to the negative log-\n    likelihood up to constant terms (not involving raw_prediction) and\n    simplifies the computation of the gradients.\n    We also skip the constant term `-log(y_true_i) - 1`.\n    \"\"\"\n\n    def gradient(self, y_true, raw_prediction, sample_weight=None, gradient_out=None, n_threads=1):\n        \"\"\"Compute gradient of loss w.r.t raw_prediction for each input.\n\n        Parameters\n        ----------\n        y_true : C-contiguous array of shape (n_samples,)\n            Observed, true target values.\n        raw_prediction : C-contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)\n            Raw prediction values (in link space).\n        sample_weight : None or C-contiguous array of shape (n_samples,)\n            Sample weights.\n        gradient_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\n            A location into which the result is stored. If None, a new array\n            might be created.\n        n_threads : int, default=1\n            Might use openmp thread parallelism.\n\n        Returns\n        -------\n        gradient : array of shape (n_samples,) or (n_samples, n_classes)\n            Element-wise gradients.\n        \"\"\"\n        if gradient_out is None:\n            gradient_out = np.empty_like(raw_prediction)\n        if raw_prediction.ndim == 2 and raw_prediction.shape[1] == 1:\n            raw_prediction = raw_prediction.squeeze(1)\n        if gradient_out.ndim == 2 and gradient_out.shape[1] == 1:\n            gradient_out = gradient_out.squeeze(1)\n        self.closs.gradient(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight, gradient_out=gradient_out, n_threads=n_threads)\n        return gradient_out\n\n    def in_y_pred_range(self, y):\n        \"\"\"Return True if y is in the valid range of y_pred.\n\n        Parameters\n        ----------\n        y : ndarray\n        \"\"\"\n        return self.interval_y_pred.includes(y)\n\n    def constant_to_optimal_zero(self, y_true, sample_weight=None):\n        term = -np.log(y_true) - 1\n        if sample_weight is not None:\n            term *= sample_weight\n        return term\n\n    def __init__(self, sample_weight=None):\n        super().__init__(closs=CyHalfGammaLoss(), link=LogLink())\n        self.interval_y_true = Interval(0, np.inf, False, False)\n\n    def fit_intercept_only(self, y_true, sample_weight=None):\n        \"\"\"Compute raw_prediction of an intercept-only model.\n\n        This can be used as initial estimates of predictions, i.e. before the\n        first iteration in fit.\n\n        Parameters\n        ----------\n        y_true : array-like of shape (n_samples,)\n            Observed, true target values.\n        sample_weight : None or array of shape (n_samples,)\n            Sample weights.\n\n        Returns\n        -------\n        raw_prediction : numpy scalar or array of shape (n_classes,)\n            Raw predictions of an intercept-only model.\n        \"\"\"\n        y_pred = np.average(y_true, weights=sample_weight, axis=0)\n        eps = 10 * np.finfo(y_pred.dtype).eps\n        if self.interval_y_pred.low == -np.inf:\n            a_min = None\n        elif self.interval_y_pred.low_inclusive:\n            a_min = self.interval_y_pred.low\n        else:\n            a_min = self.interval_y_pred.low + eps\n        if self.interval_y_pred.high == np.inf:\n            a_max = None\n        elif self.interval_y_pred.high_inclusive:\n            a_max = self.interval_y_pred.high\n        else:\n            a_max = self.interval_y_pred.high - eps\n        if a_min is None and a_max is None:\n            return self.link.link(y_pred)\n        else:\n            return self.link.link(np.clip(y_pred, a_min, a_max))", "class_fn": true, "question_id": "sklearn/sklearn._loss.loss/HalfGammaLoss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/loss.py", "fn_id": "", "content": "class HalfTweedieLossIdentity(BaseLoss):\n    \"\"\"Half Tweedie deviance loss with identity link, for regression.\n\n    Domain:\n    y_true in real numbers for power <= 0\n    y_true in non-negative real numbers for 0 < power < 2\n    y_true in positive real numbers for 2 <= power\n    y_pred in positive real numbers for power != 0\n    y_pred in real numbers for power = 0\n    power in real numbers\n\n    Link:\n    y_pred = raw_prediction\n\n    For a given sample x_i, half Tweedie deviance loss with p=power is defined\n    as::\n\n        loss(x_i) = max(y_true_i, 0)**(2-p) / (1-p) / (2-p)\n                    - y_true_i * raw_prediction_i**(1-p) / (1-p)\n                    + raw_prediction_i**(2-p) / (2-p)\n\n    Note that the minimum value of this loss is 0.\n\n    Note furthermore that although no Tweedie distribution exists for\n    0 < power < 1, it still gives a strictly consistent scoring function for\n    the expectation.\n    \"\"\"\n\n    def in_y_pred_range(self, y):\n        \"\"\"Return True if y is in the valid range of y_pred.\n\n        Parameters\n        ----------\n        y : ndarray\n        \"\"\"\n        return self.interval_y_pred.includes(y)\n\n    def fit_intercept_only(self, y_true, sample_weight=None):\n        \"\"\"Compute raw_prediction of an intercept-only model.\n\n        This can be used as initial estimates of predictions, i.e. before the\n        first iteration in fit.\n\n        Parameters\n        ----------\n        y_true : array-like of shape (n_samples,)\n            Observed, true target values.\n        sample_weight : None or array of shape (n_samples,)\n            Sample weights.\n\n        Returns\n        -------\n        raw_prediction : numpy scalar or array of shape (n_classes,)\n            Raw predictions of an intercept-only model.\n        \"\"\"\n        y_pred = np.average(y_true, weights=sample_weight, axis=0)\n        eps = 10 * np.finfo(y_pred.dtype).eps\n        if self.interval_y_pred.low == -np.inf:\n            a_min = None\n        elif self.interval_y_pred.low_inclusive:\n            a_min = self.interval_y_pred.low\n        else:\n            a_min = self.interval_y_pred.low + eps\n        if self.interval_y_pred.high == np.inf:\n            a_max = None\n        elif self.interval_y_pred.high_inclusive:\n            a_max = self.interval_y_pred.high\n        else:\n            a_max = self.interval_y_pred.high - eps\n        if a_min is None and a_max is None:\n            return self.link.link(y_pred)\n        else:\n            return self.link.link(np.clip(y_pred, a_min, a_max))\n\n    def constant_to_optimal_zero(self, y_true, sample_weight=None):\n        \"\"\"Calculate term dropped in loss.\n\n        With this term added, the loss of perfect predictions is zero.\n        \"\"\"\n        return np.zeros_like(y_true)\n\n    def __init__(self, sample_weight=None, power=1.5):\n        super().__init__(closs=CyHalfTweedieLossIdentity(power=float(power)), link=IdentityLink())\n        if self.closs.power <= 0:\n            self.interval_y_true = Interval(-np.inf, np.inf, False, False)\n        elif self.closs.power < 2:\n            self.interval_y_true = Interval(0, np.inf, True, False)\n        else:\n            self.interval_y_true = Interval(0, np.inf, False, False)\n        if self.closs.power == 0:\n            self.interval_y_pred = Interval(-np.inf, np.inf, False, False)\n        else:\n            self.interval_y_pred = Interval(0, np.inf, False, False)", "class_fn": true, "question_id": "sklearn/sklearn._loss.loss/HalfTweedieLossIdentity", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/base.py", "fn_id": "", "content": "class ClusterMixin:\n    \"\"\"Mixin class for all cluster estimators in scikit-learn.\n\n    - `_estimator_type` class attribute defaulting to `\"clusterer\"`;\n    - `fit_predict` method returning the cluster labels associated to each sample.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, ClusterMixin\n    >>> class MyClusterer(ClusterMixin, BaseEstimator):\n    ...     def fit(self, X, y=None):\n    ...         self.labels_ = np.ones(shape=(len(X),), dtype=np.int64)\n    ...         return self\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> MyClusterer().fit_predict(X)\n    array([1, 1, 1])\n    \"\"\"\n    _estimator_type = 'clusterer'\n\n    def fit_predict(self, X, y=None, **kwargs):\n        \"\"\"\n        Perform clustering on `X` and returns cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **kwargs : dict\n            Arguments to be passed to ``fit``.\n\n            .. versionadded:: 1.4\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,), dtype=np.int64\n            Cluster labels.\n        \"\"\"\n        self.fit(X, **kwargs)\n        return self.labels_\n\n    def _more_tags(self):\n        return {'preserves_dtype': []}", "class_fn": true, "question_id": "sklearn/sklearn.base/ClusterMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/base.py", "fn_id": "", "content": "class MultiOutputMixin:\n    \"\"\"Mixin to mark estimators that support multioutput.\"\"\"\n\n    def _more_tags(self):\n        return {'multioutput': True}", "class_fn": true, "question_id": "sklearn/sklearn.base/MultiOutputMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/calibration.py", "fn_id": "", "content": "class _SigmoidCalibration(RegressorMixin, BaseEstimator):\n    \"\"\"Sigmoid regression model.\n\n    Attributes\n    ----------\n    a_ : float\n        The slope.\n\n    b_ : float\n        The intercept.\n    \"\"\"\n\n    def _more_tags(self):\n        return {'requires_y': True}\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,)\n            Data to predict from.\n\n        Returns\n        -------\n        T_ : ndarray of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        T = column_or_1d(T)\n        return expit(-(self.a_ * T + self.b_))\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        X = column_or_1d(X)\n        y = column_or_1d(y)\n        (X, y) = indexable(X, y)\n        (self.a_, self.b_) = _sigmoid_calibration(X, y, sample_weight)\n        return self\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out", "class_fn": true, "question_id": "sklearn/sklearn.calibration/_SigmoidCalibration", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/datasets/_openml.py", "fn_id": "", "content": "class OpenMLError(ValueError):\n    \"\"\"HTTP 412 is a specific OpenML error code, indicating a generic error\"\"\"\n    pass", "class_fn": true, "question_id": "sklearn/sklearn.datasets._openml/OpenMLError", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class ArffException(Exception):\n    message: Optional[str] = None\n\n    def __init__(self):\n        self.line = -1\n\n    def __str__(self):\n        return self.message % self.line", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/ArffException", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class BadAttributeType(ArffException):\n    \"\"\"Error raised when some invalid type is provided into the attribute\n    declaration.\"\"\"\n    message = 'Bad @ATTRIBUTE type, at line %d.'\n\n    def __init__(self):\n        self.line = -1\n\n    def __str__(self):\n        return self.message % self.line", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/BadAttributeType", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class BadNominalFormatting(ArffException):\n    \"\"\"Error raised when a nominal value with space is not properly quoted.\"\"\"\n\n    def __str__(self):\n        return self.message % self.line\n\n    def __init__(self, value):\n        super().__init__()\n        self.message = 'Nominal data value \"%s\" not properly quoted in line ' % value + '%d.'", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/BadNominalFormatting", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class BadObject(ArffException):\n    \"\"\"Error raised when the object representing the ARFF file has something\n    wrong.\"\"\"\n\n    def __init__(self, msg='Invalid object.'):\n        self.msg = msg\n\n    def __str__(self):\n        return '%s' % self.msg", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/BadObject", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class EncodedNominalConversor:\n\n    def __init__(self, values):\n        self.values = {v: i for (i, v) in enumerate(values)}\n        self.values[0] = 0\n\n    def __call__(self, value):\n        try:\n            return self.values[value]\n        except KeyError:\n            raise BadNominalValue(value)", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/EncodedNominalConversor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class _DataListMixin:\n    \"\"\"Mixin to return a list from decode_rows instead of a generator\"\"\"\n\n    def decode_rows(self, stream, conversors):\n        return list(super().decode_rows(stream, conversors))", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/_DataListMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_packaging/version.py", "fn_id": "", "content": "class LegacyVersion(_BaseVersion):\n\n    def __repr__(self) -> str:\n        return f\"<LegacyVersion('{self}')>\"\n\n    def __init__(self, version: str) -> None:\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)\n        warnings.warn('Creating a LegacyVersion has been deprecated and will be removed in the next major release', DeprecationWarning)\n\n    def __str__(self) -> str:\n        return self._version\n\n    @property\n    def public(self) -> str:\n        return self._version\n\n    @property\n    def base_version(self) -> str:\n        return self._version\n\n    @property\n    def epoch(self) -> int:\n        return -1\n\n    @property\n    def release(self) -> None:\n        return None\n\n    @property\n    def pre(self) -> None:\n        return None\n\n    @property\n    def post(self) -> None:\n        return None\n\n    @property\n    def dev(self) -> None:\n        return None\n\n    @property\n    def local(self) -> None:\n        return None\n\n    @property\n    def is_prerelease(self) -> bool:\n        return False\n\n    @property\n    def is_postrelease(self) -> bool:\n        return False\n\n    @property\n    def is_devrelease(self) -> bool:\n        return False\n\n    def __ge__(self, other: '_BaseVersion') -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n        return self._key >= other._key\n\n    def __ne__(self, other: object) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n        return self._key != other._key\n\n    def __le__(self, other: '_BaseVersion') -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n        return self._key <= other._key", "class_fn": true, "question_id": "sklearn/sklearn.externals._packaging.version/LegacyVersion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/gaussian_process/tests/_mini_sequence_kernel.py", "fn_id": "", "content": "class MiniSeqKernel(GenericKernelMixin, StationaryKernelMixin, Kernel):\n    \"\"\"\n    A minimal (but valid) convolutional kernel for sequences of variable\n    length.\n    \"\"\"\n\n    def get_params(self, deep=True):\n        \"\"\"Get parameters of this kernel.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        params = dict()\n        cls = self.__class__\n        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n        init_sign = signature(init)\n        (args, varargs) = ([], [])\n        for parameter in init_sign.parameters.values():\n            if parameter.kind != parameter.VAR_KEYWORD and parameter.name != 'self':\n                args.append(parameter.name)\n            if parameter.kind == parameter.VAR_POSITIONAL:\n                varargs.append(parameter.name)\n        if len(varargs) != 0:\n            raise RuntimeError(\"scikit-learn kernels should always specify their parameters in the signature of their __init__ (no varargs). %s doesn't follow this convention.\" % (cls,))\n        for arg in args:\n            params[arg] = getattr(self, arg)\n        return params\n\n    @property\n    def hyperparameter_baseline_similarity(self):\n        return Hyperparameter('baseline_similarity', 'numeric', self.baseline_similarity_bounds)\n\n    def __init__(self, baseline_similarity=0.5, baseline_similarity_bounds=(1e-05, 1)):\n        self.baseline_similarity = baseline_similarity\n        self.baseline_similarity_bounds = baseline_similarity_bounds\n\n    def _g(self, s1, s2):\n        return sum([0.0 if c1 == c2 else 1.0 for c1 in s1 for c2 in s2])\n\n    def _f(self, s1, s2):\n        return sum([1.0 if c1 == c2 else self.baseline_similarity for c1 in s1 for c2 in s2])\n\n    def is_stationary(self):\n        \"\"\"Returns whether the kernel is stationary.\"\"\"\n        return True\n\n    def diag(self, X):\n        return np.array([self._f(x, x) for x in X])\n\n    def clone_with_theta(self, theta):\n        cloned = clone(self)\n        cloned.theta = theta\n        return cloned\n\n    def __call__(self, X, Y=None, eval_gradient=False):\n        if Y is None:\n            Y = X\n        if eval_gradient:\n            return (np.array([[self._f(x, y) for y in Y] for x in X]), np.array([[[self._g(x, y)] for y in Y] for x in X]))\n        else:\n            return np.array([[self._f(x, y) for y in Y] for x in X])\n\n    def __repr__(self):\n        return '{0}({1})'.format(self.__class__.__name__, ', '.join(map('{0:.3g}'.format, self.theta)))", "class_fn": true, "question_id": "sklearn/sklearn.gaussian_process.tests._mini_sequence_kernel/MiniSeqKernel", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/linear_model/_ridge.py", "fn_id": "", "content": "class _IdentityClassifier(LinearClassifierMixin):\n    \"\"\"Fake classifier which will directly output the prediction.\n\n    We inherit from LinearClassifierMixin to get the proper shape for the\n    output `y`.\n    \"\"\"\n\n    def decision_function(self, y_predict):\n        return y_predict\n\n    def __init__(self, classes):\n        self.classes_ = classes\n\n    def _predict_proba_lr(self, X):\n        \"\"\"Probability estimation for OvR logistic regression.\n\n        Positive class probabilities are computed as\n        1. / (1. + np.exp(-self.decision_function(X)));\n        multiclass is handled by normalizing that over all classes.\n        \"\"\"\n        prob = self.decision_function(X)\n        expit(prob, out=prob)\n        if prob.ndim == 1:\n            return np.vstack([1 - prob, prob]).T\n        else:\n            prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n            return prob\n\n    def predict(self, X):\n        \"\"\"\n        Predict class labels for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the predictions.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Vector containing the class labels for each sample.\n        \"\"\"\n        (xp, _) = get_namespace(X)\n        scores = self.decision_function(X)\n        if len(scores.shape) == 1:\n            indices = xp.astype(scores > 0, indexing_dtype(xp))\n        else:\n            indices = xp.argmax(scores, axis=1)\n        return xp.take(self.classes_, indices, axis=0)", "class_fn": true, "question_id": "sklearn/sklearn.linear_model._ridge/_IdentityClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/linear_model/_ridge.py", "fn_id": "", "content": "class _X_CenterStackOp(sparse.linalg.LinearOperator):\n    \"\"\"Behaves as centered and scaled X with an added intercept column.\n\n    This operator behaves as\n    np.hstack([X - sqrt_sw[:, None] * X_mean, sqrt_sw[:, None]])\n    \"\"\"\n\n    def __init__(self, X, X_mean, sqrt_sw):\n        (n_samples, n_features) = X.shape\n        super().__init__(X.dtype, (n_samples, n_features + 1))\n        self.X = X\n        self.X_mean = X_mean\n        self.sqrt_sw = sqrt_sw\n\n    def _matvec(self, v):\n        v = v.ravel()\n        return safe_sparse_dot(self.X, v[:-1], dense_output=True) - self.sqrt_sw * self.X_mean.dot(v[:-1]) + v[-1] * self.sqrt_sw\n\n    def _matmat(self, v):\n        return safe_sparse_dot(self.X, v[:-1], dense_output=True) - self.sqrt_sw[:, None] * self.X_mean.dot(v[:-1]) + v[-1] * self.sqrt_sw[:, None]\n\n    def _transpose(self):\n        return _XT_CenterStackOp(self.X, self.X_mean, self.sqrt_sw)", "class_fn": true, "question_id": "sklearn/sklearn.linear_model._ridge/_X_CenterStackOp", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/model_selection/_search_successive_halving.py", "fn_id": "", "content": "class _SubsampleMetaSplitter:\n    \"\"\"Splitter that subsamples a given fraction of the dataset\"\"\"\n\n    def __init__(self, *, base_cv, fraction, subsample_test, random_state):\n        self.base_cv = base_cv\n        self.fraction = fraction\n        self.subsample_test = subsample_test\n        self.random_state = random_state\n\n    def split(self, X, y, **kwargs):\n        for (train_idx, test_idx) in self.base_cv.split(X, y, **kwargs):\n            train_idx = resample(train_idx, replace=False, random_state=self.random_state, n_samples=int(self.fraction * len(train_idx)))\n            if self.subsample_test:\n                test_idx = resample(test_idx, replace=False, random_state=self.random_state, n_samples=int(self.fraction * len(test_idx)))\n            yield (train_idx, test_idx)", "class_fn": true, "question_id": "sklearn/sklearn.model_selection._search_successive_halving/_SubsampleMetaSplitter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/model_selection/_split.py", "fn_id": "", "content": "class _CVIterableWrapper(BaseCrossValidator):\n    \"\"\"Wrapper class for old style cv objects and iterables.\"\"\"\n\n    def split(self, X=None, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        for (train, test) in self.cv:\n            yield (train, test)\n\n    def _iter_test_masks(self, X=None, y=None, groups=None):\n        \"\"\"Generates boolean masks corresponding to test sets.\n\n        By default, delegates to _iter_test_indices(X, y, groups)\n        \"\"\"\n        for test_index in self._iter_test_indices(X, y, groups):\n            test_mask = np.zeros(_num_samples(X), dtype=bool)\n            test_mask[test_index] = True\n            yield test_mask\n\n    def __init__(self, cv):\n        self.cv = list(cv)\n\n    def __repr__(self):\n        return _build_repr(self)\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator.\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return len(self.cv)\n\n    def _iter_test_indices(self, X=None, y=None, groups=None):\n        \"\"\"Generates integer indices corresponding to test sets.\"\"\"\n        raise NotImplementedError", "class_fn": true, "question_id": "sklearn/sklearn.model_selection._split/_CVIterableWrapper", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/multiclass.py", "fn_id": "", "content": "class _ConstantPredictor(BaseEstimator):\n    \"\"\"Helper predictor to be used when only one class is present.\"\"\"\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def decision_function(self, X):\n        check_is_fitted(self)\n        self._validate_data(X, force_all_finite=False, dtype=None, accept_sparse=True, ensure_2d=False, reset=False)\n        return np.repeat(self.y_, _num_samples(X))\n\n    def fit(self, X, y):\n        check_params = dict(force_all_finite=False, dtype=None, ensure_2d=False, accept_sparse=True)\n        self._validate_data(X, y, reset=True, validate_separately=(check_params, check_params))\n        self.y_ = y\n        return self\n\n    def predict(self, X):\n        check_is_fitted(self)\n        self._validate_data(X, force_all_finite=False, dtype=None, accept_sparse=True, ensure_2d=False, reset=False)\n        return np.repeat(self.y_, _num_samples(X))\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def predict_proba(self, X):\n        check_is_fitted(self)\n        self._validate_data(X, force_all_finite=False, dtype=None, accept_sparse=True, ensure_2d=False, reset=False)\n        y_ = self.y_.astype(np.float64)\n        return np.repeat([np.hstack([1 - y_, y_])], _num_samples(X), axis=0)\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)", "class_fn": true, "question_id": "sklearn/sklearn.multiclass/_ConstantPredictor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/tests/metadata_routing_common.py", "fn_id": "", "content": "class ConsumingRegressor(RegressorMixin, BaseEstimator):\n    \"\"\"A regressor consuming metadata.\n\n    Parameters\n    ----------\n    registry : list, default=None\n        If a list, the estimator will append itself to the list in order to have\n        a reference to the estimator later on. Since that reference is not\n        required in all tests, registration can be skipped by leaving this value\n        as None.\n    \"\"\"\n\n    def predict(self, X, y=None, sample_weight='default', metadata='default'):\n        record_metadata_not_default(self, 'predict', sample_weight=sample_weight, metadata=metadata)\n        return np.zeros(shape=(len(X),))\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def __init__(self, registry=None):\n        self.registry = registry\n\n    def partial_fit(self, X, y, sample_weight='default', metadata='default'):\n        if self.registry is not None:\n            self.registry.append(self)\n        record_metadata_not_default(self, 'partial_fit', sample_weight=sample_weight, metadata=metadata)\n        return self\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def score(self, X, y, sample_weight='default', metadata='default'):\n        record_metadata_not_default(self, 'score', sample_weight=sample_weight, metadata=metadata)\n        return 1\n\n    def fit(self, X, y, sample_weight='default', metadata='default'):\n        if self.registry is not None:\n            self.registry.append(self)\n        record_metadata_not_default(self, 'fit', sample_weight=sample_weight, metadata=metadata)\n        return self\n\n    def _more_tags(self):\n        return {'requires_y': True}", "class_fn": true, "question_id": "sklearn/sklearn.tests.metadata_routing_common/ConsumingRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/tests/metadata_routing_common.py", "fn_id": "", "content": "class ConsumingTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"A transformer which accepts metadata on fit and transform.\n\n    Parameters\n    ----------\n    registry : list, default=None\n        If a list, the estimator will append itself to the list in order to have\n        a reference to the estimator later on. Since that reference is not\n        required in all tests, registration can be skipped by leaving this value\n        as None.\n    \"\"\"\n\n    def inverse_transform(self, X, sample_weight=None, metadata=None):\n        record_metadata(self, 'inverse_transform', sample_weight=sample_weight, metadata=metadata)\n        return X\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def transform(self, X, sample_weight=None, metadata=None):\n        record_metadata(self, 'transform', sample_weight=sample_weight, metadata=metadata)\n        return X\n\n    def fit_transform(self, X, y, sample_weight=None, metadata=None):\n        record_metadata(self, 'fit_transform', sample_weight=sample_weight, metadata=metadata)\n        return self.fit(X, y, sample_weight=sample_weight, metadata=metadata).transform(X, sample_weight=sample_weight, metadata=metadata)\n\n    def __init__(self, registry=None):\n        self.registry = registry\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def fit(self, X, y=None, sample_weight=None, metadata=None):\n        if self.registry is not None:\n            self.registry.append(self)\n        record_metadata_not_default(self, 'fit', sample_weight=sample_weight, metadata=metadata)\n        return self\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_", "class_fn": true, "question_id": "sklearn/sklearn.tests.metadata_routing_common/ConsumingTransformer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/tests/metadata_routing_common.py", "fn_id": "", "content": "class NonConsumingClassifier(ClassifierMixin, BaseEstimator):\n    \"\"\"A classifier which accepts no metadata on any method.\"\"\"\n\n    def predict(self, X):\n        y_pred = np.empty(shape=(len(X),))\n        y_pred[:len(X) // 2] = 0\n        y_pred[len(X) // 2:] = 1\n        return y_pred\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        return self\n\n    def partial_fit(self, X, y, classes=None):\n        return self\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def __init__(self, alpha=0.0):\n        self.alpha = alpha\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def decision_function(self, X):\n        return self.predict(X)", "class_fn": true, "question_id": "sklearn/sklearn.tests.metadata_routing_common/NonConsumingClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/tests/metadata_routing_common.py", "fn_id": "", "content": "class WeightedMetaRegressor(MetaEstimatorMixin, RegressorMixin, BaseEstimator):\n    \"\"\"A meta-regressor which is also a consumer.\"\"\"\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def get_metadata_routing(self):\n        router = MetadataRouter(owner=self.__class__.__name__).add_self_request(self).add(estimator=self.estimator, method_mapping=MethodMapping().add(caller='fit', callee='fit').add(caller='predict', callee='predict'))\n        return router\n\n    def predict(self, X, **predict_params):\n        params = process_routing(self, 'predict', **predict_params)\n        return self.estimator_.predict(X, **params.estimator.predict)\n\n    def _more_tags(self):\n        return {'requires_y': True}\n\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        if self.registry is not None:\n            self.registry.append(self)\n        record_metadata(self, 'fit', sample_weight=sample_weight)\n        params = process_routing(self, 'fit', sample_weight=sample_weight, **fit_params)\n        self.estimator_ = clone(self.estimator).fit(X, y, **params.estimator.fit)\n        return self\n\n    def __init__(self, estimator, registry=None):\n        self.estimator = estimator\n        self.registry = registry\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)", "class_fn": true, "question_id": "sklearn/sklearn.tests.metadata_routing_common/WeightedMetaRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/tree/_reingold_tilford.py", "fn_id": "", "content": "class DrawTree:\n\n    def __init__(self, tree, parent=None, depth=0, number=1):\n        self.x = -1.0\n        self.y = depth\n        self.tree = tree\n        self.children = [DrawTree(c, self, depth + 1, i + 1) for (i, c) in enumerate(tree.children)]\n        self.parent = parent\n        self.thread = None\n        self.mod = 0\n        self.ancestor = self\n        self.change = self.shift = 0\n        self._lmost_sibling = None\n        self.number = number\n\n    def left(self):\n        return self.thread or (len(self.children) and self.children[0])\n\n    def right(self):\n        return self.thread or (len(self.children) and self.children[-1])\n\n    def lbrother(self):\n        n = None\n        if self.parent:\n            for node in self.parent.children:\n                if node == self:\n                    return n\n                else:\n                    n = node\n        return n\n\n    def get_lmost_sibling(self):\n        if not self._lmost_sibling and self.parent and (self != self.parent.children[0]):\n            self._lmost_sibling = self.parent.children[0]\n        return self._lmost_sibling\n    lmost_sibling = property(get_lmost_sibling)\n\n    def __str__(self):\n        return '%s: x=%s mod=%s' % (self.tree, self.x, self.mod)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def max_extents(self):\n        extents = [c.max_extents() for c in self.children]\n        extents.append((self.x, self.y))\n        return np.max(extents, axis=0)", "class_fn": true, "question_id": "sklearn/sklearn.tree._reingold_tilford/DrawTree", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_available_if.py", "fn_id": "", "content": "class _AvailableIfDescriptor:\n    \"\"\"Implements a conditional property using the descriptor protocol.\n\n    Using this class to create a decorator will raise an ``AttributeError``\n    if check(self) returns a falsey value. Note that if check raises an error\n    this will also result in hasattr returning false.\n\n    See https://docs.python.org/3/howto/descriptor.html for an explanation of\n    descriptors.\n    \"\"\"\n\n    def __init__(self, fn, check, attribute_name):\n        self.fn = fn\n        self.check = check\n        self.attribute_name = attribute_name\n        update_wrapper(self, fn)\n\n    def _check(self, obj, owner):\n        attr_err_msg = f'This {repr(owner.__name__)} has no attribute {repr(self.attribute_name)}'\n        try:\n            check_result = self.check(obj)\n        except Exception as e:\n            raise AttributeError(attr_err_msg) from e\n        if not check_result:\n            raise AttributeError(attr_err_msg)\n\n    def __get__(self, obj, owner=None):\n        if obj is not None:\n            self._check(obj, owner=owner)\n            out = MethodType(self.fn, obj)\n        else:\n\n            @wraps(self.fn)\n            def out(*args, **kwargs):\n                self._check(args[0], owner=owner)\n                return self.fn(*args, **kwargs)\n        return out", "class_fn": true, "question_id": "sklearn/sklearn.utils._available_if/_AvailableIfDescriptor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_encode.py", "fn_id": "", "content": "class _nandict(dict):\n    \"\"\"Dictionary with support for nans.\"\"\"\n\n    def __init__(self, mapping):\n        super().__init__(mapping)\n        for (key, value) in mapping.items():\n            if is_scalar_nan(key):\n                self.nan_value = value\n                break\n\n    def __missing__(self, key):\n        if hasattr(self, 'nan_value') and is_scalar_nan(key):\n            return self.nan_value\n        raise KeyError(key)", "class_fn": true, "question_id": "sklearn/sklearn.utils._encode/_nandict", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_metadata_requests.py", "fn_id": "", "content": "class MethodMapping:\n    \"\"\"Stores the mapping between caller and callee methods for a router.\n\n    This class is primarily used in a ``get_metadata_routing()`` of a router\n    object when defining the mapping between the router's methods and a sub-object (a\n    sub-estimator or a scorer).\n\n    Iterating through an instance of this class yields\n    ``MethodPair(caller, callee)`` instances.\n\n    .. versionadded:: 1.3\n    \"\"\"\n\n    def __init__(self):\n        self._routes = []\n\n    def __iter__(self):\n        return iter(self._routes)\n\n    def add(self, *, caller, callee):\n        \"\"\"Add a method mapping.\n\n        Parameters\n        ----------\n\n        caller : str\n            Parent estimator's method name in which the ``callee`` is called.\n\n        callee : str\n            Child object's method name. This method is called in ``caller``.\n\n        Returns\n        -------\n        self : MethodMapping\n            Returns self.\n        \"\"\"\n        if caller not in METHODS:\n            raise ValueError(f'Given caller:{caller} is not a valid method. Valid methods are: {METHODS}')\n        if callee not in METHODS:\n            raise ValueError(f'Given callee:{callee} is not a valid method. Valid methods are: {METHODS}')\n        self._routes.append(MethodPair(caller=caller, callee=callee))\n        return self\n\n    def _serialize(self):\n        \"\"\"Serialize the object.\n\n        Returns\n        -------\n        obj : list\n            A serialized version of the instance in the form of a list.\n        \"\"\"\n        result = list()\n        for route in self._routes:\n            result.append({'caller': route.caller, 'callee': route.callee})\n        return result\n\n    def __repr__(self):\n        return str(self._serialize())\n\n    def __str__(self):\n        return str(repr(self))", "class_fn": true, "question_id": "sklearn/sklearn.utils._metadata_requests/MethodMapping", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_mocking.py", "fn_id": "", "content": "class MockDataFrame:\n    \"\"\"\n    Parameters\n    ----------\n    array\n    \"\"\"\n\n    def __init__(self, array):\n        self.array = array\n        self.values = array\n        self.shape = array.shape\n        self.ndim = array.ndim\n        self.iloc = ArraySlicingWrapper(array)\n\n    def __len__(self):\n        return len(self.array)\n\n    def __array__(self, dtype=None):\n        return self.array\n\n    def __eq__(self, other):\n        return MockDataFrame(self.array == other.array)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def take(self, indices, axis=0):\n        return MockDataFrame(self.array.take(indices, axis=axis))", "class_fn": true, "question_id": "sklearn/sklearn.utils._mocking/MockDataFrame", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class HasMethods(_Constraint):\n    \"\"\"Constraint representing objects that expose specific methods.\n\n    It is useful for parameters following a protocol and where we don't want to impose\n    an affiliation to a specific module or class.\n\n    Parameters\n    ----------\n    methods : str or list of str\n        The method(s) that the object is expected to expose.\n    \"\"\"\n\n    @validate_params({'methods': [str, list]}, prefer_skip_nested_validation=True)\n    def __init__(self, methods):\n        super().__init__()\n        if isinstance(methods, str):\n            methods = [methods]\n        self.methods = methods\n\n    def is_satisfied_by(self, val):\n        return all((callable(getattr(val, method, None)) for method in self.methods))\n\n    def __str__(self):\n        if len(self.methods) == 1:\n            methods = f'{self.methods[0]!r}'\n        else:\n            methods = f\"{', '.join([repr(m) for m in self.methods[:-1]])} and {self.methods[-1]!r}\"\n        return f'an object implementing {methods}'", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/HasMethods", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class Options(_Constraint):\n    \"\"\"Constraint representing a finite set of instances of a given type.\n\n    Parameters\n    ----------\n    type : type\n\n    options : set\n        The set of valid scalars.\n\n    deprecated : set or None, default=None\n        A subset of the `options` to mark as deprecated in the string\n        representation of the constraint.\n    \"\"\"\n\n    def __init__(self, type, options, *, deprecated=None):\n        super().__init__()\n        self.type = type\n        self.options = options\n        self.deprecated = deprecated or set()\n        if self.deprecated - self.options:\n            raise ValueError('The deprecated options must be a subset of the options.')\n\n    def is_satisfied_by(self, val):\n        return isinstance(val, self.type) and val in self.options\n\n    def _mark_if_deprecated(self, option):\n        \"\"\"Add a deprecated mark to an option if needed.\"\"\"\n        option_str = f'{option!r}'\n        if option in self.deprecated:\n            option_str = f'{option_str} (deprecated)'\n        return option_str\n\n    def __str__(self):\n        options_str = f\"{', '.join([self._mark_if_deprecated(o) for o in self.options])}\"\n        return f'a {_type_name(self.type)} among {{{options_str}}}'", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/Options", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class _Booleans(_Constraint):\n    \"\"\"Constraint representing boolean likes.\n\n    Convenience class for\n    [bool, np.bool_]\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._constraints = [_InstancesOf(bool), _InstancesOf(np.bool_)]\n\n    def is_satisfied_by(self, val):\n        return any((c.is_satisfied_by(val) for c in self._constraints))\n\n    def __str__(self):\n        return f\"{', '.join([str(c) for c in self._constraints[:-1]])} or {self._constraints[-1]}\"", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/_Booleans", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class _Constraint(ABC):\n    \"\"\"Base class for the constraint objects.\"\"\"\n\n    def __init__(self):\n        self.hidden = False\n\n    @abstractmethod\n    def is_satisfied_by(self, val):\n        \"\"\"Whether or not a value satisfies the constraint.\n\n        Parameters\n        ----------\n        val : object\n            The value to check.\n\n        Returns\n        -------\n        is_satisfied : bool\n            Whether or not the constraint is satisfied by this value.\n        \"\"\"\n\n    @abstractmethod\n    def __str__(self):\n        \"\"\"A human readable representational string of the constraint.\"\"\"", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/_Constraint", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class _NanConstraint(_Constraint):\n    \"\"\"Constraint representing the indicator `np.nan`.\"\"\"\n\n    def is_satisfied_by(self, val):\n        return not isinstance(val, Integral) and isinstance(val, Real) and math.isnan(val)\n\n    def __init__(self):\n        self.hidden = False\n\n    def __str__(self):\n        return 'numpy.nan'", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/_NanConstraint", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_param_validation.py", "fn_id": "", "content": "class _RandomStates(_Constraint):\n    \"\"\"Constraint representing random states.\n\n    Convenience class for\n    [Interval(Integral, 0, 2**32 - 1, closed=\"both\"), np.random.RandomState, None]\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._constraints = [Interval(Integral, 0, 2 ** 32 - 1, closed='both'), _InstancesOf(np.random.RandomState), _NoneConstraint()]\n\n    def is_satisfied_by(self, val):\n        return any((c.is_satisfied_by(val) for c in self._constraints))\n\n    def __str__(self):\n        return f\"{', '.join([str(c) for c in self._constraints[:-1]])} or {self._constraints[-1]}\"", "class_fn": true, "question_id": "sklearn/sklearn.utils._param_validation/_RandomStates", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_plotting.py", "fn_id": "", "content": "class _BinaryClassifierCurveDisplayMixin:\n    \"\"\"Mixin class to be used in Displays requiring a binary classifier.\n\n    The aim of this class is to centralize some validations regarding the estimator and\n    the target and gather the response of the estimator.\n    \"\"\"\n\n    def _validate_plot_params(self, *, ax=None, name=None):\n        check_matplotlib_support(f'{self.__class__.__name__}.plot')\n        import matplotlib.pyplot as plt\n        if ax is None:\n            (_, ax) = plt.subplots()\n        name = self.estimator_name if name is None else name\n        return (ax, ax.figure, name)\n\n    @classmethod\n    def _validate_and_get_response_values(cls, estimator, X, y, *, response_method='auto', pos_label=None, name=None):\n        check_matplotlib_support(f'{cls.__name__}.from_estimator')\n        name = estimator.__class__.__name__ if name is None else name\n        (y_pred, pos_label) = _get_response_values_binary(estimator, X, response_method=response_method, pos_label=pos_label)\n        return (y_pred, pos_label, name)\n\n    @classmethod\n    def _validate_from_predictions_params(cls, y_true, y_pred, *, sample_weight=None, pos_label=None, name=None):\n        check_matplotlib_support(f'{cls.__name__}.from_predictions')\n        if type_of_target(y_true) != 'binary':\n            raise ValueError(f'The target y is not binary. Got {type_of_target(y_true)} type of target.')\n        check_consistent_length(y_true, y_pred, sample_weight)\n        pos_label = _check_pos_label_consistency(pos_label, y_true)\n        name = name if name is not None else 'Classifier'\n        return (pos_label, name)", "class_fn": true, "question_id": "sklearn/sklearn.utils._plotting/_BinaryClassifierCurveDisplayMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_set_output.py", "fn_id": "", "content": "class ContainerAdaptersManager:\n\n    def __init__(self):\n        self.adapters = {}\n\n    @property\n    def supported_outputs(self):\n        return {'default'} | set(self.adapters)\n\n    def register(self, adapter):\n        self.adapters[adapter.container_lib] = adapter", "class_fn": true, "question_id": "sklearn/sklearn.utils._set_output/ContainerAdaptersManager", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_testing.py", "fn_id": "", "content": "class MinimalClassifier:\n    \"\"\"Minimal classifier implementation without inheriting from BaseEstimator.\n\n    This estimator should be tested with:\n\n    * `check_estimator` in `test_estimator_checks.py`;\n    * within a `Pipeline` in `test_pipeline.py`;\n    * within a `SearchCV` in `test_search.py`.\n    \"\"\"\n    _estimator_type = 'classifier'\n\n    def __init__(self, param=None):\n        self.param = param\n\n    def get_params(self, deep=True):\n        return {'param': self.param}\n\n    def set_params(self, **params):\n        for (key, value) in params.items():\n            setattr(self, key, value)\n        return self\n\n    def fit(self, X, y):\n        (X, y) = check_X_y(X, y)\n        check_classification_targets(y)\n        (self.classes_, counts) = np.unique(y, return_counts=True)\n        self._most_frequent_class_idx = counts.argmax()\n        return self\n\n    def predict_proba(self, X):\n        check_is_fitted(self)\n        X = check_array(X)\n        proba_shape = (X.shape[0], self.classes_.size)\n        y_proba = np.zeros(shape=proba_shape, dtype=np.float64)\n        y_proba[:, self._most_frequent_class_idx] = 1.0\n        return y_proba\n\n    def predict(self, X):\n        y_proba = self.predict_proba(X)\n        y_pred = y_proba.argmax(axis=1)\n        return self.classes_[y_pred]\n\n    def score(self, X, y):\n        from sklearn.metrics import accuracy_score\n        return accuracy_score(y, self.predict(X))", "class_fn": true, "question_id": "sklearn/sklearn.utils._testing/MinimalClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/_testing.py", "fn_id": "", "content": "class TempMemmap:\n    \"\"\"\n    Parameters\n    ----------\n    data\n    mmap_mode : str, default='r'\n    \"\"\"\n\n    def __init__(self, data, mmap_mode='r'):\n        self.mmap_mode = mmap_mode\n        self.data = data\n\n    def __enter__(self):\n        (data_read_only, self.temp_folder) = create_memmap_backed_data(self.data, mmap_mode=self.mmap_mode, return_folder=True)\n        return data_read_only\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        _delete_folder(self.temp_folder)", "class_fn": true, "question_id": "sklearn/sklearn.utils._testing/TempMemmap", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/estimator_checks.py", "fn_id": "", "content": "class _NotAnArray:\n    \"\"\"An object that is convertible to an array.\n\n    Parameters\n    ----------\n    data : array-like\n        The data.\n    \"\"\"\n\n    def __init__(self, data):\n        self.data = np.asarray(data)\n\n    def __array__(self, dtype=None, copy=None):\n        return self.data\n\n    def __array_function__(self, func, types, args, kwargs):\n        if func.__name__ == 'may_share_memory':\n            return True\n        raise TypeError(\"Don't want to call array_function {}!\".format(func.__name__))", "class_fn": true, "question_id": "sklearn/sklearn.utils.estimator_checks/_NotAnArray", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class BadBalancedWeightsClassifier(BaseBadClassifier):\n\n    def predict(self, X):\n        return np.ones(X.shape[0])\n\n    def fit(self, X, y):\n        from sklearn.preprocessing import LabelEncoder\n        from sklearn.utils import compute_class_weight\n        label_encoder = LabelEncoder().fit(y)\n        classes = label_encoder.classes_\n        class_weight = compute_class_weight(self.class_weight, classes=classes, y=y)\n        if self.class_weight == 'balanced':\n            class_weight += 1.0\n        self.coef_ = class_weight\n        return self\n\n    def __init__(self, class_weight=None):\n        self.class_weight = class_weight", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/BadBalancedWeightsClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class BrokenArrayAPI(BaseEstimator):\n    \"\"\"Make different predictions when using Numpy and the Array API\"\"\"\n\n    def predict(self, X):\n        enabled = get_config()['array_api_dispatch']\n        (xp, _) = _array_api.get_namespace(X)\n        if enabled:\n            return xp.asarray([1, 2, 3])\n        else:\n            return np.array([3, 2, 1])\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def fit(self, X, y):\n        return self\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/BrokenArrayAPI", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class ChangesWrongAttribute(BaseEstimator):\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 1\n        (X, y) = self._validate_data(X, y)\n        return self\n\n    def __init__(self, wrong_attribute=0):\n        self.wrong_attribute = wrong_attribute\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/ChangesWrongAttribute", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class EstimatorMissingDefaultTags(BaseEstimator):\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def _get_tags(self):\n        tags = super()._get_tags().copy()\n        del tags['allow_nan']\n        return tags\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/EstimatorMissingDefaultTags", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class LargeSparseNotSupportedClassifier(BaseEstimator):\n    \"\"\"Estimator that claims to support large sparse data\n    (accept_large_sparse=True), but doesn't\"\"\"\n\n    def __init__(self, raise_for_type=None):\n        self.raise_for_type = raise_for_type\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def fit(self, X, y):\n        (X, y) = self._validate_data(X, y, accept_sparse=('csr', 'csc', 'coo'), accept_large_sparse=True, multi_output=True, y_numeric=True)\n        if self.raise_for_type == 'sparse_array':\n            correct_type = isinstance(X, sp.sparray)\n        elif self.raise_for_type == 'sparse_matrix':\n            correct_type = isinstance(X, sp.spmatrix)\n        if correct_type:\n            if X.format == 'coo':\n                if X.row.dtype == 'int64' or X.col.dtype == 'int64':\n                    raise ValueError(\"Estimator doesn't support 64-bit indices\")\n            elif X.format in ['csc', 'csr']:\n                assert 'int64' not in (X.indices.dtype, X.indptr.dtype), \"Estimator doesn't support 64-bit indices\"\n        return self\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/LargeSparseNotSupportedClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class NoCheckinPredict(BaseBadClassifier):\n\n    def predict(self, X):\n        return np.ones(X.shape[0])\n\n    def fit(self, X, y):\n        (X, y) = self._validate_data(X, y)\n        return self", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/NoCheckinPredict", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class NotInvariantPredict(BaseEstimator):\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def fit(self, X, y):\n        (X, y) = self._validate_data(X, y, accept_sparse=('csr', 'csc'), multi_output=True, y_numeric=True)\n        return self\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def predict(self, X):\n        X = check_array(X)\n        if X.shape[0] > 1:\n            return np.ones(X.shape[0])\n        return np.zeros(X.shape[0])", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/NotInvariantPredict", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class PartialFitChecksName(BaseEstimator):\n\n    def fit(self, X, y):\n        self._validate_data(X, y)\n        return self\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def partial_fit(self, X, y):\n        reset = not hasattr(self, '_fitted')\n        self._validate_data(X, y, reset=reset)\n        self._fitted = True\n        return self\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/PartialFitChecksName", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class RequiresPositiveXRegressor(LinearRegression):\n\n    def _more_tags(self):\n        return {'requires_positive_X': True}\n\n    def fit(self, X, y):\n        (X, y) = self._validate_data(X, y, multi_output=True)\n        if (X < 0).any():\n            raise ValueError('negative X values not supported!')\n        return super().fit(X, y)\n\n    def __init__(self, *, fit_intercept=True, copy_X=True, n_jobs=None, positive=False):\n        self.fit_intercept = fit_intercept\n        self.copy_X = copy_X\n        self.n_jobs = n_jobs\n        self.positive = positive", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/RequiresPositiveXRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class SparseTransformer(BaseEstimator):\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def fit(self, X, y=None):\n        self.X_shape_ = self._validate_data(X).shape\n        return self\n\n    def transform(self, X):\n        X = check_array(X)\n        if X.shape[1] != self.X_shape_[1]:\n            raise ValueError('Bad number of features')\n        return self.sparse_container(X)\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X)\n\n    def __init__(self, sparse_container=None):\n        self.sparse_container = sparse_container\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/SparseTransformer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_estimator_checks.py", "fn_id": "", "content": "class _BaseMultiLabelClassifierMock(ClassifierMixin, BaseEstimator):\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def __init__(self, response_output):\n        self.response_output = response_output\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def _more_tags(self):\n        return {'multilabel': True}\n\n    def fit(self, X, y):\n        return self\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_estimator_checks/_BaseMultiLabelClassifierMock", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_pprint.py", "fn_id": "", "content": "class LogisticRegression(BaseEstimator):\n\n    def fit(self, X, y):\n        return self\n\n    def __init__(self, penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None):\n        self.penalty = penalty\n        self.dual = dual\n        self.tol = tol\n        self.C = C\n        self.fit_intercept = fit_intercept\n        self.intercept_scaling = intercept_scaling\n        self.class_weight = class_weight\n        self.random_state = random_state\n        self.solver = solver\n        self.max_iter = max_iter\n        self.multi_class = multi_class\n        self.verbose = verbose\n        self.warm_start = warm_start\n        self.n_jobs = n_jobs\n        self.l1_ratio = l1_ratio\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_pprint/LogisticRegression", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_pprint.py", "fn_id": "", "content": "class Pipeline(BaseEstimator):\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def __init__(self, steps, memory=None):\n        self.steps = steps\n        self.memory = memory\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_pprint/Pipeline", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/utils/tests/test_pprint.py", "fn_id": "", "content": "class SimpleImputer(BaseEstimator):\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def __init__(self, missing_values=np.nan, strategy='mean', fill_value=None, verbose=0, copy=True):\n        self.missing_values = missing_values\n        self.strategy = strategy\n        self.fill_value = fill_value\n        self.verbose = verbose\n        self.copy = copy", "class_fn": true, "question_id": "sklearn/sklearn.utils.tests.test_pprint/SimpleImputer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/link.py", "fn_id": "", "content": "class MultinomialLogit(BaseLink):\n    \"\"\"The symmetric multinomial logit function.\n\n    Convention:\n        - y_pred.shape = raw_prediction.shape = (n_samples, n_classes)\n\n    Notes:\n        - The inverse link h is the softmax function.\n        - The sum is over the second axis, i.e. axis=1 (n_classes).\n\n    We have to choose additional constraints in order to make\n\n        y_pred[k] = exp(raw_pred[k]) / sum(exp(raw_pred[k]), k=0..n_classes-1)\n\n    for n_classes classes identifiable and invertible.\n    We choose the symmetric side constraint where the geometric mean response\n    is set as reference category, see [2]:\n\n    The symmetric multinomial logit link function for a single data point is\n    then defined as\n\n        raw_prediction[k] = g(y_pred[k]) = log(y_pred[k]/gmean(y_pred))\n        = log(y_pred[k]) - mean(log(y_pred)).\n\n    Note that this is equivalent to the definition in [1] and implies mean\n    centered raw predictions:\n\n        sum(raw_prediction[k], k=0..n_classes-1) = 0.\n\n    For linear models with raw_prediction = X @ coef, this corresponds to\n    sum(coef[k], k=0..n_classes-1) = 0, i.e. the sum over classes for every\n    feature is zero.\n\n    Reference\n    ---------\n    .. [1] Friedman, Jerome; Hastie, Trevor; Tibshirani, Robert. \"Additive\n        logistic regression: a statistical view of boosting\" Ann. Statist.\n        28 (2000), no. 2, 337--407. doi:10.1214/aos/1016218223.\n        https://projecteuclid.org/euclid.aos/1016218223\n\n    .. [2] Zahid, Faisal Maqbool and Gerhard Tutz. \"Ridge estimation for\n        multinomial logit models with symmetric side constraints.\"\n        Computational Statistics 28 (2013): 1017-1034.\n        http://epub.ub.uni-muenchen.de/11001/1/tr067.pdf\n    \"\"\"\n    is_multiclass = True\n    interval_y_pred = Interval(0, 1, False, False)\n\n    def symmetrize_raw_prediction(self, raw_prediction):\n        return raw_prediction - np.mean(raw_prediction, axis=1)[:, np.newaxis]\n\n    def link(self, y_pred, out=None):\n        gm = gmean(y_pred, axis=1)\n        return np.log(y_pred / gm[:, np.newaxis], out=out)\n\n    def inverse(self, raw_prediction, out=None):\n        if out is None:\n            return softmax(raw_prediction, copy=True)\n        else:\n            np.copyto(out, raw_prediction)\n            softmax(out, copy=False)\n            return out", "class_fn": true, "question_id": "sklearn/sklearn._loss.link/MultinomialLogit", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/loss.py", "fn_id": "", "content": "class HalfBinomialLoss(BaseLoss):\n    \"\"\"Half Binomial deviance loss with logit link, for binary classification.\n\n    This is also know as binary cross entropy, log-loss and logistic loss.\n\n    Domain:\n    y_true in [0, 1], i.e. regression on the unit interval\n    y_pred in (0, 1), i.e. boundaries excluded\n\n    Link:\n    y_pred = expit(raw_prediction)\n\n    For a given sample x_i, half Binomial deviance is defined as the negative\n    log-likelihood of the Binomial/Bernoulli distribution and can be expressed\n    as::\n\n        loss(x_i) = log(1 + exp(raw_pred_i)) - y_true_i * raw_pred_i\n\n    See The Elements of Statistical Learning, by Hastie, Tibshirani, Friedman,\n    section 4.4.1 (about logistic regression).\n\n    Note that the formulation works for classification, y = {0, 1}, as well as\n    logistic regression, y = [0, 1].\n    If you add `constant_to_optimal_zero` to the loss, you get half the\n    Bernoulli/binomial deviance.\n\n    More details: Inserting the predicted probability y_pred = expit(raw_prediction)\n    in the loss gives the well known::\n\n        loss(x_i) = - y_true_i * log(y_pred_i) - (1 - y_true_i) * log(1 - y_pred_i)\n    \"\"\"\n\n    def constant_to_optimal_zero(self, y_true, sample_weight=None):\n        term = xlogy(y_true, y_true) + xlogy(1 - y_true, 1 - y_true)\n        if sample_weight is not None:\n            term *= sample_weight\n        return term\n\n    def predict_proba(self, raw_prediction):\n        \"\"\"Predict probabilities.\n\n        Parameters\n        ----------\n        raw_prediction : array of shape (n_samples,) or (n_samples, 1)\n            Raw prediction values (in link space).\n\n        Returns\n        -------\n        proba : array of shape (n_samples, 2)\n            Element-wise class probabilities.\n        \"\"\"\n        if raw_prediction.ndim == 2 and raw_prediction.shape[1] == 1:\n            raw_prediction = raw_prediction.squeeze(1)\n        proba = np.empty((raw_prediction.shape[0], 2), dtype=raw_prediction.dtype)\n        proba[:, 1] = self.link.inverse(raw_prediction)\n        proba[:, 0] = 1 - proba[:, 1]\n        return proba\n\n    def __init__(self, sample_weight=None):\n        super().__init__(closs=CyHalfBinomialLoss(), link=LogitLink(), n_classes=2)\n        self.interval_y_true = Interval(0, 1, True, True)\n\n    def fit_intercept_only(self, y_true, sample_weight=None):\n        \"\"\"Compute raw_prediction of an intercept-only model.\n\n        This can be used as initial estimates of predictions, i.e. before the\n        first iteration in fit.\n\n        Parameters\n        ----------\n        y_true : array-like of shape (n_samples,)\n            Observed, true target values.\n        sample_weight : None or array of shape (n_samples,)\n            Sample weights.\n\n        Returns\n        -------\n        raw_prediction : numpy scalar or array of shape (n_classes,)\n            Raw predictions of an intercept-only model.\n        \"\"\"\n        y_pred = np.average(y_true, weights=sample_weight, axis=0)\n        eps = 10 * np.finfo(y_pred.dtype).eps\n        if self.interval_y_pred.low == -np.inf:\n            a_min = None\n        elif self.interval_y_pred.low_inclusive:\n            a_min = self.interval_y_pred.low\n        else:\n            a_min = self.interval_y_pred.low + eps\n        if self.interval_y_pred.high == np.inf:\n            a_max = None\n        elif self.interval_y_pred.high_inclusive:\n            a_max = self.interval_y_pred.high\n        else:\n            a_max = self.interval_y_pred.high - eps\n        if a_min is None and a_max is None:\n            return self.link.link(y_pred)\n        else:\n            return self.link.link(np.clip(y_pred, a_min, a_max))\n\n    def in_y_pred_range(self, y):\n        \"\"\"Return True if y is in the valid range of y_pred.\n\n        Parameters\n        ----------\n        y : ndarray\n        \"\"\"\n        return self.interval_y_pred.includes(y)\n\n    def gradient(self, y_true, raw_prediction, sample_weight=None, gradient_out=None, n_threads=1):\n        \"\"\"Compute gradient of loss w.r.t raw_prediction for each input.\n\n        Parameters\n        ----------\n        y_true : C-contiguous array of shape (n_samples,)\n            Observed, true target values.\n        raw_prediction : C-contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)\n            Raw prediction values (in link space).\n        sample_weight : None or C-contiguous array of shape (n_samples,)\n            Sample weights.\n        gradient_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\n            A location into which the result is stored. If None, a new array\n            might be created.\n        n_threads : int, default=1\n            Might use openmp thread parallelism.\n\n        Returns\n        -------\n        gradient : array of shape (n_samples,) or (n_samples, n_classes)\n            Element-wise gradients.\n        \"\"\"\n        if gradient_out is None:\n            gradient_out = np.empty_like(raw_prediction)\n        if raw_prediction.ndim == 2 and raw_prediction.shape[1] == 1:\n            raw_prediction = raw_prediction.squeeze(1)\n        if gradient_out.ndim == 2 and gradient_out.shape[1] == 1:\n            gradient_out = gradient_out.squeeze(1)\n        self.closs.gradient(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight, gradient_out=gradient_out, n_threads=n_threads)\n        return gradient_out", "class_fn": true, "question_id": "sklearn/sklearn._loss.loss/HalfBinomialLoss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/_loss/loss.py", "fn_id": "", "content": "class HuberLoss(BaseLoss):\n    \"\"\"Huber loss, for regression.\n\n    Domain:\n    y_true and y_pred all real numbers\n    quantile in (0, 1)\n\n    Link:\n    y_pred = raw_prediction\n\n    For a given sample x_i, the Huber loss is defined as::\n\n        loss(x_i) = 1/2 * abserr**2            if abserr <= delta\n                    delta * (abserr - delta/2) if abserr > delta\n\n        abserr = |y_true_i - raw_prediction_i|\n        delta = quantile(abserr, self.quantile)\n\n    Note: HuberLoss(quantile=1) equals HalfSquaredError and HuberLoss(quantile=0)\n    equals delta * (AbsoluteError() - delta/2).\n\n    Additional Attributes\n    ---------------------\n    quantile : float\n        The quantile level which defines the breaking point `delta` to distinguish\n        between absolute error and squared error. Must be in range (0, 1).\n\n     Reference\n    ---------\n    .. [1] Friedman, J.H. (2001). :doi:`Greedy function approximation: A gradient\n      boosting machine <10.1214/aos/1013203451>`.\n      Annals of Statistics, 29, 1189-1232.\n    \"\"\"\n    differentiable = False\n    need_update_leaves_values = True\n\n    def __init__(self, sample_weight=None, quantile=0.9, delta=0.5):\n        check_scalar(quantile, 'quantile', target_type=numbers.Real, min_val=0, max_val=1, include_boundaries='neither')\n        self.quantile = quantile\n        super().__init__(closs=CyHuberLoss(delta=float(delta)), link=IdentityLink())\n        self.approx_hessian = True\n        self.constant_hessian = False\n\n    def gradient(self, y_true, raw_prediction, sample_weight=None, gradient_out=None, n_threads=1):\n        \"\"\"Compute gradient of loss w.r.t raw_prediction for each input.\n\n        Parameters\n        ----------\n        y_true : C-contiguous array of shape (n_samples,)\n            Observed, true target values.\n        raw_prediction : C-contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)\n            Raw prediction values (in link space).\n        sample_weight : None or C-contiguous array of shape (n_samples,)\n            Sample weights.\n        gradient_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\n            A location into which the result is stored. If None, a new array\n            might be created.\n        n_threads : int, default=1\n            Might use openmp thread parallelism.\n\n        Returns\n        -------\n        gradient : array of shape (n_samples,) or (n_samples, n_classes)\n            Element-wise gradients.\n        \"\"\"\n        if gradient_out is None:\n            gradient_out = np.empty_like(raw_prediction)\n        if raw_prediction.ndim == 2 and raw_prediction.shape[1] == 1:\n            raw_prediction = raw_prediction.squeeze(1)\n        if gradient_out.ndim == 2 and gradient_out.shape[1] == 1:\n            gradient_out = gradient_out.squeeze(1)\n        self.closs.gradient(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight, gradient_out=gradient_out, n_threads=n_threads)\n        return gradient_out\n\n    def fit_intercept_only(self, y_true, sample_weight=None):\n        \"\"\"Compute raw_prediction of an intercept-only model.\n\n        This is the weighted median of the target, i.e. over the samples\n        axis=0.\n        \"\"\"\n        if sample_weight is None:\n            median = np.percentile(y_true, 50, axis=0)\n        else:\n            median = _weighted_percentile(y_true, sample_weight, 50)\n        diff = y_true - median\n        term = np.sign(diff) * np.minimum(self.closs.delta, np.abs(diff))\n        return median + np.average(term, weights=sample_weight)\n\n    def in_y_pred_range(self, y):\n        \"\"\"Return True if y is in the valid range of y_pred.\n\n        Parameters\n        ----------\n        y : ndarray\n        \"\"\"\n        return self.interval_y_pred.includes(y)\n\n    def constant_to_optimal_zero(self, y_true, sample_weight=None):\n        \"\"\"Calculate term dropped in loss.\n\n        With this term added, the loss of perfect predictions is zero.\n        \"\"\"\n        return np.zeros_like(y_true)", "class_fn": true, "question_id": "sklearn/sklearn._loss.loss/HuberLoss", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/base.py", "fn_id": "", "content": "class ClassifierMixin:\n    \"\"\"Mixin class for all classifiers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - `_estimator_type` class attribute defaulting to `\"classifier\"`;\n    - `score` method that default to :func:`~sklearn.metrics.accuracy_score`.\n    - enforce that `fit` requires `y` to be passed through the `requires_y` tag.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, ClassifierMixin\n    >>> # Mixin classes should always be on the left-hand side for a correct MRO\n    >>> class MyEstimator(ClassifierMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=1)\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([1, 1, 1])\n    >>> estimator.score(X, y)\n    0.66...\n    \"\"\"\n    _estimator_type = 'classifier'\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n        \"\"\"\n        from .metrics import accuracy_score\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n\n    def _more_tags(self):\n        return {'requires_y': True}", "class_fn": true, "question_id": "sklearn/sklearn.base/ClassifierMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/base.py", "fn_id": "", "content": "class TransformerMixin(_SetOutputMixin):\n    \"\"\"Mixin class for all transformers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - a `fit_transform` method that delegates to `fit` and `transform`;\n    - a `set_output` method to output `X` as a specific container type.\n\n    If :term:`get_feature_names_out` is defined, then :class:`BaseEstimator` will\n    automatically wrap `transform` and `fit_transform` to follow the `set_output`\n    API. See the :ref:`developer_api_set_output` for details.\n\n    :class:`OneToOneFeatureMixin` and\n    :class:`ClassNamePrefixFeaturesOutMixin` are helpful mixins for\n    defining :term:`get_feature_names_out`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, TransformerMixin\n    >>> class MyTransformer(TransformerMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         return self\n    ...     def transform(self, X):\n    ...         return np.full(shape=len(X), fill_value=self.param)\n    >>> transformer = MyTransformer()\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> transformer.fit_transform(X)\n    array([1, 1, 1])\n    \"\"\"\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        if _routing_enabled():\n            transform_params = self.get_metadata_routing().consumes(method='transform', params=fit_params.keys())\n            if transform_params:\n                warnings.warn(f\"This object ({self.__class__.__name__}) has a `transform` method which consumes metadata, but `fit_transform` does not forward metadata to `transform`. Please implement a custom `fit_transform` method to forward metadata to `transform` as well. Alternatively, you can explicitly do `set_transform_request`and set all values to `False` to disable metadata routed to `transform`, if that's an option.\", UserWarning)\n        if y is None:\n            return self.fit(X, **fit_params).transform(X)\n        else:\n            return self.fit(X, y, **fit_params).transform(X)", "class_fn": true, "question_id": "sklearn/sklearn.base/TransformerMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/calibration.py", "fn_id": "", "content": "class _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method='sigmoid'):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        (predictions, _) = _get_response_values(self.estimator, X, response_method=['decision_function', 'predict_proba'])\n        if predictions.ndim == 1:\n            predictions = predictions.reshape(-1, 1)\n        n_classes = len(self.classes)\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n        proba = np.zeros((_num_samples(X), n_classes))\n        for (class_idx, this_pred, calibrator) in zip(pos_class_indices, predictions.T, self.calibrators):\n            if n_classes == 2:\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(proba, denominator, out=uniform_proba, where=denominator != 0)\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-05)] = 1.0\n        return proba", "class_fn": true, "question_id": "sklearn/sklearn.calibration/_CalibratedClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_agglomerative.py", "fn_id": "", "content": "class FeatureAgglomeration(ClassNamePrefixFeaturesOutMixin, AgglomerativeClustering, AgglomerationTransform):\n    \"\"\"Agglomerate features.\n\n    Recursively merges pair of clusters of features.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int or None, default=2\n        The number of clusters to find. It must be ``None`` if\n        ``distance_threshold`` is not ``None``.\n\n    metric : str or callable, default=\"euclidean\"\n        Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n        \"manhattan\", \"cosine\", or \"precomputed\". If linkage is \"ward\", only\n        \"euclidean\" is accepted. If \"precomputed\", a distance matrix is needed\n        as input for the fit method.\n\n        .. versionadded:: 1.2\n\n        .. deprecated:: 1.4\n           `metric=None` is deprecated in 1.4 and will be removed in 1.6.\n           Let `metric` be the default value (i.e. `\"euclidean\"`) instead.\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    connectivity : array-like, sparse matrix, or callable, default=None\n        Connectivity matrix. Defines for each feature the neighboring\n        features following a given structure of the data.\n        This can be a connectivity matrix itself or a callable that transforms\n        the data into a connectivity matrix, such as derived from\n        `kneighbors_graph`. Default is `None`, i.e, the\n        hierarchical clustering algorithm is unstructured.\n\n    compute_full_tree : 'auto' or bool, default='auto'\n        Stop early the construction of the tree at `n_clusters`. This is useful\n        to decrease computation time if the number of clusters is not small\n        compared to the number of features. This option is useful only when\n        specifying a connectivity matrix. Note also that when varying the\n        number of clusters and using caching, it may be advantageous to compute\n        the full tree. It must be ``True`` if ``distance_threshold`` is not\n        ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n        to `True` when `distance_threshold` is not `None` or that `n_clusters`\n        is inferior to the maximum between 100 or `0.02 * n_samples`.\n        Otherwise, \"auto\" is equivalent to `False`.\n\n    linkage : {\"ward\", \"complete\", \"average\", \"single\"}, default=\"ward\"\n        Which linkage criterion to use. The linkage criterion determines which\n        distance to use between sets of features. The algorithm will merge\n        the pairs of cluster that minimize this criterion.\n\n        - \"ward\" minimizes the variance of the clusters being merged.\n        - \"complete\" or maximum linkage uses the maximum distances between\n          all features of the two sets.\n        - \"average\" uses the average of the distances of each feature of\n          the two sets.\n        - \"single\" uses the minimum of the distances between all features\n          of the two sets.\n\n    pooling_func : callable, default=np.mean\n        This combines the values of agglomerated features into a single\n        value, and should accept an array of shape [M, N] and the keyword\n        argument `axis=1`, and reduce it to an array of size [M].\n\n    distance_threshold : float, default=None\n        The linkage distance threshold at or above which clusters will not be\n        merged. If not ``None``, ``n_clusters`` must be ``None`` and\n        ``compute_full_tree`` must be ``True``.\n\n        .. versionadded:: 0.21\n\n    compute_distances : bool, default=False\n        Computes distances between clusters even if `distance_threshold` is not\n        used. This can be used to make dendrogram visualization, but introduces\n        a computational and memory overhead.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    n_clusters_ : int\n        The number of clusters found by the algorithm. If\n        ``distance_threshold=None``, it will be equal to the given\n        ``n_clusters``.\n\n    labels_ : array-like of (n_features,)\n        Cluster labels for each feature.\n\n    n_leaves_ : int\n        Number of leaves in the hierarchical tree.\n\n    n_connected_components_ : int\n        The estimated number of connected components in the graph.\n\n        .. versionadded:: 0.21\n            ``n_connected_components_`` was added to replace ``n_components_``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    children_ : array-like of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_features`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_features` is a non-leaf\n        node and has children `children_[i - n_features]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_features + i`.\n\n    distances_ : array-like of shape (n_nodes-1,)\n        Distances between nodes in the corresponding place in `children_`.\n        Only computed if `distance_threshold` is used or `compute_distances`\n        is set to `True`.\n\n    See Also\n    --------\n    AgglomerativeClustering : Agglomerative clustering samples instead of\n        features.\n    ward_tree : Hierarchical clustering with ward linkage.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import datasets, cluster\n    >>> digits = datasets.load_digits()\n    >>> images = digits.images\n    >>> X = np.reshape(images, (len(images), -1))\n    >>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n    >>> agglo.fit(X)\n    FeatureAgglomeration(n_clusters=32)\n    >>> X_reduced = agglo.transform(X)\n    >>> X_reduced.shape\n    (1797, 32)\n    \"\"\"\n    _parameter_constraints: dict = {'n_clusters': [Interval(Integral, 1, None, closed='left'), None], 'metric': [StrOptions(set(_VALID_METRICS) | {'precomputed'}), callable, Hidden(None)], 'memory': [str, HasMethods('cache'), None], 'connectivity': ['array-like', 'sparse matrix', callable, None], 'compute_full_tree': [StrOptions({'auto'}), 'boolean'], 'linkage': [StrOptions(set(_TREE_BUILDERS.keys()))], 'pooling_func': [callable], 'distance_threshold': [Interval(Real, 0, None, closed='left'), None], 'compute_distances': ['boolean']}\n\n    def _fit(self, X):\n        \"\"\"Fit without validation\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features) or (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``.\n\n        Returns\n        -------\n        self : object\n            Returns the fitted instance.\n        \"\"\"\n        memory = check_memory(self.memory)\n        if self.metric is None:\n            warnings.warn(\"`metric=None` is deprecated in version 1.4 and will be removed in version 1.6. Let `metric` be the default value (i.e. `'euclidean'`) instead.\", FutureWarning)\n            self._metric = 'euclidean'\n        else:\n            self._metric = self.metric\n        if not (self.n_clusters is None) ^ (self.distance_threshold is None):\n            raise ValueError('Exactly one of n_clusters and distance_threshold has to be set, and the other needs to be None.')\n        if self.distance_threshold is not None and (not self.compute_full_tree):\n            raise ValueError('compute_full_tree must be True if distance_threshold is set.')\n        if self.linkage == 'ward' and self._metric != 'euclidean':\n            raise ValueError(f'{self._metric} was provided as metric. Ward can only work with euclidean distances.')\n        tree_builder = _TREE_BUILDERS[self.linkage]\n        connectivity = self.connectivity\n        if self.connectivity is not None:\n            if callable(self.connectivity):\n                connectivity = self.connectivity(X)\n            connectivity = check_array(connectivity, accept_sparse=['csr', 'coo', 'lil'])\n        n_samples = len(X)\n        compute_full_tree = self.compute_full_tree\n        if self.connectivity is None:\n            compute_full_tree = True\n        if compute_full_tree == 'auto':\n            if self.distance_threshold is not None:\n                compute_full_tree = True\n            else:\n                compute_full_tree = self.n_clusters < max(100, 0.02 * n_samples)\n        n_clusters = self.n_clusters\n        if compute_full_tree:\n            n_clusters = None\n        kwargs = {}\n        if self.linkage != 'ward':\n            kwargs['linkage'] = self.linkage\n            kwargs['affinity'] = self._metric\n        distance_threshold = self.distance_threshold\n        return_distance = distance_threshold is not None or self.compute_distances\n        out = memory.cache(tree_builder)(X, connectivity=connectivity, n_clusters=n_clusters, return_distance=return_distance, **kwargs)\n        (self.children_, self.n_connected_components_, self.n_leaves_, parents) = out[:4]\n        if return_distance:\n            self.distances_ = out[-1]\n        if self.distance_threshold is not None:\n            self.n_clusters_ = np.count_nonzero(self.distances_ >= distance_threshold) + 1\n        else:\n            self.n_clusters_ = self.n_clusters\n        if compute_full_tree:\n            self.labels_ = _hc_cut(self.n_clusters_, self.children_, self.n_leaves_)\n        else:\n            labels = _hierarchical.hc_get_heads(parents, copy=False)\n            labels = np.copy(labels[:n_samples])\n            self.labels_ = np.searchsorted(np.unique(labels), labels)\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the hierarchical clustering on the data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the transformer.\n        \"\"\"\n        X = self._validate_data(X, ensure_min_features=2)\n        super()._fit(X.T)\n        self._n_features_out = self.n_clusters_\n        return self\n\n    @property\n    def fit_predict(self):\n        \"\"\"Fit and return the result of each sample's clustering assignment.\"\"\"\n        raise AttributeError\n\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.\n        \"\"\"\n        check_is_fitted(self, '_n_features_out')\n        return _generate_get_feature_names_out(self, self._n_features_out, input_features=input_features)\n\n    def __init__(self, n_clusters=2, *, metric='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', pooling_func=np.mean, distance_threshold=None, compute_distances=False):\n        super().__init__(n_clusters=n_clusters, memory=memory, connectivity=connectivity, compute_full_tree=compute_full_tree, linkage=linkage, metric=metric, distance_threshold=distance_threshold, compute_distances=compute_distances)\n        self.pooling_func = pooling_func\n\n    def inverse_transform(self, X=None, *, Xt=None):\n        \"\"\"\n        Inverse the transformation and return a vector of size `n_features`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_clusters) or (n_clusters,)\n            The values to be assigned to each cluster of samples.\n\n        Xt : array-like of shape (n_samples, n_clusters) or (n_clusters,)\n            The values to be assigned to each cluster of samples.\n\n            .. deprecated:: 1.5\n                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.\n\n        Returns\n        -------\n        X : ndarray of shape (n_samples, n_features) or (n_features,)\n            A vector of size `n_samples` with the values of `Xred` assigned to\n            each of the cluster of samples.\n        \"\"\"\n        X = _deprecate_Xt_in_inverse_transform(X, Xt)\n        check_is_fitted(self)\n        (unil, inverse) = np.unique(self.labels_, return_inverse=True)\n        return X[..., inverse]", "class_fn": true, "question_id": "sklearn/sklearn.cluster._agglomerative/FeatureAgglomeration", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_bicluster.py", "fn_id": "", "content": "class SpectralCoclustering(BaseSpectral):\n    \"\"\"Spectral Co-Clustering algorithm (Dhillon, 2001).\n\n    Clusters rows and columns of an array `X` to solve the relaxed\n    normalized cut of the bipartite graph created from `X` as follows:\n    the edge between row vertex `i` and column vertex `j` has weight\n    `X[i, j]`.\n\n    The resulting bicluster structure is block-diagonal, since each\n    row and each column belongs to exactly one bicluster.\n\n    Supports sparse matrices, as long as they are nonnegative.\n\n    Read more in the :ref:`User Guide <spectral_coclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int, default=3\n        The number of biclusters to find.\n\n    svd_method : {'randomized', 'arpack'}, default='randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', use\n        :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', use\n        :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, default=None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, default=False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random'}, or ndarray of shape             (n_clusters, n_features), default='k-means++'\n        Method for initialization of k-means algorithm; defaults to\n        'k-means++'.\n\n    n_init : int, default=10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    random_state : int, RandomState instance, default=None\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like of shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like of shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like of shape (n_rows,)\n        The bicluster label of each row.\n\n    column_labels_ : array-like of shape (n_cols,)\n        The bicluster label of each column.\n\n    biclusters_ : tuple of two ndarrays\n        The tuple contains the `rows_` and `columns_` arrays.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    SpectralBiclustering : Partitions rows and columns under the assumption\n        that the data has an underlying checkerboard structure.\n\n    References\n    ----------\n    * :doi:`Dhillon, Inderjit S, 2001. Co-clustering documents and words using\n      bipartite spectral graph partitioning.\n      <10.1145/502512.502550>`\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralCoclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_ #doctest: +SKIP\n    array([0, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_ #doctest: +SKIP\n    array([0, 0], dtype=int32)\n    >>> clustering\n    SpectralCoclustering(n_clusters=2, random_state=0)\n    \"\"\"\n    _parameter_constraints: dict = {**BaseSpectral._parameter_constraints, 'n_clusters': [Interval(Integral, 1, None, closed='left')]}\n\n    def _more_tags(self):\n        return {'_xfail_checks': {'check_estimators_dtypes': 'raises nan error', 'check_fit2d_1sample': '_scale_normalize fails', 'check_fit2d_1feature': 'raises apply_along_axis error', 'check_estimator_sparse_matrix': 'does not fail gracefully', 'check_estimator_sparse_array': 'does not fail gracefully', 'check_methods_subset_invariance': 'empty array passed inside', 'check_dont_overwrite_parameters': 'empty array passed inside', 'check_fit2d_predict1d': 'empty array passed inside'}}\n\n    def _svd(self, array, n_components, n_discard):\n        \"\"\"Returns first `n_components` left and right singular\n        vectors u and v, discarding the first `n_discard`.\n        \"\"\"\n        if self.svd_method == 'randomized':\n            kwargs = {}\n            if self.n_svd_vecs is not None:\n                kwargs['n_oversamples'] = self.n_svd_vecs\n            (u, _, vt) = randomized_svd(array, n_components, random_state=self.random_state, **kwargs)\n        elif self.svd_method == 'arpack':\n            (u, _, vt) = svds(array, k=n_components, ncv=self.n_svd_vecs)\n            if np.any(np.isnan(vt)):\n                A = safe_sparse_dot(array.T, array)\n                random_state = check_random_state(self.random_state)\n                v0 = random_state.uniform(-1, 1, A.shape[0])\n                (_, v) = eigsh(A, ncv=self.n_svd_vecs, v0=v0)\n                vt = v.T\n            if np.any(np.isnan(u)):\n                A = safe_sparse_dot(array, array.T)\n                random_state = check_random_state(self.random_state)\n                v0 = random_state.uniform(-1, 1, A.shape[0])\n                (_, u) = eigsh(A, ncv=self.n_svd_vecs, v0=v0)\n        assert_all_finite(u)\n        assert_all_finite(vt)\n        u = u[:, n_discard:]\n        vt = vt[n_discard:]\n        return (u, vt.T)\n\n    def _k_means(self, data, n_clusters):\n        if self.mini_batch:\n            model = MiniBatchKMeans(n_clusters, init=self.init, n_init=self.n_init, random_state=self.random_state)\n        else:\n            model = KMeans(n_clusters, init=self.init, n_init=self.n_init, random_state=self.random_state)\n        model.fit(data)\n        centroid = model.cluster_centers_\n        labels = model.labels_\n        return (centroid, labels)\n\n    def _check_parameters(self, n_samples):\n        if self.n_clusters > n_samples:\n            raise ValueError(f'n_clusters should be <= n_samples={n_samples}. Got {self.n_clusters} instead.')\n\n    def _fit(self, X):\n        (normalized_data, row_diag, col_diag) = _scale_normalize(X)\n        n_sv = 1 + int(np.ceil(np.log2(self.n_clusters)))\n        (u, v) = self._svd(normalized_data, n_sv, n_discard=1)\n        z = np.vstack((row_diag[:, np.newaxis] * u, col_diag[:, np.newaxis] * v))\n        (_, labels) = self._k_means(z, self.n_clusters)\n        n_rows = X.shape[0]\n        self.row_labels_ = labels[:n_rows]\n        self.column_labels_ = labels[n_rows:]\n        self.rows_ = np.vstack([self.row_labels_ == c for c in range(self.n_clusters)])\n        self.columns_ = np.vstack([self.column_labels_ == c for c in range(self.n_clusters)])\n\n    def __init__(self, n_clusters=3, *, svd_method='randomized', n_svd_vecs=None, mini_batch=False, init='k-means++', n_init=10, random_state=None):\n        super().__init__(n_clusters, svd_method, n_svd_vecs, mini_batch, init, n_init, random_state)", "class_fn": true, "question_id": "sklearn/sklearn.cluster._bicluster/SpectralCoclustering", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_birch.py", "fn_id": "", "content": "class _CFSubcluster:\n    \"\"\"Each subcluster in a CFNode is called a CFSubcluster.\n\n    A CFSubcluster can have a CFNode has its child.\n\n    Parameters\n    ----------\n    linear_sum : ndarray of shape (n_features,), default=None\n        Sample. This is kept optional to allow initialization of empty\n        subclusters.\n\n    Attributes\n    ----------\n    n_samples_ : int\n        Number of samples that belong to each subcluster.\n\n    linear_sum_ : ndarray\n        Linear sum of all the samples in a subcluster. Prevents holding\n        all sample data in memory.\n\n    squared_sum_ : float\n        Sum of the squared l2 norms of all samples belonging to a subcluster.\n\n    centroid_ : ndarray of shape (branching_factor + 1, n_features)\n        Centroid of the subcluster. Prevent recomputing of centroids when\n        ``CFNode.centroids_`` is called.\n\n    child_ : _CFNode\n        Child Node of the subcluster. Once a given _CFNode is set as the child\n        of the _CFNode, it is set to ``self.child_``.\n\n    sq_norm_ : ndarray of shape (branching_factor + 1,)\n        Squared norm of the subcluster. Used to prevent recomputing when\n        pairwise minimum distances are computed.\n    \"\"\"\n\n    def __init__(self, *, linear_sum=None):\n        if linear_sum is None:\n            self.n_samples_ = 0\n            self.squared_sum_ = 0.0\n            self.centroid_ = self.linear_sum_ = 0\n        else:\n            self.n_samples_ = 1\n            self.centroid_ = self.linear_sum_ = linear_sum\n            self.squared_sum_ = self.sq_norm_ = np.dot(self.linear_sum_, self.linear_sum_)\n        self.child_ = None\n\n    def update(self, subcluster):\n        self.n_samples_ += subcluster.n_samples_\n        self.linear_sum_ += subcluster.linear_sum_\n        self.squared_sum_ += subcluster.squared_sum_\n        self.centroid_ = self.linear_sum_ / self.n_samples_\n        self.sq_norm_ = np.dot(self.centroid_, self.centroid_)\n\n    def merge_subcluster(self, nominee_cluster, threshold):\n        \"\"\"Check if a cluster is worthy enough to be merged. If\n        yes then merge.\n        \"\"\"\n        new_ss = self.squared_sum_ + nominee_cluster.squared_sum_\n        new_ls = self.linear_sum_ + nominee_cluster.linear_sum_\n        new_n = self.n_samples_ + nominee_cluster.n_samples_\n        new_centroid = 1 / new_n * new_ls\n        new_sq_norm = np.dot(new_centroid, new_centroid)\n        sq_radius = new_ss / new_n - new_sq_norm\n        if sq_radius <= threshold ** 2:\n            (self.n_samples_, self.linear_sum_, self.squared_sum_, self.centroid_, self.sq_norm_) = (new_n, new_ls, new_ss, new_centroid, new_sq_norm)\n            return True\n        return False\n\n    @property\n    def radius(self):\n        \"\"\"Return radius of the subcluster\"\"\"\n        sq_radius = self.squared_sum_ / self.n_samples_ - self.sq_norm_\n        return sqrt(max(0, sq_radius))", "class_fn": true, "question_id": "sklearn/sklearn.cluster._birch/_CFSubcluster", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_feature_agglomeration.py", "fn_id": "", "content": "class AgglomerationTransform(TransformerMixin):\n    \"\"\"\n    A class for feature agglomeration via the transform interface.\n    \"\"\"\n    __metadata_request__inverse_transform = {'Xt': metadata_routing.UNUSED}\n\n    def transform(self, X):\n        \"\"\"\n        Transform a new matrix using the built clustering.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)\n            A M by N array of M observations in N dimensions or a length\n            M array of M one-dimensional observations.\n\n        Returns\n        -------\n        Y : ndarray of shape (n_samples, n_clusters) or (n_clusters,)\n            The pooled values for each feature cluster.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        if self.pooling_func == np.mean and (not issparse(X)):\n            size = np.bincount(self.labels_)\n            n_samples = X.shape[0]\n            nX = np.array([np.bincount(self.labels_, X[i, :]) / size for i in range(n_samples)])\n        else:\n            nX = [self.pooling_func(X[:, self.labels_ == l], axis=1) for l in np.unique(self.labels_)]\n            nX = np.array(nX).T\n        return nX\n\n    def inverse_transform(self, X=None, *, Xt=None):\n        \"\"\"\n        Inverse the transformation and return a vector of size `n_features`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_clusters) or (n_clusters,)\n            The values to be assigned to each cluster of samples.\n\n        Xt : array-like of shape (n_samples, n_clusters) or (n_clusters,)\n            The values to be assigned to each cluster of samples.\n\n            .. deprecated:: 1.5\n                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.\n\n        Returns\n        -------\n        X : ndarray of shape (n_samples, n_features) or (n_features,)\n            A vector of size `n_samples` with the values of `Xred` assigned to\n            each of the cluster of samples.\n        \"\"\"\n        X = _deprecate_Xt_in_inverse_transform(X, Xt)\n        check_is_fitted(self)\n        (unil, inverse) = np.unique(self.labels_, return_inverse=True)\n        return X[..., inverse]", "class_fn": true, "question_id": "sklearn/sklearn.cluster._feature_agglomeration/AgglomerationTransform", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_kmeans.py", "fn_id": "", "content": "class MiniBatchKMeans(_BaseKMeans):\n    \"\"\"\n    Mini-Batch K-Means clustering.\n\n    Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centroids using sampling based on\n        an empirical probability distribution of the points' contribution to the\n        overall inertia. This technique speeds up convergence. The algorithm\n        implemented is \"greedy k-means++\". It differs from the vanilla k-means++\n        by making several trials at each sampling step and choosing the best centroid\n        among them.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an array is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    max_iter : int, default=100\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n    batch_size : int, default=1024\n        Size of the mini batches.\n        For faster computations, you can set the ``batch_size`` greater than\n        256 * number of cores to enable parallelism on all cores.\n\n        .. versionchanged:: 1.0\n           `batch_size` default changed from 100 to 1024.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    compute_labels : bool, default=True\n        Compute label assignment and inertia for the complete dataset\n        once the minibatch optimization has converged in fit.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization and\n        random reassignment. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Control early stopping based on the relative center changes as\n        measured by a smoothed, variance-normalized of the mean center\n        squared position changes. This early stopping heuristics is\n        closer to the one used for the batch variant of the algorithms\n        but induces a slight computational and memory overhead over the\n        inertia heuristic.\n\n        To disable convergence detection based on normalized center\n        change, set tol to 0.0 (default).\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini\n        batches that does not yield an improvement on the smoothed inertia.\n\n        To disable convergence detection based on inertia, set\n        max_no_improvement to None.\n\n    init_size : int, default=None\n        Number of samples to randomly sample for speeding up the\n        initialization (sometimes at the expense of accuracy): the\n        only algorithm is initialized by running a batch KMeans on a\n        random subset of the data. This needs to be larger than n_clusters.\n\n        If `None`, the heuristic is `init_size = 3 * batch_size` if\n        `3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`.\n\n    n_init : 'auto' or int, default=\"auto\"\n        Number of random initializations that are tried.\n        In contrast to KMeans, the algorithm is only run once, using the best of\n        the `n_init` initializations as measured by inertia. Several runs are\n        recommended for sparse high-dimensional problems (see\n        :ref:`kmeans_sparse_high_dim`).\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        3 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'` in version.\n\n    reassignment_ratio : float, default=0.01\n        Control the fraction of the maximum number of counts for a center to\n        be reassigned. A higher value means that low count centers are more\n        easily reassigned, which means that the model will take longer to\n        converge, but should converge in a better clustering. However, too high\n        a value may cause convergence issues, especially with a small batch\n        size.\n\n    Attributes\n    ----------\n\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point (if compute_labels is set to True).\n\n    inertia_ : float\n        The value of the inertia criterion associated with the chosen\n        partition if compute_labels is set to True. If compute_labels is set to\n        False, it's an approximation of the inertia based on an exponentially\n        weighted average of the batch inertiae.\n        The inertia is defined as the sum of square distances of samples to\n        their cluster center, weighted by the sample weights if provided.\n\n    n_iter_ : int\n        Number of iterations over the full dataset.\n\n    n_steps_ : int\n        Number of minibatches processed.\n\n        .. versionadded:: 1.0\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    KMeans : The classic implementation of the clustering method based on the\n        Lloyd's algorithm. It consumes the whole set of input data at each\n        iteration.\n\n    Notes\n    -----\n    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n    When there are too few points in the dataset, some centers may be\n    duplicated, which means that a proper clustering in terms of the number\n    of requesting clusters and the number of returned clusters will not\n    always match. One solution is to set `reassignment_ratio=0`, which\n    prevents reassignments of clusters that are too small.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MiniBatchKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 0], [4, 4],\n    ...               [4, 5], [0, 1], [2, 2],\n    ...               [3, 2], [5, 5], [1, -1]])\n    >>> # manually fit on batches\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          n_init=\"auto\")\n    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n    >>> kmeans.cluster_centers_\n    array([[3.375, 3.  ],\n           [0.75 , 0.5 ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    >>> # fit on the whole data\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          max_iter=10,\n    ...                          n_init=\"auto\").fit(X)\n    >>> kmeans.cluster_centers_\n    array([[3.55102041, 2.48979592],\n           [1.06896552, 1.        ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    \"\"\"\n    _parameter_constraints: dict = {**_BaseKMeans._parameter_constraints, 'batch_size': [Interval(Integral, 1, None, closed='left')], 'compute_labels': ['boolean'], 'max_no_improvement': [Interval(Integral, 0, None, closed='left'), None], 'init_size': [Interval(Integral, 1, None, closed='left'), None], 'reassignment_ratio': [Interval(Real, 0, None, closed='left')]}\n\n    def _init_centroids(self, X, x_squared_norms, init, random_state, sample_weight, init_size=None, n_centroids=None):\n        \"\"\"Compute the initial centroids.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        x_squared_norms : ndarray of shape (n_samples,)\n            Squared euclidean norm of each data point. Pass it if you have it\n            at hands already to avoid it being recomputed here.\n\n        init : {'k-means++', 'random'}, callable or ndarray of shape                 (n_clusters, n_features)\n            Method for initialization.\n\n        random_state : RandomState instance\n            Determines random number generation for centroid initialization.\n            See :term:`Glossary <random_state>`.\n\n        sample_weight : ndarray of shape (n_samples,)\n            The weights for each observation in X. `sample_weight` is not used\n            during initialization if `init` is a callable or a user provided\n            array.\n\n        init_size : int, default=None\n            Number of samples to randomly sample for speeding up the\n            initialization (sometimes at the expense of accuracy).\n\n        n_centroids : int, default=None\n            Number of centroids to initialize.\n            If left to 'None' the number of centroids will be equal to\n            number of clusters to form (self.n_clusters).\n\n        Returns\n        -------\n        centers : ndarray of shape (n_clusters, n_features)\n            Initial centroids of clusters.\n        \"\"\"\n        n_samples = X.shape[0]\n        n_clusters = self.n_clusters if n_centroids is None else n_centroids\n        if init_size is not None and init_size < n_samples:\n            init_indices = random_state.randint(0, n_samples, init_size)\n            X = X[init_indices]\n            x_squared_norms = x_squared_norms[init_indices]\n            n_samples = X.shape[0]\n            sample_weight = sample_weight[init_indices]\n        if isinstance(init, str) and init == 'k-means++':\n            (centers, _) = _kmeans_plusplus(X, n_clusters, random_state=random_state, x_squared_norms=x_squared_norms, sample_weight=sample_weight)\n        elif isinstance(init, str) and init == 'random':\n            seeds = random_state.choice(n_samples, size=n_clusters, replace=False, p=sample_weight / sample_weight.sum())\n            centers = X[seeds]\n        elif _is_arraylike_not_scalar(self.init):\n            centers = init\n        elif callable(init):\n            centers = init(X, n_clusters, random_state=random_state)\n            centers = check_array(centers, dtype=X.dtype, copy=False, order='C')\n            self._validate_center_shape(X, centers)\n        if sp.issparse(centers):\n            centers = centers.toarray()\n        return centers\n\n    def _random_reassign(self):\n        \"\"\"Check if a random reassignment needs to be done.\n\n        Do random reassignments each time 10 * n_clusters samples have been\n        processed.\n\n        If there are empty clusters we always want to reassign.\n        \"\"\"\n        self._n_since_last_reassign += self._batch_size\n        if (self._counts == 0).any() or self._n_since_last_reassign >= 10 * self.n_clusters:\n            self._n_since_last_reassign = 0\n            return True\n        return False\n\n    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight).labels_\n\n    def _check_params_vs_input(self, X):\n        super()._check_params_vs_input(X, default_n_init=3)\n        self._batch_size = min(self.batch_size, X.shape[0])\n        self._init_size = self.init_size\n        if self._init_size is None:\n            self._init_size = 3 * self._batch_size\n            if self._init_size < self.n_clusters:\n                self._init_size = 3 * self.n_clusters\n        elif self._init_size < self.n_clusters:\n            warnings.warn(f'init_size={self._init_size} should be larger than n_clusters={self.n_clusters}. Setting it to min(3*n_clusters, n_samples)', RuntimeWarning, stacklevel=2)\n            self._init_size = 3 * self.n_clusters\n        self._init_size = min(self._init_size, X.shape[0])\n        if self.reassignment_ratio < 0:\n            raise ValueError(f'reassignment_ratio should be >= 0, got {self.reassignment_ratio} instead.')\n\n    def _mini_batch_convergence(self, step, n_steps, n_samples, centers_squared_diff, batch_inertia):\n        \"\"\"Helper function to encapsulate the early stopping logic\"\"\"\n        batch_inertia /= self._batch_size\n        step = step + 1\n        if step == 1:\n            if self.verbose:\n                print(f'Minibatch step {step}/{n_steps}: mean batch inertia: {batch_inertia}')\n            return False\n        if self._ewa_inertia is None:\n            self._ewa_inertia = batch_inertia\n        else:\n            alpha = self._batch_size * 2.0 / (n_samples + 1)\n            alpha = min(alpha, 1)\n            self._ewa_inertia = self._ewa_inertia * (1 - alpha) + batch_inertia * alpha\n        if self.verbose:\n            print(f'Minibatch step {step}/{n_steps}: mean batch inertia: {batch_inertia}, ewa inertia: {self._ewa_inertia}')\n        if self._tol > 0.0 and centers_squared_diff <= self._tol:\n            if self.verbose:\n                print(f'Converged (small centers change) at step {step}/{n_steps}')\n            return True\n        if self._ewa_inertia_min is None or self._ewa_inertia < self._ewa_inertia_min:\n            self._no_improvement = 0\n            self._ewa_inertia_min = self._ewa_inertia\n        else:\n            self._no_improvement += 1\n        if self.max_no_improvement is not None and self._no_improvement >= self.max_no_improvement:\n            if self.verbose:\n                print(f'Converged (lack of improvement in inertia) at step {step}/{n_steps}')\n            return True\n        return False\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr', dtype=[np.float64, np.float32], order='C', accept_large_sparse=False)\n        self._check_params_vs_input(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self._n_threads = _openmp_effective_n_threads()\n        (n_samples, n_features) = X.shape\n        init = self.init\n        if _is_arraylike_not_scalar(init):\n            init = check_array(init, dtype=X.dtype, copy=True, order='C')\n            self._validate_center_shape(X, init)\n        self._check_mkl_vcomp(X, self._batch_size)\n        x_squared_norms = row_norms(X, squared=True)\n        validation_indices = random_state.randint(0, n_samples, self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(f'Init {init_idx + 1}/{self._n_init} with method {init}')\n            cluster_centers = self._init_centroids(X, x_squared_norms=x_squared_norms, init=init, random_state=random_state, init_size=self._init_size, sample_weight=sample_weight)\n            (_, inertia) = _labels_inertia_threadpool_limit(X_valid, sample_weight_valid, cluster_centers, n_threads=self._n_threads)\n            if self.verbose:\n                print(f'Inertia for init {init_idx + 1}/{self._n_init}: {inertia}')\n            if best_inertia is None or inertia < best_inertia:\n                init_centers = cluster_centers\n                best_inertia = inertia\n        centers = init_centers\n        centers_new = np.empty_like(centers)\n        self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n        self._ewa_inertia = None\n        self._ewa_inertia_min = None\n        self._no_improvement = 0\n        self._n_since_last_reassign = 0\n        n_steps = self.max_iter * n_samples // self._batch_size\n        with _get_threadpool_controller().limit(limits=1, user_api='blas'):\n            for i in range(n_steps):\n                minibatch_indices = random_state.randint(0, n_samples, self._batch_size)\n                batch_inertia = _mini_batch_step(X=X[minibatch_indices], sample_weight=sample_weight[minibatch_indices], centers=centers, centers_new=centers_new, weight_sums=self._counts, random_state=random_state, random_reassign=self._random_reassign(), reassignment_ratio=self.reassignment_ratio, verbose=self.verbose, n_threads=self._n_threads)\n                if self._tol > 0.0:\n                    centers_squared_diff = np.sum((centers_new - centers) ** 2)\n                else:\n                    centers_squared_diff = 0\n                (centers, centers_new) = (centers_new, centers)\n                if self._mini_batch_convergence(i, n_steps, n_samples, centers_squared_diff, batch_inertia):\n                    break\n        self.cluster_centers_ = centers\n        self._n_features_out = self.cluster_centers_.shape[0]\n        self.n_steps_ = i + 1\n        self.n_iter_ = int(np.ceil((i + 1) * self._batch_size / n_samples))\n        if self.compute_labels:\n            (self.labels_, self.inertia_) = _labels_inertia_threadpool_limit(X, sample_weight, self.cluster_centers_, n_threads=self._n_threads)\n        else:\n            self.inertia_ = self._ewa_inertia * n_samples\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Update k means estimate on a single mini-batch X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n        Returns\n        -------\n        self : object\n            Return updated estimator.\n        \"\"\"\n        has_centers = hasattr(self, 'cluster_centers_')\n        X = self._validate_data(X, accept_sparse='csr', dtype=[np.float64, np.float32], order='C', accept_large_sparse=False, reset=not has_centers)\n        self._random_state = getattr(self, '_random_state', check_random_state(self.random_state))\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self.n_steps_ = getattr(self, 'n_steps_', 0)\n        x_squared_norms = row_norms(X, squared=True)\n        if not has_centers:\n            self._check_params_vs_input(X)\n            self._n_threads = _openmp_effective_n_threads()\n            init = self.init\n            if _is_arraylike_not_scalar(init):\n                init = check_array(init, dtype=X.dtype, copy=True, order='C')\n                self._validate_center_shape(X, init)\n            self._check_mkl_vcomp(X, X.shape[0])\n            self.cluster_centers_ = self._init_centroids(X, x_squared_norms=x_squared_norms, init=init, random_state=self._random_state, init_size=self._init_size, sample_weight=sample_weight)\n            self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n            self._n_since_last_reassign = 0\n        with _get_threadpool_controller().limit(limits=1, user_api='blas'):\n            _mini_batch_step(X, sample_weight=sample_weight, centers=self.cluster_centers_, centers_new=self.cluster_centers_, weight_sums=self._counts, random_state=self._random_state, random_reassign=self._random_reassign(), reassignment_ratio=self.reassignment_ratio, verbose=self.verbose, n_threads=self._n_threads)\n        if self.compute_labels:\n            (self.labels_, self.inertia_) = _labels_inertia_threadpool_limit(X, sample_weight, self.cluster_centers_, n_threads=self._n_threads)\n        self.n_steps_ += 1\n        self._n_features_out = self.cluster_centers_.shape[0]\n        return self\n\n    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n        warnings.warn(f'MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= {self._n_threads * CHUNK_SIZE} or by setting the environment variable OMP_NUM_THREADS={n_active_threads}')\n\n    def __init__(self, n_clusters=8, *, init='k-means++', max_iter=100, batch_size=1024, verbose=0, compute_labels=True, random_state=None, tol=0.0, max_no_improvement=10, init_size=None, n_init='auto', reassignment_ratio=0.01):\n        super().__init__(n_clusters=n_clusters, init=init, max_iter=max_iter, verbose=verbose, random_state=random_state, tol=tol, n_init=n_init)\n        self.max_no_improvement = max_no_improvement\n        self.batch_size = batch_size\n        self.compute_labels = compute_labels\n        self.init_size = init_size\n        self.reassignment_ratio = reassignment_ratio\n\n    def _check_test_data(self, X):\n        X = self._validate_data(X, accept_sparse='csr', reset=False, dtype=[np.float64, np.float32], order='C', accept_large_sparse=False)\n        return X", "class_fn": true, "question_id": "sklearn/sklearn.cluster._kmeans/MiniBatchKMeans", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cluster/_optics.py", "fn_id": "", "content": "class OPTICS(ClusterMixin, BaseEstimator):\n    \"\"\"Estimate clustering structure from vector array.\n\n    OPTICS (Ordering Points To Identify the Clustering Structure), closely\n    related to DBSCAN, finds core sample of high density and expands clusters\n    from them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\n    neighborhood radius. Better suited for usage on large datasets than the\n    current sklearn implementation of DBSCAN.\n\n    Clusters are then extracted using a DBSCAN-like method\n    (cluster_method = 'dbscan') or an automatic\n    technique proposed in [1]_ (cluster_method = 'xi').\n\n    This implementation deviates from the original OPTICS by first performing\n    k-nearest-neighborhood searches on all points to identify core sizes, then\n    computing only the distances to unprocessed points when constructing the\n    cluster order. Note that we do not employ a heap to manage the expansion\n    candidates, so the time complexity will be O(n^2).\n\n    Read more in the :ref:`User Guide <optics>`.\n\n    Parameters\n    ----------\n    min_samples : int > 1 or float between 0 and 1, default=5\n        The number of samples in a neighborhood for a point to be considered as\n        a core point. Also, up and down steep regions can't have more than\n        ``min_samples`` consecutive non-steep points. Expressed as an absolute\n        number or a fraction of the number of samples (rounded to be at least\n        2).\n\n    max_eps : float, default=np.inf\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. Default value of ``np.inf`` will\n        identify clusters across all scales; reducing ``max_eps`` will result\n        in shorter run times.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Any metric from scikit-learn\n        or scipy.spatial.distance can be used.\n\n        If metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays as input and return one value indicating the\n        distance between them. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string. If metric is\n        \"precomputed\", `X` is assumed to be a distance matrix and must be\n        square.\n\n        Valid values for metric are:\n\n        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n          'manhattan']\n\n        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n          'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n          'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n          'yule']\n\n        Sparse matrices are only supported by scikit-learn metrics.\n        See the documentation for scipy.spatial.distance for details on these\n        metrics.\n\n        .. note::\n           `'kulsinski'` is deprecated from SciPy 1.9 and will removed in SciPy 1.11.\n\n    p : float, default=2\n        Parameter for the Minkowski metric from\n        :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    cluster_method : str, default='xi'\n        The extraction method used to extract clusters using the calculated\n        reachability and ordering. Possible values are \"xi\" and \"dbscan\".\n\n    eps : float, default=None\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. By default it assumes the same value\n        as ``max_eps``.\n        Used only when ``cluster_method='dbscan'``.\n\n    xi : float between 0 and 1, default=0.05\n        Determines the minimum steepness on the reachability plot that\n        constitutes a cluster boundary. For example, an upwards point in the\n        reachability plot is defined by the ratio from one point to its\n        successor being at most 1-xi.\n        Used only when ``cluster_method='xi'``.\n\n    predecessor_correction : bool, default=True\n        Correct clusters according to the predecessors calculated by OPTICS\n        [2]_. This parameter has minimal effect on most datasets.\n        Used only when ``cluster_method='xi'``.\n\n    min_cluster_size : int > 1 or float between 0 and 1, default=None\n        Minimum number of samples in an OPTICS cluster, expressed as an\n        absolute number or a fraction of the number of samples (rounded to be\n        at least 2). If ``None``, the value of ``min_samples`` is used instead.\n        Used only when ``cluster_method='xi'``.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`~sklearn.neighbors.BallTree`.\n        - 'kd_tree' will use :class:`~sklearn.neighbors.KDTree`.\n        - 'brute' will use a brute-force search.\n        - 'auto' (default) will attempt to decide the most appropriate\n          algorithm based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to :class:`~sklearn.neighbors.BallTree` or\n        :class:`~sklearn.neighbors.KDTree`. This can affect the speed of the\n        construction and query, as well as the memory required to store the\n        tree. The optimal value depends on the nature of the problem.\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    labels_ : ndarray of shape (n_samples,)\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples and points which are not included in a leaf cluster\n        of ``cluster_hierarchy_`` are labeled as -1.\n\n    reachability_ : ndarray of shape (n_samples,)\n        Reachability distances per sample, indexed by object order. Use\n        ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\n    ordering_ : ndarray of shape (n_samples,)\n        The cluster ordered list of sample indices.\n\n    core_distances_ : ndarray of shape (n_samples,)\n        Distance at which each sample becomes a core point, indexed by object\n        order. Points which will never be core have a distance of inf. Use\n        ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\n    predecessor_ : ndarray of shape (n_samples,)\n        Point that a sample was reached from, indexed by object order.\n        Seed points have a predecessor of -1.\n\n    cluster_hierarchy_ : ndarray of shape (n_clusters, 2)\n        The list of clusters in the form of ``[start, end]`` in each row, with\n        all indices inclusive. The clusters are ordered according to\n        ``(end, -start)`` (ascending) so that larger clusters encompassing\n        smaller clusters come after those smaller ones. Since ``labels_`` does\n        not reflect the hierarchy, usually\n        ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also\n        note that these indices are of the ``ordering_``, i.e.\n        ``X[ordering_][start:end + 1]`` form a cluster.\n        Only available when ``cluster_method='xi'``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DBSCAN : A similar clustering for a specified neighborhood radius (eps).\n        Our implementation is optimized for runtime.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n    .. [2] Schubert, Erich, Michael Gertz.\n       \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n       the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import OPTICS\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> clustering = OPTICS(min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n\n    For a more detailed example see\n    :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`.\n    \"\"\"\n    _parameter_constraints: dict = {'min_samples': [Interval(Integral, 2, None, closed='left'), Interval(RealNotInt, 0, 1, closed='both')], 'max_eps': [Interval(Real, 0, None, closed='both')], 'metric': [StrOptions(set(_VALID_METRICS) | {'precomputed'}), callable], 'p': [Interval(Real, 1, None, closed='left')], 'metric_params': [dict, None], 'cluster_method': [StrOptions({'dbscan', 'xi'})], 'eps': [Interval(Real, 0, None, closed='both'), None], 'xi': [Interval(Real, 0, 1, closed='both')], 'predecessor_correction': ['boolean'], 'min_cluster_size': [Interval(Integral, 2, None, closed='left'), Interval(RealNotInt, 0, 1, closed='right'), None], 'algorithm': [StrOptions({'auto', 'brute', 'ball_tree', 'kd_tree'})], 'leaf_size': [Interval(Integral, 1, None, closed='left')], 'memory': [str, HasMethods('cache'), None], 'n_jobs': [Integral, None]}\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    @_fit_context(prefer_skip_nested_validation=False)\n    def fit(self, X, y=None):\n        \"\"\"Perform OPTICS clustering.\n\n        Extracts an ordered list of points and reachability distances, and\n        performs initial clustering using ``max_eps`` distance specified at\n        OPTICS object instantiation.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features), or                 (n_samples, n_samples) if metric='precomputed'\n            A feature array, or array of distances between samples if\n            metric='precomputed'. If a sparse matrix is provided, it will be\n            converted into CSR format.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns a fitted instance of self.\n        \"\"\"\n        dtype = bool if self.metric in PAIRWISE_BOOLEAN_FUNCTIONS else float\n        if dtype is bool and X.dtype != bool:\n            msg = f'Data will be converted to boolean for metric {self.metric}, to avoid this warning, you may convert the data prior to calling fit.'\n            warnings.warn(msg, DataConversionWarning)\n        X = self._validate_data(X, dtype=dtype, accept_sparse='csr')\n        if self.metric == 'precomputed' and issparse(X):\n            X = X.copy()\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', SparseEfficiencyWarning)\n                X.setdiag(X.diagonal())\n        memory = check_memory(self.memory)\n        (self.ordering_, self.core_distances_, self.reachability_, self.predecessor_) = memory.cache(compute_optics_graph)(X=X, min_samples=self.min_samples, algorithm=self.algorithm, leaf_size=self.leaf_size, metric=self.metric, metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs, max_eps=self.max_eps)\n        if self.cluster_method == 'xi':\n            (labels_, clusters_) = cluster_optics_xi(reachability=self.reachability_, predecessor=self.predecessor_, ordering=self.ordering_, min_samples=self.min_samples, min_cluster_size=self.min_cluster_size, xi=self.xi, predecessor_correction=self.predecessor_correction)\n            self.cluster_hierarchy_ = clusters_\n        elif self.cluster_method == 'dbscan':\n            if self.eps is None:\n                eps = self.max_eps\n            else:\n                eps = self.eps\n            if eps > self.max_eps:\n                raise ValueError('Specify an epsilon smaller than %s. Got %s.' % (self.max_eps, eps))\n            labels_ = cluster_optics_dbscan(reachability=self.reachability_, core_distances=self.core_distances_, ordering=self.ordering_, eps=eps)\n        self.labels_ = labels_\n        return self\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def __init__(self, *, min_samples=5, max_eps=np.inf, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05, predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, memory=None, n_jobs=None):\n        self.max_eps = max_eps\n        self.min_samples = min_samples\n        self.min_cluster_size = min_cluster_size\n        self.algorithm = algorithm\n        self.metric = metric\n        self.metric_params = metric_params\n        self.p = p\n        self.leaf_size = leaf_size\n        self.cluster_method = cluster_method\n        self.eps = eps\n        self.xi = xi\n        self.predecessor_correction = predecessor_correction\n        self.memory = memory\n        self.n_jobs = n_jobs", "class_fn": true, "question_id": "sklearn/sklearn.cluster._optics/OPTICS", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/compose/_column_transformer.py", "fn_id": "", "content": "class _RemainderColsList(UserList):\n    \"\"\"A list that raises a warning whenever items are accessed.\n\n    It is used to store the columns handled by the \"remainder\" entry of\n    ``ColumnTransformer.transformers_``, ie ``transformers_[-1][-1]``.\n\n    For some values of the ``ColumnTransformer`` ``transformers`` parameter,\n    this list of indices will be replaced by either a list of column names or a\n    boolean mask; in those cases we emit a ``FutureWarning`` the first time an\n    element is accessed.\n\n    Parameters\n    ----------\n    columns : list of int\n        The remainder columns.\n\n    future_dtype : {'str', 'bool'}, default=None\n        The dtype that will be used by a ColumnTransformer with the same inputs\n        in a future release. There is a default value because providing a\n        constructor that takes a single argument is a requirement for\n        subclasses of UserList, but we do not use it in practice. It would only\n        be used if a user called methods that return a new list such are\n        copying or concatenating `_RemainderColsList`.\n\n    warning_was_emitted : bool, default=False\n       Whether the warning for that particular list was already shown, so we\n       only emit it once.\n\n    warning_enabled : bool, default=True\n        When False, the list never emits the warning nor updates\n        `warning_was_emitted``. This is used to obtain a quiet copy of the list\n        for use by the `ColumnTransformer` itself, so that the warning is only\n        shown when a user accesses it directly.\n    \"\"\"\n\n    def __init__(self, columns, *, future_dtype=None, warning_was_emitted=False, warning_enabled=True):\n        super().__init__(columns)\n        self.future_dtype = future_dtype\n        self.warning_was_emitted = warning_was_emitted\n        self.warning_enabled = warning_enabled\n\n    def __getitem__(self, index):\n        self._show_remainder_cols_warning()\n        return super().__getitem__(index)\n\n    def _show_remainder_cols_warning(self):\n        if self.warning_was_emitted or not self.warning_enabled:\n            return\n        self.warning_was_emitted = True\n        future_dtype_description = {'str': 'column names (of type str)', 'bool': 'a mask array (of type bool)', None: 'a different type depending on the ColumnTransformer inputs'}.get(self.future_dtype, self.future_dtype)\n        warnings.warn(f\"\\nThe format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\\nAt the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as {future_dtype_description}.\\nTo use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\\n\", category=FutureWarning)\n\n    def _repr_pretty_(self, printer, *_):\n        \"\"\"Override display in ipython console, otherwise the class name is shown.\"\"\"\n        printer.text(repr(self.data))", "class_fn": true, "question_id": "sklearn/sklearn.compose._column_transformer/_RemainderColsList", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/covariance/_elliptic_envelope.py", "fn_id": "", "content": "class EllipticEnvelope(OutlierMixin, MinCovDet):\n    \"\"\"An object for detecting outliers in a Gaussian distributed dataset.\n\n    Read more in the :ref:`User Guide <outlier_detection>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, the support of robust location and covariance estimates\n        is computed, and a covariance estimate is recomputed from it,\n        without centering the data.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, the robust location and covariance are directly computed\n        with the FastMCD algorithm without additional treatment.\n\n    support_fraction : float, default=None\n        The proportion of points to be included in the support of the raw\n        MCD estimate. If None, the minimum value of support_fraction will\n        be used within the algorithm: `(n_samples + n_features + 1) / 2 * n_samples`.\n        Range is (0, 1).\n\n    contamination : float, default=0.1\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set. Range is (0, 0.5].\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the pseudo random number generator for shuffling\n        the data. Pass an int for reproducible results across multiple function\n        calls. See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated robust location.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated robust covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute the\n        robust estimates of location and shape.\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores.\n        We have the relation: ``decision_function = score_samples - offset_``.\n        The offset depends on the contamination parameter and is defined in\n        such a way we obtain the expected number of outliers (samples with\n        decision function < 0) in training.\n\n        .. versionadded:: 0.20\n\n    raw_location_ : ndarray of shape (n_features,)\n        The raw robust estimated location before correction and re-weighting.\n\n    raw_covariance_ : ndarray of shape (n_features, n_features)\n        The raw robust estimated covariance before correction and re-weighting.\n\n    raw_support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the raw robust estimates of location and shape, before correction\n        and re-weighting.\n\n    dist_ : ndarray of shape (n_samples,)\n        Mahalanobis distances of the training set (on which :meth:`fit` is\n        called) observations.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    OAS : Oracle Approximating Shrinkage Estimator.\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Notes\n    -----\n    Outlier detection from covariance estimation may break or not\n    perform well in high-dimensional settings. In particular, one will\n    always take care to work with ``n_samples > n_features ** 2``.\n\n    References\n    ----------\n    .. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the\n       minimum covariance determinant estimator\" Technometrics 41(3), 212\n       (1999)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EllipticEnvelope\n    >>> true_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n    ...                                                  cov=true_cov,\n    ...                                                  size=500)\n    >>> cov = EllipticEnvelope(random_state=0).fit(X)\n    >>> # predict returns 1 for an inlier and -1 for an outlier\n    >>> cov.predict([[0, 0],\n    ...              [3, 3]])\n    array([ 1, -1])\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])\n    \"\"\"\n    _parameter_constraints: dict = {**MinCovDet._parameter_constraints, 'contamination': [Interval(Real, 0, 0.5, closed='right')]}\n\n    def __init__(self, *, store_precision=True, assume_centered=False, support_fraction=None, contamination=0.1, random_state=None):\n        super().__init__(store_precision=store_precision, assume_centered=assume_centered, support_fraction=support_fraction, random_state=random_state)\n        self.contamination = contamination\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the EllipticEnvelope model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        super().fit(X)\n        self.offset_ = np.percentile(-self.dist_, 100.0 * self.contamination)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict labels (1 inlier, -1 outlier) of X according to fitted model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        is_inlier : ndarray of shape (n_samples,)\n            Returns -1 for anomalies/outliers and +1 for inliers.\n        \"\"\"\n        values = self.decision_function(X)\n        is_inlier = np.full(values.shape[0], -1, dtype=int)\n        is_inlier[values >= 0] = 1\n        return is_inlier\n\n    def reweight_covariance(self, data):\n        \"\"\"Re-weight raw Minimum Covariance Determinant estimates.\n\n        Re-weight observations using Rousseeuw's method (equivalent to\n        deleting outlying observations from the data set before\n        computing location and covariance estimates) described\n        in [RVDriessen]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.\n\n        Returns\n        -------\n        location_reweighted : ndarray of shape (n_features,)\n            Re-weighted robust location estimate.\n\n        covariance_reweighted : ndarray of shape (n_features, n_features)\n            Re-weighted robust covariance estimate.\n\n        support_reweighted : ndarray of shape (n_samples,), dtype=bool\n            A mask of the observations that have been used to compute\n            the re-weighted robust location and covariance estimates.\n\n        References\n        ----------\n\n        .. [RVDriessen] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS\n        \"\"\"\n        (n_samples, n_features) = data.shape\n        mask = self.dist_ < chi2(n_features).isf(0.025)\n        if self.assume_centered:\n            location_reweighted = np.zeros(n_features)\n        else:\n            location_reweighted = data[mask].mean(0)\n        covariance_reweighted = self._nonrobust_covariance(data[mask], assume_centered=self.assume_centered)\n        support_reweighted = np.zeros(n_samples, dtype=bool)\n        support_reweighted[mask] = True\n        self._set_covariance(covariance_reweighted)\n        self.location_ = location_reweighted\n        self.support_ = support_reweighted\n        X_centered = data - self.location_\n        self.dist_ = np.sum(np.dot(X_centered, self.get_precision()) * X_centered, 1)\n        return (location_reweighted, covariance_reweighted, support_reweighted)\n\n    def score_samples(self, X):\n        \"\"\"Compute the negative Mahalanobis distances.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        negative_mahal_distances : array-like of shape (n_samples,)\n            Opposite of the Mahalanobis distances.\n        \"\"\"\n        check_is_fitted(self)\n        return -self.mahalanobis(X)\n\n    def correct_covariance(self, data):\n        \"\"\"Apply a correction to raw Minimum Covariance Determinant estimates.\n\n        Correction using the empirical correction factor suggested\n        by Rousseeuw and Van Driessen in [RVD]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.\n\n        Returns\n        -------\n        covariance_corrected : ndarray of shape (n_features, n_features)\n            Corrected robust covariance estimate.\n\n        References\n        ----------\n\n        .. [RVD] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS\n        \"\"\"\n        n_samples = len(self.dist_)\n        n_support = np.sum(self.support_)\n        if n_support < n_samples and np.allclose(self.raw_covariance_, 0):\n            raise ValueError('The covariance matrix of the support data is equal to 0, try to increase support_fraction')\n        correction = np.median(self.dist_) / chi2(data.shape[1]).isf(0.5)\n        covariance_corrected = self.raw_covariance_ * correction\n        self.dist_ /= correction\n        return covariance_corrected\n\n    def fit_predict(self, X, y=None, **kwargs):\n        \"\"\"Perform fit on X and returns labels for X.\n\n        Returns -1 for outliers and 1 for inliers.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **kwargs : dict\n            Arguments to be passed to ``fit``.\n\n            .. versionadded:: 1.4\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            1 for inliers, -1 for outliers.\n        \"\"\"\n        if _routing_enabled():\n            transform_params = self.get_metadata_routing().consumes(method='predict', params=kwargs.keys())\n            if transform_params:\n                warnings.warn(f\"This object ({self.__class__.__name__}) has a `predict` method which consumes metadata, but `fit_predict` does not forward metadata to `predict`. Please implement a custom `fit_predict` method to forward metadata to `predict` as well.Alternatively, you can explicitly do `set_predict_request`and set all values to `False` to disable metadata routed to `predict`, if that's an option.\", UserWarning)\n        return self.fit(X, **kwargs).predict(X)\n\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of the given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        decision : ndarray of shape (n_samples,)\n            Decision function of the samples.\n            It is equal to the shifted Mahalanobis distances.\n            The threshold for being an outlier is 0, which ensures a\n            compatibility with other outlier detection algorithms.\n        \"\"\"\n        check_is_fitted(self)\n        negative_mahal_dist = self.score_samples(X)\n        return negative_mahal_dist - self.offset_\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) w.r.t. y.\n        \"\"\"\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)", "class_fn": true, "question_id": "sklearn/sklearn.covariance._elliptic_envelope/EllipticEnvelope", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/covariance/_graph_lasso.py", "fn_id": "", "content": "class GraphicalLassoCV(BaseGraphicalLasso):\n    \"\"\"Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    .. versionchanged:: v0.20\n        GraphLassoCV has been renamed to GraphicalLassoCV\n\n    Parameters\n    ----------\n    alphas : int or array-like of shape (n_alphas,), dtype=float, default=4\n        If an integer is given, it fixes the number of points on the\n        grids of alpha to be used. If a list is given, it gives the\n        grid to be used. See the notes in the class docstring for\n        more details. Range is [1, inf) for an integer.\n        Range is (0, inf] for an array-like of floats.\n\n    n_refinements : int, default=4\n        The number of times the grid is refined. Not used if explicit\n        values of alphas are passed. Range is [1, inf).\n\n    cv : int, cross-validation generator or iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    tol : float, default=1e-4\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped. Range is (0, inf].\n\n    enet_tol : float, default=1e-4\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'. Range is (0, inf].\n\n    max_iter : int, default=100\n        Maximum number of iterations.\n\n    mode : {'cd', 'lars'}, default='cd'\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where number of features is greater\n        than number of samples. Elsewhere prefer cd which is more numerically\n        stable.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    verbose : bool, default=False\n        If verbose is True, the objective function and duality gap are\n        printed at each iteration.\n\n    eps : float, default=eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Default is `np.finfo(np.float64).eps`.\n\n        .. versionadded:: 1.3\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated precision matrix (inverse covariance).\n\n    costs_ : list of (objective, dual_gap) pairs\n        The list of values of the objective function and the dual gap at\n        each iteration. Returned only if return_costs is True.\n\n        .. versionadded:: 1.3\n\n    alpha_ : float\n        Penalization parameter selected.\n\n    cv_results_ : dict of ndarrays\n        A dict with keys:\n\n        alphas : ndarray of shape (n_alphas,)\n            All penalization parameters explored.\n\n        split(k)_test_score : ndarray of shape (n_alphas,)\n            Log-likelihood score on left-out data across (k)th fold.\n\n            .. versionadded:: 1.0\n\n        mean_test_score : ndarray of shape (n_alphas,)\n            Mean of scores over the folds.\n\n            .. versionadded:: 1.0\n\n        std_test_score : ndarray of shape (n_alphas,)\n            Standard deviation of scores over the folds.\n\n            .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of iterations run for the optimal alpha.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    graphical_lasso : L1-penalized covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n\n    Notes\n    -----\n    The search for the optimal penalization parameter (`alpha`) is done on an\n    iteratively refined grid: first the cross-validated scores on a grid are\n    computed, then a new refined grid is centered around the maximum, and so\n    on.\n\n    One of the challenges which is faced here is that the solvers can\n    fail to converge to a well-conditioned estimate. The corresponding\n    values of `alpha` then come out as missing values, but the optimum may\n    be close to these missing values.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import GraphicalLassoCV\n    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n    ...                      [0.0, 0.4, 0.0, 0.0],\n    ...                      [0.2, 0.0, 0.3, 0.1],\n    ...                      [0.0, 0.0, 0.1, 0.7]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n    ...                                   cov=true_cov,\n    ...                                   size=200)\n    >>> cov = GraphicalLassoCV().fit(X)\n    >>> np.around(cov.covariance_, decimals=3)\n    array([[0.816, 0.051, 0.22 , 0.017],\n           [0.051, 0.364, 0.018, 0.036],\n           [0.22 , 0.018, 0.322, 0.094],\n           [0.017, 0.036, 0.094, 0.69 ]])\n    >>> np.around(cov.location_, decimals=3)\n    array([0.073, 0.04 , 0.038, 0.143])\n    \"\"\"\n    _parameter_constraints: dict = {**BaseGraphicalLasso._parameter_constraints, 'alphas': [Interval(Integral, 0, None, closed='left'), 'array-like'], 'n_refinements': [Interval(Integral, 1, None, closed='left')], 'cv': ['cv_object'], 'n_jobs': [Integral, None]}\n\n    def __init__(self, *, alphas=4, n_refinements=4, cv=None, tol=0.0001, enet_tol=0.0001, max_iter=100, mode='cd', n_jobs=None, verbose=False, eps=np.finfo(np.float64).eps, assume_centered=False):\n        super().__init__(tol=tol, enet_tol=enet_tol, max_iter=max_iter, mode=mode, verbose=verbose, eps=eps, assume_centered=assume_centered)\n        self.alphas = alphas\n        self.n_refinements = n_refinements\n        self.cv = cv\n        self.n_jobs = n_jobs\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None, **params):\n        \"\"\"Fit the GraphicalLasso covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **params : dict, default=None\n            Parameters to be passed to the CV splitter and the\n            cross_val_score function.\n\n            .. versionadded:: 1.5\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        _raise_for_params(params, self, 'fit')\n        X = self._validate_data(X, ensure_min_features=2)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        emp_cov = empirical_covariance(X, assume_centered=self.assume_centered)\n        cv = check_cv(self.cv, y, classifier=False)\n        path = list()\n        n_alphas = self.alphas\n        inner_verbose = max(0, self.verbose - 1)\n        if _is_arraylike_not_scalar(n_alphas):\n            for alpha in self.alphas:\n                check_scalar(alpha, 'alpha', Real, min_val=0, max_val=np.inf, include_boundaries='right')\n            alphas = self.alphas\n            n_refinements = 1\n        else:\n            n_refinements = self.n_refinements\n            alpha_1 = alpha_max(emp_cov)\n            alpha_0 = 0.01 * alpha_1\n            alphas = np.logspace(np.log10(alpha_0), np.log10(alpha_1), n_alphas)[::-1]\n        if _routing_enabled():\n            routed_params = process_routing(self, 'fit', **params)\n        else:\n            routed_params = Bunch(splitter=Bunch(split={}))\n        t0 = time.time()\n        for i in range(n_refinements):\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', ConvergenceWarning)\n                this_path = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(graphical_lasso_path)(X[train], alphas=alphas, X_test=X[test], mode=self.mode, tol=self.tol, enet_tol=self.enet_tol, max_iter=int(0.1 * self.max_iter), verbose=inner_verbose, eps=self.eps) for (train, test) in cv.split(X, y, **routed_params.splitter.split)))\n            (covs, _, scores) = zip(*this_path)\n            covs = zip(*covs)\n            scores = zip(*scores)\n            path.extend(zip(alphas, scores, covs))\n            path = sorted(path, key=operator.itemgetter(0), reverse=True)\n            best_score = -np.inf\n            last_finite_idx = 0\n            for (index, (alpha, scores, _)) in enumerate(path):\n                this_score = np.mean(scores)\n                if this_score >= 0.1 / np.finfo(np.float64).eps:\n                    this_score = np.nan\n                if np.isfinite(this_score):\n                    last_finite_idx = index\n                if this_score >= best_score:\n                    best_score = this_score\n                    best_index = index\n            if best_index == 0:\n                alpha_1 = path[0][0]\n                alpha_0 = path[1][0]\n            elif best_index == last_finite_idx and (not best_index == len(path) - 1):\n                alpha_1 = path[best_index][0]\n                alpha_0 = path[best_index + 1][0]\n            elif best_index == len(path) - 1:\n                alpha_1 = path[best_index][0]\n                alpha_0 = 0.01 * path[best_index][0]\n            else:\n                alpha_1 = path[best_index - 1][0]\n                alpha_0 = path[best_index + 1][0]\n            if not _is_arraylike_not_scalar(n_alphas):\n                alphas = np.logspace(np.log10(alpha_1), np.log10(alpha_0), n_alphas + 2)\n                alphas = alphas[1:-1]\n            if self.verbose and n_refinements > 1:\n                print('[GraphicalLassoCV] Done refinement % 2i out of %i: % 3is' % (i + 1, n_refinements, time.time() - t0))\n        path = list(zip(*path))\n        grid_scores = list(path[1])\n        alphas = list(path[0])\n        alphas.append(0)\n        grid_scores.append(cross_val_score(EmpiricalCovariance(), X, cv=cv, n_jobs=self.n_jobs, verbose=inner_verbose, params=params))\n        grid_scores = np.array(grid_scores)\n        self.cv_results_ = {'alphas': np.array(alphas)}\n        for i in range(grid_scores.shape[1]):\n            self.cv_results_[f'split{i}_test_score'] = grid_scores[:, i]\n        self.cv_results_['mean_test_score'] = np.mean(grid_scores, axis=1)\n        self.cv_results_['std_test_score'] = np.std(grid_scores, axis=1)\n        best_alpha = alphas[best_index]\n        self.alpha_ = best_alpha\n        (self.covariance_, self.precision_, self.costs_, self.n_iter_) = _graphical_lasso(emp_cov, alpha=best_alpha, mode=self.mode, tol=self.tol, enet_tol=self.enet_tol, max_iter=self.max_iter, verbose=inner_verbose, eps=self.eps)\n        return self\n\n    def get_metadata_routing(self):\n        \"\"\"Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.5\n\n        Returns\n        -------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.\n        \"\"\"\n        router = MetadataRouter(owner=self.__class__.__name__).add(splitter=check_cv(self.cv), method_mapping=MethodMapping().add(callee='split', caller='fit'))\n        return router", "class_fn": true, "question_id": "sklearn/sklearn.covariance._graph_lasso/GraphicalLassoCV", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/covariance/_shrunk_covariance.py", "fn_id": "", "content": "class OAS(EmpiricalCovariance):\n    \"\"\"Oracle Approximating Shrinkage Estimator.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data will be centered before computation.\n\n    Attributes\n    ----------\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    shrinkage_ : float\n      coefficient in the convex combination used for the computation\n      of the shrunk estimate. Range is [0, 1].\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with cross-validated\n        choice of the l1 penalty.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features),\n\n    where mu = trace(cov) / n_features and shrinkage is given by the OAS formula\n    (see [1]_).\n\n    The shrinkage formulation implemented here differs from Eq. 23 in [1]_. In\n    the original article, formula (23) states that 2/p (p being the number of\n    features) is multiplied by Trace(cov*cov) in both the numerator and\n    denominator, but this operation is omitted because for a large p, the value\n    of 2/p is so small that it doesn't affect the value of the estimator.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Shrinkage algorithms for MMSE covariance estimation.\",\n           Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.\n           IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\n           <0907.4698>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import OAS\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> oas = OAS().fit(X)\n    >>> oas.covariance_\n    array([[0.7533..., 0.2763...],\n           [0.2763..., 0.3964...]])\n    >>> oas.precision_\n    array([[ 1.7833..., -1.2431... ],\n           [-1.2431...,  3.3889...]])\n    >>> oas.shrinkage_\n    np.float64(0.0195...)\n    \"\"\"\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the Oracle Approximating Shrinkage covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = self._validate_data(X)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        (covariance, shrinkage) = _oas(X - self.location_, assume_centered=True)\n        self.shrinkage_ = shrinkage\n        self._set_covariance(covariance)\n        return self\n\n    def score(self, X_test, y=None):\n        \"\"\"Compute the log-likelihood of `X_test` under the estimated Gaussian model.\n\n        The Gaussian model is defined by its mean and covariance matrix which are\n        represented respectively by `self.location_` and `self.covariance_`.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where `n_samples` is\n            the number of samples and `n_features` is the number of features.\n            `X_test` is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        res : float\n            The log-likelihood of `X_test` with `self.location_` and `self.covariance_`\n            as estimators of the Gaussian model mean and covariance matrix respectively.\n        \"\"\"\n        X_test = self._validate_data(X_test, reset=False)\n        test_cov = empirical_covariance(X_test - self.location_, assume_centered=True)\n        res = log_likelihood(test_cov, self.get_precision())\n        return res\n\n    def get_precision(self):\n        \"\"\"Getter for the precision matrix.\n\n        Returns\n        -------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.\n        \"\"\"\n        if self.store_precision:\n            precision = self.precision_\n        else:\n            precision = linalg.pinvh(self.covariance_, check_finite=False)\n        return precision\n\n    def mahalanobis(self, X):\n        \"\"\"Compute the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.\n\n        Returns\n        -------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.\n        \"\"\"\n        X = self._validate_data(X, reset=False)\n        precision = self.get_precision()\n        with config_context(assume_finite=True):\n            dist = pairwise_distances(X, self.location_[np.newaxis, :], metric='mahalanobis', VI=precision)\n        return np.reshape(dist, (len(X),)) ** 2", "class_fn": true, "question_id": "sklearn/sklearn.covariance._shrunk_covariance/OAS", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cross_decomposition/_pls.py", "fn_id": "", "content": "class PLSCanonical(_PLS):\n    \"\"\"Partial Least Squares transformer and regressor.\n\n    For a comparison between other cross decomposition algorithms, see\n    :ref:`sphx_glr_auto_examples_cross_decomposition_plot_compare_cross_decomposition.py`.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    .. versionadded:: 0.8\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Number of components to keep. Should be in `[1, min(n_samples,\n        n_features, n_targets)]`.\n\n    scale : bool, default=True\n        Whether to scale `X` and `Y`.\n\n    algorithm : {'nipals', 'svd'}, default='nipals'\n        The algorithm used to estimate the first singular vectors of the\n        cross-covariance matrix. 'nipals' uses the power method while 'svd'\n        will compute the whole SVD.\n\n    max_iter : int, default=500\n        The maximum number of iterations of the power method when\n        `algorithm='nipals'`. Ignored otherwise.\n\n    tol : float, default=1e-06\n        The tolerance used as convergence criteria in the power method: the\n        algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n        than `tol`, where `u` corresponds to the left singular vector.\n\n    copy : bool, default=True\n        Whether to copy `X` and `Y` in fit before applying centering, and\n        potentially scaling. If False, these operations will be done inplace,\n        modifying both arrays.\n\n    Attributes\n    ----------\n    x_weights_ : ndarray of shape (n_features, n_components)\n        The left singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    y_weights_ : ndarray of shape (n_targets, n_components)\n        The right singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    x_loadings_ : ndarray of shape (n_features, n_components)\n        The loadings of `X`.\n\n    y_loadings_ : ndarray of shape (n_targets, n_components)\n        The loadings of `Y`.\n\n    x_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `X`.\n\n    y_rotations_ : ndarray of shape (n_targets, n_components)\n        The projection matrix used to transform `Y`.\n\n    coef_ : ndarray of shape (n_targets, n_features)\n        The coefficients of the linear model such that `Y` is approximated as\n        `Y = X @ coef_.T + intercept_`.\n\n    intercept_ : ndarray of shape (n_targets,)\n        The intercepts of the linear model such that `Y` is approximated as\n        `Y = X @ coef_.T + intercept_`.\n\n        .. versionadded:: 1.1\n\n    n_iter_ : list of shape (n_components,)\n        Number of iterations of the power method, for each\n        component. Empty if `algorithm='svd'`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    CCA : Canonical Correlation Analysis.\n    PLSSVD : Partial Least Square SVD.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import PLSCanonical\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    >>> y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> plsca = PLSCanonical(n_components=2)\n    >>> plsca.fit(X, y)\n    PLSCanonical()\n    >>> X_c, y_c = plsca.transform(X, y)\n    \"\"\"\n    _parameter_constraints: dict = {**_PLS._parameter_constraints}\n    for param in ('deflation_mode', 'mode'):\n        _parameter_constraints.pop(param)\n\n    def _more_tags(self):\n        return {'poor_score': True, 'requires_y': False}\n\n    def predict(self, X, copy=True):\n        \"\"\"Predict targets of given samples.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples.\n\n        copy : bool, default=True\n            Whether to copy `X` and `Y`, or perform in-place normalization.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n\n        Notes\n        -----\n        This call requires the estimation of a matrix of shape\n        `(n_features, n_targets)`, which may be an issue in high dimensional\n        space.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, copy=copy, dtype=FLOAT_DTYPES, reset=False)\n        X -= self._x_mean\n        Ypred = X @ self.coef_.T + self.intercept_\n        return Ypred.ravel() if self._predict_1d else Ypred\n\n    def inverse_transform(self, X, y=None, Y=None):\n        \"\"\"Transform data back to its original space.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            New data, where `n_samples` is the number of samples\n            and `n_components` is the number of pls components.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_components)\n            New target, where `n_samples` is the number of samples\n            and `n_components` is the number of pls components.\n\n        Y : array-like of shape (n_samples, n_components)\n            New target, where `n_samples` is the number of samples\n            and `n_components` is the number of pls components.\n\n            .. deprecated:: 1.5\n               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.\n\n        Returns\n        -------\n        X_reconstructed : ndarray of shape (n_samples, n_features)\n            Return the reconstructed `X` data.\n\n        y_reconstructed : ndarray of shape (n_samples, n_targets)\n            Return the reconstructed `X` target. Only returned when `y` is given.\n\n        Notes\n        -----\n        This transformation will only be exact if `n_components=n_features`.\n        \"\"\"\n        y = _deprecate_Y_when_optional(y, Y)\n        check_is_fitted(self)\n        X = check_array(X, input_name='X', dtype=FLOAT_DTYPES)\n        X_reconstructed = np.matmul(X, self.x_loadings_.T)\n        X_reconstructed *= self._x_std\n        X_reconstructed += self._x_mean\n        if y is not None:\n            y = check_array(y, input_name='y', dtype=FLOAT_DTYPES)\n            y_reconstructed = np.matmul(y, self.y_loadings_.T)\n            y_reconstructed *= self._y_std\n            y_reconstructed += self._y_mean\n            return (X_reconstructed, y_reconstructed)\n        return X_reconstructed\n\n    def __init__(self, n_components=2, *, scale=True, algorithm='nipals', max_iter=500, tol=1e-06, copy=True):\n        super().__init__(n_components=n_components, scale=scale, deflation_mode='canonical', mode='A', algorithm=algorithm, max_iter=max_iter, tol=tol, copy=copy)", "class_fn": true, "question_id": "sklearn/sklearn.cross_decomposition._pls/PLSCanonical", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/cross_decomposition/_pls.py", "fn_id": "", "content": "class _PLS(ClassNamePrefixFeaturesOutMixin, TransformerMixin, RegressorMixin, MultiOutputMixin, BaseEstimator, metaclass=ABCMeta):\n    \"\"\"Partial Least Squares (PLS)\n\n    This class implements the generic PLS algorithm.\n\n    Main ref: Wegelin, a survey of Partial Least Squares (PLS) methods,\n    with emphasis on the two-block case\n    https://stat.uw.edu/sites/default/files/files/reports/2000/tr371.pdf\n    \"\"\"\n    _parameter_constraints: dict = {'n_components': [Interval(Integral, 1, None, closed='left')], 'scale': ['boolean'], 'deflation_mode': [StrOptions({'regression', 'canonical'})], 'mode': [StrOptions({'A', 'B'})], 'algorithm': [StrOptions({'svd', 'nipals'})], 'max_iter': [Interval(Integral, 1, None, closed='left')], 'tol': [Interval(Real, 0, None, closed='left')], 'copy': ['boolean']}\n\n    @abstractmethod\n    def __init__(self, n_components=2, *, scale=True, deflation_mode='regression', mode='A', algorithm='nipals', max_iter=500, tol=1e-06, copy=True):\n        self.n_components = n_components\n        self.deflation_mode = deflation_mode\n        self.mode = mode\n        self.scale = scale\n        self.algorithm = algorithm\n        self.max_iter = max_iter\n        self.tol = tol\n        self.copy = copy\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None, Y=None):\n        \"\"\"Fit model to data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of predictors.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target vectors, where `n_samples` is the number of samples and\n            `n_targets` is the number of response variables.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target vectors, where `n_samples` is the number of samples and\n            `n_targets` is the number of response variables.\n\n            .. deprecated:: 1.5\n               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.\n\n        Returns\n        -------\n        self : object\n            Fitted model.\n        \"\"\"\n        y = _deprecate_Y_when_required(y, Y)\n        check_consistent_length(X, y)\n        X = self._validate_data(X, dtype=np.float64, force_writeable=True, copy=self.copy, ensure_min_samples=2)\n        y = check_array(y, input_name='y', dtype=np.float64, force_writeable=True, copy=self.copy, ensure_2d=False)\n        if y.ndim == 1:\n            self._predict_1d = True\n            y = y.reshape(-1, 1)\n        else:\n            self._predict_1d = False\n        n = X.shape[0]\n        p = X.shape[1]\n        q = y.shape[1]\n        n_components = self.n_components\n        rank_upper_bound = p if self.deflation_mode == 'regression' else min(n, p, q)\n        if n_components > rank_upper_bound:\n            raise ValueError(f'`n_components` upper bound is {rank_upper_bound}. Got {n_components} instead. Reduce `n_components`.')\n        self._norm_y_weights = self.deflation_mode == 'canonical'\n        norm_y_weights = self._norm_y_weights\n        (Xk, yk, self._x_mean, self._y_mean, self._x_std, self._y_std) = _center_scale_xy(X, y, self.scale)\n        self.x_weights_ = np.zeros((p, n_components))\n        self.y_weights_ = np.zeros((q, n_components))\n        self._x_scores = np.zeros((n, n_components))\n        self._y_scores = np.zeros((n, n_components))\n        self.x_loadings_ = np.zeros((p, n_components))\n        self.y_loadings_ = np.zeros((q, n_components))\n        self.n_iter_ = []\n        y_eps = np.finfo(yk.dtype).eps\n        for k in range(n_components):\n            if self.algorithm == 'nipals':\n                yk_mask = np.all(np.abs(yk) < 10 * y_eps, axis=0)\n                yk[:, yk_mask] = 0.0\n                try:\n                    (x_weights, y_weights, n_iter_) = _get_first_singular_vectors_power_method(Xk, yk, mode=self.mode, max_iter=self.max_iter, tol=self.tol, norm_y_weights=norm_y_weights)\n                except StopIteration as e:\n                    if str(e) != 'y residual is constant':\n                        raise\n                    warnings.warn(f'y residual is constant at iteration {k}')\n                    break\n                self.n_iter_.append(n_iter_)\n            elif self.algorithm == 'svd':\n                (x_weights, y_weights) = _get_first_singular_vectors_svd(Xk, yk)\n            _svd_flip_1d(x_weights, y_weights)\n            x_scores = np.dot(Xk, x_weights)\n            if norm_y_weights:\n                y_ss = 1\n            else:\n                y_ss = np.dot(y_weights, y_weights)\n            y_scores = np.dot(yk, y_weights) / y_ss\n            x_loadings = np.dot(x_scores, Xk) / np.dot(x_scores, x_scores)\n            Xk -= np.outer(x_scores, x_loadings)\n            if self.deflation_mode == 'canonical':\n                y_loadings = np.dot(y_scores, yk) / np.dot(y_scores, y_scores)\n                yk -= np.outer(y_scores, y_loadings)\n            if self.deflation_mode == 'regression':\n                y_loadings = np.dot(x_scores, yk) / np.dot(x_scores, x_scores)\n                yk -= np.outer(x_scores, y_loadings)\n            self.x_weights_[:, k] = x_weights\n            self.y_weights_[:, k] = y_weights\n            self._x_scores[:, k] = x_scores\n            self._y_scores[:, k] = y_scores\n            self.x_loadings_[:, k] = x_loadings\n            self.y_loadings_[:, k] = y_loadings\n        self.x_rotations_ = np.dot(self.x_weights_, pinv2(np.dot(self.x_loadings_.T, self.x_weights_), check_finite=False))\n        self.y_rotations_ = np.dot(self.y_weights_, pinv2(np.dot(self.y_loadings_.T, self.y_weights_), check_finite=False))\n        self.coef_ = np.dot(self.x_rotations_, self.y_loadings_.T)\n        self.coef_ = (self.coef_ * self._y_std).T / self._x_std\n        self.intercept_ = self._y_mean\n        self._n_features_out = self.x_rotations_.shape[1]\n        return self\n\n    def transform(self, X, y=None, Y=None, copy=True):\n        \"\"\"Apply the dimension reduction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to transform.\n\n        y : array-like of shape (n_samples, n_targets), default=None\n            Target vectors.\n\n        Y : array-like of shape (n_samples, n_targets), default=None\n            Target vectors.\n\n            .. deprecated:: 1.5\n               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.\n\n        copy : bool, default=True\n            Whether to copy `X` and `Y`, or perform in-place normalization.\n\n        Returns\n        -------\n        x_scores, y_scores : array-like or tuple of array-like\n            Return `x_scores` if `Y` is not given, `(x_scores, y_scores)` otherwise.\n        \"\"\"\n        y = _deprecate_Y_when_optional(y, Y)\n        check_is_fitted(self)\n        X = self._validate_data(X, copy=copy, dtype=FLOAT_DTYPES, reset=False)\n        X -= self._x_mean\n        X /= self._x_std\n        x_scores = np.dot(X, self.x_rotations_)\n        if y is not None:\n            y = check_array(y, input_name='y', ensure_2d=False, copy=copy, dtype=FLOAT_DTYPES)\n            if y.ndim == 1:\n                y = y.reshape(-1, 1)\n            y -= self._y_mean\n            y /= self._y_std\n            y_scores = np.dot(y, self.y_rotations_)\n            return (x_scores, y_scores)\n        return x_scores\n\n    def inverse_transform(self, X, y=None, Y=None):\n        \"\"\"Transform data back to its original space.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            New data, where `n_samples` is the number of samples\n            and `n_components` is the number of pls components.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_components)\n            New target, where `n_samples` is the number of samples\n            and `n_components` is the number of pls components.\n\n        Y : array-like of shape (n_samples, n_components)\n            New target, where `n_samples` is the number of samples\n            and `n_components` is the number of pls components.\n\n            .. deprecated:: 1.5\n               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.\n\n        Returns\n        -------\n        X_reconstructed : ndarray of shape (n_samples, n_features)\n            Return the reconstructed `X` data.\n\n        y_reconstructed : ndarray of shape (n_samples, n_targets)\n            Return the reconstructed `X` target. Only returned when `y` is given.\n\n        Notes\n        -----\n        This transformation will only be exact if `n_components=n_features`.\n        \"\"\"\n        y = _deprecate_Y_when_optional(y, Y)\n        check_is_fitted(self)\n        X = check_array(X, input_name='X', dtype=FLOAT_DTYPES)\n        X_reconstructed = np.matmul(X, self.x_loadings_.T)\n        X_reconstructed *= self._x_std\n        X_reconstructed += self._x_mean\n        if y is not None:\n            y = check_array(y, input_name='y', dtype=FLOAT_DTYPES)\n            y_reconstructed = np.matmul(y, self.y_loadings_.T)\n            y_reconstructed *= self._y_std\n            y_reconstructed += self._y_mean\n            return (X_reconstructed, y_reconstructed)\n        return X_reconstructed\n\n    def predict(self, X, copy=True):\n        \"\"\"Predict targets of given samples.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples.\n\n        copy : bool, default=True\n            Whether to copy `X` and `Y`, or perform in-place normalization.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n\n        Notes\n        -----\n        This call requires the estimation of a matrix of shape\n        `(n_features, n_targets)`, which may be an issue in high dimensional\n        space.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, copy=copy, dtype=FLOAT_DTYPES, reset=False)\n        X -= self._x_mean\n        Ypred = X @ self.coef_.T + self.intercept_\n        return Ypred.ravel() if self._predict_1d else Ypred\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Learn and apply the dimension reduction on the train data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of predictors.\n\n        y : array-like of shape (n_samples, n_targets), default=None\n            Target vectors, where `n_samples` is the number of samples and\n            `n_targets` is the number of response variables.\n\n        Returns\n        -------\n        self : ndarray of shape (n_samples, n_components)\n            Return `x_scores` if `Y` is not given, `(x_scores, y_scores)` otherwise.\n        \"\"\"\n        return self.fit(X, y).transform(X, y)\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def _more_tags(self):\n        return {'poor_score': True, 'requires_y': False}", "class_fn": true, "question_id": "sklearn/sklearn.cross_decomposition._pls/_PLS", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_dict_learning.py", "fn_id": "", "content": "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\n    \"\"\"Mini-batch dictionary learning.\n\n    Finds a dictionary (a set of atoms) that performs well at sparsely\n    encoding the fitted data.\n\n    Solves the optimization problem::\n\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                    (U,V)\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\n\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\n    the entry-wise matrix norm which is the sum of the absolute values\n    of all the entries in the matrix.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of dictionary elements to extract.\n\n    alpha : float, default=1\n        Sparsity controlling parameter.\n\n    max_iter : int, default=1_000\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n        .. versionadded:: 1.1\n\n        .. deprecated:: 1.4\n           `max_iter=None` is deprecated in 1.4 and will be removed in 1.6.\n           Use the default value (i.e. `1_000`) instead.\n\n    fit_algorithm : {'lars', 'cd'}, default='lars'\n        The algorithm used:\n\n        - `'lars'`: uses the least angle regression method to solve the lasso\n          problem (`linear_model.lars_path`)\n        - `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    batch_size : int, default=256\n        Number of samples in each mini-batch.\n\n        .. versionchanged:: 1.3\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\n\n    shuffle : bool, default=True\n        Whether to shuffle the samples before forming batches.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial value of the dictionary for warm restart scenarios.\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n        Algorithm used to transform the data:\n\n        - `'lars'`: uses the least angle regression method\n          (`linear_model.lars_path`);\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\n          if the estimated components are sparse.\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution.\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\n          the projection ``dictionary * X'``.\n\n    transform_n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and\n        `algorithm='omp'`. If `None`, then\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\n\n    transform_alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `None`, defaults to `alpha`.\n\n        .. versionchanged:: 1.2\n            When None, default value changed from 1.0 to `alpha`.\n\n    verbose : bool or int, default=False\n        To control the verbosity of the procedure.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initializing the dictionary when ``dict_init`` is not\n        specified, randomly shuffling the data when ``shuffle`` is set to\n        ``True``, and updating the dictionary. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `'lasso_lars'`.\n\n        .. versionadded:: 0.22\n\n    callback : callable, default=None\n        A callable that gets invoked at the end of each iteration.\n\n        .. versionadded:: 1.1\n\n    tol : float, default=1e-3\n        Control early stopping based on the norm of the differences in the\n        dictionary between 2 steps.\n\n        To disable early stopping based on changes in the dictionary, set\n        `tol` to 0.0.\n\n        .. versionadded:: 1.1\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to None.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components extracted from the data.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of iterations over the full dataset.\n\n    n_steps_ : int\n        Number of mini-batches processed.\n\n        .. versionadded:: 1.1\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n\n    References\n    ----------\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\n    >>> X, dictionary, code = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42)\n    >>> dict_learner = MiniBatchDictionaryLearning(\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\n    ...     transform_alpha=0.1, max_iter=20, random_state=42)\n    >>> X_transformed = dict_learner.fit_transform(X)\n\n    We can check the level of sparsity of `X_transformed`:\n\n    >>> np.mean(X_transformed == 0) > 0.5\n    np.True_\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = X_transformed @ dict_learner.components_\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.052...)\n    \"\"\"\n    _parameter_constraints: dict = {'n_components': [Interval(Integral, 1, None, closed='left'), None], 'alpha': [Interval(Real, 0, None, closed='left')], 'max_iter': [Interval(Integral, 0, None, closed='left'), Hidden(None)], 'fit_algorithm': [StrOptions({'cd', 'lars'})], 'n_jobs': [None, Integral], 'batch_size': [Interval(Integral, 1, None, closed='left')], 'shuffle': ['boolean'], 'dict_init': [None, np.ndarray], 'transform_algorithm': [StrOptions({'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'})], 'transform_n_nonzero_coefs': [Interval(Integral, 1, None, closed='left'), None], 'transform_alpha': [Interval(Real, 0, None, closed='left'), None], 'verbose': ['verbose'], 'split_sign': ['boolean'], 'random_state': ['random_state'], 'positive_code': ['boolean'], 'positive_dict': ['boolean'], 'transform_max_iter': [Interval(Integral, 0, None, closed='left')], 'callback': [None, callable], 'tol': [Interval(Real, 0, None, closed='left')], 'max_no_improvement': [Interval(Integral, 0, None, closed='left'), None]}\n\n    def _check_feature_names(self, X, *, reset):\n        \"\"\"Set or check the `feature_names_in_` attribute.\n\n        .. versionadded:: 1.0\n\n        Parameters\n        ----------\n        X : {ndarray, dataframe} of shape (n_samples, n_features)\n            The input samples.\n\n        reset : bool\n            Whether to reset the `feature_names_in_` attribute.\n            If False, the input will be checked for consistency with\n            feature names of data provided when reset was last True.\n            .. note::\n               It is recommended to call `reset=True` in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n        \"\"\"\n        if reset:\n            feature_names_in = _get_feature_names(X)\n            if feature_names_in is not None:\n                self.feature_names_in_ = feature_names_in\n            elif hasattr(self, 'feature_names_in_'):\n                delattr(self, 'feature_names_in_')\n            return\n        fitted_feature_names = getattr(self, 'feature_names_in_', None)\n        X_feature_names = _get_feature_names(X)\n        if fitted_feature_names is None and X_feature_names is None:\n            return\n        if X_feature_names is not None and fitted_feature_names is None:\n            warnings.warn(f'X has feature names, but {self.__class__.__name__} was fitted without feature names')\n            return\n        if X_feature_names is None and fitted_feature_names is not None:\n            warnings.warn(f'X does not have valid feature names, but {self.__class__.__name__} was fitted with feature names')\n            return\n        if len(fitted_feature_names) != len(X_feature_names) or np.any(fitted_feature_names != X_feature_names):\n            message = 'The feature names should match those that were passed during fit.\\n'\n            fitted_feature_names_set = set(fitted_feature_names)\n            X_feature_names_set = set(X_feature_names)\n            unexpected_names = sorted(X_feature_names_set - fitted_feature_names_set)\n            missing_names = sorted(fitted_feature_names_set - X_feature_names_set)\n\n            def add_names(names):\n                output = ''\n                max_n_names = 5\n                for (i, name) in enumerate(names):\n                    if i >= max_n_names:\n                        output += '- ...\\n'\n                        break\n                    output += f'- {name}\\n'\n                return output\n            if unexpected_names:\n                message += 'Feature names unseen at fit time:\\n'\n                message += add_names(unexpected_names)\n            if missing_names:\n                message += 'Feature names seen at fit time, yet now missing:\\n'\n                message += add_names(missing_names)\n            if not missing_names and (not unexpected_names):\n                message += 'Feature names must be in the same order as they were in fit.\\n'\n            raise ValueError(message)\n\n    def _more_tags(self):\n        return {'preserves_dtype': [np.float64, np.float32]}\n\n    def __init__(self, n_components=None, *, alpha=1, max_iter=1000, fit_algorithm='lars', n_jobs=None, batch_size=256, shuffle=True, dict_init=None, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, verbose=False, split_sign=False, random_state=None, positive_code=False, positive_dict=False, transform_max_iter=1000, callback=None, tol=0.001, max_no_improvement=10):\n        super().__init__(transform_algorithm, transform_n_nonzero_coefs, transform_alpha, split_sign, n_jobs, positive_code, transform_max_iter)\n        self.n_components = n_components\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.fit_algorithm = fit_algorithm\n        self.dict_init = dict_init\n        self.verbose = verbose\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.split_sign = split_sign\n        self.random_state = random_state\n        self.positive_dict = positive_dict\n        self.callback = callback\n        self.max_no_improvement = max_no_improvement\n        self.tol = tol\n\n    def _check_params(self, X):\n        self._n_components = self.n_components\n        if self._n_components is None:\n            self._n_components = X.shape[1]\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\n        self._fit_algorithm = 'lasso_' + self.fit_algorithm\n        self._batch_size = min(self.batch_size, X.shape[0])\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def _check_convergence(self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps):\n        \"\"\"Helper function to encapsulate the early stopping logic.\n\n        Early stopping is based on two factors:\n        - A small change of the dictionary between two minibatch updates. This is\n          controlled by the tol parameter.\n        - No more improvement on a smoothed estimate of the objective function for a\n          a certain number of consecutive minibatch updates. This is controlled by\n          the max_no_improvement parameter.\n        \"\"\"\n        batch_size = X.shape[0]\n        step = step + 1\n        if step <= min(100, n_samples / batch_size):\n            if self.verbose:\n                print(f'Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}')\n            return False\n        if self._ewa_cost is None:\n            self._ewa_cost = batch_cost\n        else:\n            alpha = batch_size / (n_samples + 1)\n            alpha = min(alpha, 1)\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\n        if self.verbose:\n            print(f'Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}, ewa cost: {self._ewa_cost}')\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\n        if self.tol > 0 and dict_diff <= self.tol:\n            if self.verbose:\n                print(f'Converged (small dictionary change) at step {step}/{n_steps}')\n            return True\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\n            self._no_improvement = 0\n            self._ewa_cost_min = self._ewa_cost\n        else:\n            self._no_improvement += 1\n        if self.max_no_improvement is not None and self._no_improvement >= self.max_no_improvement:\n            if self.verbose:\n                print(f'Converged (lack of improvement in objective function) at step {step}/{n_steps}')\n            return True\n        return False\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = self._validate_data(X, dtype=[np.float64, np.float32], order='C', copy=False)\n        self._check_params(X)\n        self._random_state = check_random_state(self.random_state)\n        dictionary = self._initialize_dict(X, self._random_state)\n        old_dict = dictionary.copy()\n        if self.shuffle:\n            X_train = X.copy()\n            self._random_state.shuffle(X_train)\n        else:\n            X_train = X\n        (n_samples, n_features) = X_train.shape\n        if self.verbose:\n            print('[dict_learning]')\n        self._A = np.zeros((self._n_components, self._n_components), dtype=X_train.dtype)\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\n        if self.max_iter is None:\n            warn('`max_iter=None` is deprecated in version 1.4 and will be removed in version 1.6. Use the default value (i.e. `1_000`) instead.', FutureWarning)\n            max_iter = 1000\n        else:\n            max_iter = self.max_iter\n        self._ewa_cost = None\n        self._ewa_cost_min = None\n        self._no_improvement = 0\n        batches = gen_batches(n_samples, self._batch_size)\n        batches = itertools.cycle(batches)\n        n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\n        n_steps = max_iter * n_steps_per_iter\n        i = -1\n        for (i, batch) in zip(range(n_steps), batches):\n            X_batch = X_train[batch]\n            batch_cost = self._minibatch_step(X_batch, dictionary, self._random_state, i)\n            if self._check_convergence(X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps):\n                break\n            if self.callback is not None:\n                self.callback(locals())\n            old_dict[:] = dictionary\n        self.n_steps_ = i + 1\n        self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\n        self.components_ = dictionary\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def partial_fit(self, X, y=None):\n        \"\"\"Update the model using the data in X as a mini-batch.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Return the instance itself.\n        \"\"\"\n        has_components = hasattr(self, 'components_')\n        X = self._validate_data(X, dtype=[np.float64, np.float32], order='C', reset=not has_components)\n        if not has_components:\n            self._check_params(X)\n            self._random_state = check_random_state(self.random_state)\n            dictionary = self._initialize_dict(X, self._random_state)\n            self.n_steps_ = 0\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\n        else:\n            dictionary = self.components_\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\n        self.components_ = dictionary\n        self.n_steps_ += 1\n        return self\n\n    @property\n    def _n_features_out(self):\n        \"\"\"Number of transformed output features.\"\"\"\n        return self.components_.shape[0]\n\n    def _minibatch_step(self, X, dictionary, random_state, step):\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\n        batch_size = X.shape[0]\n        code = _sparse_encode(X, dictionary, algorithm=self._fit_algorithm, alpha=self.alpha, n_jobs=self.n_jobs, positive=self.positive_code, max_iter=self.transform_max_iter, verbose=self.verbose)\n        batch_cost = (0.5 * ((X - code @ dictionary) ** 2).sum() + self.alpha * np.sum(np.abs(code))) / batch_size\n        self._update_inner_stats(X, code, batch_size, step)\n        _update_dict(dictionary, X, code, self._A, self._B, verbose=self.verbose, random_state=random_state, positive=self.positive_dict)\n        return batch_cost\n\n    def _initialize_dict(self, X, random_state):\n        \"\"\"Initialization of the dictionary.\"\"\"\n        if self.dict_init is not None:\n            dictionary = self.dict_init\n        else:\n            (_, S, dictionary) = randomized_svd(X, self._n_components, random_state=random_state)\n            dictionary = S[:, np.newaxis] * dictionary\n        if self._n_components <= len(dictionary):\n            dictionary = dictionary[:self._n_components, :]\n        else:\n            dictionary = np.concatenate((dictionary, np.zeros((self._n_components - len(dictionary), dictionary.shape[1]), dtype=dictionary.dtype)))\n        dictionary = check_array(dictionary, order='F', dtype=X.dtype, copy=False)\n        dictionary = np.require(dictionary, requirements='W')\n        return dictionary\n\n    def _update_inner_stats(self, X, code, batch_size, step):\n        \"\"\"Update the inner stats inplace.\"\"\"\n        if step < batch_size - 1:\n            theta = (step + 1) * batch_size\n        else:\n            theta = batch_size ** 2 + step + 1 - batch_size\n        beta = (theta + 1 - batch_size) / (theta + 1)\n        self._A *= beta\n        self._A += code.T @ code / batch_size\n        self._B *= beta\n        self._B += X.T @ code / batch_size\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_factor_analysis.py", "fn_id": "", "content": "class FactorAnalysis(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Factor Analysis (FA).\n\n    A simple linear generative model with Gaussian latent variables.\n\n    The observations are assumed to be caused by a linear transformation of\n    lower dimensional latent factors and added Gaussian noise.\n    Without loss of generality the factors are distributed according to a\n    Gaussian with zero mean and unit covariance. The noise is also zero mean\n    and has an arbitrary diagonal covariance matrix.\n\n    If we would restrict the model further, by assuming that the Gaussian\n    noise is even isotropic (all diagonal entries are the same) we would obtain\n    :class:`PCA`.\n\n    FactorAnalysis performs a maximum likelihood estimate of the so-called\n    `loading` matrix, the transformation of the latent variables to the\n    observed ones, using SVD based approach.\n\n    Read more in the :ref:`User Guide <FA>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Dimensionality of latent space, the number of components\n        of ``X`` that are obtained after ``transform``.\n        If None, n_components is set to the number of features.\n\n    tol : float, default=1e-2\n        Stopping tolerance for log-likelihood increase.\n\n    copy : bool, default=True\n        Whether to make a copy of X. If ``False``, the input X gets overwritten\n        during fitting.\n\n    max_iter : int, default=1000\n        Maximum number of iterations.\n\n    noise_variance_init : array-like of shape (n_features,), default=None\n        The initial guess of the noise variance for each feature.\n        If None, it defaults to np.ones(n_features).\n\n    svd_method : {'lapack', 'randomized'}, default='randomized'\n        Which SVD method to use. If 'lapack' use standard SVD from\n        scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n        Defaults to 'randomized'. For most applications 'randomized' will\n        be sufficiently precise while providing significant speed gains.\n        Accuracy can also be improved by setting higher values for\n        `iterated_power`. If this is not sufficient, for maximum precision\n        you should choose 'lapack'.\n\n    iterated_power : int, default=3\n        Number of iterations for the power method. 3 by default. Only used\n        if ``svd_method`` equals 'randomized'.\n\n    rotation : {'varimax', 'quartimax'}, default=None\n        If not None, apply the indicated rotation. Currently, varimax and\n        quartimax are implemented. See\n        `\"The varimax criterion for analytic rotation in factor analysis\"\n        <https://link.springer.com/article/10.1007%2FBF02289233>`_\n        H. F. Kaiser, 1958.\n\n        .. versionadded:: 0.24\n\n    random_state : int or RandomState instance, default=0\n        Only used when ``svd_method`` equals 'randomized'. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components with maximum variance.\n\n    loglike_ : list of shape (n_iterations,)\n        The log likelihood at each iteration.\n\n    noise_variance_ : ndarray of shape (n_features,)\n        The estimated noise variance for each feature.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    PCA: Principal component analysis is also a latent linear variable model\n        which however assumes equal noise variance for each feature.\n        This extra assumption makes probabilistic PCA faster as it can be\n        computed in closed form.\n    FastICA: Independent component analysis, a latent variable model with\n        non-Gaussian latent variables.\n\n    References\n    ----------\n    - David Barber, Bayesian Reasoning and Machine Learning,\n      Algorithm 21.1.\n\n    - Christopher M. Bishop: Pattern Recognition and Machine Learning,\n      Chapter 12.2.4.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FactorAnalysis\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FactorAnalysis(n_components=7, random_state=0)\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n    \"\"\"\n    _parameter_constraints: dict = {'n_components': [Interval(Integral, 0, None, closed='left'), None], 'tol': [Interval(Real, 0.0, None, closed='left')], 'copy': ['boolean'], 'max_iter': [Interval(Integral, 1, None, closed='left')], 'noise_variance_init': ['array-like', None], 'svd_method': [StrOptions({'randomized', 'lapack'})], 'iterated_power': [Interval(Integral, 0, None, closed='left')], 'rotation': [StrOptions({'varimax', 'quartimax'}), None], 'random_state': ['random_state']}\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the FactorAnalysis model to X using SVD based approach.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Ignored parameter.\n\n        Returns\n        -------\n        self : object\n            FactorAnalysis class instance.\n        \"\"\"\n        X = self._validate_data(X, copy=self.copy, dtype=np.float64, force_writeable=True)\n        (n_samples, n_features) = X.shape\n        n_components = self.n_components\n        if n_components is None:\n            n_components = n_features\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n        nsqrt = sqrt(n_samples)\n        llconst = n_features * log(2.0 * np.pi) + n_components\n        var = np.var(X, axis=0)\n        if self.noise_variance_init is None:\n            psi = np.ones(n_features, dtype=X.dtype)\n        else:\n            if len(self.noise_variance_init) != n_features:\n                raise ValueError('noise_variance_init dimension does not with number of features : %d != %d' % (len(self.noise_variance_init), n_features))\n            psi = np.array(self.noise_variance_init)\n        loglike = []\n        old_ll = -np.inf\n        SMALL = 1e-12\n        if self.svd_method == 'lapack':\n\n            def my_svd(X):\n                (_, s, Vt) = linalg.svd(X, full_matrices=False, check_finite=False)\n                return (s[:n_components], Vt[:n_components], squared_norm(s[n_components:]))\n        else:\n            random_state = check_random_state(self.random_state)\n\n            def my_svd(X):\n                (_, s, Vt) = randomized_svd(X, n_components, random_state=random_state, n_iter=self.iterated_power)\n                return (s, Vt, squared_norm(X) - squared_norm(s))\n        for i in range(self.max_iter):\n            sqrt_psi = np.sqrt(psi) + SMALL\n            (s, Vt, unexp_var) = my_svd(X / (sqrt_psi * nsqrt))\n            s **= 2\n            W = np.sqrt(np.maximum(s - 1.0, 0.0))[:, np.newaxis] * Vt\n            del Vt\n            W *= sqrt_psi\n            ll = llconst + np.sum(np.log(s))\n            ll += unexp_var + np.sum(np.log(psi))\n            ll *= -n_samples / 2.0\n            loglike.append(ll)\n            if ll - old_ll < self.tol:\n                break\n            old_ll = ll\n            psi = np.maximum(var - np.sum(W ** 2, axis=0), SMALL)\n        else:\n            warnings.warn('FactorAnalysis did not converge.' + ' You might want' + ' to increase the number of iterations.', ConvergenceWarning)\n        self.components_ = W\n        if self.rotation is not None:\n            self.components_ = self._rotate(W)\n        self.noise_variance_ = psi\n        self.loglike_ = loglike\n        self.n_iter_ = i + 1\n        return self\n\n    def transform(self, X):\n        \"\"\"Apply dimensionality reduction to X using the model.\n\n        Compute the expected mean of the latent variables.\n        See Barber, 21.2.33 (or Bishop, 12.66).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            The latent variables of X.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        Ih = np.eye(len(self.components_))\n        X_transformed = X - self.mean_\n        Wpsi = self.components_ / self.noise_variance_\n        cov_z = linalg.inv(Ih + np.dot(Wpsi, self.components_.T))\n        tmp = np.dot(X_transformed, Wpsi.T)\n        X_transformed = np.dot(tmp, cov_z)\n        return X_transformed\n\n    def _rotate(self, components, n_components=None, tol=1e-06):\n        \"\"\"Rotate the factor analysis solution.\"\"\"\n        return _ortho_rotation(components.T, method=self.rotation, tol=tol)[:self.n_components]\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def get_precision(self):\n        \"\"\"Compute data precision matrix with the FactorAnalysis model.\n\n        Returns\n        -------\n        precision : ndarray of shape (n_features, n_features)\n            Estimated precision of data.\n        \"\"\"\n        check_is_fitted(self)\n        n_features = self.components_.shape[1]\n        if self.n_components == 0:\n            return np.diag(1.0 / self.noise_variance_)\n        if self.n_components == n_features:\n            return linalg.inv(self.get_covariance())\n        components_ = self.components_\n        precision = np.dot(components_ / self.noise_variance_, components_.T)\n        precision.flat[::len(precision) + 1] += 1.0\n        precision = np.dot(components_.T, np.dot(linalg.inv(precision), components_))\n        precision /= self.noise_variance_[:, np.newaxis]\n        precision /= -self.noise_variance_[np.newaxis, :]\n        precision.flat[::len(precision) + 1] += 1.0 / self.noise_variance_\n        return precision\n\n    def get_covariance(self):\n        \"\"\"Compute data covariance with the FactorAnalysis model.\n\n        ``cov = components_.T * components_ + diag(noise_variance)``\n\n        Returns\n        -------\n        cov : ndarray of shape (n_features, n_features)\n            Estimated covariance of data.\n        \"\"\"\n        check_is_fitted(self)\n        cov = np.dot(self.components_.T, self.components_)\n        cov.flat[::len(cov) + 1] += self.noise_variance_\n        return cov\n\n    @property\n    def _n_features_out(self):\n        \"\"\"Number of transformed output features.\"\"\"\n        return self.components_.shape[0]\n\n    def score(self, X, y=None):\n        \"\"\"Compute the average log-likelihood of the samples.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data.\n\n        y : Ignored\n            Ignored parameter.\n\n        Returns\n        -------\n        ll : float\n            Average log-likelihood of the samples under the current model.\n        \"\"\"\n        return np.mean(self.score_samples(X))\n\n    def __init__(self, n_components=None, *, tol=0.01, copy=True, max_iter=1000, noise_variance_init=None, svd_method='randomized', iterated_power=3, rotation=None, random_state=0):\n        self.n_components = n_components\n        self.copy = copy\n        self.tol = tol\n        self.max_iter = max_iter\n        self.svd_method = svd_method\n        self.noise_variance_init = noise_variance_init\n        self.iterated_power = iterated_power\n        self.random_state = random_state\n        self.rotation = rotation\n\n    def score_samples(self, X):\n        \"\"\"Compute the log-likelihood of each sample.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        ll : ndarray of shape (n_samples,)\n            Log-likelihood of each sample under the current model.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        Xr = X - self.mean_\n        precision = self.get_precision()\n        n_features = X.shape[1]\n        log_like = -0.5 * (Xr * np.dot(Xr, precision)).sum(axis=1)\n        log_like -= 0.5 * (n_features * log(2.0 * np.pi) - fast_logdet(precision))\n        return log_like", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._factor_analysis/FactorAnalysis", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_kernel_pca.py", "fn_id": "", "content": "class KernelPCA(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Kernel Principal component analysis (KPCA).\n\n    Non-linear dimensionality reduction through the use of kernels [1]_, see also\n    :ref:`metrics`.\n\n    It uses the :func:`scipy.linalg.eigh` LAPACK implementation of the full SVD\n    or the :func:`scipy.sparse.linalg.eigsh` ARPACK implementation of the\n    truncated SVD, depending on the shape of the input data and the number of\n    components to extract. It can also use a randomized truncated SVD by the\n    method proposed in [3]_, see `eigen_solver`.\n\n    For a usage example and comparison between\n    Principal Components Analysis (PCA) and its kernelized version (KPCA), see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_kernel_pca.py`.\n\n    For a usage example in denoising images using KPCA, see\n    :ref:`sphx_glr_auto_examples_applications_plot_digits_denoising.py`.\n\n    Read more in the :ref:`User Guide <kernel_PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components. If None, all non-zero components are kept.\n\n    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed'}             or callable, default='linear'\n        Kernel used for PCA.\n\n    gamma : float, default=None\n        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n        kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.\n\n    degree : float, default=3\n        Degree for poly kernels. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Independent term in poly and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : dict, default=None\n        Parameters (keyword arguments) and\n        values for kernel passed as callable object.\n        Ignored by other kernels.\n\n    alpha : float, default=1.0\n        Hyperparameter of the ridge regression that learns the\n        inverse transform (when fit_inverse_transform=True).\n\n    fit_inverse_transform : bool, default=False\n        Learn the inverse transform for non-precomputed kernels\n        (i.e. learn to find the pre-image of a point). This method is based\n        on [2]_.\n\n    eigen_solver : {'auto', 'dense', 'arpack', 'randomized'},             default='auto'\n        Select eigensolver to use. If `n_components` is much\n        less than the number of training samples, randomized (or arpack to a\n        smaller extent) may be more efficient than the dense eigensolver.\n        Randomized SVD is performed according to the method of Halko et al\n        [3]_.\n\n        auto :\n            the solver is selected by a default policy based on n_samples\n            (the number of training samples) and `n_components`:\n            if the number of components to extract is less than 10 (strict) and\n            the number of samples is more than 200 (strict), the 'arpack'\n            method is enabled. Otherwise the exact full eigenvalue\n            decomposition is computed and optionally truncated afterwards\n            ('dense' method).\n        dense :\n            run exact full eigenvalue decomposition calling the standard\n            LAPACK solver via `scipy.linalg.eigh`, and select the components\n            by postprocessing\n        arpack :\n            run SVD truncated to n_components calling ARPACK solver using\n            `scipy.sparse.linalg.eigsh`. It requires strictly\n            0 < n_components < n_samples\n        randomized :\n            run randomized SVD by the method of Halko et al. [3]_. The current\n            implementation selects eigenvalues based on their module; therefore\n            using this method can lead to unexpected results if the kernel is\n            not positive semi-definite. See also [4]_.\n\n        .. versionchanged:: 1.0\n           `'randomized'` was added.\n\n    tol : float, default=0\n        Convergence tolerance for arpack.\n        If 0, optimal value will be chosen by arpack.\n\n    max_iter : int, default=None\n        Maximum number of iterations for arpack.\n        If None, optimal value will be chosen by arpack.\n\n    iterated_power : int >= 0, or 'auto', default='auto'\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'. When 'auto', it is set to 7 when\n        `n_components < 0.1 * min(X.shape)`, other it is set to 4.\n\n        .. versionadded:: 1.0\n\n    remove_zero_eig : bool, default=False\n        If True, then all components with zero eigenvalues are removed, so\n        that the number of components in the output may be < n_components\n        (and sometimes even zero due to numerical instability).\n        When n_components is None, this parameter is ignored and components\n        with zero eigenvalues are removed regardless.\n\n    random_state : int, RandomState instance or None, default=None\n        Used when ``eigen_solver`` == 'arpack' or 'randomized'. Pass an int\n        for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.18\n\n    copy_X : bool, default=True\n        If True, input X is copied and stored by the model in the `X_fit_`\n        attribute. If no further changes will be done to X, setting\n        `copy_X=False` saves memory by storing a reference.\n\n        .. versionadded:: 0.18\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    Attributes\n    ----------\n    eigenvalues_ : ndarray of shape (n_components,)\n        Eigenvalues of the centered kernel matrix in decreasing order.\n        If `n_components` and `remove_zero_eig` are not set,\n        then all values are stored.\n\n    eigenvectors_ : ndarray of shape (n_samples, n_components)\n        Eigenvectors of the centered kernel matrix. If `n_components` and\n        `remove_zero_eig` are not set, then all components are stored.\n\n    dual_coef_ : ndarray of shape (n_samples, n_features)\n        Inverse transform matrix. Only available when\n        ``fit_inverse_transform`` is True.\n\n    X_transformed_fit_ : ndarray of shape (n_samples, n_components)\n        Projection of the fitted data on the kernel principal components.\n        Only available when ``fit_inverse_transform`` is True.\n\n    X_fit_ : ndarray of shape (n_samples, n_features)\n        The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n        a reference. This attribute is used for the calls to transform.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    gamma_ : float\n        Kernel coefficient for rbf, poly and sigmoid kernels. When `gamma`\n        is explicitly provided, this is just the same as `gamma`. When `gamma`\n        is `None`, this is the actual value of kernel coefficient.\n\n        .. versionadded:: 1.3\n\n    See Also\n    --------\n    FastICA : A fast algorithm for Independent Component Analysis.\n    IncrementalPCA : Incremental Principal Component Analysis.\n    NMF : Non-Negative Matrix Factorization.\n    PCA : Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] `Sch\u00f6lkopf, Bernhard, Alexander Smola, and Klaus-Robert M\u00fcller.\n       \"Kernel principal component analysis.\"\n       International conference on artificial neural networks.\n       Springer, Berlin, Heidelberg, 1997.\n       <https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf>`_\n\n    .. [2] `Bak\u0131r, G\u00f6khan H., Jason Weston, and Bernhard Sch\u00f6lkopf.\n       \"Learning to find pre-images.\"\n       Advances in neural information processing systems 16 (2004): 449-456.\n       <https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_\n\n    .. [3] :arxiv:`Halko, Nathan, Per-Gunnar Martinsson, and Joel A. Tropp.\n       \"Finding structure with randomness: Probabilistic algorithms for\n       constructing approximate matrix decompositions.\"\n       SIAM review 53.2 (2011): 217-288. <0909.4061>`\n\n    .. [4] `Martinsson, Per-Gunnar, Vladimir Rokhlin, and Mark Tygert.\n       \"A randomized algorithm for the decomposition of matrices.\"\n       Applied and Computational Harmonic Analysis 30.1 (2011): 47-68.\n       <https://www.sciencedirect.com/science/article/pii/S1063520310000242>`_\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import KernelPCA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = KernelPCA(n_components=7, kernel='linear')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n    \"\"\"\n    _parameter_constraints: dict = {'n_components': [Interval(Integral, 1, None, closed='left'), None], 'kernel': [StrOptions({'linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed'}), callable], 'gamma': [Interval(Real, 0, None, closed='left'), None], 'degree': [Interval(Real, 0, None, closed='left')], 'coef0': [Interval(Real, None, None, closed='neither')], 'kernel_params': [dict, None], 'alpha': [Interval(Real, 0, None, closed='left')], 'fit_inverse_transform': ['boolean'], 'eigen_solver': [StrOptions({'auto', 'dense', 'arpack', 'randomized'})], 'tol': [Interval(Real, 0, None, closed='left')], 'max_iter': [Interval(Integral, 1, None, closed='left'), None], 'iterated_power': [Interval(Integral, 0, None, closed='left'), StrOptions({'auto'})], 'remove_zero_eig': ['boolean'], 'random_state': ['random_state'], 'copy_X': ['boolean'], 'n_jobs': [None, Integral]}\n\n    def _get_kernel(self, X, Y=None):\n        if callable(self.kernel):\n            params = self.kernel_params or {}\n        else:\n            params = {'gamma': self.gamma_, 'degree': self.degree, 'coef0': self.coef0}\n        return pairwise_kernels(X, Y, metric=self.kernel, filter_params=True, n_jobs=self.n_jobs, **params)\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def _more_tags(self):\n        return {'preserves_dtype': [np.float64, np.float32], 'pairwise': self.kernel == 'precomputed'}\n\n    def __init__(self, n_components=None, *, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None, alpha=1.0, fit_inverse_transform=False, eigen_solver='auto', tol=0, max_iter=None, iterated_power='auto', remove_zero_eig=False, random_state=None, copy_X=True, n_jobs=None):\n        self.n_components = n_components\n        self.kernel = kernel\n        self.kernel_params = kernel_params\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.alpha = alpha\n        self.fit_inverse_transform = fit_inverse_transform\n        self.eigen_solver = eigen_solver\n        self.tol = tol\n        self.max_iter = max_iter\n        self.iterated_power = iterated_power\n        self.remove_zero_eig = remove_zero_eig\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n        self.copy_X = copy_X\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        if self.fit_inverse_transform and self.kernel == 'precomputed':\n            raise ValueError('Cannot fit_inverse_transform with a precomputed kernel.')\n        X = self._validate_data(X, accept_sparse='csr', copy=self.copy_X)\n        self.gamma_ = 1 / X.shape[1] if self.gamma is None else self.gamma\n        self._centerer = KernelCenterer().set_output(transform='default')\n        K = self._get_kernel(X)\n        self._fit_transform(K)\n        if self.fit_inverse_transform:\n            X_transformed = self.eigenvectors_ * np.sqrt(self.eigenvalues_)\n            self._fit_inverse_transform(X_transformed, X)\n        self.X_fit_ = X\n        return self\n\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **params : kwargs\n            Parameters (keyword arguments) and values passed to\n            the fit_transform instance.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Returns the instance itself.\n        \"\"\"\n        self.fit(X, **params)\n        X_transformed = self.eigenvectors_ * np.sqrt(self.eigenvalues_)\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n        return X_transformed\n\n    def transform(self, X):\n        \"\"\"Transform X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Returns the instance itself.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse='csr', reset=False)\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        non_zeros = np.flatnonzero(self.eigenvalues_)\n        scaled_alphas = np.zeros_like(self.eigenvectors_)\n        scaled_alphas[:, non_zeros] = self.eigenvectors_[:, non_zeros] / np.sqrt(self.eigenvalues_[non_zeros])\n        return np.dot(K, scaled_alphas)\n\n    def inverse_transform(self, X):\n        \"\"\"Transform X back to original space.\n\n        ``inverse_transform`` approximates the inverse transformation using\n        a learned pre-image. The pre-image is learned by kernel ridge\n        regression of the original data on their low-dimensional representation\n        vectors.\n\n        .. note:\n            :meth:`~sklearn.decomposition.fit` internally uses a centered\n            kernel. As the centered kernel no longer contains the information\n            of the mean of kernel features, such information is not taken into\n            account in reconstruction.\n\n        .. note::\n            When users want to compute inverse transformation for 'linear'\n            kernel, it is recommended that they use\n            :class:`~sklearn.decomposition.PCA` instead. Unlike\n            :class:`~sklearn.decomposition.PCA`,\n            :class:`~sklearn.decomposition.KernelPCA`'s ``inverse_transform``\n            does not reconstruct the mean of data when 'linear' kernel is used\n            due to the use of centered kernel.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_components)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_features)\n            Returns the instance itself.\n\n        References\n        ----------\n        `Bak\u0131r, G\u00f6khan H., Jason Weston, and Bernhard Sch\u00f6lkopf.\n        \"Learning to find pre-images.\"\n        Advances in neural information processing systems 16 (2004): 449-456.\n        <https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_\n        \"\"\"\n        if not self.fit_inverse_transform:\n            raise NotFittedError('The fit_inverse_transform parameter was not set to True when instantiating and hence the inverse transform is not available.')\n        K = self._get_kernel(X, self.X_transformed_fit_)\n        return np.dot(K, self.dual_coef_)\n\n    def _fit_transform(self, K):\n        \"\"\"Fit's using kernel K\"\"\"\n        K = self._centerer.fit_transform(K)\n        if self.n_components is None:\n            n_components = K.shape[0]\n        else:\n            n_components = min(K.shape[0], self.n_components)\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n        if eigen_solver == 'dense':\n            (self.eigenvalues_, self.eigenvectors_) = eigh(K, subset_by_index=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            v0 = _init_arpack_v0(K.shape[0], self.random_state)\n            (self.eigenvalues_, self.eigenvectors_) = eigsh(K, n_components, which='LA', tol=self.tol, maxiter=self.max_iter, v0=v0)\n        elif eigen_solver == 'randomized':\n            (self.eigenvalues_, self.eigenvectors_) = _randomized_eigsh(K, n_components=n_components, n_iter=self.iterated_power, random_state=self.random_state, selection='module')\n        self.eigenvalues_ = _check_psd_eigenvalues(self.eigenvalues_, enable_warnings=False)\n        (self.eigenvectors_, _) = svd_flip(u=self.eigenvectors_, v=None)\n        indices = self.eigenvalues_.argsort()[::-1]\n        self.eigenvalues_ = self.eigenvalues_[indices]\n        self.eigenvectors_ = self.eigenvectors_[:, indices]\n        if self.remove_zero_eig or self.n_components is None:\n            self.eigenvectors_ = self.eigenvectors_[:, self.eigenvalues_ > 0]\n            self.eigenvalues_ = self.eigenvalues_[self.eigenvalues_ > 0]\n        return K\n\n    @property\n    def _n_features_out(self):\n        \"\"\"Number of transformed output features.\"\"\"\n        return self.eigenvalues_.shape[0]\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def _fit_inverse_transform(self, X_transformed, X):\n        if hasattr(X, 'tocsr'):\n            raise NotImplementedError('Inverse transform not implemented for sparse matrices!')\n        n_samples = X_transformed.shape[0]\n        K = self._get_kernel(X_transformed)\n        K.flat[::n_samples + 1] += self.alpha\n        self.dual_coef_ = linalg.solve(K, X, assume_a='pos', overwrite_a=True)\n        self.X_transformed_fit_ = X_transformed", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._kernel_pca/KernelPCA", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_nmf.py", "fn_id": "", "content": "class NMF(_BaseNMF):\n    \"\"\"Non-Negative Matrix Factorization (NMF).\n\n    Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)\n    whose product approximates the non-negative matrix X. This factorization can be used\n    for example for dimensionality reduction, source separation or topic extraction.\n\n    The objective function is:\n\n        .. math::\n\n            L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n            &+ alpha\\\\_W * l1\\\\_ratio * n\\\\_features * ||vec(W)||_1\n\n            &+ alpha\\\\_H * l1\\\\_ratio * n\\\\_samples * ||vec(H)||_1\n\n            &+ 0.5 * alpha\\\\_W * (1 - l1\\\\_ratio) * n\\\\_features * ||W||_{Fro}^2\n\n            &+ 0.5 * alpha\\\\_H * (1 - l1\\\\_ratio) * n\\\\_samples * ||H||_{Fro}^2\n\n    Where:\n\n    :math:`||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n    :math:`||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\n    The generic norm :math:`||X - WH||_{loss}` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for\n    `H` to keep their impact balanced with respect to one another and to the data fit\n    term as independent as possible of the size `n_samples` of the training set.\n\n    The objective function is minimized with an alternating minimization of W\n    and H.\n\n    Note that the transformed data is named W and the components matrix is named H. In\n    the NMF literature, the naming convention is usually the opposite since the data\n    matrix X is transposed.\n\n    Read more in the :ref:`User Guide <NMF>`.\n\n    Parameters\n    ----------\n    n_components : int or {'auto'} or None, default=None\n        Number of components, if n_components is not set all features\n        are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from W or H shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Valid options:\n\n        - `None`: 'nndsvda' if n_components <= min(n_samples, n_features),\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness)\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired)\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired)\n\n        - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n        .. versionchanged:: 1.1\n            When `init=None` and n_components is less than n_samples and n_features\n            defaults to `nndsvda` instead of `nndsvd`.\n\n    solver : {'cd', 'mu'}, default='cd'\n        Numerical solver to use:\n\n        - 'cd' is a Coordinate Descent solver.\n        - 'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n        .. versionadded:: 1.0\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n        .. versionadded:: 1.0\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n        .. versionadded:: 0.17\n           Regularization parameter *l1_ratio* used in the Coordinate Descent\n           solver.\n\n    verbose : int, default=0\n        Whether to be verbose.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n        .. versionadded:: 0.17\n           *shuffle* parameter used in the Coordinate Descent solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data ``X`` and the reconstructed data ``WH`` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of iterations.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    PCA : Principal component analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n    \"\"\"\n    _parameter_constraints: dict = {**_BaseNMF._parameter_constraints, 'solver': [StrOptions({'mu', 'cd'})], 'shuffle': ['boolean']}\n\n    def _fit_transform(self, X, y=None, W=None, H=None, update_H=True):\n        \"\"\"Learn a NMF model for the data X and returns the transformed data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data matrix to be decomposed\n\n        y : Ignored\n\n        W : array-like of shape (n_samples, n_components), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `update_H=False`, it is initialised as an array of zeros, unless\n            `solver='mu'`, then it is filled with values calculated by\n            `np.sqrt(X.mean() / self._n_components)`.\n            If `None`, uses the initialisation method specified in `init`.\n\n        H : array-like of shape (n_components, n_features), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `update_H=False`, it is used as a constant, to solve for W only.\n            If `None`, uses the initialisation method specified in `init`.\n\n        update_H : bool, default=True\n            If True, both W and H will be estimated from initial guesses,\n            this corresponds to a call to the 'fit_transform' method.\n            If False, only W will be estimated, this corresponds to a call\n            to the 'transform' method.\n\n        Returns\n        -------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.\n\n        H : ndarray of shape (n_components, n_features)\n            Factorization matrix, sometimes called 'dictionary'.\n\n        n_iter_ : int\n            Actual number of iterations.\n        \"\"\"\n        check_non_negative(X, 'NMF (input X)')\n        self._check_params(X)\n        if X.min() == 0 and self._beta_loss <= 0:\n            raise ValueError('When beta_loss <= 0 and X contains zeros, the solver may diverge. Please add small values to X, or use a positive beta_loss.')\n        (W, H) = self._check_w_h(X, W, H, update_H)\n        (l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H) = self._compute_regularization(X)\n        if self.solver == 'cd':\n            (W, H, n_iter) = _fit_coordinate_descent(X, W, H, self.tol, self.max_iter, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H=update_H, verbose=self.verbose, shuffle=self.shuffle, random_state=self.random_state)\n        elif self.solver == 'mu':\n            (W, H, n_iter, *_) = _fit_multiplicative_update(X, W, H, self._beta_loss, self.max_iter, self.tol, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, self.verbose)\n        else:\n            raise ValueError(\"Invalid solver parameter '%s'.\" % self.solver)\n        if n_iter == self.max_iter and self.tol > 0:\n            warnings.warn('Maximum number of iterations %d reached. Increase it to improve convergence.' % self.max_iter, ConvergenceWarning)\n        return (W, H, n_iter)\n\n    def _check_params(self, X):\n        super()._check_params(X)\n        if self.solver != 'mu' and self.beta_loss not in (2, 'frobenius'):\n            raise ValueError(f'Invalid beta_loss parameter: solver {self.solver!r} does not handle beta_loss = {self.beta_loss!r}')\n        if self.solver == 'mu' and self.init == 'nndsvd':\n            warnings.warn(\"The multiplicative update ('mu') solver cannot update zeros present in the initialization, and so leads to poorer results when used jointly with init='nndsvd'. You may try init='nndsvda' or init='nndsvdar' instead.\", UserWarning)\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit_transform(self, X, y=None, W=None, H=None):\n        \"\"\"Learn a NMF model for the data X and returns the transformed data.\n\n        This is more efficient than calling fit followed by transform.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        W : array-like of shape (n_samples, n_components), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `None`, uses the initialisation method specified in `init`.\n\n        H : array-like of shape (n_components, n_features), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `None`, uses the initialisation method specified in `init`.\n\n        Returns\n        -------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=('csr', 'csc'), dtype=[np.float64, np.float32])\n        with config_context(assume_finite=True):\n            (W, H, n_iter) = self._fit_transform(X, W=W, H=H)\n        self.reconstruction_err_ = _beta_divergence(X, W, H, self._beta_loss, square_root=True)\n        self.n_components_ = H.shape[0]\n        self.components_ = H\n        self.n_iter_ = n_iter\n        return W\n\n    def __init__(self, n_components='warn', *, init=None, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=200, random_state=None, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, verbose=0, shuffle=False):\n        super().__init__(n_components=n_components, init=init, beta_loss=beta_loss, tol=tol, max_iter=max_iter, random_state=random_state, alpha_W=alpha_W, alpha_H=alpha_H, l1_ratio=l1_ratio, verbose=verbose)\n        self.solver = solver\n        self.shuffle = shuffle\n\n    def transform(self, X):\n        \"\"\"Transform the data X according to the fitted NMF model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        Returns\n        -------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=('csr', 'csc'), dtype=[np.float64, np.float32], reset=False)\n        with config_context(assume_finite=True):\n            (W, *_) = self._fit_transform(X, H=self.components_, update_H=False)\n        return W\n\n    def inverse_transform(self, X=None, *, Xt=None):\n        \"\"\"Transform data back to its original space.\n\n        .. versionadded:: 0.18\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_components)\n            Transformed data matrix.\n\n        Xt : {ndarray, sparse matrix} of shape (n_samples, n_components)\n            Transformed data matrix.\n\n            .. deprecated:: 1.5\n                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.\n\n        Returns\n        -------\n        X : ndarray of shape (n_samples, n_features)\n            Returns a data matrix of the original shape.\n        \"\"\"\n        X = _deprecate_Xt_in_inverse_transform(X, Xt)\n        check_is_fitted(self)\n        return X @ self.components_\n\n    def _more_tags(self):\n        return {'requires_positive_X': True, 'preserves_dtype': [np.float64, np.float32]}\n\n    def _check_w_h(self, X, W, H, update_H):\n        \"\"\"Check W and H, or initialize them.\"\"\"\n        (n_samples, n_features) = X.shape\n        if self.init == 'custom' and update_H:\n            _check_init(H, (self._n_components, n_features), 'NMF (input H)')\n            _check_init(W, (n_samples, self._n_components), 'NMF (input W)')\n            if self._n_components == 'auto':\n                self._n_components = H.shape[0]\n            if H.dtype != X.dtype or W.dtype != X.dtype:\n                raise TypeError('H and W should have the same dtype as X. Got H.dtype = {} and W.dtype = {}.'.format(H.dtype, W.dtype))\n        elif not update_H:\n            if W is not None:\n                warnings.warn('When update_H=False, the provided initial W is not used.', RuntimeWarning)\n            _check_init(H, (self._n_components, n_features), 'NMF (input H)')\n            if self._n_components == 'auto':\n                self._n_components = H.shape[0]\n            if H.dtype != X.dtype:\n                raise TypeError('H should have the same dtype as X. Got H.dtype = {}.'.format(H.dtype))\n            if self.solver == 'mu':\n                avg = np.sqrt(X.mean() / self._n_components)\n                W = np.full((n_samples, self._n_components), avg, dtype=X.dtype)\n            else:\n                W = np.zeros((n_samples, self._n_components), dtype=X.dtype)\n        else:\n            if W is not None or H is not None:\n                warnings.warn(\"When init!='custom', provided W or H are ignored. Set  init='custom' to use them as initialization.\", RuntimeWarning)\n            if self._n_components == 'auto':\n                self._n_components = X.shape[1]\n            (W, H) = _initialize_nmf(X, self._n_components, init=self.init, random_state=self.random_state)\n        return (W, H)", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._nmf/NMF", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_sparse_pca.py", "fn_id": "", "content": "class MiniBatchSparsePCA(_BaseSparsePCA):\n    \"\"\"Mini-batch Sparse Principal Components Analysis.\n\n    Finds the set of sparse components that can optimally reconstruct\n    the data.  The amount of sparseness is controllable by the coefficient\n    of the L1 penalty, given by the parameter alpha.\n\n    For an example comparing sparse PCA to PCA, see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`\n\n    Read more in the :ref:`User Guide <SparsePCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of sparse atoms to extract. If None, then ``n_components``\n        is set to ``n_features``.\n\n    alpha : int, default=1\n        Sparsity controlling parameter. Higher values lead to sparser\n        components.\n\n    ridge_alpha : float, default=0.01\n        Amount of ridge shrinkage to apply in order to improve\n        conditioning when calling the transform method.\n\n    max_iter : int, default=1_000\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n        .. versionadded:: 1.2\n\n        .. deprecated:: 1.4\n           `max_iter=None` is deprecated in 1.4 and will be removed in 1.6.\n           Use the default value (i.e. `100`) instead.\n\n    callback : callable, default=None\n        Callable that gets invoked every five iterations.\n\n    batch_size : int, default=3\n        The number of features to take in each mini batch.\n\n    verbose : int or bool, default=False\n        Controls the verbosity; the higher, the more messages. Defaults to 0.\n\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting it in batches.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    method : {'lars', 'cd'}, default='lars'\n        Method to be used for optimization.\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for random shuffling when ``shuffle`` is set to ``True``,\n        during online dictionary learning. Pass an int for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=1e-3\n        Control early stopping based on the norm of the differences in the\n        dictionary between 2 steps.\n\n        To disable early stopping based on changes in the dictionary, set\n        `tol` to 0.0.\n\n        .. versionadded:: 1.1\n\n    max_no_improvement : int or None, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to `None`.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Sparse components extracted from the data.\n\n    n_components_ : int\n        Estimated number of components.\n\n        .. versionadded:: 0.23\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n        Equal to ``X.mean(axis=0)``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    IncrementalPCA : Incremental principal components analysis.\n    PCA : Principal component analysis.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.decomposition import MiniBatchSparsePCA\n    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n    >>> transformer = MiniBatchSparsePCA(n_components=5, batch_size=50,\n    ...                                  max_iter=10, random_state=0)\n    >>> transformer.fit(X)\n    MiniBatchSparsePCA(...)\n    >>> X_transformed = transformer.transform(X)\n    >>> X_transformed.shape\n    (200, 5)\n    >>> # most values in the components_ are zero (sparsity)\n    >>> np.mean(transformer.components_ == 0)\n    np.float64(0.9...)\n    \"\"\"\n    _parameter_constraints: dict = {**_BaseSparsePCA._parameter_constraints, 'max_iter': [Interval(Integral, 0, None, closed='left'), Hidden(None)], 'callback': [None, callable], 'batch_size': [Interval(Integral, 1, None, closed='left')], 'shuffle': ['boolean'], 'max_no_improvement': [Interval(Integral, 0, None, closed='left'), None]}\n\n    def inverse_transform(self, X):\n        \"\"\"Transform data from the latent space to the original space.\n\n        This inversion is an approximation due to the loss of information\n        induced by the forward decomposition.\n\n        .. versionadded:: 1.2\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_components)\n            Data in the latent space.\n\n        Returns\n        -------\n        X_original : ndarray of shape (n_samples, n_features)\n            Reconstructed data in the original space.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X)\n        return X @ self.components_ + self.mean_\n\n    def _more_tags(self):\n        return {'preserves_dtype': [np.float64, np.float32]}\n\n    def __init__(self, n_components=None, *, alpha=1, ridge_alpha=0.01, max_iter=1000, callback=None, batch_size=3, verbose=False, shuffle=True, n_jobs=None, method='lars', random_state=None, tol=0.001, max_no_improvement=10):\n        super().__init__(n_components=n_components, alpha=alpha, ridge_alpha=ridge_alpha, max_iter=max_iter, tol=tol, method=method, n_jobs=n_jobs, verbose=verbose, random_state=random_state)\n        self.callback = callback\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.max_no_improvement = max_no_improvement\n\n    def _fit(self, X, n_components, random_state):\n        \"\"\"Specialized `fit` for MiniBatchSparsePCA.\"\"\"\n        transform_algorithm = 'lasso_' + self.method\n        est = MiniBatchDictionaryLearning(n_components=n_components, alpha=self.alpha, max_iter=self.max_iter, dict_init=None, batch_size=self.batch_size, shuffle=self.shuffle, n_jobs=self.n_jobs, fit_algorithm=self.method, random_state=random_state, transform_algorithm=transform_algorithm, transform_alpha=self.alpha, verbose=self.verbose, callback=self.callback, tol=self.tol, max_no_improvement=self.max_no_improvement)\n        est.set_output(transform='default')\n        est.fit(X.T)\n        (self.components_, self.n_iter_) = (est.transform(X.T).T, est.n_iter_)\n        components_norm = np.linalg.norm(self.components_, axis=1)[:, np.newaxis]\n        components_norm[components_norm == 0] = 1\n        self.components_ /= components_norm\n        self.n_components_ = len(self.components_)\n        return self", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/decomposition/_truncated_svd.py", "fn_id": "", "content": "class TruncatedSVD(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Dimensionality reduction using truncated SVD (aka LSA).\n\n    This transformer performs linear dimensionality reduction by means of\n    truncated singular value decomposition (SVD). Contrary to PCA, this\n    estimator does not center the data before computing the singular value\n    decomposition. This means it can work with sparse matrices\n    efficiently.\n\n    In particular, truncated SVD works on term count/tf-idf matrices as\n    returned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In\n    that context, it is known as latent semantic analysis (LSA).\n\n    This estimator supports two algorithms: a fast randomized SVD solver, and\n    a \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or\n    `X.T * X`, whichever is more efficient.\n\n    Read more in the :ref:`User Guide <LSA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Desired dimensionality of output data.\n        If algorithm='arpack', must be strictly less than the number of features.\n        If algorithm='randomized', must be less than or equal to the number of features.\n        The default value is useful for visualisation. For LSA, a value of\n        100 is recommended.\n\n    algorithm : {'arpack', 'randomized'}, default='randomized'\n        SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n        algorithm due to Halko (2009).\n\n    n_iter : int, default=5\n        Number of iterations for randomized SVD solver. Not used by ARPACK. The\n        default is larger than the default in\n        :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\n        matrices that may have large slowly decaying spectrum.\n\n    n_oversamples : int, default=10\n        Number of oversamples for randomized SVD solver. Not used by ARPACK.\n        See :func:`~sklearn.utils.extmath.randomized_svd` for a complete\n        description.\n\n        .. versionadded:: 1.1\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Power iteration normalizer for randomized SVD solver.\n        Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n        for more details.\n\n        .. versionadded:: 1.1\n\n    random_state : int, RandomState instance or None, default=None\n        Used during randomized svd. Pass an int for reproducible results across\n        multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n        SVD solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The right singular vectors of the input data.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        The variance of the training samples transformed by a projection to\n        each component.\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    FactorAnalysis : A simple linear generative model with\n        Gaussian latent variables.\n    IncrementalPCA : Incremental principal components analysis.\n    KernelPCA : Kernel Principal component analysis.\n    NMF : Non-Negative Matrix Factorization.\n    PCA : Principal component analysis.\n\n    Notes\n    -----\n    SVD suffers from a problem called \"sign indeterminacy\", which means the\n    sign of the ``components_`` and the output from transform depend on the\n    algorithm and random state. To work around this, fit instances of this\n    class to data once, then keep the instance around to do transformations.\n\n    References\n    ----------\n    :arxiv:`Halko, et al. (2009). \"Finding structure with randomness:\n    Stochastic algorithms for constructing approximate matrix decompositions\"\n    <0909.4061>`\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import TruncatedSVD\n    >>> from scipy.sparse import csr_matrix\n    >>> import numpy as np\n    >>> np.random.seed(0)\n    >>> X_dense = np.random.rand(100, 100)\n    >>> X_dense[:, 2 * np.arange(50)] = 0\n    >>> X = csr_matrix(X_dense)\n    >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> svd.fit(X)\n    TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> print(svd.explained_variance_ratio_)\n    [0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]\n    >>> print(svd.explained_variance_ratio_.sum())\n    0.2102...\n    >>> print(svd.singular_values_)\n    [35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]\n    \"\"\"\n    _parameter_constraints: dict = {'n_components': [Interval(Integral, 1, None, closed='left')], 'algorithm': [StrOptions({'arpack', 'randomized'})], 'n_iter': [Interval(Integral, 0, None, closed='left')], 'n_oversamples': [Interval(Integral, 1, None, closed='left')], 'power_iteration_normalizer': [StrOptions({'auto', 'OR', 'LU', 'none'})], 'random_state': ['random_state'], 'tol': [Interval(Real, 0, None, closed='left')]}\n\n    def __init__(self, n_components=2, *, algorithm='randomized', n_iter=5, n_oversamples=10, power_iteration_normalizer='auto', random_state=None, tol=0.0):\n        self.algorithm = algorithm\n        self.n_components = n_components\n        self.n_iter = n_iter\n        self.n_oversamples = n_oversamples\n        self.power_iteration_normalizer = power_iteration_normalizer\n        self.random_state = random_state\n        self.tol = tol\n\n    def _more_tags(self):\n        return {'preserves_dtype': [np.float64, np.float32]}\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit_transform(self, X, y=None):\n        \"\"\"Fit model to X and perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=['csr', 'csc'], ensure_min_features=2)\n        random_state = check_random_state(self.random_state)\n        if self.algorithm == 'arpack':\n            v0 = _init_arpack_v0(min(X.shape), random_state)\n            (U, Sigma, VT) = svds(X, k=self.n_components, tol=self.tol, v0=v0)\n            Sigma = Sigma[::-1]\n            (U, VT) = svd_flip(U[:, ::-1], VT[::-1], u_based_decision=False)\n        elif self.algorithm == 'randomized':\n            if self.n_components > X.shape[1]:\n                raise ValueError(f'n_components({self.n_components}) must be <= n_features({X.shape[1]}).')\n            (U, Sigma, VT) = randomized_svd(X, self.n_components, n_iter=self.n_iter, n_oversamples=self.n_oversamples, power_iteration_normalizer=self.power_iteration_normalizer, random_state=random_state, flip_sign=False)\n            (U, VT) = svd_flip(U, VT, u_based_decision=False)\n        self.components_ = VT\n        if self.algorithm == 'randomized' or (self.algorithm == 'arpack' and self.tol > 0):\n            X_transformed = safe_sparse_dot(X, self.components_.T)\n        else:\n            X_transformed = U * Sigma\n        self.explained_variance_ = exp_var = np.var(X_transformed, axis=0)\n        if sp.issparse(X):\n            (_, full_var) = mean_variance_axis(X, axis=0)\n            full_var = full_var.sum()\n        else:\n            full_var = np.var(X, axis=0).sum()\n        self.explained_variance_ratio_ = exp_var / full_var\n        self.singular_values_ = Sigma\n        return X_transformed\n\n    def transform(self, X):\n        \"\"\"Perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=['csr', 'csc'], reset=False)\n        return safe_sparse_dot(X, self.components_.T)\n\n    def fit(self, X, y=None):\n        \"\"\"Fit model on training data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the transformer object.\n        \"\"\"\n        self.fit_transform(X)\n        return self\n\n    def inverse_transform(self, X):\n        \"\"\"Transform X back to its original space.\n\n        Returns an array X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            New data.\n\n        Returns\n        -------\n        X_original : ndarray of shape (n_samples, n_features)\n            Note that this is always a dense array.\n        \"\"\"\n        X = check_array(X)\n        return np.dot(X, self.components_)\n\n    @property\n    def _n_features_out(self):\n        \"\"\"Number of transformed output features.\"\"\"\n        return self.components_.shape[0]\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out", "class_fn": true, "question_id": "sklearn/sklearn.decomposition._truncated_svd/TruncatedSVD", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/dummy.py", "fn_id": "", "content": "class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):\n    \"\"\"DummyClassifier makes predictions that ignore the input features.\n\n    This classifier serves as a simple baseline to compare against other more\n    complex classifiers.\n\n    The specific behavior of the baseline is selected with the `strategy`\n    parameter.\n\n    All strategies make predictions that ignore the input feature values passed\n    as the `X` argument to `fit` and `predict`. The predictions, however,\n    typically depend on values observed in the `y` parameter passed to `fit`.\n\n    Note that the \"stratified\" and \"uniform\" strategies lead to\n    non-deterministic predictions that can be rendered deterministic by setting\n    the `random_state` parameter if needed. The other strategies are naturally\n    deterministic and, once fit, always return the same constant prediction\n    for any value of `X`.\n\n    Read more in the :ref:`User Guide <dummy_estimators>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    strategy : {\"most_frequent\", \"prior\", \"stratified\", \"uniform\",             \"constant\"}, default=\"prior\"\n        Strategy to use to generate predictions.\n\n        * \"most_frequent\": the `predict` method always returns the most\n          frequent class label in the observed `y` argument passed to `fit`.\n          The `predict_proba` method returns the matching one-hot encoded\n          vector.\n        * \"prior\": the `predict` method always returns the most frequent\n          class label in the observed `y` argument passed to `fit` (like\n          \"most_frequent\"). ``predict_proba`` always returns the empirical\n          class distribution of `y` also known as the empirical class prior\n          distribution.\n        * \"stratified\": the `predict_proba` method randomly samples one-hot\n          vectors from a multinomial distribution parametrized by the empirical\n          class prior probabilities.\n          The `predict` method returns the class label which got probability\n          one in the one-hot vector of `predict_proba`.\n          Each sampled row of both methods is therefore independent and\n          identically distributed.\n        * \"uniform\": generates predictions uniformly at random from the list\n          of unique classes observed in `y`, i.e. each class has equal\n          probability.\n        * \"constant\": always predicts a constant label that is provided by\n          the user. This is useful for metrics that evaluate a non-majority\n          class.\n\n          .. versionchanged:: 0.24\n             The default value of `strategy` has changed to \"prior\" in version\n             0.24.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the randomness to generate the predictions when\n        ``strategy='stratified'`` or ``strategy='uniform'``.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    constant : int or str or array-like of shape (n_outputs,), default=None\n        The explicit constant as predicted by the \"constant\" strategy. This\n        parameter is useful only for the \"constant\" strategy.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,) or list of such arrays\n        Unique class labels observed in `y`. For multi-output classification\n        problems, this attribute is a list of arrays as each output has an\n        independent set of possible classes.\n\n    n_classes_ : int or list of int\n        Number of label for each output.\n\n    class_prior_ : ndarray of shape (n_classes,) or list of such arrays\n        Frequency of each class observed in `y`. For multioutput classification\n        problems, this is computed independently for each output.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X` has\n        feature names that are all strings.\n\n    n_outputs_ : int\n        Number of outputs.\n\n    sparse_output_ : bool\n        True if the array returned from predict is to be in sparse CSC format.\n        Is automatically set to True if the input `y` is passed in sparse\n        format.\n\n    See Also\n    --------\n    DummyRegressor : Regressor that makes predictions using simple rules.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.dummy import DummyClassifier\n    >>> X = np.array([-1, 1, 1, 1])\n    >>> y = np.array([0, 1, 1, 1])\n    >>> dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n    >>> dummy_clf.fit(X, y)\n    DummyClassifier(strategy='most_frequent')\n    >>> dummy_clf.predict(X)\n    array([1, 1, 1, 1])\n    >>> dummy_clf.score(X, y)\n    0.75\n    \"\"\"\n    _parameter_constraints: dict = {'strategy': [StrOptions({'most_frequent', 'prior', 'stratified', 'uniform', 'constant'})], 'random_state': ['random_state'], 'constant': [Integral, str, 'array-like', None]}\n\n    def _more_tags(self):\n        return {'poor_score': True, 'no_validation': True, '_xfail_checks': {'check_methods_subset_invariance': 'fails for the predict method', 'check_methods_sample_order_invariance': 'fails for the predict method'}}\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the baseline classifier.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self._validate_data(X, cast_to_ndarray=False)\n        self._strategy = self.strategy\n        if self._strategy == 'uniform' and sp.issparse(y):\n            y = y.toarray()\n            warnings.warn('A local copy of the target data has been converted to a numpy array. Predicting on sparse target data with the uniform strategy would not save memory and would be slower.', UserWarning)\n        self.sparse_output_ = sp.issparse(y)\n        if not self.sparse_output_:\n            y = np.asarray(y)\n            y = np.atleast_1d(y)\n        if y.ndim == 1:\n            y = np.reshape(y, (-1, 1))\n        self.n_outputs_ = y.shape[1]\n        check_consistent_length(X, y)\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X)\n        if self._strategy == 'constant':\n            if self.constant is None:\n                raise ValueError('Constant target value has to be specified when the constant strategy is used.')\n            else:\n                constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))\n                if constant.shape[0] != self.n_outputs_:\n                    raise ValueError('Constant target value should have shape (%d, 1).' % self.n_outputs_)\n        (self.classes_, self.n_classes_, self.class_prior_) = class_distribution(y, sample_weight)\n        if self._strategy == 'constant':\n            for k in range(self.n_outputs_):\n                if not any((constant[k][0] == c for c in self.classes_[k])):\n                    err_msg = 'The constant target value must be present in the training data. You provided constant={}. Possible values are: {}.'.format(self.constant, self.classes_[k].tolist())\n                    raise ValueError(err_msg)\n        if self.n_outputs_ == 1:\n            self.n_classes_ = self.n_classes_[0]\n            self.classes_ = self.classes_[0]\n            self.class_prior_ = self.class_prior_[0]\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"\n        Return probability estimates for the test vectors X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test data.\n\n        Returns\n        -------\n        P : ndarray of shape (n_samples, n_classes) or list of such arrays\n            Returns the probability of the sample for each class in\n            the model, where classes are ordered arithmetically, for each\n            output.\n        \"\"\"\n        check_is_fitted(self)\n        n_samples = _num_samples(X)\n        rs = check_random_state(self.random_state)\n        n_classes_ = self.n_classes_\n        classes_ = self.classes_\n        class_prior_ = self.class_prior_\n        constant = self.constant\n        if self.n_outputs_ == 1:\n            n_classes_ = [n_classes_]\n            classes_ = [classes_]\n            class_prior_ = [class_prior_]\n            constant = [constant]\n        P = []\n        for k in range(self.n_outputs_):\n            if self._strategy == 'most_frequent':\n                ind = class_prior_[k].argmax()\n                out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n                out[:, ind] = 1.0\n            elif self._strategy == 'prior':\n                out = np.ones((n_samples, 1)) * class_prior_[k]\n            elif self._strategy == 'stratified':\n                out = rs.multinomial(1, class_prior_[k], size=n_samples)\n                out = out.astype(np.float64)\n            elif self._strategy == 'uniform':\n                out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)\n                out /= n_classes_[k]\n            elif self._strategy == 'constant':\n                ind = np.where(classes_[k] == constant[k])\n                out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n                out[:, ind] = 1.0\n            P.append(out)\n        if self.n_outputs_ == 1:\n            P = P[0]\n        return P\n\n    def __init__(self, *, strategy='prior', random_state=None, constant=None):\n        self.strategy = strategy\n        self.random_state = random_state\n        self.constant = constant\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def __sklearn_clone__(self):\n        return _clone_parametrized(self)\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def predict(self, X):\n        \"\"\"Perform classification on test vectors X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test data.\n\n        Returns\n        -------\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            Predicted target values for X.\n        \"\"\"\n        check_is_fitted(self)\n        n_samples = _num_samples(X)\n        rs = check_random_state(self.random_state)\n        n_classes_ = self.n_classes_\n        classes_ = self.classes_\n        class_prior_ = self.class_prior_\n        constant = self.constant\n        if self.n_outputs_ == 1:\n            n_classes_ = [n_classes_]\n            classes_ = [classes_]\n            class_prior_ = [class_prior_]\n            constant = [constant]\n        if self._strategy == 'stratified':\n            proba = self.predict_proba(X)\n            if self.n_outputs_ == 1:\n                proba = [proba]\n        if self.sparse_output_:\n            class_prob = None\n            if self._strategy in ('most_frequent', 'prior'):\n                classes_ = [np.array([cp.argmax()]) for cp in class_prior_]\n            elif self._strategy == 'stratified':\n                class_prob = class_prior_\n            elif self._strategy == 'uniform':\n                raise ValueError('Sparse target prediction is not supported with the uniform strategy')\n            elif self._strategy == 'constant':\n                classes_ = [np.array([c]) for c in constant]\n            y = _random_choice_csc(n_samples, classes_, class_prob, self.random_state)\n        else:\n            if self._strategy in ('most_frequent', 'prior'):\n                y = np.tile([classes_[k][class_prior_[k].argmax()] for k in range(self.n_outputs_)], [n_samples, 1])\n            elif self._strategy == 'stratified':\n                y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for k in range(self.n_outputs_)]).T\n            elif self._strategy == 'uniform':\n                ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)] for k in range(self.n_outputs_)]\n                y = np.vstack(ret).T\n            elif self._strategy == 'constant':\n                y = np.tile(self.constant, (n_samples, 1))\n            if self.n_outputs_ == 1:\n                y = np.ravel(y)\n        return y\n\n    def predict_log_proba(self, X):\n        \"\"\"\n        Return log probability estimates for the test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, object with finite length or shape}\n            Training data.\n\n        Returns\n        -------\n        P : ndarray of shape (n_samples, n_classes) or list of such arrays\n            Returns the log probability of the sample for each class in\n            the model, where classes are ordered arithmetically for each\n            output.\n        \"\"\"\n        proba = self.predict_proba(X)\n        if self.n_outputs_ == 1:\n            return np.log(proba)\n        else:\n            return [np.log(p) for p in proba]\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : None or array-like of shape (n_samples, n_features)\n            Test samples. Passing None as test samples gives the same result\n            as passing real test samples, since DummyClassifier\n            operates independently of the sampled observations.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) w.r.t. y.\n        \"\"\"\n        if X is None:\n            X = np.zeros(shape=(len(y), 1))\n        return super().score(X, y, sample_weight)", "class_fn": true, "question_id": "sklearn/sklearn.dummy/DummyClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_bagging.py", "fn_id": "", "content": "class BaggingRegressor(RegressorMixin, BaseBagging):\n    \"\"\"A Bagging regressor.\n\n    A Bagging regressor is an ensemble meta-estimator that fits base\n    regressors each on random subsets of the original dataset and then\n    aggregate their individual predictions (either by voting or by averaging)\n    to form a final prediction. Such a meta-estimator can typically be used as\n    a way to reduce the variance of a black-box estimator (e.g., a decision\n    tree), by introducing randomization into its construction procedure and\n    then making an ensemble out of it.\n\n    This algorithm encompasses several works from the literature. When random\n    subsets of the dataset are drawn as random subsets of the samples, then\n    this algorithm is known as Pasting [1]_. If samples are drawn with\n    replacement, then the method is known as Bagging [2]_. When random subsets\n    of the dataset are drawn as random subsets of the features, then the method\n    is known as Random Subspaces [3]_. Finally, when base estimators are built\n    on subsets of both samples and features, then the method is known as\n    Random Patches [4]_.\n\n    Read more in the :ref:`User Guide <bagging>`.\n\n    .. versionadded:: 0.15\n\n    Parameters\n    ----------\n    estimator : object, default=None\n        The base estimator to fit on random subsets of the dataset.\n        If None, then the base estimator is a\n        :class:`~sklearn.tree.DecisionTreeRegressor`.\n\n        .. versionadded:: 1.2\n           `base_estimator` was renamed to `estimator`.\n\n    n_estimators : int, default=10\n        The number of base estimators in the ensemble.\n\n    max_samples : int or float, default=1.0\n        The number of samples to draw from X to train each base estimator (with\n        replacement by default, see `bootstrap` for more details).\n\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n\n    max_features : int or float, default=1.0\n        The number of features to draw from X to train each base estimator (\n        without replacement by default, see `bootstrap_features` for more\n        details).\n\n        - If int, then draw `max_features` features.\n        - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n\n    bootstrap : bool, default=True\n        Whether samples are drawn with replacement. If False, sampling\n        without replacement is performed.\n\n    bootstrap_features : bool, default=False\n        Whether features are drawn with replacement.\n\n    oob_score : bool, default=False\n        Whether to use out-of-bag samples to estimate\n        the generalization error. Only available if bootstrap=True.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit\n        a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for both :meth:`fit` and\n        :meth:`predict`. ``None`` means 1 unless in a\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n        processors. See :term:`Glossary <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random resampling of the original dataset\n        (sample wise and feature wise).\n        If the base estimator accepts a `random_state` attribute, a different\n        seed is generated for each instance in the ensemble.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    Attributes\n    ----------\n    estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    estimators_ : list of estimators\n        The collection of fitted sub-estimators.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator. Each subset is defined by an array of the indices selected.\n\n    estimators_features_ : list of arrays\n        The subset of drawn features for each base estimator.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_prediction_ : ndarray of shape (n_samples,)\n        Prediction computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_prediction_` might contain NaN. This attribute exists only\n        when ``oob_score`` is True.\n\n    See Also\n    --------\n    BaggingClassifier : A Bagging classifier.\n\n    References\n    ----------\n\n    .. [1] L. Breiman, \"Pasting small votes for classification in large\n           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n           1996.\n\n    .. [3] T. Ho, \"The random subspace method for constructing decision\n           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n           1998.\n\n    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVR\n    >>> from sklearn.ensemble import BaggingRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_samples=100, n_features=4,\n    ...                        n_informative=2, n_targets=1,\n    ...                        random_state=0, shuffle=False)\n    >>> regr = BaggingRegressor(estimator=SVR(),\n    ...                         n_estimators=10, random_state=0).fit(X, y)\n    >>> regr.predict([[0, 0, 0, 0]])\n    array([-2.8720...])\n    \"\"\"\n\n    def _validate_y(self, y):\n        if len(y.shape) == 1 or y.shape[1] == 1:\n            return column_or_1d(y, warn=True)\n        return y\n\n    def _get_estimators_indices(self):\n        for seed in self._seeds:\n            (feature_indices, sample_indices) = _generate_bagging_indices(seed, self.bootstrap_features, self.bootstrap, self.n_features_in_, self._n_samples, self._max_features, self._max_samples)\n            yield (feature_indices, sample_indices)\n\n    def __init__(self, estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0):\n        super().__init__(estimator=estimator, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, bootstrap=bootstrap, bootstrap_features=bootstrap_features, oob_score=oob_score, warm_start=warm_start, n_jobs=n_jobs, random_state=random_state, verbose=verbose)\n\n    def _get_estimator(self):\n        \"\"\"Resolve which estimator to return (default is DecisionTreeClassifier)\"\"\"\n        if self.estimator is None:\n            return DecisionTreeRegressor()\n        return self.estimator\n\n    def _set_oob_score(self, X, y):\n        n_samples = y.shape[0]\n        predictions = np.zeros((n_samples,))\n        n_predictions = np.zeros((n_samples,))\n        for (estimator, samples, features) in zip(self.estimators_, self.estimators_samples_, self.estimators_features_):\n            mask = ~indices_to_mask(samples, n_samples)\n            predictions[mask] += estimator.predict(X[mask, :][:, features])\n            n_predictions[mask] += 1\n        if (n_predictions == 0).any():\n            warn('Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.')\n            n_predictions[n_predictions == 0] = 1\n        predictions /= n_predictions\n        self.oob_prediction_ = predictions\n        self.oob_score_ = r2_score(y, predictions)\n\n    def predict(self, X):\n        \"\"\"Predict regression target for X.\n\n        The predicted regression target of an input sample is computed as the\n        mean predicted regression targets of the estimators in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=['csr', 'csc'], dtype=None, force_all_finite=False, reset=False)\n        (n_jobs, _, starts) = _partition_estimators(self.n_estimators, self.n_jobs)\n        all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose)((delayed(_parallel_predict_regression)(self.estimators_[starts[i]:starts[i + 1]], self.estimators_features_[starts[i]:starts[i + 1]], X) for i in range(n_jobs)))\n        y_hat = sum(all_y_hat) / self.n_estimators\n        return y_hat\n\n    def _fit(self, X, y, max_samples=None, max_depth=None, check_input=True, **fit_params):\n        \"\"\"Build a Bagging ensemble of estimators from the training\n           set (X, y).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        y : array-like of shape (n_samples,)\n            The target values (class labels in classification, real numbers in\n            regression).\n\n        max_samples : int or float, default=None\n            Argument to use instead of self.max_samples.\n\n        max_depth : int, default=None\n            Override value used when constructing base estimator. Only\n            supported if the base estimator has a max_depth parameter.\n\n        check_input : bool, default=True\n            Override value used when fitting base estimator. Only supported\n            if the base estimator has a check_input parameter for fit function.\n            If the meta-estimator already checks the input, set this value to\n            False to prevent redundant input validation.\n\n        **fit_params : dict, default=None\n            Parameters to pass to the :term:`fit` method of the underlying\n            estimator.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        random_state = check_random_state(self.random_state)\n        n_samples = X.shape[0]\n        self._n_samples = n_samples\n        y = self._validate_y(y)\n        self._validate_estimator(self._get_estimator())\n        if _routing_enabled():\n            routed_params = process_routing(self, 'fit', **fit_params)\n        else:\n            routed_params = Bunch()\n            routed_params.estimator = Bunch(fit=fit_params)\n            if 'sample_weight' in fit_params:\n                routed_params.estimator.fit['sample_weight'] = fit_params['sample_weight']\n        if max_depth is not None:\n            self.estimator_.max_depth = max_depth\n        if max_samples is None:\n            max_samples = self.max_samples\n        elif not isinstance(max_samples, numbers.Integral):\n            max_samples = int(max_samples * X.shape[0])\n        if max_samples > X.shape[0]:\n            raise ValueError('max_samples must be <= n_samples')\n        self._max_samples = max_samples\n        if isinstance(self.max_features, numbers.Integral):\n            max_features = self.max_features\n        elif isinstance(self.max_features, float):\n            max_features = int(self.max_features * self.n_features_in_)\n        if max_features > self.n_features_in_:\n            raise ValueError('max_features must be <= n_features')\n        max_features = max(1, int(max_features))\n        self._max_features = max_features\n        if not self.bootstrap and self.oob_score:\n            raise ValueError('Out of bag estimation only available if bootstrap=True')\n        if self.warm_start and self.oob_score:\n            raise ValueError('Out of bag estimate only available if warm_start=False')\n        if hasattr(self, 'oob_score_') and self.warm_start:\n            del self.oob_score_\n        if not self.warm_start or not hasattr(self, 'estimators_'):\n            self.estimators_ = []\n            self.estimators_features_ = []\n        n_more_estimators = self.n_estimators - len(self.estimators_)\n        if n_more_estimators < 0:\n            raise ValueError('n_estimators=%d must be larger or equal to len(estimators_)=%d when warm_start==True' % (self.n_estimators, len(self.estimators_)))\n        elif n_more_estimators == 0:\n            warn('Warm-start fitting without increasing n_estimators does not fit new trees.')\n            return self\n        (n_jobs, n_estimators, starts) = _partition_estimators(n_more_estimators, self.n_jobs)\n        total_n_estimators = sum(n_estimators)\n        if self.warm_start and len(self.estimators_) > 0:\n            random_state.randint(MAX_INT, size=len(self.estimators_))\n        seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n        self._seeds = seeds\n        all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose, **self._parallel_args())((delayed(_parallel_build_estimators)(n_estimators[i], self, X, y, seeds[starts[i]:starts[i + 1]], total_n_estimators, verbose=self.verbose, check_input=check_input, fit_params=routed_params.estimator.fit) for i in range(n_jobs)))\n        self.estimators_ += list(itertools.chain.from_iterable((t[0] for t in all_results)))\n        self.estimators_features_ += list(itertools.chain.from_iterable((t[1] for t in all_results)))\n        if self.oob_score:\n            self._set_oob_score(X, y)\n        return self", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._bagging/BaggingRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_base.py", "fn_id": "", "content": "class _BaseHeterogeneousEnsemble(MetaEstimatorMixin, _BaseComposition, metaclass=ABCMeta):\n    \"\"\"Base class for heterogeneous ensemble of learners.\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator) tuples\n        The ensemble of estimators to use in the ensemble. Each element of the\n        list is defined as a tuple of string (i.e. name of the estimator) and\n        an estimator instance. An estimator can be set to `'drop'` using\n        `set_params`.\n\n    Attributes\n    ----------\n    estimators_ : list of estimators\n        The elements of the estimators parameter, having been fitted on the\n        training data. If an estimator has been set to `'drop'`, it will not\n        appear in `estimators_`.\n    \"\"\"\n    _required_parameters = ['estimators']\n\n    @property\n    def named_estimators(self):\n        \"\"\"Dictionary to access any fitted sub-estimators by name.\n\n        Returns\n        -------\n        :class:`~sklearn.utils.Bunch`\n        \"\"\"\n        return Bunch(**dict(self.estimators))\n\n    @abstractmethod\n    def __init__(self, estimators):\n        self.estimators = estimators\n\n    def _replace_estimator(self, attr, name, new_val):\n        new_estimators = list(getattr(self, attr))\n        for (i, (estimator_name, _)) in enumerate(new_estimators):\n            if estimator_name == name:\n                new_estimators[i] = (name, new_val)\n                break\n        setattr(self, attr, new_estimators)\n\n    def _set_params(self, attr, **params):\n        if attr in params:\n            setattr(self, attr, params.pop(attr))\n        items = getattr(self, attr)\n        if isinstance(items, list) and items:\n            with suppress(TypeError):\n                (item_names, _) = zip(*items)\n                for name in list(params.keys()):\n                    if '__' not in name and name in item_names:\n                        self._replace_estimator(attr, name, params.pop(name))\n        super().set_params(**params)\n        return self\n\n    def _more_tags(self):\n        try:\n            allow_nan = all((_safe_tags(est[1])['allow_nan'] if est[1] != 'drop' else True for est in self.estimators))\n        except Exception:\n            allow_nan = False\n        return {'preserves_dtype': [], 'allow_nan': allow_nan}\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of an estimator from the ensemble.\n\n        Valid parameter keys can be listed with `get_params()`. Note that you\n        can directly set the parameters of the estimators contained in\n        `estimators`.\n\n        Parameters\n        ----------\n        **params : keyword arguments\n            Specific parameters using e.g.\n            `set_params(parameter_name=new_value)`. In addition, to setting the\n            parameters of the estimator, the individual estimator of the\n            estimators can also be set, or can be removed by setting them to\n            'drop'.\n\n        Returns\n        -------\n        self : object\n            Estimator instance.\n        \"\"\"\n        super()._set_params('estimators', **params)\n        return self\n\n    def _validate_estimators(self):\n        if len(self.estimators) == 0:\n            raise ValueError(\"Invalid 'estimators' attribute, 'estimators' should be a non-empty list of (string, estimator) tuples.\")\n        (names, estimators) = zip(*self.estimators)\n        self._validate_names(names)\n        has_estimator = any((est != 'drop' for est in estimators))\n        if not has_estimator:\n            raise ValueError('All estimators are dropped. At least one is required to be an estimator.')\n        is_estimator_type = is_classifier if is_classifier(self) else is_regressor\n        for est in estimators:\n            if est != 'drop' and (not is_estimator_type(est)):\n                raise ValueError('The estimator {} should be a {}.'.format(est.__class__.__name__, is_estimator_type.__name__[3:]))\n        return (names, estimators)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get the parameters of an estimator from the ensemble.\n\n        Returns the parameters given in the constructor as well as the\n        estimators contained within the `estimators` parameter.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            Setting it to True gets the various estimators and the parameters\n            of the estimators as well.\n\n        Returns\n        -------\n        params : dict\n            Parameter and estimator names mapped to their values or parameter\n            names mapped to their values.\n        \"\"\"\n        return super()._get_params('estimators', deep=deep)\n\n    def _get_params(self, attr, deep=True):\n        out = super().get_params(deep=deep)\n        if not deep:\n            return out\n        estimators = getattr(self, attr)\n        try:\n            out.update(estimators)\n        except (TypeError, ValueError):\n            return out\n        for (name, estimator) in estimators:\n            if hasattr(estimator, 'get_params'):\n                for (key, value) in estimator.get_params(deep=True).items():\n                    out['%s__%s' % (name, key)] = value\n        return out", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._base/_BaseHeterogeneousEnsemble", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_forest.py", "fn_id": "", "content": "class ExtraTreesRegressor(ForestRegressor):\n    \"\"\"\n    An extra-trees regressor.\n\n    This class implements a meta estimator that fits a number of\n    randomized decision trees (a.k.a. extra-trees) on various sub-samples\n    of the dataset and uses averaging to improve the predictive accuracy\n    and control over-fitting.\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n        The function to measure the quality of a split. Supported criteria\n        are \"squared_error\" for the mean squared error, which is equal to\n        variance reduction as feature selection criterion and minimizes the L2\n        loss using the mean of each terminal node, \"friedman_mse\", which uses\n        mean squared error with Friedman's improvement score for potential\n        splits, \"absolute_error\" for the mean absolute error, which minimizes\n        the L1 loss using the median of each terminal node, and \"poisson\" which\n        uses reduction in Poisson deviance to find splits.\n        Training using \"absolute_error\" is significantly slower\n        than when using \"squared_error\".\n\n        .. versionadded:: 0.18\n           Mean Absolute Error (MAE) criterion.\n\n    max_depth : int, default=None\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `max(1, int(max_features * n_features_in_))` features are considered at each\n          split.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None or 1.0, then `max_features=n_features`.\n\n        .. note::\n            The default of 1.0 is equivalent to bagged trees and more\n            randomness can be achieved by setting smaller values, e.g. 0.3.\n\n        .. versionchanged:: 1.1\n            The default of `max_features` changed from `\"auto\"` to 1.0.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    bootstrap : bool, default=False\n        Whether bootstrap samples are used when building trees. If False, the\n        whole dataset is used to build each tree.\n\n    oob_score : bool or callable, default=False\n        Whether to use out-of-bag samples to estimate the generalization score.\n        By default, :func:`~sklearn.metrics.r2_score` is used.\n        Provide a callable with signature `metric(y_true, y_pred)` to use a\n        custom metric. Only available if `bootstrap=True`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls 3 sources of randomness:\n\n        - the bootstrapping of the samples used when building trees\n          (if ``bootstrap=True``)\n        - the sampling of the features to consider when looking for the best\n          split at each node (if ``max_features < n_features``)\n        - the draw of the splits for each of the `max_features`\n\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`Glossary <warm_start>` and\n        :ref:`tree_ensemble_warm_start` for details.\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n          `max_samples` should be in the interval `(0.0, 1.0]`.\n\n        .. versionadded:: 0.22\n\n    monotonic_cst : array-like of int of shape (n_features), default=None\n        Indicates the monotonicity constraint to enforce on each feature.\n          - 1: monotonically increasing\n          - 0: no constraint\n          - -1: monotonically decreasing\n\n        If monotonic_cst is None, no constraints are applied.\n\n        Monotonicity constraints are not supported for:\n          - multioutput regressions (i.e. when `n_outputs_ > 1`),\n          - regressions trained on data with missing values.\n\n        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n        .. versionadded:: 1.4\n\n    Attributes\n    ----------\n    estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor`\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    estimators_ : list of DecisionTreeRegressor\n        The collection of fitted sub-estimators.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_outputs_ : int\n        The number of outputs.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n        Prediction computed with out-of-bag estimate on the training set.\n        This attribute exists only when ``oob_score`` is True.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator. Each subset is defined by an array of the indices selected.\n\n        .. versionadded:: 1.4\n\n    See Also\n    --------\n    ExtraTreesClassifier : An extra-trees classifier with random splits.\n    RandomForestClassifier : A random forest classifier with optimal splits.\n    RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    References\n    ----------\n    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n           Machine Learning, 63(1), 3-42, 2006.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_diabetes\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.ensemble import ExtraTreesRegressor\n    >>> X, y = load_diabetes(return_X_y=True)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=0)\n    >>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n    ...    X_train, y_train)\n    >>> reg.score(X_test, y_test)\n    0.2727...\n    \"\"\"\n    _parameter_constraints: dict = {**ForestRegressor._parameter_constraints, **DecisionTreeRegressor._parameter_constraints}\n    _parameter_constraints.pop('splitter')\n\n    def __init__(self, n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None):\n        super().__init__(estimator=ExtraTreeRegressor(), n_estimators=n_estimators, estimator_params=('criterion', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'min_weight_fraction_leaf', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'random_state', 'ccp_alpha', 'monotonic_cst'), bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, verbose=verbose, warm_start=warm_start, max_samples=max_samples)\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.ccp_alpha = ccp_alpha\n        self.monotonic_cst = monotonic_cst\n\n    def _set_oob_score_and_attributes(self, X, y, scoring_function=None):\n        \"\"\"Compute and set the OOB score and attributes.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n        y : ndarray of shape (n_samples, n_outputs)\n            The target matrix.\n        scoring_function : callable, default=None\n            Scoring function for OOB score. Defaults to `r2_score`.\n        \"\"\"\n        self.oob_prediction_ = super()._compute_oob_predictions(X, y).squeeze(axis=1)\n        if self.oob_prediction_.shape[-1] == 1:\n            self.oob_prediction_ = self.oob_prediction_.squeeze(axis=-1)\n        if scoring_function is None:\n            scoring_function = r2_score\n        self.oob_score_ = scoring_function(y, self.oob_prediction_)\n\n    def _more_tags(self):\n        return {'multilabel': True}\n\n    def _compute_partial_dependence_recursion(self, grid, target_features):\n        \"\"\"Fast partial dependence computation.\n\n        Parameters\n        ----------\n        grid : ndarray of shape (n_samples, n_target_features), dtype=DTYPE\n            The grid points on which the partial dependence should be\n            evaluated.\n        target_features : ndarray of shape (n_target_features), dtype=np.intp\n            The set of target features for which the partial dependence\n            should be evaluated.\n\n        Returns\n        -------\n        averaged_predictions : ndarray of shape (n_samples,)\n            The value of the partial dependence function on each grid point.\n        \"\"\"\n        grid = np.asarray(grid, dtype=DTYPE, order='C')\n        target_features = np.asarray(target_features, dtype=np.intp, order='C')\n        averaged_predictions = np.zeros(shape=grid.shape[0], dtype=np.float64, order='C')\n        for tree in self.estimators_:\n            tree.tree_.compute_partial_dependence(grid, target_features, averaged_predictions)\n        averaged_predictions /= len(self.estimators_)\n        return averaged_predictions", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._forest/ExtraTreesRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_forest.py", "fn_id": "", "content": "class RandomForestClassifier(ForestClassifier):\n    \"\"\"\n    A random forest classifier.\n\n    A random forest is a meta estimator that fits a number of decision tree\n    classifiers on various sub-samples of the dataset and uses averaging to\n    improve the predictive accuracy and control over-fitting.\n    Trees in the forest use the best split strategy, i.e. equivalent to passing\n    `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\n    The sub-sample size is controlled with the `max_samples` parameter if\n    `bootstrap=True` (default), otherwise the whole dataset is used to build\n    each tree.\n\n    For a comparison between tree-based ensemble models see the example\n    :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n        Shannon information gain, see :ref:`tree_mathematical_formulation`.\n        Note: This parameter is tree-specific.\n\n    max_depth : int, default=None\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `max(1, int(max_features * n_features_in_))` features are considered at each\n          split.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        .. versionchanged:: 1.1\n            The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    bootstrap : bool, default=True\n        Whether bootstrap samples are used when building trees. If False, the\n        whole dataset is used to build each tree.\n\n    oob_score : bool or callable, default=False\n        Whether to use out-of-bag samples to estimate the generalization score.\n        By default, :func:`~sklearn.metrics.accuracy_score` is used.\n        Provide a callable with signature `metric(y_true, y_pred)` to use a\n        custom metric. Only available if `bootstrap=True`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls both the randomness of the bootstrapping of the samples used\n        when building trees (if ``bootstrap=True``) and the sampling of the\n        features to consider when looking for the best split at each node\n        (if ``max_features < n_features``).\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`Glossary <warm_start>` and\n        :ref:`tree_ensemble_warm_start` for details.\n\n    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n        weights are computed based on the bootstrap sample for every tree\n        grown.\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n          `max_samples` should be in the interval `(0.0, 1.0]`.\n\n        .. versionadded:: 0.22\n\n    monotonic_cst : array-like of int of shape (n_features), default=None\n        Indicates the monotonicity constraint to enforce on each feature.\n          - 1: monotonic increase\n          - 0: no constraint\n          - -1: monotonic decrease\n\n        If monotonic_cst is None, no constraints are applied.\n\n        Monotonicity constraints are not supported for:\n          - multiclass classifications (i.e. when `n_classes > 2`),\n          - multioutput classifications (i.e. when `n_outputs_ > 1`),\n          - classifications trained on data with missing values.\n\n        The constraints hold over the probability of the positive class.\n\n        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n        .. versionadded:: 1.4\n\n    Attributes\n    ----------\n    estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    estimators_ : list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    classes_ : ndarray of shape (n_classes,) or a list of such arrays\n        The classes labels (single output problem), or a list of arrays of\n        class labels (multi-output problem).\n\n    n_classes_ : int or list\n        The number of classes (single output problem), or a list containing the\n        number of classes for each output (multi-output problem).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n        Decision function computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_decision_function_` might contain NaN. This attribute exists\n        only when ``oob_score`` is True.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator. Each subset is defined by an array of the indices selected.\n\n        .. versionadded:: 1.4\n\n    See Also\n    --------\n    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n    sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n        tree classifiers.\n    sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n        Boosting Classification Tree, very fast for big datasets (n_samples >=\n        10_000).\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data,\n    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n    of the criterion is identical for several splits enumerated during the\n    search of the best split. To obtain a deterministic behaviour during\n    fitting, ``random_state`` has to be fixed.\n\n    References\n    ----------\n    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n    >>> clf.fit(X, y)\n    RandomForestClassifier(...)\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]\n    \"\"\"\n    _parameter_constraints: dict = {**ForestClassifier._parameter_constraints, **DecisionTreeClassifier._parameter_constraints, 'class_weight': [StrOptions({'balanced_subsample', 'balanced'}), dict, list, None]}\n    _parameter_constraints.pop('splitter')\n\n    def predict(self, X):\n        \"\"\"\n        Predict class for X.\n\n        The predicted class of an input sample is a vote by the trees in\n        the forest, weighted by their probability estimates. That is,\n        the predicted class is the one with highest mean probability\n        estimate across the trees.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n            The predicted classes.\n        \"\"\"\n        proba = self.predict_proba(X)\n        if self.n_outputs_ == 1:\n            return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n        else:\n            n_samples = proba[0].shape[0]\n            class_type = self.classes_[0].dtype\n            predictions = np.empty((n_samples, self.n_outputs_), dtype=class_type)\n            for k in range(self.n_outputs_):\n                predictions[:, k] = self.classes_[k].take(np.argmax(proba[k], axis=1), axis=0)\n            return predictions\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample are computed as\n        the mean predicted class probabilities of the trees in the forest.\n        The class probability of a single tree is the fraction of samples of\n        the same class in a leaf.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_X_predict(X)\n        (n_jobs, _, _) = _partition_estimators(self.n_estimators, self.n_jobs)\n        all_proba = [np.zeros((X.shape[0], j), dtype=np.float64) for j in np.atleast_1d(self.n_classes_)]\n        lock = threading.Lock()\n        Parallel(n_jobs=n_jobs, verbose=self.verbose, require='sharedmem')((delayed(_accumulate_prediction)(e.predict_proba, X, all_proba, lock) for e in self.estimators_))\n        for proba in all_proba:\n            proba /= len(self.estimators_)\n        if len(all_proba) == 1:\n            return all_proba[0]\n        else:\n            return all_proba\n\n    def __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None):\n        super().__init__(estimator=DecisionTreeClassifier(), n_estimators=n_estimators, estimator_params=('criterion', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'min_weight_fraction_leaf', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'random_state', 'ccp_alpha', 'monotonic_cst'), bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, verbose=verbose, warm_start=warm_start, class_weight=class_weight, max_samples=max_samples)\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.monotonic_cst = monotonic_cst\n        self.ccp_alpha = ccp_alpha\n\n    def predict_log_proba(self, X):\n        \"\"\"\n        Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the log of the mean predicted class probabilities of the trees in the\n        forest.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        proba = self.predict_proba(X)\n        if self.n_outputs_ == 1:\n            return np.log(proba)\n        else:\n            for k in range(self.n_outputs_):\n                proba[k] = np.log(proba[k])\n            return proba", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._forest/RandomForestClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_gb.py", "fn_id": "", "content": "class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):\n    \"\"\"Abstract base class for Gradient Boosting.\"\"\"\n    _parameter_constraints: dict = {**DecisionTreeRegressor._parameter_constraints, 'learning_rate': [Interval(Real, 0.0, None, closed='left')], 'n_estimators': [Interval(Integral, 1, None, closed='left')], 'criterion': [StrOptions({'friedman_mse', 'squared_error'})], 'subsample': [Interval(Real, 0.0, 1.0, closed='right')], 'verbose': ['verbose'], 'warm_start': ['boolean'], 'validation_fraction': [Interval(Real, 0.0, 1.0, closed='neither')], 'n_iter_no_change': [Interval(Integral, 1, None, closed='left'), None], 'tol': [Interval(Real, 0.0, None, closed='left')]}\n    _parameter_constraints.pop('splitter')\n    _parameter_constraints.pop('monotonic_cst')\n\n    def _raw_predict(self, X):\n        \"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\n        check_is_fitted(self)\n        raw_predictions = self._raw_predict_init(X)\n        predict_stages(self.estimators_, X, self.learning_rate, raw_predictions)\n        return raw_predictions\n\n    def __getitem__(self, index):\n        \"\"\"Return the index'th estimator in the ensemble.\"\"\"\n        return self.estimators_[index]\n\n    def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage=0, monitor=None):\n        \"\"\"Iteratively fits the stages.\n\n        For each stage it computes the progress (OOB, train score)\n        and delegates to ``_fit_stage``.\n        Returns the number of stages fit; might differ from ``n_estimators``\n        due to early stopping.\n        \"\"\"\n        n_samples = X.shape[0]\n        do_oob = self.subsample < 1.0\n        sample_mask = np.ones((n_samples,), dtype=bool)\n        n_inbag = max(1, int(self.subsample * n_samples))\n        if self.verbose:\n            verbose_reporter = VerboseReporter(verbose=self.verbose)\n            verbose_reporter.init(self, begin_at_stage)\n        X_csc = csc_matrix(X) if issparse(X) else None\n        X_csr = csr_matrix(X) if issparse(X) else None\n        if self.n_iter_no_change is not None:\n            loss_history = np.full(self.n_iter_no_change, np.inf)\n            y_val_pred_iter = self._staged_raw_predict(X_val, check_input=False)\n        if isinstance(self._loss, (HalfSquaredError, HalfBinomialLoss)):\n            factor = 2\n        else:\n            factor = 1\n        i = begin_at_stage\n        for i in range(begin_at_stage, self.n_estimators):\n            if do_oob:\n                sample_mask = _random_sample_mask(n_samples, n_inbag, random_state)\n                y_oob_masked = y[~sample_mask]\n                sample_weight_oob_masked = sample_weight[~sample_mask]\n                if i == 0:\n                    initial_loss = factor * self._loss(y_true=y_oob_masked, raw_prediction=raw_predictions[~sample_mask], sample_weight=sample_weight_oob_masked)\n            raw_predictions = self._fit_stage(i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc=X_csc, X_csr=X_csr)\n            if do_oob:\n                self.train_score_[i] = factor * self._loss(y_true=y[sample_mask], raw_prediction=raw_predictions[sample_mask], sample_weight=sample_weight[sample_mask])\n                self.oob_scores_[i] = factor * self._loss(y_true=y_oob_masked, raw_prediction=raw_predictions[~sample_mask], sample_weight=sample_weight_oob_masked)\n                previous_loss = initial_loss if i == 0 else self.oob_scores_[i - 1]\n                self.oob_improvement_[i] = previous_loss - self.oob_scores_[i]\n                self.oob_score_ = self.oob_scores_[-1]\n            else:\n                self.train_score_[i] = factor * self._loss(y_true=y, raw_prediction=raw_predictions, sample_weight=sample_weight)\n            if self.verbose > 0:\n                verbose_reporter.update(i, self)\n            if monitor is not None:\n                early_stopping = monitor(i, self, locals())\n                if early_stopping:\n                    break\n            if self.n_iter_no_change is not None:\n                validation_loss = factor * self._loss(y_val, next(y_val_pred_iter), sample_weight_val)\n                if np.any(validation_loss + self.tol < loss_history):\n                    loss_history[i % len(loss_history)] = validation_loss\n                else:\n                    break\n        return i + 1\n\n    def _compute_partial_dependence_recursion(self, grid, target_features):\n        \"\"\"Fast partial dependence computation.\n\n        Parameters\n        ----------\n        grid : ndarray of shape (n_samples, n_target_features), dtype=np.float32\n            The grid points on which the partial dependence should be\n            evaluated.\n        target_features : ndarray of shape (n_target_features,), dtype=np.intp\n            The set of target features for which the partial dependence\n            should be evaluated.\n\n        Returns\n        -------\n        averaged_predictions : ndarray of shape                 (n_trees_per_iteration_, n_samples)\n            The value of the partial dependence function on each grid point.\n        \"\"\"\n        if self.init is not None:\n            warnings.warn('Using recursion method with a non-constant init predictor will lead to incorrect partial dependence values. Got init=%s.' % self.init, UserWarning)\n        grid = np.asarray(grid, dtype=DTYPE, order='C')\n        (n_estimators, n_trees_per_stage) = self.estimators_.shape\n        averaged_predictions = np.zeros((n_trees_per_stage, grid.shape[0]), dtype=np.float64, order='C')\n        target_features = np.asarray(target_features, dtype=np.intp, order='C')\n        for stage in range(n_estimators):\n            for k in range(n_trees_per_stage):\n                tree = self.estimators_[stage, k].tree_\n                tree.compute_partial_dependence(grid, target_features, averaged_predictions[k])\n        averaged_predictions *= self.learning_rate\n        return averaged_predictions\n\n    def _check_initialized(self):\n        \"\"\"Check that the estimator is initialized, raising an error if not.\"\"\"\n        check_is_fitted(self)\n\n    def _clear_state(self):\n        \"\"\"Clear the state of the gradient boosting model.\"\"\"\n        if hasattr(self, 'estimators_'):\n            self.estimators_ = np.empty((0, 0), dtype=object)\n        if hasattr(self, 'train_score_'):\n            del self.train_score_\n        if hasattr(self, 'oob_improvement_'):\n            del self.oob_improvement_\n        if hasattr(self, 'oob_scores_'):\n            del self.oob_scores_\n        if hasattr(self, 'oob_score_'):\n            del self.oob_score_\n        if hasattr(self, 'init_'):\n            del self.init_\n        if hasattr(self, '_rng'):\n            del self._rng\n\n    def _staged_raw_predict(self, X, check_input=True):\n        \"\"\"Compute raw predictions of ``X`` for each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        check_input : bool, default=True\n            If False, the input arrays X will not be checked.\n\n        Returns\n        -------\n        raw_predictions : generator of ndarray of shape (n_samples, k)\n            The raw predictions of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification are special cases with\n            ``k == 1``, otherwise ``k==n_classes``.\n        \"\"\"\n        if check_input:\n            X = self._validate_data(X, dtype=DTYPE, order='C', accept_sparse='csr', reset=False)\n        raw_predictions = self._raw_predict_init(X)\n        for i in range(self.estimators_.shape[0]):\n            predict_stage(self.estimators_, i, X, self.learning_rate, raw_predictions)\n            yield raw_predictions.copy()\n\n    def _raw_predict_init(self, X):\n        \"\"\"Check input and compute raw predictions of the init estimator.\"\"\"\n        self._check_initialized()\n        X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n        if self.init_ == 'zero':\n            raw_predictions = np.zeros(shape=(X.shape[0], self.n_trees_per_iteration_), dtype=np.float64)\n        else:\n            raw_predictions = _init_raw_predictions(X, self.init_, self._loss, is_classifier(self))\n        return raw_predictions\n\n    def __len__(self):\n        \"\"\"Return the number of estimators in the ensemble.\"\"\"\n        return len(self.estimators_)\n\n    def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc=None, X_csr=None):\n        \"\"\"Fit another stage of ``n_trees_per_iteration_`` trees.\"\"\"\n        original_y = y\n        if isinstance(self._loss, HuberLoss):\n            set_huber_delta(loss=self._loss, y_true=y, raw_prediction=raw_predictions, sample_weight=sample_weight)\n        neg_gradient = -self._loss.gradient(y_true=y, raw_prediction=raw_predictions, sample_weight=None)\n        if neg_gradient.ndim == 1:\n            neg_g_view = neg_gradient.reshape((-1, 1))\n        else:\n            neg_g_view = neg_gradient\n        for k in range(self.n_trees_per_iteration_):\n            if self._loss.is_multiclass:\n                y = np.array(original_y == k, dtype=np.float64)\n            tree = DecisionTreeRegressor(criterion=self.criterion, splitter='best', max_depth=self.max_depth, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, min_weight_fraction_leaf=self.min_weight_fraction_leaf, min_impurity_decrease=self.min_impurity_decrease, max_features=self.max_features, max_leaf_nodes=self.max_leaf_nodes, random_state=random_state, ccp_alpha=self.ccp_alpha)\n            if self.subsample < 1.0:\n                sample_weight = sample_weight * sample_mask.astype(np.float64)\n            X = X_csc if X_csc is not None else X\n            tree.fit(X, neg_g_view[:, k], sample_weight=sample_weight, check_input=False)\n            X_for_tree_update = X_csr if X_csr is not None else X\n            _update_terminal_regions(self._loss, tree.tree_, X_for_tree_update, y, neg_g_view[:, k], raw_predictions, sample_weight, sample_mask, learning_rate=self.learning_rate, k=k)\n            self.estimators_[i, k] = tree\n        return raw_predictions\n\n    @_fit_context(prefer_skip_nested_validation=False)\n    def fit(self, X, y, sample_weight=None, monitor=None):\n        \"\"\"Fit the gradient boosting model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        y : array-like of shape (n_samples,)\n            Target values (strings or integers in classification, real numbers\n            in regression)\n            For classification, labels must correspond to classes.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted. Splits\n            that would create child nodes with net zero or negative weight are\n            ignored while searching for a split in each node. In the case of\n            classification, splits are also ignored if they would result in any\n            single class carrying a negative weight in either child node.\n\n        monitor : callable, default=None\n            The monitor is called after each iteration with the current\n            iteration, a reference to the estimator and the local variables of\n            ``_fit_stages`` as keyword arguments ``callable(i, self,\n            locals())``. If the callable returns ``True`` the fitting procedure\n            is stopped. The monitor can be used for various things such as\n            computing held-out estimates, early stopping, model introspect, and\n            snapshotting.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        if not self.warm_start:\n            self._clear_state()\n        (X, y) = self._validate_data(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE, multi_output=True)\n        sample_weight_is_none = sample_weight is None\n        sample_weight = _check_sample_weight(sample_weight, X)\n        if sample_weight_is_none:\n            y = self._encode_y(y=y, sample_weight=None)\n        else:\n            y = self._encode_y(y=y, sample_weight=sample_weight)\n        y = column_or_1d(y, warn=True)\n        self._set_max_features()\n        self._loss = self._get_loss(sample_weight=sample_weight)\n        if self.n_iter_no_change is not None:\n            stratify = y if is_classifier(self) else None\n            (X_train, X_val, y_train, y_val, sample_weight_train, sample_weight_val) = train_test_split(X, y, sample_weight, random_state=self.random_state, test_size=self.validation_fraction, stratify=stratify)\n            if is_classifier(self):\n                if self.n_classes_ != np.unique(y_train).shape[0]:\n                    raise ValueError('The training data after the early stopping split is missing some classes. Try using another random seed.')\n        else:\n            (X_train, y_train, sample_weight_train) = (X, y, sample_weight)\n            X_val = y_val = sample_weight_val = None\n        n_samples = X_train.shape[0]\n        if not self._is_fitted():\n            self._init_state()\n            if self.init_ == 'zero':\n                raw_predictions = np.zeros(shape=(n_samples, self.n_trees_per_iteration_), dtype=np.float64)\n            else:\n                if sample_weight_is_none:\n                    self.init_.fit(X_train, y_train)\n                else:\n                    msg = 'The initial estimator {} does not support sample weights.'.format(self.init_.__class__.__name__)\n                    try:\n                        self.init_.fit(X_train, y_train, sample_weight=sample_weight_train)\n                    except TypeError as e:\n                        if \"unexpected keyword argument 'sample_weight'\" in str(e):\n                            raise ValueError(msg) from e\n                        else:\n                            raise\n                    except ValueError as e:\n                        if 'pass parameters to specific steps of your pipeline using the stepname__parameter' in str(e):\n                            raise ValueError(msg) from e\n                        else:\n                            raise\n                raw_predictions = _init_raw_predictions(X_train, self.init_, self._loss, is_classifier(self))\n            begin_at_stage = 0\n            self._rng = check_random_state(self.random_state)\n        else:\n            if self.n_estimators < self.estimators_.shape[0]:\n                raise ValueError('n_estimators=%d must be larger or equal to estimators_.shape[0]=%d when warm_start==True' % (self.n_estimators, self.estimators_.shape[0]))\n            begin_at_stage = self.estimators_.shape[0]\n            X_train = check_array(X_train, dtype=DTYPE, order='C', accept_sparse='csr', force_all_finite=False)\n            raw_predictions = self._raw_predict(X_train)\n            self._resize_state()\n        n_stages = self._fit_stages(X_train, y_train, raw_predictions, sample_weight_train, self._rng, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\n        if n_stages != self.estimators_.shape[0]:\n            self.estimators_ = self.estimators_[:n_stages]\n            self.train_score_ = self.train_score_[:n_stages]\n            if hasattr(self, 'oob_improvement_'):\n                self.oob_improvement_ = self.oob_improvement_[:n_stages]\n                self.oob_scores_ = self.oob_scores_[:n_stages]\n                self.oob_score_ = self.oob_scores_[-1]\n        self.n_estimators_ = n_stages\n        return self\n\n    def apply(self, X):\n        \"\"\"Apply trees in the ensemble to X, return leaf indices.\n\n        .. versionadded:: 0.17\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will\n            be converted to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        X_leaves : array-like of shape (n_samples, n_estimators, n_classes)\n            For each datapoint x in X and for each tree in the ensemble,\n            return the index of the leaf x ends up in each estimator.\n            In the case of binary classification n_classes is 1.\n        \"\"\"\n        self._check_initialized()\n        X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n        (n_estimators, n_classes) = self.estimators_.shape\n        leaves = np.zeros((X.shape[0], n_estimators, n_classes))\n        for i in range(n_estimators):\n            for j in range(n_classes):\n                estimator = self.estimators_[i, j]\n                leaves[:, i, j] = estimator.apply(X, check_input=False)\n        return leaves\n\n    @abstractmethod\n    def _encode_y(self, y=None, sample_weight=None):\n        \"\"\"Called by fit to validate and encode y.\"\"\"\n\n    def _set_max_features(self):\n        \"\"\"Set self.max_features_.\"\"\"\n        if isinstance(self.max_features, str):\n            if self.max_features == 'auto':\n                if is_classifier(self):\n                    max_features = max(1, int(np.sqrt(self.n_features_in_)))\n                else:\n                    max_features = self.n_features_in_\n            elif self.max_features == 'sqrt':\n                max_features = max(1, int(np.sqrt(self.n_features_in_)))\n            else:\n                max_features = max(1, int(np.log2(self.n_features_in_)))\n        elif self.max_features is None:\n            max_features = self.n_features_in_\n        elif isinstance(self.max_features, Integral):\n            max_features = self.max_features\n        else:\n            max_features = max(1, int(self.max_features * self.n_features_in_))\n        self.max_features_ = max_features\n\n    def _resize_state(self):\n        \"\"\"Add additional ``n_estimators`` entries to all attributes.\"\"\"\n        total_n_estimators = self.n_estimators\n        if total_n_estimators < self.estimators_.shape[0]:\n            raise ValueError('resize with smaller n_estimators %d < %d' % (total_n_estimators, self.estimators_[0]))\n        self.estimators_ = np.resize(self.estimators_, (total_n_estimators, self.n_trees_per_iteration_))\n        self.train_score_ = np.resize(self.train_score_, total_n_estimators)\n        if self.subsample < 1 or hasattr(self, 'oob_improvement_'):\n            if hasattr(self, 'oob_improvement_'):\n                self.oob_improvement_ = np.resize(self.oob_improvement_, total_n_estimators)\n                self.oob_scores_ = np.resize(self.oob_scores_, total_n_estimators)\n                self.oob_score_ = np.nan\n            else:\n                self.oob_improvement_ = np.zeros((total_n_estimators,), dtype=np.float64)\n                self.oob_scores_ = np.zeros((total_n_estimators,), dtype=np.float64)\n                self.oob_score_ = np.nan\n\n    def _make_estimator(self, append=True):\n        raise NotImplementedError()\n\n    @property\n    def feature_importances_(self):\n        \"\"\"The impurity-based feature importances.\n\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n        Returns\n        -------\n        feature_importances_ : ndarray of shape (n_features,)\n            The values of this array sum to 1, unless all trees are single node\n            trees consisting of only the root node, in which case it will be an\n            array of zeros.\n        \"\"\"\n        self._check_initialized()\n        relevant_trees = [tree for stage in self.estimators_ for tree in stage if tree.tree_.node_count > 1]\n        if not relevant_trees:\n            return np.zeros(shape=self.n_features_in_, dtype=np.float64)\n        relevant_feature_importances = [tree.tree_.compute_feature_importances(normalize=False) for tree in relevant_trees]\n        avg_feature_importances = np.mean(relevant_feature_importances, axis=0, dtype=np.float64)\n        return avg_feature_importances / np.sum(avg_feature_importances)\n\n    def _init_state(self):\n        \"\"\"Initialize model state and allocate model state data structures.\"\"\"\n        self.init_ = self.init\n        if self.init_ is None:\n            if is_classifier(self):\n                self.init_ = DummyClassifier(strategy='prior')\n            elif isinstance(self._loss, (AbsoluteError, HuberLoss)):\n                self.init_ = DummyRegressor(strategy='quantile', quantile=0.5)\n            elif isinstance(self._loss, PinballLoss):\n                self.init_ = DummyRegressor(strategy='quantile', quantile=self.alpha)\n            else:\n                self.init_ = DummyRegressor(strategy='mean')\n        self.estimators_ = np.empty((self.n_estimators, self.n_trees_per_iteration_), dtype=object)\n        self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)\n        if self.subsample < 1.0:\n            self.oob_improvement_ = np.zeros(self.n_estimators, dtype=np.float64)\n            self.oob_scores_ = np.zeros(self.n_estimators, dtype=np.float64)\n            self.oob_score_ = np.nan\n\n    def __iter__(self):\n        \"\"\"Return iterator over estimators in the ensemble.\"\"\"\n        return iter(self.estimators_)\n\n    @abstractmethod\n    def _get_loss(self, sample_weight):\n        \"\"\"Get loss object from sklearn._loss.loss.\"\"\"\n\n    @abstractmethod\n    def __init__(self, *, loss, learning_rate, n_estimators, criterion, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, min_impurity_decrease, init, subsample, max_features, ccp_alpha, random_state, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.loss = loss\n        self.criterion = criterion\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.subsample = subsample\n        self.max_features = max_features\n        self.max_depth = max_depth\n        self.min_impurity_decrease = min_impurity_decrease\n        self.ccp_alpha = ccp_alpha\n        self.init = init\n        self.random_state = random_state\n        self.alpha = alpha\n        self.verbose = verbose\n        self.max_leaf_nodes = max_leaf_nodes\n        self.warm_start = warm_start\n        self.validation_fraction = validation_fraction\n        self.n_iter_no_change = n_iter_no_change\n        self.tol = tol\n\n    def _is_fitted(self):\n        return len(getattr(self, 'estimators_', [])) > 0", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._gb/BaseGradientBoosting", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_gb.py", "fn_id": "", "content": "class VerboseReporter:\n    \"\"\"Reports verbose output to stdout.\n\n    Parameters\n    ----------\n    verbose : int\n        Verbosity level. If ``verbose==1`` output is printed once in a while\n        (when iteration mod verbose_mod is zero).; if larger than 1 then output\n        is printed for each update.\n    \"\"\"\n\n    def __init__(self, verbose):\n        self.verbose = verbose\n\n    def init(self, est, begin_at_stage=0):\n        \"\"\"Initialize reporter\n\n        Parameters\n        ----------\n        est : Estimator\n            The estimator\n\n        begin_at_stage : int, default=0\n            stage at which to begin reporting\n        \"\"\"\n        header_fields = ['Iter', 'Train Loss']\n        verbose_fmt = ['{iter:>10d}', '{train_score:>16.4f}']\n        if est.subsample < 1:\n            header_fields.append('OOB Improve')\n            verbose_fmt.append('{oob_impr:>16.4f}')\n        header_fields.append('Remaining Time')\n        verbose_fmt.append('{remaining_time:>16s}')\n        print(('%10s ' + '%16s ' * (len(header_fields) - 1)) % tuple(header_fields))\n        self.verbose_fmt = ' '.join(verbose_fmt)\n        self.verbose_mod = 1\n        self.start_time = time()\n        self.begin_at_stage = begin_at_stage\n\n    def update(self, j, est):\n        \"\"\"Update reporter with new iteration.\n\n        Parameters\n        ----------\n        j : int\n            The new iteration.\n        est : Estimator\n            The estimator.\n        \"\"\"\n        do_oob = est.subsample < 1\n        i = j - self.begin_at_stage\n        if (i + 1) % self.verbose_mod == 0:\n            oob_impr = est.oob_improvement_[j] if do_oob else 0\n            remaining_time = (est.n_estimators - (j + 1)) * (time() - self.start_time) / float(i + 1)\n            if remaining_time > 60:\n                remaining_time = '{0:.2f}m'.format(remaining_time / 60.0)\n            else:\n                remaining_time = '{0:.2f}s'.format(remaining_time)\n            print(self.verbose_fmt.format(iter=j + 1, train_score=est.train_score_[j], oob_impr=oob_impr, remaining_time=remaining_time))\n            if self.verbose == 1 and (i + 1) // (self.verbose_mod * 10) > 0:\n                self.verbose_mod *= 10", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._gb/VerboseReporter", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py", "fn_id": "", "content": "class HistGradientBoostingClassifier(ClassifierMixin, BaseHistGradientBoosting):\n    \"\"\"Histogram-based Gradient Boosting Classification Tree.\n\n    This estimator is much faster than\n    :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\n    for big datasets (n_samples >= 10 000).\n\n    This estimator has native support for missing values (NaNs). During\n    training, the tree grower learns at each split point whether samples\n    with missing values should go to the left or right child, based on the\n    potential gain. When predicting, samples with missing values are\n    assigned to the left or right child consequently. If no missing values\n    were encountered for a given feature during training, then samples with\n    missing values are mapped to whichever child has the most samples.\n\n    This implementation is inspired by\n    `LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n    Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n    .. versionadded:: 0.21\n\n    Parameters\n    ----------\n    loss : {'log_loss'}, default='log_loss'\n        The loss function to use in the boosting process.\n\n        For binary classification problems, 'log_loss' is also known as logistic loss,\n        binomial deviance or binary crossentropy. Internally, the model fits one tree\n        per boosting iteration and uses the logistic sigmoid function (expit) as\n        inverse link function to compute the predicted positive class probability.\n\n        For multiclass classification problems, 'log_loss' is also known as multinomial\n        deviance or categorical crossentropy. Internally, the model fits one tree per\n        boosting iteration and per class and uses the softmax function as inverse link\n        function to compute the predicted probabilities of the classes.\n\n    learning_rate : float, default=0.1\n        The learning rate, also known as *shrinkage*. This is used as a\n        multiplicative factor for the leaves values. Use ``1`` for no\n        shrinkage.\n    max_iter : int, default=100\n        The maximum number of iterations of the boosting process, i.e. the\n        maximum number of trees for binary classification. For multiclass\n        classification, `n_classes` trees per iteration are built.\n    max_leaf_nodes : int or None, default=31\n        The maximum number of leaves for each tree. Must be strictly greater\n        than 1. If None, there is no maximum limit.\n    max_depth : int or None, default=None\n        The maximum depth of each tree. The depth of a tree is the number of\n        edges to go from the root to the deepest leaf.\n        Depth isn't constrained by default.\n    min_samples_leaf : int, default=20\n        The minimum number of samples per leaf. For small datasets with less\n        than a few hundred samples, it is recommended to lower this value\n        since only very shallow trees would be built.\n    l2_regularization : float, default=0\n        The L2 regularization parameter penalizing leaves with small hessians.\n        Use ``0`` for no regularization (default).\n    max_features : float, default=1.0\n        Proportion of randomly chosen features in each and every node split.\n        This is a form of regularization, smaller values make the trees weaker\n        learners and might prevent overfitting.\n        If interaction constraints from `interaction_cst` are present, only allowed\n        features are taken into account for the subsampling.\n\n        .. versionadded:: 1.4\n\n    max_bins : int, default=255\n        The maximum number of bins to use for non-missing values. Before\n        training, each feature of the input array `X` is binned into\n        integer-valued bins, which allows for a much faster training stage.\n        Features with a small number of unique values may use less than\n        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n        is always reserved for missing values. Must be no larger than 255.\n    categorical_features : array-like of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,), default=None\n        Indicates the categorical features.\n\n        - None : no feature will be considered categorical.\n        - boolean array-like : boolean mask indicating categorical features.\n        - integer array-like : integer indices indicating categorical\n          features.\n        - str array-like: names of categorical features (assuming the training\n          data has feature names).\n        - `\"from_dtype\"`: dataframe columns with dtype \"category\" are\n          considered to be categorical features. The input must be an object\n          exposing a ``__dataframe__`` method such as pandas or polars\n          DataFrames to use this feature.\n\n        For each categorical feature, there must be at most `max_bins` unique\n        categories. Negative values for categorical features encoded as numeric\n        dtypes are treated as missing values. All categorical values are\n        converted to floating point numbers. This means that categorical values\n        of 1.0 and 1 are treated as the same category.\n\n        Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n        .. versionadded:: 0.24\n\n        .. versionchanged:: 1.2\n           Added support for feature names.\n\n        .. versionchanged:: 1.4\n           Added `\"from_dtype\"` option. The default will change to `\"from_dtype\"` in\n           v1.6.\n\n    monotonic_cst : array-like of int of shape (n_features) or dict, default=None\n        Monotonic constraint to enforce on each feature are specified using the\n        following integer values:\n\n        - 1: monotonic increase\n        - 0: no constraint\n        - -1: monotonic decrease\n\n        If a dict with str keys, map feature to monotonic constraints by name.\n        If an array, the features are mapped to constraints by position. See\n        :ref:`monotonic_cst_features_names` for a usage example.\n\n        The constraints are only valid for binary classifications and hold\n        over the probability of the positive class.\n        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n        .. versionadded:: 0.23\n\n        .. versionchanged:: 1.2\n           Accept dict of constraints with feature names as keys.\n\n    interaction_cst : {\"pairwise\", \"no_interactions\"} or sequence of lists/tuples/sets             of int, default=None\n        Specify interaction constraints, the sets of features which can\n        interact with each other in child node splits.\n\n        Each item specifies the set of feature indices that are allowed\n        to interact with each other. If there are more features than\n        specified in these constraints, they are treated as if they were\n        specified as an additional set.\n\n        The strings \"pairwise\" and \"no_interactions\" are shorthands for\n        allowing only pairwise or no interactions, respectively.\n\n        For instance, with 5 features in total, `interaction_cst=[{0, 1}]`\n        is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`,\n        and specifies that each branch of a tree will either only split\n        on features 0 and 1 or only split on features 2, 3 and 4.\n\n        .. versionadded:: 1.2\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble. For results to be valid, the\n        estimator should be re-trained on the same data only.\n        See :term:`the Glossary <warm_start>`.\n    early_stopping : 'auto' or bool, default='auto'\n        If 'auto', early stopping is enabled if the sample size is larger than\n        10000. If True, early stopping is enabled, otherwise early stopping is\n        disabled.\n\n        .. versionadded:: 0.23\n\n    scoring : str or callable or None, default='loss'\n        Scoring parameter to use for early stopping. It can be a single\n        string (see :ref:`scoring_parameter`) or a callable (see\n        :ref:`scoring`). If None, the estimator's default scorer\n        is used. If ``scoring='loss'``, early stopping is checked\n        w.r.t the loss value. Only used if early stopping is performed.\n    validation_fraction : int or float or None, default=0.1\n        Proportion (or absolute size) of training data to set aside as\n        validation data for early stopping. If None, early stopping is done on\n        the training data. Only used if early stopping is performed.\n    n_iter_no_change : int, default=10\n        Used to determine when to \"early stop\". The fitting process is\n        stopped when none of the last ``n_iter_no_change`` scores are better\n        than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n        tolerance. Only used if early stopping is performed.\n    tol : float, default=1e-7\n        The absolute tolerance to use when comparing scores. The higher the\n        tolerance, the more likely we are to early stop: higher tolerance\n        means that it will be harder for subsequent iterations to be\n        considered an improvement upon the reference score.\n    verbose : int, default=0\n        The verbosity level. If not zero, print some information about the\n        fitting process.\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the subsampling in the\n        binning process, and the train/validation data split if early stopping\n        is enabled.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form `{class_label: weight}`.\n        If not given, all classes are supposed to have weight one.\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as `n_samples / (n_classes * np.bincount(y))`.\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if `sample_weight` is specified.\n\n        .. versionadded:: 1.2\n\n    Attributes\n    ----------\n    classes_ : array, shape = (n_classes,)\n        Class labels.\n    do_early_stopping_ : bool\n        Indicates whether early stopping is used during training.\n    n_iter_ : int\n        The number of iterations as selected by early stopping, depending on\n        the `early_stopping` parameter. Otherwise it corresponds to max_iter.\n    n_trees_per_iteration_ : int\n        The number of tree that are built at each iteration. This is equal to 1\n        for binary classification, and to ``n_classes`` for multiclass\n        classification.\n    train_score_ : ndarray, shape (n_iter_+1,)\n        The scores at each iteration on the training data. The first entry\n        is the score of the ensemble before the first iteration. Scores are\n        computed according to the ``scoring`` parameter. If ``scoring`` is\n        not 'loss', scores are computed on a subset of at most 10 000\n        samples. Empty if no early stopping.\n    validation_score_ : ndarray, shape (n_iter_+1,)\n        The scores at each iteration on the held-out validation data. The\n        first entry is the score of the ensemble before the first iteration.\n        Scores are computed according to the ``scoring`` parameter. Empty if\n        no early stopping or if ``validation_fraction`` is None.\n    is_categorical_ : ndarray, shape (n_features, ) or None\n        Boolean mask for the categorical features. ``None`` if there are no\n        categorical features.\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    GradientBoostingClassifier : Exact gradient boosting method that does not\n        scale as good on datasets with a large number of samples.\n    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n    RandomForestClassifier : A meta-estimator that fits a number of decision\n        tree classifiers on various sub-samples of the dataset and uses\n        averaging to improve the predictive accuracy and control over-fitting.\n    AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n        on the original dataset and then fits additional copies of the\n        classifier on the same dataset where the weights of incorrectly\n        classified instances are adjusted such that subsequent classifiers\n        focus more on difficult cases.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import HistGradientBoostingClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = HistGradientBoostingClassifier().fit(X, y)\n    >>> clf.score(X, y)\n    1.0\n    \"\"\"\n    _parameter_constraints: dict = {**BaseHistGradientBoosting._parameter_constraints, 'loss': [StrOptions({'log_loss'}), BaseLoss], 'class_weight': [dict, StrOptions({'balanced'}), None]}\n\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        decision : ndarray, shape (n_samples,) or                 (n_samples, n_trees_per_iteration)\n            The raw predicted values (i.e. the sum of the trees leaves) for\n            each sample. n_trees_per_iteration is equal to the number of\n            classes in multiclass classification.\n        \"\"\"\n        decision = self._raw_predict(X)\n        if decision.shape[1] == 1:\n            decision = decision.ravel()\n        return decision\n\n    def _staged_raw_predict(self, X):\n        \"\"\"Compute raw predictions of ``X`` for each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        ------\n        raw_predictions : generator of ndarray of shape             (n_samples, n_trees_per_iteration)\n            The raw predictions of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._preprocess_X(X, reset=False)\n        if X.shape[1] != self._n_features:\n            raise ValueError('X has {} features but this estimator was trained with {} features.'.format(X.shape[1], self._n_features))\n        n_samples = X.shape[0]\n        raw_predictions = np.zeros(shape=(n_samples, self.n_trees_per_iteration_), dtype=self._baseline_prediction.dtype, order='F')\n        raw_predictions += self._baseline_prediction\n        n_threads = _openmp_effective_n_threads()\n        for iteration in range(len(self._predictors)):\n            self._predict_iterations(X, self._predictors[iteration:iteration + 1], raw_predictions, is_binned=False, n_threads=n_threads)\n            yield raw_predictions.copy()\n\n    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        y : ndarray, shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        raw_predictions = self._raw_predict(X)\n        if raw_predictions.shape[1] == 1:\n            encoded_classes = (raw_predictions.ravel() > 0).astype(int)\n        else:\n            encoded_classes = np.argmax(raw_predictions, axis=1)\n        return self.classes_[encoded_classes]\n\n    def staged_decision_function(self, X):\n        \"\"\"Compute decision function of ``X`` for each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        ------\n        decision : generator of ndarray of shape (n_samples,) or                 (n_samples, n_trees_per_iteration)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        for staged_decision in self._staged_raw_predict(X):\n            if staged_decision.shape[1] == 1:\n                staged_decision = staged_decision.ravel()\n            yield staged_decision\n\n    def _get_loss(self, sample_weight):\n        if self.n_trees_per_iteration_ == 1:\n            return HalfBinomialLoss(sample_weight=sample_weight)\n        else:\n            return HalfMultinomialLoss(sample_weight=sample_weight, n_classes=self.n_trees_per_iteration_)\n\n    def staged_predict(self, X):\n        \"\"\"Predict classes at each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        .. versionadded:: 0.24\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted classes of the input samples, for each iteration.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            if raw_predictions.shape[1] == 1:\n                encoded_classes = (raw_predictions.ravel() > 0).astype(int)\n            else:\n                encoded_classes = np.argmax(raw_predictions, axis=1)\n            yield self.classes_.take(encoded_classes, axis=0)\n\n    def _check_categorical_features(self, X):\n        \"\"\"Check and validate categorical features in X\n\n        Parameters\n        ----------\n        X : {array-like, pandas DataFrame} of shape (n_samples, n_features)\n            Input data.\n\n        Return\n        ------\n        is_categorical : ndarray of shape (n_features,) or None, dtype=bool\n            Indicates whether a feature is categorical. If no feature is\n            categorical, this is None.\n        \"\"\"\n        if _is_pandas_df(X):\n            X_is_dataframe = True\n            categorical_columns_mask = np.asarray(X.dtypes == 'category')\n            X_has_categorical_columns = categorical_columns_mask.any()\n        elif hasattr(X, '__dataframe__'):\n            X_is_dataframe = True\n            categorical_columns_mask = np.asarray([c.dtype[0].name == 'CATEGORICAL' for c in X.__dataframe__().get_columns()])\n            X_has_categorical_columns = categorical_columns_mask.any()\n        else:\n            X_is_dataframe = False\n            categorical_columns_mask = None\n            X_has_categorical_columns = False\n        if isinstance(self.categorical_features, str) and self.categorical_features == 'warn':\n            if X_has_categorical_columns:\n                warnings.warn(\"The categorical_features parameter will change to 'from_dtype' in v1.6. The 'from_dtype' option automatically treats categorical dtypes in a DataFrame as categorical features.\", FutureWarning)\n            categorical_features = None\n        else:\n            categorical_features = self.categorical_features\n        categorical_by_dtype = isinstance(categorical_features, str) and categorical_features == 'from_dtype'\n        no_categorical_dtype = categorical_features is None or (categorical_by_dtype and (not X_is_dataframe))\n        if no_categorical_dtype:\n            return None\n        use_pandas_categorical = categorical_by_dtype and X_is_dataframe\n        if use_pandas_categorical:\n            categorical_features = categorical_columns_mask\n        else:\n            categorical_features = np.asarray(categorical_features)\n        if categorical_features.size == 0:\n            return None\n        if categorical_features.dtype.kind not in ('i', 'b', 'U', 'O'):\n            raise ValueError(f'categorical_features must be an array-like of bool, int or str, got: {categorical_features.dtype.name}.')\n        if categorical_features.dtype.kind == 'O':\n            types = set((type(f) for f in categorical_features))\n            if types != {str}:\n                raise ValueError(f\"categorical_features must be an array-like of bool, int or str, got: {', '.join(sorted((t.__name__ for t in types)))}.\")\n        n_features = X.shape[1]\n        feature_names_in_ = getattr(X, 'columns', None)\n        if categorical_features.dtype.kind in ('U', 'O'):\n            if feature_names_in_ is None:\n                raise ValueError('categorical_features should be passed as an array of integers or as a boolean mask when the model is fitted on data without feature names.')\n            is_categorical = np.zeros(n_features, dtype=bool)\n            feature_names = list(feature_names_in_)\n            for feature_name in categorical_features:\n                try:\n                    is_categorical[feature_names.index(feature_name)] = True\n                except ValueError as e:\n                    raise ValueError(f\"categorical_features has a item value '{feature_name}' which is not a valid feature name of the training data. Observed feature names: {feature_names}\") from e\n        elif categorical_features.dtype.kind == 'i':\n            if np.max(categorical_features) >= n_features or np.min(categorical_features) < 0:\n                raise ValueError('categorical_features set as integer indices must be in [0, n_features - 1]')\n            is_categorical = np.zeros(n_features, dtype=bool)\n            is_categorical[categorical_features] = True\n        else:\n            if categorical_features.shape[0] != n_features:\n                raise ValueError(f'categorical_features set as a boolean mask must have shape (n_features,), got: {categorical_features.shape}')\n            is_categorical = categorical_features\n        if not np.any(is_categorical):\n            return None\n        return is_categorical\n\n    def staged_predict_proba(self, X):\n        \"\"\"Predict class probabilities at each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted class probabilities of the input samples,\n            for each iteration.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            yield self._loss.predict_proba(raw_predictions)\n\n    def _finalize_sample_weight(self, sample_weight, y):\n        \"\"\"Adjust sample_weights with class_weights.\"\"\"\n        if self.class_weight is None:\n            return sample_weight\n        expanded_class_weight = compute_sample_weight(self.class_weight, y)\n        if sample_weight is not None:\n            return sample_weight * expanded_class_weight\n        else:\n            return expanded_class_weight\n\n    def _encode_y(self, y):\n        check_classification_targets(y)\n        label_encoder = LabelEncoder()\n        encoded_y = label_encoder.fit_transform(y)\n        self.classes_ = label_encoder.classes_\n        n_classes = self.classes_.shape[0]\n        self.n_trees_per_iteration_ = 1 if n_classes <= 2 else n_classes\n        encoded_y = encoded_y.astype(Y_DTYPE, copy=False)\n        return encoded_y\n\n    def __init__(self, loss='log_loss', *, learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_features=1.0, max_bins=255, categorical_features='warn', monotonic_cst=None, interaction_cst=None, warm_start=False, early_stopping='auto', scoring='loss', validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None, class_weight=None):\n        super(HistGradientBoostingClassifier, self).__init__(loss=loss, learning_rate=learning_rate, max_iter=max_iter, max_leaf_nodes=max_leaf_nodes, max_depth=max_depth, min_samples_leaf=min_samples_leaf, l2_regularization=l2_regularization, max_features=max_features, max_bins=max_bins, categorical_features=categorical_features, monotonic_cst=monotonic_cst, interaction_cst=interaction_cst, warm_start=warm_start, early_stopping=early_stopping, scoring=scoring, validation_fraction=validation_fraction, n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose, random_state=random_state)\n        self.class_weight = class_weight\n\n    def _check_interaction_cst(self, n_features):\n        \"\"\"Check and validation for interaction constraints.\"\"\"\n        if self.interaction_cst is None:\n            return None\n        if self.interaction_cst == 'no_interactions':\n            interaction_cst = [[i] for i in range(n_features)]\n        elif self.interaction_cst == 'pairwise':\n            interaction_cst = itertools.combinations(range(n_features), 2)\n        else:\n            interaction_cst = self.interaction_cst\n        try:\n            constraints = [set(group) for group in interaction_cst]\n        except TypeError:\n            raise ValueError(f'Interaction constraints must be a sequence of tuples or lists, got: {self.interaction_cst!r}.')\n        for group in constraints:\n            for x in group:\n                if not (isinstance(x, Integral) and 0 <= x < n_features):\n                    raise ValueError(f'Interaction constraints must consist of integer indices in [0, n_features - 1] = [0, {n_features - 1}], specifying the position of features, got invalid indices: {group!r}')\n        rest = set(range(n_features)) - set().union(*constraints)\n        if len(rest) > 0:\n            constraints.append(rest)\n        return constraints\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        p : ndarray, shape (n_samples, n_classes)\n            The class probabilities of the input samples.\n        \"\"\"\n        raw_predictions = self._raw_predict(X)\n        return self._loss.predict_proba(raw_predictions)", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_hist_gradient_boosting/grower.py", "fn_id": "", "content": "class TreeNode:\n    \"\"\"Tree Node class used in TreeGrower.\n\n    This isn't used for prediction purposes, only for training (see\n    TreePredictor).\n\n    Parameters\n    ----------\n    depth : int\n        The depth of the node, i.e. its distance from the root.\n    sample_indices : ndarray of shape (n_samples_at_node,), dtype=np.uint32\n        The indices of the samples at the node.\n    partition_start : int\n        start position of the node's sample_indices in splitter.partition.\n    partition_stop : int\n        stop position of the node's sample_indices in splitter.partition.\n    sum_gradients : float\n        The sum of the gradients of the samples at the node.\n    sum_hessians : float\n        The sum of the hessians of the samples at the node.\n\n    Attributes\n    ----------\n    depth : int\n        The depth of the node, i.e. its distance from the root.\n    sample_indices : ndarray of shape (n_samples_at_node,), dtype=np.uint32\n        The indices of the samples at the node.\n    sum_gradients : float\n        The sum of the gradients of the samples at the node.\n    sum_hessians : float\n        The sum of the hessians of the samples at the node.\n    split_info : SplitInfo or None\n        The result of the split evaluation.\n    is_leaf : bool\n        True if node is a leaf\n    left_child : TreeNode or None\n        The left child of the node. None for leaves.\n    right_child : TreeNode or None\n        The right child of the node. None for leaves.\n    value : float or None\n        The value of the leaf, as computed in finalize_leaf(). None for\n        non-leaf nodes.\n    partition_start : int\n        start position of the node's sample_indices in splitter.partition.\n    partition_stop : int\n        stop position of the node's sample_indices in splitter.partition.\n    allowed_features : None or ndarray, dtype=int\n        Indices of features allowed to split for children.\n    interaction_cst_indices : None or list of ints\n        Indices of the interaction sets that have to be applied on splits of\n        child nodes. The fewer sets the stronger the constraint as fewer sets\n        contain fewer features.\n    children_lower_bound : float\n    children_upper_bound : float\n    \"\"\"\n\n    def __init__(self, *, depth, sample_indices, partition_start, partition_stop, sum_gradients, sum_hessians, value=None):\n        self.depth = depth\n        self.sample_indices = sample_indices\n        self.n_samples = sample_indices.shape[0]\n        self.sum_gradients = sum_gradients\n        self.sum_hessians = sum_hessians\n        self.value = value\n        self.is_leaf = False\n        self.allowed_features = None\n        self.interaction_cst_indices = None\n        self.set_children_bounds(float('-inf'), float('+inf'))\n        self.split_info = None\n        self.left_child = None\n        self.right_child = None\n        self.histograms = None\n        self.partition_start = partition_start\n        self.partition_stop = partition_stop\n\n    def set_children_bounds(self, lower, upper):\n        \"\"\"Set children values bounds to respect monotonic constraints.\"\"\"\n        self.children_lower_bound = lower\n        self.children_upper_bound = upper\n\n    def __lt__(self, other_node):\n        \"\"\"Comparison for priority queue.\n\n        Nodes with high gain are higher priority than nodes with low gain.\n\n        heapq.heappush only need the '<' operator.\n        heapq.heappop take the smallest item first (smaller is higher\n        priority).\n\n        Parameters\n        ----------\n        other_node : TreeNode\n            The node to compare with.\n        \"\"\"\n        return self.split_info.gain > other_node.split_info.gain", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._hist_gradient_boosting.grower/TreeNode", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_stacking.py", "fn_id": "", "content": "class StackingClassifier(_RoutingNotSupportedMixin, ClassifierMixin, _BaseStacking):\n    \"\"\"Stack of estimators with a final classifier.\n\n    Stacked generalization consists in stacking the output of individual\n    estimator and use a classifier to compute the final prediction. Stacking\n    allows to use the strength of each individual estimator by using their\n    output as input of a final estimator.\n\n    Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n    is trained using cross-validated predictions of the base estimators using\n    `cross_val_predict`.\n\n    Read more in the :ref:`User Guide <stacking>`.\n\n    .. versionadded:: 0.22\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator)\n        Base estimators which will be stacked together. Each element of the\n        list is defined as a tuple of string (i.e. name) and an estimator\n        instance. An estimator can be set to 'drop' using `set_params`.\n\n        The type of estimator is generally expected to be a classifier.\n        However, one can pass a regressor for some use case (e.g. ordinal\n        regression).\n\n    final_estimator : estimator, default=None\n        A classifier which will be used to combine the base estimators.\n        The default classifier is a\n        :class:`~sklearn.linear_model.LogisticRegression`.\n\n    cv : int, cross-validation generator, iterable, or \"prefit\", default=None\n        Determines the cross-validation splitting strategy used in\n        `cross_val_predict` to train `final_estimator`. Possible inputs for\n        cv are:\n\n        * None, to use the default 5-fold cross validation,\n        * integer, to specify the number of folds in a (Stratified) KFold,\n        * An object to be used as a cross-validation generator,\n        * An iterable yielding train, test splits,\n        * `\"prefit\"` to assume the `estimators` are prefit. In this case, the\n          estimators will not be refitted.\n\n        For integer/None inputs, if the estimator is a classifier and y is\n        either binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used.\n        In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n        These splitters are instantiated with `shuffle=False` so the splits\n        will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        If \"prefit\" is passed, it is assumed that all `estimators` have\n        been fitted already. The `final_estimator_` is trained on the `estimators`\n        predictions on the full training set and are **not** cross validated\n        predictions. Please note that if the models have been trained on the same\n        data to train the stacking model, there is a very high risk of overfitting.\n\n        .. versionadded:: 1.1\n            The 'prefit' option was added in 1.1\n\n        .. note::\n           A larger number of split will provide no benefits if the number\n           of training samples is large enough. Indeed, the training time\n           will increase. ``cv`` is not used for model evaluation but for\n           prediction.\n\n    stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'},             default='auto'\n        Methods called for each base estimator. It can be:\n\n        * if 'auto', it will try to invoke, for each estimator,\n          `'predict_proba'`, `'decision_function'` or `'predict'` in that\n          order.\n        * otherwise, one of `'predict_proba'`, `'decision_function'` or\n          `'predict'`. If the method is not implemented by the estimator, it\n          will raise an error.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel all `estimators` `fit`.\n        `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n        using all processors. See Glossary for more details.\n\n    passthrough : bool, default=False\n        When False, only the predictions of estimators will be used as\n        training data for `final_estimator`. When True, the\n        `final_estimator` is trained on the predictions as well as the\n        original training data.\n\n    verbose : int, default=0\n        Verbosity level.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,) or list of ndarray if `y`         is of type `\"multilabel-indicator\"`.\n        Class labels.\n\n    estimators_ : list of estimators\n        The elements of the `estimators` parameter, having been fitted on the\n        training data. If an estimator has been set to `'drop'`, it\n        will not appear in `estimators_`. When `cv=\"prefit\"`, `estimators_`\n        is set to `estimators` and is not fitted again.\n\n    named_estimators_ : :class:`~sklearn.utils.Bunch`\n        Attribute to access any fitted sub-estimators by name.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying classifier exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimators expose such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    final_estimator_ : estimator\n        The classifier which predicts given the output of `estimators_`.\n\n    stack_method_ : list of str\n        The method used by each base estimator.\n\n    See Also\n    --------\n    StackingRegressor : Stack of estimators with a final regressor.\n\n    Notes\n    -----\n    When `predict_proba` is used by each estimator (i.e. most of the time for\n    `stack_method='auto'` or specifically for `stack_method='predict_proba'`),\n    The first column predicted by each estimator will be dropped in the case\n    of a binary classification problem. Indeed, both feature will be perfectly\n    collinear.\n\n    In some cases (e.g. ordinal regression), one can pass regressors as the\n    first layer of the :class:`StackingClassifier`. However, note that `y` will\n    be internally encoded in a numerically increasing order or lexicographic\n    order. If this ordering is not adequate, one should manually numerically\n    encode the classes in the desired order.\n\n    References\n    ----------\n    .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n       (1992): 241-259.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.svm import LinearSVC\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.ensemble import StackingClassifier\n    >>> X, y = load_iris(return_X_y=True)\n    >>> estimators = [\n    ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n    ...     ('svr', make_pipeline(StandardScaler(),\n    ...                           LinearSVC(random_state=42)))\n    ... ]\n    >>> clf = StackingClassifier(\n    ...     estimators=estimators, final_estimator=LogisticRegression()\n    ... )\n    >>> from sklearn.model_selection import train_test_split\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, stratify=y, random_state=42\n    ... )\n    >>> clf.fit(X_train, y_train).score(X_test, y_test)\n    0.9...\n    \"\"\"\n    _parameter_constraints: dict = {**_BaseStacking._parameter_constraints, 'stack_method': [StrOptions({'auto', 'predict_proba', 'decision_function', 'predict'})]}\n\n    def _concatenate_predictions(self, X, predictions):\n        \"\"\"Concatenate the predictions of each first layer learner and\n        possibly the input dataset `X`.\n\n        If `X` is sparse and `self.passthrough` is False, the output of\n        `transform` will be dense (the predictions). If `X` is sparse\n        and `self.passthrough` is True, the output of `transform` will\n        be sparse.\n\n        This helper is in charge of ensuring the predictions are 2D arrays and\n        it will drop one of the probability column when using probabilities\n        in the binary case. Indeed, the p(y|c=0) = 1 - p(y|c=1)\n\n        When `y` type is `\"multilabel-indicator\"`` and the method used is\n        `predict_proba`, `preds` can be either a `ndarray` of shape\n        `(n_samples, n_class)` or for some estimators a list of `ndarray`.\n        This function will drop one of the probability column in this situation as well.\n        \"\"\"\n        X_meta = []\n        for (est_idx, preds) in enumerate(predictions):\n            if isinstance(preds, list):\n                for pred in preds:\n                    X_meta.append(pred[:, 1:])\n            elif preds.ndim == 1:\n                X_meta.append(preds.reshape(-1, 1))\n            elif self.stack_method_[est_idx] == 'predict_proba' and len(self.classes_) == 2:\n                X_meta.append(preds[:, 1:])\n            else:\n                X_meta.append(preds)\n        self._n_feature_outs = [pred.shape[1] for pred in X_meta]\n        if self.passthrough:\n            X_meta.append(X)\n            if sparse.issparse(X):\n                return sparse.hstack(X_meta, format=X.format)\n        return np.hstack(X_meta)\n\n    def _validate_estimators(self):\n        \"\"\"Overload the method of `_BaseHeterogeneousEnsemble` to be more\n        lenient towards the type of `estimators`.\n\n        Regressors can be accepted for some cases such as ordinal regression.\n        \"\"\"\n        if len(self.estimators) == 0:\n            raise ValueError(\"Invalid 'estimators' attribute, 'estimators' should be a non-empty list of (string, estimator) tuples.\")\n        (names, estimators) = zip(*self.estimators)\n        self._validate_names(names)\n        has_estimator = any((est != 'drop' for est in estimators))\n        if not has_estimator:\n            raise ValueError('All estimators are dropped. At least one is required to be an estimator.')\n        return (names, estimators)\n\n    def _clone_final_estimator(self, default):\n        if self.final_estimator is not None:\n            self.final_estimator_ = clone(self.final_estimator)\n        else:\n            self.final_estimator_ = clone(default)\n\n    def _sk_visual_block_(self):\n        if self.final_estimator is None:\n            final_estimator = LogisticRegression()\n        else:\n            final_estimator = self.final_estimator\n        return super()._sk_visual_block_with_final_estimator(final_estimator)\n\n    @available_if(_estimator_has('predict'))\n    def predict(self, X, **predict_params):\n        \"\"\"Predict target for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        **predict_params : dict of str -> obj\n            Parameters to the `predict` called by the `final_estimator`. Note\n            that this may be used to return uncertainties from some estimators\n            with `return_std` or `return_cov`. Be aware that it will only\n            accounts for uncertainty in the final estimator.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n            Predicted targets.\n        \"\"\"\n        y_pred = super().predict(X, **predict_params)\n        if isinstance(self._label_encoder, list):\n            y_pred = np.array([self._label_encoder[target_idx].inverse_transform(target) for (target_idx, target) in enumerate(y_pred.T)]).T\n        else:\n            y_pred = self._label_encoder.inverse_transform(y_pred)\n        return y_pred\n\n    @available_if(_estimator_has('predict_proba'))\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for `X` using the final estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        probabilities : ndarray of shape (n_samples, n_classes) or             list of ndarray of shape (n_output,)\n            The class probabilities of the input samples.\n        \"\"\"\n        check_is_fitted(self)\n        y_pred = self.final_estimator_.predict_proba(self.transform(X))\n        if isinstance(self._label_encoder, list):\n            y_pred = np.array([preds[:, 0] for preds in y_pred]).T\n        return y_pred\n\n    @available_if(_estimator_has('decision_function'))\n    def decision_function(self, X):\n        \"\"\"Decision function for samples in `X` using the final estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        decisions : ndarray of shape (n_samples,), (n_samples, n_classes),             or (n_samples, n_classes * (n_classes-1) / 2)\n            The decision function computed the final estimator.\n        \"\"\"\n        check_is_fitted(self)\n        return self.final_estimator_.decision_function(self.transform(X))\n\n    def transform(self, X):\n        \"\"\"Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        y_preds : ndarray of shape (n_samples, n_estimators) or                 (n_samples, n_classes * n_estimators)\n            Prediction outputs for each estimator.\n        \"\"\"\n        return self._transform(X)\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values. Note that `y` will be internally encoded in\n            numerically increasing order or lexicographic order. If the order\n            matter (e.g. for ordinal regression), one should numerically encode\n            the target `y` before calling :term:`fit`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n        Returns\n        -------\n        self : object\n            Returns a fitted instance of estimator.\n        \"\"\"\n        _raise_for_unsupported_routing(self, 'fit', sample_weight=sample_weight)\n        check_classification_targets(y)\n        if type_of_target(y) == 'multilabel-indicator':\n            self._label_encoder = [LabelEncoder().fit(yk) for yk in y.T]\n            self.classes_ = [le.classes_ for le in self._label_encoder]\n            y_encoded = np.array([self._label_encoder[target_idx].transform(target) for (target_idx, target) in enumerate(y.T)]).T\n        else:\n            self._label_encoder = LabelEncoder().fit(y)\n            self.classes_ = self._label_encoder.classes_\n            y_encoded = self._label_encoder.transform(y)\n        return super().fit(X, y_encoded, sample_weight)\n\n    def __init__(self, estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0):\n        super().__init__(estimators=estimators, final_estimator=final_estimator, cv=cv, stack_method=stack_method, n_jobs=n_jobs, passthrough=passthrough, verbose=verbose)\n\n    def _validate_final_estimator(self):\n        self._clone_final_estimator(default=LogisticRegression())\n        if not is_classifier(self.final_estimator_):\n            raise ValueError(\"'final_estimator' parameter should be a classifier. Got {}\".format(self.final_estimator_))\n\n    def _sk_visual_block_with_final_estimator(self, final_estimator):\n        (names, estimators) = zip(*self.estimators)\n        parallel = _VisualBlock('parallel', estimators, names=names, dash_wrapped=False)\n        final_block = _VisualBlock('parallel', [final_estimator], names=['final_estimator'], dash_wrapped=False)\n        return _VisualBlock('serial', (parallel, final_block), dash_wrapped=False)", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._stacking/StackingClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_voting.py", "fn_id": "", "content": "class VotingClassifier(ClassifierMixin, _BaseVoting):\n    \"\"\"Soft Voting/Majority Rule classifier for unfitted estimators.\n\n    Read more in the :ref:`User Guide <voting_classifier>`.\n\n    .. versionadded:: 0.17\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator) tuples\n        Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n        of those original estimators that will be stored in the class attribute\n        ``self.estimators_``. An estimator can be set to ``'drop'`` using\n        :meth:`set_params`.\n\n        .. versionchanged:: 0.21\n            ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n            support was removed in 0.24.\n\n    voting : {'hard', 'soft'}, default='hard'\n        If 'hard', uses predicted class labels for majority rule voting.\n        Else if 'soft', predicts the class label based on the argmax of\n        the sums of the predicted probabilities, which is recommended for\n        an ensemble of well-calibrated classifiers.\n\n    weights : array-like of shape (n_classifiers,), default=None\n        Sequence of weights (`float` or `int`) to weight the occurrences of\n        predicted class labels (`hard` voting) or class probabilities\n        before averaging (`soft` voting). Uses uniform weights if `None`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for ``fit``.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    flatten_transform : bool, default=True\n        Affects shape of transform output only when voting='soft'\n        If voting='soft' and flatten_transform=True, transform method returns\n        matrix with shape (n_samples, n_classifiers * n_classes). If\n        flatten_transform=False, it returns\n        (n_classifiers, n_samples, n_classes).\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting will be printed as it\n        is completed.\n\n        .. versionadded:: 0.23\n\n    Attributes\n    ----------\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators as defined in ``estimators``\n        that are not 'drop'.\n\n    named_estimators_ : :class:`~sklearn.utils.Bunch`\n        Attribute to access any fitted sub-estimators by name.\n\n        .. versionadded:: 0.20\n\n    le_ : :class:`~sklearn.preprocessing.LabelEncoder`\n        Transformer used to encode the labels during fit and decode during\n        prediction.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying classifier exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimators expose such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    VotingRegressor : Prediction voting regressor.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n    >>> clf1 = LogisticRegression(random_state=1)\n    >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    >>> clf3 = GaussianNB()\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> eclf1 = VotingClassifier(estimators=[\n    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n    >>> eclf1 = eclf1.fit(X, y)\n    >>> print(eclf1.predict(X))\n    [1 1 1 2 2 2]\n    >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n    ...                eclf1.named_estimators_['lr'].predict(X))\n    True\n    >>> eclf2 = VotingClassifier(estimators=[\n    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    ...         voting='soft')\n    >>> eclf2 = eclf2.fit(X, y)\n    >>> print(eclf2.predict(X))\n    [1 1 1 2 2 2]\n\n    To drop an estimator, :meth:`set_params` can be used to remove it. Here we\n    dropped one of the estimators, resulting in 2 fitted estimators:\n\n    >>> eclf2 = eclf2.set_params(lr='drop')\n    >>> eclf2 = eclf2.fit(X, y)\n    >>> len(eclf2.estimators_)\n    2\n\n    Setting `flatten_transform=True` with `voting='soft'` flattens output shape of\n    `transform`:\n\n    >>> eclf3 = VotingClassifier(estimators=[\n    ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    ...        voting='soft', weights=[2,1,1],\n    ...        flatten_transform=True)\n    >>> eclf3 = eclf3.fit(X, y)\n    >>> print(eclf3.predict(X))\n    [1 1 1 2 2 2]\n    >>> print(eclf3.transform(X).shape)\n    (6, 6)\n    \"\"\"\n    _parameter_constraints: dict = {**_BaseVoting._parameter_constraints, 'voting': [StrOptions({'hard', 'soft'})], 'flatten_transform': ['boolean']}\n\n    def _check_voting(self):\n        if self.voting == 'hard':\n            raise AttributeError(f'predict_proba is not available when voting={repr(self.voting)}')\n        return True\n\n    @_fit_context(prefer_skip_nested_validation=False)\n    @_deprecate_positional_args(version='1.7')\n    def fit(self, X, y, *, sample_weight=None, **fit_params):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n            .. versionadded:: 0.18\n\n        **fit_params : dict\n            Parameters to pass to the underlying estimators.\n\n            .. versionadded:: 1.5\n\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        _raise_for_params(fit_params, self, 'fit')\n        y_type = type_of_target(y, input_name='y')\n        if y_type in ('unknown', 'continuous'):\n            raise ValueError(f'Unknown label type: {y_type}. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.')\n        elif y_type not in ('binary', 'multiclass'):\n            raise NotImplementedError(f'{self.__class__.__name__} only supports binary or multiclass classification. Multilabel and multi-output classification are not supported.')\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        transformed_y = self.le_.transform(y)\n        if sample_weight is not None:\n            fit_params['sample_weight'] = sample_weight\n        return super().fit(X, transformed_y, **fit_params)\n\n    def __init__(self, estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False):\n        super().__init__(estimators=estimators)\n        self.voting = voting\n        self.weights = weights\n        self.n_jobs = n_jobs\n        self.flatten_transform = flatten_transform\n        self.verbose = verbose\n\n    def _log_message(self, name, idx, total):\n        if not self.verbose:\n            return None\n        return f'({idx} of {total}) Processing {name}'\n\n    def _predict(self, X):\n        \"\"\"Collect results from clf.predict calls.\"\"\"\n        return np.asarray([est.predict(X) for est in self.estimators_]).T\n\n    @available_if(_check_voting)\n    def predict_proba(self, X):\n        \"\"\"Compute probabilities of possible outcomes for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        avg : array-like of shape (n_samples, n_classes)\n            Weighted average probability for each class per sample.\n        \"\"\"\n        check_is_fitted(self)\n        avg = np.average(self._collect_probas(X), axis=0, weights=self._weights_not_none)\n        return avg\n\n    def transform(self, X):\n        \"\"\"Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        probabilities_or_labels\n            If `voting='soft'` and `flatten_transform=True`:\n                returns ndarray of shape (n_samples, n_classifiers * n_classes),\n                being class probabilities calculated by each classifier.\n            If `voting='soft' and `flatten_transform=False`:\n                ndarray of shape (n_classifiers, n_samples, n_classes)\n            If `voting='hard'`:\n                ndarray of shape (n_samples, n_classifiers), being\n                class labels predicted by each classifier.\n        \"\"\"\n        check_is_fitted(self)\n        if self.voting == 'soft':\n            probas = self._collect_probas(X)\n            if not self.flatten_transform:\n                return probas\n            return np.hstack(probas)\n        else:\n            return self._predict(X)\n\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.\n        \"\"\"\n        check_is_fitted(self, 'n_features_in_')\n        if self.voting == 'soft' and (not self.flatten_transform):\n            raise ValueError(\"get_feature_names_out is not supported when `voting='soft'` and `flatten_transform=False`\")\n        _check_feature_names_in(self, input_features, generate_names=False)\n        class_name = self.__class__.__name__.lower()\n        active_names = [name for (name, est) in self.estimators if est != 'drop']\n        if self.voting == 'hard':\n            return np.asarray([f'{class_name}_{name}' for name in active_names], dtype=object)\n        n_classes = len(self.classes_)\n        names_out = [f'{class_name}_{name}{i}' for name in active_names for i in range(n_classes)]\n        return np.asarray(names_out, dtype=object)\n\n    def predict(self, X):\n        \"\"\"Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        maj : array-like of shape (n_samples,)\n            Predicted class labels.\n        \"\"\"\n        check_is_fitted(self)\n        if self.voting == 'soft':\n            maj = np.argmax(self.predict_proba(X), axis=1)\n        else:\n            predictions = self._predict(X)\n            maj = np.apply_along_axis(lambda x: np.argmax(np.bincount(x, weights=self._weights_not_none)), axis=1, arr=predictions)\n        maj = self.le_.inverse_transform(maj)\n        return maj\n\n    def _collect_probas(self, X):\n        \"\"\"Collect results from clf.predict calls.\"\"\"\n        return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n\n    def _sk_visual_block_(self):\n        (names, estimators) = zip(*self.estimators)\n        return _VisualBlock('parallel', estimators, names=names)", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._voting/VotingClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/ensemble/_weight_boosting.py", "fn_id": "", "content": "class AdaBoostClassifier(_RoutingNotSupportedMixin, ClassifierMixin, BaseWeightBoosting):\n    \"\"\"An AdaBoost classifier.\n\n    An AdaBoost [1]_ classifier is a meta-estimator that begins by fitting a\n    classifier on the original dataset and then fits additional copies of the\n    classifier on the same dataset but where the weights of incorrectly\n    classified instances are adjusted such that subsequent classifiers focus\n    more on difficult cases.\n\n    This class implements the algorithm based on [2]_.\n\n    Read more in the :ref:`User Guide <adaboost>`.\n\n    .. versionadded:: 0.14\n\n    Parameters\n    ----------\n    estimator : object, default=None\n        The base estimator from which the boosted ensemble is built.\n        Support for sample weighting is required, as well as proper\n        ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n        the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n        initialized with `max_depth=1`.\n\n        .. versionadded:: 1.2\n           `base_estimator` was renamed to `estimator`.\n\n    n_estimators : int, default=50\n        The maximum number of estimators at which boosting is terminated.\n        In case of perfect fit, the learning procedure is stopped early.\n        Values must be in the range `[1, inf)`.\n\n    learning_rate : float, default=1.0\n        Weight applied to each classifier at each boosting iteration. A higher\n        learning rate increases the contribution of each classifier. There is\n        a trade-off between the `learning_rate` and `n_estimators` parameters.\n        Values must be in the range `(0.0, inf)`.\n\n    algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n        ``estimator`` must support calculation of class probabilities.\n        If 'SAMME' then use the SAMME discrete boosting algorithm.\n        The SAMME.R algorithm typically converges faster than SAMME,\n        achieving a lower test error with fewer boosting iterations.\n\n        .. deprecated:: 1.4\n            `\"SAMME.R\"` is deprecated and will be removed in version 1.6.\n            '\"SAMME\"' will become the default.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given at each `estimator` at each\n        boosting iteration.\n        Thus, it is only used when `estimator` exposes a `random_state`.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_classes_ : int\n        The number of classes.\n\n    estimator_weights_ : ndarray of floats\n        Weights for each estimator in the boosted ensemble.\n\n    estimator_errors_ : ndarray of floats\n        Classification error for each estimator in the boosted\n        ensemble.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances if supported by the\n        ``estimator`` (when based on decision trees).\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n        regressor on the original dataset and then fits additional copies of\n        the regressor on the same dataset but where the weights of instances\n        are adjusted according to the error of the current prediction.\n\n    GradientBoostingClassifier : GB builds an additive model in a forward\n        stage-wise fashion. Regression trees are fit on the negative gradient\n        of the binomial or multinomial deviance loss function. Binary\n        classification is a special case where only a single regression tree is\n        induced.\n\n    sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n        method used for classification.\n        Creates a model that predicts the value of a target variable by\n        learning simple decision rules inferred from the data features.\n\n    References\n    ----------\n    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n           on-Line Learning and an Application to Boosting\", 1995.\n\n    .. [2] :doi:`J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class adaboost.\"\n           Statistics and its Interface 2.3 (2009): 349-360.\n           <10.4310/SII.2009.v2.n3.a8>`\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import AdaBoostClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0)\n    >>> clf.fit(X, y)\n    AdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)\n    >>> clf.predict([[0, 0, 0, 0]])\n    array([1])\n    >>> clf.score(X, y)\n    0.96...\n\n    For a detailed example of using AdaBoost to fit a sequence of DecisionTrees\n    as weaklearners, please refer to\n    :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_multiclass.py`.\n\n    For a detailed example of using AdaBoost to fit a non-linearly seperable\n    classification dataset composed of two Gaussian quantiles clusters, please\n    refer to :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_twoclass.py`.\n    \"\"\"\n    _parameter_constraints: dict = {**BaseWeightBoosting._parameter_constraints, 'algorithm': [StrOptions({'SAMME', 'SAMME.R'})]}\n\n    def staged_decision_function(self, X):\n        \"\"\"Compute decision function of ``X`` for each boosting iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each boosting iteration.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        ------\n        score : generator of ndarray of shape (n_samples, k)\n            The decision function of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n            Binary classification is a special cases with ``k == 1``,\n            otherwise ``k==n_classes``. For binary classification,\n            values closer to -1 or 1 mean more like the first or second\n            class in ``classes_``, respectively.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n        n_classes = self.n_classes_\n        classes = self.classes_[:, np.newaxis]\n        pred = None\n        norm = 0.0\n        for (weight, estimator) in zip(self.estimator_weights_, self.estimators_):\n            norm += weight\n            if self.algorithm == 'SAMME.R':\n                current_pred = _samme_proba(estimator, n_classes, X)\n            else:\n                current_pred = np.where((estimator.predict(X) == classes).T, weight, -1 / (n_classes - 1) * weight)\n            if pred is None:\n                pred = current_pred\n            else:\n                pred += current_pred\n            if n_classes == 2:\n                tmp_pred = np.copy(pred)\n                tmp_pred[:, 0] *= -1\n                yield (tmp_pred / norm).sum(axis=1)\n            else:\n                yield (pred / norm)\n\n    def _boost(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost.\n\n        Perform a single boost according to the real multi-class SAMME.R\n        algorithm or to the discrete SAMME algorithm and return the updated\n        sample weights.\n\n        Parameters\n        ----------\n        iboost : int\n            The index of the current boost iteration.\n\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,)\n            The target values (class labels).\n\n        sample_weight : array-like of shape (n_samples,)\n            The current sample weights.\n\n        random_state : RandomState instance\n            The RandomState instance used if the base estimator accepts a\n            `random_state` attribute.\n\n        Returns\n        -------\n        sample_weight : array-like of shape (n_samples,) or None\n            The reweighted sample weights.\n            If None then boosting has terminated early.\n\n        estimator_weight : float\n            The weight for the current boost.\n            If None then boosting has terminated early.\n\n        estimator_error : float\n            The classification error for the current boost.\n            If None then boosting has terminated early.\n        \"\"\"\n        if self.algorithm == 'SAMME.R':\n            return self._boost_real(iboost, X, y, sample_weight, random_state)\n        else:\n            return self._boost_discrete(iboost, X, y, sample_weight, random_state)\n\n    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the weighted mean predicted class log-probabilities of the classifiers\n        in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n        return np.log(self.predict_proba(X))\n\n    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        The predicted class of an input sample is computed as the weighted mean\n        prediction of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        pred = self.decision_function(X)\n        if self.n_classes_ == 2:\n            return self.classes_.take(pred > 0, axis=0)\n        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n\n    def _more_tags(self):\n        return {'requires_y': True}\n\n    def staged_score(self, X, y, sample_weight=None):\n        \"\"\"Return staged scores for X, y.\n\n        This generator method yields the ensemble score after each iteration of\n        boosting and therefore allows monitoring, such as to determine the\n        score on a test set after each boost.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        y : array-like of shape (n_samples,)\n            Labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Yields\n        ------\n        z : float\n        \"\"\"\n        X = self._check_X(X)\n        for y_pred in self.staged_predict(X):\n            if is_classifier(self):\n                yield accuracy_score(y, y_pred, sample_weight=sample_weight)\n            else:\n                yield r2_score(y, y_pred, sample_weight=sample_weight)\n\n    def _validate_estimator(self):\n        \"\"\"Check the estimator and set the estimator_ attribute.\"\"\"\n        super()._validate_estimator(default=DecisionTreeClassifier(max_depth=1))\n        if self.algorithm != 'SAMME':\n            warnings.warn('The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.', FutureWarning)\n            if not hasattr(self.estimator_, 'predict_proba'):\n                raise TypeError(\"AdaBoostClassifier with algorithm='SAMME.R' requires that the weak learner supports the calculation of class probabilities with a predict_proba method.\\nPlease change the base estimator or set algorithm='SAMME' instead.\")\n        if not has_fit_parameter(self.estimator_, 'sample_weight'):\n            raise ValueError(f\"{self.estimator.__class__.__name__} doesn't support sample_weight.\")\n\n    def _boost_real(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n        estimator = self._make_estimator(random_state=random_state)\n        estimator.fit(X, y, sample_weight=sample_weight)\n        y_predict_proba = estimator.predict_proba(X)\n        if iboost == 0:\n            self.classes_ = getattr(estimator, 'classes_', None)\n            self.n_classes_ = len(self.classes_)\n        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1), axis=0)\n        incorrect = y_predict != y\n        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n        if estimator_error <= 0:\n            return (sample_weight, 1.0, 0.0)\n        n_classes = self.n_classes_\n        classes = self.classes_\n        y_codes = np.array([-1.0 / (n_classes - 1), 1.0])\n        y_coding = y_codes.take(classes == y[:, np.newaxis])\n        proba = y_predict_proba\n        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n        estimator_weight = -1.0 * self.learning_rate * ((n_classes - 1.0) / n_classes) * xlogy(y_coding, y_predict_proba).sum(axis=1)\n        if not iboost == self.n_estimators - 1:\n            sample_weight *= np.exp(estimator_weight * ((sample_weight > 0) | (estimator_weight < 0)))\n        return (sample_weight, 1.0, estimator_error)\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities of the classifiers\n        in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n        check_is_fitted(self)\n        n_classes = self.n_classes_\n        if n_classes == 1:\n            return np.ones((_num_samples(X), 1))\n        decision = self.decision_function(X)\n        return self._compute_proba_from_decision(decision, n_classes)\n\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        score : ndarray of shape of (n_samples, k)\n            The decision function of the input samples. The order of\n            outputs is the same as that of the :term:`classes_` attribute.\n            Binary classification is a special cases with ``k == 1``,\n            otherwise ``k==n_classes``. For binary classification,\n            values closer to -1 or 1 mean more like the first or second\n            class in ``classes_``, respectively.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n        n_classes = self.n_classes_\n        classes = self.classes_[:, np.newaxis]\n        if self.algorithm == 'SAMME.R':\n            pred = sum((_samme_proba(estimator, n_classes, X) for estimator in self.estimators_))\n        else:\n            pred = sum((np.where((estimator.predict(X) == classes).T, w, -1 / (n_classes - 1) * w) for (estimator, w) in zip(self.estimators_, self.estimator_weights_)))\n        pred /= self.estimator_weights_.sum()\n        if n_classes == 2:\n            pred[:, 0] *= -1\n            return pred.sum(axis=1)\n        return pred\n\n    def _boost_discrete(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\n        estimator = self._make_estimator(random_state=random_state)\n        estimator.fit(X, y, sample_weight=sample_weight)\n        y_predict = estimator.predict(X)\n        if iboost == 0:\n            self.classes_ = getattr(estimator, 'classes_', None)\n            self.n_classes_ = len(self.classes_)\n        incorrect = y_predict != y\n        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n        if estimator_error <= 0:\n            return (sample_weight, 1.0, 0.0)\n        n_classes = self.n_classes_\n        if estimator_error >= 1.0 - 1.0 / n_classes:\n            self.estimators_.pop(-1)\n            if len(self.estimators_) == 0:\n                raise ValueError('BaseClassifier in AdaBoostClassifier ensemble is worse than random, ensemble can not be fit.')\n            return (None, None, None)\n        estimator_weight = self.learning_rate * (np.log((1.0 - estimator_error) / estimator_error) + np.log(n_classes - 1.0))\n        if not iboost == self.n_estimators - 1:\n            sample_weight = np.exp(np.log(sample_weight) + estimator_weight * incorrect * (sample_weight > 0))\n        return (sample_weight, estimator_weight, estimator_error)\n\n    def __init__(self, estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None):\n        super().__init__(estimator=estimator, n_estimators=n_estimators, learning_rate=learning_rate, random_state=random_state)\n        self.algorithm = algorithm\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n        \"\"\"\n        from .metrics import accuracy_score\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n\n    @staticmethod\n    def _compute_proba_from_decision(decision, n_classes):\n        \"\"\"Compute probabilities from the decision function.\n\n        This is based eq. (15) of [1] where:\n            p(y=c|X) = exp((1 / K-1) f_c(X)) / sum_k(exp((1 / K-1) f_k(X)))\n                     = softmax((1 / K-1) * f(X))\n\n        References\n        ----------\n        .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\",\n               2009.\n        \"\"\"\n        if n_classes == 2:\n            decision = np.vstack([-decision, decision]).T / 2\n        else:\n            decision /= n_classes - 1\n        return softmax(decision, copy=False)\n\n    def staged_predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities of the classifiers\n        in the ensemble.\n\n        This generator method yields the ensemble predicted class probabilities\n        after each iteration of boosting and therefore allows monitoring, such\n        as to determine the predicted class probabilities on a test set after\n        each boost.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        ------\n        p : generator of ndarray of shape (n_samples,)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n        n_classes = self.n_classes_\n        for decision in self.staged_decision_function(X):\n            yield self._compute_proba_from_decision(decision, n_classes)\n\n    def staged_predict(self, X):\n        \"\"\"Return staged predictions for X.\n\n        The predicted class of an input sample is computed as the weighted mean\n        prediction of the classifiers in the ensemble.\n\n        This generator method yields the ensemble prediction after each\n        iteration of boosting and therefore allows monitoring, such as to\n        determine the prediction on a test set after each boost.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        X = self._check_X(X)\n        n_classes = self.n_classes_\n        classes = self.classes_\n        if n_classes == 2:\n            for pred in self.staged_decision_function(X):\n                yield np.array(classes.take(pred > 0, axis=0))\n        else:\n            for pred in self.staged_decision_function(X):\n                yield np.array(classes.take(np.argmax(pred, axis=1), axis=0))", "class_fn": true, "question_id": "sklearn/sklearn.ensemble._weight_boosting/AdaBoostClassifier", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/exceptions.py", "fn_id": "", "content": "class ConvergenceWarning(UserWarning):\n    \"\"\"Custom warning to capture convergence problems\n\n    .. versionchanged:: 0.18\n       Moved from sklearn.utils.\n    \"\"\"", "class_fn": true, "question_id": "sklearn/sklearn.exceptions/ConvergenceWarning", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/exceptions.py", "fn_id": "", "content": "class EfficiencyWarning(UserWarning):\n    \"\"\"Warning used to notify the user of inefficient computation.\n\n    This warning notifies the user that the efficiency may not be optimal due\n    to some reason which may be included as a part of the warning message.\n    This may be subclassed into a more specific Warning class.\n\n    .. versionadded:: 0.18\n    \"\"\"", "class_fn": true, "question_id": "sklearn/sklearn.exceptions/EfficiencyWarning", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/exceptions.py", "fn_id": "", "content": "class PositiveSpectrumWarning(UserWarning):\n    \"\"\"Warning raised when the eigenvalues of a PSD matrix have issues\n\n    This warning is typically raised by ``_check_psd_eigenvalues`` when the\n    eigenvalues of a positive semidefinite (PSD) matrix such as a gram matrix\n    (kernel) present significant negative eigenvalues, or bad conditioning i.e.\n    very small non-zero eigenvalues compared to the largest eigenvalue.\n\n    .. versionadded:: 0.22\n    \"\"\"", "class_fn": true, "question_id": "sklearn/sklearn.exceptions/PositiveSpectrumWarning", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class ArffDecoder:\n    \"\"\"An ARFF decoder.\"\"\"\n\n    def __init__(self):\n        \"\"\"Constructor.\"\"\"\n        self._conversors = []\n        self._current_line = 0\n\n    def _decode_comment(self, s):\n        \"\"\"(INTERNAL) Decodes a comment line.\n\n        Comments are single line strings starting, obligatorily, with the ``%``\n        character, and can have any symbol, including whitespaces or special\n        characters.\n\n        This method must receive a normalized string, i.e., a string without\n        padding, including the \"\\r\n\" characters.\n\n        :param s: a normalized string.\n        :return: a string with the decoded comment.\n        \"\"\"\n        res = re.sub('^\\\\%( )?', '', s)\n        return res\n\n    def _decode_relation(self, s):\n        \"\"\"(INTERNAL) Decodes a relation line.\n\n        The relation declaration is a line with the format ``@RELATION\n        <relation-name>``, where ``relation-name`` is a string. The string must\n        start with alphabetic character and must be quoted if the name includes\n        spaces, otherwise this method will raise a `BadRelationFormat` exception.\n\n        This method must receive a normalized string, i.e., a string without\n        padding, including the \"\\r\n\" characters.\n\n        :param s: a normalized string.\n        :return: a string with the decoded relation name.\n        \"\"\"\n        (_, v) = s.split(' ', 1)\n        v = v.strip()\n        if not _RE_RELATION.match(v):\n            raise BadRelationFormat()\n        res = str(v.strip('\"\\''))\n        return res\n\n    def _decode_attribute(self, s):\n        \"\"\"(INTERNAL) Decodes an attribute line.\n\n        The attribute is the most complex declaration in an arff file. All\n        attributes must follow the template::\n\n             @attribute <attribute-name> <datatype>\n\n        where ``attribute-name`` is a string, quoted if the name contains any\n        whitespace, and ``datatype`` can be:\n\n        - Numerical attributes as ``NUMERIC``, ``INTEGER`` or ``REAL``.\n        - Strings as ``STRING``.\n        - Dates (NOT IMPLEMENTED).\n        - Nominal attributes with format:\n\n            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...}\n\n        The nominal names follow the rules for the attribute names, i.e., they\n        must be quoted if the name contains whitespaces.\n\n        This method must receive a normalized string, i.e., a string without\n        padding, including the \"\\r\n\" characters.\n\n        :param s: a normalized string.\n        :return: a tuple (ATTRIBUTE_NAME, TYPE_OR_VALUES).\n        \"\"\"\n        (_, v) = s.split(' ', 1)\n        v = v.strip()\n        m = _RE_ATTRIBUTE.match(v)\n        if not m:\n            raise BadAttributeFormat()\n        (name, type_) = m.groups()\n        name = str(name.strip('\"\\''))\n        if type_[:1] == '{' and type_[-1:] == '}':\n            try:\n                type_ = _parse_values(type_.strip('{} '))\n            except Exception:\n                raise BadAttributeType()\n            if isinstance(type_, dict):\n                raise BadAttributeType()\n        else:\n            type_ = str(type_).upper()\n            if type_ not in ['NUMERIC', 'REAL', 'INTEGER', 'STRING']:\n                raise BadAttributeType()\n        return (name, type_)\n\n    def _decode(self, s, encode_nominal=False, matrix_type=DENSE):\n        \"\"\"Do the job the ``encode``.\"\"\"\n        self._current_line = 0\n        if isinstance(s, str):\n            s = s.strip('\\r\\n ').replace('\\r\\n', '\\n').split('\\n')\n        obj: ArffContainerType = {'description': '', 'relation': '', 'attributes': [], 'data': []}\n        attribute_names = {}\n        data = _get_data_object_for_decoding(matrix_type)\n        STATE = _TK_DESCRIPTION\n        s = iter(s)\n        for row in s:\n            self._current_line += 1\n            row = row.strip(' \\r\\n')\n            if not row:\n                continue\n            u_row = row.upper()\n            if u_row.startswith(_TK_DESCRIPTION) and STATE == _TK_DESCRIPTION:\n                obj['description'] += self._decode_comment(row) + '\\n'\n            elif u_row.startswith(_TK_RELATION):\n                if STATE != _TK_DESCRIPTION:\n                    raise BadLayout()\n                STATE = _TK_RELATION\n                obj['relation'] = self._decode_relation(row)\n            elif u_row.startswith(_TK_ATTRIBUTE):\n                if STATE != _TK_RELATION and STATE != _TK_ATTRIBUTE:\n                    raise BadLayout()\n                STATE = _TK_ATTRIBUTE\n                attr = self._decode_attribute(row)\n                if attr[0] in attribute_names:\n                    raise BadAttributeName(attr[0], attribute_names[attr[0]])\n                else:\n                    attribute_names[attr[0]] = self._current_line\n                obj['attributes'].append(attr)\n                if isinstance(attr[1], (list, tuple)):\n                    if encode_nominal:\n                        conversor = EncodedNominalConversor(attr[1])\n                    else:\n                        conversor = NominalConversor(attr[1])\n                else:\n                    CONVERSOR_MAP = {'STRING': str, 'INTEGER': lambda x: int(float(x)), 'NUMERIC': float, 'REAL': float}\n                    conversor = CONVERSOR_MAP[attr[1]]\n                self._conversors.append(conversor)\n            elif u_row.startswith(_TK_DATA):\n                if STATE != _TK_ATTRIBUTE:\n                    raise BadLayout()\n                break\n            elif u_row.startswith(_TK_COMMENT):\n                pass\n        else:\n            raise BadLayout()\n\n        def stream():\n            for row in s:\n                self._current_line += 1\n                row = row.strip()\n                if row and (not row.startswith(_TK_COMMENT)):\n                    yield row\n        obj['data'] = data.decode_rows(stream(), self._conversors)\n        if obj['description'].endswith('\\n'):\n            obj['description'] = obj['description'][:-1]\n        return obj\n\n    def decode(self, s, encode_nominal=False, return_type=DENSE):\n        \"\"\"Returns the Python representation of a given ARFF file.\n\n        When a file object is passed as an argument, this method reads lines\n        iteratively, avoiding to load unnecessary information to the memory.\n\n        :param s: a string or file object with the ARFF file.\n        :param encode_nominal: boolean, if True perform a label encoding\n            while reading the .arff file.\n        :param return_type: determines the data structure used to store the\n            dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,\n            `arff.DENSE_GEN` or `arff.LOD_GEN`.\n            Consult the sections on `working with sparse data`_ and `loading\n            progressively`_.\n        \"\"\"\n        try:\n            return self._decode(s, encode_nominal=encode_nominal, matrix_type=return_type)\n        except ArffException as e:\n            e.line = self._current_line\n            raise e", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/ArffDecoder", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_arff.py", "fn_id": "", "content": "class Data(_DataListMixin, DenseGeneratorData):\n    pass\n\n    def encode_data(self, data, attributes):\n        \"\"\"(INTERNAL) Encodes a line of data.\n\n        Data instances follow the csv format, i.e, attribute values are\n        delimited by commas. After converted from csv.\n\n        :param data: a list of values.\n        :param attributes: a list of attributes. Used to check if data is valid.\n        :return: a string with the encoded data line.\n        \"\"\"\n        current_row = 0\n        for inst in data:\n            if len(inst) != len(attributes):\n                raise BadObject('Instance %d has %d attributes, expected %d' % (current_row, len(inst), len(attributes)))\n            new_data = []\n            for value in inst:\n                if value is None or value == '' or value != value:\n                    s = '?'\n                else:\n                    s = encode_string(str(value))\n                new_data.append(s)\n            current_row += 1\n            yield ','.join(new_data)\n\n    @staticmethod\n    def _decode_values(values, conversors):\n        try:\n            values = [None if value is None else conversor(value) for (conversor, value) in zip(conversors, values)]\n        except ValueError as exc:\n            if 'float: ' in str(exc):\n                raise BadNumericalValue()\n        return values\n\n    def decode_rows(self, stream, conversors):\n        return list(super().decode_rows(stream, conversors))", "class_fn": true, "question_id": "sklearn/sklearn.externals._arff/Data", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/externals/_packaging/version.py", "fn_id": "", "content": "class InvalidVersion(ValueError):\n    \"\"\"\n    An invalid version was found, users should refer to PEP 440.\n    \"\"\"", "class_fn": true, "question_id": "sklearn/sklearn.externals._packaging.version/InvalidVersion", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_extraction/_hash.py", "fn_id": "", "content": "class FeatureHasher(TransformerMixin, BaseEstimator):\n    \"\"\"Implements feature hashing, aka the hashing trick.\n\n    This class turns sequences of symbolic feature names (strings) into\n    scipy.sparse matrices, using a hash function to compute the matrix column\n    corresponding to a name. The hash function employed is the signed 32-bit\n    version of Murmurhash3.\n\n    Feature names of type byte string are used as-is. Unicode strings are\n    converted to UTF-8 first, but no Unicode normalization is done.\n    Feature values must be (finite) numbers.\n\n    This class is a low-memory alternative to DictVectorizer and\n    CountVectorizer, intended for large-scale (online) learning and situations\n    where memory is tight, e.g. when running prediction code on embedded\n    devices.\n\n    For an efficiency comparison of the different feature extractors, see\n    :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\n    Read more in the :ref:`User Guide <feature_hashing>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_features : int, default=2**20\n        The number of features (columns) in the output matrices. Small numbers\n        of features are likely to cause hash collisions, but large numbers\n        will cause larger coefficient dimensions in linear learners.\n    input_type : str, default='dict'\n        Choose a string from {'dict', 'pair', 'string'}.\n        Either \"dict\" (the default) to accept dictionaries over\n        (feature_name, value); \"pair\" to accept pairs of (feature_name, value);\n        or \"string\" to accept single strings.\n        feature_name should be a string, while value should be a number.\n        In the case of \"string\", a value of 1 is implied.\n        The feature_name is hashed to find the appropriate column for the\n        feature. The value's sign might be flipped in the output (but see\n        non_negative, below).\n    dtype : numpy dtype, default=np.float64\n        The type of feature values. Passed to scipy.sparse matrix constructors\n        as the dtype argument. Do not set this to bool, np.boolean or any\n        unsigned integer type.\n    alternate_sign : bool, default=True\n        When True, an alternating sign is added to the features as to\n        approximately conserve the inner product in the hashed space even for\n        small n_features. This approach is similar to sparse random projection.\n\n        .. versionchanged:: 0.19\n            ``alternate_sign`` replaces the now deprecated ``non_negative``\n            parameter.\n\n    See Also\n    --------\n    DictVectorizer : Vectorizes string-valued features using a hash table.\n    sklearn.preprocessing.OneHotEncoder : Handles nominal/categorical features.\n\n    Notes\n    -----\n    This estimator is :term:`stateless` and does not need to be fitted.\n    However, we recommend to call :meth:`fit_transform` instead of\n    :meth:`transform`, as parameter validation is only performed in\n    :meth:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction import FeatureHasher\n    >>> h = FeatureHasher(n_features=10)\n    >>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n    >>> f = h.transform(D)\n    >>> f.toarray()\n    array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],\n           [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])\n\n    With `input_type=\"string\"`, the input must be an iterable over iterables of\n    strings:\n\n    >>> h = FeatureHasher(n_features=8, input_type=\"string\")\n    >>> raw_X = [[\"dog\", \"cat\", \"snake\"], [\"snake\", \"dog\"], [\"cat\", \"bird\"]]\n    >>> f = h.transform(raw_X)\n    >>> f.toarray()\n    array([[ 0.,  0.,  0., -1.,  0., -1.,  0.,  1.],\n           [ 0.,  0.,  0., -1.,  0., -1.,  0.,  0.],\n           [ 0., -1.,  0.,  0.,  0.,  0.,  0.,  1.]])\n    \"\"\"\n    _parameter_constraints: dict = {'n_features': [Interval(Integral, 1, np.iinfo(np.int32).max, closed='both')], 'input_type': [StrOptions({'dict', 'pair', 'string'})], 'dtype': 'no_validation', 'alternate_sign': ['boolean']}\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X=None, y=None):\n        \"\"\"Only validates estimator's parameters.\n\n        This method allows to: (i) validate the estimator's parameters and\n        (ii) be consistent with the scikit-learn transformer API.\n\n        Parameters\n        ----------\n        X : Ignored\n            Not used, present here for API consistency by convention.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            FeatureHasher class instance.\n        \"\"\"\n        return self\n\n    def transform(self, raw_X):\n        \"\"\"Transform a sequence of instances to a scipy.sparse matrix.\n\n        Parameters\n        ----------\n        raw_X : iterable over iterable over raw features, length = n_samples\n            Samples. Each sample must be iterable an (e.g., a list or tuple)\n            containing/generating feature names (and optionally values, see\n            the input_type constructor argument) which will be hashed.\n            raw_X need not support the len function, so it can be the result\n            of a generator; n_samples is determined on the fly.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Feature matrix, for use with estimators or further transformers.\n        \"\"\"\n        raw_X = iter(raw_X)\n        if self.input_type == 'dict':\n            raw_X = (_iteritems(d) for d in raw_X)\n        elif self.input_type == 'string':\n            first_raw_X = next(raw_X)\n            if isinstance(first_raw_X, str):\n                raise ValueError('Samples can not be a single string. The input must be an iterable over iterables of strings.')\n            raw_X_ = chain([first_raw_X], raw_X)\n            raw_X = (((f, 1) for f in x) for x in raw_X_)\n        (indices, indptr, values) = _hashing_transform(raw_X, self.n_features, self.dtype, self.alternate_sign, seed=0)\n        n_samples = indptr.shape[0] - 1\n        if n_samples == 0:\n            raise ValueError('Cannot vectorize empty sequence.')\n        X = sp.csr_matrix((values, indices, indptr), dtype=self.dtype, shape=(n_samples, self.n_features))\n        X.sum_duplicates()\n        return X\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def __init__(self, n_features=2 ** 20, *, input_type='dict', dtype=np.float64, alternate_sign=True):\n        self.dtype = dtype\n        self.input_type = input_type\n        self.n_features = n_features\n        self.alternate_sign = alternate_sign\n\n    def _more_tags(self):\n        return {'X_types': [self.input_type]}\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_", "class_fn": true, "question_id": "sklearn/sklearn.feature_extraction._hash/FeatureHasher", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_extraction/text.py", "fn_id": "", "content": "class HashingVectorizer(TransformerMixin, _VectorizerMixin, BaseEstimator, auto_wrap_output_keys=None):\n    \"\"\"Convert a collection of text documents to a matrix of token occurrences.\n\n    It turns a collection of text documents into a scipy.sparse matrix holding\n    token occurrence counts (or binary occurrence information), possibly\n    normalized as token frequencies if norm='l1' or projected on the euclidean\n    unit sphere if norm='l2'.\n\n    This text vectorizer implementation uses the hashing trick to find the\n    token string name to feature integer index mapping.\n\n    This strategy has several advantages:\n\n    - it is very low memory scalable to large datasets as there is no need to\n      store a vocabulary dictionary in memory.\n\n    - it is fast to pickle and un-pickle as it holds no state besides the\n      constructor parameters.\n\n    - it can be used in a streaming (partial fit) or parallel pipeline as there\n      is no state computed during fit.\n\n    There are also a couple of cons (vs using a CountVectorizer with an\n    in-memory vocabulary):\n\n    - there is no way to compute the inverse transform (from feature indices to\n      string feature names) which can be a problem when trying to introspect\n      which features are most important to a model.\n\n    - there can be collisions: distinct tokens can be mapped to the same\n      feature index. However in practice this is rarely an issue if n_features\n      is large enough (e.g. 2 ** 18 for text classification problems).\n\n    - no IDF weighting as this would render the transformer stateful.\n\n    The hash function employed is the signed 32-bit version of Murmurhash3.\n\n    For an efficiency comparison of the different feature extractors, see\n    :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\n    For an example of document clustering and comparison with\n    :class:`~sklearn.feature_extraction.text.TfidfVectorizer`, see\n    :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n    input : {'filename', 'file', 'content'}, default='content'\n        - If `'filename'`, the sequence passed as an argument to fit is\n          expected to be a list of filenames that need reading to fetch\n          the raw content to analyze.\n\n        - If `'file'`, the sequence items must have a 'read' method (file-like\n          object) that is called to fetch the bytes in memory.\n\n        - If `'content'`, the input is expected to be a sequence of items that\n          can be of type string or byte.\n\n    encoding : str, default='utf-8'\n        If bytes or files are given to analyze, this encoding is used to\n        decode.\n\n    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. By default, it is\n        'strict', meaning that a UnicodeDecodeError will be raised. Other\n        values are 'ignore' and 'replace'.\n\n    strip_accents : {'ascii', 'unicode'} or callable, default=None\n        Remove accents and perform other character normalization\n        during the preprocessing step.\n        'ascii' is a fast method that only works on characters that have\n        a direct ASCII mapping.\n        'unicode' is a slightly slower method that works on any character.\n        None (default) means no character normalization is performed.\n\n        Both 'ascii' and 'unicode' use NFKD normalization from\n        :func:`unicodedata.normalize`.\n\n    lowercase : bool, default=True\n        Convert all characters to lowercase before tokenizing.\n\n    preprocessor : callable, default=None\n        Override the preprocessing (string transformation) stage while\n        preserving the tokenizing and n-grams generation steps.\n        Only applies if ``analyzer`` is not callable.\n\n    tokenizer : callable, default=None\n        Override the string tokenization step while preserving the\n        preprocessing and n-grams generation steps.\n        Only applies if ``analyzer == 'word'``.\n\n    stop_words : {'english'}, list, default=None\n        If 'english', a built-in stop word list for English is used.\n        There are several known issues with 'english' and you should\n        consider an alternative (see :ref:`stop_words`).\n\n        If a list, that list is assumed to contain stop words, all of which\n        will be removed from the resulting tokens.\n        Only applies if ``analyzer == 'word'``.\n\n    token_pattern : str or None, default=r\"(?u)\\\\\\\\b\\\\\\\\w\\\\\\\\w+\\\\\\\\b\"\n        Regular expression denoting what constitutes a \"token\", only used\n        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n        or more alphanumeric characters (punctuation is completely ignored\n        and always treated as a token separator).\n\n        If there is a capturing group in token_pattern then the\n        captured group content, not the entire match, becomes the token.\n        At most one capturing group is permitted.\n\n    ngram_range : tuple (min_n, max_n), default=(1, 1)\n        The lower and upper boundary of the range of n-values for different\n        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n        only bigrams.\n        Only applies if ``analyzer`` is not callable.\n\n    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n        Whether the feature should be made of word or character n-grams.\n        Option 'char_wb' creates character n-grams only from text inside\n        word boundaries; n-grams at the edges of words are padded with space.\n\n        If a callable is passed it is used to extract the sequence of features\n        out of the raw, unprocessed input.\n\n        .. versionchanged:: 0.21\n            Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n            is first read from the file and then passed to the given callable\n            analyzer.\n\n    n_features : int, default=(2 ** 20)\n        The number of features (columns) in the output matrices. Small numbers\n        of features are likely to cause hash collisions, but large numbers\n        will cause larger coefficient dimensions in linear learners.\n\n    binary : bool, default=False\n        If True, all non zero counts are set to 1. This is useful for discrete\n        probabilistic models that model binary events rather than integer\n        counts.\n\n    norm : {'l1', 'l2'}, default='l2'\n        Norm used to normalize term vectors. None for no normalization.\n\n    alternate_sign : bool, default=True\n        When True, an alternating sign is added to the features as to\n        approximately conserve the inner product in the hashed space even for\n        small n_features. This approach is similar to sparse random projection.\n\n        .. versionadded:: 0.19\n\n    dtype : type, default=np.float64\n        Type of the matrix returned by fit_transform() or transform().\n\n    See Also\n    --------\n    CountVectorizer : Convert a collection of text documents to a matrix of\n        token counts.\n    TfidfVectorizer : Convert a collection of raw documents to a matrix of\n        TF-IDF features.\n\n    Notes\n    -----\n    This estimator is :term:`stateless` and does not need to be fitted.\n    However, we recommend to call :meth:`fit_transform` instead of\n    :meth:`transform`, as parameter validation is only performed in\n    :meth:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction.text import HashingVectorizer\n    >>> corpus = [\n    ...     'This is the first document.',\n    ...     'This document is the second document.',\n    ...     'And this is the third one.',\n    ...     'Is this the first document?',\n    ... ]\n    >>> vectorizer = HashingVectorizer(n_features=2**4)\n    >>> X = vectorizer.fit_transform(corpus)\n    >>> print(X.shape)\n    (4, 16)\n    \"\"\"\n    _parameter_constraints: dict = {'input': [StrOptions({'filename', 'file', 'content'})], 'encoding': [str], 'decode_error': [StrOptions({'strict', 'ignore', 'replace'})], 'strip_accents': [StrOptions({'ascii', 'unicode'}), None, callable], 'lowercase': ['boolean'], 'preprocessor': [callable, None], 'tokenizer': [callable, None], 'stop_words': [StrOptions({'english'}), list, None], 'token_pattern': [str, None], 'ngram_range': [tuple], 'analyzer': [StrOptions({'word', 'char', 'char_wb'}), callable], 'n_features': [Interval(Integral, 1, np.iinfo(np.int32).max, closed='left')], 'binary': ['boolean'], 'norm': [StrOptions({'l1', 'l2'}), None], 'alternate_sign': ['boolean'], 'dtype': 'no_validation'}\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def partial_fit(self, X, y=None):\n        \"\"\"Only validates estimator's parameters.\n\n        This method allows to: (i) validate the estimator's parameters and\n        (ii) be consistent with the scikit-learn transformer API.\n\n        Parameters\n        ----------\n        X : ndarray of shape [n_samples, n_features]\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            HashingVectorizer instance.\n        \"\"\"\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Only validates estimator's parameters.\n\n        This method allows to: (i) validate the estimator's parameters and\n        (ii) be consistent with the scikit-learn transformer API.\n\n        Parameters\n        ----------\n        X : ndarray of shape [n_samples, n_features]\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            HashingVectorizer instance.\n        \"\"\"\n        if isinstance(X, str):\n            raise ValueError('Iterable over raw text documents expected, string object received.')\n        self._warn_for_unused_params()\n        self._validate_ngram_range()\n        self._get_hasher().fit(X, y=y)\n        return self\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Transform a sequence of documents to a document-term matrix.\n\n        Parameters\n        ----------\n        X : iterable over raw text documents, length = n_samples\n            Samples. Each sample must be a text document (either bytes or\n            unicode strings, file name or file object depending on the\n            constructor argument) which will be tokenized and hashed.\n        y : any\n            Ignored. This parameter exists only for compatibility with\n            sklearn.pipeline.Pipeline.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        return self.fit(X, y).transform(X)\n\n    def __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', n_features=2 ** 20, binary=False, norm='l2', alternate_sign=True, dtype=np.float64):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.n_features = n_features\n        self.ngram_range = ngram_range\n        self.binary = binary\n        self.norm = norm\n        self.alternate_sign = alternate_sign\n        self.dtype = dtype\n\n    def _warn_for_unused_params(self):\n        if self.tokenizer is not None and self.token_pattern is not None:\n            warnings.warn(\"The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\")\n        if self.preprocessor is not None and callable(self.analyzer):\n            warnings.warn(\"The parameter 'preprocessor' will not be used since 'analyzer' is callable'\")\n        if self.ngram_range != (1, 1) and self.ngram_range is not None and callable(self.analyzer):\n            warnings.warn(\"The parameter 'ngram_range' will not be used since 'analyzer' is callable'\")\n        if self.analyzer != 'word' or callable(self.analyzer):\n            if self.stop_words is not None:\n                warnings.warn(\"The parameter 'stop_words' will not be used since 'analyzer' != 'word'\")\n            if self.token_pattern is not None and self.token_pattern != '(?u)\\\\b\\\\w\\\\w+\\\\b':\n                warnings.warn(\"The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\")\n            if self.tokenizer is not None:\n                warnings.warn(\"The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\")\n\n    def _get_hasher(self):\n        return FeatureHasher(n_features=self.n_features, input_type='string', dtype=self.dtype, alternate_sign=self.alternate_sign)\n\n    def _more_tags(self):\n        return {'X_types': ['string']}\n\n    def transform(self, X):\n        \"\"\"Transform a sequence of documents to a document-term matrix.\n\n        Parameters\n        ----------\n        X : iterable over raw text documents, length = n_samples\n            Samples. Each sample must be a text document (either bytes or\n            unicode strings, file name or file object depending on the\n            constructor argument) which will be tokenized and hashed.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        if isinstance(X, str):\n            raise ValueError('Iterable over raw text documents expected, string object received.')\n        self._validate_ngram_range()\n        analyzer = self.build_analyzer()\n        X = self._get_hasher().transform((analyzer(doc) for doc in X))\n        if self.binary:\n            X.data.fill(1)\n        if self.norm is not None:\n            X = normalize(X, norm=self.norm, copy=False)\n        return X", "class_fn": true, "question_id": "sklearn/sklearn.feature_extraction.text/HashingVectorizer", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_extraction/text.py", "fn_id": "", "content": "class _VectorizerMixin:\n    \"\"\"Provides common code for text vectorizers (tokenization logic).\"\"\"\n    _white_spaces = re.compile('\\\\s\\\\s+')\n\n    def decode(self, doc):\n        \"\"\"Decode the input into a string of unicode symbols.\n\n        The decoding strategy depends on the vectorizer parameters.\n\n        Parameters\n        ----------\n        doc : bytes or str\n            The string to decode.\n\n        Returns\n        -------\n        doc: str\n            A string of unicode symbols.\n        \"\"\"\n        if self.input == 'filename':\n            with open(doc, 'rb') as fh:\n                doc = fh.read()\n        elif self.input == 'file':\n            doc = doc.read()\n        if isinstance(doc, bytes):\n            doc = doc.decode(self.encoding, self.decode_error)\n        if doc is np.nan:\n            raise ValueError('np.nan is an invalid document, expected byte or unicode string.')\n        return doc\n\n    def _word_ngrams(self, tokens, stop_words=None):\n        \"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\n        if stop_words is not None:\n            tokens = [w for w in tokens if w not in stop_words]\n        (min_n, max_n) = self.ngram_range\n        if max_n != 1:\n            original_tokens = tokens\n            if min_n == 1:\n                tokens = list(original_tokens)\n                min_n += 1\n            else:\n                tokens = []\n            n_original_tokens = len(original_tokens)\n            tokens_append = tokens.append\n            space_join = ' '.join\n            for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):\n                for i in range(n_original_tokens - n + 1):\n                    tokens_append(space_join(original_tokens[i:i + n]))\n        return tokens\n\n    def _char_ngrams(self, text_document):\n        \"\"\"Tokenize text_document into a sequence of character n-grams\"\"\"\n        text_document = self._white_spaces.sub(' ', text_document)\n        text_len = len(text_document)\n        (min_n, max_n) = self.ngram_range\n        if min_n == 1:\n            ngrams = list(text_document)\n            min_n += 1\n        else:\n            ngrams = []\n        ngrams_append = ngrams.append\n        for n in range(min_n, min(max_n + 1, text_len + 1)):\n            for i in range(text_len - n + 1):\n                ngrams_append(text_document[i:i + n])\n        return ngrams\n\n    def _char_wb_ngrams(self, text_document):\n        \"\"\"Whitespace sensitive char-n-gram tokenization.\n\n        Tokenize text_document into a sequence of character n-grams\n        operating only inside word boundaries. n-grams at the edges\n        of words are padded with space.\"\"\"\n        text_document = self._white_spaces.sub(' ', text_document)\n        (min_n, max_n) = self.ngram_range\n        ngrams = []\n        ngrams_append = ngrams.append\n        for w in text_document.split():\n            w = ' ' + w + ' '\n            w_len = len(w)\n            for n in range(min_n, max_n + 1):\n                offset = 0\n                ngrams_append(w[offset:offset + n])\n                while offset + n < w_len:\n                    offset += 1\n                    ngrams_append(w[offset:offset + n])\n                if offset == 0:\n                    break\n        return ngrams\n\n    def build_preprocessor(self):\n        \"\"\"Return a function to preprocess the text before tokenization.\n\n        Returns\n        -------\n        preprocessor: callable\n              A function to preprocess the text before tokenization.\n        \"\"\"\n        if self.preprocessor is not None:\n            return self.preprocessor\n        if not self.strip_accents:\n            strip_accents = None\n        elif callable(self.strip_accents):\n            strip_accents = self.strip_accents\n        elif self.strip_accents == 'ascii':\n            strip_accents = strip_accents_ascii\n        elif self.strip_accents == 'unicode':\n            strip_accents = strip_accents_unicode\n        else:\n            raise ValueError('Invalid value for \"strip_accents\": %s' % self.strip_accents)\n        return partial(_preprocess, accent_function=strip_accents, lower=self.lowercase)\n\n    def build_tokenizer(self):\n        \"\"\"Return a function that splits a string into a sequence of tokens.\n\n        Returns\n        -------\n        tokenizer: callable\n              A function to split a string into a sequence of tokens.\n        \"\"\"\n        if self.tokenizer is not None:\n            return self.tokenizer\n        token_pattern = re.compile(self.token_pattern)\n        if token_pattern.groups > 1:\n            raise ValueError('More than 1 capturing group in token pattern. Only a single group should be captured.')\n        return token_pattern.findall\n\n    def get_stop_words(self):\n        \"\"\"Build or fetch the effective stop words list.\n\n        Returns\n        -------\n        stop_words: list or None\n                A list of stop words.\n        \"\"\"\n        return _check_stop_list(self.stop_words)\n\n    def _check_stop_words_consistency(self, stop_words, preprocess, tokenize):\n        \"\"\"Check if stop words are consistent\n\n        Returns\n        -------\n        is_consistent : True if stop words are consistent with the preprocessor\n                        and tokenizer, False if they are not, None if the check\n                        was previously performed, \"error\" if it could not be\n                        performed (e.g. because of the use of a custom\n                        preprocessor / tokenizer)\n        \"\"\"\n        if id(self.stop_words) == getattr(self, '_stop_words_id', None):\n            return None\n        try:\n            inconsistent = set()\n            for w in stop_words or ():\n                tokens = list(tokenize(preprocess(w)))\n                for token in tokens:\n                    if token not in stop_words:\n                        inconsistent.add(token)\n            self._stop_words_id = id(self.stop_words)\n            if inconsistent:\n                warnings.warn('Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens %r not in stop_words.' % sorted(inconsistent))\n            return not inconsistent\n        except Exception:\n            self._stop_words_id = id(self.stop_words)\n            return 'error'\n\n    def build_analyzer(self):\n        \"\"\"Return a callable to process input data.\n\n        The callable handles preprocessing, tokenization, and n-grams generation.\n\n        Returns\n        -------\n        analyzer: callable\n            A function to handle preprocessing, tokenization\n            and n-grams generation.\n        \"\"\"\n        if callable(self.analyzer):\n            return partial(_analyze, analyzer=self.analyzer, decoder=self.decode)\n        preprocess = self.build_preprocessor()\n        if self.analyzer == 'char':\n            return partial(_analyze, ngrams=self._char_ngrams, preprocessor=preprocess, decoder=self.decode)\n        elif self.analyzer == 'char_wb':\n            return partial(_analyze, ngrams=self._char_wb_ngrams, preprocessor=preprocess, decoder=self.decode)\n        elif self.analyzer == 'word':\n            stop_words = self.get_stop_words()\n            tokenize = self.build_tokenizer()\n            self._check_stop_words_consistency(stop_words, preprocess, tokenize)\n            return partial(_analyze, ngrams=self._word_ngrams, tokenizer=tokenize, preprocessor=preprocess, decoder=self.decode, stop_words=stop_words)\n        else:\n            raise ValueError('%s is not a valid tokenization scheme/analyzer' % self.analyzer)\n\n    def _validate_vocabulary(self):\n        vocabulary = self.vocabulary\n        if vocabulary is not None:\n            if isinstance(vocabulary, set):\n                vocabulary = sorted(vocabulary)\n            if not isinstance(vocabulary, Mapping):\n                vocab = {}\n                for (i, t) in enumerate(vocabulary):\n                    if vocab.setdefault(t, i) != i:\n                        msg = 'Duplicate term in vocabulary: %r' % t\n                        raise ValueError(msg)\n                vocabulary = vocab\n            else:\n                indices = set(vocabulary.values())\n                if len(indices) != len(vocabulary):\n                    raise ValueError('Vocabulary contains repeated indices.')\n                for i in range(len(vocabulary)):\n                    if i not in indices:\n                        msg = \"Vocabulary of size %d doesn't contain index %d.\" % (len(vocabulary), i)\n                        raise ValueError(msg)\n            if not vocabulary:\n                raise ValueError('empty vocabulary passed to fit')\n            self.fixed_vocabulary_ = True\n            self.vocabulary_ = dict(vocabulary)\n        else:\n            self.fixed_vocabulary_ = False\n\n    def _check_vocabulary(self):\n        \"\"\"Check if vocabulary is empty or missing (not fitted)\"\"\"\n        if not hasattr(self, 'vocabulary_'):\n            self._validate_vocabulary()\n            if not self.fixed_vocabulary_:\n                raise NotFittedError('Vocabulary not fitted or provided')\n        if len(self.vocabulary_) == 0:\n            raise ValueError('Vocabulary is empty')\n\n    def _validate_ngram_range(self):\n        \"\"\"Check validity of ngram_range parameter\"\"\"\n        (min_n, max_m) = self.ngram_range\n        if min_n > max_m:\n            raise ValueError('Invalid value for ngram_range=%s lower boundary larger than the upper boundary.' % str(self.ngram_range))\n\n    def _warn_for_unused_params(self):\n        if self.tokenizer is not None and self.token_pattern is not None:\n            warnings.warn(\"The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\")\n        if self.preprocessor is not None and callable(self.analyzer):\n            warnings.warn(\"The parameter 'preprocessor' will not be used since 'analyzer' is callable'\")\n        if self.ngram_range != (1, 1) and self.ngram_range is not None and callable(self.analyzer):\n            warnings.warn(\"The parameter 'ngram_range' will not be used since 'analyzer' is callable'\")\n        if self.analyzer != 'word' or callable(self.analyzer):\n            if self.stop_words is not None:\n                warnings.warn(\"The parameter 'stop_words' will not be used since 'analyzer' != 'word'\")\n            if self.token_pattern is not None and self.token_pattern != '(?u)\\\\b\\\\w\\\\w+\\\\b':\n                warnings.warn(\"The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\")\n            if self.tokenizer is not None:\n                warnings.warn(\"The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\")", "class_fn": true, "question_id": "sklearn/sklearn.feature_extraction.text/_VectorizerMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_selection/_rfe.py", "fn_id": "", "content": "class RFE(_RoutingNotSupportedMixin, SelectorMixin, MetaEstimatorMixin, BaseEstimator):\n    \"\"\"Feature ranking with recursive feature elimination.\n\n    Given an external estimator that assigns weights to features (e.g., the\n    coefficients of a linear model), the goal of recursive feature elimination\n    (RFE) is to select features by recursively considering smaller and smaller\n    sets of features. First, the estimator is trained on the initial set of\n    features and the importance of each feature is obtained either through\n    any specific attribute or callable.\n    Then, the least important features are pruned from current set of features.\n    That procedure is recursively repeated on the pruned set until the desired\n    number of features to select is eventually reached.\n\n    Read more in the :ref:`User Guide <rfe>`.\n\n    Parameters\n    ----------\n    estimator : ``Estimator`` instance\n        A supervised learning estimator with a ``fit`` method that provides\n        information about feature importance\n        (e.g. `coef_`, `feature_importances_`).\n\n    n_features_to_select : int or float, default=None\n        The number of features to select. If `None`, half of the features are\n        selected. If integer, the parameter is the absolute number of features\n        to select. If float between 0 and 1, it is the fraction of features to\n        select.\n\n        .. versionchanged:: 0.24\n           Added float values for fractions.\n\n    step : int or float, default=1\n        If greater than or equal to 1, then ``step`` corresponds to the\n        (integer) number of features to remove at each iteration.\n        If within (0.0, 1.0), then ``step`` corresponds to the percentage\n        (rounded down) of features to remove at each iteration.\n\n    verbose : int, default=0\n        Controls verbosity of output.\n\n    importance_getter : str or callable, default='auto'\n        If 'auto', uses the feature importance either through a `coef_`\n        or `feature_importances_` attributes of estimator.\n\n        Also accepts a string that specifies an attribute name/path\n        for extracting feature importance (implemented with `attrgetter`).\n        For example, give `regressor_.coef_` in case of\n        :class:`~sklearn.compose.TransformedTargetRegressor`  or\n        `named_steps.clf.feature_importances_` in case of\n        class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n        If `callable`, overrides the default feature importance getter.\n        The callable is passed with the fitted estimator and it should\n        return importance for each feature.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels. Only available when `estimator` is a classifier.\n\n    estimator_ : ``Estimator`` instance\n        The fitted estimator used to select features.\n\n    n_features_ : int\n        The number of selected features.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    ranking_ : ndarray of shape (n_features,)\n        The feature ranking, such that ``ranking_[i]`` corresponds to the\n        ranking position of the i-th feature. Selected (i.e., estimated\n        best) features are assigned rank 1.\n\n    support_ : ndarray of shape (n_features,)\n        The mask of selected features.\n\n    See Also\n    --------\n    RFECV : Recursive feature elimination with built-in cross-validated\n        selection of the best number of features.\n    SelectFromModel : Feature selection based on thresholds of importance\n        weights.\n    SequentialFeatureSelector : Sequential cross-validation based feature\n        selection. Does not rely on importance weights.\n\n    Notes\n    -----\n    Allows NaN/Inf in the input if the underlying estimator does as well.\n\n    References\n    ----------\n\n    .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n           for cancer classification using support vector machines\",\n           Mach. Learn., 46(1-3), 389--422, 2002.\n\n    Examples\n    --------\n    The following example shows how to retrieve the 5 most informative\n    features in the Friedman #1 dataset.\n\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.feature_selection import RFE\n    >>> from sklearn.svm import SVR\n    >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n    >>> estimator = SVR(kernel=\"linear\")\n    >>> selector = RFE(estimator, n_features_to_select=5, step=1)\n    >>> selector = selector.fit(X, y)\n    >>> selector.support_\n    array([ True,  True,  True,  True,  True, False, False, False, False,\n           False])\n    >>> selector.ranking_\n    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n    \"\"\"\n    _parameter_constraints: dict = {'estimator': [HasMethods(['fit'])], 'n_features_to_select': [None, Interval(RealNotInt, 0, 1, closed='right'), Interval(Integral, 0, None, closed='neither')], 'step': [Interval(Integral, 0, None, closed='neither'), Interval(RealNotInt, 0, 1, closed='neither')], 'verbose': ['verbose'], 'importance_getter': [str, callable]}\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    @property\n    def _estimator_type(self):\n        return self.estimator._estimator_type\n\n    @property\n    def classes_(self):\n        \"\"\"Classes labels available when `estimator` is a classifier.\n\n        Returns\n        -------\n        ndarray of shape (n_classes,)\n        \"\"\"\n        return self.estimator_.classes_\n\n    @_fit_context(prefer_skip_nested_validation=False)\n    def fit(self, X, y, **fit_params):\n        \"\"\"Fit the RFE model and then the underlying estimator on the selected features.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        **fit_params : dict\n            Additional parameters passed to the `fit` method of the underlying\n            estimator.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        _raise_for_unsupported_routing(self, 'fit', **fit_params)\n        return self._fit(X, y, **fit_params)\n\n    def _check_feature_names(self, X, *, reset):\n        \"\"\"Set or check the `feature_names_in_` attribute.\n\n        .. versionadded:: 1.0\n\n        Parameters\n        ----------\n        X : {ndarray, dataframe} of shape (n_samples, n_features)\n            The input samples.\n\n        reset : bool\n            Whether to reset the `feature_names_in_` attribute.\n            If False, the input will be checked for consistency with\n            feature names of data provided when reset was last True.\n            .. note::\n               It is recommended to call `reset=True` in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n        \"\"\"\n        if reset:\n            feature_names_in = _get_feature_names(X)\n            if feature_names_in is not None:\n                self.feature_names_in_ = feature_names_in\n            elif hasattr(self, 'feature_names_in_'):\n                delattr(self, 'feature_names_in_')\n            return\n        fitted_feature_names = getattr(self, 'feature_names_in_', None)\n        X_feature_names = _get_feature_names(X)\n        if fitted_feature_names is None and X_feature_names is None:\n            return\n        if X_feature_names is not None and fitted_feature_names is None:\n            warnings.warn(f'X has feature names, but {self.__class__.__name__} was fitted without feature names')\n            return\n        if X_feature_names is None and fitted_feature_names is not None:\n            warnings.warn(f'X does not have valid feature names, but {self.__class__.__name__} was fitted with feature names')\n            return\n        if len(fitted_feature_names) != len(X_feature_names) or np.any(fitted_feature_names != X_feature_names):\n            message = 'The feature names should match those that were passed during fit.\\n'\n            fitted_feature_names_set = set(fitted_feature_names)\n            X_feature_names_set = set(X_feature_names)\n            unexpected_names = sorted(X_feature_names_set - fitted_feature_names_set)\n            missing_names = sorted(fitted_feature_names_set - X_feature_names_set)\n\n            def add_names(names):\n                output = ''\n                max_n_names = 5\n                for (i, name) in enumerate(names):\n                    if i >= max_n_names:\n                        output += '- ...\\n'\n                        break\n                    output += f'- {name}\\n'\n                return output\n            if unexpected_names:\n                message += 'Feature names unseen at fit time:\\n'\n                message += add_names(unexpected_names)\n            if missing_names:\n                message += 'Feature names seen at fit time, yet now missing:\\n'\n                message += add_names(missing_names)\n            if not missing_names and (not unexpected_names):\n                message += 'Feature names must be in the same order as they were in fit.\\n'\n            raise ValueError(message)\n\n    @available_if(_estimator_has('predict'))\n    def predict(self, X):\n        \"\"\"Reduce X to the selected features and predict using the estimator.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        Returns\n        -------\n        y : array of shape [n_samples]\n            The predicted target values.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict(self.transform(X))\n\n    @available_if(_estimator_has('score'))\n    def score(self, X, y, **fit_params):\n        \"\"\"Reduce X to the selected features and return the score of the estimator.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        y : array of shape [n_samples]\n            The target values.\n\n        **fit_params : dict\n            Parameters to pass to the `score` method of the underlying\n            estimator.\n\n            .. versionadded:: 1.0\n\n        Returns\n        -------\n        score : float\n            Score of the underlying base estimator computed with the selected\n            features returned by `rfe.transform(X)` and `y`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.score(self.transform(X), y, **fit_params)\n\n    def _fit(self, X, y, step_score=None, **fit_params):\n        (X, y) = self._validate_data(X, y, accept_sparse='csc', ensure_min_features=2, force_all_finite=False, multi_output=True)\n        n_features = X.shape[1]\n        if self.n_features_to_select is None:\n            n_features_to_select = n_features // 2\n        elif isinstance(self.n_features_to_select, Integral):\n            n_features_to_select = self.n_features_to_select\n            if n_features_to_select > n_features:\n                warnings.warn(f'Found n_features_to_select={n_features_to_select!r} > n_features={n_features!r}. There will be no feature selection and all features will be kept.', UserWarning)\n        else:\n            n_features_to_select = int(n_features * self.n_features_to_select)\n        if 0.0 < self.step < 1.0:\n            step = int(max(1, self.step * n_features))\n        else:\n            step = int(self.step)\n        support_ = np.ones(n_features, dtype=bool)\n        ranking_ = np.ones(n_features, dtype=int)\n        if step_score:\n            self.step_n_features_ = []\n            self.step_scores_ = []\n        while np.sum(support_) > n_features_to_select:\n            features = np.arange(n_features)[support_]\n            estimator = clone(self.estimator)\n            if self.verbose > 0:\n                print('Fitting estimator with %d features.' % np.sum(support_))\n            estimator.fit(X[:, features], y, **fit_params)\n            importances = _get_feature_importances(estimator, self.importance_getter, transform_func='square')\n            ranks = np.argsort(importances)\n            ranks = np.ravel(ranks)\n            threshold = min(step, np.sum(support_) - n_features_to_select)\n            if step_score:\n                self.step_n_features_.append(len(features))\n                self.step_scores_.append(step_score(estimator, features))\n            support_[features[ranks][:threshold]] = False\n            ranking_[np.logical_not(support_)] += 1\n        features = np.arange(n_features)[support_]\n        self.estimator_ = clone(self.estimator)\n        self.estimator_.fit(X[:, features], y, **fit_params)\n        if step_score:\n            self.step_n_features_.append(len(features))\n            self.step_scores_.append(step_score(self.estimator_, features))\n        self.n_features_ = support_.sum()\n        self.support_ = support_\n        self.ranking_ = ranking_\n        return self\n\n    @available_if(_estimator_has('decision_function'))\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        score : array, shape = [n_samples, n_classes] or [n_samples]\n            The decision function of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification produce an array of shape\n            [n_samples].\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.decision_function(self.transform(X))\n\n    @available_if(_estimator_has('predict_proba'))\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : array of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict_proba(self.transform(X))\n\n    @available_if(_estimator_has('predict_log_proba'))\n    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        Returns\n        -------\n        p : array of shape (n_samples, n_classes)\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict_log_proba(self.transform(X))\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n        return self.support_\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def __init__(self, estimator, *, n_features_to_select=None, step=1, verbose=0, importance_getter='auto'):\n        self.estimator = estimator\n        self.n_features_to_select = n_features_to_select\n        self.step = step\n        self.importance_getter = importance_getter\n        self.verbose = verbose\n\n    def _more_tags(self):\n        tags = {'poor_score': True, 'requires_y': True, 'allow_nan': True}\n        if hasattr(self.estimator, '_get_tags'):\n            tags['allow_nan'] = self.estimator._get_tags()['allow_nan']\n        return tags", "class_fn": true, "question_id": "sklearn/sklearn.feature_selection._rfe/RFE", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_selection/_univariate_selection.py", "fn_id": "", "content": "class GenericUnivariateSelect(_BaseFilter):\n    \"\"\"Univariate feature selector with configurable strategy.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable, default=f_classif\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues). For modes 'percentile' or 'kbest' it can return\n        a single array scores.\n\n    mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'\n        Feature selection mode. Note that the `'percentile'` and `'kbest'`\n        modes are supporting unsupervised feature selection (when `y` is `None`).\n\n    param : \"all\", float or int, default=1e-5\n        Parameter of the corresponding mode.\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores, None if `score_func` returned scores only.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    mutual_info_classif : Mutual information for a discrete target.\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    f_regression : F-value between label/feature for regression tasks.\n    mutual_info_regression : Mutual information for a continuous target.\n    SelectPercentile : Select features based on percentile of the highest\n        scores.\n    SelectKBest : Select features based on the k highest scores.\n    SelectFpr : Select features based on a false positive rate test.\n    SelectFdr : Select features based on an estimated false discovery rate.\n    SelectFwe : Select features based on family-wise error rate.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.feature_selection import GenericUnivariateSelect, chi2\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> X.shape\n    (569, 30)\n    >>> transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n    >>> X_new = transformer.fit_transform(X, y)\n    >>> X_new.shape\n    (569, 20)\n    \"\"\"\n    _selection_modes: dict = {'percentile': SelectPercentile, 'k_best': SelectKBest, 'fpr': SelectFpr, 'fdr': SelectFdr, 'fwe': SelectFwe}\n    _parameter_constraints: dict = {**_BaseFilter._parameter_constraints, 'mode': [StrOptions(set(_selection_modes.keys()))], 'param': [Interval(Real, 0, None, closed='left'), StrOptions({'all'})]}\n\n    def __init__(self, score_func=f_classif, *, mode='percentile', param=1e-05):\n        super().__init__(score_func=score_func)\n        self.mode = mode\n        self.param = param\n\n    def _make_selector(self):\n        selector = self._selection_modes[self.mode](score_func=self.score_func)\n        possible_params = selector._get_param_names()\n        possible_params.remove('score_func')\n        selector.set_params(**{possible_params[0]: self.param})\n        return selector\n\n    def _more_tags(self):\n        return {'preserves_dtype': [np.float64, np.float32]}\n\n    def _check_params(self, X, y):\n        self._make_selector()._check_params(X, y)\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n        selector = self._make_selector()\n        selector.pvalues_ = self.pvalues_\n        selector.scores_ = self.scores_\n        return selector._get_support_mask()", "class_fn": true, "question_id": "sklearn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_selection/_univariate_selection.py", "fn_id": "", "content": "class SelectFwe(_BaseFilter):\n    \"\"\"Filter: Select the p-values corresponding to Family-wise error rate.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable, default=f_classif\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues).\n        Default is f_classif (see below \"See Also\"). The default function only\n        works with classification tasks.\n\n    alpha : float, default=5e-2\n        The highest uncorrected p-value for features to keep.\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    f_regression : F-value between label/feature for regression tasks.\n    SelectPercentile : Select features based on percentile of the highest\n        scores.\n    SelectKBest : Select features based on the k highest scores.\n    SelectFpr : Select features based on a false positive rate test.\n    SelectFdr : Select features based on an estimated false discovery rate.\n    GenericUnivariateSelect : Univariate feature selector with configurable\n        mode.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.feature_selection import SelectFwe, chi2\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> X.shape\n    (569, 30)\n    >>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n    >>> X_new.shape\n    (569, 15)\n    \"\"\"\n    _parameter_constraints: dict = {**_BaseFilter._parameter_constraints, 'alpha': [Interval(Real, 0, 1, closed='both')]}\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n        return self.pvalues_ < self.alpha / len(self.pvalues_)\n\n    def _more_tags(self):\n        return {'requires_y': True}\n\n    def _check_params(self, X, y):\n        pass\n\n    def __init__(self, score_func=f_classif, *, alpha=0.05):\n        super().__init__(score_func=score_func)\n        self.alpha = alpha", "class_fn": true, "question_id": "sklearn/sklearn.feature_selection._univariate_selection/SelectFwe", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/feature_selection/_variance_threshold.py", "fn_id": "", "content": "class VarianceThreshold(SelectorMixin, BaseEstimator):\n    \"\"\"Feature selector that removes all low-variance features.\n\n    This feature selection algorithm looks only at the features (X), not the\n    desired outputs (y), and can thus be used for unsupervised learning.\n\n    Read more in the :ref:`User Guide <variance_threshold>`.\n\n    Parameters\n    ----------\n    threshold : float, default=0\n        Features with a training-set variance lower than this threshold will\n        be removed. The default is to keep all features with non-zero variance,\n        i.e. remove the features that have the same value in all samples.\n\n    Attributes\n    ----------\n    variances_ : array, shape (n_features,)\n        Variances of individual features.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    SelectFromModel: Meta-transformer for selecting features based on\n        importance weights.\n    SelectPercentile : Select features according to a percentile of the highest\n        scores.\n    SequentialFeatureSelector : Transformer that performs Sequential Feature\n        Selection.\n\n    Notes\n    -----\n    Allows NaN in the input.\n    Raises ValueError if no feature in X meets the variance threshold.\n\n    Examples\n    --------\n    The following dataset has integer features, two of which are the same\n    in every sample. These are removed with the default setting for threshold::\n\n        >>> from sklearn.feature_selection import VarianceThreshold\n        >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n        >>> selector = VarianceThreshold()\n        >>> selector.fit_transform(X)\n        array([[2, 0],\n               [1, 4],\n               [1, 1]])\n    \"\"\"\n    _parameter_constraints: dict = {'threshold': [Interval(Real, 0, None, closed='left')]}\n\n    def __init__(self, threshold=0.0):\n        self.threshold = threshold\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Learn empirical variances from X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Data from which to compute variances, where `n_samples` is\n            the number of samples and `n_features` is the number of features.\n\n        y : any, default=None\n            Ignored. This parameter exists only for compatibility with\n            sklearn.pipeline.Pipeline.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=('csr', 'csc'), dtype=np.float64, force_all_finite='allow-nan')\n        if hasattr(X, 'toarray'):\n            (_, self.variances_) = mean_variance_axis(X, axis=0)\n            if self.threshold == 0:\n                (mins, maxes) = min_max_axis(X, axis=0)\n                peak_to_peaks = maxes - mins\n        else:\n            self.variances_ = np.nanvar(X, axis=0)\n            if self.threshold == 0:\n                peak_to_peaks = np.ptp(X, axis=0)\n        if self.threshold == 0:\n            compare_arr = np.array([self.variances_, peak_to_peaks])\n            self.variances_ = np.nanmin(compare_arr, axis=0)\n        if np.all(~np.isfinite(self.variances_) | (self.variances_ <= self.threshold)):\n            msg = 'No feature in X meets the variance threshold {0:.5f}'\n            if X.shape[0] == 1:\n                msg += ' (X contains only one sample)'\n            raise ValueError(msg.format(self.threshold))\n        return self\n\n    def _transform(self, X):\n        \"\"\"Reduce X to the selected features.\"\"\"\n        mask = self.get_support()\n        if not mask.any():\n            warnings.warn('No features were selected: either the data is too noisy or the selection test too strict.', UserWarning)\n            if hasattr(X, 'iloc'):\n                return X.iloc[:, :0]\n            return np.empty(0, dtype=X.dtype).reshape((X.shape[0], 0))\n        return _safe_indexing(X, mask, axis=1)\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n        return self.variances_ > self.threshold\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)\n\n    def _more_tags(self):\n        return {'allow_nan': True}\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_", "class_fn": true, "question_id": "sklearn/sklearn.feature_selection._variance_threshold/VarianceThreshold", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/gaussian_process/_gpr.py", "fn_id": "", "content": "class GaussianProcessRegressor(MultiOutputMixin, RegressorMixin, BaseEstimator):\n    \"\"\"Gaussian process regression (GPR).\n\n    The implementation is based on Algorithm 2.1 of [RW2006]_.\n\n    In addition to standard scikit-learn estimator API,\n    :class:`GaussianProcessRegressor`:\n\n       * allows prediction without prior fitting (based on the GP prior)\n       * provides an additional method `sample_y(X)`, which evaluates samples\n         drawn from the GPR (prior or posterior) at given inputs\n       * exposes a method `log_marginal_likelihood(theta)`, which can be used\n         externally for other ways of selecting hyperparameters, e.g., via\n         Markov chain Monte Carlo.\n\n    To learn the difference between a point-estimate approach vs. a more\n    Bayesian modelling approach, refer to the example entitled\n    :ref:`sphx_glr_auto_examples_gaussian_process_plot_compare_gpr_krr.py`.\n\n    Read more in the :ref:`User Guide <gaussian_process>`.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    kernel : kernel instance, default=None\n        The kernel specifying the covariance function of the GP. If None is\n        passed, the kernel ``ConstantKernel(1.0, constant_value_bounds=\"fixed\")\n        * RBF(1.0, length_scale_bounds=\"fixed\")`` is used as default. Note that\n        the kernel hyperparameters are optimized during fitting unless the\n        bounds are marked as \"fixed\".\n\n    alpha : float or ndarray of shape (n_samples,), default=1e-10\n        Value added to the diagonal of the kernel matrix during fitting.\n        This can prevent a potential numerical issue during fitting, by\n        ensuring that the calculated values form a positive definite matrix.\n        It can also be interpreted as the variance of additional Gaussian\n        measurement noise on the training observations. Note that this is\n        different from using a `WhiteKernel`. If an array is passed, it must\n        have the same number of entries as the data used for fitting and is\n        used as datapoint-dependent noise level. Allowing to specify the\n        noise level directly as a parameter is mainly for convenience and\n        for consistency with :class:`~sklearn.linear_model.Ridge`.\n\n    optimizer : \"fmin_l_bfgs_b\", callable or None, default=\"fmin_l_bfgs_b\"\n        Can either be one of the internally supported optimizers for optimizing\n        the kernel's parameters, specified by a string, or an externally\n        defined optimizer passed as a callable. If a callable is passed, it\n        must have the signature::\n\n            def optimizer(obj_func, initial_theta, bounds):\n                # * 'obj_func': the objective function to be minimized, which\n                #   takes the hyperparameters theta as a parameter and an\n                #   optional flag eval_gradient, which determines if the\n                #   gradient is returned additionally to the function value\n                # * 'initial_theta': the initial value for theta, which can be\n                #   used by local optimizers\n                # * 'bounds': the bounds on the values of theta\n                ....\n                # Returned are the best found hyperparameters theta and\n                # the corresponding value of the target function.\n                return theta_opt, func_min\n\n        Per default, the L-BFGS-B algorithm from `scipy.optimize.minimize`\n        is used. If None is passed, the kernel's parameters are kept fixed.\n        Available internal optimizers are: `{'fmin_l_bfgs_b'}`.\n\n    n_restarts_optimizer : int, default=0\n        The number of restarts of the optimizer for finding the kernel's\n        parameters which maximize the log-marginal likelihood. The first run\n        of the optimizer is performed from the kernel's initial parameters,\n        the remaining ones (if any) from thetas sampled log-uniform randomly\n        from the space of allowed theta-values. If greater than 0, all bounds\n        must be finite. Note that `n_restarts_optimizer == 0` implies that one\n        run is performed.\n\n    normalize_y : bool, default=False\n        Whether or not to normalize the target values `y` by removing the mean\n        and scaling to unit-variance. This is recommended for cases where\n        zero-mean, unit-variance priors are used. Note that, in this\n        implementation, the normalisation is reversed before the GP predictions\n        are reported.\n\n        .. versionchanged:: 0.23\n\n    copy_X_train : bool, default=True\n        If True, a persistent copy of the training data is stored in the\n        object. Otherwise, just a reference to the training data is stored,\n        which might cause predictions to change if the data is modified\n        externally.\n\n    n_targets : int, default=None\n        The number of dimensions of the target values. Used to decide the number\n        of outputs when sampling from the prior distributions (i.e. calling\n        :meth:`sample_y` before :meth:`fit`). This parameter is ignored once\n        :meth:`fit` has been called.\n\n        .. versionadded:: 1.3\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation used to initialize the centers.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    X_train_ : array-like of shape (n_samples, n_features) or list of object\n        Feature vectors or other representations of training data (also\n        required for prediction).\n\n    y_train_ : array-like of shape (n_samples,) or (n_samples, n_targets)\n        Target values in training data (also required for prediction).\n\n    kernel_ : kernel instance\n        The kernel used for prediction. The structure of the kernel is the\n        same as the one passed as parameter but with optimized hyperparameters.\n\n    L_ : array-like of shape (n_samples, n_samples)\n        Lower-triangular Cholesky decomposition of the kernel in ``X_train_``.\n\n    alpha_ : array-like of shape (n_samples,)\n        Dual coefficients of training data points in kernel space.\n\n    log_marginal_likelihood_value_ : float\n        The log-marginal-likelihood of ``self.kernel_.theta``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    GaussianProcessClassifier : Gaussian process classification (GPC)\n        based on Laplace approximation.\n\n    References\n    ----------\n    .. [RW2006] `Carl E. Rasmussen and Christopher K.I. Williams,\n       \"Gaussian Processes for Machine Learning\",\n       MIT Press 2006 <https://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_friedman2\n    >>> from sklearn.gaussian_process import GaussianProcessRegressor\n    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n    >>> kernel = DotProduct() + WhiteKernel()\n    >>> gpr = GaussianProcessRegressor(kernel=kernel,\n    ...         random_state=0).fit(X, y)\n    >>> gpr.score(X, y)\n    0.3680...\n    >>> gpr.predict(X[:2,:], return_std=True)\n    (array([653.0..., 592.1...]), array([316.6..., 316.6...]))\n    \"\"\"\n    _parameter_constraints: dict = {'kernel': [None, Kernel], 'alpha': [Interval(Real, 0, None, closed='left'), np.ndarray], 'optimizer': [StrOptions({'fmin_l_bfgs_b'}), callable, None], 'n_restarts_optimizer': [Interval(Integral, 0, None, closed='left')], 'normalize_y': ['boolean'], 'copy_X_train': ['boolean'], 'n_targets': [Interval(Integral, 1, None, closed='left'), None], 'random_state': ['random_state']}\n\n    def predict(self, X, return_std=False, return_cov=False):\n        \"\"\"Predict using the Gaussian process regression model.\n\n        We can also predict based on an unfitted model by using the GP prior.\n        In addition to the mean of the predictive distribution, optionally also\n        returns its standard deviation (`return_std=True`) or covariance\n        (`return_cov=True`). Note that at most one of the two can be requested.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or list of object\n            Query points where the GP is evaluated.\n\n        return_std : bool, default=False\n            If True, the standard-deviation of the predictive distribution at\n            the query points is returned along with the mean.\n\n        return_cov : bool, default=False\n            If True, the covariance of the joint predictive distribution at\n            the query points is returned along with the mean.\n\n        Returns\n        -------\n        y_mean : ndarray of shape (n_samples,) or (n_samples, n_targets)\n            Mean of predictive distribution at query points.\n\n        y_std : ndarray of shape (n_samples,) or (n_samples, n_targets), optional\n            Standard deviation of predictive distribution at query points.\n            Only returned when `return_std` is True.\n\n        y_cov : ndarray of shape (n_samples, n_samples) or                 (n_samples, n_samples, n_targets), optional\n            Covariance of joint predictive distribution at query points.\n            Only returned when `return_cov` is True.\n        \"\"\"\n        if return_std and return_cov:\n            raise RuntimeError('At most one of return_std or return_cov can be requested.')\n        if self.kernel is None or self.kernel.requires_vector_input:\n            (dtype, ensure_2d) = ('numeric', True)\n        else:\n            (dtype, ensure_2d) = (None, False)\n        X = self._validate_data(X, ensure_2d=ensure_2d, dtype=dtype, reset=False)\n        if not hasattr(self, 'X_train_'):\n            if self.kernel is None:\n                kernel = C(1.0, constant_value_bounds='fixed') * RBF(1.0, length_scale_bounds='fixed')\n            else:\n                kernel = self.kernel\n            n_targets = self.n_targets if self.n_targets is not None else 1\n            y_mean = np.zeros(shape=(X.shape[0], n_targets)).squeeze()\n            if return_cov:\n                y_cov = kernel(X)\n                if n_targets > 1:\n                    y_cov = np.repeat(np.expand_dims(y_cov, -1), repeats=n_targets, axis=-1)\n                return (y_mean, y_cov)\n            elif return_std:\n                y_var = kernel.diag(X)\n                if n_targets > 1:\n                    y_var = np.repeat(np.expand_dims(y_var, -1), repeats=n_targets, axis=-1)\n                return (y_mean, np.sqrt(y_var))\n            else:\n                return y_mean\n        else:\n            K_trans = self.kernel_(X, self.X_train_)\n            y_mean = K_trans @ self.alpha_\n            y_mean = self._y_train_std * y_mean + self._y_train_mean\n            if y_mean.ndim > 1 and y_mean.shape[1] == 1:\n                y_mean = np.squeeze(y_mean, axis=1)\n            V = solve_triangular(self.L_, K_trans.T, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n            if return_cov:\n                y_cov = self.kernel_(X) - V.T @ V\n                y_cov = np.outer(y_cov, self._y_train_std ** 2).reshape(*y_cov.shape, -1)\n                if y_cov.shape[2] == 1:\n                    y_cov = np.squeeze(y_cov, axis=2)\n                return (y_mean, y_cov)\n            elif return_std:\n                y_var = self.kernel_.diag(X).copy()\n                y_var -= np.einsum('ij,ji->i', V.T, V)\n                y_var_negative = y_var < 0\n                if np.any(y_var_negative):\n                    warnings.warn('Predicted variances smaller than 0. Setting those variances to 0.')\n                    y_var[y_var_negative] = 0.0\n                y_var = np.outer(y_var, self._y_train_std ** 2).reshape(*y_var.shape, -1)\n                if y_var.shape[1] == 1:\n                    y_var = np.squeeze(y_var, axis=1)\n                return (y_mean, np.sqrt(y_var))\n            else:\n                return y_mean\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y):\n        \"\"\"Fit Gaussian process regression model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or list of object\n            Feature vectors or other representations of training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            GaussianProcessRegressor class instance.\n        \"\"\"\n        if self.kernel is None:\n            self.kernel_ = C(1.0, constant_value_bounds='fixed') * RBF(1.0, length_scale_bounds='fixed')\n        else:\n            self.kernel_ = clone(self.kernel)\n        self._rng = check_random_state(self.random_state)\n        if self.kernel_.requires_vector_input:\n            (dtype, ensure_2d) = ('numeric', True)\n        else:\n            (dtype, ensure_2d) = (None, False)\n        (X, y) = self._validate_data(X, y, multi_output=True, y_numeric=True, ensure_2d=ensure_2d, dtype=dtype)\n        n_targets_seen = y.shape[1] if y.ndim > 1 else 1\n        if self.n_targets is not None and n_targets_seen != self.n_targets:\n            raise ValueError(f'The number of targets seen in `y` is different from the parameter `n_targets`. Got {n_targets_seen} != {self.n_targets}.')\n        if self.normalize_y:\n            self._y_train_mean = np.mean(y, axis=0)\n            self._y_train_std = _handle_zeros_in_scale(np.std(y, axis=0), copy=False)\n            y = (y - self._y_train_mean) / self._y_train_std\n        else:\n            shape_y_stats = (y.shape[1],) if y.ndim == 2 else 1\n            self._y_train_mean = np.zeros(shape=shape_y_stats)\n            self._y_train_std = np.ones(shape=shape_y_stats)\n        if np.iterable(self.alpha) and self.alpha.shape[0] != y.shape[0]:\n            if self.alpha.shape[0] == 1:\n                self.alpha = self.alpha[0]\n            else:\n                raise ValueError(f'alpha must be a scalar or an array with same number of entries as y. ({self.alpha.shape[0]} != {y.shape[0]})')\n        self.X_train_ = np.copy(X) if self.copy_X_train else X\n        self.y_train_ = np.copy(y) if self.copy_X_train else y\n        if self.optimizer is not None and self.kernel_.n_dims > 0:\n\n            def obj_func(theta, eval_gradient=True):\n                if eval_gradient:\n                    (lml, grad) = self.log_marginal_likelihood(theta, eval_gradient=True, clone_kernel=False)\n                    return (-lml, -grad)\n                else:\n                    return -self.log_marginal_likelihood(theta, clone_kernel=False)\n            optima = [self._constrained_optimization(obj_func, self.kernel_.theta, self.kernel_.bounds)]\n            if self.n_restarts_optimizer > 0:\n                if not np.isfinite(self.kernel_.bounds).all():\n                    raise ValueError('Multiple optimizer restarts (n_restarts_optimizer>0) requires that all bounds are finite.')\n                bounds = self.kernel_.bounds\n                for iteration in range(self.n_restarts_optimizer):\n                    theta_initial = self._rng.uniform(bounds[:, 0], bounds[:, 1])\n                    optima.append(self._constrained_optimization(obj_func, theta_initial, bounds))\n            lml_values = list(map(itemgetter(1), optima))\n            self.kernel_.theta = optima[np.argmin(lml_values)][0]\n            self.kernel_._check_bounds_params()\n            self.log_marginal_likelihood_value_ = -np.min(lml_values)\n        else:\n            self.log_marginal_likelihood_value_ = self.log_marginal_likelihood(self.kernel_.theta, clone_kernel=False)\n        K = self.kernel_(self.X_train_)\n        K[np.diag_indices_from(K)] += self.alpha\n        try:\n            self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n        except np.linalg.LinAlgError as exc:\n            exc.args = (f\"The kernel, {self.kernel_}, is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\",) + exc.args\n            raise\n        self.alpha_ = cho_solve((self.L_, GPR_CHOLESKY_LOWER), self.y_train_, check_finite=False)\n        return self\n\n    def _constrained_optimization(self, obj_func, initial_theta, bounds):\n        if self.optimizer == 'fmin_l_bfgs_b':\n            opt_res = scipy.optimize.minimize(obj_func, initial_theta, method='L-BFGS-B', jac=True, bounds=bounds)\n            _check_optimize_result('lbfgs', opt_res)\n            (theta_opt, func_min) = (opt_res.x, opt_res.fun)\n        elif callable(self.optimizer):\n            (theta_opt, func_min) = self.optimizer(obj_func, initial_theta, bounds=bounds)\n        else:\n            raise ValueError(f'Unknown optimizer {self.optimizer}.')\n        return (theta_opt, func_min)\n\n    def log_marginal_likelihood(self, theta=None, eval_gradient=False, clone_kernel=True):\n        \"\"\"Return log-marginal likelihood of theta for training data.\n\n        Parameters\n        ----------\n        theta : array-like of shape (n_kernel_params,) default=None\n            Kernel hyperparameters for which the log-marginal likelihood is\n            evaluated. If None, the precomputed log_marginal_likelihood\n            of ``self.kernel_.theta`` is returned.\n\n        eval_gradient : bool, default=False\n            If True, the gradient of the log-marginal likelihood with respect\n            to the kernel hyperparameters at position theta is returned\n            additionally. If True, theta must not be None.\n\n        clone_kernel : bool, default=True\n            If True, the kernel attribute is copied. If False, the kernel\n            attribute is modified, but may result in a performance improvement.\n\n        Returns\n        -------\n        log_likelihood : float\n            Log-marginal likelihood of theta for training data.\n\n        log_likelihood_gradient : ndarray of shape (n_kernel_params,), optional\n            Gradient of the log-marginal likelihood with respect to the kernel\n            hyperparameters at position theta.\n            Only returned when eval_gradient is True.\n        \"\"\"\n        if theta is None:\n            if eval_gradient:\n                raise ValueError('Gradient can only be evaluated for theta!=None')\n            return self.log_marginal_likelihood_value_\n        if clone_kernel:\n            kernel = self.kernel_.clone_with_theta(theta)\n        else:\n            kernel = self.kernel_\n            kernel.theta = theta\n        if eval_gradient:\n            (K, K_gradient) = kernel(self.X_train_, eval_gradient=True)\n        else:\n            K = kernel(self.X_train_)\n        K[np.diag_indices_from(K)] += self.alpha\n        try:\n            L = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n        except np.linalg.LinAlgError:\n            return (-np.inf, np.zeros_like(theta)) if eval_gradient else -np.inf\n        y_train = self.y_train_\n        if y_train.ndim == 1:\n            y_train = y_train[:, np.newaxis]\n        alpha = cho_solve((L, GPR_CHOLESKY_LOWER), y_train, check_finite=False)\n        log_likelihood_dims = -0.5 * np.einsum('ik,ik->k', y_train, alpha)\n        log_likelihood_dims -= np.log(np.diag(L)).sum()\n        log_likelihood_dims -= K.shape[0] / 2 * np.log(2 * np.pi)\n        log_likelihood = log_likelihood_dims.sum(axis=-1)\n        if eval_gradient:\n            inner_term = np.einsum('ik,jk->ijk', alpha, alpha)\n            K_inv = cho_solve((L, GPR_CHOLESKY_LOWER), np.eye(K.shape[0]), check_finite=False)\n            inner_term -= K_inv[..., np.newaxis]\n            log_likelihood_gradient_dims = 0.5 * np.einsum('ijl,jik->kl', inner_term, K_gradient)\n            log_likelihood_gradient = log_likelihood_gradient_dims.sum(axis=-1)\n        if eval_gradient:\n            return (log_likelihood, log_likelihood_gradient)\n        else:\n            return log_likelihood\n\n    def __repr__(self, N_CHAR_MAX=700):\n        from .utils._pprint import _EstimatorPrettyPrinter\n        N_MAX_ELEMENTS_TO_SHOW = 30\n        pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n        repr_ = pp.pformat(self)\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2\n            regex = '^(\\\\s*\\\\S){%d}' % lim\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n            if '\\n' in repr_[left_lim:-right_lim]:\n                regex += '[^\\\\n]*\\\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n        return repr_\n\n    def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n\n            `estimator=self` is automatically added to these dicts to generate\n            more informative error message in case of invalid input data.\n\n        cast_to_ndarray : bool, default=True\n            Cast `X` and `y` to ndarray with checks in `check_params`. If\n            `False`, `X` and `y` are unchanged and only `feature_names_in_` and\n            `n_features_in_` are checked.\n\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n            `estimator=self` is automatically added to these params to generate\n            more informative error message in case of invalid input data.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        self._check_feature_names(X, reset=reset)\n        if y is None and self._get_tags()['requires_y']:\n            raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n        no_val_X = isinstance(X, str) and X == 'no_validation'\n        no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n        if no_val_X and no_val_y:\n            raise ValueError('Validation should be done on X, y or both.')\n        default_check_params = {'estimator': self}\n        check_params = {**default_check_params, **check_params}\n        if not cast_to_ndarray:\n            if not no_val_X and no_val_y:\n                out = X\n            elif no_val_X and (not no_val_y):\n                out = y\n            else:\n                out = (X, y)\n        elif not no_val_X and no_val_y:\n            out = check_array(X, input_name='X', **check_params)\n        elif no_val_X and (not no_val_y):\n            out = _check_y(y, **check_params)\n        else:\n            if validate_separately:\n                (check_X_params, check_y_params) = validate_separately\n                if 'estimator' not in check_X_params:\n                    check_X_params = {**default_check_params, **check_X_params}\n                X = check_array(X, input_name='X', **check_X_params)\n                if 'estimator' not in check_y_params:\n                    check_y_params = {**default_check_params, **check_y_params}\n                y = check_array(y, input_name='y', **check_y_params)\n            else:\n                (X, y) = check_X_y(X, y, **check_params)\n            out = (X, y)\n        if not no_val_X and check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n        return out\n\n    def __init__(self, kernel=None, *, alpha=1e-10, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=0, normalize_y=False, copy_X_train=True, n_targets=None, random_state=None):\n        self.kernel = kernel\n        self.alpha = alpha\n        self.optimizer = optimizer\n        self.n_restarts_optimizer = n_restarts_optimizer\n        self.normalize_y = normalize_y\n        self.copy_X_train = copy_X_train\n        self.n_targets = n_targets\n        self.random_state = random_state\n\n    def sample_y(self, X, n_samples=1, random_state=0):\n        \"\"\"Draw samples from Gaussian process and evaluate at X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples_X, n_features) or list of object\n            Query points where the GP is evaluated.\n\n        n_samples : int, default=1\n            Number of samples drawn from the Gaussian process per query point.\n\n        random_state : int, RandomState instance or None, default=0\n            Determines random number generation to randomly draw samples.\n            Pass an int for reproducible results across multiple function\n            calls.\n            See :term:`Glossary <random_state>`.\n\n        Returns\n        -------\n        y_samples : ndarray of shape (n_samples_X, n_samples), or             (n_samples_X, n_targets, n_samples)\n            Values of n_samples samples drawn from Gaussian process and\n            evaluated at query points.\n        \"\"\"\n        rng = check_random_state(random_state)\n        (y_mean, y_cov) = self.predict(X, return_cov=True)\n        if y_mean.ndim == 1:\n            y_samples = rng.multivariate_normal(y_mean, y_cov, n_samples).T\n        else:\n            y_samples = [rng.multivariate_normal(y_mean[:, target], y_cov[..., target], n_samples).T[:, np.newaxis] for target in range(y_mean.shape[1])]\n            y_samples = np.hstack(y_samples)\n        return y_samples\n\n    def _more_tags(self):\n        return {'requires_fit': False}\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\\\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).\n        \"\"\"\n        from .metrics import r2_score\n        y_pred = self.predict(X)\n        return r2_score(y, y_pred, sample_weight=sample_weight)", "class_fn": true, "question_id": "sklearn/sklearn.gaussian_process._gpr/GaussianProcessRegressor", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/gaussian_process/kernels.py", "fn_id": "", "content": "class DotProduct(Kernel):\n    \"\"\"Dot-Product kernel.\n\n    The DotProduct kernel is non-stationary and can be obtained from linear\n    regression by putting :math:`N(0, 1)` priors on the coefficients\n    of :math:`x_d (d = 1, . . . , D)` and a prior of :math:`N(0, \\\\sigma_0^2)`\n    on the bias. The DotProduct kernel is invariant to a rotation of\n    the coordinates about the origin, but not translations.\n    It is parameterized by a parameter sigma_0 :math:`\\\\sigma`\n    which controls the inhomogenity of the kernel. For :math:`\\\\sigma_0^2 =0`,\n    the kernel is called the homogeneous linear kernel, otherwise\n    it is inhomogeneous. The kernel is given by\n\n    .. math::\n        k(x_i, x_j) = \\\\sigma_0 ^ 2 + x_i \\\\cdot x_j\n\n    The DotProduct kernel is commonly combined with exponentiation.\n\n    See [1]_, Chapter 4, Section 4.2, for further details regarding the\n    DotProduct kernel.\n\n    Read more in the :ref:`User Guide <gp_kernels>`.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    sigma_0 : float >= 0, default=1.0\n        Parameter controlling the inhomogenity of the kernel. If sigma_0=0,\n        the kernel is homogeneous.\n\n    sigma_0_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n        The lower and upper bound on 'sigma_0'.\n        If set to \"fixed\", 'sigma_0' cannot be changed during\n        hyperparameter tuning.\n\n    References\n    ----------\n    .. [1] `Carl Edward Rasmussen, Christopher K. I. Williams (2006).\n        \"Gaussian Processes for Machine Learning\". The MIT Press.\n        <http://www.gaussianprocess.org/gpml/>`_\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_friedman2\n    >>> from sklearn.gaussian_process import GaussianProcessRegressor\n    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n    >>> kernel = DotProduct() + WhiteKernel()\n    >>> gpr = GaussianProcessRegressor(kernel=kernel,\n    ...         random_state=0).fit(X, y)\n    >>> gpr.score(X, y)\n    0.3680...\n    >>> gpr.predict(X[:2,:], return_std=True)\n    (array([653.0..., 592.1...]), array([316.6..., 316.6...]))\n    \"\"\"\n\n    def diag(self, X):\n        \"\"\"Returns the diagonal of the kernel k(X, X).\n\n        The result of this method is identical to np.diag(self(X)); however,\n        it can be evaluated more efficiently since only the diagonal is\n        evaluated.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y).\n\n        Returns\n        -------\n        K_diag : ndarray of shape (n_samples_X,)\n            Diagonal of kernel k(X, X).\n        \"\"\"\n        return np.einsum('ij,ij->i', X, X) + self.sigma_0 ** 2\n\n    @property\n    def hyperparameter_sigma_0(self):\n        return Hyperparameter('sigma_0', 'numeric', self.sigma_0_bounds)\n\n    def __repr__(self):\n        return '{0}(sigma_0={1:.3g})'.format(self.__class__.__name__, self.sigma_0)\n\n    def get_params(self, deep=True):\n        \"\"\"Get parameters of this kernel.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        params = dict()\n        cls = self.__class__\n        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n        init_sign = signature(init)\n        (args, varargs) = ([], [])\n        for parameter in init_sign.parameters.values():\n            if parameter.kind != parameter.VAR_KEYWORD and parameter.name != 'self':\n                args.append(parameter.name)\n            if parameter.kind == parameter.VAR_POSITIONAL:\n                varargs.append(parameter.name)\n        if len(varargs) != 0:\n            raise RuntimeError(\"scikit-learn kernels should always specify their parameters in the signature of their __init__ (no varargs). %s doesn't follow this convention.\" % (cls,))\n        for arg in args:\n            params[arg] = getattr(self, arg)\n        return params\n\n    def is_stationary(self):\n        \"\"\"Returns whether the kernel is stationary.\"\"\"\n        return False\n\n    def __mul__(self, b):\n        if not isinstance(b, Kernel):\n            return Product(self, ConstantKernel(b))\n        return Product(self, b)\n\n    def __call__(self, X, Y=None, eval_gradient=False):\n        \"\"\"Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : ndarray of shape (n_samples_Y, n_features), default=None\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool, default=False\n            Determines whether the gradient with respect to the log of\n            the kernel hyperparameter is computed.\n            Only supported when Y is None.\n\n        Returns\n        -------\n        K : ndarray of shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),                optional\n            The gradient of the kernel k(X, X) with respect to the log of the\n            hyperparameter of the kernel. Only returned when `eval_gradient`\n            is True.\n        \"\"\"\n        X = np.atleast_2d(X)\n        if Y is None:\n            K = np.inner(X, X) + self.sigma_0 ** 2\n        else:\n            if eval_gradient:\n                raise ValueError('Gradient can only be evaluated when Y is None.')\n            K = np.inner(X, Y) + self.sigma_0 ** 2\n        if eval_gradient:\n            if not self.hyperparameter_sigma_0.fixed:\n                K_gradient = np.empty((K.shape[0], K.shape[1], 1))\n                K_gradient[..., 0] = 2 * self.sigma_0 ** 2\n                return (K, K_gradient)\n            else:\n                return (K, np.empty((X.shape[0], X.shape[0], 0)))\n        else:\n            return K\n\n    def __eq__(self, b):\n        if type(self) != type(b):\n            return False\n        params_a = self.get_params()\n        params_b = b.get_params()\n        for key in set(list(params_a.keys()) + list(params_b.keys())):\n            if np.any(params_a.get(key, None) != params_b.get(key, None)):\n                return False\n        return True\n\n    def __init__(self, sigma_0=1.0, sigma_0_bounds=(1e-05, 100000.0)):\n        self.sigma_0 = sigma_0\n        self.sigma_0_bounds = sigma_0_bounds", "class_fn": true, "question_id": "sklearn/sklearn.gaussian_process.kernels/DotProduct", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/gaussian_process/kernels.py", "fn_id": "", "content": "class GenericKernelMixin:\n    \"\"\"Mixin for kernels which operate on generic objects such as variable-\n    length sequences, trees, and graphs.\n\n    .. versionadded:: 0.22\n    \"\"\"\n\n    @property\n    def requires_vector_input(self):\n        \"\"\"Whether the kernel works only on fixed-length feature vectors.\"\"\"\n        return False", "class_fn": true, "question_id": "sklearn/sklearn.gaussian_process.kernels/GenericKernelMixin", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": false}
{"repo": "sklearn", "url": "sklearn==1.5.2", "last_update_at": "", "file": "sklearn/gaussian_process/kernels.py", "fn_id": "", "content": "class KernelOperator(Kernel):\n    \"\"\"Base class for all kernel operators.\n\n    .. versionadded:: 0.18\n    \"\"\"\n\n    def __mul__(self, b):\n        if not isinstance(b, Kernel):\n            return Product(self, ConstantKernel(b))\n        return Product(self, b)\n\n    def __repr__(self):\n        return '{0}({1})'.format(self.__class__.__name__, ', '.join(map('{0:.3g}'.format, self.theta)))\n\n    @property\n    def hyperparameters(self):\n        \"\"\"Returns a list of all hyperparameter.\"\"\"\n        r = [Hyperparameter('k1__' + hyperparameter.name, hyperparameter.value_type, hyperparameter.bounds, hyperparameter.n_elements) for hyperparameter in self.k1.hyperparameters]\n        for hyperparameter in self.k2.hyperparameters:\n            r.append(Hyperparameter('k2__' + hyperparameter.name, hyperparameter.value_type, hyperparameter.bounds, hyperparameter.n_elements))\n        return r\n\n    @property\n    def theta(self):\n        \"\"\"Returns the (flattened, log-transformed) non-fixed hyperparameters.\n\n        Note that theta are typically the log-transformed values of the\n        kernel's hyperparameters as this representation of the search space\n        is more amenable for hyperparameter search, as hyperparameters like\n        length-scales naturally live on a log-scale.\n\n        Returns\n        -------\n        theta : ndarray of shape (n_dims,)\n            The non-fixed, log-transformed hyperparameters of the kernel\n        \"\"\"\n        return np.append(self.k1.theta, self.k2.theta)\n\n    @theta.setter\n    def theta(self, theta):\n        \"\"\"Sets the (flattened, log-transformed) non-fixed hyperparameters.\n\n        Parameters\n        ----------\n        theta : ndarray of shape (n_dims,)\n            The non-fixed, log-transformed hyperparameters of the kernel\n        \"\"\"\n        k1_dims = self.k1.n_dims\n        self.k1.theta = theta[:k1_dims]\n        self.k2.theta = theta[k1_dims:]\n\n    @property\n    def bounds(self):\n        \"\"\"Returns the log-transformed bounds on the theta.\n\n        Returns\n        -------\n        bounds : ndarray of shape (n_dims, 2)\n            The log-transformed bounds on the kernel's hyperparameters theta\n        \"\"\"\n        if self.k1.bounds.size == 0:\n            return self.k2.bounds\n        if self.k2.bounds.size == 0:\n            return self.k1.bounds\n        return np.vstack((self.k1.bounds, self.k2.bounds))\n\n    def get_params(self, deep=True):\n        \"\"\"Get parameters of this kernel.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        params = dict(k1=self.k1, k2=self.k2)\n        if deep:\n            deep_items = self.k1.get_params().items()\n            params.update((('k1__' + k, val) for (k, val) in deep_items))\n            deep_items = self.k2.get_params().items()\n            params.update((('k2__' + k, val) for (k, val) in deep_items))\n        return params\n\n    def __eq__(self, b):\n        if type(self) != type(b):\n            return False\n        return self.k1 == b.k1 and self.k2 == b.k2 or (self.k1 == b.k2 and self.k2 == b.k1)\n\n    @property\n    def requires_vector_input(self):\n        \"\"\"Returns whether the kernel is stationary.\"\"\"\n        return self.k1.requires_vector_input or self.k2.requires_vector_input\n\n    def __init__(self, k1, k2):\n        self.k1 = k1\n        self.k2 = k2\n\n    def clone_with_theta(self, theta):\n        \"\"\"Returns a clone of self with given hyperparameters theta.\n\n        Parameters\n        ----------\n        theta : ndarray of shape (n_dims,)\n            The hyperparameters\n        \"\"\"\n        cloned = clone(self)\n        cloned.theta = theta\n        return cloned\n\n    def is_stationary(self):\n        \"\"\"Returns whether the kernel is stationary.\"\"\"\n        return self.k1.is_stationary() and self.k2.is_stationary()", "class_fn": true, "question_id": "sklearn/sklearn.gaussian_process.kernels/KernelOperator", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [], "refactored": true}
