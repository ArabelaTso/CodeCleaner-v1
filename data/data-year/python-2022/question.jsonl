{"hexsha": "8ad289b92000df0c97a926e605855965c9db0ef2", "ext": "py", "lang": "Python", "content": "async def async_setup_platform(hass, config, async_add_entities, discovery_info=None):\n    \"\"\"Set up the Sense sensor.\"\"\"\n    if discovery_info is None:\n        return\n    data = hass.data[SENSE_DATA]\n\n    @Throttle(MIN_TIME_BETWEEN_DAILY_UPDATES)\n    async def update_trends():\n        \"\"\"Update the daily power usage.\"\"\"\n        await data.update_trend_data()\n\n    async def update_active():\n        \"\"\"Update the active power usage.\"\"\"\n        await data.update_realtime()\n\n    devices = []\n    for typ in SENSOR_TYPES.values():\n        for var in SENSOR_VARIANTS:\n            name = typ.name\n            sensor_type = typ.sensor_type\n            is_production = var == PRODUCTION_NAME.lower()\n            if sensor_type == ACTIVE_TYPE:\n                update_call = update_active\n            else:\n                update_call = update_trends\n            devices.append(Sense(data, name, sensor_type, is_production, update_call))\n\n    async_add_entities(devices)", "fn_id": 0, "class_fn": false, "repo": "alemuro/home-assistant", "file": "homeassistant/components/sense/sensor.py", "last_update_at": "2022-01-29T10:33:20+00:00", "question_id": "8ad289b92000df0c97a926e605855965c9db0ef2_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def async_setup_platform(hass, config, async_add_entities, discovery_info=None):\n    \"\"\"Set up the Sense sensor.\"\"\"\n    if discovery_info is None:\n        return\n    data = hass.data[SENSE_DATA]\n\n    @Throttle(MIN_TIME_BETWEEN_DAILY_UPDATES)\n    async def update_trends():\n        \"\"\"Update the daily power usage.\"\"\"\n        await data.update_trend_data()\n\n    async def update_active():\n        \"\"\"Update the active power usage.\"\"\"\n        await data.update_realtime()\n    devices = []\n    for typ in SENSOR_TYPES.values():\n        for var in SENSOR_VARIANTS:\n            name = typ.name\n            sensor_type = typ.sensor_type\n            is_production = var == PRODUCTION_NAME.lower()\n            if sensor_type == ACTIVE_TYPE:\n                update_call = update_active\n            else:\n                update_call = update_trends\n            devices.append(Sense(data, name, sensor_type, is_production, update_call))\n"]]}
{"hexsha": "e13713df839db72fe116f231b7c8f845532df4f9", "ext": "py", "lang": "Python", "content": "def find_object_name(scene, name):\n    if scene.name == name:\n        return scene\n\n    for child in scene.children:\n        result = find_object_name(child, name)\n        if result is not None:\n            return result\n\n    return None", "fn_id": 8, "class_fn": false, "repo": "greck2908/BeamNGpy", "file": "tests/test_scenario.py", "last_update_at": "2022-03-26T15:02:54+00:00", "question_id": "e13713df839db72fe116f231b7c8f845532df4f9_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def find_object_name(scene, name):\n    if scene.name == name:\n        return scene\n    for child in scene.children:\n        result = find_object_name(child, name)\n        if result is not None:\n            return result\n"]]}
{"hexsha": "0caeb81d302573fdde3e70660b676ceddefc8756", "ext": "py", "lang": "Python", "content": "def one_nash_filter(solver, threshold=0.001):\n    num_players = solver._num_players\n    filtered_idx_list = []\n    len_filtered_strategies = []\n\n    meta_games = solver.get_meta_game()\n    policies = solver.get_policies()\n    num_str, _ = np.shape(meta_games[0])\n    if num_str <= solver.strategy_set_size:\n        return meta_games, policies\n\n    solver.update_meta_strategies()\n    nash = solver.get_meta_strategies()\n\n    for player in range(num_players):\n        zero_pos = np.where(nash[player] <= threshold)[0]\n        filtered_idx_list.append(zero_pos)\n        len_filtered_strategies.append(len(zero_pos))\n\n    if np.sum(len_filtered_strategies) == 0:\n        return solver._meta_games, solver._policies\n\n    for player in range(num_players):\n        # filter meta_games.\n        for dim in range(num_players):\n            filtered_idx = filtered_idx_list[dim]\n            meta_games[player] = np.delete(meta_games[player], filtered_idx, axis=dim)\n        # filter policies.\n        policies[player] = np.delete(policies[player], filtered_idx_list[player])\n        policies[player] = list(policies[player])\n\n    print(\"Strategies filtered:\")\n    num_str_players = []\n    for player in range(num_players):\n        print(\"Player \" + str(player) + \":\", filtered_idx_list[player])\n        num_str_players.append(len(policies[player]))\n    print(\"Number of strategies after filtering:\", num_str_players)\n\n    return meta_games, policies", "fn_id": 4, "class_fn": false, "repo": "wyz2368/open_spiel3", "file": "open_spiel/python/algorithms/filtered_psro/strategy_fliter.py", "last_update_at": "2022-02-10T02:08:27+00:00", "question_id": "0caeb81d302573fdde3e70660b676ceddefc8756_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def one_nash_filter(solver, threshold=0.001):\n    num_players = solver._num_players\n    filtered_idx_list = []\n    len_filtered_strategies = []\n    meta_games = solver.get_meta_game()\n    policies = solver.get_policies()\n    num_str, _ = np.shape(meta_games[0])\n    if num_str <= solver.strategy_set_size:\n        return (meta_games, policies)\n    solver.update_meta_strategies()\n    nash = solver.get_meta_strategies()\n    for player in range(num_players):\n        zero_pos = np.where(nash[player] <= threshold)[0]\n        filtered_idx_list.append(zero_pos)\n        len_filtered_strategies.append(len(zero_pos))\n    if np.sum(len_filtered_strategies) == 0:\n        return (solver._meta_games, solver._policies)\n    for player in range(num_players):\n        for dim in range(num_players):\n            filtered_idx = filtered_idx_list[dim]\n            meta_games[player] = np.delete(meta_games[player], filtered_idx, axis=dim)\n        policies[player] = np.delete(policies[player], filtered_idx_list[player])\n        policies[player] = list(policies[player])\n    print('Strategies filtered:')\n    num_str_players = []\n    for player in range(num_players):\n        print('Player ' + str(player) + ':', filtered_idx_list[player])\n        num_str_players.append(len(policies[player]))\n    print('Number of strategies after filtering:', num_str_players)\n"]]}
{"hexsha": "df21fb9ee1f9f03ad1344e0bb4c6d7fd1dee98e3", "ext": "py", "lang": "Python", "content": "def set_votes(user, category, winner, loser):\n    query = '''\n        INSERT INTO cafe.votes (user_id, category_id, winner_baker_id,\n                    loser_baker_id)\n             VALUES (%(user_id)s, %(category_id)s, %(winner_id)s, %(loser_id)s)\n    '''\n    params = {\n        'user_id': user.id,\n        'category_id': category['id'],\n        'winner_id': winner['baker_id'],\n        'loser_id': loser['baker_id'],\n    }\n    conn = connection()\n    cursor = conn.cursor()\n    cursor.execute(query, params)\n    conn.commit()\n    conn.close()", "fn_id": 9, "class_fn": false, "repo": "lekha/banana-bread", "file": "server/server/database.py", "last_update_at": "2022-02-26T09:41:40+00:00", "question_id": "df21fb9ee1f9f03ad1344e0bb4c6d7fd1dee98e3_9", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def set_votes(user, category, winner, loser):\n    query = '\\n        INSERT INTO cafe.votes (user_id, category_id, winner_baker_id,\\n                    loser_baker_id)\\n             VALUES (%(user_id)s, %(category_id)s, %(winner_id)s, %(loser_id)s)\\n    '\n    params = {'user_id': user.id, 'category_id': category['id'], 'winner_id': winner['baker_id'], 'loser_id': loser['baker_id']}\n    conn = connection()\n    cursor = conn.cursor()\n    cursor.execute(query, params)\n    conn.commit()\n"]]}
{"hexsha": "ffa6de6d762ee4a605a4e2e3dd37330e1a623ba6", "ext": "py", "lang": "Python", "content": "def test_dot_real(data_dict):\n    def get_iter(path, data_shape, batch_size):\n        data_train = mx.io.LibSVMIter(data_libsvm=path,\n                                      data_shape=data_shape,\n                                      batch_size=batch_size)\n        data_iter = iter(data_train)\n        return data_iter\n\n    data_dir = os.path.join(os.getcwd(), 'data')\n\n    path = os.path.join(data_dir, data_dict['data_name'])\n    if not os.path.exists(path):\n        get_bz2_data(\n            data_dir,\n            data_dict['data_name'],\n            data_dict['url'],\n            data_dict['data_origin_name']\n        )\n        assert os.path.exists(path)\n\n    k = data_dict['feature_dim']\n    m = data_dict['m']\n    density = estimate_density(path, data_dict['feature_dim'])\n\n    mini_path = os.path.join(data_dir, data_dict['data_mini'])\n    if not os.path.exists(mini_path):\n        os.system(\"head -n 2000 %r > %r\" % (path, mini_path))\n        assert os.path.exists(mini_path)\n\n    print(\"Running Benchmarking on %r data\" % data_dict['data_mini'])\n    for batch_size in data_dict['batch_size']:  # iterator through different batch size of choice\n        print(\"batch_size is %d\" % batch_size)\n        # model\n        data_shape = (k, )\n        train_iter = get_iter(mini_path, data_shape, batch_size)\n        weight = mx.nd.random.uniform(low=0, high=1, shape=(k, m))\n\n        csr_data = []\n        dns_data = []\n        num_batch = 0\n        for batch in train_iter:\n            data = train_iter.getdata()\n            csr_data.append(data)\n            dns_data.append(data.tostype('default'))\n            num_batch += 1\n        bag_of_data = [csr_data, dns_data]\n        num_repeat = 5\n        costs = []\n        for d in bag_of_data:\n            weight.wait_to_read()\n            cost = 0.\n            count = 0\n            for d_batch in d:\n                d_batch.wait_to_read()\n                cost += measure_cost(num_repeat, mx.nd.dot, d_batch, weight)\n                count += 1\n            costs.append(cost/count)\n        t_sparse = costs[0]\n        t_dense = costs[1]\n        ratio = t_dense / t_sparse\n        print('density(%)\\tn\\tm\\tk\\tt_dense/t_sparse\\tt_dense\\tt_sparse')\n        fmt = \"%0.4f\\t\\t%d\\t%d\\t%d\\t%0.2f\\t\\t\\t%0.4f\\t%0.6f\"\n        print(fmt % (density * 100, batch_size, m, k, ratio, t_dense, t_sparse))", "fn_id": 1, "class_fn": false, "repo": "Vikas-kum/incubator-mxnet", "file": "benchmark/python/sparse/sparse_op.py", "last_update_at": "2022-03-08T17:02:02+00:00", "question_id": "ffa6de6d762ee4a605a4e2e3dd37330e1a623ba6_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_dot_real(data_dict):\n\n    def get_iter(path, data_shape, batch_size):\n        data_train = mx.io.LibSVMIter(data_libsvm=path, data_shape=data_shape, batch_size=batch_size)\n        data_iter = iter(data_train)\n        return data_iter\n    data_dir = os.path.join(os.getcwd(), 'data')\n    path = os.path.join(data_dir, data_dict['data_name'])\n    if not os.path.exists(path):\n        get_bz2_data(data_dir, data_dict['data_name'], data_dict['url'], data_dict['data_origin_name'])\n        assert os.path.exists(path)\n    k = data_dict['feature_dim']\n    m = data_dict['m']\n    density = estimate_density(path, data_dict['feature_dim'])\n    mini_path = os.path.join(data_dir, data_dict['data_mini'])\n    if not os.path.exists(mini_path):\n        os.system('head -n 2000 %r > %r' % (path, mini_path))\n        assert os.path.exists(mini_path)\n    print('Running Benchmarking on %r data' % data_dict['data_mini'])\n    for batch_size in data_dict['batch_size']:\n        print('batch_size is %d' % batch_size)\n        data_shape = (k,)\n        train_iter = get_iter(mini_path, data_shape, batch_size)\n        weight = mx.nd.random.uniform(low=0, high=1, shape=(k, m))\n        csr_data = []\n        dns_data = []\n        num_batch = 0\n        for batch in train_iter:\n            data = train_iter.getdata()\n            csr_data.append(data)\n            dns_data.append(data.tostype('default'))\n            num_batch += 1\n        bag_of_data = [csr_data, dns_data]\n        num_repeat = 5\n        costs = []\n        for d in bag_of_data:\n            weight.wait_to_read()\n            cost = 0.0\n            count = 0\n            for d_batch in d:\n                d_batch.wait_to_read()\n                cost += measure_cost(num_repeat, mx.nd.dot, d_batch, weight)\n                count += 1\n            costs.append(cost / count)\n        t_sparse = costs[0]\n        t_dense = costs[1]\n        ratio = t_dense / t_sparse\n        print('density(%)\\tn\\tm\\tk\\tt_dense/t_sparse\\tt_dense\\tt_sparse')\n        fmt = '%0.4f\\t\\t%d\\t%d\\t%d\\t%0.2f\\t\\t\\t%0.4f\\t%0.6f'\n"]]}
{"hexsha": "5379a06205929b39569693f7b2e2e97805dedd9b", "ext": "py", "lang": "Python", "content": "def get_class_property_groups(df):\n    \"\"\"\n    :param df:\n    :return: d, Counter\n        d: {\n            \"<concept> <property>\": \"<concept>/<property_fname>\",\n        },\n        Counter: number of rows for each concept/property combination\n            of \"<concept>/<property_fname>\"\n    \"\"\"\n    d = dict()\n    counts = []\n    for idx, row in df.iterrows():\n        c = row[\"concept\"]\n        ps = row[\"property\"].split(';')\n        prev_identified = None\n        v = \"%s/%s\" % (c, uri_to_fname(ps[0]).replace('-', ':'))\n        for p in ps:\n            k = c + \" \" + p\n            if k in d:\n                prev_identified = d[k]\n                break\n            d[k] = v\n\n        if prev_identified:\n            # print(\"picking a prev-identified\\t%s\\t%d\" % (prev_identified, idx))\n            v = prev_identified\n            for p in ps:\n                k = c + \" \" + p\n                d[k] = prev_identified\n        counts.append(v)\n    return d, Counter(counts)", "fn_id": 1, "class_fn": false, "repo": "ahmad88me/tada-qq-experiment", "file": "clus/t2dv2.py", "last_update_at": "2022-03-28T08:24:00+00:00", "question_id": "5379a06205929b39569693f7b2e2e97805dedd9b_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_class_property_groups(df):\n    \"\"\"\n    :param df:\n    :return: d, Counter\n        d: {\n            \"<concept> <property>\": \"<concept>/<property_fname>\",\n        },\n        Counter: number of rows for each concept/property combination\n            of \"<concept>/<property_fname>\"\n    \"\"\"\n    d = dict()\n    counts = []\n    for idx, row in df.iterrows():\n        c = row['concept']\n        ps = row['property'].split(';')\n        prev_identified = None\n        v = '%s/%s' % (c, uri_to_fname(ps[0]).replace('-', ':'))\n        for p in ps:\n            k = c + ' ' + p\n            if k in d:\n                prev_identified = d[k]\n                break\n            d[k] = v\n        if prev_identified:\n            v = prev_identified\n            for p in ps:\n                k = c + ' ' + p\n                d[k] = prev_identified\n        counts.append(v)\n"]]}
{"hexsha": "3ae144074b21550c77cdd8b0c2475e1b752ee01e", "ext": "py", "lang": "Python", "content": "def os_path_nextname(basename):\n    \"\"\"Pick the next Filename not already existing in the Cwd\"\"\"\n\n    pattern = basename + \"*\"\n\n    paths = list(glob.glob(pattern))\n\n    last_path = basename\n    if not paths:\n\n        return basename\n\n    paths.sort(key=os_path_intrev)\n    last_path = paths[-1]\n\n    (base, ext, _) = os_path_partition(last_path)\n    last_intrev = os_path_intrev(last_path)\n    next_intrev = last_intrev + 1\n\n    next_path = \"{}{}~\".format(base, ext)\n    if next_intrev != 1:\n        next_path = \"{}{}~{}~\".format(base, ext, next_intrev)\n\n    return next_path", "fn_id": 3, "class_fn": false, "repo": "pelavarre/pybashish", "file": "bin/cp.py", "last_update_at": "2022-02-16T02:11:20+00:00", "question_id": "3ae144074b21550c77cdd8b0c2475e1b752ee01e_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def os_path_nextname(basename):\n    \"\"\"Pick the next Filename not already existing in the Cwd\"\"\"\n    pattern = basename + '*'\n    paths = list(glob.glob(pattern))\n    last_path = basename\n    if not paths:\n        return basename\n    paths.sort(key=os_path_intrev)\n    last_path = paths[-1]\n    base, ext, _ = os_path_partition(last_path)\n    last_intrev = os_path_intrev(last_path)\n    next_intrev = last_intrev + 1\n    next_path = '{}{}~'.format(base, ext)\n    if next_intrev != 1:\n        next_path = '{}{}~{}~'.format(base, ext, next_intrev)\n"]]}
{"hexsha": "4b0bd9fdd83363d0c9a5038d97c6a43cfbd57a0b", "ext": "py", "lang": "Python", "content": "def filter_by_period_freq (period_frequencies, period_thresh=5):\n\t\"\"\" Return True if more than `period_thresh` periods have zero counts\"\"\"\n\tzero_freq_dist = [period_frequencies[period] == 0 for period in period_frequencies]\n\treturn is_count_greater (zero_freq_dist, thresh=period_thresh)", "fn_id": 2, "class_fn": false, "repo": "sandeepsoni/jca_release", "file": "scripts/create_filters.py", "last_update_at": "2022-02-10T17:43:31+00:00", "question_id": "4b0bd9fdd83363d0c9a5038d97c6a43cfbd57a0b_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def filter_by_period_freq(period_frequencies, period_thresh=5):\n    \"\"\" Return True if more than `period_thresh` periods have zero counts\"\"\"\n    zero_freq_dist = [period_frequencies[period] == 0 for period in period_frequencies]\n"]]}
{"hexsha": "a648b495b9d5ba66f2c643fd68483de1a1c9691e", "ext": "py", "lang": "Python", "content": "def beta_posteriors_all(\n    totals: List[int],\n    positives: List[int],\n    sim_count: int,\n    a_priors_beta: List[Union[float, int]],\n    b_priors_beta: List[Union[float, int]],\n    seed: Union[int, np.random.bit_generator.SeedSequence] = None,\n) -> np.ndarray:\n    \"\"\"\n    Draw from beta posterior distributions for all variants at once.\n\n    Parameters\n    ----------\n    totals : List of numbers of experiment observations (e.g. number of sessions) for each variant.\n    positives : List of numbers of ones (e.g. number of conversions) for each variant.\n    sim_count : Number of simulations to be used for probability estimation.\n    a_priors_beta : List of prior alpha parameters for Beta distributions for each variant.\n    b_priors_beta : List of prior beta parameters for Beta distributions for each variant.\n    seed : Random seed.\n\n    Returns\n    -------\n    beta_samples : List of lists of beta distribution samples for all variants.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    beta_samples = np.array(\n        [\n            rng.beta(\n                positives[i] + a_priors_beta[i],\n                totals[i] - positives[i] + b_priors_beta[i],\n                sim_count,\n            )\n            for i in range(len(totals))\n        ]\n    )\n    return beta_samples", "fn_id": 0, "class_fn": false, "repo": "Matt52/bayes-ab-test", "file": "bayesian_testing/metrics/posteriors.py", "last_update_at": "2022-03-22T16:52:18+00:00", "question_id": "a648b495b9d5ba66f2c643fd68483de1a1c9691e_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def beta_posteriors_all(totals: List[int], positives: List[int], sim_count: int, a_priors_beta: List[Union[float, int]], b_priors_beta: List[Union[float, int]], seed: Union[int, np.random.bit_generator.SeedSequence]=None) -> np.ndarray:\n    \"\"\"\n    Draw from beta posterior distributions for all variants at once.\n\n    Parameters\n    ----------\n    totals : List of numbers of experiment observations (e.g. number of sessions) for each variant.\n    positives : List of numbers of ones (e.g. number of conversions) for each variant.\n    sim_count : Number of simulations to be used for probability estimation.\n    a_priors_beta : List of prior alpha parameters for Beta distributions for each variant.\n    b_priors_beta : List of prior beta parameters for Beta distributions for each variant.\n    seed : Random seed.\n\n    Returns\n    -------\n    beta_samples : List of lists of beta distribution samples for all variants.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    beta_samples = np.array([rng.beta(positives[i] + a_priors_beta[i], totals[i] - positives[i] + b_priors_beta[i], sim_count) for i in range(len(totals))])\n"]]}
{"hexsha": "069480b4526cb83e33845af661111e3bb7696a46", "ext": "py", "lang": "Python", "content": "def list_services(conn):\n    print(\"List Services:\")\n\n    for service in conn.identity.services():\n        print(service)", "fn_id": 5, "class_fn": false, "repo": "NeCTAR-RC/openstacksdk", "file": "examples/identity/list.py", "last_update_at": "2022-01-23T17:22:13+00:00", "question_id": "069480b4526cb83e33845af661111e3bb7696a46_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def list_services(conn):\n    print('List Services:')\n    for service in conn.identity.services():\n"]]}
{"hexsha": "ddb1981b4c8e7bc5d1904a336ef584dc10f249a3", "ext": "py", "lang": "Python", "content": "def calculate_errors(out_dir, data, name, naieve_model=True):\n  \"\"\"\n  Advanced plotting function for validation of refugee registration numbers in camps.\n  \"\"\"\n  plt.clf()\n\n  # data.loc[:,[\"%s sim\" % name,\"%s data\" % name]]).as_matrix()\n  y1 = data[\"%s sim\" % name].as_matrix()\n\n  y2 = data[\"%s data\" % name].as_matrix()\n  days = np.arange(len(y1))\n\n  naieve_early_day = 7\n  naieve_training_day = 30\n\n  # Rescaled values\n  plt.clf()\n\n  plt.xlabel(\"Days elapsed\")\n  plt.ylabel(\"Number of refugees\")\n\n  simtot = data[\"refugees in camps (simulation)\"].as_matrix().flatten()\n  untot = data[\"refugees in camps (UNHCR)\"].as_matrix().flatten()\n\n  y1_rescaled = np.zeros(len(y1))\n  for i in range(0, len(y1_rescaled)):\n    # Only rescale if simtot > 0\n    if simtot[i] > 0:\n      y1_rescaled[i] = y1[i] * untot[i] / simtot[i]\n\n  \"\"\"\n  Error quantification phase:\n  - Quantify the errors and mismatches for this camp.\n  \"\"\"\n\n  lerr = dd.LocationErrors()\n\n  # absolute difference\n  lerr.errors[\"absolute difference\"] = a.abs_diffs(y1, y2)\n\n  # absolute difference (rescaled)\n  lerr.errors[\"absolute difference rescaled\"] = a.abs_diffs(y1_rescaled, y2)\n\n  # ratio difference\n  lerr.errors[\"ratio difference\"] = a.abs_diffs(y1, y2) / (np.maximum(untot, np.ones(len(untot))))\n\n  \"\"\" Errors of which I'm usure whether to report:\n   - accuracy ratio (forecast / true value), because it crashes if denominator is 0.\n   - ln(accuracy ratio).\n  \"\"\"\n\n  # We can only calculate the Mean Absolute Scaled Error if we have a naieve model in our plot.\n  if naieve_model:\n\n    # Number of observations (aggrgate refugee days in UNHCR data set for this location)\n    lerr.errors[\"N\"] = np.sum(y2)\n\n    # flat naieve model (7 day)\n    lerr.errors[\"MASE7\"] = a.calculate_MASE(y1_rescaled, y2, n1, naieve_early_day)\n    lerr.errors[\"MASE7-sloped\"] = a.calculate_MASE(y1_rescaled, y2, n3, naieve_early_day)\n    lerr.errors[\"MASE7-ratio\"] = a.calculate_MASE(y1_rescaled, y2, n5, naieve_early_day)\n\n    # flat naieve model (30 day)\n    lerr.errors[\"MASE30\"] = a.calculate_MASE(y1_rescaled, y2, n2, naieve_training_day)\n    lerr.errors[\"MASE30-sloped\"] = a.calculate_MASE(y1_rescaled, y2, n4, naieve_training_day)\n    lerr.errors[\"MASE30-ratio\"] = a.calculate_MASE(y1_rescaled, y2, n6, naieve_training_day)\n\n\n    # Accuracy ratio doesn't work because of 0 values in the data.\n    print(\"%s,%s,%s,%s,%s,%s,%s,%s,%s\" % (out_dir, name, lerr.errors[\"MASE7\"],lerr.errors[\"MASE7-sloped\"], lerr.errors[\"MASE7-ratio\"],lerr.errors[\"MASE30\"],lerr.errors[\"MASE30-sloped\"],lerr.errors[\"MASE30-ratio\"],lerr.errors[\"N\"]))\n\n  return lerr", "fn_id": 0, "class_fn": false, "repo": "alirezajahani60/flee-release", "file": "outputanalysis/CalculateDiagnostics.py", "last_update_at": "2022-03-02T15:43:15+00:00", "question_id": "ddb1981b4c8e7bc5d1904a336ef584dc10f249a3_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def calculate_errors(out_dir, data, name, naieve_model=True):\n    \"\"\"\n  Advanced plotting function for validation of refugee registration numbers in camps.\n  \"\"\"\n    plt.clf()\n    y1 = data['%s sim' % name].as_matrix()\n    y2 = data['%s data' % name].as_matrix()\n    days = np.arange(len(y1))\n    naieve_early_day = 7\n    naieve_training_day = 30\n    plt.clf()\n    plt.xlabel('Days elapsed')\n    plt.ylabel('Number of refugees')\n    simtot = data['refugees in camps (simulation)'].as_matrix().flatten()\n    untot = data['refugees in camps (UNHCR)'].as_matrix().flatten()\n    y1_rescaled = np.zeros(len(y1))\n    for i in range(0, len(y1_rescaled)):\n        if simtot[i] > 0:\n            y1_rescaled[i] = y1[i] * untot[i] / simtot[i]\n    '\\n  Error quantification phase:\\n  - Quantify the errors and mismatches for this camp.\\n  '\n    lerr = dd.LocationErrors()\n    lerr.errors['absolute difference'] = a.abs_diffs(y1, y2)\n    lerr.errors['absolute difference rescaled'] = a.abs_diffs(y1_rescaled, y2)\n    lerr.errors['ratio difference'] = a.abs_diffs(y1, y2) / np.maximum(untot, np.ones(len(untot)))\n    \" Errors of which I'm usure whether to report:\\n   - accuracy ratio (forecast / true value), because it crashes if denominator is 0.\\n   - ln(accuracy ratio).\\n  \"\n    if naieve_model:\n        lerr.errors['N'] = np.sum(y2)\n        lerr.errors['MASE7'] = a.calculate_MASE(y1_rescaled, y2, n1, naieve_early_day)\n        lerr.errors['MASE7-sloped'] = a.calculate_MASE(y1_rescaled, y2, n3, naieve_early_day)\n        lerr.errors['MASE7-ratio'] = a.calculate_MASE(y1_rescaled, y2, n5, naieve_early_day)\n        lerr.errors['MASE30'] = a.calculate_MASE(y1_rescaled, y2, n2, naieve_training_day)\n        lerr.errors['MASE30-sloped'] = a.calculate_MASE(y1_rescaled, y2, n4, naieve_training_day)\n        lerr.errors['MASE30-ratio'] = a.calculate_MASE(y1_rescaled, y2, n6, naieve_training_day)\n        print('%s,%s,%s,%s,%s,%s,%s,%s,%s' % (out_dir, name, lerr.errors['MASE7'], lerr.errors['MASE7-sloped'], lerr.errors['MASE7-ratio'], lerr.errors['MASE30'], lerr.errors['MASE30-sloped'], lerr.errors['MASE30-ratio'], lerr.errors['N']))\n"]]}
{"hexsha": "099625f87b0b4ac8aab4aca696b8c929a6e85cb8", "ext": "py", "lang": "Python", "content": "def _validate_stream_header(file: File):\n    try:\n        header = parser.cparser_be.stream_header_t(file)\n    except EOFError:\n        return False\n\n    return (\n        header.magic == STREAM_MAGIC\n        and header.version == HUFFMAN_VERSION\n        and HUNDRED_K_BLOCK_MIN <= header.hundred_k_blocksize <= HUNDRED_K_BLOCK_MAX\n    )", "fn_id": 1, "class_fn": false, "repo": "IoT-Inspector/unblob", "file": "unblob/handlers/compression/bzip2.py", "last_update_at": "2022-03-10T15:40:41+00:00", "question_id": "099625f87b0b4ac8aab4aca696b8c929a6e85cb8_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _validate_stream_header(file: File):\n    try:\n        header = parser.cparser_be.stream_header_t(file)\n    except EOFError:\n        return False\n"]]}
{"hexsha": "15706a072451c3500888d62350c61f355113ef36", "ext": "py", "lang": "Python", "content": "def union_bb(a,b):\n    if a is None:\n        return b\n    elif b is None:\n        return a\n    elif isinstance(b,list):\n        return union_bb(a,(b,b))\n    else:\n        return ([min(c[0],c[1]) for c in zip(a[0],b[0])],   \\\n                [max(c[0],c[1]) for c in zip(a[1],b[1])])", "fn_id": 6, "class_fn": false, "repo": "joaomcm/Klampt", "file": "Python/klampt/io/povray.py", "last_update_at": "2022-03-30T22:48:45+00:00", "question_id": "15706a072451c3500888d62350c61f355113ef36_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def union_bb(a, b):\n    if a is None:\n        return b\n    elif b is None:\n        return a\n    elif isinstance(b, list):\n        return union_bb(a, (b, b))\n    else:\n"]]}
{"hexsha": "a16e14e1249d975ce0be4d15725ae70a8d3a8290", "ext": "py", "lang": "Python", "content": "@xfail_incompatible\ndef test_hash_invalid_algorithm():\n    with pytest.raises(pip_api.exceptions.InvalidArguments):\n        pip_api.hash(\"whatever\", \"invalid\")", "fn_id": 2, "class_fn": false, "repo": "woodruffw-forks/pip-api", "file": "tests/test_hash.py", "last_update_at": "2022-02-11T09:30:13+00:00", "question_id": "a16e14e1249d975ce0be4d15725ae70a8d3a8290_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@xfail_incompatible\ndef test_hash_invalid_algorithm():\n    with pytest.raises(pip_api.exceptions.InvalidArguments):\n"]]}
{"hexsha": "c2f82c3ffffd49ff0339318dc03aa161f0e37a1e", "ext": "py", "lang": "Python", "content": "def main():\n    import os\n\n    import requests\n    import datetime as dt\n\n    t = dt.datetime.now()\n    today = t.strftime(\"%d/%m/%Y\")\n    today_time = t.strftime(\"%H:%M:%S\")\n\n    APP_ID = os.environ[\"APP_ID\"]\n    API_KEY = os.environ[\"API_KEY\"]\n    USERNAME = os.environ[\"USERNAME\"]\n    PROJECT_NAME = \"exerciseTracking\"\n    SHEET_NAME = \"workouts\"\n    GENDER = \"male\"\n    WEIGHT_KG = 80\n    HEIGHT_CM = 157\n    AGE = 27\n    headers = {\n        \"x-app-id\": APP_ID,\n        \"x-app-key\": API_KEY,\n    }\n\n    request_body = {\n        \"query\": input(\"Tell me which exercises you did: \"),\n        \"gender\": GENDER,\n        \"weight_kg\": WEIGHT_KG,\n        \"height_cm\": HEIGHT_CM,\n        \"age\": AGE\n    }\n\n    auth = (os.environ[\"UNAME\"], os.environ[\"PWD\"])\n    nutritionix_endpoint = \"https://trackapi.nutritionix.com/v2/natural/exercise\"\n    sheety_endpoint = f\"https://api.sheety.co/{USERNAME}/{PROJECT_NAME}/{SHEET_NAME}\"\n\n    nutritionix_data = requests.post(url=nutritionix_endpoint, json=request_body, headers=headers)\n    n = nutritionix_data.json()\n\n    for exercise in n[\"exercises\"]:\n        msg_body = {\n            \"workout\": {\n                \"date\": today,\n                \"time\": today_time,\n                \"exercise\": exercise[\"name\"].title(),\n                \"duration\": exercise[\"duration_min\"],\n                \"calories\": exercise[\"nf_calories\"]\n            }\n        }\n        sheety_data = requests.post(url=sheety_endpoint, json=msg_body, auth=auth)\n        print(sheety_data.text)", "fn_id": 0, "class_fn": false, "repo": "rubix-coder/python-basic-to-professional", "file": "Udemy_100_days_of_python/projects/exercise_tracker/main.py", "last_update_at": "2022-01-24T23:12:49+00:00", "question_id": "c2f82c3ffffd49ff0339318dc03aa161f0e37a1e_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    import os\n    import requests\n    import datetime as dt\n    t = dt.datetime.now()\n    today = t.strftime('%d/%m/%Y')\n    today_time = t.strftime('%H:%M:%S')\n    APP_ID = os.environ['APP_ID']\n    API_KEY = os.environ['API_KEY']\n    USERNAME = os.environ['USERNAME']\n    PROJECT_NAME = 'exerciseTracking'\n    SHEET_NAME = 'workouts'\n    GENDER = 'male'\n    WEIGHT_KG = 80\n    HEIGHT_CM = 157\n    AGE = 27\n    headers = {'x-app-id': APP_ID, 'x-app-key': API_KEY}\n    request_body = {'query': input('Tell me which exercises you did: '), 'gender': GENDER, 'weight_kg': WEIGHT_KG, 'height_cm': HEIGHT_CM, 'age': AGE}\n    auth = (os.environ['UNAME'], os.environ['PWD'])\n    nutritionix_endpoint = 'https://trackapi.nutritionix.com/v2/natural/exercise'\n    sheety_endpoint = f'https://api.sheety.co/{USERNAME}/{PROJECT_NAME}/{SHEET_NAME}'\n    nutritionix_data = requests.post(url=nutritionix_endpoint, json=request_body, headers=headers)\n    n = nutritionix_data.json()\n    for exercise in n['exercises']:\n        msg_body = {'workout': {'date': today, 'time': today_time, 'exercise': exercise['name'].title(), 'duration': exercise['duration_min'], 'calories': exercise['nf_calories']}}\n        sheety_data = requests.post(url=sheety_endpoint, json=msg_body, auth=auth)\n"]]}
{"hexsha": "444ebb538831c94b107893d873af3b0de5c75481", "ext": "py", "lang": "Python", "content": "def parse_tree_string(parsetree, indent=None, b64_source=True, indent_level=0, debug=False):\n    indent_str = (' ' * indent * indent_level) if indent else ''\n    if isinstance(parsetree, ParseTree):\n        children = [parse_tree_string(child, indent, b64_source, indent_level+1, debug) for child in parsetree.children]\n        debug_str = parsetree.debug_str() if debug else ''\n        if indent is None or len(children) == 0:\n            return '{0}({1}: {2}{3})'.format(indent_str, parsetree.nonterminal, debug_str, ', '.join(children))\n        else:\n            return '{0}({1}:{2}\\n{3}\\n{4})'.format(\n                indent_str,\n                parsetree.nonterminal,\n                debug_str,\n                ',\\n'.join(children),\n                indent_str\n            )\n    elif isinstance(parsetree, Terminal):\n        return indent_str + parsetree.dumps(b64_source=b64_source)", "fn_id": 0, "class_fn": false, "repo": "broadinstitute/pywdl", "file": "wdl/parser.py", "last_update_at": "2022-02-02T10:35:00+00:00", "question_id": "444ebb538831c94b107893d873af3b0de5c75481_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def parse_tree_string(parsetree, indent=None, b64_source=True, indent_level=0, debug=False):\n    indent_str = ' ' * indent * indent_level if indent else ''\n    if isinstance(parsetree, ParseTree):\n        children = [parse_tree_string(child, indent, b64_source, indent_level + 1, debug) for child in parsetree.children]\n        debug_str = parsetree.debug_str() if debug else ''\n        if indent is None or len(children) == 0:\n            return '{0}({1}: {2}{3})'.format(indent_str, parsetree.nonterminal, debug_str, ', '.join(children))\n        else:\n            return '{0}({1}:{2}\\n{3}\\n{4})'.format(indent_str, parsetree.nonterminal, debug_str, ',\\n'.join(children), indent_str)\n    elif isinstance(parsetree, Terminal):\n"]]}
{"hexsha": "b1683a8f02e1062c2e4756dfeabfe6c3f6a90f46", "ext": "py", "lang": "Python", "content": "def get_icc_profile(decoded_data):\n    \"\"\"\n    Returns ICC image profile (if it exists and was correctly decoded)\n    \"\"\"\n    # fixme: move this function somewhere?\n    icc_profiles = [res.data for res in decoded_data.image_resource_blocks\n                   if res.resource_id == ImageResourceID.ICC_PROFILE]\n\n    if not icc_profiles:\n        return None\n\n    icc_profile = icc_profiles[0]\n\n    if isinstance(icc_profile, bytes): # profile was not decoded\n        return None\n\n    return icc_profile", "fn_id": 5, "class_fn": false, "repo": "LeZuse/psd-tools", "file": "src/psd_tools/user_api/pil_support.py", "last_update_at": "2022-01-02T17:28:11+00:00", "question_id": "b1683a8f02e1062c2e4756dfeabfe6c3f6a90f46_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_icc_profile(decoded_data):\n    \"\"\"\n    Returns ICC image profile (if it exists and was correctly decoded)\n    \"\"\"\n    icc_profiles = [res.data for res in decoded_data.image_resource_blocks if res.resource_id == ImageResourceID.ICC_PROFILE]\n    if not icc_profiles:\n        return None\n    icc_profile = icc_profiles[0]\n    if isinstance(icc_profile, bytes):\n        return None\n"]]}
{"hexsha": "52394c2d903832ac2b591631e658422fa845379a", "ext": "py", "lang": "Python", "content": "@pytest.fixture(autouse=True)\ndef mocked_logger(logger):\n    with patch.object(default_network, 'getLogger') as get_logger:\n        get_logger.return_value = logger\n        yield", "fn_id": 0, "class_fn": false, "repo": "jayneeljariwala/box-python-sdk", "file": "test/unit/network/test_network.py", "last_update_at": "2022-03-16T23:39:58+00:00", "question_id": "52394c2d903832ac2b591631e658422fa845379a_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.fixture(autouse=True)\ndef mocked_logger(logger):\n    with patch.object(default_network, 'getLogger') as get_logger:\n        get_logger.return_value = logger\n"]]}
{"hexsha": "f4f61dcbba1bd6942e5b9556d0cca3e69f34e044", "ext": "py", "lang": "Python", "content": "def main(argv=None):\n    if argv is None:\n        argv = sys.argv\n\n    # defaults\n    osc_freq = 16000 # kHz\n    baud = 115200\n    cpu = \"autodetect\"\n    flash_addr_base = 0\n    erase_all = False\n    erase_only = False\n    verify = False\n    verify_only = False\n    blank_check = False\n    xonxoff = False\n    start = False\n    control = False\n    select_bank = False\n    read = False\n    readlen = 0\n    get_serial_number = False\n    udp = False\n    port = -1\n    mac = \"\" # \"0C-1D-12-E0-1F-10\"\n\n    optlist, args = getopt.getopt(argv[1:], '',\n            ['cpu=', 'oscfreq=', 'baud=', 'addr=', 'start=',\n                'filetype=', 'bank=', 'read=', 'len=', 'serialnumber',\n                'udp', 'port=', 'mac=', 'verify', 'verifyonly', 'blankcheck',\n                'xonxoff', 'eraseall', 'eraseonly', 'list', 'control'])\n\n    for o, a in optlist:\n        if o == \"--list\":\n            log(\"Supported cpus:\")\n            for val in sorted(cpu_parms.keys()):\n                log(\" %s\" % val)\n            sys.exit(0)\n        if o == \"--cpu\":\n            cpu = a\n        elif o == \"--xonxoff\":\n            xonxoff = True\n        elif o == \"--oscfreq\":\n            osc_freq = int(a)\n        elif o == \"--addr\":\n            flash_addr_base = int(a, 0)\n        elif o == \"--baud\":\n            baud = int(a)\n        elif o == \"--eraseall\":\n            erase_all = True\n        elif o == \"--eraseonly\":\n            erase_only = True\n        elif o == \"--verify\":\n            verify = True\n        elif o == \"--verifyonly\":\n            verify = True\n            verify_only = True\n        elif o == \"--blankcheck\":\n            verify = True\n            blank_check = True\n        elif o == \"--control\":\n            control = True\n        elif o == \"--start\":\n            start = True\n            if a:\n                startaddr = int(a, 0)\n            else:\n                startaddr = 0\n        elif o == \"--bank\":\n            select_bank = True\n            bank = int(a)\n        elif o == \"--read\":\n            read = True\n            readfile = a\n        elif o == \"--serialnumber\":\n            get_serial_number = True\n        elif o == \"--len\":\n            readlen = int(a)\n        elif o == \"--udp\":\n            udp = True\n        elif o == \"--port\":\n            port = int(a)\n        elif o == \"--mac\":\n            mac = a\n        else:\n            panic(\"Unhandled option: %s\" % o)\n\n    if cpu != \"autodetect\" and not cpu in cpu_parms:\n        panic(\"Unsupported cpu %s\" % cpu)\n\n    if len(args) == 0:\n        syntax()\n\n    device = args[0]\n\n    if udp:\n        if '.' in device:\n            if ':' in device:\n                device, port = tuple(device.split(':'))\n                port = int(port)\n                if port<0 or port>65535:\n                    panic(\"Bad port number: %d\" % port)\n            parts = [int(x) for x in device.split('.')]\n            if len(parts)!=4 or min(parts)<0 or max(parts)>255:\n                panic(\"Bad IPv4-address: %s\" % device)\n            device = '.'.join([str(x) for x in parts])\n        elif ':' in device:\n            # panic(\"Bad IPv6-address: %s\" % device)\n            pass\n        else:\n            panic(\"Bad IP-address: %s\" % device)\n        if port < 0:\n            port = 41825\n        if mac:\n            parts = [int(x, 16) for x in mac.split('-')]\n            if len(parts)!=6 or min(parts)<0 or max(parts)>255:\n                panic(\"Bad MAC-address: %s\" % mac)\n            mac = '-'.join(['%02x'%x for x in parts])\n            log(\"cpu=%s ip=%s:%d mac=%s\" % (cpu, device, port, mac))\n        else:\n            log(\"cpu=%s ip=%s:%d\" % (cpu, device, port))\n    else:\n        log(\"cpu=%s oscfreq=%d device=%s baud=%d\" % (cpu, osc_freq, device, baud))\n\n    prog = nxpprog(cpu, device, baud, osc_freq, xonxoff, control, (device, port, mac) if udp else None, verify)\n\n    if erase_only:\n        prog.erase_all(verify)\n    elif blank_check:\n        prog.blank_check_all()\n    elif start:\n        prog.start(startaddr)\n    elif select_bank:\n        prog.select_bank(bank)\n    elif get_serial_number:\n        sn = prog.get_serial_number()\n        sys.stdout.write(sn)\n    elif read:\n        if not readlen:\n            panic(\"Read length is 0\")\n        fd = open(readfile, \"w\")\n        prog.read_block(flash_addr_base, readlen, fd)\n        fd.close()\n    else:\n        if len(args) != 2:\n            syntax()\n\n        filename = args[1]\n\n        image = open(filename, \"rb\").read()\n\n        if not verify_only:\n            start = time.time()\n            success = prog.prog_image(image, flash_addr_base, erase_all, verify)\n            stop = time.time()\n            elapsed = stop - start\n            log(\"Programmed %s in %.1f seconds\" % (\"successfully\" if success else \"with errors\", elapsed))\n\n        if verify:\n            start = time.time()\n            success = prog.verify_image(flash_addr_base, image)\n            stop = time.time()\n            elapsed = stop - start\n            log(\"Verified %s in %.1f seconds\" % (\"successfully\" if success else \"with errors\", elapsed))\n\n        if not verify_only:\n            prog.start(flash_addr_base)", "fn_id": 1, "class_fn": false, "repo": "leka/mbed-cmake-template", "file": "cmake/scripts/nxpprog.py", "last_update_at": "2022-01-21T12:50:32+00:00", "question_id": "f4f61dcbba1bd6942e5b9556d0cca3e69f34e044_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main(argv=None):\n    if argv is None:\n        argv = sys.argv\n    osc_freq = 16000\n    baud = 115200\n    cpu = 'autodetect'\n    flash_addr_base = 0\n    erase_all = False\n    erase_only = False\n    verify = False\n    verify_only = False\n    blank_check = False\n    xonxoff = False\n    start = False\n    control = False\n    select_bank = False\n    read = False\n    readlen = 0\n    get_serial_number = False\n    udp = False\n    port = -1\n    mac = ''\n    optlist, args = getopt.getopt(argv[1:], '', ['cpu=', 'oscfreq=', 'baud=', 'addr=', 'start=', 'filetype=', 'bank=', 'read=', 'len=', 'serialnumber', 'udp', 'port=', 'mac=', 'verify', 'verifyonly', 'blankcheck', 'xonxoff', 'eraseall', 'eraseonly', 'list', 'control'])\n    for o, a in optlist:\n        if o == '--list':\n            log('Supported cpus:')\n            for val in sorted(cpu_parms.keys()):\n                log(' %s' % val)\n            sys.exit(0)\n        if o == '--cpu':\n            cpu = a\n        elif o == '--xonxoff':\n            xonxoff = True\n        elif o == '--oscfreq':\n            osc_freq = int(a)\n        elif o == '--addr':\n            flash_addr_base = int(a, 0)\n        elif o == '--baud':\n            baud = int(a)\n        elif o == '--eraseall':\n            erase_all = True\n        elif o == '--eraseonly':\n            erase_only = True\n        elif o == '--verify':\n            verify = True\n        elif o == '--verifyonly':\n            verify = True\n            verify_only = True\n        elif o == '--blankcheck':\n            verify = True\n            blank_check = True\n        elif o == '--control':\n            control = True\n        elif o == '--start':\n            start = True\n            if a:\n                startaddr = int(a, 0)\n            else:\n                startaddr = 0\n        elif o == '--bank':\n            select_bank = True\n            bank = int(a)\n        elif o == '--read':\n            read = True\n            readfile = a\n        elif o == '--serialnumber':\n            get_serial_number = True\n        elif o == '--len':\n            readlen = int(a)\n        elif o == '--udp':\n            udp = True\n        elif o == '--port':\n            port = int(a)\n        elif o == '--mac':\n            mac = a\n        else:\n            panic('Unhandled option: %s' % o)\n    if cpu != 'autodetect' and (not cpu in cpu_parms):\n        panic('Unsupported cpu %s' % cpu)\n    if len(args) == 0:\n        syntax()\n    device = args[0]\n    if udp:\n        if '.' in device:\n            if ':' in device:\n                device, port = tuple(device.split(':'))\n                port = int(port)\n                if port < 0 or port > 65535:\n                    panic('Bad port number: %d' % port)\n            parts = [int(x) for x in device.split('.')]\n            if len(parts) != 4 or min(parts) < 0 or max(parts) > 255:\n                panic('Bad IPv4-address: %s' % device)\n            device = '.'.join([str(x) for x in parts])\n        elif ':' in device:\n            pass\n        else:\n            panic('Bad IP-address: %s' % device)\n        if port < 0:\n            port = 41825\n        if mac:\n            parts = [int(x, 16) for x in mac.split('-')]\n            if len(parts) != 6 or min(parts) < 0 or max(parts) > 255:\n                panic('Bad MAC-address: %s' % mac)\n            mac = '-'.join(['%02x' % x for x in parts])\n            log('cpu=%s ip=%s:%d mac=%s' % (cpu, device, port, mac))\n        else:\n            log('cpu=%s ip=%s:%d' % (cpu, device, port))\n    else:\n        log('cpu=%s oscfreq=%d device=%s baud=%d' % (cpu, osc_freq, device, baud))\n    prog = nxpprog(cpu, device, baud, osc_freq, xonxoff, control, (device, port, mac) if udp else None, verify)\n    if erase_only:\n        prog.erase_all(verify)\n    elif blank_check:\n        prog.blank_check_all()\n    elif start:\n        prog.start(startaddr)\n    elif select_bank:\n        prog.select_bank(bank)\n    elif get_serial_number:\n        sn = prog.get_serial_number()\n        sys.stdout.write(sn)\n    elif read:\n        if not readlen:\n            panic('Read length is 0')\n        fd = open(readfile, 'w')\n        prog.read_block(flash_addr_base, readlen, fd)\n        fd.close()\n    else:\n        if len(args) != 2:\n            syntax()\n        filename = args[1]\n        image = open(filename, 'rb').read()\n        if not verify_only:\n            start = time.time()\n            success = prog.prog_image(image, flash_addr_base, erase_all, verify)\n            stop = time.time()\n            elapsed = stop - start\n            log('Programmed %s in %.1f seconds' % ('successfully' if success else 'with errors', elapsed))\n        if verify:\n            start = time.time()\n            success = prog.verify_image(flash_addr_base, image)\n            stop = time.time()\n            elapsed = stop - start\n            log('Verified %s in %.1f seconds' % ('successfully' if success else 'with errors', elapsed))\n        if not verify_only:\n"]]}
{"hexsha": "9c42751e8cb69669028dbe7b3c60f5a852b9760e", "ext": "py", "lang": "Python", "content": "def test_parse_empty_background_with_short_description(parser):\n    \"\"\"The parser should parse an empty Background with a short description\"\"\"\n    # given\n    feature_file = \"\"\"\n        Feature: My Feature\n\n            Background: My Background\n    \"\"\"\n\n    # when\n    ast = parser.parse_contents(None, feature_file)\n\n    # then\n    assert ast.background.short_description == \"My Background\"", "fn_id": 29, "class_fn": false, "repo": "bingyujin/radish", "file": "tests/unit/test_parser.py", "last_update_at": "2022-03-19T11:49:10+00:00", "question_id": "9c42751e8cb69669028dbe7b3c60f5a852b9760e_29", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_parse_empty_background_with_short_description(parser):\n    \"\"\"The parser should parse an empty Background with a short description\"\"\"\n    feature_file = '\\n        Feature: My Feature\\n\\n            Background: My Background\\n    '\n    ast = parser.parse_contents(None, feature_file)\n"]]}
{"hexsha": "f078afa2ebfb3cc439cbeeb8307fb8affce1793a", "ext": "py", "lang": "Python", "content": "async def sample_update_datasource_credential_async(datasource_credential):\n    # [START update_datasource_credential_async]\n    from azure.ai.metricsadvisor import MetricsAdvisorKeyCredential, MetricsAdvisorAdministrationClient\n\n    service_endpoint = os.getenv(\"METRICS_ADVISOR_ENDPOINT\")\n    subscription_key = os.getenv(\"METRICS_ADVISOR_SUBSCRIPTION_KEY\")\n    api_key = os.getenv(\"METRICS_ADVISOR_API_KEY\")\n\n    client = MetricsAdvisorAdministrationClient(service_endpoint,\n                                  MetricsAdvisorKeyCredential(subscription_key, api_key))\n\n    datasource_credential.description = \"updated description\"\n\n    updated = await client.update_datasource_credential(datasource_credential)\n    print(\"Credential type: {}\".format(updated.credential_type))\n    print(\"Credential name: {}\".format(updated.name))\n    print(\"Description: {}\\n\".format(updated.description))", "fn_id": 3, "class_fn": false, "repo": "rsdoherty/azure-sdk-for-python", "file": "sdk/metricsadvisor/azure-ai-metricsadvisor/samples/async_samples/sample_datasource_credentials_async.py", "last_update_at": "2022-03-31T14:50:33+00:00", "question_id": "f078afa2ebfb3cc439cbeeb8307fb8affce1793a_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def sample_update_datasource_credential_async(datasource_credential):\n    from azure.ai.metricsadvisor import MetricsAdvisorKeyCredential, MetricsAdvisorAdministrationClient\n    service_endpoint = os.getenv('METRICS_ADVISOR_ENDPOINT')\n    subscription_key = os.getenv('METRICS_ADVISOR_SUBSCRIPTION_KEY')\n    api_key = os.getenv('METRICS_ADVISOR_API_KEY')\n    client = MetricsAdvisorAdministrationClient(service_endpoint, MetricsAdvisorKeyCredential(subscription_key, api_key))\n    datasource_credential.description = 'updated description'\n    updated = await client.update_datasource_credential(datasource_credential)\n    print('Credential type: {}'.format(updated.credential_type))\n    print('Credential name: {}'.format(updated.name))\n"]]}
{"hexsha": "525b8ac86d181a78d28a50cd33cd89141aeb42c8", "ext": "py", "lang": "Python", "content": "@pytest.mark.asyncio\nasync def test_command_pdelhook(tile38):\n    response = (\n        await tile38.setchan(name).within(key).circle(52.25, 13.37, 100).activate()\n    )\n    assert response.ok\n\n    response = await tile38.chans()\n    assert response.ok\n    assert len(response.chans) == 1\n\n    response = await tile38.pdelchan(\"*\")\n    assert response.ok\n\n    response = await tile38.chans()\n    assert response.ok\n    assert len(response.chans) == 0", "fn_id": 5, "class_fn": false, "repo": "iwpnd/pyle38", "file": "tests/test_command_setchan_delchan_pdelchan.py", "last_update_at": "2022-02-16T15:06:05+00:00", "question_id": "525b8ac86d181a78d28a50cd33cd89141aeb42c8_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.asyncio\nasync def test_command_pdelhook(tile38):\n    response = await tile38.setchan(name).within(key).circle(52.25, 13.37, 100).activate()\n    assert response.ok\n    response = await tile38.chans()\n    assert response.ok\n    assert len(response.chans) == 1\n    response = await tile38.pdelchan('*')\n    assert response.ok\n    response = await tile38.chans()\n    assert response.ok\n"]]}
{"hexsha": "77d4bc8a68803f698f17bde6b1f272f41a1e3c25", "ext": "py", "lang": "Python", "content": "def train_kmeans(x, num_clusters=10, num_gpus=1):\n    \"\"\"\n    Runs k-means clustering on one or several GPUs\n    \"\"\"\n    d = x.shape[1]\n    kmeans = faiss.Clustering(d, num_clusters)\n    kmeans.verbose = True\n    kmeans.niter = 20\n\n    kmeans.max_points_per_centroid = 100000\n\n    res = [faiss.StandardGpuResources() for i in range(num_gpus)]\n\n    flat_config = []\n    for i in range(num_gpus):\n        cfg = faiss.GpuIndexFlatConfig()\n        cfg.useFloat16 = False\n        cfg.device = i\n        flat_config.append(cfg)\n\n    if num_gpus == 1:\n        index = faiss.GpuIndexFlatL2(res[0], d, flat_config[0])\n    else:\n        indexes = [faiss.GpuIndexFlatL2(res[i], d, flat_config[i])\n                   for i in range(num_gpus)]\n        index = faiss.IndexProxy()\n        for sub_index in indexes:\n            index.addIndex(sub_index)\n\n    # perform the training\n    kmeans.train(x, index)\n    centroids = faiss.vector_float_to_array(kmeans.centroids)\n\n    stats = kmeans.iteration_stats\n    objective = np.array([\n        stats.at(i).obj for i in range(stats.size())\n    ])\n\n    print((\"Final objective: %.4g\" % objective[-1]))\n\n    return centroids.reshape(num_clusters, d)", "fn_id": 5, "class_fn": false, "repo": "Guanzhou-Ke/EMC-Net", "file": "utils.py", "last_update_at": "2022-03-24T10:28:30+00:00", "question_id": "77d4bc8a68803f698f17bde6b1f272f41a1e3c25_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def train_kmeans(x, num_clusters=10, num_gpus=1):\n    \"\"\"\n    Runs k-means clustering on one or several GPUs\n    \"\"\"\n    d = x.shape[1]\n    kmeans = faiss.Clustering(d, num_clusters)\n    kmeans.verbose = True\n    kmeans.niter = 20\n    kmeans.max_points_per_centroid = 100000\n    res = [faiss.StandardGpuResources() for i in range(num_gpus)]\n    flat_config = []\n    for i in range(num_gpus):\n        cfg = faiss.GpuIndexFlatConfig()\n        cfg.useFloat16 = False\n        cfg.device = i\n        flat_config.append(cfg)\n    if num_gpus == 1:\n        index = faiss.GpuIndexFlatL2(res[0], d, flat_config[0])\n    else:\n        indexes = [faiss.GpuIndexFlatL2(res[i], d, flat_config[i]) for i in range(num_gpus)]\n        index = faiss.IndexProxy()\n        for sub_index in indexes:\n            index.addIndex(sub_index)\n    kmeans.train(x, index)\n    centroids = faiss.vector_float_to_array(kmeans.centroids)\n    stats = kmeans.iteration_stats\n    objective = np.array([stats.at(i).obj for i in range(stats.size())])\n    print('Final objective: %.4g' % objective[-1])\n"]]}
{"hexsha": "c705f112ed561835325a2e14c85ecd8321f2a466", "ext": "py", "lang": "Python", "content": "def _clenshaw_curtis_weights(n):\n    \"\"\"\n    Computes the Clenshaw-Curtis quadrature using a fast FFT method.\n\n    This is a 'brainless' port of MATLAB code found in:\n    Fast Construction of the Fejer and Clenshaw-Curtis Quadrature Rules\n    Jorg Waldvogel, 2005\n    http://www.sam.math.ethz.ch/~joergw/Papers/fejer.pdf\n\n    :param n:\n    :return:\n    \"\"\"\n    from scipy.fftpack import ifft, fft, fftshift\n\n    # TODO python3 handles division differently from python2. Check how MATLAB interprets /, and if this code is still correct for python3\n\n    # function [wf1,wf2,wcc] = fejer(n)\n    # Weights of the Fejer2, Clenshaw-Curtis and Fejer1 quadratures by DFTs\n    # n>1. Nodes: x_k = cos(k*pi/n)\n    # N = [1:2:n-1]'; l=length(N); m=n-l; K=[0:m-1]';\n    N = np.arange(start=1, stop=n, step=2)[:, None]\n    l = N.size\n    m = n - l\n    K = np.arange(start=0, stop=m)[:, None]\n\n    # Fejer2 nodes: k=0,1,...,n; weights: wf2, wf2_n=wf2_0=0\n    # v0 = [2./N./(N-2); 1/N(end); zeros(m,1)];\n    v0 = np.vstack([2. / N / (N-2), 1. / N[-1]] + [0] * m)\n\n    # v2 = -v0(1:end-1) - v0(end:-1:2);\n    # wf2 = ifft(v2);\n    v2 = -v0[:-1] - v0[:0:-1]\n\n    # Clenshaw-Curtis nodes: k=0,1,...,n; weights: wcc, wcc_n=wcc_0\n    # g0 = -ones(n,1);\n    g0 = -np.ones((n, 1))\n\n    # g0(1 + l) = g0(1 + l) + n;\n    g0[l] = g0[l] + n\n\n    # g0(1+m) = g0(1 + m) + n;\n    g0[m] = g0[m] + n\n\n    # g = g0/(n^2-1+mod(n,2));\n    g = g0 / (n ** 2 - 1 + n % 2)\n\n    # wcc=ifft(v2 + g);\n    wcc = ifft((v2 + g).flatten()).real\n    wcc = np.hstack([wcc, wcc[0]])\n\n    # Fejer1 nodes: k=1/2,3/2,...,n-1/2; vector of weights: wf1\n    # v0=[2*exp(i*pi*K/n)./(1-4*K.^2); zeros(l+1,1)];\n    # v1=v0(1:end-1)+conj(v0(end:-1:2)); wf1=ifft(v1);\n    # don't need these\n\n    return wcc * np.pi / (n / 2 + 1)  # adjust for different scaling of python vs MATLAB fft", "fn_id": 8, "class_fn": false, "repo": "machism0/lie_learn", "file": "lie_learn/spaces/S2.py", "last_update_at": "2022-03-05T05:40:39+00:00", "question_id": "c705f112ed561835325a2e14c85ecd8321f2a466_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _clenshaw_curtis_weights(n):\n    \"\"\"\n    Computes the Clenshaw-Curtis quadrature using a fast FFT method.\n\n    This is a 'brainless' port of MATLAB code found in:\n    Fast Construction of the Fejer and Clenshaw-Curtis Quadrature Rules\n    Jorg Waldvogel, 2005\n    http://www.sam.math.ethz.ch/~joergw/Papers/fejer.pdf\n\n    :param n:\n    :return:\n    \"\"\"\n    from scipy.fftpack import ifft, fft, fftshift\n    N = np.arange(start=1, stop=n, step=2)[:, None]\n    l = N.size\n    m = n - l\n    K = np.arange(start=0, stop=m)[:, None]\n    v0 = np.vstack([2.0 / N / (N - 2), 1.0 / N[-1]] + [0] * m)\n    v2 = -v0[:-1] - v0[:0:-1]\n    g0 = -np.ones((n, 1))\n    g0[l] = g0[l] + n\n    g0[m] = g0[m] + n\n    g = g0 / (n ** 2 - 1 + n % 2)\n    wcc = ifft((v2 + g).flatten()).real\n    wcc = np.hstack([wcc, wcc[0]])\n"]]}
{"hexsha": "7c4c68cf052509418e7f0b77bb3fbc0bc66ae006", "ext": "py", "lang": "Python", "content": "def query_SRS_for_program_list(url, inventory, lists_of_interest):\n    try:\n        chemicallistresponse = requests.get(url)\n        chemicallistjson = json.loads(chemicallistresponse.text)\n    except:\n        return \"Error:404\"\n    all_chemicals_list = []\n    for chemical in chemicallistjson:\n       #get cas\n       chemicaldict = {}\n       chemicaldict['SRS_CAS'] = chemical['currentCasNumber']\n       chemicaldict['SRS_ID'] = chemical['subsKey']\n       #get synonyms\n       #extract from the json\n       synonyms = chemical['synonyms']\n       #ids are deeply embedded in this list. Go get ids relevant to these lists of interest\n       alternateids = []\n       for i in synonyms:\n           if i['listName'] in lists_of_interest:\n               # print('True for' + i['listName'])\n               #chem_alt_id_info = {}\n               for l in i['alternateIds']:\n                   #chem_alt_id_info['alternateId'] = l['alternateId']\n                   #chem_alt_id_info['alternateIdTypeName']\n                   alternateids.append(l['alternateId'])\n\n       #make list of alt ids unique by converting to a set, then back to a list\n       alternateids = list(set(alternateids))\n\n       if len(alternateids) > 0:\n           for id in range(0, len(alternateids)):\n               chemicaldict['PGM_ID'] = alternateids[id]\n               all_chemicals_list.append(chemicaldict.copy())\n       else:\n           all_chemicals_list.append(chemicaldict)\n    #Write it into a df\n    all_inventory_chemicals = pd.DataFrame(all_chemicals_list)\n    return all_inventory_chemicals", "fn_id": 3, "class_fn": false, "repo": "USEPA/standardizedinventories", "file": "chemicalmatcher/globals.py", "last_update_at": "2022-03-31T18:23:28+00:00", "question_id": "7c4c68cf052509418e7f0b77bb3fbc0bc66ae006_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def query_SRS_for_program_list(url, inventory, lists_of_interest):\n    try:\n        chemicallistresponse = requests.get(url)\n        chemicallistjson = json.loads(chemicallistresponse.text)\n    except:\n        return 'Error:404'\n    all_chemicals_list = []\n    for chemical in chemicallistjson:\n        chemicaldict = {}\n        chemicaldict['SRS_CAS'] = chemical['currentCasNumber']\n        chemicaldict['SRS_ID'] = chemical['subsKey']\n        synonyms = chemical['synonyms']\n        alternateids = []\n        for i in synonyms:\n            if i['listName'] in lists_of_interest:\n                for l in i['alternateIds']:\n                    alternateids.append(l['alternateId'])\n        alternateids = list(set(alternateids))\n        if len(alternateids) > 0:\n            for id in range(0, len(alternateids)):\n                chemicaldict['PGM_ID'] = alternateids[id]\n                all_chemicals_list.append(chemicaldict.copy())\n        else:\n            all_chemicals_list.append(chemicaldict)\n    all_inventory_chemicals = pd.DataFrame(all_chemicals_list)\n"]]}
{"hexsha": "aa0b4ad8a7193b7baa10b68fef04ad9e37be1322", "ext": "py", "lang": "Python", "content": "def run():\n    pi = PI()\n    checks = {}\n    for x, y in zip(range(1000), pi):\n        #print y,\n        if x in tests:\n            checks[x] = pi.a1\n    if tests != checks:\n        raise RuntimeError", "fn_id": 1, "class_fn": false, "repo": "lucapele/pele-c", "file": "zippy/benchmarks/src/benchmarks/parrot-b2.py", "last_update_at": "2022-03-18T02:36:58+00:00", "question_id": "aa0b4ad8a7193b7baa10b68fef04ad9e37be1322_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def run():\n    pi = PI()\n    checks = {}\n    for x, y in zip(range(1000), pi):\n        if x in tests:\n            checks[x] = pi.a1\n    if tests != checks:\n"]]}
{"hexsha": "a9090642e298505642306c029da34d2eaad075f0", "ext": "py", "lang": "Python", "content": "def mock_layout_get(*args, **kwargs):\n    \"\"\" Mocks the BIDSLayout.get method to get the files\n    It returns a list of MockBidsFiles\n    \"\"\"\n    # We add one extra file (\"N_RUNS + 1\") so that we can have a BIDS\n    # file that doesn't exist in the \"scan_df\" dataframe and test a\n    # special case in \"determine_scan_durations\"\n    return [MockBidsFile(path='scan_{}'.format(i)) for i in range(N_RUNS + 1)]", "fn_id": 1, "class_fn": false, "repo": "cbinyu/dcm2bidsphysio", "file": "bidsphysio.session/tests/test_session2bids.py", "last_update_at": "2022-03-21T06:05:41+00:00", "question_id": "a9090642e298505642306c029da34d2eaad075f0_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def mock_layout_get(*args, **kwargs):\n    \"\"\" Mocks the BIDSLayout.get method to get the files\n    It returns a list of MockBidsFiles\n    \"\"\"\n"]]}
{"hexsha": "44fd15d47121bfeb9c407bcc6477125e774a2345", "ext": "py", "lang": "Python", "content": "@cpython_api([PyObject, PyObject, Py_ssize_t, Py_ssize_t], Py_ssize_t, error=-1)\ndef PyUnicode_Count(space, w_str, w_substr, start, end):\n    \"\"\"Return the number of non-overlapping occurrences of substr in\n    str[start:end].  Return -1 if an error occurred.\"\"\"\n    w_count = space.call_method(w_str, \"count\", w_substr,\n                                space.newint(start), space.newint(end))\n    return space.int_w(w_count)", "fn_id": 76, "class_fn": false, "repo": "hollmmax/zig", "file": "pypy/module/cpyext/unicodeobject.py", "last_update_at": "2022-03-30T11:42:37+00:00", "question_id": "44fd15d47121bfeb9c407bcc6477125e774a2345_76", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@cpython_api([PyObject, PyObject, Py_ssize_t, Py_ssize_t], Py_ssize_t, error=-1)\ndef PyUnicode_Count(space, w_str, w_substr, start, end):\n    \"\"\"Return the number of non-overlapping occurrences of substr in\n    str[start:end].  Return -1 if an error occurred.\"\"\"\n    w_count = space.call_method(w_str, 'count', w_substr, space.newint(start), space.newint(end))\n"]]}
{"hexsha": "107b1786a4aef1e85b9f701b0d28b1fe33f5ad08", "ext": "py", "lang": "Python", "content": "@contextmanager\ndef timing():\n    t0 = time.time()\n    yield lambda: (t1 - t0)\n    t1 = time.time()", "fn_id": 0, "class_fn": false, "repo": "zillow/quantile-forest", "file": "examples/plot_quantile_weighted_vs_unweighted.py", "last_update_at": "2022-03-31T19:30:41+00:00", "question_id": "107b1786a4aef1e85b9f701b0d28b1fe33f5ad08_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@contextmanager\ndef timing():\n    t0 = time.time()\n    yield (lambda: t1 - t0)\n"]]}
{"hexsha": "5eaa1bfa518d7c8c59715f469c49473f89282dc4", "ext": "py", "lang": "Python", "content": "def eq(args):\n    args = valueify(args)\n    if len(args) != 2: raise MethodInputError(\"Incorrect number of inputs, should be 2, %s were given\" % len(args))\n    return tokenz.Token(\"bool\", args[0].val == args[1].val)", "fn_id": 4, "class_fn": false, "repo": "TheTechRobo/Scotch-Language", "file": "methods/logic.py", "last_update_at": "2022-02-09T20:30:23+00:00", "question_id": "5eaa1bfa518d7c8c59715f469c49473f89282dc4_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def eq(args):\n    args = valueify(args)\n    if len(args) != 2:\n        raise MethodInputError('Incorrect number of inputs, should be 2, %s were given' % len(args))\n"]]}
{"hexsha": "d229cb0ad6444d38ae49dd9f9a95dd6134cf4c89", "ext": "py", "lang": "Python", "content": "def save(queue):\n    # type: (Queue) -> None\n\n    number = 0\n    output = \"screenshots/file_{}.png\"\n    to_png = mss.tools.to_png\n\n    while \"there are screenshots\":\n        img = queue.get()\n        if img is None:\n            break\n\n        to_png(img.rgb, img.size, output=output.format(number))\n        number += 1", "fn_id": 1, "class_fn": false, "repo": "vaibhav623/python-mss", "file": "docs/source/examples/fps_multiprocessing.py", "last_update_at": "2022-03-28T19:29:45+00:00", "question_id": "d229cb0ad6444d38ae49dd9f9a95dd6134cf4c89_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def save(queue):\n    number = 0\n    output = 'screenshots/file_{}.png'\n    to_png = mss.tools.to_png\n    while 'there are screenshots':\n        img = queue.get()\n        if img is None:\n            break\n        to_png(img.rgb, img.size, output=output.format(number))\n"]]}
{"hexsha": "b40bd299a26c37f69ba583ce535e99e2ba71fd32", "ext": "py", "lang": "Python", "content": "@pytest.mark.skip(\"\")\n@pytest.mark.parametrize('op', BINARY_MATH_OPS_ZERO_ISSUES)\n@given(a=npst.arrays(shape=len(JAN_EVENT_2020_VALUES),\n                     dtype=np.float,\n                     elements=st.floats(allow_infinity=False, allow_nan=False)))\ndef test_div_array(a, op):\n    # Not supported as the cmp operators are handled by numpy comparing each item individually.\n    # np.testing.assert_array_equal(op(a, JAN_EVENT).sample_from(DATES_2020),\n    #                               op(a, JAN_EVENT_2020_VALUES))\n\n    a[a == 0] = 1\n\n    np.testing.assert_array_equal(op(JAN_EVENT, a).sample_from(DATES_2020),\n                                  op(JAN_EVENT_2020_VALUES, a))", "fn_id": 6, "class_fn": false, "repo": "TNonet/happenings", "file": "tests/test_events__methods__.py", "last_update_at": "2022-03-04T16:36:04+00:00", "question_id": "b40bd299a26c37f69ba583ce535e99e2ba71fd32_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.skip('')\n@pytest.mark.parametrize('op', BINARY_MATH_OPS_ZERO_ISSUES)\n@given(a=npst.arrays(shape=len(JAN_EVENT_2020_VALUES), dtype=np.float, elements=st.floats(allow_infinity=False, allow_nan=False)))\ndef test_div_array(a, op):\n    a[a == 0] = 1\n"]]}
{"hexsha": "a7949c8a1d4ce2bfd5a91876e53711bd34fab737", "ext": "py", "lang": "Python", "content": "def get_access_token(request):\n    # Happens when the user hits index the first time and hasn't authenticated on Learn\n    # Part II. Get an access token for the user that logged in. Put that on their session.\n    bb_json = request.session.get('bb_json')\n    target_view = request.session.get('target_view')\n    print('VIEWS: get_access_token: got BbRest from session')\n    bb = jsonpickle.decode(bb_json)\n    bb.supported_functions() # This and the following are required after\n    bb.method_generator()    # unpickling the pickled object.\n    # Next, get the code parameter value from the request\n    redirect_uri = reverse(get_access_token)\n    absolute_redirect_uri = f\"https://{request.get_host()}{redirect_uri}\"\n\n    state = request.GET.get('state', default= \"NOSTATE\")\n    print(f'VIEWS: get_access_token: GOT BACK state: {state}')\n    stored_state = request.session.get('state')\n    print(f'VIEWS: get_access_token: STORED STATE: {stored_state}')\n    if (stored_state != state):\n        return HttpResponseRedirect(reverse('notauthorized'))\n\n    code =  request.GET.get('code', default = None)\n    if (code == None):\n        exit()\n\n    #Rebuild a new BbRest object to get an access token with the user's authcode.\n    # if (CUSTOM_LOGIN_URL):\n    #     print(\"CUSTOM_LOGIN_URL\")\n    #     user_bb = BbRest(KEY, SECRET, f\"https://{CUSTOM_LOGIN_URL}\", code=code, redirect_uri=absolute_redirect_uri )\n    # else:\n    user_bb = BbRest(KEY, SECRET, f\"https://{LEARNFQDN}\", code=code, redirect_uri=absolute_redirect_uri )    \n    bb_json = jsonpickle.encode(user_bb)\n    if (isGuestUser(bb_json)):\n        context = {\n            'learn_server': LEARNFQDN,\n        }   \n        return render(request, 'guestusernotallowed.html', context=context )\n\n    print('VIEWS: get_access_token: pickled BbRest and putting it on session')\n    request.session['bb_json'] = bb_json\n    return HttpResponseRedirect(reverse(f'{target_view}'))", "fn_id": 3, "class_fn": false, "repo": "kurotsuki007/OSCELOT-DSKTOOL-for-HEROKU", "file": "dsktool/views.py", "last_update_at": "2022-02-22T12:46:41+00:00", "question_id": "a7949c8a1d4ce2bfd5a91876e53711bd34fab737_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_access_token(request):\n    bb_json = request.session.get('bb_json')\n    target_view = request.session.get('target_view')\n    print('VIEWS: get_access_token: got BbRest from session')\n    bb = jsonpickle.decode(bb_json)\n    bb.supported_functions()\n    bb.method_generator()\n    redirect_uri = reverse(get_access_token)\n    absolute_redirect_uri = f'https://{request.get_host()}{redirect_uri}'\n    state = request.GET.get('state', default='NOSTATE')\n    print(f'VIEWS: get_access_token: GOT BACK state: {state}')\n    stored_state = request.session.get('state')\n    print(f'VIEWS: get_access_token: STORED STATE: {stored_state}')\n    if stored_state != state:\n        return HttpResponseRedirect(reverse('notauthorized'))\n    code = request.GET.get('code', default=None)\n    if code == None:\n        exit()\n    user_bb = BbRest(KEY, SECRET, f'https://{LEARNFQDN}', code=code, redirect_uri=absolute_redirect_uri)\n    bb_json = jsonpickle.encode(user_bb)\n    if isGuestUser(bb_json):\n        context = {'learn_server': LEARNFQDN}\n        return render(request, 'guestusernotallowed.html', context=context)\n    print('VIEWS: get_access_token: pickled BbRest and putting it on session')\n    request.session['bb_json'] = bb_json\n"]]}
{"hexsha": "0a110d4a8b8a71da2af25b88982c93be862843a2", "ext": "py", "lang": "Python", "content": "def lisp_send_map_request ( lisp_sockets , lisp_ephem_port , seid , deid , rloc ,\n pubsub = False ) :\n global lisp_last_map_request_sent\n if 7 - 7: i1IIi . I1IiiI\n if 68 - 68: OoooooooOO\n if 91 - 91: IiII . ooOoO0o * I11i\n if 39 - 39: o0oOOo0O0Ooo + i11iIiiIii\n if 69 - 69: iIii1I11I1II1 . II111iiii\n if 36 - 36: I1IiiI * i1IIi + OoOoOO00\n oO000oo0 = OOoO0ooo0ooo0 = None\n if ( rloc ) :\n  oO000oo0 = rloc . rloc\n  OOoO0ooo0ooo0 = rloc . translated_port if lisp_i_am_rtr else LISP_DATA_PORT\n  if 51 - 51: iIii1I11I1II1 % Ii1I + iIii1I11I1II1 + oO0o - IiII * o0oOOo0O0Ooo\n  if 13 - 13: iIii1I11I1II1\n  if 10 - 10: I1IiiI * iII111i * ooOoO0o . IiII\n  if 7 - 7: iIii1I11I1II1\n  if 60 - 60: OOooOOo . Ii1I . Ii1I % II111iiii + OoO0O00\n ooOoOoOo , OOOOO0 , OoO = lisp_myrlocs\n if ( ooOoOoOo == None ) :\n  lprint ( \"Suppress sending Map-Request, IPv4 RLOC not found\" )\n  return\n  if 49 - 49: I1IiiI . I1ii11iIi11i / Oo0Ooo\n if ( OOOOO0 == None and oO000oo0 != None and oO000oo0 . is_ipv6 ( ) ) :\n  lprint ( \"Suppress sending Map-Request, IPv6 RLOC not found\" )\n  return\n  if 24 - 24: II111iiii\n  if 40 - 40: o0oOOo0O0Ooo . I1IiiI - o0oOOo0O0Ooo\n i1i = lisp_map_request ( )\n i1i . record_count = 1\n i1i . nonce = lisp_get_control_nonce ( )\n i1i . rloc_probe = ( oO000oo0 != None )\n i1i . subscribe_bit = pubsub\n i1i . xtr_id_present = pubsub\n if 62 - 62: oO0o\n if 71 - 71: i1IIi . I1ii11iIi11i / i11iIiiIii + II111iiii\n if 14 - 14: iII111i\n if 35 - 35: Ii1I\n if 54 - 54: OOooOOo\n if 83 - 83: i1IIi / II111iiii - I1IiiI + I1ii11iIi11i . IiII * oO0o\n if 92 - 92: OoOoOO00 + oO0o % Ii1I / Ii1I - iII111i\n if ( rloc ) : rloc . last_rloc_probe_nonce = i1i . nonce\n if 11 - 11: Oo0Ooo % II111iiii * Ii1I + II111iiii\n I1IIiIiIIiIiI = deid . is_multicast_address ( )\n if ( I1IIiIiIIiIiI ) :\n  i1i . target_eid = seid\n  i1i . target_group = deid\n else :\n  i1i . target_eid = deid\n  if 9 - 9: I1Ii111\n  if 69 - 69: i1IIi + ooOoO0o + Ii1I\n  if 88 - 88: OoOoOO00 + iII111i % O0 + OOooOOo / OoooooooOO / OOooOOo\n  if 95 - 95: ooOoO0o . Oo0Ooo % IiII + iII111i\n  if 16 - 16: I11i * OoO0O00 % o0oOOo0O0Ooo - O0 % II111iiii - I1IiiI\n  if 72 - 72: OoooooooOO * OoOoOO00 . OOooOOo + Ii1I . OOooOOo / II111iiii\n  if 8 - 8: i1IIi\n  if 1 - 1: OoOoOO00 . OoO0O00 . OoO0O00 * O0\n  if 97 - 97: OoooooooOO % ooOoO0o . I1Ii111 / iII111i\n if ( i1i . rloc_probe == False ) :\n  oooOOoO0oo0 = lisp_get_signature_eid ( )\n  if ( oooOOoO0oo0 ) :\n   i1i . signature_eid . copy_address ( oooOOoO0oo0 . eid )\n   i1i . privkey_filename = \"./lisp-sig.pem\"\n   if 59 - 59: II111iiii + O0 . I1ii11iIi11i . Oo0Ooo * OoO0O00\n   if 35 - 35: oO0o / I1Ii111 * OOooOOo + OoooooooOO . IiII\n   if 1 - 1: I1IiiI + I1Ii111 / OOooOOo . Ii1I . oO0o / I1ii11iIi11i\n   if 54 - 54: OOooOOo\n   if 86 - 86: oO0o * Oo0Ooo / OOooOOo\n   if 18 - 18: II111iiii - I1Ii111\n if ( seid == None or I1IIiIiIIiIiI ) :\n  i1i . source_eid . afi = LISP_AFI_NONE\n else :\n  i1i . source_eid = seid\n  if 13 - 13: i11iIiiIii - O0 % OoOoOO00 + OOooOOo * ooOoO0o\n  if 55 - 55: i1IIi - OOooOOo / I11i * Ii1I\n  if 20 - 20: OoOoOO00 * iIii1I11I1II1 % O0 - i1IIi\n  if 51 - 51: I1ii11iIi11i * Ii1I - oO0o / O0 * OoooooooOO\n  if 12 - 12: i1IIi / iIii1I11I1II1 / O0 * OoO0O00\n  if 15 - 15: i11iIiiIii / IiII + Ii1I % OOooOOo % I1ii11iIi11i * oO0o\n  if 24 - 24: OOooOOo / OOooOOo + I11i / iII111i . oO0o - iII111i\n  if 59 - 59: I1ii11iIi11i % II111iiii - i11iIiiIii - I1Ii111\n  if 34 - 34: II111iiii + iII111i / IiII\n  if 47 - 47: OoO0O00\n  if 40 - 40: o0oOOo0O0Ooo / iII111i . o0oOOo0O0Ooo\n  if 63 - 63: o0oOOo0O0Ooo * iIii1I11I1II1 * II111iiii . OoO0O00 - oO0o / OoOoOO00\n if ( oO000oo0 != None and lisp_nat_traversal and lisp_i_am_rtr == False ) :\n  if ( oO000oo0 . is_private_address ( ) == False ) :\n   ooOoOoOo = lisp_get_any_translated_rloc ( )\n   if 78 - 78: i11iIiiIii / OoO0O00 / i1IIi . i11iIiiIii\n  if ( ooOoOoOo == None ) :\n   lprint ( \"Suppress sending Map-Request, translated RLOC not found\" )\n   return\n   if 100 - 100: II111iiii . IiII . I11i\n   if 60 - 60: OoOoOO00 % OOooOOo * i1IIi\n   if 3 - 3: OoooooooOO\n   if 75 - 75: OoooooooOO * I1Ii111 * o0oOOo0O0Ooo + I1ii11iIi11i . iIii1I11I1II1 / O0\n   if 23 - 23: oO0o - O0 * IiII + i11iIiiIii * Ii1I\n   if 8 - 8: ooOoO0o / II111iiii . I1ii11iIi11i * ooOoO0o % oO0o\n   if 36 - 36: I1ii11iIi11i % OOooOOo - ooOoO0o - I11i + I1IiiI\n   if 37 - 37: I1ii11iIi11i * IiII\n if ( oO000oo0 == None or oO000oo0 . is_ipv4 ( ) ) :\n  if ( lisp_nat_traversal and oO000oo0 == None ) :\n   O0ooOoOO0OoO0O0 = lisp_get_any_translated_rloc ( )\n   if ( O0ooOoOO0OoO0O0 != None ) : ooOoOoOo = O0ooOoOO0OoO0O0\n   if 86 - 86: I11i * iIii1I11I1II1 - I1ii11iIi11i % IiII . OOooOOo * I1Ii111\n  i1i . itr_rlocs . append ( ooOoOoOo )\n  if 49 - 49: OoOoOO00 * OoOoOO00\n if ( oO000oo0 == None or oO000oo0 . is_ipv6 ( ) ) :\n  if ( OOOOO0 == None or OOOOO0 . is_ipv6_link_local ( ) ) :\n   OOOOO0 = None\n  else :\n   i1i . itr_rloc_count = 1 if ( oO000oo0 == None ) else 0\n   i1i . itr_rlocs . append ( OOOOO0 )\n   if 86 - 86: iIii1I11I1II1\n   if 42 - 42: ooOoO0o * ooOoO0o . I1ii11iIi11i . ooOoO0o / I1IiiI * iIii1I11I1II1\n   if 50 - 50: Ii1I\n   if 16 - 16: OoooooooOO / oO0o + I1IiiI / O0\n   if 12 - 12: ooOoO0o / I1IiiI % Oo0Ooo - II111iiii / i11iIiiIii\n   if 33 - 33: o0oOOo0O0Ooo + IiII / OoOoOO00 / ooOoO0o\n   if 9 - 9: OoOoOO00\n   if 44 - 44: Oo0Ooo . i11iIiiIii % OOooOOo\n   if 87 - 87: o0oOOo0O0Ooo\n if ( oO000oo0 != None and i1i . itr_rlocs != [ ] ) :\n  oO0o0o00O = i1i . itr_rlocs [ 0 ]\n else :\n  if ( deid . is_ipv4 ( ) ) :\n   oO0o0o00O = ooOoOoOo\n  elif ( deid . is_ipv6 ( ) ) :\n   oO0o0o00O = OOOOO0\n  else :\n   oO0o0o00O = ooOoOoOo\n   if 41 - 41: OoooooooOO . iII111i / oO0o\n   if 16 - 16: iII111i + o0oOOo0O0Ooo / II111iiii * i11iIiiIii * OoO0O00 . iIii1I11I1II1\n   if 34 - 34: I11i / o0oOOo0O0Ooo * OOooOOo * OOooOOo\n   if 89 - 89: I1ii11iIi11i . OoooooooOO\n   if 61 - 61: i1IIi + i11iIiiIii\n   if 59 - 59: i11iIiiIii * OOooOOo + i1IIi * iIii1I11I1II1 + I11i\n o0o0ooOOo0oO = i1i . encode ( oO000oo0 , OOoO0ooo0ooo0 )\n i1i . print_map_request ( )\n if 97 - 97: OoO0O00 - I11i . OoooooooOO\n if 58 - 58: I1ii11iIi11i / II111iiii / i11iIiiIii\n if 27 - 27: iIii1I11I1II1 - O0 + OoOoOO00\n if 28 - 28: oO0o . IiII * iII111i % Oo0Ooo - OoO0O00 / I11i\n if 67 - 67: i11iIiiIii + i11iIiiIii / ooOoO0o - o0oOOo0O0Ooo\n if 94 - 94: O0 + OoO0O00 / I1IiiI * II111iiii * i11iIiiIii\n if ( oO000oo0 != None ) :\n  if ( rloc . is_rloc_translated ( ) ) :\n   I1OOoO0OoOOo0 = lisp_get_nat_info ( oO000oo0 , rloc . rloc_name )\n   if 55 - 55: OoooooooOO * O0 + i1IIi % I1IiiI\n   if 10 - 10: II111iiii - Ii1I . I11i . O0 + Ii1I\n   if 50 - 50: iIii1I11I1II1 / Ii1I . ooOoO0o / ooOoO0o * OoOoOO00 * iII111i\n   if 15 - 15: o0oOOo0O0Ooo % II111iiii + I1IiiI\n   if ( I1OOoO0OoOOo0 == None ) :\n    iiiI1I = rloc . rloc . print_address_no_iid ( )\n    OoIi1I1I = \"gleaned-{}\" . format ( iiiI1I )\n    IIIiIIi111 = rloc . translated_port\n    I1OOoO0OoOOo0 = lisp_nat_info ( iiiI1I , OoIi1I1I , IIIiIIi111 )\n    if 21 - 21: I1ii11iIi11i - ooOoO0o\n   lisp_encapsulate_rloc_probe ( lisp_sockets , oO000oo0 , I1OOoO0OoOOo0 ,\n o0o0ooOOo0oO )\n   return\n   if 81 - 81: iII111i / i11iIiiIii / I1Ii111\n   if 70 - 70: I1ii11iIi11i / i11iIiiIii\n  O0O0 = oO000oo0 . print_address_no_iid ( )\n  OO0oooOO = lisp_convert_4to6 ( O0O0 )\n  lisp_send ( lisp_sockets , OO0oooOO , LISP_CTRL_PORT , o0o0ooOOo0oO )\n  return\n  if 90 - 90: II111iiii / OoOoOO00 . Ii1I . OoooooooOO\n  if 76 - 76: OoooooooOO\n  if 78 - 78: IiII % i11iIiiIii\n  if 23 - 23: iIii1I11I1II1 - o0oOOo0O0Ooo - Ii1I % OOooOOo\n  if 100 - 100: oO0o . OoO0O00 . i11iIiiIii % II111iiii * IiII\n  if 81 - 81: OOooOOo - OOooOOo + OoOoOO00\n iiII11iI = None if lisp_i_am_rtr else seid\n if ( lisp_decent_pull_xtr_configured ( ) ) :\n  oooO = lisp_get_decent_map_resolver ( deid )\n else :\n  oooO = lisp_get_map_resolver ( None , iiII11iI )\n  if 34 - 34: OoOoOO00 * o0oOOo0O0Ooo * i11iIiiIii - I11i % oO0o / OoO0O00\n if ( oooO == None ) :\n  lprint ( \"Cannot find Map-Resolver for source-EID {}\" . format ( green ( seid . print_address ( ) , False ) ) )\n  if 75 - 75: i1IIi / Ii1I * OoO0O00 - I1ii11iIi11i * O0 . IiII\n  return\n  if 11 - 11: I11i / Ii1I % oO0o\n oooO . last_used = lisp_get_timestamp ( )\n oooO . map_requests_sent += 1\n if ( oooO . last_nonce == 0 ) : oooO . last_nonce = i1i . nonce\n if 50 - 50: i11iIiiIii\n if 93 - 93: i1IIi / Ii1I * II111iiii - Oo0Ooo . OoOoOO00 - OOooOOo\n if 25 - 25: I11i / ooOoO0o % ooOoO0o - OOooOOo\n if 59 - 59: I1IiiI + o0oOOo0O0Ooo . iIii1I11I1II1 - O0 - i11iIiiIii\n if ( seid == None ) : seid = oO0o0o00O\n lisp_send_ecm ( lisp_sockets , o0o0ooOOo0oO , seid , lisp_ephem_port , deid ,\n oooO . map_resolver )\n if 4 - 4: I1IiiI\n if 36 - 36: Ii1I\n if 76 - 76: i11iIiiIii + i1IIi\n if 56 - 56: OoOoOO00 + II111iiii / i11iIiiIii * OoOoOO00 * OoooooooOO\n lisp_last_map_request_sent = lisp_get_timestamp ( )\n if 15 - 15: OoOoOO00 / OoooooooOO + OOooOOo\n if 76 - 76: Ii1I * iII111i . OoooooooOO\n if 92 - 92: iIii1I11I1II1 - Oo0Ooo - I1IiiI - OOooOOo * I1Ii111\n if 44 - 44: I1Ii111 - II111iiii / OOooOOo\n oooO . resolve_dns_name ( )\n return\n if 50 - 50: I11i / I1ii11iIi11i\n if 60 - 60: II111iiii / Ii1I + OoO0O00 % I1IiiI * i1IIi / II111iiii\n if 91 - 91: I1IiiI * I1Ii111 * i11iIiiIii - oO0o - IiII + I1ii11iIi11i\n if 99 - 99: OoO0O00 % o0oOOo0O0Ooo\n if 3 - 3: OOooOOo / OoOoOO00 % iIii1I11I1II1\n if 47 - 47: ooOoO0o . i11iIiiIii / OoO0O00\n if 48 - 48: O0\n if 89 - 89: i11iIiiIii % OoO0O00 . OoOoOO00 + Oo0Ooo + OoOoOO00", "fn_id": 155, "class_fn": false, "repo": "farinacci/lispers.net", "file": "build/releases/release-0.549/ob/lisp.py", "last_update_at": "2022-03-25T04:40:38+00:00", "question_id": "0a110d4a8b8a71da2af25b88982c93be862843a2_155", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def lisp_send_map_request(lisp_sockets, lisp_ephem_port, seid, deid, rloc, pubsub=False):\n    global lisp_last_map_request_sent\n    if 7 - 7:\n        i1IIi.I1IiiI\n    if 68 - 68:\n        OoooooooOO\n    if 91 - 91:\n        IiII.ooOoO0o * I11i\n    if 39 - 39:\n        o0oOOo0O0Ooo + i11iIiiIii\n    if 69 - 69:\n        iIii1I11I1II1.II111iiii\n    if 36 - 36:\n        I1IiiI * i1IIi + OoOoOO00\n    oO000oo0 = OOoO0ooo0ooo0 = None\n    if rloc:\n        oO000oo0 = rloc.rloc\n        OOoO0ooo0ooo0 = rloc.translated_port if lisp_i_am_rtr else LISP_DATA_PORT\n        if 51 - 51:\n            iIii1I11I1II1 % Ii1I + iIii1I11I1II1 + oO0o - IiII * o0oOOo0O0Ooo\n        if 13 - 13:\n            iIii1I11I1II1\n        if 10 - 10:\n            I1IiiI * iII111i * ooOoO0o.IiII\n        if 7 - 7:\n            iIii1I11I1II1\n        if 60 - 60:\n            OOooOOo.Ii1I.Ii1I % II111iiii + OoO0O00\n    ooOoOoOo, OOOOO0, OoO = lisp_myrlocs\n    if ooOoOoOo == None:\n        lprint('Suppress sending Map-Request, IPv4 RLOC not found')\n        return\n        if 49 - 49:\n            I1IiiI.I1ii11iIi11i / Oo0Ooo\n    if OOOOO0 == None and oO000oo0 != None and oO000oo0.is_ipv6():\n        lprint('Suppress sending Map-Request, IPv6 RLOC not found')\n        return\n        if 24 - 24:\n            II111iiii\n        if 40 - 40:\n            o0oOOo0O0Ooo.I1IiiI - o0oOOo0O0Ooo\n    i1i = lisp_map_request()\n    i1i.record_count = 1\n    i1i.nonce = lisp_get_control_nonce()\n    i1i.rloc_probe = oO000oo0 != None\n    i1i.subscribe_bit = pubsub\n    i1i.xtr_id_present = pubsub\n    if 62 - 62:\n        oO0o\n    if 71 - 71:\n        i1IIi.I1ii11iIi11i / i11iIiiIii + II111iiii\n    if 14 - 14:\n        iII111i\n    if 35 - 35:\n        Ii1I\n    if 54 - 54:\n        OOooOOo\n    if 83 - 83:\n        i1IIi / II111iiii - I1IiiI + I1ii11iIi11i.IiII * oO0o\n    if 92 - 92:\n        OoOoOO00 + oO0o % Ii1I / Ii1I - iII111i\n    if rloc:\n        rloc.last_rloc_probe_nonce = i1i.nonce\n    if 11 - 11:\n        Oo0Ooo % II111iiii * Ii1I + II111iiii\n    I1IIiIiIIiIiI = deid.is_multicast_address()\n    if I1IIiIiIIiIiI:\n        i1i.target_eid = seid\n        i1i.target_group = deid\n    else:\n        i1i.target_eid = deid\n        if 9 - 9:\n            I1Ii111\n        if 69 - 69:\n            i1IIi + ooOoO0o + Ii1I\n        if 88 - 88:\n            OoOoOO00 + iII111i % O0 + OOooOOo / OoooooooOO / OOooOOo\n        if 95 - 95:\n            ooOoO0o.Oo0Ooo % IiII + iII111i\n        if 16 - 16:\n            I11i * OoO0O00 % o0oOOo0O0Ooo - O0 % II111iiii - I1IiiI\n        if 72 - 72:\n            OoooooooOO * OoOoOO00.OOooOOo + Ii1I.OOooOOo / II111iiii\n        if 8 - 8:\n            i1IIi\n        if 1 - 1:\n            OoOoOO00.OoO0O00.OoO0O00 * O0\n        if 97 - 97:\n            OoooooooOO % ooOoO0o.I1Ii111 / iII111i\n    if i1i.rloc_probe == False:\n        oooOOoO0oo0 = lisp_get_signature_eid()\n        if oooOOoO0oo0:\n            i1i.signature_eid.copy_address(oooOOoO0oo0.eid)\n            i1i.privkey_filename = './lisp-sig.pem'\n            if 59 - 59:\n                II111iiii + O0.I1ii11iIi11i.Oo0Ooo * OoO0O00\n            if 35 - 35:\n                oO0o / I1Ii111 * OOooOOo + OoooooooOO.IiII\n            if 1 - 1:\n                I1IiiI + I1Ii111 / OOooOOo.Ii1I.oO0o / I1ii11iIi11i\n            if 54 - 54:\n                OOooOOo\n            if 86 - 86:\n                oO0o * Oo0Ooo / OOooOOo\n            if 18 - 18:\n                II111iiii - I1Ii111\n    if seid == None or I1IIiIiIIiIiI:\n        i1i.source_eid.afi = LISP_AFI_NONE\n    else:\n        i1i.source_eid = seid\n        if 13 - 13:\n            i11iIiiIii - O0 % OoOoOO00 + OOooOOo * ooOoO0o\n        if 55 - 55:\n            i1IIi - OOooOOo / I11i * Ii1I\n        if 20 - 20:\n            OoOoOO00 * iIii1I11I1II1 % O0 - i1IIi\n        if 51 - 51:\n            I1ii11iIi11i * Ii1I - oO0o / O0 * OoooooooOO\n        if 12 - 12:\n            i1IIi / iIii1I11I1II1 / O0 * OoO0O00\n        if 15 - 15:\n            i11iIiiIii / IiII + Ii1I % OOooOOo % I1ii11iIi11i * oO0o\n        if 24 - 24:\n            OOooOOo / OOooOOo + I11i / iII111i.oO0o - iII111i\n        if 59 - 59:\n            I1ii11iIi11i % II111iiii - i11iIiiIii - I1Ii111\n        if 34 - 34:\n            II111iiii + iII111i / IiII\n        if 47 - 47:\n            OoO0O00\n        if 40 - 40:\n            o0oOOo0O0Ooo / iII111i.o0oOOo0O0Ooo\n        if 63 - 63:\n            o0oOOo0O0Ooo * iIii1I11I1II1 * II111iiii.OoO0O00 - oO0o / OoOoOO00\n    if oO000oo0 != None and lisp_nat_traversal and (lisp_i_am_rtr == False):\n        if oO000oo0.is_private_address() == False:\n            ooOoOoOo = lisp_get_any_translated_rloc()\n            if 78 - 78:\n                i11iIiiIii / OoO0O00 / i1IIi.i11iIiiIii\n        if ooOoOoOo == None:\n            lprint('Suppress sending Map-Request, translated RLOC not found')\n            return\n            if 100 - 100:\n                II111iiii.IiII.I11i\n            if 60 - 60:\n                OoOoOO00 % OOooOOo * i1IIi\n            if 3 - 3:\n                OoooooooOO\n            if 75 - 75:\n                OoooooooOO * I1Ii111 * o0oOOo0O0Ooo + I1ii11iIi11i.iIii1I11I1II1 / O0\n            if 23 - 23:\n                oO0o - O0 * IiII + i11iIiiIii * Ii1I\n            if 8 - 8:\n                ooOoO0o / II111iiii.I1ii11iIi11i * ooOoO0o % oO0o\n            if 36 - 36:\n                I1ii11iIi11i % OOooOOo - ooOoO0o - I11i + I1IiiI\n            if 37 - 37:\n                I1ii11iIi11i * IiII\n    if oO000oo0 == None or oO000oo0.is_ipv4():\n        if lisp_nat_traversal and oO000oo0 == None:\n            O0ooOoOO0OoO0O0 = lisp_get_any_translated_rloc()\n            if O0ooOoOO0OoO0O0 != None:\n                ooOoOoOo = O0ooOoOO0OoO0O0\n            if 86 - 86:\n                I11i * iIii1I11I1II1 - I1ii11iIi11i % IiII.OOooOOo * I1Ii111\n        i1i.itr_rlocs.append(ooOoOoOo)\n        if 49 - 49:\n            OoOoOO00 * OoOoOO00\n    if oO000oo0 == None or oO000oo0.is_ipv6():\n        if OOOOO0 == None or OOOOO0.is_ipv6_link_local():\n            OOOOO0 = None\n        else:\n            i1i.itr_rloc_count = 1 if oO000oo0 == None else 0\n            i1i.itr_rlocs.append(OOOOO0)\n            if 86 - 86:\n                iIii1I11I1II1\n            if 42 - 42:\n                ooOoO0o * ooOoO0o.I1ii11iIi11i.ooOoO0o / I1IiiI * iIii1I11I1II1\n            if 50 - 50:\n                Ii1I\n            if 16 - 16:\n                OoooooooOO / oO0o + I1IiiI / O0\n            if 12 - 12:\n                ooOoO0o / I1IiiI % Oo0Ooo - II111iiii / i11iIiiIii\n            if 33 - 33:\n                o0oOOo0O0Ooo + IiII / OoOoOO00 / ooOoO0o\n            if 9 - 9:\n                OoOoOO00\n            if 44 - 44:\n                Oo0Ooo.i11iIiiIii % OOooOOo\n            if 87 - 87:\n                o0oOOo0O0Ooo\n    if oO000oo0 != None and i1i.itr_rlocs != []:\n        oO0o0o00O = i1i.itr_rlocs[0]\n    elif deid.is_ipv4():\n        oO0o0o00O = ooOoOoOo\n    elif deid.is_ipv6():\n        oO0o0o00O = OOOOO0\n    else:\n        oO0o0o00O = ooOoOoOo\n        if 41 - 41:\n            OoooooooOO.iII111i / oO0o\n        if 16 - 16:\n            iII111i + o0oOOo0O0Ooo / II111iiii * i11iIiiIii * OoO0O00.iIii1I11I1II1\n        if 34 - 34:\n            I11i / o0oOOo0O0Ooo * OOooOOo * OOooOOo\n        if 89 - 89:\n            I1ii11iIi11i.OoooooooOO\n        if 61 - 61:\n            i1IIi + i11iIiiIii\n        if 59 - 59:\n            i11iIiiIii * OOooOOo + i1IIi * iIii1I11I1II1 + I11i\n    o0o0ooOOo0oO = i1i.encode(oO000oo0, OOoO0ooo0ooo0)\n    i1i.print_map_request()\n    if 97 - 97:\n        OoO0O00 - I11i.OoooooooOO\n    if 58 - 58:\n        I1ii11iIi11i / II111iiii / i11iIiiIii\n    if 27 - 27:\n        iIii1I11I1II1 - O0 + OoOoOO00\n    if 28 - 28:\n        oO0o.IiII * iII111i % Oo0Ooo - OoO0O00 / I11i\n    if 67 - 67:\n        i11iIiiIii + i11iIiiIii / ooOoO0o - o0oOOo0O0Ooo\n    if 94 - 94:\n        O0 + OoO0O00 / I1IiiI * II111iiii * i11iIiiIii\n    if oO000oo0 != None:\n        if rloc.is_rloc_translated():\n            I1OOoO0OoOOo0 = lisp_get_nat_info(oO000oo0, rloc.rloc_name)\n            if 55 - 55:\n                OoooooooOO * O0 + i1IIi % I1IiiI\n            if 10 - 10:\n                II111iiii - Ii1I.I11i.O0 + Ii1I\n            if 50 - 50:\n                iIii1I11I1II1 / Ii1I.ooOoO0o / ooOoO0o * OoOoOO00 * iII111i\n            if 15 - 15:\n                o0oOOo0O0Ooo % II111iiii + I1IiiI\n            if I1OOoO0OoOOo0 == None:\n                iiiI1I = rloc.rloc.print_address_no_iid()\n                OoIi1I1I = 'gleaned-{}'.format(iiiI1I)\n                IIIiIIi111 = rloc.translated_port\n                I1OOoO0OoOOo0 = lisp_nat_info(iiiI1I, OoIi1I1I, IIIiIIi111)\n                if 21 - 21:\n                    I1ii11iIi11i - ooOoO0o\n            lisp_encapsulate_rloc_probe(lisp_sockets, oO000oo0, I1OOoO0OoOOo0, o0o0ooOOo0oO)\n            return\n            if 81 - 81:\n                iII111i / i11iIiiIii / I1Ii111\n            if 70 - 70:\n                I1ii11iIi11i / i11iIiiIii\n        O0O0 = oO000oo0.print_address_no_iid()\n        OO0oooOO = lisp_convert_4to6(O0O0)\n        lisp_send(lisp_sockets, OO0oooOO, LISP_CTRL_PORT, o0o0ooOOo0oO)\n        return\n        if 90 - 90:\n            II111iiii / OoOoOO00.Ii1I.OoooooooOO\n        if 76 - 76:\n            OoooooooOO\n        if 78 - 78:\n            IiII % i11iIiiIii\n        if 23 - 23:\n            iIii1I11I1II1 - o0oOOo0O0Ooo - Ii1I % OOooOOo\n        if 100 - 100:\n            oO0o.OoO0O00.i11iIiiIii % II111iiii * IiII\n        if 81 - 81:\n            OOooOOo - OOooOOo + OoOoOO00\n    iiII11iI = None if lisp_i_am_rtr else seid\n    if lisp_decent_pull_xtr_configured():\n        oooO = lisp_get_decent_map_resolver(deid)\n    else:\n        oooO = lisp_get_map_resolver(None, iiII11iI)\n        if 34 - 34:\n            OoOoOO00 * o0oOOo0O0Ooo * i11iIiiIii - I11i % oO0o / OoO0O00\n    if oooO == None:\n        lprint('Cannot find Map-Resolver for source-EID {}'.format(green(seid.print_address(), False)))\n        if 75 - 75:\n            i1IIi / Ii1I * OoO0O00 - I1ii11iIi11i * O0.IiII\n        return\n        if 11 - 11:\n            I11i / Ii1I % oO0o\n    oooO.last_used = lisp_get_timestamp()\n    oooO.map_requests_sent += 1\n    if oooO.last_nonce == 0:\n        oooO.last_nonce = i1i.nonce\n    if 50 - 50:\n        i11iIiiIii\n    if 93 - 93:\n        i1IIi / Ii1I * II111iiii - Oo0Ooo.OoOoOO00 - OOooOOo\n    if 25 - 25:\n        I11i / ooOoO0o % ooOoO0o - OOooOOo\n    if 59 - 59:\n        I1IiiI + o0oOOo0O0Ooo.iIii1I11I1II1 - O0 - i11iIiiIii\n    if seid == None:\n        seid = oO0o0o00O\n    lisp_send_ecm(lisp_sockets, o0o0ooOOo0oO, seid, lisp_ephem_port, deid, oooO.map_resolver)\n    if 4 - 4:\n        I1IiiI\n    if 36 - 36:\n        Ii1I\n    if 76 - 76:\n        i11iIiiIii + i1IIi\n    if 56 - 56:\n        OoOoOO00 + II111iiii / i11iIiiIii * OoOoOO00 * OoooooooOO\n    lisp_last_map_request_sent = lisp_get_timestamp()\n    if 15 - 15:\n        OoOoOO00 / OoooooooOO + OOooOOo\n    if 76 - 76:\n        Ii1I * iII111i.OoooooooOO\n    if 92 - 92:\n        iIii1I11I1II1 - Oo0Ooo - I1IiiI - OOooOOo * I1Ii111\n    if 44 - 44:\n        I1Ii111 - II111iiii / OOooOOo\n    oooO.resolve_dns_name()\n    return\n    if 50 - 50:\n        I11i / I1ii11iIi11i\n    if 60 - 60:\n        II111iiii / Ii1I + OoO0O00 % I1IiiI * i1IIi / II111iiii\n    if 91 - 91:\n        I1IiiI * I1Ii111 * i11iIiiIii - oO0o - IiII + I1ii11iIi11i\n    if 99 - 99:\n        OoO0O00 % o0oOOo0O0Ooo\n    if 3 - 3:\n        OOooOOo / OoOoOO00 % iIii1I11I1II1\n    if 47 - 47:\n        ooOoO0o.i11iIiiIii / OoO0O00\n    if 48 - 48:\n        O0\n    if 89 - 89:\n"]]}
{"hexsha": "b752c168ea08dcf89e18dfa775764df3a73f31cf", "ext": "py", "lang": "Python", "content": "def update_inter_deployment_dependencies(sm):\n    dependencies_list = sm.list(models.InterDeploymentDependencies)\n    for dependency in dependencies_list:\n        if (dependency.target_deployment_func and\n                not dependency.external_target):\n            _update_dependency_target_deployment(sm, dependency)", "fn_id": 15, "class_fn": false, "repo": "cloudify-cosmo/cloudify-manager", "file": "rest-service/manager_rest/rest/rest_utils.py", "last_update_at": "2022-02-26T23:12:06+00:00", "question_id": "b752c168ea08dcf89e18dfa775764df3a73f31cf_15", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def update_inter_deployment_dependencies(sm):\n    dependencies_list = sm.list(models.InterDeploymentDependencies)\n    for dependency in dependencies_list:\n        if dependency.target_deployment_func and (not dependency.external_target):\n"]]}
{"hexsha": "53b8bd173fccd280bd430a9e0f66641a1e2f1f50", "ext": "py", "lang": "Python", "content": "def port(valobj):\n    val = valobj.unsigned\n    if (val & 0xF) == 0x7:\n        target = valobj.target\n        if etp_arch_bits(target) == 64 and not etp_halfword(target):\n            if etp_big_endian(target):\n                data = (val >> 36) & 0x0FFFFFFF\n            else:\n                data = (val >> 4) & 0x0FFFFFFF\n        else:\n            data = pixdata2data(valobj)\n        return '#Port<0.%u>' % data\n    else:\n        return '#NotPort<%#x>' % val", "fn_id": 18, "class_fn": false, "repo": "abc3/otp", "file": "erts/etc/unix/etp.py", "last_update_at": "2022-03-30T04:09:46+00:00", "question_id": "53b8bd173fccd280bd430a9e0f66641a1e2f1f50_18", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def port(valobj):\n    val = valobj.unsigned\n    if val & 15 == 7:\n        target = valobj.target\n        if etp_arch_bits(target) == 64 and (not etp_halfword(target)):\n            if etp_big_endian(target):\n                data = val >> 36 & 268435455\n            else:\n                data = val >> 4 & 268435455\n        else:\n            data = pixdata2data(valobj)\n        return '#Port<0.%u>' % data\n    else:\n"]]}
{"hexsha": "04e31660902b5e889bb5392c7f71a26c9cce503f", "ext": "py", "lang": "Python", "content": "def fetch_disk_by_id(id, devices):\n    for d in devices:\n        if d['RaidDevice'] == id:\n            return d\n    return []", "fn_id": 4, "class_fn": false, "repo": "meramsey/python-scripts-collection", "file": "scripts/smart_check.py", "last_update_at": "2022-03-06T14:34:10+00:00", "question_id": "04e31660902b5e889bb5392c7f71a26c9cce503f_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def fetch_disk_by_id(id, devices):\n    for d in devices:\n        if d['RaidDevice'] == id:\n            return d\n"]]}
{"hexsha": "6ee4a7a1bf37d799738f456c5c942a298c4d124b", "ext": "py", "lang": "Python", "content": "def test_dereference_as_vmethod_using_known_methods_x(mock_indexed_resource):\n    resource = Resource(\n        id=\"did:example:123#did-communication\",\n        service_endpoint=\"did:example:123\",\n        type=\"did-communication\",\n    )\n    test = mock_indexed_resource(resource)\n    with pytest.raises(ValueError):\n        test.dereference_as(KnownVerificationMethods, \"test\")", "fn_id": 7, "class_fn": false, "repo": "dbluhm/pydid", "file": "tests/test_resource.py", "last_update_at": "2022-03-22T11:34:18+00:00", "question_id": "6ee4a7a1bf37d799738f456c5c942a298c4d124b_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_dereference_as_vmethod_using_known_methods_x(mock_indexed_resource):\n    resource = Resource(id='did:example:123#did-communication', service_endpoint='did:example:123', type='did-communication')\n    test = mock_indexed_resource(resource)\n    with pytest.raises(ValueError):\n"]]}
{"hexsha": "1b301db7d9722f350f904e1d5905599acdb5b466", "ext": "py", "lang": "Python", "content": "def calculate_blkio_bytes(d):\n    \"\"\"\n\n    :param d:\n    :return: (read_bytes, wrote_bytes), ints\n    \"\"\"\n    bytes_stats = graceful_chain_get(d, \"blkio_stats\", \"io_service_bytes_recursive\")\n    if not bytes_stats:\n        return 0, 0\n    r = 0\n    w = 0\n    for s in bytes_stats:\n        if s[\"op\"] == \"Read\":\n            r += s[\"value\"]\n        elif s[\"op\"] == \"Write\":\n            w += s[\"value\"]\n    return r, w", "fn_id": 7, "class_fn": false, "repo": "lachmanfrantisek/sen", "file": "sen/util.py", "last_update_at": "2022-03-21T02:27:28+00:00", "question_id": "1b301db7d9722f350f904e1d5905599acdb5b466_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def calculate_blkio_bytes(d):\n    \"\"\"\n\n    :param d:\n    :return: (read_bytes, wrote_bytes), ints\n    \"\"\"\n    bytes_stats = graceful_chain_get(d, 'blkio_stats', 'io_service_bytes_recursive')\n    if not bytes_stats:\n        return (0, 0)\n    r = 0\n    w = 0\n    for s in bytes_stats:\n        if s['op'] == 'Read':\n            r += s['value']\n        elif s['op'] == 'Write':\n            w += s['value']\n"]]}
{"hexsha": "20395cd788c5cd7b10e4f9c757c29fb21de0c88c", "ext": "py", "lang": "Python", "content": "def extract_all_links(site):\n    html = requests.get(site).text\n    soup = BeautifulSoup(html, \"html.parser\").find_all(\"a\")\n    links = [link.get(\"href\") for link in soup]\n    return links", "fn_id": 0, "class_fn": false, "repo": "Kalebu/Link-scraper-in-python", "file": "link_spider.py", "last_update_at": "2022-03-12T13:28:35+00:00", "question_id": "20395cd788c5cd7b10e4f9c757c29fb21de0c88c_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def extract_all_links(site):\n    html = requests.get(site).text\n    soup = BeautifulSoup(html, 'html.parser').find_all('a')\n    links = [link.get('href') for link in soup]\n"]]}
{"hexsha": "74ff59c439b5efe8f7e48ad718d5d621828d65a7", "ext": "py", "lang": "Python", "content": "def sep_knowns(cycles):\n    unknowns = []\n    knowns = []\n\n    i = 0\n    while i < len(cycles):\n        if i + 1 < len(cycles) and len(cycles[i]) == len(cycles[i + 1]):\n            unknowns.append(cycles[i])\n            unknowns.append(cycles[i + 1])\n            i += 2\n        else:\n            knowns.append(cycles[i])\n            i += 1\n\n    return knowns, unknowns", "fn_id": 4, "class_fn": false, "repo": "mihaid-b/CyberSakura", "file": "Gathered CTF writeups/ctf-7867/2020/pbctf/queensarah2/guess.py", "last_update_at": "2022-03-27T06:00:41+00:00", "question_id": "74ff59c439b5efe8f7e48ad718d5d621828d65a7_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def sep_knowns(cycles):\n    unknowns = []\n    knowns = []\n    i = 0\n    while i < len(cycles):\n        if i + 1 < len(cycles) and len(cycles[i]) == len(cycles[i + 1]):\n            unknowns.append(cycles[i])\n            unknowns.append(cycles[i + 1])\n            i += 2\n        else:\n            knowns.append(cycles[i])\n            i += 1\n"]]}
{"hexsha": "e6c2a00b8f42410bfe9bfd9c8b2f683b0ac63e3a", "ext": "py", "lang": "Python", "content": "def glorot(tensor):\n    if tensor is not None:\n        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n        tensor.data.uniform_(-stdv, stdv)", "fn_id": 0, "class_fn": false, "repo": "yvquanli/TrimNet", "file": "trimnet_drug/source/model.py", "last_update_at": "2022-03-23T11:30:23+00:00", "question_id": "e6c2a00b8f42410bfe9bfd9c8b2f683b0ac63e3a_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def glorot(tensor):\n    if tensor is not None:\n        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n"]]}
{"hexsha": "4426a623f623fd1623255345c1f28dec064b7fcd", "ext": "py", "lang": "Python", "content": "def spider(url) -> Tuple[List[str], List[Result]]:\n    global _links, _insecure, _tasks, _lock\n\n    results: List[Result] = []\n\n    # create processing pool\n    pool = Pool()\n    mgr = Manager()\n    queue = mgr.Queue()\n\n    asy = pool.apply_async(_get_links, (url, [url], queue, pool))\n\n    with _lock:\n        _tasks.append(asy)\n\n    while True:\n        if all(t is None or t.ready() for t in _tasks):\n            break\n        else:\n            count_none = 0\n            count_ready = 0\n            count_not_ready = 0\n\n            for t in _tasks:\n                if t is None:\n                    count_none += 1\n                elif t.ready():\n                    count_ready += 1\n                else:\n                    count_not_ready += 1\n\n            output.debug(\n                f\"Spider Task Status: None: {count_none}, Ready: {count_ready}, Not Ready: {count_not_ready}\"\n            )\n\n        time.sleep(3)\n\n    pool.close()\n\n    for t in _tasks:\n        try:\n            t.get()\n        except Exception:\n            output.debug_exception()\n\n    while not queue.empty():\n        res = queue.get()\n\n        if len(res) > 0:\n            for re in res:\n                if re not in results:\n                    results.append(re)\n\n    # copy data and reset\n    links = _links[:]\n    _links = []\n    _insecure = []\n    _tasks = []\n\n    return links, results", "fn_id": 0, "class_fn": false, "repo": "Prodject/yawast", "file": "yawast/scanner/plugins/http/spider.py", "last_update_at": "2022-03-14T00:50:25+00:00", "question_id": "4426a623f623fd1623255345c1f28dec064b7fcd_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def spider(url) -> Tuple[List[str], List[Result]]:\n    global _links, _insecure, _tasks, _lock\n    results: List[Result] = []\n    pool = Pool()\n    mgr = Manager()\n    queue = mgr.Queue()\n    asy = pool.apply_async(_get_links, (url, [url], queue, pool))\n    with _lock:\n        _tasks.append(asy)\n    while True:\n        if all((t is None or t.ready() for t in _tasks)):\n            break\n        else:\n            count_none = 0\n            count_ready = 0\n            count_not_ready = 0\n            for t in _tasks:\n                if t is None:\n                    count_none += 1\n                elif t.ready():\n                    count_ready += 1\n                else:\n                    count_not_ready += 1\n            output.debug(f'Spider Task Status: None: {count_none}, Ready: {count_ready}, Not Ready: {count_not_ready}')\n        time.sleep(3)\n    pool.close()\n    for t in _tasks:\n        try:\n            t.get()\n        except Exception:\n            output.debug_exception()\n    while not queue.empty():\n        res = queue.get()\n        if len(res) > 0:\n            for re in res:\n                if re not in results:\n                    results.append(re)\n    links = _links[:]\n    _links = []\n    _insecure = []\n    _tasks = []\n"]]}
{"hexsha": "467ae70af6eefaf632ced2f238a3bcb8b79bcbae", "ext": "py", "lang": "Python", "content": "def create_canonical_request_string(method,\n                                    uri,\n                                    headers,\n                                    auth_method):\n    \"\"\"\n    Create a canonical request string from aspects of the request.\n    \"\"\"\n    headers_of_interest = []\n    for header_name in ['content-type', 'x-altus-date']:\n        found = False\n        for key in headers:\n            key_lc = key.lower()\n            if headers[key] is not None and key_lc == header_name:\n                headers_of_interest.append(headers[key].strip())\n                found = True\n        if not found:\n            headers_of_interest.append('')\n\n    # Our signature verification with treat a query with no = as part of the\n    # path, so we do as well. It appears to be a behavior left to the server\n    # implementation, and python and our java servlet implementation disagree.\n    uri_components = urlparse(uri)\n    path = uri_components.path\n    if not path:\n        path = '/'\n    if uri_components.query and '=' not in uri_components.query:\n        path += '?' + uri_components.query\n\n    canonical_string = method.upper() + '\\n'\n    canonical_string += '\\n'.join(headers_of_interest) + '\\n'\n    canonical_string += path + '\\n'\n    canonical_string += auth_method\n\n    return canonical_string", "fn_id": 0, "class_fn": false, "repo": "cloudera/cloudera-airflow-plugins", "file": "cloudera_airflow_provider/cloudera/cdp/security/cdp_requests/cdpv1sign.py", "last_update_at": "2022-02-01T09:41:12+00:00", "question_id": "467ae70af6eefaf632ced2f238a3bcb8b79bcbae_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_canonical_request_string(method, uri, headers, auth_method):\n    \"\"\"\n    Create a canonical request string from aspects of the request.\n    \"\"\"\n    headers_of_interest = []\n    for header_name in ['content-type', 'x-altus-date']:\n        found = False\n        for key in headers:\n            key_lc = key.lower()\n            if headers[key] is not None and key_lc == header_name:\n                headers_of_interest.append(headers[key].strip())\n                found = True\n        if not found:\n            headers_of_interest.append('')\n    uri_components = urlparse(uri)\n    path = uri_components.path\n    if not path:\n        path = '/'\n    if uri_components.query and '=' not in uri_components.query:\n        path += '?' + uri_components.query\n    canonical_string = method.upper() + '\\n'\n    canonical_string += '\\n'.join(headers_of_interest) + '\\n'\n    canonical_string += path + '\\n'\n    canonical_string += auth_method\n"]]}
{"hexsha": "0af95f25fd9bf6e9d668ff3d159566e805dc121f", "ext": "py", "lang": "Python", "content": "def get_teamocil_dir():\n    \"\"\"\n    Return teamocil configuration directory.\n\n    Returns\n    -------\n    str :\n        absolute path to teamocil config directory\n\n    See Also\n    --------\n    :meth:`tmuxp.config.import_teamocil`\n    \"\"\"\n    return os.path.expanduser('~/.teamocil/')", "fn_id": 2, "class_fn": false, "repo": "betoSolares/tmuxp", "file": "tmuxp/cli.py", "last_update_at": "2022-02-09T14:10:46+00:00", "question_id": "0af95f25fd9bf6e9d668ff3d159566e805dc121f_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_teamocil_dir():\n    \"\"\"\n    Return teamocil configuration directory.\n\n    Returns\n    -------\n    str :\n        absolute path to teamocil config directory\n\n    See Also\n    --------\n    :meth:`tmuxp.config.import_teamocil`\n    \"\"\"\n"]]}
{"hexsha": "ede59415b222058bdfb718d056a94fa5e203948f", "ext": "py", "lang": "Python", "content": "@pytest.mark.asyncio\nasync def test_sound_volume(bleak_client, client):\n    bleak_client.read_gatt_char.return_value = b\"a\" * 32\n    await client.get_sound_volume()", "fn_id": 11, "class_fn": false, "repo": "quantsini/pymoof", "file": "tests/sx3_test.py", "last_update_at": "2022-03-22T20:19:42+00:00", "question_id": "ede59415b222058bdfb718d056a94fa5e203948f_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.asyncio\nasync def test_sound_volume(bleak_client, client):\n    bleak_client.read_gatt_char.return_value = b'a' * 32\n"]]}
{"hexsha": "897605ab48e22dd8b2c5b5938d21db144d77699d", "ext": "py", "lang": "Python", "content": "def split_name (name):\n    \"\"\"Returns base, lower, upper strings of name\"\"\"\n    def _find (n, s):\n        idx = n.find(s)\n        return idx if idx > 0 else len(n)\n\n    base_end = min(map(lambda x: _find(name, x), [LOWER_TOK, UPPER_TOK]))\n    low_start = _find(name, LOWER_TOK)\n    up_start = _find(name, UPPER_TOK)\n    low_end = up_start if up_start > low_start else len(name)\n    up_end = low_start if up_start < low_start else len(name)\n    base = name[:base_end]\n    low = name[low_start:low_end].strip(LOWER_TOK + '{' + '}')\n    up = name[up_start:up_end].strip(UPPER_TOK + '{' + '}')\n    return base, low, up", "fn_id": 0, "class_fn": false, "repo": "IgnitionProject/ignition", "file": "ignition/dsl/flame/tensors/tensor_names.py", "last_update_at": "2022-03-09T17:39:12+00:00", "question_id": "897605ab48e22dd8b2c5b5938d21db144d77699d_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def split_name(name):\n    \"\"\"Returns base, lower, upper strings of name\"\"\"\n\n    def _find(n, s):\n        idx = n.find(s)\n        return idx if idx > 0 else len(n)\n    base_end = min(map(lambda x: _find(name, x), [LOWER_TOK, UPPER_TOK]))\n    low_start = _find(name, LOWER_TOK)\n    up_start = _find(name, UPPER_TOK)\n    low_end = up_start if up_start > low_start else len(name)\n    up_end = low_start if up_start < low_start else len(name)\n    base = name[:base_end]\n    low = name[low_start:low_end].strip(LOWER_TOK + '{' + '}')\n    up = name[up_start:up_end].strip(UPPER_TOK + '{' + '}')\n"]]}
{"hexsha": "073900b812eade913892526f02d60c827bc0c8b0", "ext": "py", "lang": "Python", "content": "def test_output(tmpdir):\n    f = tmpdir.join(\"test.py\")\n    f.write(\n        \"\"\"\nimport ddtrace\n\"\"\".lstrip()\n    )\n    p = subprocess.Popen(\n        [\"ddtrace-run\", sys.executable, \"test.py\"],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        cwd=str(tmpdir),\n    )\n    p.wait()\n    assert p.stderr.read() == six.b(\"\")\n    assert p.stdout.read() == six.b(\"\")\n    assert p.returncode == 0", "fn_id": 2, "class_fn": false, "repo": "tilan7663/dd-trace-py", "file": "tests/integration/test_integration.py", "last_update_at": "2022-01-11T10:20:15+00:00", "question_id": "073900b812eade913892526f02d60c827bc0c8b0_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_output(tmpdir):\n    f = tmpdir.join('test.py')\n    f.write('\\nimport ddtrace\\n'.lstrip())\n    p = subprocess.Popen(['ddtrace-run', sys.executable, 'test.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=str(tmpdir))\n    p.wait()\n    assert p.stderr.read() == six.b('')\n    assert p.stdout.read() == six.b('')\n"]]}
{"hexsha": "44c50dded00c553f091bf66142e1117deaf0d6f7", "ext": "py", "lang": "Python", "content": "def _build_mFIZ_tab(parent_layout):\n    mFIZ_tab_layout = pm.columnLayout('mFIZTab',\n                                       adj=True,\n                                       width=100)\n\n    _build_add_mFIZ_node_frame(mFIZ_tab_layout)\n\n    pm.setParent(parent_layout)\n    return mFIZ_tab_layout", "fn_id": 30, "class_fn": false, "repo": "Bootsmaat/Mimic", "file": "mimic/scripts/mimic_ui.py", "last_update_at": "2022-03-30T03:47:56+00:00", "question_id": "44c50dded00c553f091bf66142e1117deaf0d6f7_30", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _build_mFIZ_tab(parent_layout):\n    mFIZ_tab_layout = pm.columnLayout('mFIZTab', adj=True, width=100)\n    _build_add_mFIZ_node_frame(mFIZ_tab_layout)\n    pm.setParent(parent_layout)\n"]]}
{"hexsha": "b53e5b32b9de322459deba00499085812a761241", "ext": "py", "lang": "Python", "content": "def create_library_changelog():\n    log('CHANGELOG')\n    log('-----------------------------------------------------------------------', True)\n    initialize_global_params_from_config_file()\n    if current_export_file:\n        previous_song_rows = []\n        current_song_rows = []\n        if previous_export_file:\n            log('Previous export file: ')\n            log(os.path.abspath(previous_export_file))\n            previous_song_rows.extend(import_track_records_from_csv_file(previous_export_file))\n        log('\\nCurrent export file: ')\n        log(os.path.abspath(current_export_file))\n        current_song_rows.extend(import_track_records_from_csv_file(current_export_file))\n        log('\\nFiles loaded successfully, Creating changelog...', True)\n\n        track_matches = create_match_results(previous_song_rows, current_song_rows)\n        duplicates = create_duplicates_results(current_song_rows)\n\n        changelog_results = track_matches + duplicates\n        # setup the output directory, create it if needed\n        create_dir_if_not_exist(output_dir)\n        filename = export_track_matches_to_csv_file(changelog_results)\n        log('Changelog has been created. File with results has been saved in:')\n        log(filename, True)\n    else:\n        log('Changelog cannot be created. Previous and Current Export files not found.', True)", "fn_id": 6, "class_fn": false, "repo": "hharzer/ytmusic-lib-tracker", "file": "ytmusiclibtracker/create_library_changelog.py", "last_update_at": "2022-02-11T01:54:20+00:00", "question_id": "b53e5b32b9de322459deba00499085812a761241_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_library_changelog():\n    log('CHANGELOG')\n    log('-----------------------------------------------------------------------', True)\n    initialize_global_params_from_config_file()\n    if current_export_file:\n        previous_song_rows = []\n        current_song_rows = []\n        if previous_export_file:\n            log('Previous export file: ')\n            log(os.path.abspath(previous_export_file))\n            previous_song_rows.extend(import_track_records_from_csv_file(previous_export_file))\n        log('\\nCurrent export file: ')\n        log(os.path.abspath(current_export_file))\n        current_song_rows.extend(import_track_records_from_csv_file(current_export_file))\n        log('\\nFiles loaded successfully, Creating changelog...', True)\n        track_matches = create_match_results(previous_song_rows, current_song_rows)\n        duplicates = create_duplicates_results(current_song_rows)\n        changelog_results = track_matches + duplicates\n        create_dir_if_not_exist(output_dir)\n        filename = export_track_matches_to_csv_file(changelog_results)\n        log('Changelog has been created. File with results has been saved in:')\n        log(filename, True)\n    else:\n"]]}
{"hexsha": "74494d5bd7009eefa8bef398eb0fa6085ce734cc", "ext": "py", "lang": "Python", "content": "def test_ngram_first_word_match():\n    \"\"\"\n    Test that a first word match is not enough to match.\n    \"\"\"\n    c = load_from_file(fixture_location('long.conll'))\n    it = find_ngrams(c, 'un cabinet'.split())\n\n    with pytest.raises(StopIteration):\n        next(it)", "fn_id": 3, "class_fn": false, "repo": "dantiston/pyconll", "file": "tests/test_util.py", "last_update_at": "2022-03-11T17:40:37+00:00", "question_id": "74494d5bd7009eefa8bef398eb0fa6085ce734cc_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_ngram_first_word_match():\n    \"\"\"\n    Test that a first word match is not enough to match.\n    \"\"\"\n    c = load_from_file(fixture_location('long.conll'))\n    it = find_ngrams(c, 'un cabinet'.split())\n    with pytest.raises(StopIteration):\n"]]}
{"hexsha": "b107d2de721ccfeffa46f77baa875d6b1dec2eb9", "ext": "py", "lang": "Python", "content": "def main():\n\n    parser = argparse.ArgumentParser(description='Convert DEM to OBJ format tri mesh')\n    parser.add_argument(\"-i\", \"--input\", nargs=1, help=\"filename of a GDAL-supported raster format DEM\", required=True)\n    parser.add_argument(\"-o\", \"--output\", nargs=1, help=\"filename to write to\", required=True)\n    parser.add_argument(\"-x\", \"--exaggeration\", type=float, nargs=1, help=\"vertical exaggeration (default 1.0)\", required=False)\n    parser.add_argument(\"-s\", \"--scaling\", type=float, nargs=1, help=\"global scaling (default 0.001)\", required=False)\n    parser.add_argument(\"-v\", \"--verbose\", action='store_true', help=\"verbose (show progress & scary messages)\", required=False)\n    parser.add_argument(\"-w\", \"--wgs84\", action='store_true',  help=\"wgs84 settings (x,y in degrees, elevation in meters\", required=False)\n    parser.add_argument(\"-j\", \"--jitter\", action='store_true', help=\"add jitter (small random offset to x and y)\", required=False)\n\n    scale = 1.0\n    exaggeration = 1.0\n    jitter = False\n\n    arguments = parser.parse_args()\n    filename = arguments.input[0]\n    filename_out = arguments.output[0]\n    if arguments.jitter:\n        jitter = True\n    if arguments.scaling:\n        scale = arguments.scaling[0]\n    if arguments.exaggeration:\n        exaggeration = arguments.exaggeration[0]\n    verbose = arguments.verbose\n    wgs84 = arguments.wgs84\n\n    if verbose:\n        logging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG, datefmt='%m/%d/%Y %I:%M:%S %p')\n    else:\n        logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO, datefmt='%m/%d/%Y %I:%M:%S %p')\n\n    if wgs84:\n        scale = 1.0\n        exaggeration = scale/110000.0\n        logging.info(\"Using WGS84 mode (scale={:.2f}, axeggeration={:.8f}\".format(scale, exaggeration))\n\n    reader = RasterReader()\n    metadata = reader.get_metadata(filename)\n    size = (metadata[\"width\"], metadata[\"height\"])\n    logging.info(\"Image size is {}\".format(size))\n    obj_maker = ObjBuilder(size)\n    mid_x, mid_y = (metadata[\"center_x\"], metadata[\"center_y\"])\n    logging.info(\"Mid point at {},{}\".format(mid_x, mid_y))\n\n    logging.info(\"Scanning raster for xyz values...\")\n    points = 0\n\n    dx = 0.25 * metadata[\"size_x\"]\n    dy = 0.25 * metadata[\"size_y\"]\n\n    for x, y, z in reader.load_raster_xyz(filename):\n        if not jitter:\n            obj_maker.add_vertex(((x-mid_x)*scale, (y-mid_y)*scale, z*scale*exaggeration))\n        else:\n            x_offset = random.uniform(-dx, dx)\n            y_offset = random.uniform(-dy, dy)\n            obj_maker.add_vertex(((x - mid_x + x_offset) * scale, (y - mid_y + y_offset) * scale, z * scale * exaggeration))\n        points += 1\n        if points % 10000 == 0:\n            percent = (points/(size[0]*size[1]))*100.0\n            logging.debug(\"Added {} vertices ({:.2f}%)\".format(points, percent))\n    logging.info(\"Writing OBJ file...\")\n    obj_maker.write_file(filename_out)\n    logging.info(\"OBJ file written\")", "fn_id": 0, "class_fn": false, "repo": "kyleedwardbradley/tectoplot", "file": "pythonscripts/rastertomesh.py", "last_update_at": "2022-01-16T23:27:04+00:00", "question_id": "b107d2de721ccfeffa46f77baa875d6b1dec2eb9_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    parser = argparse.ArgumentParser(description='Convert DEM to OBJ format tri mesh')\n    parser.add_argument('-i', '--input', nargs=1, help='filename of a GDAL-supported raster format DEM', required=True)\n    parser.add_argument('-o', '--output', nargs=1, help='filename to write to', required=True)\n    parser.add_argument('-x', '--exaggeration', type=float, nargs=1, help='vertical exaggeration (default 1.0)', required=False)\n    parser.add_argument('-s', '--scaling', type=float, nargs=1, help='global scaling (default 0.001)', required=False)\n    parser.add_argument('-v', '--verbose', action='store_true', help='verbose (show progress & scary messages)', required=False)\n    parser.add_argument('-w', '--wgs84', action='store_true', help='wgs84 settings (x,y in degrees, elevation in meters', required=False)\n    parser.add_argument('-j', '--jitter', action='store_true', help='add jitter (small random offset to x and y)', required=False)\n    scale = 1.0\n    exaggeration = 1.0\n    jitter = False\n    arguments = parser.parse_args()\n    filename = arguments.input[0]\n    filename_out = arguments.output[0]\n    if arguments.jitter:\n        jitter = True\n    if arguments.scaling:\n        scale = arguments.scaling[0]\n    if arguments.exaggeration:\n        exaggeration = arguments.exaggeration[0]\n    verbose = arguments.verbose\n    wgs84 = arguments.wgs84\n    if verbose:\n        logging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG, datefmt='%m/%d/%Y %I:%M:%S %p')\n    else:\n        logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO, datefmt='%m/%d/%Y %I:%M:%S %p')\n    if wgs84:\n        scale = 1.0\n        exaggeration = scale / 110000.0\n        logging.info('Using WGS84 mode (scale={:.2f}, axeggeration={:.8f}'.format(scale, exaggeration))\n    reader = RasterReader()\n    metadata = reader.get_metadata(filename)\n    size = (metadata['width'], metadata['height'])\n    logging.info('Image size is {}'.format(size))\n    obj_maker = ObjBuilder(size)\n    mid_x, mid_y = (metadata['center_x'], metadata['center_y'])\n    logging.info('Mid point at {},{}'.format(mid_x, mid_y))\n    logging.info('Scanning raster for xyz values...')\n    points = 0\n    dx = 0.25 * metadata['size_x']\n    dy = 0.25 * metadata['size_y']\n    for x, y, z in reader.load_raster_xyz(filename):\n        if not jitter:\n            obj_maker.add_vertex(((x - mid_x) * scale, (y - mid_y) * scale, z * scale * exaggeration))\n        else:\n            x_offset = random.uniform(-dx, dx)\n            y_offset = random.uniform(-dy, dy)\n            obj_maker.add_vertex(((x - mid_x + x_offset) * scale, (y - mid_y + y_offset) * scale, z * scale * exaggeration))\n        points += 1\n        if points % 10000 == 0:\n            percent = points / (size[0] * size[1]) * 100.0\n            logging.debug('Added {} vertices ({:.2f}%)'.format(points, percent))\n    logging.info('Writing OBJ file...')\n    obj_maker.write_file(filename_out)\n"]]}
{"hexsha": "e6e52b062ce90dce00f33a01a55d2781d6d9a952", "ext": "py", "lang": "Python", "content": "def get_mkl_lib(device_id=None, verbose=False):\n    if sys.platform == 'win32':\n        # find *.dll\n        current_path = os.path.dirname(os.path.realpath(__file__))\n        mkl_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'mklml.dll')\n        if not os.path.isfile(mkl_engine_path):\n            neon_logger.display(\"mklml.dll not found\")\n            return 0\n\n        mkl_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'mklEngine.dll')\n        if not os.path.isfile(mkl_engine_path):\n            neon_logger.display(\"mklEngine.dll not found\")\n            return 0\n\n        math_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'cmath.dll')\n        if not os.path.isfile(math_engine_path):\n            neon_logger.display(\"cmath.dll not found\")\n            return 0\n\n        header_path = os.path.join(os.path.dirname(__file__), 'mklEngine',\n                                   'src', 'math_cpu.header')\n        if os.path.isfile(header_path):\n            neon_logger.display(\"math_cpu.header not found\")\n            return 0\n        return 1\n\n    elif sys.platform == 'darwin':\n        # find *.dylib\n        current_path = os.path.dirname(os.path.realpath(__file__))\n        mkl_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'mklEngine.dylib')\n        if not os.path.isfile(mkl_engine_path):\n            neon_logger.display(\"mklEngine.dylib not found\")\n            return 0\n\n        math_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'cmath.dylib')\n        if not os.path.isfile(math_engine_path):\n            neon_logger.display(\"cmath.dylib not found\")\n            return 0\n\n        header_path = os.path.join(os.path.dirname(__file__), 'mklEngine',\n                                   'src', 'math_cpu.header')\n        if os.path.isfile(header_path):\n            neon_logger.display(\"math_cpu.header not found\")\n            return 0\n        return 1\n\n    else:\n        # find *.so\n        current_path = os.path.dirname(os.path.realpath(__file__))\n        mkl_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'mklEngine.so')\n        if not os.path.isfile(mkl_engine_path):\n            neon_logger.display(\"mklEngine.so not found\")\n            return 0\n\n        math_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'cmath.so')\n        if not os.path.isfile(math_engine_path):\n            neon_logger.display(\"cmath.so not found\")\n            return 0\n\n        header_path = os.path.join(os.path.dirname(__file__), 'mklEngine',\n                                   'src', 'math_cpu.header')\n        if os.path.isfile(header_path):\n            neon_logger.display(\"math_cpu.header not found\")\n            return 0\n        return 1", "fn_id": 0, "class_fn": false, "repo": "rsketine/neon", "file": "neon/backends/util/check_mkl.py", "last_update_at": "2022-03-22T13:38:45+00:00", "question_id": "e6e52b062ce90dce00f33a01a55d2781d6d9a952_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_mkl_lib(device_id=None, verbose=False):\n    if sys.platform == 'win32':\n        current_path = os.path.dirname(os.path.realpath(__file__))\n        mkl_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'mklml.dll')\n        if not os.path.isfile(mkl_engine_path):\n            neon_logger.display('mklml.dll not found')\n            return 0\n        mkl_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'mklEngine.dll')\n        if not os.path.isfile(mkl_engine_path):\n            neon_logger.display('mklEngine.dll not found')\n            return 0\n        math_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'cmath.dll')\n        if not os.path.isfile(math_engine_path):\n            neon_logger.display('cmath.dll not found')\n            return 0\n        header_path = os.path.join(os.path.dirname(__file__), 'mklEngine', 'src', 'math_cpu.header')\n        if os.path.isfile(header_path):\n            neon_logger.display('math_cpu.header not found')\n            return 0\n        return 1\n    elif sys.platform == 'darwin':\n        current_path = os.path.dirname(os.path.realpath(__file__))\n        mkl_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'mklEngine.dylib')\n        if not os.path.isfile(mkl_engine_path):\n            neon_logger.display('mklEngine.dylib not found')\n            return 0\n        math_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'cmath.dylib')\n        if not os.path.isfile(math_engine_path):\n            neon_logger.display('cmath.dylib not found')\n            return 0\n        header_path = os.path.join(os.path.dirname(__file__), 'mklEngine', 'src', 'math_cpu.header')\n        if os.path.isfile(header_path):\n            neon_logger.display('math_cpu.header not found')\n            return 0\n        return 1\n    else:\n        current_path = os.path.dirname(os.path.realpath(__file__))\n        mkl_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'mklEngine.so')\n        if not os.path.isfile(mkl_engine_path):\n            neon_logger.display('mklEngine.so not found')\n            return 0\n        math_engine_path = os.path.join(current_path, os.pardir, 'mklEngine', 'cmath.so')\n        if not os.path.isfile(math_engine_path):\n            neon_logger.display('cmath.so not found')\n            return 0\n        header_path = os.path.join(os.path.dirname(__file__), 'mklEngine', 'src', 'math_cpu.header')\n        if os.path.isfile(header_path):\n            neon_logger.display('math_cpu.header not found')\n            return 0\n"]]}
{"hexsha": "d24cef7ccf5f7221c75cc27b06e15f148aa63ee8", "ext": "py", "lang": "Python", "content": "def fetchable_genetics(projects: bool = False) -> List[str]:\n    \"\"\"\n    Lists genetics data available to download from the PPMI\n\n    Parameters\n    ----------\n    projects : bool, optional\n        List available projects instead of individual data files available for\n        download. Due to the size of genetic data, many datasets are split up\n        into multiple files associated with a single project or analysis; you\n        can specify these projects when downloading data with\n        :py:func:`pypmi.datasets.fetch_genetics` and all associated files\n        will be fetched.\n\n    Returns\n    -------\n    available : list\n        List of available data files\n\n    See Also\n    --------\n    pypmi.fetch_genetics\n    \"\"\"\n\n    if projects:\n        return ['project {}'.format(project)\n                for project in [107, 108, 115, 116, 118, 120, 133]]\n    else:\n        return list(_GENETICS.keys())", "fn_id": 3, "class_fn": false, "repo": "greydongilmore/pypmi", "file": "pypmi/fetchers.py", "last_update_at": "2022-03-29T08:36:37+00:00", "question_id": "d24cef7ccf5f7221c75cc27b06e15f148aa63ee8_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def fetchable_genetics(projects: bool=False) -> List[str]:\n    \"\"\"\n    Lists genetics data available to download from the PPMI\n\n    Parameters\n    ----------\n    projects : bool, optional\n        List available projects instead of individual data files available for\n        download. Due to the size of genetic data, many datasets are split up\n        into multiple files associated with a single project or analysis; you\n        can specify these projects when downloading data with\n        :py:func:`pypmi.datasets.fetch_genetics` and all associated files\n        will be fetched.\n\n    Returns\n    -------\n    available : list\n        List of available data files\n\n    See Also\n    --------\n    pypmi.fetch_genetics\n    \"\"\"\n    if projects:\n        return ['project {}'.format(project) for project in [107, 108, 115, 116, 118, 120, 133]]\n    else:\n"]]}
{"hexsha": "54c77769f977a7f078c48115055697d6b756a565", "ext": "py", "lang": "Python", "content": "def merge_and_filter(peaks, args):\n    rep_peaks = get_replicating_peaks( peaks )\n    fil_peaks = filter_by_guide_coverage( rep_peaks, \n                                          pd.read_table( args.casa_guide_file, sep='\\t', header=0 )['Coordinates'],\n                                          min_coverage=args.min_guide_coverage\n                                        )\n    return fil_peaks", "fn_id": 2, "class_fn": false, "repo": "sjgosai/casa", "file": "casa/casa2encode.py", "last_update_at": "2022-02-15T17:31:30+00:00", "question_id": "54c77769f977a7f078c48115055697d6b756a565_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def merge_and_filter(peaks, args):\n    rep_peaks = get_replicating_peaks(peaks)\n    fil_peaks = filter_by_guide_coverage(rep_peaks, pd.read_table(args.casa_guide_file, sep='\\t', header=0)['Coordinates'], min_coverage=args.min_guide_coverage)\n"]]}
{"hexsha": "7b4accb344a7c5bc9011a33b491440566105035e", "ext": "py", "lang": "Python", "content": "def validate_config(app, config):\n    if len(app.config.panels_delimiters) != 3:\n        raise AssertionError(\n            \"panels_delimiters config must be of form: (header, body, footer)\"\n        )\n    if len(set(app.config.panels_delimiters)) != 3:\n        raise AssertionError(\"panels_delimiters config must contain unique values\")\n    try:\n        app.config.panels_delimiters = tuple(\n            [re.compile(s) for s in app.config.panels_delimiters]\n        )\n    except Exception as err:\n        raise AssertionError(\n            \"panels_delimiters config must contain only compilable regexes: {}\".format(\n                err\n            )\n        )", "fn_id": 4, "class_fn": false, "repo": "choldgraf/sphinx-panels", "file": "sphinx_panels/panels.py", "last_update_at": "2022-03-07T15:52:34+00:00", "question_id": "7b4accb344a7c5bc9011a33b491440566105035e_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def validate_config(app, config):\n    if len(app.config.panels_delimiters) != 3:\n        raise AssertionError('panels_delimiters config must be of form: (header, body, footer)')\n    if len(set(app.config.panels_delimiters)) != 3:\n        raise AssertionError('panels_delimiters config must contain unique values')\n    try:\n        app.config.panels_delimiters = tuple([re.compile(s) for s in app.config.panels_delimiters])\n    except Exception as err:\n"]]}
{"hexsha": "c7b5f06e69efa7b2fbbec77ca3b81d61b34645a9", "ext": "py", "lang": "Python", "content": "@lower_builtin(\"set.difference_update\", types.Set, types.IterableType)\ndef set_difference_update(context, builder, sig, args):\n    inst = SetInstance(context, builder, sig.args[0], args[0])\n    other = SetInstance(context, builder, sig.args[1], args[1])\n\n    inst.difference(other)\n\n    return context.get_dummy_value()", "fn_id": 20, "class_fn": false, "repo": "blair1306/numba", "file": "numba/cpython/setobj.py", "last_update_at": "2022-02-14T15:30:21+00:00", "question_id": "c7b5f06e69efa7b2fbbec77ca3b81d61b34645a9_20", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@lower_builtin('set.difference_update', types.Set, types.IterableType)\ndef set_difference_update(context, builder, sig, args):\n    inst = SetInstance(context, builder, sig.args[0], args[0])\n    other = SetInstance(context, builder, sig.args[1], args[1])\n    inst.difference(other)\n"]]}
{"hexsha": "b7d8aba58c8896f79e236018db55298dd7d93010", "ext": "py", "lang": "Python", "content": "def compute_timestep(u, current_delta_t):\n    \"\"\"Return the timestep, based upon the CFL criterion\"\"\"\n\n    ref_vel.interpolate(dot(JacobianInverse(mesh), u))\n    ts_min = 1. / mesh.comm.allreduce(ref_vel.dat.data.max(), MPI.MAX)\n    # Grab (smallest) maximum permitted on all cores:\n    ts_max = min(float(current_delta_t)*increase_tolerance, maximum_timestep)\n    # Compute timestep:\n    tstep = min(ts_min*target_cfl_no, ts_max)\n    return tstep", "fn_id": 2, "class_fn": false, "repo": "sghelichkhani/G-ADOPT", "file": "Davies_etal_GMD_2021/3D_spherical/3d_spherical.py", "last_update_at": "2022-02-11T10:01:04+00:00", "question_id": "b7d8aba58c8896f79e236018db55298dd7d93010_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def compute_timestep(u, current_delta_t):\n    \"\"\"Return the timestep, based upon the CFL criterion\"\"\"\n    ref_vel.interpolate(dot(JacobianInverse(mesh), u))\n    ts_min = 1.0 / mesh.comm.allreduce(ref_vel.dat.data.max(), MPI.MAX)\n    ts_max = min(float(current_delta_t) * increase_tolerance, maximum_timestep)\n    tstep = min(ts_min * target_cfl_no, ts_max)\n"]]}
{"hexsha": "ae8ce29f43055862d58a694a64553975ee0298b6", "ext": "py", "lang": "Python", "content": "def cmp(a, b):\n    if a == b:\n        return 0\n    elif a < b:\n        return -1\n    else:\n        return 1", "fn_id": 25, "class_fn": false, "repo": "albertz/music-player", "file": "mac/pyobjc-core/Lib/objc/_convenience.py", "last_update_at": "2022-03-09T12:51:01+00:00", "question_id": "ae8ce29f43055862d58a694a64553975ee0298b6_25", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def cmp(a, b):\n    if a == b:\n        return 0\n    elif a < b:\n        return -1\n    else:\n"]]}
{"hexsha": "c9e0eae1777a2c1aae0ba0fda714b5777f97799c", "ext": "py", "lang": "Python", "content": "def Init():\n    \n    \"\"\"\n   \n\n    Returns\n    -------\n    edelta : TYPE\n        DESCRIPTION.\n    comspace : TYPE\n        DESCRIPTION.\n    cna_sigs : TYPE\n        DESCRIPTION.\n    adj : TYPE\n        DESCRIPTION.\n    agcn : TYPE\n        DESCRIPTION.\n    com : TYPE\n        DESCRIPTION.\n    comdist : TYPE\n        DESCRIPTION.\n    surf_atoms : TYPE\n        DESCRIPTION.\n    comAu : TYPE\n        DESCRIPTION.\n    comPt : TYPE\n        DESCRIPTION.\n    hoadjAu : TYPE\n        DESCRIPTION.\n    hoadjPt : TYPE\n        DESCRIPTION.\n    comdistAu : TYPE\n        DESCRIPTION.\n    comdistPt : TYPE\n        DESCRIPTION.\n    midcomdistAu : TYPE\n        DESCRIPTION.\n    midcomdistPt : TYPE\n        DESCRIPTION.\n    surf_atomsPt : TYPE\n        DESCRIPTION.\n    headj : TYPE\n        DESCRIPTION.\n    mix : TYPE\n        DESCRIPTION.\n\n    \"\"\"\n    \n    edelta = {}; comspace = {}; cna_sigs = {}\n    com = {}; comdist = {}; surf_atoms = {} \n    comAu = {}; comPt = {}; hoadjAu = {}; hoadjPt = {} \n    comdistAu = {}; comdistPt = {}; midcomdistPt = {} ; nn = {}\n    midcomdistAu = {}; surf_atomsPt = {}; headj = {}; mix = {}\n    PtAu = {}; PtOnly = {}; AvgCoPt = {}; GyrationPt = {}; Gyration = {}\n    return (edelta, comspace, cna_sigs, com, comdist, \n            surf_atoms, comAu, comPt, hoadjAu, hoadjPt, comdistAu, \n            comdistPt, midcomdistAu, midcomdistPt, surf_atomsPt, \n            headj, mix, nn, PtAu, PtOnly, AvgCoPt, Gyration, GyrationPt)", "fn_id": 4, "class_fn": false, "repo": "JonesRobM/SAPPHIRE", "file": "main/Sapphire/Graphing/Read_Plot.py", "last_update_at": "2022-03-30T12:34:41+00:00", "question_id": "c9e0eae1777a2c1aae0ba0fda714b5777f97799c_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def Init():\n    \"\"\"\n   \n\n    Returns\n    -------\n    edelta : TYPE\n        DESCRIPTION.\n    comspace : TYPE\n        DESCRIPTION.\n    cna_sigs : TYPE\n        DESCRIPTION.\n    adj : TYPE\n        DESCRIPTION.\n    agcn : TYPE\n        DESCRIPTION.\n    com : TYPE\n        DESCRIPTION.\n    comdist : TYPE\n        DESCRIPTION.\n    surf_atoms : TYPE\n        DESCRIPTION.\n    comAu : TYPE\n        DESCRIPTION.\n    comPt : TYPE\n        DESCRIPTION.\n    hoadjAu : TYPE\n        DESCRIPTION.\n    hoadjPt : TYPE\n        DESCRIPTION.\n    comdistAu : TYPE\n        DESCRIPTION.\n    comdistPt : TYPE\n        DESCRIPTION.\n    midcomdistAu : TYPE\n        DESCRIPTION.\n    midcomdistPt : TYPE\n        DESCRIPTION.\n    surf_atomsPt : TYPE\n        DESCRIPTION.\n    headj : TYPE\n        DESCRIPTION.\n    mix : TYPE\n        DESCRIPTION.\n\n    \"\"\"\n    edelta = {}\n    comspace = {}\n    cna_sigs = {}\n    com = {}\n    comdist = {}\n    surf_atoms = {}\n    comAu = {}\n    comPt = {}\n    hoadjAu = {}\n    hoadjPt = {}\n    comdistAu = {}\n    comdistPt = {}\n    midcomdistPt = {}\n    nn = {}\n    midcomdistAu = {}\n    surf_atomsPt = {}\n    headj = {}\n    mix = {}\n    PtAu = {}\n    PtOnly = {}\n    AvgCoPt = {}\n    GyrationPt = {}\n    Gyration = {}\n"]]}
{"hexsha": "7e4f941d449ba5ee070ac7ec9133e403949f6fe6", "ext": "py", "lang": "Python", "content": "@mock.patch('pyecoregen.cli.EcoreGenerator')\ndef test__generate_from_cli(generator_mock, cwd_module_dir):\n    mock_generator = generator_mock()\n    mock_generator.generate = mock.MagicMock()\n\n    generate_from_cli(['-e', 'input/library.ecore', '-o', 'some/folder'])\n\n    # look at arguments of generate call:\n    mock_generate = generator_mock().generate\n    model = mock_generator.generate.call_args[0][0]\n    path = mock_generator.generate.call_args[0][1]\n\n    assert isinstance(model, pyecore.ecore.EPackage)\n    assert model.name == 'library'\n    assert path == 'some/folder'", "fn_id": 0, "class_fn": false, "repo": "bmjjr/pyecoregen", "file": "tests/test_cli.py", "last_update_at": "2022-03-09T17:22:34+00:00", "question_id": "7e4f941d449ba5ee070ac7ec9133e403949f6fe6_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@mock.patch('pyecoregen.cli.EcoreGenerator')\ndef test__generate_from_cli(generator_mock, cwd_module_dir):\n    mock_generator = generator_mock()\n    mock_generator.generate = mock.MagicMock()\n    generate_from_cli(['-e', 'input/library.ecore', '-o', 'some/folder'])\n    mock_generate = generator_mock().generate\n    model = mock_generator.generate.call_args[0][0]\n    path = mock_generator.generate.call_args[0][1]\n    assert isinstance(model, pyecore.ecore.EPackage)\n    assert model.name == 'library'\n"]]}
{"hexsha": "45eecb320480f8f962379b706f6ae923faad9d9b", "ext": "py", "lang": "Python", "content": "def make_positive(datalist):\n    MIN = 1\n    m = min(datalist)\n    if m>=MIN:\n        shift = 0\n    else:\n        shift = MIN-m\n    shifted_data = [d+shift for d in datalist]\n    return shift,shifted_data", "fn_id": 1, "class_fn": false, "repo": "chrisc20042001/sDNA", "file": "arcscripts/sdnaregutilities.py", "last_update_at": "2022-03-21T11:30:32+00:00", "question_id": "45eecb320480f8f962379b706f6ae923faad9d9b_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def make_positive(datalist):\n    MIN = 1\n    m = min(datalist)\n    if m >= MIN:\n        shift = 0\n    else:\n        shift = MIN - m\n    shifted_data = [d + shift for d in datalist]\n"]]}
{"hexsha": "b8ac2fe73d1edfb866a8f3034f337623ee0df724", "ext": "py", "lang": "Python", "content": "def test_pairwise_dist():\n    emb_anc = tf.random.uniform((8,64))\n    emb_pos = tf.random.uniform((32,64))\n    emb_anc = tf.math.l2_normalize(emb_anc, axis=1)\n    emb_pos = tf.math.l2_normalize(emb_pos, axis=1)\n    \n    loss_obj = OnlineTripletLoss(bsz=40, n_anchor=8, n_pos_per_anchor=4, use_anc_as_pos=True)\n    dist1 = loss_obj._pairwise_distances_v2(emb_anc, emb_pos, use_anc_as_pos=True, squared=True) \n    dist2 = 2 * (1 - loss_obj._pairwise_dotprod(emb_anc, emb_pos, use_anc_as_pos=True))\n    dist3 = loss_obj._pairwise_distances_v2_fast(emb_anc, emb_pos, use_anc_as_pos=True, squared=True)  \n    \n    assert(tf.reduce_sum(dist1-dist2) < 0.0000001)\n    assert(tf.reduce_sum(dist1-dist3) < 0.0000001)\n    return       ", "fn_id": 1, "class_fn": false, "repo": "serkef/neural-audio-fp", "file": "model/fp/online_triplet_loss.py", "last_update_at": "2022-03-24T04:04:18+00:00", "question_id": "b8ac2fe73d1edfb866a8f3034f337623ee0df724_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_pairwise_dist():\n    emb_anc = tf.random.uniform((8, 64))\n    emb_pos = tf.random.uniform((32, 64))\n    emb_anc = tf.math.l2_normalize(emb_anc, axis=1)\n    emb_pos = tf.math.l2_normalize(emb_pos, axis=1)\n    loss_obj = OnlineTripletLoss(bsz=40, n_anchor=8, n_pos_per_anchor=4, use_anc_as_pos=True)\n    dist1 = loss_obj._pairwise_distances_v2(emb_anc, emb_pos, use_anc_as_pos=True, squared=True)\n    dist2 = 2 * (1 - loss_obj._pairwise_dotprod(emb_anc, emb_pos, use_anc_as_pos=True))\n    dist3 = loss_obj._pairwise_distances_v2_fast(emb_anc, emb_pos, use_anc_as_pos=True, squared=True)\n    assert tf.reduce_sum(dist1 - dist2) < 1e-07\n    assert tf.reduce_sum(dist1 - dist3) < 1e-07\n"]]}
{"hexsha": "a0c89e48a9b7570d3b3706f3bab5a0fcb21e6941", "ext": "py", "lang": "Python", "content": "def pad(tensor, paddings, mode='CONSTANT', constant_values=0):\n    \"\"\"\n    Pads a tensor.\n\n    Parameters\n    ----------\n    tensor : tensor\n        A Tensor.\n    paddings : tensor\n        A Tensor of type int32.\n    mode : str\n        One of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\" (case-insensitive)\n    constant_values : int\n        In \"CONSTANT\" mode, the scalar pad value to use. Must be same type as tensor.\n\n    Returns\n    -------\n        A Tensor. Has the same type as tensor.\n    \"\"\"\n    pad_obj = Pad(paddings, mode, constant_values=constant_values)\n    return pad_obj(tensor)", "fn_id": 21, "class_fn": false, "repo": "tensorlayer/TensorLayerX", "file": "tensorlayerx/backend/ops/torch_backend.py", "last_update_at": "2022-03-13T08:34:34+00:00", "question_id": "a0c89e48a9b7570d3b3706f3bab5a0fcb21e6941_21", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def pad(tensor, paddings, mode='CONSTANT', constant_values=0):\n    \"\"\"\n    Pads a tensor.\n\n    Parameters\n    ----------\n    tensor : tensor\n        A Tensor.\n    paddings : tensor\n        A Tensor of type int32.\n    mode : str\n        One of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\" (case-insensitive)\n    constant_values : int\n        In \"CONSTANT\" mode, the scalar pad value to use. Must be same type as tensor.\n\n    Returns\n    -------\n        A Tensor. Has the same type as tensor.\n    \"\"\"\n    pad_obj = Pad(paddings, mode, constant_values=constant_values)\n"]]}
{"hexsha": "07fb98a321250e0bee2d22c3cdc80755cc9567f4", "ext": "py", "lang": "Python", "content": "def train(opt):\n    print(\"Training model with the following parameters:\")\n    print(\"\\t number of stages: {}\".format(opt.train_stages))\n    print(\"\\t number of concurrently trained stages: {}\".format(opt.train_depth))\n    print(\"\\t learning rate scaling: {}\".format(opt.lr_scale))\n    print(\"\\t non-linearity: {}\".format(opt.activation))\n\n    real = functions.read_image(opt)\n    real = functions.adjust_scales2image(real, opt)\n    reals = functions.create_reals_pyramid(real, opt)\n    print(\"Training on image pyramid: {}\".format([r.shape for r in reals]))\n    print(\"\")\n\n    if opt.naive_img != \"\":\n        naive_img = functions.read_image_dir(opt.naive_img, opt)\n        naive_img_large = imresize_to_shape(naive_img, reals[-1].shape[2:], opt)\n        naive_img = imresize_to_shape(naive_img, reals[0].shape[2:], opt)\n        naive_img = functions.convert_image_np(naive_img)*255.0\n    else:\n        naive_img = None\n        naive_img_large = None\n\n    if opt.fine_tune:\n        img_to_augment = naive_img\n    else:\n        img_to_augment = functions.convert_image_np(reals[0])*255.0\n\n    if opt.train_mode == \"editing\":\n        opt.noise_scaling = 0.1\n\n    generator = init_G(opt)\n    if opt.fine_tune:\n        for _ in range(opt.train_stages-1):\n            generator.init_next_stage()\n        generator.load_state_dict(torch.load('{}/{}/netG.pth'.format(opt.model_dir, opt.train_stages-1),\n                                             map_location=\"cuda:{}\".format(torch.cuda.current_device())))\n\n\n    fixed_noise = []\n    noise_amp = []\n\n    for scale_num in range(opt.start_scale, opt.train_stages):\n        opt.out_ = functions.generate_dir2save(opt)\n        opt.outf = '%s/%d' % (opt.out_,scale_num)\n        try:\n            os.makedirs(opt.outf)\n        except OSError:\n                print(OSError)\n                pass\n        functions.save_image('{}/real_scale.jpg'.format(opt.outf), reals[scale_num])\n\n        d_curr = init_D(opt)\n        if opt.fine_tune:\n            d_curr.load_state_dict(torch.load('{}/{}/netD.pth'.format(opt.model_dir, opt.train_stages-1),\n                                              map_location=\"cuda:{}\".format(torch.cuda.current_device())))\n        elif scale_num > 0:\n            d_curr.load_state_dict(torch.load('%s/%d/netD.pth' % (opt.out_, scale_num - 1)))\n            generator.init_next_stage()\n\n        writer = SummaryWriter(log_dir=opt.outf)\n        fixed_noise, noise_amp, generator, d_curr = train_single_scale(d_curr, generator, reals, img_to_augment,\n                                                                       naive_img, naive_img_large, fixed_noise,\n                                                                       noise_amp, opt, scale_num, writer)\n\n        torch.save(fixed_noise, '%s/fixed_noise.pth' % (opt.out_))\n        torch.save(generator, '%s/G.pth' % (opt.out_))\n        torch.save(reals, '%s/reals.pth' % (opt.out_))\n        torch.save(noise_amp, '%s/noise_amp.pth' % (opt.out_))\n        del d_curr\n    writer.close()\n    return", "fn_id": 0, "class_fn": false, "repo": "tboen1/ConSinGAN", "file": "ConSinGAN/training_harmonization_editing.py", "last_update_at": "2022-03-29T05:27:48+00:00", "question_id": "07fb98a321250e0bee2d22c3cdc80755cc9567f4_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def train(opt):\n    print('Training model with the following parameters:')\n    print('\\t number of stages: {}'.format(opt.train_stages))\n    print('\\t number of concurrently trained stages: {}'.format(opt.train_depth))\n    print('\\t learning rate scaling: {}'.format(opt.lr_scale))\n    print('\\t non-linearity: {}'.format(opt.activation))\n    real = functions.read_image(opt)\n    real = functions.adjust_scales2image(real, opt)\n    reals = functions.create_reals_pyramid(real, opt)\n    print('Training on image pyramid: {}'.format([r.shape for r in reals]))\n    print('')\n    if opt.naive_img != '':\n        naive_img = functions.read_image_dir(opt.naive_img, opt)\n        naive_img_large = imresize_to_shape(naive_img, reals[-1].shape[2:], opt)\n        naive_img = imresize_to_shape(naive_img, reals[0].shape[2:], opt)\n        naive_img = functions.convert_image_np(naive_img) * 255.0\n    else:\n        naive_img = None\n        naive_img_large = None\n    if opt.fine_tune:\n        img_to_augment = naive_img\n    else:\n        img_to_augment = functions.convert_image_np(reals[0]) * 255.0\n    if opt.train_mode == 'editing':\n        opt.noise_scaling = 0.1\n    generator = init_G(opt)\n    if opt.fine_tune:\n        for _ in range(opt.train_stages - 1):\n            generator.init_next_stage()\n        generator.load_state_dict(torch.load('{}/{}/netG.pth'.format(opt.model_dir, opt.train_stages - 1), map_location='cuda:{}'.format(torch.cuda.current_device())))\n    fixed_noise = []\n    noise_amp = []\n    for scale_num in range(opt.start_scale, opt.train_stages):\n        opt.out_ = functions.generate_dir2save(opt)\n        opt.outf = '%s/%d' % (opt.out_, scale_num)\n        try:\n            os.makedirs(opt.outf)\n        except OSError:\n            print(OSError)\n            pass\n        functions.save_image('{}/real_scale.jpg'.format(opt.outf), reals[scale_num])\n        d_curr = init_D(opt)\n        if opt.fine_tune:\n            d_curr.load_state_dict(torch.load('{}/{}/netD.pth'.format(opt.model_dir, opt.train_stages - 1), map_location='cuda:{}'.format(torch.cuda.current_device())))\n        elif scale_num > 0:\n            d_curr.load_state_dict(torch.load('%s/%d/netD.pth' % (opt.out_, scale_num - 1)))\n            generator.init_next_stage()\n        writer = SummaryWriter(log_dir=opt.outf)\n        fixed_noise, noise_amp, generator, d_curr = train_single_scale(d_curr, generator, reals, img_to_augment, naive_img, naive_img_large, fixed_noise, noise_amp, opt, scale_num, writer)\n        torch.save(fixed_noise, '%s/fixed_noise.pth' % opt.out_)\n        torch.save(generator, '%s/G.pth' % opt.out_)\n        torch.save(reals, '%s/reals.pth' % opt.out_)\n        torch.save(noise_amp, '%s/noise_amp.pth' % opt.out_)\n        del d_curr\n    writer.close()\n"]]}
{"hexsha": "65fba4fa18d8eb795a7a89ab2d5dde09b9ca051b", "ext": "py", "lang": "Python", "content": "def main():\n    if len(sys.argv) == 1:\n        raise ValueError(\"Specify csv files to include as command line arguments.\")\n\n    csvs = []\n    for path in sys.argv[1:]:\n        p = pathlib.Path(path)\n        if p.is_dir():\n            csvs.extend(p.glob(\"*.csv\"))\n        else:\n            csvs.append(p)\n\n    all_data = MultiStats.from_recorded_data(*csvs)\n\n    fig = plot_detection_fraction(all_data)\n    fig.set_size_inches(15, 5)\n    fig.savefig(OUT_DIR / \"grouped_detection_fractions.png\", bbox_inches='tight', dpi=200)\n\n    plt.show()", "fn_id": 0, "class_fn": false, "repo": "Strilanc/honeycomb-boundaries", "file": "src/hcb/artifacts/make_grouped_detection_fraction_plot.py", "last_update_at": "2022-03-23T21:09:04+00:00", "question_id": "65fba4fa18d8eb795a7a89ab2d5dde09b9ca051b_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    if len(sys.argv) == 1:\n        raise ValueError('Specify csv files to include as command line arguments.')\n    csvs = []\n    for path in sys.argv[1:]:\n        p = pathlib.Path(path)\n        if p.is_dir():\n            csvs.extend(p.glob('*.csv'))\n        else:\n            csvs.append(p)\n    all_data = MultiStats.from_recorded_data(*csvs)\n    fig = plot_detection_fraction(all_data)\n    fig.set_size_inches(15, 5)\n    fig.savefig(OUT_DIR / 'grouped_detection_fractions.png', bbox_inches='tight', dpi=200)\n"]]}
{"hexsha": "802d3137f1544c142a8c64ecc7a5d9eb56025a11", "ext": "py", "lang": "Python", "content": "def rsync(ctx, *args, **kwargs):  # type: ignore\n    \"\"\"Ugly workaround for https://github.com/fabric/patchwork/issues/16.\"\"\"\n    ssh_agent = os.environ.get('SSH_AUTH_SOCK', None)\n    if ssh_agent:\n        ctx.config['run']['env']['SSH_AUTH_SOCK'] = ssh_agent\n    return rsync_(ctx, *args, **kwargs)", "fn_id": 1, "class_fn": false, "repo": "exhuma/powonline-frontend", "file": "tasks.py", "last_update_at": "2022-02-26T02:36:04+00:00", "question_id": "802d3137f1544c142a8c64ecc7a5d9eb56025a11_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def rsync(ctx, *args, **kwargs):\n    \"\"\"Ugly workaround for https://github.com/fabric/patchwork/issues/16.\"\"\"\n    ssh_agent = os.environ.get('SSH_AUTH_SOCK', None)\n    if ssh_agent:\n        ctx.config['run']['env']['SSH_AUTH_SOCK'] = ssh_agent\n"]]}
{"hexsha": "4a5ff1646cdadb2db61f0ae2c348e0c1114c7fb2", "ext": "py", "lang": "Python", "content": "def euler_from_quaternion(x, y, z, w):\n        \n     t3 = +2.0 * (w * z + x * y)\n     t4 = +1.0 - 2.0 * (y * y + z * z)\n     yaw_z = math.atan2(t3, t4)\n     \n     return yaw_z # in radians", "fn_id": 0, "class_fn": false, "repo": "HusseinLezzaik/Consensus-Algorithm-for-2-Mobile-Robots", "file": "Real Topology  Graph/GNN Model 4/Fully Connected Graph/test_n2_robot3.py", "last_update_at": "2022-02-18T02:25:29+00:00", "question_id": "4a5ff1646cdadb2db61f0ae2c348e0c1114c7fb2_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def euler_from_quaternion(x, y, z, w):\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    yaw_z = math.atan2(t3, t4)\n"]]}
{"hexsha": "c8baad290f4cb949388236761be605e642ed2d6f", "ext": "py", "lang": "Python", "content": "def delete_job(room_lifetime):\n    \"\"\"\n    A background job (runs every hour) that deletes expired rooms\n    :param room_lifetime: room expiry (in seconds)\n    \"\"\"\n    for room in os.listdir('r'):\n        if time.time() - os.path.getmtime(f'r{os.path.sep}{room}') > room_lifetime:\n            os.remove(f'r{os.path.sep}{room}')", "fn_id": 0, "class_fn": false, "repo": "Sh3B0/pft", "file": "server.py", "last_update_at": "2022-01-30T22:40:40+00:00", "question_id": "c8baad290f4cb949388236761be605e642ed2d6f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def delete_job(room_lifetime):\n    \"\"\"\n    A background job (runs every hour) that deletes expired rooms\n    :param room_lifetime: room expiry (in seconds)\n    \"\"\"\n    for room in os.listdir('r'):\n        if time.time() - os.path.getmtime(f'r{os.path.sep}{room}') > room_lifetime:\n"]]}
{"hexsha": "afe9e99123973660bf5b7382f0c67bbc34cc5637", "ext": "py", "lang": "Python", "content": "def _parse_model(mod, item=None):\n    if isinstance(mod, str):\n        dfs = Dfs0(mod)\n        if (len(dfs.items) > 1) and (item is None):\n            raise ValueError(\"Model ambiguous - please provide item\")\n        mod = dfs.read(items=item).to_dataframe()\n    elif isinstance(mod, pd.DataFrame):\n        mod = DataFramePointModelResultItem(mod, item=item).df\n    elif isinstance(mod, pd.Series):\n        mod = mod.to_frame()\n    elif isinstance(mod, DfsModelResultItem):\n        if not mod.is_dfs0:\n            raise ValueError(\"Only dfs0 ModelResults are supported\")\n        mod = mod._extract_point_dfs0(mod.item).to_dataframe()\n    elif isinstance(mod, DfsModelResult):\n        if not mod.is_dfs0:\n            raise ValueError(\"Only dfs0 ModelResults are supported\")\n        if mod.item is None:\n            raise ValueError(\"Model ambiguous - please provide item\")\n        mod = mod._extract_point_dfs0(mod.item).to_dataframe()\n\n    assert mod.shape[1] == 1  # A single item\n\n    mod.columns = [\"Model\"]\n\n    return mod", "fn_id": 1, "class_fn": false, "repo": "caichac-dhi/fmskill", "file": "fmskill/connection.py", "last_update_at": "2022-01-11T12:28:53+00:00", "question_id": "afe9e99123973660bf5b7382f0c67bbc34cc5637_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _parse_model(mod, item=None):\n    if isinstance(mod, str):\n        dfs = Dfs0(mod)\n        if len(dfs.items) > 1 and item is None:\n            raise ValueError('Model ambiguous - please provide item')\n        mod = dfs.read(items=item).to_dataframe()\n    elif isinstance(mod, pd.DataFrame):\n        mod = DataFramePointModelResultItem(mod, item=item).df\n    elif isinstance(mod, pd.Series):\n        mod = mod.to_frame()\n    elif isinstance(mod, DfsModelResultItem):\n        if not mod.is_dfs0:\n            raise ValueError('Only dfs0 ModelResults are supported')\n        mod = mod._extract_point_dfs0(mod.item).to_dataframe()\n    elif isinstance(mod, DfsModelResult):\n        if not mod.is_dfs0:\n            raise ValueError('Only dfs0 ModelResults are supported')\n        if mod.item is None:\n            raise ValueError('Model ambiguous - please provide item')\n        mod = mod._extract_point_dfs0(mod.item).to_dataframe()\n    assert mod.shape[1] == 1\n    mod.columns = ['Model']\n"]]}
{"hexsha": "21d09c9139681e7abbce341d69f350ac83fe9e2e", "ext": "py", "lang": "Python", "content": "def infer_ros(input_cloud, model, corr2soft):\n    query = load_pc_infer(input_cloud)\n    query = np.array(query, dtype=np.float32)\n\n    out, _, _, _ = infer_model(model, corr2soft, query)\n    out_show = out.reshape(1, 32, 32) # size is related to the disco dimension\n    return out", "fn_id": 0, "class_fn": false, "repo": "MaverickPeter/DiSCO-pytorch", "file": "infer_ros.py", "last_update_at": "2022-03-31T04:57:06+00:00", "question_id": "21d09c9139681e7abbce341d69f350ac83fe9e2e_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def infer_ros(input_cloud, model, corr2soft):\n    query = load_pc_infer(input_cloud)\n    query = np.array(query, dtype=np.float32)\n    out, _, _, _ = infer_model(model, corr2soft, query)\n    out_show = out.reshape(1, 32, 32)\n"]]}
{"hexsha": "97bc33429e8de10e703c38622e297136a8a6870b", "ext": "py", "lang": "Python", "content": "def test_validate_pairs(default_conf, mocker):  # test exchange.validate_pairs directly\n    api_mock = MagicMock()\n    type(api_mock).load_markets = MagicMock(return_value={\n        'ETH/BTC': {'quote': 'BTC'},\n        'LTC/BTC': {'quote': 'BTC'},\n        'XRP/BTC': {'quote': 'BTC'},\n        'NEO/BTC': {'quote': 'BTC'},\n    })\n    id_mock = PropertyMock(return_value='test_exchange')\n    type(api_mock).id = id_mock\n\n    mocker.patch('freqtrade.exchange.Exchange._init_ccxt', MagicMock(return_value=api_mock))\n    mocker.patch('freqtrade.exchange.Exchange.validate_timeframes')\n    mocker.patch('freqtrade.exchange.Exchange._load_async_markets')\n    mocker.patch('freqtrade.exchange.Exchange.validate_stakecurrency')\n    Exchange(default_conf)", "fn_id": 24, "class_fn": false, "repo": "Fractate/freqbot", "file": "tests/exchange/test_exchange.py", "last_update_at": "2022-03-06T22:44:30+00:00", "question_id": "97bc33429e8de10e703c38622e297136a8a6870b_24", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_validate_pairs(default_conf, mocker):\n    api_mock = MagicMock()\n    type(api_mock).load_markets = MagicMock(return_value={'ETH/BTC': {'quote': 'BTC'}, 'LTC/BTC': {'quote': 'BTC'}, 'XRP/BTC': {'quote': 'BTC'}, 'NEO/BTC': {'quote': 'BTC'}})\n    id_mock = PropertyMock(return_value='test_exchange')\n    type(api_mock).id = id_mock\n    mocker.patch('freqtrade.exchange.Exchange._init_ccxt', MagicMock(return_value=api_mock))\n    mocker.patch('freqtrade.exchange.Exchange.validate_timeframes')\n    mocker.patch('freqtrade.exchange.Exchange._load_async_markets')\n    mocker.patch('freqtrade.exchange.Exchange.validate_stakecurrency')\n"]]}
{"hexsha": "af38d43d09853349aa74f4954f9df7f77f9f8183", "ext": "py", "lang": "Python", "content": "def cached(key, timeout=3600):\n    \"\"\"Cache the return value of the decorated function with the given key.\n\n    Key can be a String or a function.\n    If key is a function, it must have the same arguments as the decorated function,\n    otherwise it cannot be called successfully.\n    \"\"\"\n\n    def decorator(f):\n        @wraps(f)\n        def wrapped(*args, **kwargs):\n            cache = get_cache()\n            # Check if key is a function\n            if callable(key):\n                cache_key = key(*args, **kwargs)\n            else:\n                cache_key = key\n            # Try to get the value from cache\n            cached_val = cache.get(cache_key)\n            if cached_val is None:\n                # Call the original function and cache the result\n                cached_val = f(*args, **kwargs)\n                cache.set(cache_key, cached_val, timeout)\n            return cached_val\n\n        return wrapped\n\n    return decorator", "fn_id": 0, "class_fn": false, "repo": "kohtoa15/zubbi", "file": "zubbi/extensions.py", "last_update_at": "2022-01-20T08:50:56+00:00", "question_id": "af38d43d09853349aa74f4954f9df7f77f9f8183_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def cached(key, timeout=3600):\n    \"\"\"Cache the return value of the decorated function with the given key.\n\n    Key can be a String or a function.\n    If key is a function, it must have the same arguments as the decorated function,\n    otherwise it cannot be called successfully.\n    \"\"\"\n\n    def decorator(f):\n\n        @wraps(f)\n        def wrapped(*args, **kwargs):\n            cache = get_cache()\n            if callable(key):\n                cache_key = key(*args, **kwargs)\n            else:\n                cache_key = key\n            cached_val = cache.get(cache_key)\n            if cached_val is None:\n                cached_val = f(*args, **kwargs)\n                cache.set(cache_key, cached_val, timeout)\n            return cached_val\n        return wrapped\n"]]}
{"hexsha": "ae17e648fb3d9947d40e78d6b219151b1ed85fab", "ext": "py", "lang": "Python", "content": "@app.callback(\n\t[Output('AlphasGraph', 'figure'),Output(\"AlphasAlphasDataSummary\",\"children\")],\n\t[Input('AlphasData','children')],\n\t[State('AlphasRawData','children'), State('AlphasDirect','children'), State(\"AlphasSigNum\",\"value\")],\n\t)\ndef update_graph(jsonified_dataAlphas, jsonified_data, AlphaDir, sig):\t\n\t'''\n    Update Alpha Raw graph\n\n            Parameters:\n                    jsonified_dataAlphas (json): Jsonified Alpha data \n\t\t\t\t\tjsonified_data (json): Jsonified data\n\t\t\t\t\tAlphaDir (str): Alpha data directory\n\t\t\t\t\tsig (float): Error scaling factor\n\n            Returns:\n\t\t\t\t\tFig (dcc.Graph) : Figure element \n\t\t\t\t\tJSON (json) : jsonified summary data for display\n                    \n    '''\n\tif jsonified_data not in [0,\"0\", None, \"None\"] and jsonified_dataAlphas not in [0,\"0\", None, \"None\"]:\n\t\t#Un json data\n\t\tdata = pd.read_json(jsonified_data, orient='split')\n\t\tdata.index = data.index.tz_localize(None)\n\t\tData_i_Name = data.columns[0]\n\t\t# Statistics of interest\n\t\tWindowParameters = CalcStats.CalcStats(data[Data_i_Name])\n\t\tAlphasdf = pd.read_json(jsonified_dataAlphas, orient='split')\n\t\tAlphasdf.index = Alphasdf.index.tz_localize(None) \t\t\n\t\tFig = DashPlots.CreateAlphasFig(Alphasdf, WindowParameters, sig, FigHeightPX/2, Data_i_Name)\n\t\n\t\tif SaveFigs == 1:\n\t\t\tXRange = [data.index[0],data.index[-1]]\n\t\t\tNameDate =  \"_%s_%s\" % (XRange[0].strftime(\"%d%m%Y\"),XRange[1].strftime(\"%d%m%Y\"))\n\t\t\tp1 = Process(target=Graphing.plotMATPLOTLIBAlpha, args=[data,Alphasdf,XRange,[None,None],\"cachefiles\"+os.sep+\"Alpha\",Data_i_Name+NameDate,True])\n\t\t\tp1.start()\n\t\t\tp1.join()\n\n\t\treturn Fig, json.dumps(CalcStats.CalcAlphaStats(Alphasdf[Data_i_Name]))\n\telse:\n\t\treturn DashPlots.EmptyFig(FigHeightPX/2), json.dumps(CalcStats.CalcAlphaStatsEmpty())", "fn_id": 33, "class_fn": false, "repo": "sephwalker321/DashAnomalyDetection", "file": "app.py", "last_update_at": "2022-01-09T13:45:59+00:00", "question_id": "ae17e648fb3d9947d40e78d6b219151b1ed85fab_33", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.callback([Output('AlphasGraph', 'figure'), Output('AlphasAlphasDataSummary', 'children')], [Input('AlphasData', 'children')], [State('AlphasRawData', 'children'), State('AlphasDirect', 'children'), State('AlphasSigNum', 'value')])\ndef update_graph(jsonified_dataAlphas, jsonified_data, AlphaDir, sig):\n    \"\"\"\n    Update Alpha Raw graph\n\n            Parameters:\n                    jsonified_dataAlphas (json): Jsonified Alpha data \n\t\t\t\t\tjsonified_data (json): Jsonified data\n\t\t\t\t\tAlphaDir (str): Alpha data directory\n\t\t\t\t\tsig (float): Error scaling factor\n\n            Returns:\n\t\t\t\t\tFig (dcc.Graph) : Figure element \n\t\t\t\t\tJSON (json) : jsonified summary data for display\n                    \n    \"\"\"\n    if jsonified_data not in [0, '0', None, 'None'] and jsonified_dataAlphas not in [0, '0', None, 'None']:\n        data = pd.read_json(jsonified_data, orient='split')\n        data.index = data.index.tz_localize(None)\n        Data_i_Name = data.columns[0]\n        WindowParameters = CalcStats.CalcStats(data[Data_i_Name])\n        Alphasdf = pd.read_json(jsonified_dataAlphas, orient='split')\n        Alphasdf.index = Alphasdf.index.tz_localize(None)\n        Fig = DashPlots.CreateAlphasFig(Alphasdf, WindowParameters, sig, FigHeightPX / 2, Data_i_Name)\n        if SaveFigs == 1:\n            XRange = [data.index[0], data.index[-1]]\n            NameDate = '_%s_%s' % (XRange[0].strftime('%d%m%Y'), XRange[1].strftime('%d%m%Y'))\n            p1 = Process(target=Graphing.plotMATPLOTLIBAlpha, args=[data, Alphasdf, XRange, [None, None], 'cachefiles' + os.sep + 'Alpha', Data_i_Name + NameDate, True])\n            p1.start()\n            p1.join()\n        return (Fig, json.dumps(CalcStats.CalcAlphaStats(Alphasdf[Data_i_Name])))\n    else:\n"]]}
{"hexsha": "738741668e4fc286aa3d5766559b5f3eb6297a80", "ext": "py", "lang": "Python", "content": "def assert_poly_almost_equal(p1, p2, msg=\"\"):\n    try:\n        assert_(np.all(p1.domain == p2.domain))\n        assert_(np.all(p1.window == p2.window))\n        assert_almost_equal(p1.coef, p2.coef)\n    except AssertionError:\n        msg = \"Result: %s\\nTarget: %s\", (p1, p2)\n        raise AssertionError(msg)", "fn_id": 0, "class_fn": false, "repo": "Seifar/ChitAnalysis", "file": "venv/lib/python3.6/site-packages/numpy/polynomial/tests/test_classes.py", "last_update_at": "2022-02-25T11:48:18+00:00", "question_id": "738741668e4fc286aa3d5766559b5f3eb6297a80_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def assert_poly_almost_equal(p1, p2, msg=''):\n    try:\n        assert_(np.all(p1.domain == p2.domain))\n        assert_(np.all(p1.window == p2.window))\n        assert_almost_equal(p1.coef, p2.coef)\n    except AssertionError:\n        msg = ('Result: %s\\nTarget: %s', (p1, p2))\n"]]}
{"hexsha": "462b3496b8740b44a6ccafdec9fa5da42fa91f0d", "ext": "py", "lang": "Python", "content": "def is_version_in_range(v, min_version, max_version):\n    fmin = flatten_version(min_version)\n    fmax = flatten_version(max_version)\n    f = flatten_version(v)\n    #print(\"testing \", v, \", as \", f)\n    return f >= fmin and f <= fmax", "fn_id": 3, "class_fn": false, "repo": "Zylann/gdscript_performance", "file": "run.py", "last_update_at": "2022-02-06T23:31:41+00:00", "question_id": "462b3496b8740b44a6ccafdec9fa5da42fa91f0d_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def is_version_in_range(v, min_version, max_version):\n    fmin = flatten_version(min_version)\n    fmax = flatten_version(max_version)\n    f = flatten_version(v)\n"]]}
{"hexsha": "5377d2b13a0b95a52bd8837d251fe769cd9ece23", "ext": "py", "lang": "Python", "content": "def bc_python_premise(rule, arg_patterns, arg_context):\n  engine = rule.rule_base.engine\n  patterns = rule.goal_arg_patterns()\n  if len(arg_patterns) == len(patterns):\n    context = contexts.bc_context(rule)\n    try:\n      if all(map(lambda pat, arg:\n                   pat.match_pattern(context, context,\n                                     arg, arg_context),\n                 patterns,\n                 arg_patterns)):\n        rule.rule_base.num_bc_rules_matched += 1\n        mark1 = context.mark(True)\n        if rule.pattern(0).match_data(context, context,\n                context.lookup_data('clause_num') + 1):\n          context.end_save_all_undo()\n          with engine.prove(rule.rule_base.root_name, 'python_premise', context,\n                            (rule.pattern(1),\n                             rule.pattern(2),\n                             rule.pattern(3),\n                             rule.pattern(4),\n                             rule.pattern(5),\n                             rule.pattern(6),\n                             rule.pattern(7),)) \\\n            as gen_2:\n            for x_2 in gen_2:\n              assert x_2 is None, \\\n                \"compiler.bc_python_premise: got unexpected plan from when clause 2\"\n              rule.rule_base.num_bc_rule_successes += 1\n              yield\n        else: context.end_save_all_undo()\n        context.undo_to_mark(mark1)\n        rule.rule_base.num_bc_rule_failures += 1\n    finally:\n      context.done()", "fn_id": 36, "class_fn": false, "repo": "alimon/pyke3", "file": "pyke/krb_compiler/compiler_bc.py", "last_update_at": "2022-02-09T20:13:17+00:00", "question_id": "5377d2b13a0b95a52bd8837d251fe769cd9ece23_36", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def bc_python_premise(rule, arg_patterns, arg_context):\n    engine = rule.rule_base.engine\n    patterns = rule.goal_arg_patterns()\n    if len(arg_patterns) == len(patterns):\n        context = contexts.bc_context(rule)\n        try:\n            if all(map(lambda pat, arg: pat.match_pattern(context, context, arg, arg_context), patterns, arg_patterns)):\n                rule.rule_base.num_bc_rules_matched += 1\n                mark1 = context.mark(True)\n                if rule.pattern(0).match_data(context, context, context.lookup_data('clause_num') + 1):\n                    context.end_save_all_undo()\n                    with engine.prove(rule.rule_base.root_name, 'python_premise', context, (rule.pattern(1), rule.pattern(2), rule.pattern(3), rule.pattern(4), rule.pattern(5), rule.pattern(6), rule.pattern(7))) as gen_2:\n                        for x_2 in gen_2:\n                            assert x_2 is None, 'compiler.bc_python_premise: got unexpected plan from when clause 2'\n                            rule.rule_base.num_bc_rule_successes += 1\n                            yield\n                else:\n                    context.end_save_all_undo()\n                context.undo_to_mark(mark1)\n                rule.rule_base.num_bc_rule_failures += 1\n        finally:\n"]]}
{"hexsha": "90fef6f12ab0bafddd9352b19305a81838bbe3af", "ext": "py", "lang": "Python", "content": "def setsUsed():\n    low_estimate = {\n        'non_crit_care': 6,\n        'crit_care': 12,\n        'crit_care_vent': 12\n        }\n    \n    high_estimate = {\n        'non_crit_care': 30,\n        'crit_care': 50,\n        'crit_care_vent': 50,\n        }\n    \n    mean_estimate = {}\n    for key in low_estimate:\n        mean_estimate[key] = (low_estimate[key] + high_estimate[key])/2\n    \n    output = {\n        'low_estimate': low_estimate,\n        'mean_estimate': mean_estimate,\n        'high_estimate': high_estimate\n        }\n    return(output)", "fn_id": 11, "class_fn": false, "repo": "c19hcc/c19-modeling-tools", "file": "code/short_model_v2.py", "last_update_at": "2022-03-09T17:56:32+00:00", "question_id": "90fef6f12ab0bafddd9352b19305a81838bbe3af_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def setsUsed():\n    low_estimate = {'non_crit_care': 6, 'crit_care': 12, 'crit_care_vent': 12}\n    high_estimate = {'non_crit_care': 30, 'crit_care': 50, 'crit_care_vent': 50}\n    mean_estimate = {}\n    for key in low_estimate:\n        mean_estimate[key] = (low_estimate[key] + high_estimate[key]) / 2\n    output = {'low_estimate': low_estimate, 'mean_estimate': mean_estimate, 'high_estimate': high_estimate}\n"]]}
{"hexsha": "45108f55a094d4a955636f59156d6d792cd2f0df", "ext": "py", "lang": "Python", "content": "def _find_thread_stack(thread_id):\n    \"\"\"Returns a stack object that can be used to dump a stack trace for\n    the given thread id (or None if the id is not found).\n    \"\"\"\n    for tid, stack in sys._current_frames().items():\n        if tid == thread_id:\n            return stack\n    return None", "fn_id": 1, "class_fn": false, "repo": "zipated/src", "file": "third_party/blink/tools/blinkpy/common/system/stack_utils.py", "last_update_at": "2022-03-31T08:39:18+00:00", "question_id": "45108f55a094d4a955636f59156d6d792cd2f0df_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _find_thread_stack(thread_id):\n    \"\"\"Returns a stack object that can be used to dump a stack trace for\n    the given thread id (or None if the id is not found).\n    \"\"\"\n    for tid, stack in sys._current_frames().items():\n        if tid == thread_id:\n            return stack\n"]]}
{"hexsha": "b29bc073733fd123dee5e41aad50f80c19be76ea", "ext": "py", "lang": "Python", "content": "def get_base_renderers(dataset_name: str,\n                       label: str = 'color',\n                       property_label: str = 'shape'\n                       ) -> ml_collections.ConfigDict:\n  \"\"\"Get base config for the given dataset, label and property value.\"\"\"\n  data = get_data_config(dataset_name, label, property_label)\n  data.train_kwargs.load_kwargs.filter_fns = 'True'\n  data.train_kwargs.load_kwargs.num_samples = '0'\n  data.train_kwargs.load_kwargs.weights = [1.]\n  return data", "fn_id": 9, "class_fn": false, "repo": "deepmind/distribution_shift_framework", "file": "distribution_shift_framework/configs/disentanglement_config.py", "last_update_at": "2022-03-22T11:55:57+00:00", "question_id": "b29bc073733fd123dee5e41aad50f80c19be76ea_9", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_base_renderers(dataset_name: str, label: str='color', property_label: str='shape') -> ml_collections.ConfigDict:\n    \"\"\"Get base config for the given dataset, label and property value.\"\"\"\n    data = get_data_config(dataset_name, label, property_label)\n    data.train_kwargs.load_kwargs.filter_fns = 'True'\n    data.train_kwargs.load_kwargs.num_samples = '0'\n    data.train_kwargs.load_kwargs.weights = [1.0]\n"]]}
{"hexsha": "4905fd63189ce58f29c2f6fd68b1a3742beff180", "ext": "py", "lang": "Python", "content": "def test_full_write_access(path):\n    try:\n        testdir = os.path.join(path, \"test_write_access\")\n        os.mkdir(testdir)\n        os.rmdir(testdir)\n        return True\n    except PermissionError:\n        # There is no access to create folders in path:\n        return False", "fn_id": 3, "class_fn": false, "repo": "da3dsoul/superpaper", "file": "superpaper/sp_paths.py", "last_update_at": "2022-03-29T21:20:37+00:00", "question_id": "4905fd63189ce58f29c2f6fd68b1a3742beff180_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_full_write_access(path):\n    try:\n        testdir = os.path.join(path, 'test_write_access')\n        os.mkdir(testdir)\n        os.rmdir(testdir)\n        return True\n    except PermissionError:\n"]]}
{"hexsha": "c48dff6a289dd41de899395a7cc61ec7422f5c44", "ext": "py", "lang": "Python", "content": "@command('generate-config', 'license_key [output_file]',\n\"\"\"Generates a sample agent configuration file for <license_key>.\"\"\")\ndef generate_config(args):\n    import os\n    import sys\n\n    if len(args) == 0:\n        usage('generate-config')\n        sys.exit(1)\n\n    from newrelic import __file__ as package_root\n    package_root = os.path.dirname(package_root)\n\n    config_file = os.path.join(package_root, 'newrelic.ini')\n\n    content = open(config_file, 'r').read()\n\n    if len(args) >= 1:\n        content = content.replace('*** REPLACE ME ***', args[0])\n\n    if len(args) >= 2 and args[1] != '-':\n        output_file = open(args[1], 'w')\n        output_file.write(content)\n        output_file.close()\n    else:\n        print(content)", "fn_id": 0, "class_fn": false, "repo": "newrelic/newrelic-python-agen", "file": "newrelic/admin/generate_config.py", "last_update_at": "2022-03-01T11:13:21+00:00", "question_id": "c48dff6a289dd41de899395a7cc61ec7422f5c44_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@command('generate-config', 'license_key [output_file]', 'Generates a sample agent configuration file for <license_key>.')\ndef generate_config(args):\n    import os\n    import sys\n    if len(args) == 0:\n        usage('generate-config')\n        sys.exit(1)\n    from newrelic import __file__ as package_root\n    package_root = os.path.dirname(package_root)\n    config_file = os.path.join(package_root, 'newrelic.ini')\n    content = open(config_file, 'r').read()\n    if len(args) >= 1:\n        content = content.replace('*** REPLACE ME ***', args[0])\n    if len(args) >= 2 and args[1] != '-':\n        output_file = open(args[1], 'w')\n        output_file.write(content)\n        output_file.close()\n    else:\n"]]}
{"hexsha": "ac89e0990ef502ce2032982496fc9481e356bb33", "ext": "py", "lang": "Python", "content": "@pytest.mark.django_db(transaction=True)\ndef test_author_insert_notify_in_transaction(pg_connection):\n    with atomic():\n        author = Author.objects.create(name='Billy')\n    pg_connection.poll()\n    assert 1 == len(pg_connection.notifies)\n    assert not Post.objects.exists()\n    process_notifications(pg_connection)\n    assert 1 == Post.objects.count()\n    post = Post.objects.last()\n    assert post.author == author", "fn_id": 6, "class_fn": false, "repo": "Opus10/django-pgpubsub", "file": "pgpubsub/tests/test_core.py", "last_update_at": "2022-01-18T07:18:40+00:00", "question_id": "ac89e0990ef502ce2032982496fc9481e356bb33_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.django_db(transaction=True)\ndef test_author_insert_notify_in_transaction(pg_connection):\n    with atomic():\n        author = Author.objects.create(name='Billy')\n    pg_connection.poll()\n    assert 1 == len(pg_connection.notifies)\n    assert not Post.objects.exists()\n    process_notifications(pg_connection)\n    assert 1 == Post.objects.count()\n    post = Post.objects.last()\n"]]}
{"hexsha": "fee5495bb5c9ee33876aa22c6a6e792cb935d8d8", "ext": "py", "lang": "Python", "content": "def handle_csar(path, parsed_params):\n    \"\"\"Handles CSAR (multi-file) ADTs and returns any errors caught\n    :params: path, parsed_params\n    :type: string, dictionary\n    :return: template\n\n    | parsed_params: dictionary containing the input to change\n    | path: local or remote path to the file to parse\n    \"\"\"\n    errors = csar_validation(path, parsed_params)\n    if errors:\n        raise MultiError(errors, \"Cannot parse CSAR, issues in templates...\")\n        \n    template = parser.get_template(path, parsed_params)\n\n    template.nodetemplates = get_concrete_nodes(template)\n\n    return template", "fn_id": 0, "class_fn": false, "repo": "micado-scale/micado-parser", "file": "micadoparser/utils/csar.py", "last_update_at": "2022-01-25T12:54:04+00:00", "question_id": "fee5495bb5c9ee33876aa22c6a6e792cb935d8d8_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def handle_csar(path, parsed_params):\n    \"\"\"Handles CSAR (multi-file) ADTs and returns any errors caught\n    :params: path, parsed_params\n    :type: string, dictionary\n    :return: template\n\n    | parsed_params: dictionary containing the input to change\n    | path: local or remote path to the file to parse\n    \"\"\"\n    errors = csar_validation(path, parsed_params)\n    if errors:\n        raise MultiError(errors, 'Cannot parse CSAR, issues in templates...')\n    template = parser.get_template(path, parsed_params)\n    template.nodetemplates = get_concrete_nodes(template)\n"]]}
{"hexsha": "226880a1a12ca4045e6a898d3a66784484b667fd", "ext": "py", "lang": "Python", "content": "def load_args():\n    parser = argparse.ArgumentParser(description = \"Pipeline runner\")\n    \n    parser.add_argument(\n        'target_repo',\n        action = 'store',\n        help = \"the type of object to generate jsons for\",\n    )\n\n    parser.add_argument(\n        'problem_type',\n        action = 'store',\n        help = 'the d3m problem type to check'\n    )\n    # parser.add_argument(\n    #     'primitives_or_pipelines',\n    #     action = 'store',\n    #     help = \"the type of object to generate jsons for\",\n    # )\n    arguments = parser.parse_args()\n    return [arguments.target_repo, arguments.problem_type]", "fn_id": 0, "class_fn": false, "repo": "remram44/primitives-interfaces", "file": "build/lib/jhu_primitives/utils/util.py", "last_update_at": "2022-03-01T01:49:11+00:00", "question_id": "226880a1a12ca4045e6a898d3a66784484b667fd_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_args():\n    parser = argparse.ArgumentParser(description='Pipeline runner')\n    parser.add_argument('target_repo', action='store', help='the type of object to generate jsons for')\n    parser.add_argument('problem_type', action='store', help='the d3m problem type to check')\n    arguments = parser.parse_args()\n"]]}
{"hexsha": "ba9a89a20a5246b3dd730505a5077464e9b4eaae", "ext": "py", "lang": "Python", "content": "def save_all_datasets(args):\n    # logger.info('*' * 100)\n    # logger.info('Pre-training dataset')\n    # _ = init_dataset(args=args,\n    #                  mode=enums.TRAINING_MODE_PRE_TRAIN,\n    #                  load_if_saved=False)\n    # summarization\n    for lang in [enums.LANG_JAVA, enums.LANG_GO, enums.LANG_PHP, enums.LANG_PYTHON, enums.LANG_RUBY,\n                 enums.LANG_JAVASCRIPT]:\n        for split in ['train', 'valid', 'test']:\n            logger.info('*' * 100)\n            logger.info(f'Summarization - {lang} - {split}')\n            _ = init_dataset(args=args,\n                             mode=enums.TRAINING_MODE_FINE_TUNE,\n                             task=enums.TASK_SUMMARIZATION,\n                             language=lang,\n                             split=split,\n                             load_if_saved=False)", "fn_id": 2, "class_fn": false, "repo": "NougatCA/SPT-Code", "file": "sources/data/dataset.py", "last_update_at": "2022-03-26T03:42:40+00:00", "question_id": "ba9a89a20a5246b3dd730505a5077464e9b4eaae_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def save_all_datasets(args):\n    for lang in [enums.LANG_JAVA, enums.LANG_GO, enums.LANG_PHP, enums.LANG_PYTHON, enums.LANG_RUBY, enums.LANG_JAVASCRIPT]:\n        for split in ['train', 'valid', 'test']:\n            logger.info('*' * 100)\n            logger.info(f'Summarization - {lang} - {split}')\n"]]}
{"hexsha": "a723b9d16a072991826384f423eea3831e428ea6", "ext": "py", "lang": "Python", "content": "def delete_toolbox(self):\n    \"\"\"OptiGenAlgNsga2Deap method to delete DEAP toolbox\n    Parameters\n    ----------\n    self : OptiGenAlgNsga2Deap\n    \"\"\"\n\n    # Delete toolbox\n    self.toolbox = None\n\n    # Delete creator classes\n    del creator.FitnessMin\n    del creator.Individual", "fn_id": 0, "class_fn": false, "repo": "IrakozeFD/pyleecan", "file": "pyleecan/Methods/Optimization/OptiGenAlgNsga2Deap/delete_toolbox.py", "last_update_at": "2022-03-17T18:22:10+00:00", "question_id": "a723b9d16a072991826384f423eea3831e428ea6_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def delete_toolbox(self):\n    \"\"\"OptiGenAlgNsga2Deap method to delete DEAP toolbox\n    Parameters\n    ----------\n    self : OptiGenAlgNsga2Deap\n    \"\"\"\n    self.toolbox = None\n    del creator.FitnessMin\n"]]}
{"hexsha": "607414f84dd65ce953b25da5fd2f17868e0ddf93", "ext": "py", "lang": "Python", "content": "def post_indexing_handler(program_enrollments):\n    \"\"\"\n    Do the work which happens after a profile is reindexed\n\n    Args:\n        program_enrollments (list of ProgramEnrollment): A list of ProgramEnrollments\n    \"\"\"\n    feature_sync_user = settings.FEATURES.get('OPEN_DISCUSSIONS_USER_SYNC', False)\n\n    if not feature_sync_user:\n        log.debug('OPEN_DISCUSSIONS_USER_SYNC is set to False (so disabled) in the settings')\n\n    _refresh_all_default_indices()\n    for program_enrollment in program_enrollments:\n        try:\n            _send_automatic_emails(program_enrollment)\n        except:  # pylint: disable=bare-except\n            log.exception(\"Error sending automatic email for enrollment %s\", program_enrollment)\n\n        # only update for discussion queries for now\n        try:\n            _update_percolate_memberships(program_enrollment.user, PercolateQuery.DISCUSSION_CHANNEL_TYPE)\n        except:  # pylint: disable=bare-except\n            log.exception(\"Error syncing %s to channels\", program_enrollment.user)", "fn_id": 0, "class_fn": false, "repo": "Wassaf-Shahzad/micromasters", "file": "search/tasks.py", "last_update_at": "2022-01-15T19:35:42+00:00", "question_id": "607414f84dd65ce953b25da5fd2f17868e0ddf93_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def post_indexing_handler(program_enrollments):\n    \"\"\"\n    Do the work which happens after a profile is reindexed\n\n    Args:\n        program_enrollments (list of ProgramEnrollment): A list of ProgramEnrollments\n    \"\"\"\n    feature_sync_user = settings.FEATURES.get('OPEN_DISCUSSIONS_USER_SYNC', False)\n    if not feature_sync_user:\n        log.debug('OPEN_DISCUSSIONS_USER_SYNC is set to False (so disabled) in the settings')\n    _refresh_all_default_indices()\n    for program_enrollment in program_enrollments:\n        try:\n            _send_automatic_emails(program_enrollment)\n        except:\n            log.exception('Error sending automatic email for enrollment %s', program_enrollment)\n        try:\n            _update_percolate_memberships(program_enrollment.user, PercolateQuery.DISCUSSION_CHANNEL_TYPE)\n        except:\n"]]}
{"hexsha": "43efbd89d4e9b634fa74a76111bf547e2652f26f", "ext": "py", "lang": "Python", "content": "def OLS_prepare_data(df, model, metric='pagerank', grouped=False):\n    data = df.query(\"kind == @model & metric == @metric\").copy()\n    if grouped:\n        data = data.groupby(['fm','hmm','hMM','kind','metric']).mean().reset_index()\n    #data.loc[:,'pw'] = data.apply(lambda row: int(row.metric=='pagerank'), axis=1)\n    #data.loc[:,'hMM-hmm'] = data.apply(lambda row: row.hMM-row.hmm, axis=1)\n    #data.loc[:,'abs(hMM-hmm)'] = data.apply(lambda row: abs(row.hMM-row.hmm), axis=1)\n    data.loc[:,'Intercept'] = 1\n    return data", "fn_id": 18, "class_fn": false, "repo": "gesiscss/Homophilic_Directed_ScaleFree_Networks", "file": "org/gesis/lib/paper.py", "last_update_at": "2022-03-23T15:34:38+00:00", "question_id": "43efbd89d4e9b634fa74a76111bf547e2652f26f_18", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def OLS_prepare_data(df, model, metric='pagerank', grouped=False):\n    data = df.query('kind == @model & metric == @metric').copy()\n    if grouped:\n        data = data.groupby(['fm', 'hmm', 'hMM', 'kind', 'metric']).mean().reset_index()\n    data.loc[:, 'Intercept'] = 1\n"]]}
{"hexsha": "0293e80c48b061bbfb371c95ece0b2a540942771", "ext": "py", "lang": "Python", "content": "def getSSIM(X, Y):\n    \"\"\"\n       Computes the mean structural similarity between two images.\n    \"\"\"\n    assert (X.shape == Y.shape), \"Image-patche provided have different dimensions\"\n    nch = 1 if X.ndim==2 else X.shape[-1]\n    mssim = []\n    for ch in xrange(nch):\n        Xc, Yc = X[...,ch].astype(np.float64), Y[...,ch].astype(np.float64)\n        mssim.append(compute_ssim(Xc, Yc))\n    return np.mean(mssim)", "fn_id": 0, "class_fn": false, "repo": "edgecm/FUnIE-GAN-1", "file": "Evaluation/imqual_utils.py", "last_update_at": "2022-03-29T03:07:34+00:00", "question_id": "0293e80c48b061bbfb371c95ece0b2a540942771_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def getSSIM(X, Y):\n    \"\"\"\n       Computes the mean structural similarity between two images.\n    \"\"\"\n    assert X.shape == Y.shape, 'Image-patche provided have different dimensions'\n    nch = 1 if X.ndim == 2 else X.shape[-1]\n    mssim = []\n    for ch in xrange(nch):\n        Xc, Yc = (X[..., ch].astype(np.float64), Y[..., ch].astype(np.float64))\n        mssim.append(compute_ssim(Xc, Yc))\n"]]}
{"hexsha": "7fe0c12974fbed692742cf84672db6512533e42f", "ext": "py", "lang": "Python", "content": "def to_a1_range(start_row_index: int, start_col_index: int, end_row_index: int, end_col_index: int) -> str:\n    \"\"\"\\\n    \u6307\u5b9a\u3057\u305f2\u30bb\u30eb\u3092\u59cb\u307e\u308a\u306e\u30bb\u30eb\u3001\u7d42\u308f\u308a\u306e\u30bb\u30eb\u3068\u3057\u305f\u7bc4\u56f2\u3092 \"A1:B2\" \u306e\u3088\u3046\u306a\u5f62\u5f0f\u3067\u8fd4\u3059\u3002\n    (end_row_index, end_col_index) \u3067\u6307\u5b9a\u3055\u308c\u308b\u30bb\u30eb\u3092\u542b\u3080\n    \"\"\"\n    return \"{}:{}\".format(to_a1_cell(start_row_index, start_col_index),\n                          to_a1_cell(end_row_index, end_col_index))", "fn_id": 1, "class_fn": false, "repo": "dmiyakawa/sswrap", "file": "sswrap/common.py", "last_update_at": "2022-03-16T01:11:31+00:00", "question_id": "7fe0c12974fbed692742cf84672db6512533e42f_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def to_a1_range(start_row_index: int, start_col_index: int, end_row_index: int, end_col_index: int) -> str:\n    \"\"\"    \u6307\u5b9a\u3057\u305f2\u30bb\u30eb\u3092\u59cb\u307e\u308a\u306e\u30bb\u30eb\u3001\u7d42\u308f\u308a\u306e\u30bb\u30eb\u3068\u3057\u305f\u7bc4\u56f2\u3092 \"A1:B2\" \u306e\u3088\u3046\u306a\u5f62\u5f0f\u3067\u8fd4\u3059\u3002\n    (end_row_index, end_col_index) \u3067\u6307\u5b9a\u3055\u308c\u308b\u30bb\u30eb\u3092\u542b\u3080\n    \"\"\"\n"]]}
{"hexsha": "598b6316452b8a7d926bbe53385d206a9cdc900d", "ext": "py", "lang": "Python", "content": "def initialize(model: nn.Layer, init: str):\n    \"\"\"Initialize weights of a neural network module.\n\n    Parameters are initialized using the given method or distribution.\n\n    Custom initialization routines can be implemented into submodules\n\n    Args:\n        model (nn.Layer): Target.\n        init (str): Method of initialization.\n    \"\"\"\n    assert check_argument_types()\n\n    if init == \"xavier_uniform\":\n        nn.initializer.set_global_initializer(nn.initializer.XavierUniform(),\n                                              nn.initializer.Constant())\n    elif init == \"xavier_normal\":\n        nn.initializer.set_global_initializer(nn.initializer.XavierNormal(),\n                                              nn.initializer.Constant())\n    elif init == \"kaiming_uniform\":\n        nn.initializer.set_global_initializer(nn.initializer.KaimingUniform(),\n                                              nn.initializer.Constant())\n    elif init == \"kaiming_normal\":\n        nn.initializer.set_global_initializer(nn.initializer.KaimingNormal(),\n                                              nn.initializer.Constant())\n    else:\n        raise ValueError(\"Unknown initialization: \" + init)", "fn_id": 3, "class_fn": false, "repo": "SmileGoat/PaddleSpeech", "file": "paddlespeech/t2s/modules/nets_utils.py", "last_update_at": "2022-02-10T09:30:00+00:00", "question_id": "598b6316452b8a7d926bbe53385d206a9cdc900d_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def initialize(model: nn.Layer, init: str):\n    \"\"\"Initialize weights of a neural network module.\n\n    Parameters are initialized using the given method or distribution.\n\n    Custom initialization routines can be implemented into submodules\n\n    Args:\n        model (nn.Layer): Target.\n        init (str): Method of initialization.\n    \"\"\"\n    assert check_argument_types()\n    if init == 'xavier_uniform':\n        nn.initializer.set_global_initializer(nn.initializer.XavierUniform(), nn.initializer.Constant())\n    elif init == 'xavier_normal':\n        nn.initializer.set_global_initializer(nn.initializer.XavierNormal(), nn.initializer.Constant())\n    elif init == 'kaiming_uniform':\n        nn.initializer.set_global_initializer(nn.initializer.KaimingUniform(), nn.initializer.Constant())\n    elif init == 'kaiming_normal':\n        nn.initializer.set_global_initializer(nn.initializer.KaimingNormal(), nn.initializer.Constant())\n    else:\n"]]}
{"hexsha": "33069b4b218765c48b7eae2dc4eb5eefaa9db130", "ext": "py", "lang": "Python", "content": "def test_RealtimeProvider_free_bus_1(server):\n    provider = Provider.from_context(server)\n    seconds = time.time()\n    with server.osc_protocol.capture() as transcript:\n        with provider.at(seconds):\n            audio_bus = provider.add_bus(calculation_rate=CalculationRate.AUDIO)\n            control_bus_a = provider.add_bus()\n            control_bus_b = provider.add_bus()\n            control_bus_c = provider.add_bus()\n        with provider.at(seconds + 0.01):\n            audio_bus.free()\n            control_bus_a.free()\n            control_bus_d = provider.add_bus()\n    assert audio_bus.identifier == 16\n    assert control_bus_a.identifier == 0\n    assert control_bus_b.identifier == 1\n    assert control_bus_c.identifier == 2\n    assert control_bus_d.identifier == 0\n    assert [entry.message for entry in transcript] == []", "fn_id": 13, "class_fn": false, "repo": "josiah-wolf-oberholtzer/supriya", "file": "tests/providers/test_RealtimeProvider.py", "last_update_at": "2022-03-29T10:26:44+00:00", "question_id": "33069b4b218765c48b7eae2dc4eb5eefaa9db130_13", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_RealtimeProvider_free_bus_1(server):\n    provider = Provider.from_context(server)\n    seconds = time.time()\n    with server.osc_protocol.capture() as transcript:\n        with provider.at(seconds):\n            audio_bus = provider.add_bus(calculation_rate=CalculationRate.AUDIO)\n            control_bus_a = provider.add_bus()\n            control_bus_b = provider.add_bus()\n            control_bus_c = provider.add_bus()\n        with provider.at(seconds + 0.01):\n            audio_bus.free()\n            control_bus_a.free()\n            control_bus_d = provider.add_bus()\n    assert audio_bus.identifier == 16\n    assert control_bus_a.identifier == 0\n    assert control_bus_b.identifier == 1\n    assert control_bus_c.identifier == 2\n    assert control_bus_d.identifier == 0\n"]]}
{"hexsha": "28bb563d4e77c050ad414be9642fbea72b74da7d", "ext": "py", "lang": "Python", "content": "def ConvergenceKTest(CorrectA, J, Laplacian, k_vals, n):\n    # Make space for the result to be saved\n    Values = np.empty(len(k_vals))\n    \n    # Loop through all values of n\n    for i, k in enumerate(k_vals):\n        # Create the initial conditions\n        A = np.zeros_like(J)\n    \n        print(i)\n        # Solve the system\n        A = EM.solve_approx(J, Laplacian, 1, A, n, k)\n        \n        # Find the error\n        Values[i] = np.sum((A - CorrectA) ** 2)\n        \n    return Values", "fn_id": 2, "class_fn": false, "repo": "RasmusBruhn/EMSim", "file": "OldCode/Findk.py", "last_update_at": "2022-03-21T14:25:10+00:00", "question_id": "28bb563d4e77c050ad414be9642fbea72b74da7d_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ConvergenceKTest(CorrectA, J, Laplacian, k_vals, n):\n    Values = np.empty(len(k_vals))\n    for i, k in enumerate(k_vals):\n        A = np.zeros_like(J)\n        print(i)\n        A = EM.solve_approx(J, Laplacian, 1, A, n, k)\n        Values[i] = np.sum((A - CorrectA) ** 2)\n"]]}
{"hexsha": "d0872b6e45a853008bfc2cecec2b19fb4751243d", "ext": "py", "lang": "Python", "content": "def _grid(centroid, radius, step):\n    \"\"\"\n    Generates a list of points that will be used as probes to test coordination.\n\n    A spherical grid of equiseparated points (i.e. probes) is constructed aimed\n    to contain the whole biological system (or a zone of it in case of user\n    request). The `centroid` and `radius` parameters of the method are obtained\n    either when parsing the input `.pdb` file (grid containing the whole\n    molecule) or entered by the user with `--center` and `--radius` parameters\n    of the program (only this zone of the molecule will be explored). The `step`\n    parameter determines the distance between points of the grid.\n\n    Parameters\n    ----------\n    centroid : array_like\n        Array of 3 floats defining the center of the sphere\n    radius : float\n        Radius of the sphere used to construct the grid\n    step : float\n        Distance, in Angstroms, between two consecutive probes\n\n    Returns\n    -------\n    list of array_like\n        list of 3-float arrays containing the points (i.e. probes) of the grid\n    \"\"\"\n    #1. Two points at the ends of a cube of l=2*radius are obtained\n    xmi, ymi, zmi = centroid - radius\n    xma, yma, zma = centroid + radius\n\n    #2. Each axis x, y and z is splitted at every step distance\n    numx = int((xma - xmi) / step) + 1\n    numy = int((yma - ymi) / step) + 1\n    numz = int((zma - zmi) / step) + 1\n    x = np.linspace(xmi, xma, numx)\n    y = np.linspace(ymi, yma, numy)\n    z = np.linspace(zmi, zma, numz)\n\n    #3. A cubic grid is generated by iterating over the 3 axes\n    grid = []\n    for i in range(len(x)):\n        for j in range(len(y)):\n            for k in range(len(z)):\n                grid.append([x[i], y[j], z[k]])\n    points = np.array(grid) #Cube embedding the protein\n\n    #4. Points out of the sphere are discarded\n    is_in_sphere = np.linalg.norm(points - centroid, axis=1) <= radius\n    points = points[is_in_sphere, :]\n\n    return points", "fn_id": 0, "class_fn": false, "repo": "josan82/biometall", "file": "biometall/modules/grid.py", "last_update_at": "2022-02-21T15:58:55+00:00", "question_id": "d0872b6e45a853008bfc2cecec2b19fb4751243d_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _grid(centroid, radius, step):\n    \"\"\"\n    Generates a list of points that will be used as probes to test coordination.\n\n    A spherical grid of equiseparated points (i.e. probes) is constructed aimed\n    to contain the whole biological system (or a zone of it in case of user\n    request). The `centroid` and `radius` parameters of the method are obtained\n    either when parsing the input `.pdb` file (grid containing the whole\n    molecule) or entered by the user with `--center` and `--radius` parameters\n    of the program (only this zone of the molecule will be explored). The `step`\n    parameter determines the distance between points of the grid.\n\n    Parameters\n    ----------\n    centroid : array_like\n        Array of 3 floats defining the center of the sphere\n    radius : float\n        Radius of the sphere used to construct the grid\n    step : float\n        Distance, in Angstroms, between two consecutive probes\n\n    Returns\n    -------\n    list of array_like\n        list of 3-float arrays containing the points (i.e. probes) of the grid\n    \"\"\"\n    xmi, ymi, zmi = centroid - radius\n    xma, yma, zma = centroid + radius\n    numx = int((xma - xmi) / step) + 1\n    numy = int((yma - ymi) / step) + 1\n    numz = int((zma - zmi) / step) + 1\n    x = np.linspace(xmi, xma, numx)\n    y = np.linspace(ymi, yma, numy)\n    z = np.linspace(zmi, zma, numz)\n    grid = []\n    for i in range(len(x)):\n        for j in range(len(y)):\n            for k in range(len(z)):\n                grid.append([x[i], y[j], z[k]])\n    points = np.array(grid)\n    is_in_sphere = np.linalg.norm(points - centroid, axis=1) <= radius\n    points = points[is_in_sphere, :]\n"]]}
{"hexsha": "3458b57ed9315b3d9e28a0f1274cb6b63edb9b4d", "ext": "py", "lang": "Python", "content": "def lev_dist(first, second):\n    \"\"\"Find the Levenshtein distance between two strings.\"\"\"\n    if len(first) > len(second):\n        first, second = second, first\n    if len(second) == 0:\n        return len(first)\n    first_length = len(first) + 1\n    second_length = len(second) + 1\n    distance_matrix = [[0] * second_length for x in range(first_length)]\n    for i in range(first_length):\n       distance_matrix[i][0] = i\n    for j in range(second_length):\n       distance_matrix[0][j]=j\n    for i in xrange(1, first_length):\n        for j in range(1, second_length):\n            deletion = distance_matrix[i-1][j] + 1\n            insertion = distance_matrix[i][j-1] + 1\n            substitution = distance_matrix[i-1][j-1]\n            if first[i-1] != second[j-1]:\n                substitution += 1\n            distance_matrix[i][j] = min(insertion, deletion, substitution)\n    return distance_matrix[first_length-1][second_length-1]", "fn_id": 0, "class_fn": false, "repo": "sordonia/HierarchicalEncoderDecoder", "file": "baselines/LEV/lev_rerank.py", "last_update_at": "2022-02-03T10:26:41+00:00", "question_id": "3458b57ed9315b3d9e28a0f1274cb6b63edb9b4d_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def lev_dist(first, second):\n    \"\"\"Find the Levenshtein distance between two strings.\"\"\"\n    if len(first) > len(second):\n        first, second = (second, first)\n    if len(second) == 0:\n        return len(first)\n    first_length = len(first) + 1\n    second_length = len(second) + 1\n    distance_matrix = [[0] * second_length for x in range(first_length)]\n    for i in range(first_length):\n        distance_matrix[i][0] = i\n    for j in range(second_length):\n        distance_matrix[0][j] = j\n    for i in xrange(1, first_length):\n        for j in range(1, second_length):\n            deletion = distance_matrix[i - 1][j] + 1\n            insertion = distance_matrix[i][j - 1] + 1\n            substitution = distance_matrix[i - 1][j - 1]\n            if first[i - 1] != second[j - 1]:\n                substitution += 1\n            distance_matrix[i][j] = min(insertion, deletion, substitution)\n"]]}
{"hexsha": "f7d87fe1f5a2211e35a8b08f549a4c14b2b9f335", "ext": "py", "lang": "Python", "content": "def tune(device_name, strategy=\"bayes_opt_GPyTorch_lean\", strategy_options=None, verbose=True, quiet=False, simulation_mode=True):\n\n    #input dimensions and data\n    image_width = 4096\n    image_height = 4096\n    filter_width = 15\n    filter_height = 15\n    problem_size = (image_width, image_height)\n    size = numpy.prod(problem_size)\n\n    args = []\n\n    metrics = OrderedDict()\n    metrics[\"GFLOP/s\"] = lambda p: (image_width * image_height * filter_width * filter_height * 2 / 1e9) / (p[\"time\"] / 1e3)\n\n    #setup tunable parameters\n    tune_params = OrderedDict()\n    tune_params[\"filter_width\"] = [filter_width]\n    tune_params[\"filter_height\"] = [filter_height]\n    tune_params[\"block_size_x\"] = [1, 2, 4, 8, 16, 32, 48, 64, 80, 96, 112, 128]\n    tune_params[\"block_size_y\"] = [1, 2, 4, 8, 16, 32]\n    tune_params[\"tile_size_x\"] = [1, 2, 3, 4, 5, 6, 7, 8]\n    tune_params[\"tile_size_y\"] = [1, 2, 3, 4, 5, 6, 7, 8]\n    tune_params[\"use_padding\"] = [0, 1]\n    tune_params[\"read_only\"] = [0, 1]\n\n    restrict = [\"block_size_x*block_size_y>=64\", \"tile_size_x*tile_size_y<30\"]\n\n    grid_div_x = [\"block_size_x\", \"tile_size_x\"]\n    grid_div_y = [\"block_size_y\", \"tile_size_y\"]\n\n    #start tuning\n    results, env = kernel_tuner.tune_kernel(\"convolution_kernel\", \"convolution.cu\", problem_size, args, tune_params, grid_div_y=grid_div_y,\n                                            grid_div_x=grid_div_x, metrics=metrics, verbose=verbose, quiet=quiet, restrictions=restrict,\n                                            cache=\"cache_files/convolution_\" + device_name, strategy=strategy, strategy_options=strategy_options,\n                                            simulation_mode=simulation_mode)\n\n    # print(len(results))\n\n    return results, env", "fn_id": 0, "class_fn": false, "repo": "fjwillemsen/BayesianOptimization-autotuning", "file": "cached_data_used/convolution.py", "last_update_at": "2022-02-25T22:11:48+00:00", "question_id": "f7d87fe1f5a2211e35a8b08f549a4c14b2b9f335_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def tune(device_name, strategy='bayes_opt_GPyTorch_lean', strategy_options=None, verbose=True, quiet=False, simulation_mode=True):\n    image_width = 4096\n    image_height = 4096\n    filter_width = 15\n    filter_height = 15\n    problem_size = (image_width, image_height)\n    size = numpy.prod(problem_size)\n    args = []\n    metrics = OrderedDict()\n    metrics['GFLOP/s'] = lambda p: image_width * image_height * filter_width * filter_height * 2 / 1000000000.0 / (p['time'] / 1000.0)\n    tune_params = OrderedDict()\n    tune_params['filter_width'] = [filter_width]\n    tune_params['filter_height'] = [filter_height]\n    tune_params['block_size_x'] = [1, 2, 4, 8, 16, 32, 48, 64, 80, 96, 112, 128]\n    tune_params['block_size_y'] = [1, 2, 4, 8, 16, 32]\n    tune_params['tile_size_x'] = [1, 2, 3, 4, 5, 6, 7, 8]\n    tune_params['tile_size_y'] = [1, 2, 3, 4, 5, 6, 7, 8]\n    tune_params['use_padding'] = [0, 1]\n    tune_params['read_only'] = [0, 1]\n    restrict = ['block_size_x*block_size_y>=64', 'tile_size_x*tile_size_y<30']\n    grid_div_x = ['block_size_x', 'tile_size_x']\n    grid_div_y = ['block_size_y', 'tile_size_y']\n    results, env = kernel_tuner.tune_kernel('convolution_kernel', 'convolution.cu', problem_size, args, tune_params, grid_div_y=grid_div_y, grid_div_x=grid_div_x, metrics=metrics, verbose=verbose, quiet=quiet, restrictions=restrict, cache='cache_files/convolution_' + device_name, strategy=strategy, strategy_options=strategy_options, simulation_mode=simulation_mode)\n"]]}
{"hexsha": "76b0879b1d116786730e0b4b06b7b6527d94d5ec", "ext": "py", "lang": "Python", "content": "def test_dns_service(client, compose):\n    template = '''\n    web1:\n        image: nginx\n    web2:\n        image: nginx\n    web:\n        image: rancher/dns-service\n        links:\n        - web1\n        - web2\n    '''\n    project_name = create_project(compose, input=template)\n\n    project = find_one(client.list_stack, name=project_name)\n    services = project.services()\n\n    assert len(services) == 3\n\n    web = _get_service(services, 'web')\n\n    assert web.type == 'dnsService'\n    consumed = web.consumedservices()\n\n    assert len(consumed) == 2\n    names = {x.name for x in consumed}\n\n    assert names == {'web1', 'web2'}", "fn_id": 47, "class_fn": false, "repo": "gpinkham/rancher-compose", "file": "tests/integration/cattletest/core/test_compose.py", "last_update_at": "2022-03-24T10:07:06+00:00", "question_id": "76b0879b1d116786730e0b4b06b7b6527d94d5ec_47", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_dns_service(client, compose):\n    template = '\\n    web1:\\n        image: nginx\\n    web2:\\n        image: nginx\\n    web:\\n        image: rancher/dns-service\\n        links:\\n        - web1\\n        - web2\\n    '\n    project_name = create_project(compose, input=template)\n    project = find_one(client.list_stack, name=project_name)\n    services = project.services()\n    assert len(services) == 3\n    web = _get_service(services, 'web')\n    assert web.type == 'dnsService'\n    consumed = web.consumedservices()\n    assert len(consumed) == 2\n    names = {x.name for x in consumed}\n"]]}
{"hexsha": "a8b74ddcfc45ab2071f7d384beebc4e9c5b43638", "ext": "py", "lang": "Python", "content": "def test_fetch_incidents(requests_mock):\n    mock_data = load_mock_response('fetch_incidents_info.json')\n    fetch_incidents_response = mock_data.get('FETCH_INCIDENTS_RESPONSE', {})\n    fetch_incidents_params = mock_data.get('FETCH_INCIDENTS_PARAMS', {})\n    fetch_incidents_results = mock_data.get('FETCH_INCIDENTS_RESULTS', {})\n\n    api = integration.FETCH_INCIDENTS_API.format(\n        MONITOR_ID=MONITOR_ID,\n        API_KEY=API_KEY,\n        max_results=fetch_incidents_params.get('max_results', 0))\n\n    url = f'{integration.BASE_URL}/{api}'\n    requests_mock.post(url, json=fetch_incidents_response)\n    next_run, incidents = integration.fetch_incidents(\n        CLIENT,\n        max_results=fetch_incidents_params.get('max_results', 0),\n        last_run=fetch_incidents_params.get('last_run', {}),\n        first_fetch_time=fetch_incidents_params.get('first_fetch_time', 0),\n        incident_types=fetch_incidents_params.get('incident_types', []))\n\n    assert next_run == fetch_incidents_results.get('next_run')\n    assert len(incidents) == len(fetch_incidents_results.get('incidents', [])) == 1\n    assert isinstance(incidents, list) == isinstance(fetch_incidents_results.get('incidents', []), list)\n    assert incidents[0][\"name\"] == fetch_incidents_results.get('incidents', [])[0][\"name\"]\n    assert incidents[0][\"occurred\"] == fetch_incidents_results.get('incidents', [])[0][\"occurred\"]\n    assert json.loads(incidents[0][\"rawJSON\"]) == fetch_incidents_results.get('incidents', [])[0][\"rawJSON\"]", "fn_id": 1, "class_fn": false, "repo": "diCagri/content", "file": "Packs/KELARaDark/Integrations/RaDark/RaDark_test.py", "last_update_at": "2022-03-31T11:10:11+00:00", "question_id": "a8b74ddcfc45ab2071f7d384beebc4e9c5b43638_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_fetch_incidents(requests_mock):\n    mock_data = load_mock_response('fetch_incidents_info.json')\n    fetch_incidents_response = mock_data.get('FETCH_INCIDENTS_RESPONSE', {})\n    fetch_incidents_params = mock_data.get('FETCH_INCIDENTS_PARAMS', {})\n    fetch_incidents_results = mock_data.get('FETCH_INCIDENTS_RESULTS', {})\n    api = integration.FETCH_INCIDENTS_API.format(MONITOR_ID=MONITOR_ID, API_KEY=API_KEY, max_results=fetch_incidents_params.get('max_results', 0))\n    url = f'{integration.BASE_URL}/{api}'\n    requests_mock.post(url, json=fetch_incidents_response)\n    next_run, incidents = integration.fetch_incidents(CLIENT, max_results=fetch_incidents_params.get('max_results', 0), last_run=fetch_incidents_params.get('last_run', {}), first_fetch_time=fetch_incidents_params.get('first_fetch_time', 0), incident_types=fetch_incidents_params.get('incident_types', []))\n    assert next_run == fetch_incidents_results.get('next_run')\n    assert len(incidents) == len(fetch_incidents_results.get('incidents', [])) == 1\n    assert isinstance(incidents, list) == isinstance(fetch_incidents_results.get('incidents', []), list)\n    assert incidents[0]['name'] == fetch_incidents_results.get('incidents', [])[0]['name']\n    assert incidents[0]['occurred'] == fetch_incidents_results.get('incidents', [])[0]['occurred']\n"]]}
{"hexsha": "954dab58aee4ed757042fcd517d36b4da488ef9a", "ext": "py", "lang": "Python", "content": "def placebo_plot(g,placebo_groups,diff_data_0):\n    \"\"\"\n    Generates Figure 8: Observed treatment effect for Apulia and Basilicata and placebo units\n    \"\"\"\n    \n    diff_list = []\n    diff_list = Parallel(n_jobs=-1)(delayed(g)(pair) for pair in placebo_groups)\n\n    # Auxiliary\n    fig, axes = plt.subplots(1, 2,figsize=(13,4))\n    ax1 = axes[0]\n    ax2 = axes[1]\n    year = diff_data_0.index.values\n\n    for i in range(len(diff_list)):\n\n        ax1.plot(diff_list[i]['GDP Gap'],color='gray',label = 'Placebos' if i == 1 else \"\")\n        ax2.plot(diff_list[i]['Murder Gap'],color='gray',label = 'Placebos' if i == 1 else \"\")\n\n\n    ax1.plot(diff_data_0['GDP Gap'],color='black',label = 'Treated Region')\n    ax2.plot(diff_data_0['Murder Gap'],color='black',label = 'Treated Region')\n\n    ax1.set_xlabel('Year')\n    ax1.set_ylabel('GDP per capita, % Gap')\n    ax1.tick_params(axis='y')\n    ax1.set_ylim(-30,30)    \n    ax1.title.set_text('Fig 8(a) GDP per capita')\n    ax1.axhline(0)\n\n    ax1.set_xlabel('Year')\n    ax2.set_ylabel('Murder Rate, Difference')\n    ax2.tick_params(axis='y')\n    ax2.set_ylim(-4,4)\n    ax2.title.set_text('Fig 8(b) Murder Rate')\n    ax2.axhline(0)\n\n    ax1.axvspan(1975, 1980, color='y', alpha=0.5, lw=0,label='Mafia Outbreak')\n    ax2.axvspan(1975, 1980, color='y', alpha=0.5, lw=0,label='Mafia Outbreak')\n\n    ax1.legend(loc = 'upper center', bbox_to_anchor = (0.5, -0.22), shadow = True, ncol = 2)\n    ax2.legend(loc = 'upper center', bbox_to_anchor = (0.5, -0.22), shadow = True, ncol = 2)\n\n    plt.show()", "fn_id": 3, "class_fn": false, "repo": "CasualDan/ose-scientific-computing-course-jdx-mafia-1", "file": "auxiliary/section6_inference.py", "last_update_at": "2022-01-14T17:01:03+00:00", "question_id": "954dab58aee4ed757042fcd517d36b4da488ef9a_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def placebo_plot(g, placebo_groups, diff_data_0):\n    \"\"\"\n    Generates Figure 8: Observed treatment effect for Apulia and Basilicata and placebo units\n    \"\"\"\n    diff_list = []\n    diff_list = Parallel(n_jobs=-1)((delayed(g)(pair) for pair in placebo_groups))\n    fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n    ax1 = axes[0]\n    ax2 = axes[1]\n    year = diff_data_0.index.values\n    for i in range(len(diff_list)):\n        ax1.plot(diff_list[i]['GDP Gap'], color='gray', label='Placebos' if i == 1 else '')\n        ax2.plot(diff_list[i]['Murder Gap'], color='gray', label='Placebos' if i == 1 else '')\n    ax1.plot(diff_data_0['GDP Gap'], color='black', label='Treated Region')\n    ax2.plot(diff_data_0['Murder Gap'], color='black', label='Treated Region')\n    ax1.set_xlabel('Year')\n    ax1.set_ylabel('GDP per capita, % Gap')\n    ax1.tick_params(axis='y')\n    ax1.set_ylim(-30, 30)\n    ax1.title.set_text('Fig 8(a) GDP per capita')\n    ax1.axhline(0)\n    ax1.set_xlabel('Year')\n    ax2.set_ylabel('Murder Rate, Difference')\n    ax2.tick_params(axis='y')\n    ax2.set_ylim(-4, 4)\n    ax2.title.set_text('Fig 8(b) Murder Rate')\n    ax2.axhline(0)\n    ax1.axvspan(1975, 1980, color='y', alpha=0.5, lw=0, label='Mafia Outbreak')\n    ax2.axvspan(1975, 1980, color='y', alpha=0.5, lw=0, label='Mafia Outbreak')\n    ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.22), shadow=True, ncol=2)\n    ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.22), shadow=True, ncol=2)\n"]]}
{"hexsha": "dbf75775d7ccd7ae4436bc3c80eba63e41331ec2", "ext": "py", "lang": "Python", "content": "@public\ndef negative_start() -> range:\n    a = range(6)\n    return a[-6:5:2]", "fn_id": 1, "class_fn": false, "repo": "OnBlockIO/neo3-boa", "file": "boa3_test/test_sc/range_test/RangeSlicingWithStride.py", "last_update_at": "2022-03-08T03:23:55+00:00", "question_id": "dbf75775d7ccd7ae4436bc3c80eba63e41331ec2_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@public\ndef negative_start() -> range:\n    a = range(6)\n"]]}
{"hexsha": "f6d9ab3a6727f742bb4331e1814521fcb0ddcb0b", "ext": "py", "lang": "Python", "content": "def ecdf(data):\n\t\"\"\" Compute eCDF\n\tdata : List of values\n\tReturns:\n\tTuple of x and y values for eCDF plot\n\t\"\"\"\n\tx = np.sort(data)\n\tn = len(x)\n\ty = np.arange(n) / float(n)\n\n\n\treturn(x,y)", "fn_id": 5, "class_fn": false, "repo": "pachterlab/CBP_2021", "file": "scripts/tools.py", "last_update_at": "2022-03-04T20:37:06+00:00", "question_id": "f6d9ab3a6727f742bb4331e1814521fcb0ddcb0b_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ecdf(data):\n    \"\"\" Compute eCDF\n\tdata : List of values\n\tReturns:\n\tTuple of x and y values for eCDF plot\n\t\"\"\"\n    x = np.sort(data)\n    n = len(x)\n    y = np.arange(n) / float(n)\n"]]}
{"hexsha": "10b4ae8337b708cdd45772874ffd6a861ff36fa5", "ext": "py", "lang": "Python", "content": "def _prepare_cleanup(\n    training: MappedTriples,\n    testing: MappedTriples,\n    max_ids: Optional[Tuple[int, int]] = None,\n) -> torch.BoolTensor:\n    \"\"\"\n    Calculate a mask for the test triples with triples containing test-only entities or relations.\n\n    :param training: shape: (n, 3)\n        The training triples.\n    :param testing: shape: (m, 3)\n        The testing triples.\n\n    :return: shape: (m,)\n        The move mask.\n    \"\"\"\n    # base cases\n    if len(testing) == 0:\n        return torch.empty(0, dtype=torch.bool)\n    if len(training) == 0:\n        return torch.ones(testing.shape[0], dtype=torch.bool)\n\n    columns = [[0, 2], [1]]\n    to_move_mask = torch.zeros(1, dtype=torch.bool)\n    if max_ids is None:\n        max_ids = typing.cast(\n            Tuple[int, int],\n            tuple(max(training[:, col].max().item(), testing[:, col].max().item()) + 1 for col in columns),\n        )\n    for col, max_id in zip(columns, max_ids):\n        # IDs not in training\n        not_in_training_mask = torch.ones(max_id, dtype=torch.bool)\n        not_in_training_mask[training[:, col].view(-1)] = False\n\n        # triples with exclusive test IDs\n        exclusive_triples = not_in_training_mask[testing[:, col].view(-1)].view(-1, len(col)).any(dim=-1)\n        to_move_mask = to_move_mask | exclusive_triples\n    return to_move_mask", "fn_id": 7, "class_fn": false, "repo": "sbonner0/pykeen", "file": "src/pykeen/triples/splitting.py", "last_update_at": "2022-03-30T22:53:18+00:00", "question_id": "10b4ae8337b708cdd45772874ffd6a861ff36fa5_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _prepare_cleanup(training: MappedTriples, testing: MappedTriples, max_ids: Optional[Tuple[int, int]]=None) -> torch.BoolTensor:\n    \"\"\"\n    Calculate a mask for the test triples with triples containing test-only entities or relations.\n\n    :param training: shape: (n, 3)\n        The training triples.\n    :param testing: shape: (m, 3)\n        The testing triples.\n\n    :return: shape: (m,)\n        The move mask.\n    \"\"\"\n    if len(testing) == 0:\n        return torch.empty(0, dtype=torch.bool)\n    if len(training) == 0:\n        return torch.ones(testing.shape[0], dtype=torch.bool)\n    columns = [[0, 2], [1]]\n    to_move_mask = torch.zeros(1, dtype=torch.bool)\n    if max_ids is None:\n        max_ids = typing.cast(Tuple[int, int], tuple((max(training[:, col].max().item(), testing[:, col].max().item()) + 1 for col in columns)))\n    for col, max_id in zip(columns, max_ids):\n        not_in_training_mask = torch.ones(max_id, dtype=torch.bool)\n        not_in_training_mask[training[:, col].view(-1)] = False\n        exclusive_triples = not_in_training_mask[testing[:, col].view(-1)].view(-1, len(col)).any(dim=-1)\n        to_move_mask = to_move_mask | exclusive_triples\n"]]}
{"hexsha": "658b591953d33b3309b05337ed6f4ca65eca6141", "ext": "py", "lang": "Python", "content": "def mass_plot_io():\n    \n    m1 = np.sqrt(m0**2 + big_dm)\n    m2 = np.sqrt(m1**2 + small_dm)\n    \n    with quantity_support():\n\n        plt.loglog(m0, m0)\n        plt.loglog(m0, m1)\n        plt.loglog(m0, m2)\n        plt.loglog(m0, np.sqrt(.7 * m1**2 + .3 * m2**2 + .02 * m0**2))", "fn_id": 1, "class_fn": false, "repo": "jacopok/notes", "file": "phd_courses/theoretical_low_energy_astroparticle/figures/neutrino_mass_plots.py", "last_update_at": "2022-01-13T14:52:50+00:00", "question_id": "658b591953d33b3309b05337ed6f4ca65eca6141_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def mass_plot_io():\n    m1 = np.sqrt(m0 ** 2 + big_dm)\n    m2 = np.sqrt(m1 ** 2 + small_dm)\n    with quantity_support():\n        plt.loglog(m0, m0)\n        plt.loglog(m0, m1)\n        plt.loglog(m0, m2)\n"]]}
{"hexsha": "ac81e12f856586081d738f3e2858299fe94b6c30", "ext": "py", "lang": "Python", "content": "def lisp_close_socket ( sock , internal_name ) :\n sock . close ( )\n if ( os . path . exists ( internal_name ) ) : os . system ( \"rm \" + internal_name )\n return\n if 64 - 64: i1IIi / OoO0O00\n if 68 - 68: I11i * O0 * oO0o + OoOoOO00 / IiII\n if 42 - 42: iIii1I11I1II1 % i1IIi - OoOoOO00 % I1ii11iIi11i * Ii1I + i11iIiiIii\n if 40 - 40: OOooOOo\n if 30 - 30: o0oOOo0O0Ooo - Oo0Ooo + iII111i / O0\n if 94 - 94: IiII\n if 69 - 69: I1Ii111 . I1Ii111\n if 53 - 53: i11iIiiIii + iII111i * Oo0Ooo - I1Ii111", "fn_id": 70, "class_fn": false, "repo": "farinacci/lispers.net", "file": "build/releases/release-0.568/ob/lisp.py", "last_update_at": "2022-03-25T04:40:38+00:00", "question_id": "ac81e12f856586081d738f3e2858299fe94b6c30_70", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def lisp_close_socket(sock, internal_name):\n    sock.close()\n    if os.path.exists(internal_name):\n        os.system('rm ' + internal_name)\n    return\n    if 64 - 64:\n        i1IIi / OoO0O00\n    if 68 - 68:\n        I11i * O0 * oO0o + OoOoOO00 / IiII\n    if 42 - 42:\n        iIii1I11I1II1 % i1IIi - OoOoOO00 % I1ii11iIi11i * Ii1I + i11iIiiIii\n    if 40 - 40:\n        OOooOOo\n    if 30 - 30:\n        o0oOOo0O0Ooo - Oo0Ooo + iII111i / O0\n    if 94 - 94:\n        IiII\n    if 69 - 69:\n        I1Ii111.I1Ii111\n    if 53 - 53:\n"]]}
{"hexsha": "208bcd5d140a584e923bfcd44ee06fbcd1bdb71e", "ext": "py", "lang": "Python", "content": "def encode_numpy_slice(ndarray, convert_float=True):\n    ndarray = ndarray.copy(order='C')\n    dtype = np.dtype(ndarray.dtype).name\n    output = io.BytesIO()\n    if convert_float:\n        with Image.fromarray(img_as_ubyte(ndarray)) as im:   \n            im.save(output, format=\"JPEG\")\n    else:\n        with Image.fromarray(ndarray) as im:   \n            im.save(output, format=\"JPEG\")\n    output.seek(0)\n    #data = base64.b64encode(output.read()) \n    data = output.read()\n    data = base64.b64encode(data)\n    return dict(data=data, dtype=dtype, shape=ndarray.shape)", "fn_id": 2, "class_fn": false, "repo": "DiamondLightSource/SuRVoS2", "file": "survos2/utils.py", "last_update_at": "2022-01-14T05:57:50+00:00", "question_id": "208bcd5d140a584e923bfcd44ee06fbcd1bdb71e_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def encode_numpy_slice(ndarray, convert_float=True):\n    ndarray = ndarray.copy(order='C')\n    dtype = np.dtype(ndarray.dtype).name\n    output = io.BytesIO()\n    if convert_float:\n        with Image.fromarray(img_as_ubyte(ndarray)) as im:\n            im.save(output, format='JPEG')\n    else:\n        with Image.fromarray(ndarray) as im:\n            im.save(output, format='JPEG')\n    output.seek(0)\n    data = output.read()\n    data = base64.b64encode(data)\n"]]}
{"hexsha": "8b4276cf467857883740cd00e1930987bd949c60", "ext": "py", "lang": "Python", "content": "def test_ModeId():\n\n    # read dict\n    with open(\"./tests/test_data/ModeId_results_dict.json\") as json_file:\n        api_results = json.load(json_file)\n\n    feat = parse_results(api_results, t_zone=TZ, t_unit=None)\n    # Check dataframe conversion\n    df_df = pd.read_csv(\"./tests/test_data/ModeId_df.csv\")\n    pd.testing.assert_frame_equal(\n        feat.to_df().drop(\"datetime\", axis=1), df_df, check_less_precise=True\n    )\n\n    # Summary\n    res = feat.summary()\n    assert res[0][\"portion\"][0] == 84\n    assert res[0][\"counts\"][1] == 8\n    assert (res[1][\"counts\"][1] == 8).any()\n    assert (res[1][\"portion\"][0] == 84).any()\n\n    # Mode table (+wallclock times)\n    feat = parse_results(api_results, t_zone=TZ, t_unit=\"s\")\n    mt_df = feat.mode_table().reset_index(drop=True)\n    mt_correct_df = pd.read_pickle(\"./tests/test_data/mode_table_wc.pkl\")\n    mt_correct_df = mt_correct_df.reset_index(drop=True)\n    assert mt_correct_df.equals(mt_df.reset_index(drop=True))", "fn_id": 2, "class_fn": false, "repo": "vnadhan/mvg", "file": "tests/test_analysis_classes.py", "last_update_at": "2022-01-12T13:42:34+00:00", "question_id": "8b4276cf467857883740cd00e1930987bd949c60_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_ModeId():\n    with open('./tests/test_data/ModeId_results_dict.json') as json_file:\n        api_results = json.load(json_file)\n    feat = parse_results(api_results, t_zone=TZ, t_unit=None)\n    df_df = pd.read_csv('./tests/test_data/ModeId_df.csv')\n    pd.testing.assert_frame_equal(feat.to_df().drop('datetime', axis=1), df_df, check_less_precise=True)\n    res = feat.summary()\n    assert res[0]['portion'][0] == 84\n    assert res[0]['counts'][1] == 8\n    assert (res[1]['counts'][1] == 8).any()\n    assert (res[1]['portion'][0] == 84).any()\n    feat = parse_results(api_results, t_zone=TZ, t_unit='s')\n    mt_df = feat.mode_table().reset_index(drop=True)\n    mt_correct_df = pd.read_pickle('./tests/test_data/mode_table_wc.pkl')\n    mt_correct_df = mt_correct_df.reset_index(drop=True)\n"]]}
{"hexsha": "462e824aef2fd6c7e8d0d652b701f74c797eef13", "ext": "py", "lang": "Python", "content": "def write_smiles(dataset: Iterable[str], filename: str):\n    \"\"\"\n    Dumps a list of SMILES into a file, one per line\n    \"\"\"\n    n_lines = 0\n    with open(filename, 'w') as out:\n        for smiles_str in dataset:\n            out.write('%s\\n' % smiles_str)\n            n_lines += 1\n    print(f'{filename} contains {n_lines} molecules')", "fn_id": 4, "class_fn": false, "repo": "UnixJunkie/guacamol", "file": "guacamol/data/get_data.py", "last_update_at": "2022-03-26T19:35:17+00:00", "question_id": "462e824aef2fd6c7e8d0d652b701f74c797eef13_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def write_smiles(dataset: Iterable[str], filename: str):\n    \"\"\"\n    Dumps a list of SMILES into a file, one per line\n    \"\"\"\n    n_lines = 0\n    with open(filename, 'w') as out:\n        for smiles_str in dataset:\n            out.write('%s\\n' % smiles_str)\n            n_lines += 1\n"]]}
{"hexsha": "be5edd2ed3399ba1a7d889c69adee44b2d7cae6e", "ext": "py", "lang": "Python", "content": "def submit(args):\n    \"\"\"Submission script with MPI.\"\"\"\n    def mpi_submit(nworker, nserver, pass_envs):\n        \"\"\"Internal closure for job submission.\"\"\"\n        def run(prog):\n            \"\"\"run the program\"\"\"\n            subprocess.check_call(prog, shell=True)\n\n        cmd = ''\n        if args.host_file is not None:\n            cmd = '--hostfile %s ' % (args.host_file)\n        cmd += ' ' + ' '.join(args.command)\n\n        pass_envs['DMLC_JOB_CLUSTER'] = 'mpi'\n\n        # start workers\n        if nworker > 0:\n            logging.info('Start %d workers by mpirun' % nworker)\n            pass_envs['DMLC_ROLE'] = 'worker'\n            prog = 'mpirun -n %d %s %s' % (nworker, get_mpi_env(pass_envs), cmd)\n            thread = Thread(target=run, args=(prog,))\n            thread.setDaemon(True)\n            thread.start()\n\n\n        # start servers\n        if nserver > 0:\n            logging.info('Start %d servers by mpirun' % nserver)\n            pass_envs['DMLC_ROLE'] = 'server'\n            prog = 'mpirun -n %d %s %s' % (nserver, get_mpi_env(pass_envs), cmd)\n            thread = Thread(target=run, args=(prog,))\n            thread.setDaemon(True)\n            thread.start()\n\n\n    tracker.submit(args.num_workers, args.num_servers,\n                   fun_submit=mpi_submit,\n                   pscmd=(' '.join(args.command)))", "fn_id": 1, "class_fn": false, "repo": "felixwzh/TSGB", "file": "dmlc-core/tracker/dmlc_tracker/mpi.py", "last_update_at": "2022-01-15T13:43:17+00:00", "question_id": "be5edd2ed3399ba1a7d889c69adee44b2d7cae6e_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def submit(args):\n    \"\"\"Submission script with MPI.\"\"\"\n\n    def mpi_submit(nworker, nserver, pass_envs):\n        \"\"\"Internal closure for job submission.\"\"\"\n\n        def run(prog):\n            \"\"\"run the program\"\"\"\n            subprocess.check_call(prog, shell=True)\n        cmd = ''\n        if args.host_file is not None:\n            cmd = '--hostfile %s ' % args.host_file\n        cmd += ' ' + ' '.join(args.command)\n        pass_envs['DMLC_JOB_CLUSTER'] = 'mpi'\n        if nworker > 0:\n            logging.info('Start %d workers by mpirun' % nworker)\n            pass_envs['DMLC_ROLE'] = 'worker'\n            prog = 'mpirun -n %d %s %s' % (nworker, get_mpi_env(pass_envs), cmd)\n            thread = Thread(target=run, args=(prog,))\n            thread.setDaemon(True)\n            thread.start()\n        if nserver > 0:\n            logging.info('Start %d servers by mpirun' % nserver)\n            pass_envs['DMLC_ROLE'] = 'server'\n            prog = 'mpirun -n %d %s %s' % (nserver, get_mpi_env(pass_envs), cmd)\n            thread = Thread(target=run, args=(prog,))\n            thread.setDaemon(True)\n            thread.start()\n"]]}
{"hexsha": "6f47eee79a702f7d677979c76cfa7e6c0f56a14e", "ext": "py", "lang": "Python", "content": "def get_num_from_precision(precision: int) -> decimal.Decimal:\n    \"\"\"\n    :param precision: \u7cbe\u5ea6\uff0c\u5fc5\u987b\u5927\u4e8e\u7b49\u4e8e 0\n    :return: \u8fd4\u56de precision \u5bf9\u5e94\u7684\u5c0f\u6570\n    \"\"\"\n    if precision < 0:\n        raise ValueError('precision \u4e0d\u80fd\u5c0f\u4e8e 0')\n    return decimal.Decimal('1e{}'.format(-precision))", "fn_id": 2, "class_fn": false, "repo": "wumk/crypto_auto_trading", "file": "strategy/utils.py", "last_update_at": "2022-01-28T17:11:52+00:00", "question_id": "6f47eee79a702f7d677979c76cfa7e6c0f56a14e_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_num_from_precision(precision: int) -> decimal.Decimal:\n    \"\"\"\n    :param precision: \u7cbe\u5ea6\uff0c\u5fc5\u987b\u5927\u4e8e\u7b49\u4e8e 0\n    :return: \u8fd4\u56de precision \u5bf9\u5e94\u7684\u5c0f\u6570\n    \"\"\"\n    if precision < 0:\n        raise ValueError('precision \u4e0d\u80fd\u5c0f\u4e8e 0')\n"]]}
{"hexsha": "1415bdc005a082993a894bb214799989f44df486", "ext": "py", "lang": "Python", "content": "def main():\n    args = parse_args()\n    # init models\n    modelA = init_model(args.modelA, checkpoint=None, device=args.device)\\\n        .eval()\n    modelB = init_model(args.modelB, checkpoint=None, device=args.device)\\\n        .eval()\n\n    src_img = load_image(args.source_path).to(args.device)\n    codes = modelA.encoder(src_img)\n    latents = get_latent(modelA.decoder, codes)\n\n    _, save_swap_layer = modelA.decoder.swap_forward(\n        [latents],\n        input_is_latent=True,\n        swap=True, swap_layer_num=args.swap_layer,\n    )\n\n    image, _ = modelB.decoder.swap_forward(\n        [latents],\n        input_is_latent=True,\n        swap=True, swap_layer_num=args.swap_layer,\n        swap_layer_tensor=save_swap_layer,\n    )\n    # our generator's default output channel order is bgr\n    image = image[:, [2, 1, 0], ...]\n\n    mmcv.mkdir_or_exist(os.path.dirname(args.save_path))\n    utils.save_image(image, args.save_path, normalize=True)", "fn_id": 3, "class_fn": false, "repo": "TommyZihao/MMGEN-FaceStylor", "file": "apps/layerSwap.py", "last_update_at": "2022-03-27T12:30:42+00:00", "question_id": "1415bdc005a082993a894bb214799989f44df486_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    args = parse_args()\n    modelA = init_model(args.modelA, checkpoint=None, device=args.device).eval()\n    modelB = init_model(args.modelB, checkpoint=None, device=args.device).eval()\n    src_img = load_image(args.source_path).to(args.device)\n    codes = modelA.encoder(src_img)\n    latents = get_latent(modelA.decoder, codes)\n    _, save_swap_layer = modelA.decoder.swap_forward([latents], input_is_latent=True, swap=True, swap_layer_num=args.swap_layer)\n    image, _ = modelB.decoder.swap_forward([latents], input_is_latent=True, swap=True, swap_layer_num=args.swap_layer, swap_layer_tensor=save_swap_layer)\n    image = image[:, [2, 1, 0], ...]\n    mmcv.mkdir_or_exist(os.path.dirname(args.save_path))\n"]]}
{"hexsha": "bf2b77d462fd3190c2d9561fc842f4ec0ac12183", "ext": "py", "lang": "Python", "content": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"\"\"Create a Node toolchain by downloading components from\nhttps://nodejs.org and vendor it into the repo. Example:\n\n    ./create_node_toolchain.py --output ./v10.16.0 v10.16.0\n\"\"\",\n        formatter_class=RawTextHelpFormatter,\n    )\n    parser.add_argument(\n        \"--output\",\n        help=\"If specified, overwrite this directory with the generated DotSlash files.\",\n    )\n    parser.add_argument(\n        \"--retain-forever\",\n        action=\"store_true\",\n        help=\"\"\"Store the artifacts in Everstore with indefinite retention.\nOnly use when you plan to check the result of this script into the repo.\n\"\"\",\n    )\n    parser.add_argument(\n        \"toolchain_version\", help=\"toolchain version, such as 'v10.16.0'\"\n    )\n    return parser.parse_args()", "fn_id": 0, "class_fn": false, "repo": "kkkkv/tgnms", "file": "third-party/node/create_node_toolchain.py", "last_update_at": "2022-03-18T10:52:29+00:00", "question_id": "bf2b77d462fd3190c2d9561fc842f4ec0ac12183_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def parse_args():\n    parser = argparse.ArgumentParser(description='Create a Node toolchain by downloading components from\\nhttps://nodejs.org and vendor it into the repo. Example:\\n\\n    ./create_node_toolchain.py --output ./v10.16.0 v10.16.0\\n', formatter_class=RawTextHelpFormatter)\n    parser.add_argument('--output', help='If specified, overwrite this directory with the generated DotSlash files.')\n    parser.add_argument('--retain-forever', action='store_true', help='Store the artifacts in Everstore with indefinite retention.\\nOnly use when you plan to check the result of this script into the repo.\\n')\n    parser.add_argument('toolchain_version', help=\"toolchain version, such as 'v10.16.0'\")\n"]]}
{"hexsha": "74c00a0741d0c6dfe09a522e6c2038e228996ef2", "ext": "py", "lang": "Python", "content": "def _conv_bn_relu(**conv_params):\n  \"\"\"Helper to build a conv -> BN -> relu block.\"\"\"\n  filters = conv_params[\"filters\"]\n  kernel_size = conv_params[\"kernel_size\"]\n  strides = conv_params.setdefault(\"strides\", (1, 1))\n  kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n  padding = conv_params.setdefault(\"padding\", \"same\")\n  kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n\n  def f(inpt):\n    conv = Conv2D(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        kernel_initializer=kernel_initializer,\n        kernel_regularizer=kernel_regularizer)(\n            inpt)\n    return _bn_relu(conv)\n\n  return f", "fn_id": 1, "class_fn": false, "repo": "cclauss/episodic-curiosity", "file": "third_party/keras_resnet/models.py", "last_update_at": "2022-03-29T14:09:36+00:00", "question_id": "74c00a0741d0c6dfe09a522e6c2038e228996ef2_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _conv_bn_relu(**conv_params):\n    \"\"\"Helper to build a conv -> BN -> relu block.\"\"\"\n    filters = conv_params['filters']\n    kernel_size = conv_params['kernel_size']\n    strides = conv_params.setdefault('strides', (1, 1))\n    kernel_initializer = conv_params.setdefault('kernel_initializer', 'he_normal')\n    padding = conv_params.setdefault('padding', 'same')\n    kernel_regularizer = conv_params.setdefault('kernel_regularizer', l2(0.0001))\n\n    def f(inpt):\n        conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(inpt)\n        return _bn_relu(conv)\n"]]}
{"hexsha": "8600fc6fc5e2cffc5173a4638948df030a5c1d80", "ext": "py", "lang": "Python", "content": "def route2cost(route, dist):\n    assert route[0] == 0\n    assert route[-1] == 0\n    \n    return dist[(route[:-1], route[1:])].sum()", "fn_id": 0, "class_fn": false, "repo": "cfld/simple_tsp", "file": "simple_tsp/helpers.py", "last_update_at": "2022-03-24T04:12:05+00:00", "question_id": "8600fc6fc5e2cffc5173a4638948df030a5c1d80_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def route2cost(route, dist):\n    assert route[0] == 0\n    assert route[-1] == 0\n"]]}
{"hexsha": "9a22132332a06af7c2d99cddd9964d8a5efbd6ca", "ext": "py", "lang": "Python", "content": "def get_playable_podcast11(soup12):\n    subjects = []\n    for content in soup12.find_all('item'):\n        try:        \n            link = content.find('enclosure')\n            link = link.get('url')\n            print(\"\\n\\nLink: \", link)\n            title = content.find('title')\n            title = title.get_text()\n        except AttributeError:\n            continue\n        item = {\n                'url': link,\n                'title': title,\n                'thumbnail': \"https://cdn3.img.sputniknews.com/images/105617/96/1056179634.png\",\n        }\n        subjects.append(item)\n    return subjects", "fn_id": 32, "class_fn": false, "repo": "leopheard/RadioSputnik", "file": "resources/lib/mainaddon.py", "last_update_at": "2022-01-11T20:36:13+00:00", "question_id": "9a22132332a06af7c2d99cddd9964d8a5efbd6ca_32", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_playable_podcast11(soup12):\n    subjects = []\n    for content in soup12.find_all('item'):\n        try:\n            link = content.find('enclosure')\n            link = link.get('url')\n            print('\\n\\nLink: ', link)\n            title = content.find('title')\n            title = title.get_text()\n        except AttributeError:\n            continue\n        item = {'url': link, 'title': title, 'thumbnail': 'https://cdn3.img.sputniknews.com/images/105617/96/1056179634.png'}\n        subjects.append(item)\n"]]}
{"hexsha": "104ab3347ad29648ea47da6ffbc0282f1f77a64e", "ext": "py", "lang": "Python", "content": "def load_snap_patents_mat(nclass=5):\n    fulldata = scipy.io.loadmat(f'{DATAPATH}/snap_patents.mat')\n\n    dataset = NCDataset('snap_patents')\n    edge_index = torch.tensor(fulldata['edge_index'], dtype=torch.long)\n    node_feat = torch.tensor(\n        fulldata['node_feat'].todense(), dtype=torch.float)\n    num_nodes = int(fulldata['num_nodes'])\n    dataset.graph = {'edge_index': edge_index,\n                     'edge_feat': None,\n                     'node_feat': node_feat,\n                     'num_nodes': num_nodes}\n\n    years = fulldata['years'].flatten()\n    label = even_quantile_labels(years, nclass, verbose=False)\n    dataset.label = torch.tensor(label, dtype=torch.long)\n\n    return dataset", "fn_id": 12, "class_fn": false, "repo": "nnzhan/AutoGCN", "file": "data/ncdata.py", "last_update_at": "2022-03-07T10:41:21+00:00", "question_id": "104ab3347ad29648ea47da6ffbc0282f1f77a64e_12", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_snap_patents_mat(nclass=5):\n    fulldata = scipy.io.loadmat(f'{DATAPATH}/snap_patents.mat')\n    dataset = NCDataset('snap_patents')\n    edge_index = torch.tensor(fulldata['edge_index'], dtype=torch.long)\n    node_feat = torch.tensor(fulldata['node_feat'].todense(), dtype=torch.float)\n    num_nodes = int(fulldata['num_nodes'])\n    dataset.graph = {'edge_index': edge_index, 'edge_feat': None, 'node_feat': node_feat, 'num_nodes': num_nodes}\n    years = fulldata['years'].flatten()\n    label = even_quantile_labels(years, nclass, verbose=False)\n    dataset.label = torch.tensor(label, dtype=torch.long)\n"]]}
{"hexsha": "4b4111b001af98a8f5dfe2337470efc36e0f5491", "ext": "py", "lang": "Python", "content": "def compute_weights(samples):\n    mu, std = norm.fit(samples)\n    std_vec = std * torch.ones_like(samples)\n    mu_vec  = mu  * torch.ones_like(samples)\n    pi_vec  = np.pi    * torch.ones_like(samples)\n\n    weights = ((std_vec*torch.sqrt(2.0*pi_vec)) *\n               torch.exp(0.5*torch.square((samples-mu_vec)/std_vec)))\n\n    if rank==0:\n        print(f'min weight: {torch.min(weights)}, max weight: {torch.max(weights)}')\n    #weights = torch.clamp(weights, 0, weight_cap)\n\n    return weights", "fn_id": 3, "class_fn": false, "repo": "ashao/NCAR_ML_EKE", "file": "ml_eke/nn/pytorch_eke.py", "last_update_at": "2022-01-06T04:37:01+00:00", "question_id": "4b4111b001af98a8f5dfe2337470efc36e0f5491_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def compute_weights(samples):\n    mu, std = norm.fit(samples)\n    std_vec = std * torch.ones_like(samples)\n    mu_vec = mu * torch.ones_like(samples)\n    pi_vec = np.pi * torch.ones_like(samples)\n    weights = std_vec * torch.sqrt(2.0 * pi_vec) * torch.exp(0.5 * torch.square((samples - mu_vec) / std_vec))\n    if rank == 0:\n        print(f'min weight: {torch.min(weights)}, max weight: {torch.max(weights)}')\n"]]}
{"hexsha": "d27a7cd8fb1a33aefe639f54dcde93489cfbe955", "ext": "py", "lang": "Python", "content": "def import_stock_daily():\n    w = WindRest(WIND_REST_URL)\n    engine = get_db_engine()\n    with get_db_session(engine) as session:\n        # \u83b7\u53d6\u6bcf\u53ea\u80a1\u7968\u6700\u65b0\u4ea4\u6613\u65e5\u6570\u636e\n        sql_str = 'select wind_code, max(Trade_date) from wind_stock_daily group by wind_code'\n        table = session.execute(sql_str)\n        stock_trade_date_latest_dic = dict(table.fetchall())\n        # \u83b7\u53d6\u5e02\u573a\u6709\u6548\u4ea4\u6613\u65e5\u6570\u636e\n        sql_str = \"select trade_date from wind_trade_date where trade_date > '2005-1-1'\"\n        table = session.execute(sql_str)\n        trade_date_sorted_list = [t[0] for t in table.fetchall()]\n        trade_date_sorted_list.sort()\n        # \u83b7\u53d6\u6bcf\u53ea\u80a1\u7968\u4e0a\u5e02\u65e5\u671f\u3001\u9000\u5e02\u65e5\u671f\n        table = session.execute('SELECT wind_code, ipo_date, delist_date FROM wind_stock_info')\n        stock_date_dic = {wind_code: (ipo_date, delist_date if delist_date is None or delist_date > UN_AVAILABLE_DATE else None) for\n                          wind_code, ipo_date, delist_date in table.fetchall()}\n    today_t_1 = date.today() - ONE_DAY\n    data_df_list = []\n\n    try:\n        for wind_code, date_pair in stock_date_dic.items():\n            date_ipo, date_delist = date_pair\n            # \u83b7\u53d6 date_from\n            if wind_code in stock_trade_date_latest_dic:\n                date_latest_t1 = stock_trade_date_latest_dic[wind_code] + ONE_DAY\n                date_from = max([date_latest_t1, DATE_BASE, date_ipo])\n            else:\n                date_from = max([DATE_BASE, date_ipo])\n            date_from = get_first(trade_date_sorted_list, lambda x: x >= date_from)\n            # \u83b7\u53d6 date_to\n            if date_delist is None:\n                date_to = today_t_1\n            else:\n                date_to = min([date_delist, today_t_1])\n            date_to = get_last(trade_date_sorted_list, lambda x: x <= date_to)\n            if date_from is None or date_to is None or date_from > date_to:\n                continue\n            # \u83b7\u53d6\u80a1\u7968\u91cf\u4ef7\u7b49\u884c\u60c5\u6570\u636e\n            wind_indictor_str = \"open,high,low,close,adjfactor,volume,amt,pct_chg,maxupordown,\" + \\\n                                \"swing,turn,free_turn,trade_status,susp_days\"\n            data_df = w.wsd(wind_code, wind_indictor_str, date_from, date_to)\n            if data_df is None:\n                logging.warning('%s has no data during %s %s', wind_code, date_from, date_to)\n                continue\n            logging.info('%d data of %s', data_df.shape[0], wind_code)\n            data_df['wind_code'] = wind_code\n            data_df_list.append(data_df)\n    finally:\n        # \u5bfc\u5165\u6570\u636e\u5e93\n        if len(data_df_list) > 0:\n            data_df_all = pd.concat(data_df_list)\n            data_df_all.index.rename('trade_date', inplace=True)\n            data_df_all.reset_index(inplace=True)\n            data_df_all.set_index(['wind_code', 'trade_date'], inplace=True)\n            data_df_all.to_sql('wind_stock_daily', engine, if_exists='append',\n                               dtype={\n                                   'wind_code': String(20),\n                                   'trade_date': Date,\n                                   'open': Float,\n                                   'high': Float,\n                                   'low': Float,\n                                   'close': Float,\n                                   'adjfactor': Float,\n                                   'volume': Float,\n                                   'amt': Float,\n                                   'pct_chg': Float,\n                                   'maxupordown': Integer,\n                                   'swing': Float,\n                                   'turn': Float,\n                                   'free_turn': Float,\n                                   'trade_status': String(20),\n                                   'susp_days': Integer,\n                               }\n                               )\n            logging.info('%d data imported', data_df_all.shape[0])", "fn_id": 3, "class_fn": false, "repo": "zuoziji/transaction", "file": "JobScript/wind_to_db/wind_stock_daily_import.py", "last_update_at": "2022-03-11T23:16:46+00:00", "question_id": "d27a7cd8fb1a33aefe639f54dcde93489cfbe955_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def import_stock_daily():\n    w = WindRest(WIND_REST_URL)\n    engine = get_db_engine()\n    with get_db_session(engine) as session:\n        sql_str = 'select wind_code, max(Trade_date) from wind_stock_daily group by wind_code'\n        table = session.execute(sql_str)\n        stock_trade_date_latest_dic = dict(table.fetchall())\n        sql_str = \"select trade_date from wind_trade_date where trade_date > '2005-1-1'\"\n        table = session.execute(sql_str)\n        trade_date_sorted_list = [t[0] for t in table.fetchall()]\n        trade_date_sorted_list.sort()\n        table = session.execute('SELECT wind_code, ipo_date, delist_date FROM wind_stock_info')\n        stock_date_dic = {wind_code: (ipo_date, delist_date if delist_date is None or delist_date > UN_AVAILABLE_DATE else None) for wind_code, ipo_date, delist_date in table.fetchall()}\n    today_t_1 = date.today() - ONE_DAY\n    data_df_list = []\n    try:\n        for wind_code, date_pair in stock_date_dic.items():\n            date_ipo, date_delist = date_pair\n            if wind_code in stock_trade_date_latest_dic:\n                date_latest_t1 = stock_trade_date_latest_dic[wind_code] + ONE_DAY\n                date_from = max([date_latest_t1, DATE_BASE, date_ipo])\n            else:\n                date_from = max([DATE_BASE, date_ipo])\n            date_from = get_first(trade_date_sorted_list, lambda x: x >= date_from)\n            if date_delist is None:\n                date_to = today_t_1\n            else:\n                date_to = min([date_delist, today_t_1])\n            date_to = get_last(trade_date_sorted_list, lambda x: x <= date_to)\n            if date_from is None or date_to is None or date_from > date_to:\n                continue\n            wind_indictor_str = 'open,high,low,close,adjfactor,volume,amt,pct_chg,maxupordown,' + 'swing,turn,free_turn,trade_status,susp_days'\n            data_df = w.wsd(wind_code, wind_indictor_str, date_from, date_to)\n            if data_df is None:\n                logging.warning('%s has no data during %s %s', wind_code, date_from, date_to)\n                continue\n            logging.info('%d data of %s', data_df.shape[0], wind_code)\n            data_df['wind_code'] = wind_code\n            data_df_list.append(data_df)\n    finally:\n        if len(data_df_list) > 0:\n            data_df_all = pd.concat(data_df_list)\n            data_df_all.index.rename('trade_date', inplace=True)\n            data_df_all.reset_index(inplace=True)\n            data_df_all.set_index(['wind_code', 'trade_date'], inplace=True)\n            data_df_all.to_sql('wind_stock_daily', engine, if_exists='append', dtype={'wind_code': String(20), 'trade_date': Date, 'open': Float, 'high': Float, 'low': Float, 'close': Float, 'adjfactor': Float, 'volume': Float, 'amt': Float, 'pct_chg': Float, 'maxupordown': Integer, 'swing': Float, 'turn': Float, 'free_turn': Float, 'trade_status': String(20), 'susp_days': Integer})\n"]]}
{"hexsha": "64f0ef89b41246b45a0f27b2b98de5e0ee60111f", "ext": "py", "lang": "Python", "content": "def test_fits2radec():\n    scale = ael.fits2radec(fitsfn)\n\n    assert scale[\"ra\"].values[[32, 51, 98], [28, 92, 156]] == approx(\n        [152.313342, 157.988921, 165.012208]\n    )\n    assert scale[\"dec\"].values[[32, 51, 98], [28, 92, 156]] == approx(\n        [59.982123, 59.182819, 59.149952]\n    )", "fn_id": 1, "class_fn": false, "repo": "scienceopen/astrometry", "file": "src/astrometry_azel/tests/test_all.py", "last_update_at": "2022-03-17T08:28:43+00:00", "question_id": "64f0ef89b41246b45a0f27b2b98de5e0ee60111f_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_fits2radec():\n    scale = ael.fits2radec(fitsfn)\n    assert scale['ra'].values[[32, 51, 98], [28, 92, 156]] == approx([152.313342, 157.988921, 165.012208])\n"]]}
{"hexsha": "01fbb7f8ba1c0a0deed7c2153d9f65d39c329d54", "ext": "py", "lang": "Python", "content": "@forge.copy(request, exclude='method')\ndef get(url: str, **kwargs) -> Response:\n    \"\"\"Wrapper around :py:func:`requests.get` with additional options specific to iNat API requests\"\"\"\n    return request('GET', url, **kwargs)", "fn_id": 3, "class_fn": false, "repo": "niconoe/pyinaturalist", "file": "pyinaturalist/session.py", "last_update_at": "2022-03-17T16:32:17+00:00", "question_id": "01fbb7f8ba1c0a0deed7c2153d9f65d39c329d54_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@forge.copy(request, exclude='method')\ndef get(url: str, **kwargs) -> Response:\n    \"\"\"Wrapper around :py:func:`requests.get` with additional options specific to iNat API requests\"\"\"\n"]]}
{"hexsha": "40c1efb9d0a40c79ea15e03a04a04e5ab8913a90", "ext": "py", "lang": "Python", "content": "def get_mirror_offset_spots_dw():\n    \"\"\"\n    Mirror shenanigans placing a mirror portal with a broken camera\n    \"\"\"\n    yield ('Dark Death Mountain Offset Mirror', 'West Dark Death Mountain (Bottom)', 'Pyramid Area')", "fn_id": 11, "class_fn": false, "repo": "codemann8/ALttPDoorRandomizer", "file": "OverworldGlitchRules.py", "last_update_at": "2022-01-23T19:45:35+00:00", "question_id": "40c1efb9d0a40c79ea15e03a04a04e5ab8913a90_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_mirror_offset_spots_dw():\n    \"\"\"\n    Mirror shenanigans placing a mirror portal with a broken camera\n    \"\"\"\n"]]}
{"hexsha": "fa07c76034d9f89dd6b8009b47bc725883385a56", "ext": "py", "lang": "Python", "content": "def test_list_with_registry(baz):\n    data = get_data(baz, bar=True)\n    hp = ParentListHP.create(data={'foos': data})\n    assert isinstance(hp.foos, list)\n    assert len(hp.foos) == 2\n    foo = hp.foos[0]\n    assert isinstance(foo, Foo)\n    assert foo.baz == baz\n\n    bar = hp.foos[1]\n    assert isinstance(bar, Bar)\n    assert bar.baz == baz", "fn_id": 7, "class_fn": false, "repo": "mosaicml/yahp", "file": "tests/test_yahp_list_from_yaml_dict.py", "last_update_at": "2022-03-28T18:46:43+00:00", "question_id": "fa07c76034d9f89dd6b8009b47bc725883385a56_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_list_with_registry(baz):\n    data = get_data(baz, bar=True)\n    hp = ParentListHP.create(data={'foos': data})\n    assert isinstance(hp.foos, list)\n    assert len(hp.foos) == 2\n    foo = hp.foos[0]\n    assert isinstance(foo, Foo)\n    assert foo.baz == baz\n    bar = hp.foos[1]\n    assert isinstance(bar, Bar)\n"]]}
{"hexsha": "2f1913765aa6cc21a9f98c848fc9a38bfb006c61", "ext": "py", "lang": "Python", "content": "def get_needed_cities(request):\n    if 'city' in request.GET:\n        city_from_user = request.GET['city']\n\n        # Search similar in db\n        needed_cities = City.objects.filter(\n            name__icontains=city_from_user).only('name')[:10]\n\n        # Dump to json\n        return HttpResponse(json.dumps([city.name for city in needed_cities]), content_type=\"application/json\")\n    return HttpResponse()", "fn_id": 14, "class_fn": false, "repo": "Omrigan/gotosite", "file": "main/views.py", "last_update_at": "2022-02-08T19:12:17+00:00", "question_id": "2f1913765aa6cc21a9f98c848fc9a38bfb006c61_14", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_needed_cities(request):\n    if 'city' in request.GET:\n        city_from_user = request.GET['city']\n        needed_cities = City.objects.filter(name__icontains=city_from_user).only('name')[:10]\n        return HttpResponse(json.dumps([city.name for city in needed_cities]), content_type='application/json')\n"]]}
{"hexsha": "58b4da35f8bd638742d8006dae70304e228a4c07", "ext": "py", "lang": "Python", "content": "@borg.on(events.NewMessage(pattern=r\"\\.(.*)\", outgoing=True))\nasync def _(event):\n\n    if event.fwd_from:\n\n        return\n\n    input_str = event.pattern_match.group(1)\n\n    if input_str == \"round\":\n\n        await event.edit(input_str)\n\n        animation_chars = [\"\u26ab\", \"\u2b24\", \"\u25cf\", \"\u2218\" \"\u200e\"]\n\n        animation_interval = 0.1\n\n        animation_ttl = range(100)\n\n        for i in animation_ttl:\n\n            await asyncio.sleep(animation_interval)\n\n            await event.edit(animation_chars[i % 4])\n", "fn_id": 3, "class_fn": false, "repo": "Colossalhavoc/PepeBot", "file": "stdplugins/animate.py", "last_update_at": "2022-01-18T07:37:53+00:00", "question_id": "58b4da35f8bd638742d8006dae70304e228a4c07_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@borg.on(events.NewMessage(pattern='\\\\.(.*)', outgoing=True))\nasync def _(event):\n    if event.fwd_from:\n        return\n    input_str = event.pattern_match.group(1)\n    if input_str == 'round':\n        await event.edit(input_str)\n        animation_chars = ['\u26ab', '\u2b24', '\u25cf', '\u2218\\u200e']\n        animation_interval = 0.1\n        animation_ttl = range(100)\n        for i in animation_ttl:\n            await asyncio.sleep(animation_interval)\n"]]}
{"hexsha": "99723bae9f12e77b9eb428cc5f968407bdbb58e7", "ext": "py", "lang": "Python", "content": "def writer(id, u_dict):\n  '''\n  \u66f8\u304d\u8fbc\u307f\u7528\u95a2\u6570\n\n  \u8f9e\u66f8 -> csv\n  '''\n  dict_path = \"./config/guild/\" + str(id) + \"/\" + \"dict.csv\"\n  with open(dict_path,mode=\"w\",encoding=\"utf-16\") as f:\n    writer = csv.writer(f)\n    for k, v in u_dict.items():\n      writer.writerow([k,v])", "fn_id": 2, "class_fn": false, "repo": "0kq-github/shovel_paimon", "file": "shovel_module/dict.py", "last_update_at": "2022-01-06T06:10:32+00:00", "question_id": "99723bae9f12e77b9eb428cc5f968407bdbb58e7_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def writer(id, u_dict):\n    \"\"\"\n  \u66f8\u304d\u8fbc\u307f\u7528\u95a2\u6570\n\n  \u8f9e\u66f8 -> csv\n  \"\"\"\n    dict_path = './config/guild/' + str(id) + '/' + 'dict.csv'\n    with open(dict_path, mode='w', encoding='utf-16') as f:\n        writer = csv.writer(f)\n        for k, v in u_dict.items():\n"]]}
{"hexsha": "f11ca4d13cb39dfc5e8be3003105ccf6e96296bd", "ext": "py", "lang": "Python", "content": "def predict_mpan512(subdir, model,conf_thres=0.95):\n    mx, my = get_overlap_poly()\n    patch_creator = p512\n    keep_all = []\n    tifimages_pan = glob.glob(''.join([subdir, 'PAN/', '*.tif']))\n    tifimages_ms = [''.join([subdir, 'MS/', item.split('/')[-1].replace('PAN', 'MS')]) for item in tifimages_pan]\n    for ms, pan in list(zip(tifimages_ms, tifimages_pan)):\n        anns = {}\n        image_id = ms.split('/')[-1].replace('.tif', '').replace('MS_', '')\n        img_patches,img = get_mpan_image_patches(ms,pan,patch_creator)\n        if img is None:\n            keep_all.extend([[image_id,-1,'POLYGON EMPTY','POLYGON EMPTY']])\n            continue\n        shifts = {}\n        for k in patch_creator.coords.keys():\n            shifts[k] = (patch_creator.coords[k][2], patch_creator.coords[k][0])\n        spacenet_predctions_default = get_predictions_spacenet(image_id, img_patches, model, shifts,conf_thres=conf_thres)\n        anns[0] = spacenet_predctions_default\n        xx = [[0, 512, 256, 768],[388,900,256,768]]\n        yy = [[256, 768, 0, 512],[256,768,388,900]]\n        shift_x = {0: (0, 256), 1: (388, 256)}\n        shift_y = {k: (v[1], v[0]) for k, v in shift_x.items()}\n        img_patches = []\n        for ix in yy:\n            img_patches.append(img[ix[0]:ix[1], ix[2]:ix[3], :])\n        img_patches = np.asarray(img_patches)\n        spacenet_predctions_x = get_predictions_spacenet(image_id, img_patches, model, shift_x,conf_thres=conf_thres)\n        anns[1] = spacenet_predctions_x\n        img_patches = []\n        for ix in xx:\n            img_patches.append(img[ix[0]:ix[1], ix[2]:ix[3], :])\n        img_patches = np.asarray(img_patches)\n        spacenet_predctions_y = get_predictions_spacenet(image_id, img_patches, model, shift_y,conf_thres=conf_thres)\n        anns[2] = spacenet_predctions_y\n        keep = get_final_annotations(image_id,anns,mx,my)\n        if keep == []:\n            keep = [[image_id, -1, 'POLYGON EMPTY', \"POLYGON EMPTY\"]]\n        keep_all.extend(keep)\n    return keep_all", "fn_id": 10, "class_fn": false, "repo": "ktncktnc/SpaceNet_Off_Nadir_Solutions", "file": "number13/number13/src/prediction.py", "last_update_at": "2022-03-31T02:47:49+00:00", "question_id": "f11ca4d13cb39dfc5e8be3003105ccf6e96296bd_10", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def predict_mpan512(subdir, model, conf_thres=0.95):\n    mx, my = get_overlap_poly()\n    patch_creator = p512\n    keep_all = []\n    tifimages_pan = glob.glob(''.join([subdir, 'PAN/', '*.tif']))\n    tifimages_ms = [''.join([subdir, 'MS/', item.split('/')[-1].replace('PAN', 'MS')]) for item in tifimages_pan]\n    for ms, pan in list(zip(tifimages_ms, tifimages_pan)):\n        anns = {}\n        image_id = ms.split('/')[-1].replace('.tif', '').replace('MS_', '')\n        img_patches, img = get_mpan_image_patches(ms, pan, patch_creator)\n        if img is None:\n            keep_all.extend([[image_id, -1, 'POLYGON EMPTY', 'POLYGON EMPTY']])\n            continue\n        shifts = {}\n        for k in patch_creator.coords.keys():\n            shifts[k] = (patch_creator.coords[k][2], patch_creator.coords[k][0])\n        spacenet_predctions_default = get_predictions_spacenet(image_id, img_patches, model, shifts, conf_thres=conf_thres)\n        anns[0] = spacenet_predctions_default\n        xx = [[0, 512, 256, 768], [388, 900, 256, 768]]\n        yy = [[256, 768, 0, 512], [256, 768, 388, 900]]\n        shift_x = {0: (0, 256), 1: (388, 256)}\n        shift_y = {k: (v[1], v[0]) for k, v in shift_x.items()}\n        img_patches = []\n        for ix in yy:\n            img_patches.append(img[ix[0]:ix[1], ix[2]:ix[3], :])\n        img_patches = np.asarray(img_patches)\n        spacenet_predctions_x = get_predictions_spacenet(image_id, img_patches, model, shift_x, conf_thres=conf_thres)\n        anns[1] = spacenet_predctions_x\n        img_patches = []\n        for ix in xx:\n            img_patches.append(img[ix[0]:ix[1], ix[2]:ix[3], :])\n        img_patches = np.asarray(img_patches)\n        spacenet_predctions_y = get_predictions_spacenet(image_id, img_patches, model, shift_y, conf_thres=conf_thres)\n        anns[2] = spacenet_predctions_y\n        keep = get_final_annotations(image_id, anns, mx, my)\n        if keep == []:\n            keep = [[image_id, -1, 'POLYGON EMPTY', 'POLYGON EMPTY']]\n        keep_all.extend(keep)\n"]]}
{"hexsha": "666112bbb218859cb0654924f3b4ecab8f38180a", "ext": "py", "lang": "Python", "content": "def find_specific_class(specific_class, labels):\n    \n    img_ind = -1\n    \n    for img_ind in range(labels.shape[0]):\n        if labels[img_ind] == specific_class:\n            break\n    \n    return img_ind", "fn_id": 3, "class_fn": false, "repo": "nsfzyzz/boundary_thickness", "file": "utils3d.py", "last_update_at": "2022-02-24T10:32:39+00:00", "question_id": "666112bbb218859cb0654924f3b4ecab8f38180a_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def find_specific_class(specific_class, labels):\n    img_ind = -1\n    for img_ind in range(labels.shape[0]):\n        if labels[img_ind] == specific_class:\n            break\n"]]}
{"hexsha": "d879d766d4f7534558bd009e0e7f419ae03a5ea6", "ext": "py", "lang": "Python", "content": "def rstjinja(app, docname, source):\n    \"\"\"\n    Render our pages as a jinja template for fancy templating goodness.\n    \"\"\"\n    # Make sure we're outputting HTML\n    if app.builder.format != \"html\":\n        return\n    src = source[0]\n    rendered = app.builder.templates.render_string(src, app.config.html_context)\n    source[0] = rendered", "fn_id": 0, "class_fn": false, "repo": "MalikMlitat/robotframework-robocop", "file": "docs/conf.py", "last_update_at": "2022-03-24T00:23:54+00:00", "question_id": "d879d766d4f7534558bd009e0e7f419ae03a5ea6_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def rstjinja(app, docname, source):\n    \"\"\"\n    Render our pages as a jinja template for fancy templating goodness.\n    \"\"\"\n    if app.builder.format != 'html':\n        return\n    src = source[0]\n    rendered = app.builder.templates.render_string(src, app.config.html_context)\n"]]}
{"hexsha": "7f85061713c663e6a4dccde0e1ed24d18277597a", "ext": "py", "lang": "Python", "content": "def common_options():\n    \"\"\"Collects & returns options shared by all module implementations\n    :return: dict\n    \"\"\"\n    # Just API params for now\n    return API_CONFIG", "fn_id": 0, "class_fn": false, "repo": "JohnLieske/ansible-onepasswordconnect-collection", "file": "plugins/module_utils/specs.py", "last_update_at": "2022-03-18T19:02:49+00:00", "question_id": "7f85061713c663e6a4dccde0e1ed24d18277597a_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def common_options():\n    \"\"\"Collects & returns options shared by all module implementations\n    :return: dict\n    \"\"\"\n"]]}
{"hexsha": "4615a7bb2082ea09229eed4fac215fecde9fb19c", "ext": "py", "lang": "Python", "content": "def get_model(args, criterion):\n    \"\"\"Construct model from main args\"\"\"\n    kwargs = dict(num_classes=args.classes,\n                  num_layers=args.num_layers,\n                  kernel_size=args.kernel_size,\n                  num_filters=args.num_filters,\n                  imsize=args.imsize,\n                  padding=args.padding,\n                  batch_norm=args.batch_norm,\n                  multi_head=args.multi_head)\n    if \"warp\" in args.meta_model.lower():\n        model = WarpedOmniConv(warp_num_layers=args.warp_num_layers,\n                               warp_num_filters=args.warp_num_filters,\n                               warp_residual_connection=args.warp_residual,\n                               warp_act_fun=args.warp_act_fun,\n                               warp_batch_norm=args.warp_batch_norm,\n                               warp_final_head=args.warp_final_head,\n                               **kwargs)\n    else:\n        model = OmniConv(**kwargs)\n\n    if args.cuda:\n        model = model.cuda()\n\n    if args.log_ival > 0:\n        print(model)\n\n    if \"warp\" in args.meta_model.lower():\n        return WarpGradWrapper(\n            model,\n            args.inner_opt,\n            args.outer_opt,\n            args.inner_kwargs,\n            args.outer_kwargs,\n            args.meta_kwargs,\n            criterion)\n\n    if args.meta_model.lower() == 'leap':\n        return LeapWrapper(\n            model,\n            args.inner_opt,\n            args.outer_opt,\n            args.inner_kwargs,\n            args.outer_kwargs,\n            args.meta_kwargs,\n            criterion,\n        )\n\n    if args.meta_model.lower() == 'no':\n        return NoWrapper(\n            model,\n            args.inner_opt,\n            args.inner_kwargs,\n            criterion,\n        )\n\n    if args.meta_model.lower() == 'ft':\n        return FtWrapper(\n            model,\n            args.inner_opt,\n            args.inner_kwargs,\n            criterion,\n        )\n\n    if args.meta_model.lower() == 'fomaml':\n        return FOMAMLWrapper(\n            model,\n            args.inner_opt,\n            args.outer_opt,\n            args.inner_kwargs,\n            args.outer_kwargs,\n            criterion,\n        )\n\n    if args.meta_model.lower() == 'reptile':\n        return ReptileWrapper(\n            model,\n            args.inner_opt,\n            args.outer_opt,\n            args.inner_kwargs,\n            args.outer_kwargs,\n            criterion,\n        )\n\n    if args.meta_model.lower() == 'maml':\n        return MAMLWrapper(\n            model,\n            args.inner_opt,\n            args.outer_opt,\n            args.inner_kwargs,\n            args.outer_kwargs,\n            criterion,\n        )\n    raise NotImplementedError('Meta-learner {} unknown.'.format(\n        args.meta_model.lower()))", "fn_id": 0, "class_fn": false, "repo": "rcmalli/warpgrad", "file": "src/omniglot/model.py", "last_update_at": "2022-02-10T12:59:49+00:00", "question_id": "4615a7bb2082ea09229eed4fac215fecde9fb19c_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_model(args, criterion):\n    \"\"\"Construct model from main args\"\"\"\n    kwargs = dict(num_classes=args.classes, num_layers=args.num_layers, kernel_size=args.kernel_size, num_filters=args.num_filters, imsize=args.imsize, padding=args.padding, batch_norm=args.batch_norm, multi_head=args.multi_head)\n    if 'warp' in args.meta_model.lower():\n        model = WarpedOmniConv(warp_num_layers=args.warp_num_layers, warp_num_filters=args.warp_num_filters, warp_residual_connection=args.warp_residual, warp_act_fun=args.warp_act_fun, warp_batch_norm=args.warp_batch_norm, warp_final_head=args.warp_final_head, **kwargs)\n    else:\n        model = OmniConv(**kwargs)\n    if args.cuda:\n        model = model.cuda()\n    if args.log_ival > 0:\n        print(model)\n    if 'warp' in args.meta_model.lower():\n        return WarpGradWrapper(model, args.inner_opt, args.outer_opt, args.inner_kwargs, args.outer_kwargs, args.meta_kwargs, criterion)\n    if args.meta_model.lower() == 'leap':\n        return LeapWrapper(model, args.inner_opt, args.outer_opt, args.inner_kwargs, args.outer_kwargs, args.meta_kwargs, criterion)\n    if args.meta_model.lower() == 'no':\n        return NoWrapper(model, args.inner_opt, args.inner_kwargs, criterion)\n    if args.meta_model.lower() == 'ft':\n        return FtWrapper(model, args.inner_opt, args.inner_kwargs, criterion)\n    if args.meta_model.lower() == 'fomaml':\n        return FOMAMLWrapper(model, args.inner_opt, args.outer_opt, args.inner_kwargs, args.outer_kwargs, criterion)\n    if args.meta_model.lower() == 'reptile':\n        return ReptileWrapper(model, args.inner_opt, args.outer_opt, args.inner_kwargs, args.outer_kwargs, criterion)\n    if args.meta_model.lower() == 'maml':\n        return MAMLWrapper(model, args.inner_opt, args.outer_opt, args.inner_kwargs, args.outer_kwargs, criterion)\n"]]}
{"hexsha": "b6134939f533fdcb5ebdd86fc6456217d08305b0", "ext": "py", "lang": "Python", "content": "@connection_status\n@bot_admin\n@user_admin\n@loggable\ndef unmute(update: Update, context: CallbackContext) -> str:\n    bot, args = context.bot, context.args\n    chat = update.effective_chat\n    user = update.effective_user\n    message = update.effective_message\n\n    user_id, reason = extract_user_and_text(message, args)\n    if not user_id:\n        message.reply_text(\n            \"You'll need to either give me a username to unmute, or reply to someone to be unmuted.\"\n        )\n        return \"\"\n\n    member = chat.get_member(int(user_id))\n\n    if member.status in (\"kicked\", \"left\"):\n        message.reply_text(\n            \"This user isn't even in the chat, unmuting them won't make them talk more than they \"\n            \"already do!\",\n        )\n\n    elif (\n        member.can_send_messages\n        and member.can_send_media_messages\n        and member.can_send_other_messages\n        and member.can_add_web_page_previews\n    ):\n        message.reply_text(\"This user already has the right to speak.\")\n    else:\n        chat_permissions = ChatPermissions(\n            can_send_messages=True,\n            can_invite_users=True,\n            can_pin_messages=True,\n            can_send_polls=True,\n            can_change_info=True,\n            can_send_media_messages=True,\n            can_send_other_messages=True,\n            can_add_web_page_previews=True,\n        )\n        try:\n            bot.restrict_chat_member(chat.id, int(user_id), chat_permissions)\n        except BadRequest:\n            pass\n        bot.sendMessage(\n            chat.id,\n            \"{} [<code>{}</code>] Was Unmuted.\".format(\n                mention_html(member.user.id, member.user.first_name), member.user.id\n            ),\n            parse_mode=ParseMode.HTML,\n        )\n        return (\n            f\"<b>{html.escape(chat.title)}:</b>\\n\"\n            f\"#UNMUTE\\n\"\n            f\"<b>Admin:</b> {mention_html(user.id, user.first_name)}\\n\"\n            f\"<b>User:</b> {mention_html(member.user.id, member.user.first_name)}\"\n        )\n    return \"\"", "fn_id": 2, "class_fn": false, "repo": "Vishal324140/ElainaRobot", "file": "elaina/modules/muting.py", "last_update_at": "2022-01-31T08:44:33+00:00", "question_id": "b6134939f533fdcb5ebdd86fc6456217d08305b0_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@connection_status\n@bot_admin\n@user_admin\n@loggable\ndef unmute(update: Update, context: CallbackContext) -> str:\n    bot, args = (context.bot, context.args)\n    chat = update.effective_chat\n    user = update.effective_user\n    message = update.effective_message\n    user_id, reason = extract_user_and_text(message, args)\n    if not user_id:\n        message.reply_text(\"You'll need to either give me a username to unmute, or reply to someone to be unmuted.\")\n        return ''\n    member = chat.get_member(int(user_id))\n    if member.status in ('kicked', 'left'):\n        message.reply_text(\"This user isn't even in the chat, unmuting them won't make them talk more than they already do!\")\n    elif member.can_send_messages and member.can_send_media_messages and member.can_send_other_messages and member.can_add_web_page_previews:\n        message.reply_text('This user already has the right to speak.')\n    else:\n        chat_permissions = ChatPermissions(can_send_messages=True, can_invite_users=True, can_pin_messages=True, can_send_polls=True, can_change_info=True, can_send_media_messages=True, can_send_other_messages=True, can_add_web_page_previews=True)\n        try:\n            bot.restrict_chat_member(chat.id, int(user_id), chat_permissions)\n        except BadRequest:\n            pass\n        bot.sendMessage(chat.id, '{} [<code>{}</code>] Was Unmuted.'.format(mention_html(member.user.id, member.user.first_name), member.user.id), parse_mode=ParseMode.HTML)\n        return f'<b>{html.escape(chat.title)}:</b>\\n#UNMUTE\\n<b>Admin:</b> {mention_html(user.id, user.first_name)}\\n<b>User:</b> {mention_html(member.user.id, member.user.first_name)}'\n"]]}
{"hexsha": "676b8b8fa9edb2e0448a867726f93705fda631cd", "ext": "py", "lang": "Python", "content": "@auth.route('/login', methods=['POST'])\ndef login():\n    post_data = request.get_json()\n    username = post_data.get('username')\n    secret = post_data.get('secret')\n\n    regx = re.compile('^{}$'.format(username), re.IGNORECASE)\n    user_dict = mongo.db.users.find_one({'username': regx})\n    user_secret = secret\n\n    if is_authenticated(user_dict, user_secret):\n        user = User(user_dict['username'], user_secret)\n        flask_login.login_user(user)\n        resp = {\n            'status': 0,\n            'authenticated': True,\n            'message': 'Successfully logged in.',\n            'user': {\n                'username': user.username,\n            }\n        }\n        return jsonify(resp), 200\n\n    resp = {\n        'status': 'fail',\n        'authenticated': False,\n        'message': 'Bad login'\n    }\n    return jsonify(resp), 401", "fn_id": 0, "class_fn": false, "repo": "torniken/easynotes", "file": "server/noteapp/auth/views.py", "last_update_at": "2022-02-18T03:40:05+00:00", "question_id": "676b8b8fa9edb2e0448a867726f93705fda631cd_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@auth.route('/login', methods=['POST'])\ndef login():\n    post_data = request.get_json()\n    username = post_data.get('username')\n    secret = post_data.get('secret')\n    regx = re.compile('^{}$'.format(username), re.IGNORECASE)\n    user_dict = mongo.db.users.find_one({'username': regx})\n    user_secret = secret\n    if is_authenticated(user_dict, user_secret):\n        user = User(user_dict['username'], user_secret)\n        flask_login.login_user(user)\n        resp = {'status': 0, 'authenticated': True, 'message': 'Successfully logged in.', 'user': {'username': user.username}}\n        return (jsonify(resp), 200)\n    resp = {'status': 'fail', 'authenticated': False, 'message': 'Bad login'}\n"]]}
{"hexsha": "0adc66960576bca6324bd7ce27dec57428ef8969", "ext": "py", "lang": "Python", "content": "def compute_power_kernel_params_k(params_k, power):\n    '''\n    returns dictionary of power kernel parameter\n    \n    params_k: dictionary for kernel params\n    power: power of the kernel to be evaluated must lie in [0.5, 1]; and needs to satisfy other conditions for different kernels \n          based on the discussion of last appendix of https://arxiv.org/pdf/2110.01593.pdf \n    \n    '''\n    \n    params_k_power = dict()\n    suffix = \"_rt\" if power == 0.5 else f\"{power}_rt\"\n    params_k_power[\"name\"]  = params_k[\"name\"] + suffix\n    \n    d = params_k[\"d\"]\n    params_k_power[\"d\"] = params_k[\"d\"]\n    \n    if \"gauss\" in params_k[\"name\"]:\n        params_k_power[\"var\"] = params_k[\"var\"] * power\n        return(params_k_power)\n    \n    if \"sinc\" in params_k[\"name\"]:\n        params_k_power[\"var\"] = params_k[\"var\"] # the var parameter doesn't change, only the kernel gets scaled; which we don't care for KT\n        return(params_k_power)\n    \n    if \"laplace\" in params_k[\"name\"]:\n        params_k_power[\"var\"] = params_k[\"var\"]\n        assert(power> d/(d+1.))\n        params_k_power[\"nu\"] = power * (d+1)/2.\n        return(params_k_power)\n    \n    if \"matern\" in params_k[\"name\"]:\n        params_k_power[\"var\"] = params_k[\"var\"]\n        nu = params_k[\"nu\"]\n        assert(power > d/(2. * nu))\n        params_k_power[\"nu\"] = power * nu\n        return(params_k_power)\n    \n    if \"imq\" in params_k[\"name\"]:  # currently we implement only sqrt imq kernels using eqn 117 from https://arxiv.org/pdf/2105.05842.pdf [our choice is valid for all nu]\n        assert(power==0.5)\n        params_k_power[\"var\"] = params_k[\"var\"]\n        params_k_power[\"nu\"] = d/4 + params_k[\"nu\"]/2.\n        return(params_k_power)\n        \n    if \"bspline\" in params_k[\"name\"]:  # taken from last appendix of https://arxiv.org/pdf/2110.01593.pdf\n        beta = params_k[\"nu\"]\n        if beta%2 == 1:\n            assert(power == 0.5)\n        if beta%2 == 0:\n            assert(power== (beta+2)/(2*beta+2))\n            params_k_power[\"var\"] = params_k[\"var\"]\n            params_k_power[\"nu\"] = int(beta/2)\n            assert(params_k_power[\"nu\"] >=0)\n        return(params_k_power)\n\n    raise ValueError(\"Unrecognized kernel name {}\".format(params_k[\"name\"]))", "fn_id": 2, "class_fn": false, "repo": "microsoft/goodpoints", "file": "examples/gkt/util_k_mmd.py", "last_update_at": "2022-02-22T00:45:41+00:00", "question_id": "0adc66960576bca6324bd7ce27dec57428ef8969_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def compute_power_kernel_params_k(params_k, power):\n    \"\"\"\n    returns dictionary of power kernel parameter\n    \n    params_k: dictionary for kernel params\n    power: power of the kernel to be evaluated must lie in [0.5, 1]; and needs to satisfy other conditions for different kernels \n          based on the discussion of last appendix of https://arxiv.org/pdf/2110.01593.pdf \n    \n    \"\"\"\n    params_k_power = dict()\n    suffix = '_rt' if power == 0.5 else f'{power}_rt'\n    params_k_power['name'] = params_k['name'] + suffix\n    d = params_k['d']\n    params_k_power['d'] = params_k['d']\n    if 'gauss' in params_k['name']:\n        params_k_power['var'] = params_k['var'] * power\n        return params_k_power\n    if 'sinc' in params_k['name']:\n        params_k_power['var'] = params_k['var']\n        return params_k_power\n    if 'laplace' in params_k['name']:\n        params_k_power['var'] = params_k['var']\n        assert power > d / (d + 1.0)\n        params_k_power['nu'] = power * (d + 1) / 2.0\n        return params_k_power\n    if 'matern' in params_k['name']:\n        params_k_power['var'] = params_k['var']\n        nu = params_k['nu']\n        assert power > d / (2.0 * nu)\n        params_k_power['nu'] = power * nu\n        return params_k_power\n    if 'imq' in params_k['name']:\n        assert power == 0.5\n        params_k_power['var'] = params_k['var']\n        params_k_power['nu'] = d / 4 + params_k['nu'] / 2.0\n        return params_k_power\n    if 'bspline' in params_k['name']:\n        beta = params_k['nu']\n        if beta % 2 == 1:\n            assert power == 0.5\n        if beta % 2 == 0:\n            assert power == (beta + 2) / (2 * beta + 2)\n            params_k_power['var'] = params_k['var']\n            params_k_power['nu'] = int(beta / 2)\n            assert params_k_power['nu'] >= 0\n        return params_k_power\n"]]}
{"hexsha": "1ce101dc7e5c518d0d0e6dd29f7bf6b707078f68", "ext": "py", "lang": "Python", "content": "def test_registry_decorator():\n    async def _command():\n        return ''\n\n    registry = CommandRegistry()\n    wrapper = registry.decorator(FirstWordTrigger('one'))\n    result = wrapper(_command)\n    assert result is _command\n    assert registry[FirstWordTrigger('one')]._command_func is result", "fn_id": 24, "class_fn": false, "repo": "hamstap85/green-eggs", "file": "tests/test_commands.py", "last_update_at": "2022-02-21T22:47:30+00:00", "question_id": "1ce101dc7e5c518d0d0e6dd29f7bf6b707078f68_24", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_registry_decorator():\n\n    async def _command():\n        return ''\n    registry = CommandRegistry()\n    wrapper = registry.decorator(FirstWordTrigger('one'))\n    result = wrapper(_command)\n    assert result is _command\n"]]}
{"hexsha": "c520b2a3a969577ea7836ad808a35f9fad24f71d", "ext": "py", "lang": "Python", "content": "def prompt_user(prompt, default=False, autoconfirm=False):\n    if autoconfirm:\n        return True\n\n    if default:\n        query = \"[Y/n]\"\n    else:\n        query = \"[y/N]\"\n    prompt = f\"{prompt} {query} \"\n\n    while True:\n        out = input(prompt).strip().lower()\n\n        if not out:\n            return default\n\n        if out.startswith(\"y\"):\n            return True\n        if out.startswith(\"n\"):\n            return False", "fn_id": 3, "class_fn": false, "repo": "homebrew-limelight/opensight", "file": "requirements.py", "last_update_at": "2022-01-15T16:19:42+00:00", "question_id": "c520b2a3a969577ea7836ad808a35f9fad24f71d_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def prompt_user(prompt, default=False, autoconfirm=False):\n    if autoconfirm:\n        return True\n    if default:\n        query = '[Y/n]'\n    else:\n        query = '[y/N]'\n    prompt = f'{prompt} {query} '\n    while True:\n        out = input(prompt).strip().lower()\n        if not out:\n            return default\n        if out.startswith('y'):\n            return True\n        if out.startswith('n'):\n"]]}
{"hexsha": "6ad4f70bf2fa6bc397cf8233ae6d66df5f1c9a9c", "ext": "py", "lang": "Python", "content": "def test_check_restore_session_check_search_path(run_reframe, tmp_path):\n    run_reframe(\n        checkpath=['unittests/resources/checks_unlisted/deps_complex.py']\n    )\n    returncode, stdout, _ = run_reframe(\n        checkpath=[f'{tmp_path}/foo'],\n        more_options=[\n            f'--restore-session={tmp_path}/report.json', '-n', 'T1', '-R'\n        ],\n        action='list'\n    )\n    assert returncode == 0\n    assert 'Found 0 check(s)' in stdout", "fn_id": 8, "class_fn": false, "repo": "CLIP-HPC/reframe", "file": "unittests/test_cli.py", "last_update_at": "2022-03-31T11:19:18+00:00", "question_id": "6ad4f70bf2fa6bc397cf8233ae6d66df5f1c9a9c_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_check_restore_session_check_search_path(run_reframe, tmp_path):\n    run_reframe(checkpath=['unittests/resources/checks_unlisted/deps_complex.py'])\n    returncode, stdout, _ = run_reframe(checkpath=[f'{tmp_path}/foo'], more_options=[f'--restore-session={tmp_path}/report.json', '-n', 'T1', '-R'], action='list')\n    assert returncode == 0\n"]]}
{"hexsha": "b0f4372f914769547fb942b23ae25089ee51ebf0", "ext": "py", "lang": "Python", "content": "def PR_curve(precisions: np.ndarray, label: list, title: str, x=None):\n    # bit = precisions.shape[1]\n    from matplotlib import pyplot as plt\n    # min_presion = np.min([np.min(l) for l in precisions])\n    # max_presion = np.max([np.max(l) for l in precisions])\n    min_presion = 0.5\n    max_presion = 1\n    plt.title(title)\n    plt.xticks(np.arange(0.1, 1.1, 0.1))\n    plt.xlabel(\"recall\")\n    plt.yticks(np.arange(round(min_presion * 10 - 1) * 0.1, (round(max_presion * 10)+1) * 0.1, 0.1))\n    plt.ylabel(\"precision\")\n    if x is None:\n        x = np.arange(0.02, 1.02, 0.02)\n        # x = np.expand_dims(x, precisions.shape)\n    colors = ['red', 'blue', 'c', 'green', 'yellow', 'black', 'lime', 'grey', 'pink', 'navy']\n    markets = ['o', 'v', '^', '>', '<', '+', 'x', '*', 'd', 'D']\n    for i in range(precisions.shape[0]):\n        # plt.plot(x[i], precisions[i, :], marker=markets[i % 10], color=colors[i % 10], label=label[i])\n        plt.plot(x[i], precisions[i], color=colors[i % 10], label=label[i])\n        # plt.plot(x, precisions[i, :], color=colors[i % 10], label=label[i])\n    plt.grid()\n    ax = plt.axes()\n    ax.set(xlim=(0, 1), ylim=(round(min_presion * 10 - 1) * 0.1, (round(max_presion * 10)) * 0.1))\n    plt.legend()\n    # plt.axes('tight')\n    plt.show()", "fn_id": 3, "class_fn": false, "repo": "chrisbyd/deep-cross-modal-hashing", "file": "torchcmh/training/valid.py", "last_update_at": "2022-03-10T09:25:45+00:00", "question_id": "b0f4372f914769547fb942b23ae25089ee51ebf0_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def PR_curve(precisions: np.ndarray, label: list, title: str, x=None):\n    from matplotlib import pyplot as plt\n    min_presion = 0.5\n    max_presion = 1\n    plt.title(title)\n    plt.xticks(np.arange(0.1, 1.1, 0.1))\n    plt.xlabel('recall')\n    plt.yticks(np.arange(round(min_presion * 10 - 1) * 0.1, (round(max_presion * 10) + 1) * 0.1, 0.1))\n    plt.ylabel('precision')\n    if x is None:\n        x = np.arange(0.02, 1.02, 0.02)\n    colors = ['red', 'blue', 'c', 'green', 'yellow', 'black', 'lime', 'grey', 'pink', 'navy']\n    markets = ['o', 'v', '^', '>', '<', '+', 'x', '*', 'd', 'D']\n    for i in range(precisions.shape[0]):\n        plt.plot(x[i], precisions[i], color=colors[i % 10], label=label[i])\n    plt.grid()\n    ax = plt.axes()\n    ax.set(xlim=(0, 1), ylim=(round(min_presion * 10 - 1) * 0.1, round(max_presion * 10) * 0.1))\n    plt.legend()\n"]]}
{"hexsha": "8566a374e758f9f41eac095d5697b11bfd12d607", "ext": "py", "lang": "Python", "content": "def makepacket(number):\n\n    \n    numberstring = str(number)\n    while len(numberstring)  < 4:\n        numberstring = \"0\" + numberstring\n    \n    passcode = binascii.hexlify(numberstring.encode()).decode()\n    packet = \"45454d5030313030455bc678040000004a00000001000000001c00000000000000ffffff00455bc6640201030005200320200001ff00ff00ff00000810000000010c00000026ab9ffbdf\" + passcode + \"000000000000000000000000ac15c508\"\n    #packet = \"45454d50303130300a9c1178040000005f00000001010000001c00000000000000ffffe0000a9c00010201030005200320200001ff00ff00ff00000810000000010c0000b0e892ecf8bc\" + passcode + \"0000000000000000000000000a9c221b1100000011000000000000000e0000000100000007\"\n    return binascii.unhexlify(packet)", "fn_id": 0, "class_fn": false, "repo": "jzj/Security-Research", "file": "exploits/Epson/easymp-bruteforcer.py", "last_update_at": "2022-03-30T18:22:24+00:00", "question_id": "8566a374e758f9f41eac095d5697b11bfd12d607_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def makepacket(number):\n    numberstring = str(number)\n    while len(numberstring) < 4:\n        numberstring = '0' + numberstring\n    passcode = binascii.hexlify(numberstring.encode()).decode()\n    packet = '45454d5030313030455bc678040000004a00000001000000001c00000000000000ffffff00455bc6640201030005200320200001ff00ff00ff00000810000000010c00000026ab9ffbdf' + passcode + '000000000000000000000000ac15c508'\n"]]}
{"hexsha": "16ab4d78e447e1448969cfed303856d05bbd1f00", "ext": "py", "lang": "Python", "content": "def grouping_assign_net(message, f):\n\t@wraps(f)\n\tdef decorated(*args, **kwargs):\n\t\tresult = f(*args, **kwargs)\n\t\tnet_id = -1\n\t\tif kwargs.get(\"net\") is None:\n\t\t\tif kwargs.get(\"net_id\") is None:\n\t\t\t\tnet_id = args[1].id if isinstance(args[1], hal_py.Net) else args[1]\n\t\t\telse:\n\t\t\t\tnet_id = kwargs.get(\"net_id\")\n\t\telse:\n\t\t\tnet_id = kwargs.get(\"net\").id\n\t\tlog_string = \"Function: {}, Grouping-ID: {}, Net-ID: {}\".format(message, args[0].id, net_id)\n\t\thal_py.log_info(log_string)\n\t\treturn result\n\treturn decorated", "fn_id": 18, "class_fn": false, "repo": "e7p/hal", "file": "tools/pydecorator.py", "last_update_at": "2022-03-31T15:52:30+00:00", "question_id": "16ab4d78e447e1448969cfed303856d05bbd1f00_18", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def grouping_assign_net(message, f):\n\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        result = f(*args, **kwargs)\n        net_id = -1\n        if kwargs.get('net') is None:\n            if kwargs.get('net_id') is None:\n                net_id = args[1].id if isinstance(args[1], hal_py.Net) else args[1]\n            else:\n                net_id = kwargs.get('net_id')\n        else:\n            net_id = kwargs.get('net').id\n        log_string = 'Function: {}, Grouping-ID: {}, Net-ID: {}'.format(message, args[0].id, net_id)\n        hal_py.log_info(log_string)\n        return result\n"]]}
{"hexsha": "544e6f379b865573cbab1bed2ad04c56669e1ffb", "ext": "py", "lang": "Python", "content": "def source(*functions):\n    \"\"\" source a script as if we have written the script in jupyter notebook and executed it \"\"\"\n    source_code = '\\n\\n'.join(getsource(fn) for fn in functions)        \n    display(HTML(highlight(source_code, PythonLexer(), HtmlFormatter(full=True))))", "fn_id": 0, "class_fn": false, "repo": "SmartMobilityAlgorithms/utilities", "file": "src/jupyter.py", "last_update_at": "2022-01-24T17:17:07+00:00", "question_id": "544e6f379b865573cbab1bed2ad04c56669e1ffb_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def source(*functions):\n    \"\"\" source a script as if we have written the script in jupyter notebook and executed it \"\"\"\n    source_code = '\\n\\n'.join((getsource(fn) for fn in functions))\n"]]}
{"hexsha": "31b1d4edbe65d07219fd1e69499b5abc84f7382e", "ext": "py", "lang": "Python", "content": "def test_UI_GIVEN_user_selects_entire_shape_WHEN_choosing_pixel_layout_THEN_pixel_mapping_becomes_visible(\n    qtbot, template, pixel_options\n):\n    # Press the entire shape button under pixel layout\n    systematic_button_press(qtbot, template, pixel_options.entire_shape_radio_button)\n\n    # Check that the pixel mapping items are visible\n    assert pixel_options.pixel_options_stack.isVisible()\n    assert pixel_options.pixel_options_stack.currentIndex() == 1", "fn_id": 12, "class_fn": false, "repo": "ess-dmsc/nexus-constructor", "file": "tests/ui_tests/test_ui_pixel_options_widget.py", "last_update_at": "2022-01-06T09:23:21+00:00", "question_id": "31b1d4edbe65d07219fd1e69499b5abc84f7382e_12", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_UI_GIVEN_user_selects_entire_shape_WHEN_choosing_pixel_layout_THEN_pixel_mapping_becomes_visible(qtbot, template, pixel_options):\n    systematic_button_press(qtbot, template, pixel_options.entire_shape_radio_button)\n    assert pixel_options.pixel_options_stack.isVisible()\n"]]}
{"hexsha": "7e804b969e5bdce2e4c58f80048e6012e996d170", "ext": "py", "lang": "Python", "content": "def update_s3_dir(audio_url, orca, validation):\n    calls_path = 'calls' if orca else 'nocalls'\n    validation_path = 'validation' if validation else 'train'\n    filename = audio_url.split('/')[-1].split('.')[0]\n    subprocess.run([\n        'aws', 's3', 'mv', f'{s3_unlabeled_path}spectrograms/{filename}.png',\n        f'{s3_labeled_path}{validation_path}/{calls_path}/'\n    ])\n    subprocess.run([\n        'aws', 's3', 'mv', f'{s3_unlabeled_path}mp3/{filename}.mp3',\n        f'{s3_labeled_path}mp3/{calls_path}/'\n    ])\n    return f'{s3_url}/mp3/{calls_path}/{filename}.mp3'", "fn_id": 1, "class_fn": false, "repo": "Molkree/orcaal", "file": "api/app/active_learning.py", "last_update_at": "2022-01-21T21:23:37+00:00", "question_id": "7e804b969e5bdce2e4c58f80048e6012e996d170_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def update_s3_dir(audio_url, orca, validation):\n    calls_path = 'calls' if orca else 'nocalls'\n    validation_path = 'validation' if validation else 'train'\n    filename = audio_url.split('/')[-1].split('.')[0]\n    subprocess.run(['aws', 's3', 'mv', f'{s3_unlabeled_path}spectrograms/{filename}.png', f'{s3_labeled_path}{validation_path}/{calls_path}/'])\n    subprocess.run(['aws', 's3', 'mv', f'{s3_unlabeled_path}mp3/{filename}.mp3', f'{s3_labeled_path}mp3/{calls_path}/'])\n"]]}
{"hexsha": "36b03bf2c2032a87bbc96e3c4f9fd883ffc4b08c", "ext": "py", "lang": "Python", "content": "def events(request, response=None):\n    \"\"\"\n    Returns a list of events in the CLA system.\n    if parameters are passed returns filtered lists\n\n    :return: List of events in dict format\n    \"\"\"\n\n    event = get_event_instance()\n    events = [event.to_dict() for event in event.all()]\n    if request.params:\n        results = event.search_events(**request.params)\n        if results:\n            events = [ev.to_dict() for ev in results]\n        else:\n            # return empty list if search fails\n            response.status = HTTP_404\n            return {\"events\": []}\n\n    return {\"events\": events}", "fn_id": 0, "class_fn": false, "repo": "rinkeshbhutwala/easycla", "file": "cla-backend/cla/controllers/event.py", "last_update_at": "2022-03-29T20:42:55+00:00", "question_id": "36b03bf2c2032a87bbc96e3c4f9fd883ffc4b08c_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def events(request, response=None):\n    \"\"\"\n    Returns a list of events in the CLA system.\n    if parameters are passed returns filtered lists\n\n    :return: List of events in dict format\n    \"\"\"\n    event = get_event_instance()\n    events = [event.to_dict() for event in event.all()]\n    if request.params:\n        results = event.search_events(**request.params)\n        if results:\n            events = [ev.to_dict() for ev in results]\n        else:\n            response.status = HTTP_404\n            return {'events': []}\n"]]}
{"hexsha": "bdc9a66b8f4282a6afe68d2be2e3f041955203ab", "ext": "py", "lang": "Python", "content": "def test_pupfile():\n    with http_server(directory=CWD):\n        # with nullcontext():\n        url = \"http://localhost:38080/ps3updat-cex-3.55.pup\"\n        fh = HTTPFile(url)\n        # pup_path = importlib.resources.files(__package__) / \"ps3updat-cex-3.55.pup\"\n        # fh = open(pup_path, 'rb')\n        pupf = PUPFile(fh)\n        pupf.rootfs.dump()", "fn_id": 1, "class_fn": false, "repo": "jevinskie/ps3mfw-ng", "file": "tests/ps3mfw/test_pup.py", "last_update_at": "2022-03-09T05:22:16+00:00", "question_id": "bdc9a66b8f4282a6afe68d2be2e3f041955203ab_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_pupfile():\n    with http_server(directory=CWD):\n        url = 'http://localhost:38080/ps3updat-cex-3.55.pup'\n        fh = HTTPFile(url)\n        pupf = PUPFile(fh)\n"]]}
{"hexsha": "142745156023a2dec6586b6ac306c21912cfd8de", "ext": "py", "lang": "Python", "content": "def tag(tags, api_version=None, ids=None, is_incremental=None, latest_include_preview=None, name=None, namespace=None, parent=None, resource_group=None, resource_type=None):\n    '''\n    Tag a resource.\n\n    Required Parameters:\n    - tags -- space-separated tags: key[=value] [key[=value] ...]. Use '' to clear existing tags.\n\n    Optional Parameters:\n    - api_version -- The api version of the resource (omit for the latest stable version)\n    - ids -- One or more resource IDs (space-delimited). If provided, no other \"Resource Id\" arguments should be specified.\n    - is_incremental -- The option to add tags incrementally without deleting the original tags. If the key of new tag and original tag are duplicated, the original value will be overwritten.\n    - latest_include_preview -- Indicate that the latest api-version will be used regardless of whether it is preview version (like 2020-01-01-preview) or not. For example, if the supported api-version of resource provider is 2020-01-01-preview and 2019-01-01: when passing in this parameter it will take the latest version 2020-01-01-preview, otherwise it will take the latest stable version 2019-01-01 without passing in this parameter\n    - name -- The resource name. (Ex: myC)\n    - namespace -- Provider namespace (Ex: 'Microsoft.Provider')\n    - parent -- The parent path (Ex: 'resA/myA/resB/myB')\n    - resource_group -- Name of resource group. You can configure the default group using `az configure --defaults group=<name>`\n    - resource_type -- The resource type (Ex: 'resC'). Can also accept namespace/type format (Ex: 'Microsoft.Provider/resC')\n    '''\n    return _call_az(\"az resource tag\", locals())", "fn_id": 4, "class_fn": false, "repo": "py-az-cli/py-az-cli", "file": "pyaz/resource/__init__.py", "last_update_at": "2022-02-03T09:12:01+00:00", "question_id": "142745156023a2dec6586b6ac306c21912cfd8de_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def tag(tags, api_version=None, ids=None, is_incremental=None, latest_include_preview=None, name=None, namespace=None, parent=None, resource_group=None, resource_type=None):\n    \"\"\"\n    Tag a resource.\n\n    Required Parameters:\n    - tags -- space-separated tags: key[=value] [key[=value] ...]. Use '' to clear existing tags.\n\n    Optional Parameters:\n    - api_version -- The api version of the resource (omit for the latest stable version)\n    - ids -- One or more resource IDs (space-delimited). If provided, no other \"Resource Id\" arguments should be specified.\n    - is_incremental -- The option to add tags incrementally without deleting the original tags. If the key of new tag and original tag are duplicated, the original value will be overwritten.\n    - latest_include_preview -- Indicate that the latest api-version will be used regardless of whether it is preview version (like 2020-01-01-preview) or not. For example, if the supported api-version of resource provider is 2020-01-01-preview and 2019-01-01: when passing in this parameter it will take the latest version 2020-01-01-preview, otherwise it will take the latest stable version 2019-01-01 without passing in this parameter\n    - name -- The resource name. (Ex: myC)\n    - namespace -- Provider namespace (Ex: 'Microsoft.Provider')\n    - parent -- The parent path (Ex: 'resA/myA/resB/myB')\n    - resource_group -- Name of resource group. You can configure the default group using `az configure --defaults group=<name>`\n    - resource_type -- The resource type (Ex: 'resC'). Can also accept namespace/type format (Ex: 'Microsoft.Provider/resC')\n    \"\"\"\n"]]}
{"hexsha": "6a0e03795a8c2b6bc0e5a0259453faf1fc5beb60", "ext": "py", "lang": "Python", "content": "def _get_step_number(method_name):\n    match = re.match(LAB_METHOD_NAME_REGEX, method_name)\n    if match is None:\n        return None\n    return int(match.groups()[0])", "fn_id": 0, "class_fn": false, "repo": "zapatero-cldr/edge2ai-workshop", "file": "setup/terraform/resources/labs/__init__.py", "last_update_at": "2022-02-11T14:52:46+00:00", "question_id": "6a0e03795a8c2b6bc0e5a0259453faf1fc5beb60_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _get_step_number(method_name):\n    match = re.match(LAB_METHOD_NAME_REGEX, method_name)\n    if match is None:\n        return None\n"]]}
{"hexsha": "ffe89aee829efc4f673ecb63fcd56a9a2fe4a41e", "ext": "py", "lang": "Python", "content": "def get_play(game_id, options, fight_id, status, client_state, version):\n    \"botfights get_play function for countdown\"\n    if None == client_state:\n        return None\n    plays = {}\n    for round_id, round_data in client_state['rounds'].items():\n        target = round_data[0]\n        operands = round_data[1]\n        code = get_play_adder(target, operands)\n        plays[round_id] = code\n    return plays", "fn_id": 1, "class_fn": false, "repo": "botfights/botfights-sdk", "file": "countdown/python/sample-bot.py", "last_update_at": "2022-02-05T17:37:32+00:00", "question_id": "ffe89aee829efc4f673ecb63fcd56a9a2fe4a41e_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_play(game_id, options, fight_id, status, client_state, version):\n    \"\"\"botfights get_play function for countdown\"\"\"\n    if None == client_state:\n        return None\n    plays = {}\n    for round_id, round_data in client_state['rounds'].items():\n        target = round_data[0]\n        operands = round_data[1]\n        code = get_play_adder(target, operands)\n        plays[round_id] = code\n"]]}
{"hexsha": "41a777a7c2c3b3cd8331177bcc54b8e534199a39", "ext": "py", "lang": "Python", "content": "def find_answer_starts(doc_toks, ans_toks):\n    # CONSIDER: handle also Europe/European, portrait/portraits\n    # also cases like South Korean/South Korea\n    starts = []\n    anslen = len(ans_toks)\n    for s in range(len(doc_toks)-(anslen-1)):\n        if ans_toks == doc_toks[s:s+anslen]:\n            starts.append(s)\n    return starts", "fn_id": 2, "class_fn": false, "repo": "IBM/span-selection-pretraining", "file": "sspt/rc_data.py", "last_update_at": "2022-03-04T02:23:10+00:00", "question_id": "41a777a7c2c3b3cd8331177bcc54b8e534199a39_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def find_answer_starts(doc_toks, ans_toks):\n    starts = []\n    anslen = len(ans_toks)\n    for s in range(len(doc_toks) - (anslen - 1)):\n        if ans_toks == doc_toks[s:s + anslen]:\n            starts.append(s)\n"]]}
{"hexsha": "6997961e6b821ca491045838b32c2410bcbe738d", "ext": "py", "lang": "Python", "content": "@pytest.fixture\ndef get_include_paths():\n    with mock.patch.object(factory, 'get_include_paths') as patch:\n        patch.return_value = []\n        yield patch", "fn_id": 3, "class_fn": false, "repo": "dbt-labs/dbt-redshift", "file": "tests/unit/test_context.py", "last_update_at": "2022-03-31T20:32:12+00:00", "question_id": "6997961e6b821ca491045838b32c2410bcbe738d_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.fixture\ndef get_include_paths():\n    with mock.patch.object(factory, 'get_include_paths') as patch:\n        patch.return_value = []\n"]]}
{"hexsha": "a29848e6f69b4f35cafc72b8a84fea6a83f8192f", "ext": "py", "lang": "Python", "content": "def get_labelname(labelmap, labels):\n    num_labels = len(labelmap)\n    labelnames = []\n    if type(labels) is not list:\n        labels = [labels]\n    for label in labels:\n        labelnames.append(labelmap[int(label)])\n    return labelnames", "fn_id": 0, "class_fn": false, "repo": "predrag12/powerai", "file": "vision/tensorrt-samples/samples/python/sampleSSD/detector_deploy.py", "last_update_at": "2022-01-07T22:36:00+00:00", "question_id": "a29848e6f69b4f35cafc72b8a84fea6a83f8192f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_labelname(labelmap, labels):\n    num_labels = len(labelmap)\n    labelnames = []\n    if type(labels) is not list:\n        labels = [labels]\n    for label in labels:\n        labelnames.append(labelmap[int(label)])\n"]]}
{"hexsha": "f51b1c7f0c17fcfb1632176cae5ed2d15d54f5a6", "ext": "py", "lang": "Python", "content": "def _handle_login_response(request):\n    \"\"\" This function is used to get the login response of authorization request from microsoft login page.\n\n    :param request: Data given to REST endpoint\n    :return: HttpResponse. The response displayed on authorization URL page\n    \"\"\"\n\n    asset_id = request.GET.get('state')\n    if not asset_id:\n        return HttpResponse('ERROR: Asset ID not found in URL\\n{}'.format(json.dumps(request.GET)), content_type=\"text/plain\", status=400)\n\n    # Check for error in URL\n    error = request.GET.get('error')\n    error_description = request.GET.get('error_description')\n\n    # If there is an error in response\n    if error:\n        message = 'Error: {0}'.format(error)\n        if error_description:\n            message = '{0} Details: {1}'.format(message, error_description)\n        return HttpResponse('Server returned {0}'.format(message), content_type=\"text/plain\", status=400)\n\n    code = request.GET.get('code')\n    admin_consent = request.GET.get('admin_consent')\n\n    # If none of the code or admin_consent is available\n    if not (code or admin_consent):\n        return HttpResponse('Error while authenticating\\n{0}'.format(json.dumps(request.GET)), content_type=\"text/plain\", status=400)\n\n    state = _load_app_state(asset_id)\n\n    # If value of admin_consent is available\n    if admin_consent:\n        if admin_consent == 'True':\n            admin_consent = True\n        else:\n            admin_consent = False\n\n        state['admin_consent'] = admin_consent\n        _save_app_state(state, asset_id, None)\n\n        # If admin_consent is True\n        if admin_consent:\n            return HttpResponse('Admin Consent received. Please close this window.', content_type=\"text/plain\")\n        return HttpResponse('Admin Consent declined. Please close this window and try again later.', content_type=\"text/plain\", status=400)\n\n    # If value of admin_consent is not available, value of code is available\n    state['code'] = code\n    try:\n        state['code'] = MicrosoftTeamConnector().encrypt_state(code, \"code\")\n        state[MSTEAMS_STATE_IS_ENCRYPTED] = True\n    except Exception as e:\n        return HttpResponse(\"{}: {}\".format(MSTEAMS_ENCRYPTION_ERR, str(e)), content_type=\"text/plain\", status=400)\n    _save_app_state(state, asset_id, None)\n\n    return HttpResponse('Code received. Please close this window, the action will continue to get new token.', content_type=\"text/plain\")", "fn_id": 5, "class_fn": false, "repo": "splunk-soar-connectors/microsoftteams", "file": "microsoftteams_connector.py", "last_update_at": "2022-02-13T16:35:44+00:00", "question_id": "f51b1c7f0c17fcfb1632176cae5ed2d15d54f5a6_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _handle_login_response(request):\n    \"\"\" This function is used to get the login response of authorization request from microsoft login page.\n\n    :param request: Data given to REST endpoint\n    :return: HttpResponse. The response displayed on authorization URL page\n    \"\"\"\n    asset_id = request.GET.get('state')\n    if not asset_id:\n        return HttpResponse('ERROR: Asset ID not found in URL\\n{}'.format(json.dumps(request.GET)), content_type='text/plain', status=400)\n    error = request.GET.get('error')\n    error_description = request.GET.get('error_description')\n    if error:\n        message = 'Error: {0}'.format(error)\n        if error_description:\n            message = '{0} Details: {1}'.format(message, error_description)\n        return HttpResponse('Server returned {0}'.format(message), content_type='text/plain', status=400)\n    code = request.GET.get('code')\n    admin_consent = request.GET.get('admin_consent')\n    if not (code or admin_consent):\n        return HttpResponse('Error while authenticating\\n{0}'.format(json.dumps(request.GET)), content_type='text/plain', status=400)\n    state = _load_app_state(asset_id)\n    if admin_consent:\n        if admin_consent == 'True':\n            admin_consent = True\n        else:\n            admin_consent = False\n        state['admin_consent'] = admin_consent\n        _save_app_state(state, asset_id, None)\n        if admin_consent:\n            return HttpResponse('Admin Consent received. Please close this window.', content_type='text/plain')\n        return HttpResponse('Admin Consent declined. Please close this window and try again later.', content_type='text/plain', status=400)\n    state['code'] = code\n    try:\n        state['code'] = MicrosoftTeamConnector().encrypt_state(code, 'code')\n        state[MSTEAMS_STATE_IS_ENCRYPTED] = True\n    except Exception as e:\n        return HttpResponse('{}: {}'.format(MSTEAMS_ENCRYPTION_ERR, str(e)), content_type='text/plain', status=400)\n    _save_app_state(state, asset_id, None)\n"]]}
{"hexsha": "6a8f6859f816b4625cf3dab39935f25bd872f783", "ext": "py", "lang": "Python", "content": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('input_file_name')\n    parser.add_argument('temp_file_name')\n\n    args, extra_args = parser.parse_known_args()\n    run_test_once(args, extra_args)", "fn_id": 1, "class_fn": false, "repo": "mkinsner/llvm", "file": "clang/test/Analysis/check-analyzer-fixit.py", "last_update_at": "2022-03-31T11:00:37+00:00", "question_id": "6a8f6859f816b4625cf3dab39935f25bd872f783_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('input_file_name')\n    parser.add_argument('temp_file_name')\n    args, extra_args = parser.parse_known_args()\n"]]}
{"hexsha": "21ff7f47cbd4b6369c570afb17c5e9d9fc010ab4", "ext": "py", "lang": "Python", "content": "def heapSort(arr):  # in-place | not-stable\n    # Time Complexity O(nlogn) | Space Complexity O(1)\n\n    def heapify(arr, n, i):  # Max Heap\n        largest = i  # \ud2b8\ub9ac\uc5d0\uc11c \uac00\uc7a5 \ud070 \uac12 \ucc3e\uae30\n        l = 2 * i + 1  # Left Node\n        r = 2 * i + 2  # Right Node\n\n        if l < n and arr[largest] < arr[l]:\n            largest = l\n\n        if r < n and arr[largest] < arr[r]:\n            largest = r\n\n        # root\uac00 \ucd5c\ub300\uac00 \uc544\ub2c8\uba74\n        # \ucd5c\ub300 \uac12\uacfc \ubc14\uafb8\uace0, \uacc4\uc18d heapify\n        if largest != i:\n            arr[i], arr[largest] = arr[largest], arr[i]\n            heapify(arr, n, largest)\n\n    n = len(arr)\n\n    for i in range(n // 2, -1, -1):\n        heapify(arr, n, i)\n\n    for i in range(n - 1, 0, -1):\n        arr[i], arr[0] = arr[0], arr[i]\n        # Heapify root element\n        heapify(arr, i, 0)\n\n    return arr", "fn_id": 11, "class_fn": false, "repo": "Alfex4936/python-bigO-calculator", "file": "bigO/algorithm.py", "last_update_at": "2022-03-10T19:26:50+00:00", "question_id": "21ff7f47cbd4b6369c570afb17c5e9d9fc010ab4_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def heapSort(arr):\n\n    def heapify(arr, n, i):\n        largest = i\n        l = 2 * i + 1\n        r = 2 * i + 2\n        if l < n and arr[largest] < arr[l]:\n            largest = l\n        if r < n and arr[largest] < arr[r]:\n            largest = r\n        if largest != i:\n            arr[i], arr[largest] = (arr[largest], arr[i])\n            heapify(arr, n, largest)\n    n = len(arr)\n    for i in range(n // 2, -1, -1):\n        heapify(arr, n, i)\n    for i in range(n - 1, 0, -1):\n        arr[i], arr[0] = (arr[0], arr[i])\n        heapify(arr, i, 0)\n"]]}
{"hexsha": "dbd2459f38ac87dc671cbf3243129d7877741767", "ext": "py", "lang": "Python", "content": "def post_net(inputs, is_train):\n\n    inputs = tf.reshape(inputs, [config.batch_size, config.max_phr_len , 1, -1])\n\n    conv_1 = tf.nn.tanh(tf.layers.batch_normalization(tf.layers.conv2d(inputs, 512, (5,1), name = \"post_conv_1\",padding='same'), training = is_train, name = \"post_conv_1_BN\"))\n\n    conv_2 = tf.nn.tanh(tf.layers.batch_normalization(tf.layers.conv2d(conv_1, 512, (5,1), name = \"post_conv_2\",padding='same'), training = is_train, name = \"post_conv_2_BN\"))\n\n    conv_3 = tf.nn.tanh(tf.layers.batch_normalization(tf.layers.conv2d(conv_2, 512, (5,1), name = \"post_conv_3\",padding='same'), training = is_train, name = \"post_conv_3_BN\"))   \n\n    conv_4 = tf.nn.tanh(tf.layers.batch_normalization(tf.layers.conv2d(conv_3, 512, (5,1), name = \"post_conv_4\",padding='same'), training = is_train, name = \"post_conv_4_BN\"))\n\n    output = tf.layers.conv2d(conv_4, config.num_features, (5,1), name = \"post_conv_5\",padding='same')\n\n    return tf.squeeze(output)", "fn_id": 6, "class_fn": false, "repo": "MTG/content_choral_separation", "file": "synth/modules/autovc.py", "last_update_at": "2022-03-20T11:21:58+00:00", "question_id": "dbd2459f38ac87dc671cbf3243129d7877741767_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def post_net(inputs, is_train):\n    inputs = tf.reshape(inputs, [config.batch_size, config.max_phr_len, 1, -1])\n    conv_1 = tf.nn.tanh(tf.layers.batch_normalization(tf.layers.conv2d(inputs, 512, (5, 1), name='post_conv_1', padding='same'), training=is_train, name='post_conv_1_BN'))\n    conv_2 = tf.nn.tanh(tf.layers.batch_normalization(tf.layers.conv2d(conv_1, 512, (5, 1), name='post_conv_2', padding='same'), training=is_train, name='post_conv_2_BN'))\n    conv_3 = tf.nn.tanh(tf.layers.batch_normalization(tf.layers.conv2d(conv_2, 512, (5, 1), name='post_conv_3', padding='same'), training=is_train, name='post_conv_3_BN'))\n    conv_4 = tf.nn.tanh(tf.layers.batch_normalization(tf.layers.conv2d(conv_3, 512, (5, 1), name='post_conv_4', padding='same'), training=is_train, name='post_conv_4_BN'))\n    output = tf.layers.conv2d(conv_4, config.num_features, (5, 1), name='post_conv_5', padding='same')\n"]]}
{"hexsha": "d75e4ebf9a812335b7560c458cab8467af7eb8ee", "ext": "py", "lang": "Python", "content": "def jac(X, B, g):\n  N = B.shape[0]\n  SI = X.reshape((2,N))\n  S = SI[0]\n  I = SI[1]\n\n  # derivative of f_S\n  A1 = -  np.diag(np.einsum('ij,j->i', B, I))\n  A2 = - np.einsum('ij,i->ij', B, S)\n  A = np.concatenate([A1, A2], axis=1)\n\n  # derivative of f_I\n  B1 = -A1\n  B2 = -A2 - g*np.eye(N)\n  B = np.concatenate([B1, B2], axis=1)\n\n  return np.concatenate([A,B], axis=0)", "fn_id": 9, "class_fn": false, "repo": "gletreut/epidemiology_flux_model", "file": "code/functions.py", "last_update_at": "2022-02-07T10:56:51+00:00", "question_id": "d75e4ebf9a812335b7560c458cab8467af7eb8ee_9", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def jac(X, B, g):\n    N = B.shape[0]\n    SI = X.reshape((2, N))\n    S = SI[0]\n    I = SI[1]\n    A1 = -np.diag(np.einsum('ij,j->i', B, I))\n    A2 = -np.einsum('ij,i->ij', B, S)\n    A = np.concatenate([A1, A2], axis=1)\n    B1 = -A1\n    B2 = -A2 - g * np.eye(N)\n    B = np.concatenate([B1, B2], axis=1)\n"]]}
{"hexsha": "63515e40a6b59b2f9111de23a8cf0af9fbe36523", "ext": "py", "lang": "Python", "content": "def statespace(endog, exog=None, order=(0, 0, 0),\n               seasonal_order=(0, 0, 0, 0), include_constant=True,\n               enforce_stationarity=True, enforce_invertibility=True,\n               concentrate_scale=False, start_params=None, fit_kwargs=None):\n    \"\"\"\n    Estimate SARIMAX parameters using state space methods.\n\n    Parameters\n    ----------\n    endog : array_like\n        Input time series array.\n    order : tuple, optional\n        The (p,d,q) order of the model for the number of AR parameters,\n        differences, and MA parameters. Default is (0, 0, 0).\n    seasonal_order : tuple, optional\n        The (P,D,Q,s) order of the seasonal component of the model for the\n        AR parameters, differences, MA parameters, and periodicity. Default\n        is (0, 0, 0, 0).\n    include_constant : bool, optional\n        Whether to add a constant term in `exog` if it's not already there.\n        The estimate of the constant will then appear as one of the `exog`\n        parameters. If `exog` is None, then the constant will represent the\n        mean of the process.\n    enforce_stationarity : bool, optional\n        Whether or not to transform the AR parameters to enforce stationarity\n        in the autoregressive component of the model. Default is True.\n    enforce_invertibility : bool, optional\n        Whether or not to transform the MA parameters to enforce invertibility\n        in the moving average component of the model. Default is True.\n    concentrate_scale : bool, optional\n        Whether or not to concentrate the scale (variance of the error term)\n        out of the likelihood. This reduces the number of parameters estimated\n        by maximum likelihood by one.\n    start_params : array_like, optional\n        Initial guess of the solution for the loglikelihood maximization. The\n        AR polynomial must be stationary. If `enforce_invertibility=True` the\n        MA poylnomial must be invertible. If not provided, default starting\n        parameters are computed using the Hannan-Rissanen method.\n    fit_kwargs : dict, optional\n        Arguments to pass to the state space model's `fit` method.\n\n    Returns\n    -------\n    parameters : SARIMAXParams object\n    other_results : Bunch\n        Includes two components, `spec`, containing the `SARIMAXSpecification`\n        instance corresponding to the input arguments; and\n        `state_space_results`, corresponding to the results from the underlying\n        state space model and Kalman filter / smoother.\n\n    Notes\n    -----\n    The primary reference is [1]_.\n\n    References\n    ----------\n    .. [1] Durbin, James, and Siem Jan Koopman. 2012.\n       Time Series Analysis by State Space Methods: Second Edition.\n       Oxford University Press.\n    \"\"\"\n    # Handle including the constant (need to do it now so that the constant\n    # parameter can be included in the specification as part of `exog`.)\n    if include_constant:\n        exog = np.ones_like(endog) if exog is None else add_constant(exog)\n\n    # Create the specification\n    spec = SARIMAXSpecification(\n        endog, exog=exog, order=order, seasonal_order=seasonal_order,\n        enforce_stationarity=enforce_stationarity,\n        enforce_invertibility=enforce_invertibility,\n        concentrate_scale=concentrate_scale)\n    endog = spec.endog\n    exog = spec.exog\n    p = SARIMAXParams(spec=spec)\n\n    # Check start parameters\n    if start_params is not None:\n        sp = SARIMAXParams(spec=spec)\n        sp.params = start_params\n\n        if spec.enforce_stationarity and not sp.is_stationary:\n            raise ValueError('Given starting parameters imply a non-stationary'\n                             ' AR process with `enforce_stationarity=True`.')\n\n        if spec.enforce_invertibility and not sp.is_invertible:\n            raise ValueError('Given starting parameters imply a non-invertible'\n                             ' MA process with `enforce_invertibility=True`.')\n\n    # Create and fit the state space model\n    mod = SARIMAX(endog, exog=exog, order=spec.order,\n                  seasonal_order=spec.seasonal_order,\n                  enforce_stationarity=spec.enforce_stationarity,\n                  enforce_invertibility=spec.enforce_invertibility,\n                  concentrate_scale=spec.concentrate_scale)\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    fit_kwargs.setdefault('disp', 0)\n    res_ss = mod.fit(start_params=start_params, **fit_kwargs)\n\n    # Construct results\n    p.params = res_ss.params\n    res = Bunch({\n        'spec': spec,\n        'statespace_results': res_ss,\n    })\n\n    return p, res", "fn_id": 0, "class_fn": false, "repo": "EkremBayar/bayar", "file": "venv/Lib/site-packages/statsmodels/tsa/arima/estimators/statespace.py", "last_update_at": "2022-03-31T17:03:24+00:00", "question_id": "63515e40a6b59b2f9111de23a8cf0af9fbe36523_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def statespace(endog, exog=None, order=(0, 0, 0), seasonal_order=(0, 0, 0, 0), include_constant=True, enforce_stationarity=True, enforce_invertibility=True, concentrate_scale=False, start_params=None, fit_kwargs=None):\n    \"\"\"\n    Estimate SARIMAX parameters using state space methods.\n\n    Parameters\n    ----------\n    endog : array_like\n        Input time series array.\n    order : tuple, optional\n        The (p,d,q) order of the model for the number of AR parameters,\n        differences, and MA parameters. Default is (0, 0, 0).\n    seasonal_order : tuple, optional\n        The (P,D,Q,s) order of the seasonal component of the model for the\n        AR parameters, differences, MA parameters, and periodicity. Default\n        is (0, 0, 0, 0).\n    include_constant : bool, optional\n        Whether to add a constant term in `exog` if it's not already there.\n        The estimate of the constant will then appear as one of the `exog`\n        parameters. If `exog` is None, then the constant will represent the\n        mean of the process.\n    enforce_stationarity : bool, optional\n        Whether or not to transform the AR parameters to enforce stationarity\n        in the autoregressive component of the model. Default is True.\n    enforce_invertibility : bool, optional\n        Whether or not to transform the MA parameters to enforce invertibility\n        in the moving average component of the model. Default is True.\n    concentrate_scale : bool, optional\n        Whether or not to concentrate the scale (variance of the error term)\n        out of the likelihood. This reduces the number of parameters estimated\n        by maximum likelihood by one.\n    start_params : array_like, optional\n        Initial guess of the solution for the loglikelihood maximization. The\n        AR polynomial must be stationary. If `enforce_invertibility=True` the\n        MA poylnomial must be invertible. If not provided, default starting\n        parameters are computed using the Hannan-Rissanen method.\n    fit_kwargs : dict, optional\n        Arguments to pass to the state space model's `fit` method.\n\n    Returns\n    -------\n    parameters : SARIMAXParams object\n    other_results : Bunch\n        Includes two components, `spec`, containing the `SARIMAXSpecification`\n        instance corresponding to the input arguments; and\n        `state_space_results`, corresponding to the results from the underlying\n        state space model and Kalman filter / smoother.\n\n    Notes\n    -----\n    The primary reference is [1]_.\n\n    References\n    ----------\n    .. [1] Durbin, James, and Siem Jan Koopman. 2012.\n       Time Series Analysis by State Space Methods: Second Edition.\n       Oxford University Press.\n    \"\"\"\n    if include_constant:\n        exog = np.ones_like(endog) if exog is None else add_constant(exog)\n    spec = SARIMAXSpecification(endog, exog=exog, order=order, seasonal_order=seasonal_order, enforce_stationarity=enforce_stationarity, enforce_invertibility=enforce_invertibility, concentrate_scale=concentrate_scale)\n    endog = spec.endog\n    exog = spec.exog\n    p = SARIMAXParams(spec=spec)\n    if start_params is not None:\n        sp = SARIMAXParams(spec=spec)\n        sp.params = start_params\n        if spec.enforce_stationarity and (not sp.is_stationary):\n            raise ValueError('Given starting parameters imply a non-stationary AR process with `enforce_stationarity=True`.')\n        if spec.enforce_invertibility and (not sp.is_invertible):\n            raise ValueError('Given starting parameters imply a non-invertible MA process with `enforce_invertibility=True`.')\n    mod = SARIMAX(endog, exog=exog, order=spec.order, seasonal_order=spec.seasonal_order, enforce_stationarity=spec.enforce_stationarity, enforce_invertibility=spec.enforce_invertibility, concentrate_scale=spec.concentrate_scale)\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    fit_kwargs.setdefault('disp', 0)\n    res_ss = mod.fit(start_params=start_params, **fit_kwargs)\n    p.params = res_ss.params\n    res = Bunch({'spec': spec, 'statespace_results': res_ss})\n"]]}
{"hexsha": "c5bb99c3d18fd816302c58d233ef02f81c46252c", "ext": "py", "lang": "Python", "content": "def close_env(env_process):\n    process = psutil.Process(env_process.pid)\n    for proc in process.children(recursive=True):\n        proc.kill()\n    process.kill()", "fn_id": 0, "class_fn": false, "repo": "y437li/reinforcement-learning845", "file": "aux_functions.py", "last_update_at": "2022-03-12T00:21:37+00:00", "question_id": "c5bb99c3d18fd816302c58d233ef02f81c46252c_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def close_env(env_process):\n    process = psutil.Process(env_process.pid)\n    for proc in process.children(recursive=True):\n        proc.kill()\n"]]}
{"hexsha": "df1839168cc2107280bf8d58317bbcc5371a2473", "ext": "py", "lang": "Python", "content": "def _reload_model(ts_experiment, eval_conf, idx, use_best=False):\n    '''\n    load model into ts_experiment\n    '''\n    fn = eval_conf.df_results.iloc[idx]['model_fn']\n    # print('fn')\n    arch = eval_conf.df_results.iloc[idx]['arch']\n    print(arch)\n    ts_experiment.learn = _get_mock_learner(ts_experiment, arch)\n    if not use_best:\n        ts_experiment.learn.load(eval_conf.model_dir/Path(fn).stem)\n    else:\n        ts_experiment.learn.load(eval_conf.model_dir/(Path(fn).stem+'_best_val'))\n    return\n\n    if not test:\n        fn = eval_conf.preds_dir/eval_conf.df_results.iloc[idx]['val_preds'] \n    else:\n        fn=eval_conf.preds_dir/eval_conf.df_results.iloc[idx]['test_preds']\n    return torch.load(fn)", "fn_id": 7, "class_fn": false, "repo": "Takezo87/torchtools", "file": "torchtools/eval_utils.py", "last_update_at": "2022-02-26T06:23:52+00:00", "question_id": "df1839168cc2107280bf8d58317bbcc5371a2473_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _reload_model(ts_experiment, eval_conf, idx, use_best=False):\n    \"\"\"\n    load model into ts_experiment\n    \"\"\"\n    fn = eval_conf.df_results.iloc[idx]['model_fn']\n    arch = eval_conf.df_results.iloc[idx]['arch']\n    print(arch)\n    ts_experiment.learn = _get_mock_learner(ts_experiment, arch)\n    if not use_best:\n        ts_experiment.learn.load(eval_conf.model_dir / Path(fn).stem)\n    else:\n        ts_experiment.learn.load(eval_conf.model_dir / (Path(fn).stem + '_best_val'))\n    return\n    if not test:\n        fn = eval_conf.preds_dir / eval_conf.df_results.iloc[idx]['val_preds']\n    else:\n        fn = eval_conf.preds_dir / eval_conf.df_results.iloc[idx]['test_preds']\n"]]}
{"hexsha": "69bef9cec9270b99d230d6e3ac4423b2de4d63ed", "ext": "py", "lang": "Python", "content": "def penalize_repetition(next_token_logits, sampled_token_sequences, repetition_penalty):\n    \"\"\"repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)\"\"\"\n    #TODO: Fix bug in this function\n    if repetition_penalty != 1.0:\n        for i in range(next_token_logits.shape[0]):\n            for previous_token in set(sampled_token_sequences[i]):\n                # if score < 0 then repetition penalty has to be multiplied to reduce the previous\n                # token probability\n                if next_token_logits[i, previous_token] < 0:\n                    next_token_logits[i, previous_token] *= repetition_penalty\n                else:\n                    next_token_logits[i, previous_token] /= repetition_penalty\n    return next_token_logits", "fn_id": 0, "class_fn": false, "repo": "mrazizi/TextGAIL", "file": "TorchFly/torchfly/text/decode/transformer_decoder.py", "last_update_at": "2022-03-08T15:01:48+00:00", "question_id": "69bef9cec9270b99d230d6e3ac4423b2de4d63ed_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def penalize_repetition(next_token_logits, sampled_token_sequences, repetition_penalty):\n    \"\"\"repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)\"\"\"\n    if repetition_penalty != 1.0:\n        for i in range(next_token_logits.shape[0]):\n            for previous_token in set(sampled_token_sequences[i]):\n                if next_token_logits[i, previous_token] < 0:\n                    next_token_logits[i, previous_token] *= repetition_penalty\n                else:\n                    next_token_logits[i, previous_token] /= repetition_penalty\n"]]}
{"hexsha": "76db2bb47977d65d349e1e37bef2d5fbf3bbd87a", "ext": "py", "lang": "Python", "content": "@router.get(\"/status\", tags=[\"status\"])\ndef serverStatus():\n    cP = ConfigProvider.ConfigProvider()\n    psU = ProcessStatusUtil()\n    psD = psU.getInfo()\n    return {\"msg\": \"Status is nominal!\", \"version\": cP.getVersion(), \"status\": psD}", "fn_id": 0, "class_fn": false, "repo": "rcsb/py-rcsb_app_file", "file": "rcsb/app/file/serverStatus.py", "last_update_at": "2022-03-28T16:42:01+00:00", "question_id": "76db2bb47977d65d349e1e37bef2d5fbf3bbd87a_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@router.get('/status', tags=['status'])\ndef serverStatus():\n    cP = ConfigProvider.ConfigProvider()\n    psU = ProcessStatusUtil()\n    psD = psU.getInfo()\n"]]}
{"hexsha": "740cf5cf9297cfbc6cc70e0490f10c2adbc22a4c", "ext": "py", "lang": "Python", "content": "def get_info(package):\n    \"\"\"process the request of getting user's info\n    \"\"\"\n    params = package.get('params')\n    username = params.get(ParamType.UsernameWithDefault)\n    if username is None:\n        user = package.get('user')\n    else:\n        user = UserHelper.get_user_by_username(username)\n    if user is None:\n        return Response.error_response(\"No User\")\n\n    user = UserHelper.user_filter(user)\n    permission_public = user.get('permission')\n    user_id = user.get('id')\n    school_id = PermissionHelper.get_user_school(user_id)\n    if school_id == 0:\n        if permission_public >= 8:\n            permission_private = permission_public\n        else:\n            permission_private = -1\n        schoolname = 'public area'\n    else:\n        permission_private = PermissionHelper.get_permission(user_id, school_id)\n        school = SchoolHelper.get_school(school_id)\n        if school is None:\n            schoolname = '-'\n        else:\n            schoolname = school.get('schoolname')\n\n    download = ProgramHelper.count_user_downloadlog(user_id)\n\n    del user['permission']\n    user.update({\n        'school_name' : schoolname,\n        'permission_public' : permission_public,\n        'permission_private' : permission_private,\n        'download' : download\n    })\n    return Response.success_response({'user' : user})", "fn_id": 0, "class_fn": false, "repo": "Hrsn2861/pysat-server", "file": "user/views/info.py", "last_update_at": "2022-02-10T11:46:31+00:00", "question_id": "740cf5cf9297cfbc6cc70e0490f10c2adbc22a4c_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_info(package):\n    \"\"\"process the request of getting user's info\n    \"\"\"\n    params = package.get('params')\n    username = params.get(ParamType.UsernameWithDefault)\n    if username is None:\n        user = package.get('user')\n    else:\n        user = UserHelper.get_user_by_username(username)\n    if user is None:\n        return Response.error_response('No User')\n    user = UserHelper.user_filter(user)\n    permission_public = user.get('permission')\n    user_id = user.get('id')\n    school_id = PermissionHelper.get_user_school(user_id)\n    if school_id == 0:\n        if permission_public >= 8:\n            permission_private = permission_public\n        else:\n            permission_private = -1\n        schoolname = 'public area'\n    else:\n        permission_private = PermissionHelper.get_permission(user_id, school_id)\n        school = SchoolHelper.get_school(school_id)\n        if school is None:\n            schoolname = '-'\n        else:\n            schoolname = school.get('schoolname')\n    download = ProgramHelper.count_user_downloadlog(user_id)\n    del user['permission']\n    user.update({'school_name': schoolname, 'permission_public': permission_public, 'permission_private': permission_private, 'download': download})\n"]]}
{"hexsha": "352ab39ca3d85482307e29526b91c57912e4a1ac", "ext": "py", "lang": "Python", "content": "def inspect_list(report, terse=None, header=None):\n    \"\"\"Implements method ``buildtest inspect list``\"\"\"\n\n    test_ids = report.get_ids()\n\n    table = {\"name\": [], \"id\": []}\n\n    # print output in terse format\n    if terse:\n        # print column headers if --no-header is not specified\n        if not header:\n            print(\"|\".join(table.keys()))\n\n        for uid, name in test_ids.items():\n            print(f\"{uid}|{name}\")\n        return\n\n    for identifier, name in test_ids.items():\n        table[\"name\"].append(name)\n        table[\"id\"].append(identifier)\n\n    if os.getenv(\"BUILDTEST_COLOR\") == \"True\":\n        print(\n            tabulate(\n                table,\n                headers=[\n                    colored(field, \"blue\", attrs=[\"bold\"]) for field in table.keys()\n                ],\n                tablefmt=\"grid\",\n            )\n        )\n        return\n    print(tabulate(table, headers=table.keys(), tablefmt=\"grid\"))", "fn_id": 1, "class_fn": false, "repo": "shahzebsiddiqui/buildtest-1", "file": "buildtest/cli/inspect.py", "last_update_at": "2022-03-30T16:54:24+00:00", "question_id": "352ab39ca3d85482307e29526b91c57912e4a1ac_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def inspect_list(report, terse=None, header=None):\n    \"\"\"Implements method ``buildtest inspect list``\"\"\"\n    test_ids = report.get_ids()\n    table = {'name': [], 'id': []}\n    if terse:\n        if not header:\n            print('|'.join(table.keys()))\n        for uid, name in test_ids.items():\n            print(f'{uid}|{name}')\n        return\n    for identifier, name in test_ids.items():\n        table['name'].append(name)\n        table['id'].append(identifier)\n    if os.getenv('BUILDTEST_COLOR') == 'True':\n        print(tabulate(table, headers=[colored(field, 'blue', attrs=['bold']) for field in table.keys()], tablefmt='grid'))\n        return\n"]]}
{"hexsha": "20c0ea7cc1dc5ceb8e4405692f0ade77a4859382", "ext": "py", "lang": "Python", "content": "def trunk_2(arr_2, size_2):\n    arrs = []\n    while len(arr_2) > size_2:\n        pice = arr_2[:size_2]\n        arrs.append(pice)\n        arr_2 = arr_2[size:]\n    arrs.append(arr_2)\n    return arrs", "fn_id": 1, "class_fn": false, "repo": "twtrubiks/leetcode-python", "file": "logic_tutorial.py", "last_update_at": "2022-01-27T02:29:15+00:00", "question_id": "20c0ea7cc1dc5ceb8e4405692f0ade77a4859382_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def trunk_2(arr_2, size_2):\n    arrs = []\n    while len(arr_2) > size_2:\n        pice = arr_2[:size_2]\n        arrs.append(pice)\n        arr_2 = arr_2[size:]\n    arrs.append(arr_2)\n"]]}
{"hexsha": "5896fe849ac0cfd44b6065f64f05244189a14044", "ext": "py", "lang": "Python", "content": "@pytest.fixture\ndef get_request():\n    req = APIRequestFactory().get(\"/\", HTTP_ACCEPT=\"text/html\")\n    return APIView().initialize_request(req)", "fn_id": 0, "class_fn": false, "repo": "jdevries3133/django_htmx_rest", "file": "src/htmx_rest/tests/test_negotiator.py", "last_update_at": "2022-02-11T01:31:05+00:00", "question_id": "5896fe849ac0cfd44b6065f64f05244189a14044_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.fixture\ndef get_request():\n    req = APIRequestFactory().get('/', HTTP_ACCEPT='text/html')\n"]]}
{"hexsha": "8a4601b74a5208edccea033c3d2c445a7a94f1dd", "ext": "py", "lang": "Python", "content": "def check_password(token, password):\n    cache_key = token + hashlib.sha256(password.encode(\"utf-8\")).hexdigest()\n    if cache_key in _pw_auth_validator:\n        return _pw_auth_validator[cache_key]\n    split = token.split(\":\")\n    if len(split) != 3:\n        return False\n    algorithm = split[0]\n    check_func = query_utility(IPasswordChecker, name=algorithm)\n    if check_func is None:\n        logger.warning(f\"Could not find password checker for {algorithm}\")\n        return False\n    decision = check_func(token, password)\n    _pw_auth_validator[cache_key] = decision\n    return decision", "fn_id": 3, "class_fn": false, "repo": "rboixaderg/guillotina", "file": "guillotina/auth/validators.py", "last_update_at": "2022-03-03T06:48:56+00:00", "question_id": "8a4601b74a5208edccea033c3d2c445a7a94f1dd_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def check_password(token, password):\n    cache_key = token + hashlib.sha256(password.encode('utf-8')).hexdigest()\n    if cache_key in _pw_auth_validator:\n        return _pw_auth_validator[cache_key]\n    split = token.split(':')\n    if len(split) != 3:\n        return False\n    algorithm = split[0]\n    check_func = query_utility(IPasswordChecker, name=algorithm)\n    if check_func is None:\n        logger.warning(f'Could not find password checker for {algorithm}')\n        return False\n    decision = check_func(token, password)\n    _pw_auth_validator[cache_key] = decision\n"]]}
{"hexsha": "dfbcde33b8f8a4375d2b0ef35d57cd76d7507544", "ext": "py", "lang": "Python", "content": "def is_request_correct(url: str,\n                       p_count: int,\n                       **kwargs) -> Tuple[str, str]:\n    \"\"\"\n    Check:\n        \u2013 is the HTTP request correct (means there are no exceptions catch).\n\n        \u2013 has there been any result.\n\n        \u2013 does a page at the number exist (\n        means RNC doesn't redirect to the first page).\n\n    :return: first and last pages if everything's OK.\n\n    :exception WrongHTTPRequest: HTTP request is wrong.\n    :exception NoResultFound: no result found.\n    :exception LastPageDoesntExist: the last page doesn't exist.\n    \"\"\"\n    logger.debug(\"Validating that everything is OK\")\n    try:\n        # to reduce the number of requests\n        # the two checks are combined into one.\n        # coro writes logs by itself\n        first_page = whether_result_found(url, **kwargs)\n    except ValueError:\n        logger.error(\"HTTP request is OK, but no result found\")\n        raise NoResultFound(f\"{kwargs}\")\n    except RuntimeError:\n        logger.error(\"HTTP request is wrong\")\n        raise WrongHTTPRequest(f\"{kwargs}\")\n    logger.debug(\"HTTP request is correct, result found\")\n\n    logger.debug(\"Validating that the last page exists\")\n    try:\n        last_page = does_page_exist(url, p_count - 1, first_page, **kwargs)\n    except ValueError:\n        logger.error(\"Everything is OK, but last page doesn't exist\")\n        raise LastPageDoesntExist(f\"{kwargs}\")\n    logger.debug(\"The last page exists\")\n\n    logger.debug(\"Validated successfully\")\n    return first_page, last_page", "fn_id": 7, "class_fn": false, "repo": "kunansy/RNC", "file": "rnc/corpora_requests.py", "last_update_at": "2022-03-08T12:34:50+00:00", "question_id": "dfbcde33b8f8a4375d2b0ef35d57cd76d7507544_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def is_request_correct(url: str, p_count: int, **kwargs) -> Tuple[str, str]:\n    \"\"\"\n    Check:\n        \u2013 is the HTTP request correct (means there are no exceptions catch).\n\n        \u2013 has there been any result.\n\n        \u2013 does a page at the number exist (\n        means RNC doesn't redirect to the first page).\n\n    :return: first and last pages if everything's OK.\n\n    :exception WrongHTTPRequest: HTTP request is wrong.\n    :exception NoResultFound: no result found.\n    :exception LastPageDoesntExist: the last page doesn't exist.\n    \"\"\"\n    logger.debug('Validating that everything is OK')\n    try:\n        first_page = whether_result_found(url, **kwargs)\n    except ValueError:\n        logger.error('HTTP request is OK, but no result found')\n        raise NoResultFound(f'{kwargs}')\n    except RuntimeError:\n        logger.error('HTTP request is wrong')\n        raise WrongHTTPRequest(f'{kwargs}')\n    logger.debug('HTTP request is correct, result found')\n    logger.debug('Validating that the last page exists')\n    try:\n        last_page = does_page_exist(url, p_count - 1, first_page, **kwargs)\n    except ValueError:\n        logger.error(\"Everything is OK, but last page doesn't exist\")\n        raise LastPageDoesntExist(f'{kwargs}')\n    logger.debug('The last page exists')\n    logger.debug('Validated successfully')\n"]]}
{"hexsha": "fc004b91f3e58f1e94c240732f876a5bacc896ad", "ext": "py", "lang": "Python", "content": "def doBinomialTest(p, sample_size, observed, significance_threshold=0.05):\n    \"\"\"perform a binomial test.\n\n    Given are p: the probability of the NULL hypothesis, the sample_size\n    and the number of observed counts.\n    \"\"\"\n    pass", "fn_id": 2, "class_fn": false, "repo": "IanSudbery/sphinx-report", "file": "CGATReport/Stats.py", "last_update_at": "2022-01-03T20:22:42+00:00", "question_id": "fc004b91f3e58f1e94c240732f876a5bacc896ad_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def doBinomialTest(p, sample_size, observed, significance_threshold=0.05):\n    \"\"\"perform a binomial test.\n\n    Given are p: the probability of the NULL hypothesis, the sample_size\n    and the number of observed counts.\n    \"\"\"\n"]]}
{"hexsha": "8237e4a161d5e86e67c21247d893006918d78aa7", "ext": "py", "lang": "Python", "content": "def test_log_info(caplog):\n    # Arrange\n    test_df = pd.DataFrame({\n        'name': ['fault1', 'fault2'],\n        'i1': [720, 305],\n        'i2': [875, 342],\n        'j1': [311, 32],\n        'j2': [103, 800],\n        'k1': [791, 847],\n        'k2': [994, 494],\n        'face': ['I+', 'I+']\n    })\n\n    # Act\n    wf.write_faults_nexus('test', test_df)\n\n    # Assert\n    for record in caplog.records:\n        assert record.levelname == \"INFO\"\n    assert 'writing FNAME data in Nexus format to file: test' in caplog.text", "fn_id": 2, "class_fn": false, "repo": "bp/resqpy", "file": "tests/test_olio_write_faults.py", "last_update_at": "2022-03-15T15:47:59+00:00", "question_id": "8237e4a161d5e86e67c21247d893006918d78aa7_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_log_info(caplog):\n    test_df = pd.DataFrame({'name': ['fault1', 'fault2'], 'i1': [720, 305], 'i2': [875, 342], 'j1': [311, 32], 'j2': [103, 800], 'k1': [791, 847], 'k2': [994, 494], 'face': ['I+', 'I+']})\n    wf.write_faults_nexus('test', test_df)\n    for record in caplog.records:\n        assert record.levelname == 'INFO'\n"]]}
{"hexsha": "586610cb5605426f59bec3ab33221fad88788b12", "ext": "py", "lang": "Python", "content": "def dice_game():\n\n    high_score = 0\n    \n    print(\"Current high score is: \", high_score)\n\n    print(\"1) Roll Dice\")\n    print(\"2) Leave Game\")\n\n    choice = input(\"Enter your choice: \")\n\n    if choice == \"1\":\n        dice_roll1 = random.randint(1,6)\n        print(\"you rolled a: \", dice_roll1)\n        dice_roll2 = random.randint(1,6)\n        print(\"You rolled a: \", dice_roll2)\n\n        total = dice_roll1 + dice_roll2\n\n        print(\"You rolled a total of: \", total)\n\n        if total > high_score:\n            print(\"New high score!\\n\")\n            high_score = total\n\n    elif choice == \"2\":\n        print(\"Thank you for playing\")\n    else:\n        print(\"Enter a valid choice!\")\n        dice_game()", "fn_id": 0, "class_fn": false, "repo": "armirh/Nucamp-SQL-Devops-Training", "file": "Python/1-Fundamentals/cc_dicegame.py", "last_update_at": "2022-01-19T02:33:13+00:00", "question_id": "586610cb5605426f59bec3ab33221fad88788b12_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def dice_game():\n    high_score = 0\n    print('Current high score is: ', high_score)\n    print('1) Roll Dice')\n    print('2) Leave Game')\n    choice = input('Enter your choice: ')\n    if choice == '1':\n        dice_roll1 = random.randint(1, 6)\n        print('you rolled a: ', dice_roll1)\n        dice_roll2 = random.randint(1, 6)\n        print('You rolled a: ', dice_roll2)\n        total = dice_roll1 + dice_roll2\n        print('You rolled a total of: ', total)\n        if total > high_score:\n            print('New high score!\\n')\n            high_score = total\n    elif choice == '2':\n        print('Thank you for playing')\n    else:\n        print('Enter a valid choice!')\n"]]}
{"hexsha": "b8b73599affe70caf337aa01207aa7e753e94c1f", "ext": "py", "lang": "Python", "content": "def sketch(image):\n    # Convert image to gray scale\n    img_gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n\n    # Clean up image using Gaussian Blur\n    img_gray_blur = cv.GaussianBlur(img_gray, (5, 5), 0)\n\n    # Extract Edges\n    canny_edges = cv.Canny(img_gray_blur, 30, 70)\n\n    # Do an invert binarize the image\n    ret, mask = cv.threshold(canny_edges, 120, 255, cv.THRESH_BINARY_INV)\n\n    return mask", "fn_id": 0, "class_fn": false, "repo": "charlesaurav13/OpenCV", "file": "liveSketch.py", "last_update_at": "2022-03-25T20:44:38+00:00", "question_id": "b8b73599affe70caf337aa01207aa7e753e94c1f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def sketch(image):\n    img_gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n    img_gray_blur = cv.GaussianBlur(img_gray, (5, 5), 0)\n    canny_edges = cv.Canny(img_gray_blur, 30, 70)\n    ret, mask = cv.threshold(canny_edges, 120, 255, cv.THRESH_BINARY_INV)\n"]]}
{"hexsha": "34d7f11c6d32fae190ddc29826215149e2291bf7", "ext": "py", "lang": "Python", "content": "@shared_task\ndef sync_social_accounts(user_pk):\n    from social_django.models import UserSocialAuth\n    from forsta_auth.backend_meta import BackendMeta\n    session = apps.get_app_config('forsta_auth').session\n\n    user = get_user_model().objects.get(pk=user_pk)\n    if not user.primary:\n        return\n\n    user_social_auths = UserSocialAuth.objects.filter(user=user)\n    by_upstream_id = {str(usa.pk): usa\n                      for usa in user_social_auths}\n    online_account_url = urljoin(settings.IDM_CORE_API_URL, 'online-account/')\n\n    results, url = [], online_account_url\n    while url:\n        response = session.get(url,\n                               params={'identity': user.identity_id,\n                                       'managed_by': settings.IDM_APPLICATION_ID})\n        response.raise_for_status()\n        response_data = response.json()\n        results.extend(response_data['results'])\n        url = response_data.get('next')\n\n    for result in results:\n        usa = by_upstream_id.get(result['upstream_id'])\n        if usa:\n            backend_meta = BackendMeta.wrap(usa)\n            if backend_meta.username != result['screen_name']:\n                session.patch(result['url'], json={'screen_name': backend_meta.username}).raise_for_status()\n            del by_upstream_id[result['upstream_id']]\n        else:\n            session.delete(result['url']).raise_for_status()\n\n    for upstream_id, usa in by_upstream_id.items():\n        backend_meta = BackendMeta.wrap(usa)\n        provider_id = provider_id_override.get(backend_meta.provider, backend_meta.provider)\n        if provider_id is None:\n            continue\n        response = session.post(online_account_url, json={\n            'identity': str(user.identity_id),\n            'upstream_id': upstream_id,\n            'provider_id': provider_id,\n            'screen_name': backend_meta.username,\n            'validated': True,\n            'context': 'home',\n            'managed': True,\n            'manage_url': 'https://{}{}'.format(get_current_site(None).domain,\n                                                reverse('social-logins'))\n        })\n        if not response.ok:\n            logger.error(\"Couldn't create online-account:\\n{}\".format(response.content))\n        response.raise_for_status()", "fn_id": 0, "class_fn": false, "repo": "forsta-iam/forsta-auth", "file": "forsta_auth/tasks/social_accounts.py", "last_update_at": "2022-02-10T07:32:53+00:00", "question_id": "34d7f11c6d32fae190ddc29826215149e2291bf7_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@shared_task\ndef sync_social_accounts(user_pk):\n    from social_django.models import UserSocialAuth\n    from forsta_auth.backend_meta import BackendMeta\n    session = apps.get_app_config('forsta_auth').session\n    user = get_user_model().objects.get(pk=user_pk)\n    if not user.primary:\n        return\n    user_social_auths = UserSocialAuth.objects.filter(user=user)\n    by_upstream_id = {str(usa.pk): usa for usa in user_social_auths}\n    online_account_url = urljoin(settings.IDM_CORE_API_URL, 'online-account/')\n    results, url = ([], online_account_url)\n    while url:\n        response = session.get(url, params={'identity': user.identity_id, 'managed_by': settings.IDM_APPLICATION_ID})\n        response.raise_for_status()\n        response_data = response.json()\n        results.extend(response_data['results'])\n        url = response_data.get('next')\n    for result in results:\n        usa = by_upstream_id.get(result['upstream_id'])\n        if usa:\n            backend_meta = BackendMeta.wrap(usa)\n            if backend_meta.username != result['screen_name']:\n                session.patch(result['url'], json={'screen_name': backend_meta.username}).raise_for_status()\n            del by_upstream_id[result['upstream_id']]\n        else:\n            session.delete(result['url']).raise_for_status()\n    for upstream_id, usa in by_upstream_id.items():\n        backend_meta = BackendMeta.wrap(usa)\n        provider_id = provider_id_override.get(backend_meta.provider, backend_meta.provider)\n        if provider_id is None:\n            continue\n        response = session.post(online_account_url, json={'identity': str(user.identity_id), 'upstream_id': upstream_id, 'provider_id': provider_id, 'screen_name': backend_meta.username, 'validated': True, 'context': 'home', 'managed': True, 'manage_url': 'https://{}{}'.format(get_current_site(None).domain, reverse('social-logins'))})\n        if not response.ok:\n            logger.error(\"Couldn't create online-account:\\n{}\".format(response.content))\n"]]}
{"hexsha": "8043c1fe3256d03debb92099ce4c83c33572d279", "ext": "py", "lang": "Python", "content": "def saved_example(filename):\n    n, w = example(filename)\n\n    soln = activelo.solve(n, w)\n\n    activelo.plot(soln)\n\n    return soln", "fn_id": 2, "class_fn": false, "repo": "jzf2101/boardlaw", "file": "activelo/examples/solvers.py", "last_update_at": "2022-01-25T21:51:29+00:00", "question_id": "8043c1fe3256d03debb92099ce4c83c33572d279_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def saved_example(filename):\n    n, w = example(filename)\n    soln = activelo.solve(n, w)\n    activelo.plot(soln)\n"]]}
{"hexsha": "38d2ea165569269c1c07b37f25b8bec40d353124", "ext": "py", "lang": "Python", "content": "def train(epoch, iter_no, df_iter, batch_multiplier=10, print_freq=100, scheduler=None):  \n    \n    print('\\nEpoch: %d' % epoch)\n    if args.cyclic==False and scheduler is None:\n        adjust_learning_rate(optimizer, epoch)\n    train_loss = AverageMeter()\n    data_time = AverageMeter()\n    batch_time = AverageMeter()\n    correct = 0\n    total = 0\n\n    # switch to train mode\n    net.train()\n\n    end = time.time()\n    for batch_idx, (inputs, targets, indexes) in enumerate(trainloader):\n        if args.cyclic: scheduler.step()\n        iter_no += 1\n        data_time.update(time.time() - end)\n        inputs, targets, indexes = inputs.to(device), targets.to(device), indexes.to(device)\n        \n        if batch_idx % batch_multiplier == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            \n        features = net(inputs)\n        outputs = lemniscate(features, indexes)\n        loss = criterion(outputs, indexes) / float(batch_multiplier)\n\n        loss.backward()\n\n        train_loss.update(loss.item(), inputs.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if batch_idx % print_freq == 0 or batch_idx == len(trainloader):\n            print('Epoch: [{}][{}/{}]'\n                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f}) '\n                  'LR: {lr}'.format(\n                  epoch, batch_idx, len(trainloader), batch_time=batch_time, data_time=data_time, train_loss=train_loss, lr=optimizer.param_groups[0]['lr']))\n\n        df_iter = df_iter.append({\n            \"epoch\": epoch, \n            \"iteration\": iter_no, \n            \"learning_rate\": optimizer.param_groups[0]['lr'], \n            \"loss\": train_loss.val, \n        }, ignore_index=True)  \n\n        if scheduler is not None: scheduler.on_batch_end(True)\n                    \n    return iter_no, train_loss.avg, df_iter", "fn_id": 2, "class_fn": false, "repo": "vayzenb/open_ipcl", "file": "train_original.py", "last_update_at": "2022-02-15T15:34:44+00:00", "question_id": "38d2ea165569269c1c07b37f25b8bec40d353124_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def train(epoch, iter_no, df_iter, batch_multiplier=10, print_freq=100, scheduler=None):\n    print('\\nEpoch: %d' % epoch)\n    if args.cyclic == False and scheduler is None:\n        adjust_learning_rate(optimizer, epoch)\n    train_loss = AverageMeter()\n    data_time = AverageMeter()\n    batch_time = AverageMeter()\n    correct = 0\n    total = 0\n    net.train()\n    end = time.time()\n    for batch_idx, (inputs, targets, indexes) in enumerate(trainloader):\n        if args.cyclic:\n            scheduler.step()\n        iter_no += 1\n        data_time.update(time.time() - end)\n        inputs, targets, indexes = (inputs.to(device), targets.to(device), indexes.to(device))\n        if batch_idx % batch_multiplier == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n        features = net(inputs)\n        outputs = lemniscate(features, indexes)\n        loss = criterion(outputs, indexes) / float(batch_multiplier)\n        loss.backward()\n        train_loss.update(loss.item(), inputs.size(0))\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if batch_idx % print_freq == 0 or batch_idx == len(trainloader):\n            print('Epoch: [{}][{}/{}]Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) Data: {data_time.val:.3f} ({data_time.avg:.3f}) Loss: {train_loss.val:.4f} ({train_loss.avg:.4f}) LR: {lr}'.format(epoch, batch_idx, len(trainloader), batch_time=batch_time, data_time=data_time, train_loss=train_loss, lr=optimizer.param_groups[0]['lr']))\n        df_iter = df_iter.append({'epoch': epoch, 'iteration': iter_no, 'learning_rate': optimizer.param_groups[0]['lr'], 'loss': train_loss.val}, ignore_index=True)\n        if scheduler is not None:\n            scheduler.on_batch_end(True)\n"]]}
{"hexsha": "179976f93f8ed949a4c2f33e9d7f71db73794fa5", "ext": "py", "lang": "Python", "content": "@main.command()\n@click.argument(\"name\")\ndef create_queue(name):\n    \"\"\"Create queue with NAME supplied as argument\"\"\"\n    queue = create(name)\n    logger.info(queue.url)", "fn_id": 3, "class_fn": false, "repo": "MITLibraries/dspace-submission-service", "file": "submitter/cli.py", "last_update_at": "2022-01-26T14:25:03+00:00", "question_id": "179976f93f8ed949a4c2f33e9d7f71db73794fa5_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@main.command()\n@click.argument('name')\ndef create_queue(name):\n    \"\"\"Create queue with NAME supplied as argument\"\"\"\n    queue = create(name)\n"]]}
{"hexsha": "68289cf64967e043e55f53a151dfe72ffd61f91e", "ext": "py", "lang": "Python", "content": "@skip_unless_linux\ndef test_return_event_set(test_dir: Path, time_taken):\n    watcher = RustNotify([str(test_dir)], False, False, 0)\n\n    with time_taken(0, 20):\n        assert watcher.watch(100, 1, 500, AbstractEvent(True)) == 'stop'", "fn_id": 13, "class_fn": false, "repo": "samuelcolvin/watchfiles", "file": "tests/test_rust_notify.py", "last_update_at": "2022-03-31T13:13:59+00:00", "question_id": "68289cf64967e043e55f53a151dfe72ffd61f91e_13", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@skip_unless_linux\ndef test_return_event_set(test_dir: Path, time_taken):\n    watcher = RustNotify([str(test_dir)], False, False, 0)\n    with time_taken(0, 20):\n"]]}
{"hexsha": "11abc820cdfc2d5d2bc77037fe9672da5768605c", "ext": "py", "lang": "Python", "content": "def convert_tokens(eval_dict, qa_id, y_start_list, y_end_list):\n    \"\"\"Convert predictions to tokens from the context.\n\n    Args:\n        eval_dict (dict): Dictionary with eval info for the dataset. This is\n            used to perform the mapping from IDs and indices to actual text.\n        qa_id (int): List of QA example IDs.\n        y_start_list (list): List of start predictions.\n        y_end_list (list): List of end predictions.\n        no_answer (bool): Questions can have no answer. E.g., SQuAD 2.0.\n\n    Returns:\n        pred_dict (dict): Dictionary index IDs -> predicted answer text.\n        sub_dict (dict): Dictionary UUIDs -> predicted answer text (submission).\n    \"\"\"\n    pred_dict = {}\n    sub_dict = {}\n    for qid, y_start, y_end in zip(qa_id, y_start_list, y_end_list):\n        context = eval_dict[str(qid)][\"context\"]\n        spans = eval_dict[str(qid)][\"spans\"]\n        uuid = eval_dict[str(qid)][\"uuid\"]\n        start_idx = spans[y_start][0]\n        end_idx = spans[y_end][1]\n        pred_dict[str(qid)] = context[start_idx: end_idx]\n        sub_dict[uuid] = context[start_idx: end_idx]\n    return pred_dict, sub_dict", "fn_id": 11, "class_fn": false, "repo": "tassossapalidis/biasclassifier", "file": "backend/training/util.py", "last_update_at": "2022-02-22T07:04:14+00:00", "question_id": "11abc820cdfc2d5d2bc77037fe9672da5768605c_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def convert_tokens(eval_dict, qa_id, y_start_list, y_end_list):\n    \"\"\"Convert predictions to tokens from the context.\n\n    Args:\n        eval_dict (dict): Dictionary with eval info for the dataset. This is\n            used to perform the mapping from IDs and indices to actual text.\n        qa_id (int): List of QA example IDs.\n        y_start_list (list): List of start predictions.\n        y_end_list (list): List of end predictions.\n        no_answer (bool): Questions can have no answer. E.g., SQuAD 2.0.\n\n    Returns:\n        pred_dict (dict): Dictionary index IDs -> predicted answer text.\n        sub_dict (dict): Dictionary UUIDs -> predicted answer text (submission).\n    \"\"\"\n    pred_dict = {}\n    sub_dict = {}\n    for qid, y_start, y_end in zip(qa_id, y_start_list, y_end_list):\n        context = eval_dict[str(qid)]['context']\n        spans = eval_dict[str(qid)]['spans']\n        uuid = eval_dict[str(qid)]['uuid']\n        start_idx = spans[y_start][0]\n        end_idx = spans[y_end][1]\n        pred_dict[str(qid)] = context[start_idx:end_idx]\n        sub_dict[uuid] = context[start_idx:end_idx]\n"]]}
{"hexsha": "e92683def66cbbc9c194341d353b0acb592142f9", "ext": "py", "lang": "Python", "content": "def initialize(seed=-1):\n    \"\"\"\n    Reinitalize the library with a seed. If seed is -1 then system time is\n    used to create the seed.\n    \"\"\"\n\n    global lib, ctxt_obj, ELEM_INSTALLED, KDT_INSTALLED\n    \n    #\n    # Load C-API library and set return types\n    #\n    lib = cdll.LoadLibrary('libcskylark.so')\n    lib.sl_create_context.restype              = c_int\n    lib.sl_create_default_context.restype      = c_int\n    lib.sl_free_context.restype                = c_int\n    lib.sl_create_sketch_transform.restype     = c_int\n    lib.sl_serialize_sketch_transform.restype  = c_int\n    lib.sl_deserialize_sketch_transform.restype = c_int\n    lib.sl_wrap_raw_matrix.restype             = c_int\n    lib.sl_free_raw_matrix_wrap.restype        = c_int\n    lib.sl_wrap_raw_sp_matrix.restype          = c_int\n    lib.sl_free_raw_sp_matrix_wrap.restype     = c_int\n    lib.sl_raw_sp_matrix_nnz.restype           = c_int\n    lib.sl_raw_sp_matrix_height.restype           = c_int\n    lib.sl_raw_sp_matrix_width.restype           = c_int\n    lib.sl_raw_sp_matrix_struct_updated.restype = c_int\n    lib.sl_raw_sp_matrix_reset_update_flag.restype = c_int\n    lib.sl_raw_sp_matrix_data.restype          = c_int\n    lib.sl_strerror.restype                    = c_char_p\n    lib.sl_supported_sketch_transforms.restype = c_char_p\n    lib.sl_has_elemental.restype               = c_bool\n    lib.sl_has_combblas.restype                = c_bool\n    lib.sl_get_exception_info.restype          = None\n    lib.sl_print_exception_trace               = None\n    \n    ELEM_INSTALLED = lib.sl_has_elemental()\n    KDT_INSTALLED  = lib.sl_has_combblas()\n\n    if seed == -1:\n        seed = int(time.time())\n\n    if 'ctxt_obj' in globals():\n        lib.sl_free_context(ctxt_obj)\n            \n    ctxt_obj = c_void_p()\n    lib.sl_create_default_context(seed, byref(ctxt_obj))", "fn_id": 0, "class_fn": false, "repo": "xdata-skylark/libskylark", "file": "python-skylark/skylark/lib.py", "last_update_at": "2022-01-10T04:05:21+00:00", "question_id": "e92683def66cbbc9c194341d353b0acb592142f9_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def initialize(seed=-1):\n    \"\"\"\n    Reinitalize the library with a seed. If seed is -1 then system time is\n    used to create the seed.\n    \"\"\"\n    global lib, ctxt_obj, ELEM_INSTALLED, KDT_INSTALLED\n    lib = cdll.LoadLibrary('libcskylark.so')\n    lib.sl_create_context.restype = c_int\n    lib.sl_create_default_context.restype = c_int\n    lib.sl_free_context.restype = c_int\n    lib.sl_create_sketch_transform.restype = c_int\n    lib.sl_serialize_sketch_transform.restype = c_int\n    lib.sl_deserialize_sketch_transform.restype = c_int\n    lib.sl_wrap_raw_matrix.restype = c_int\n    lib.sl_free_raw_matrix_wrap.restype = c_int\n    lib.sl_wrap_raw_sp_matrix.restype = c_int\n    lib.sl_free_raw_sp_matrix_wrap.restype = c_int\n    lib.sl_raw_sp_matrix_nnz.restype = c_int\n    lib.sl_raw_sp_matrix_height.restype = c_int\n    lib.sl_raw_sp_matrix_width.restype = c_int\n    lib.sl_raw_sp_matrix_struct_updated.restype = c_int\n    lib.sl_raw_sp_matrix_reset_update_flag.restype = c_int\n    lib.sl_raw_sp_matrix_data.restype = c_int\n    lib.sl_strerror.restype = c_char_p\n    lib.sl_supported_sketch_transforms.restype = c_char_p\n    lib.sl_has_elemental.restype = c_bool\n    lib.sl_has_combblas.restype = c_bool\n    lib.sl_get_exception_info.restype = None\n    lib.sl_print_exception_trace = None\n    ELEM_INSTALLED = lib.sl_has_elemental()\n    KDT_INSTALLED = lib.sl_has_combblas()\n    if seed == -1:\n        seed = int(time.time())\n    if 'ctxt_obj' in globals():\n        lib.sl_free_context(ctxt_obj)\n    ctxt_obj = c_void_p()\n"]]}
{"hexsha": "4df44fc48f24e819a79de1bd3d4e9fcb8d990874", "ext": "py", "lang": "Python", "content": "async def _assert_postconditions_async(postconditions: List[Contract],\n                                       resolved_kwargs: Mapping[str, Any]) -> Optional[BaseException]:\n    \"\"\"Assert that the postconditions of an async function hold.\"\"\"\n    assert 'result' in resolved_kwargs, \\\n        \"Expected 'result' to be already set in resolved kwargs before calling this function.\"\n\n    for contract in postconditions:\n        condition_kwargs = select_condition_kwargs(contract=contract, resolved_kwargs=resolved_kwargs)\n\n        if inspect.iscoroutinefunction(contract.condition):\n            check = await contract.condition(**condition_kwargs)\n        else:\n            check_or_coroutine = contract.condition(**condition_kwargs)\n            if inspect.iscoroutine(check_or_coroutine):\n                check = await check_or_coroutine\n            else:\n                check = check_or_coroutine\n\n        if not_check(check=check, contract=contract):\n            exception = _create_violation_error(contract=contract, resolved_kwargs=resolved_kwargs)\n\n            return exception\n\n    return None", "fn_id": 13, "class_fn": false, "repo": "kklein/icontract", "file": "icontract/_checkers.py", "last_update_at": "2022-03-12T16:10:39+00:00", "question_id": "4df44fc48f24e819a79de1bd3d4e9fcb8d990874_13", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def _assert_postconditions_async(postconditions: List[Contract], resolved_kwargs: Mapping[str, Any]) -> Optional[BaseException]:\n    \"\"\"Assert that the postconditions of an async function hold.\"\"\"\n    assert 'result' in resolved_kwargs, \"Expected 'result' to be already set in resolved kwargs before calling this function.\"\n    for contract in postconditions:\n        condition_kwargs = select_condition_kwargs(contract=contract, resolved_kwargs=resolved_kwargs)\n        if inspect.iscoroutinefunction(contract.condition):\n            check = await contract.condition(**condition_kwargs)\n        else:\n            check_or_coroutine = contract.condition(**condition_kwargs)\n            if inspect.iscoroutine(check_or_coroutine):\n                check = await check_or_coroutine\n            else:\n                check = check_or_coroutine\n        if not_check(check=check, contract=contract):\n            exception = _create_violation_error(contract=contract, resolved_kwargs=resolved_kwargs)\n            return exception\n"]]}
{"hexsha": "2ed480cdcfa28dd007f6db7b0731c529c83f70f0", "ext": "py", "lang": "Python", "content": "def test():\n    url = APP_URL_DATA + \"/dumps/unpaywall/?where={\\\"doi\\\":\\\"\"\n    url += \"10.4000/rechercheformation.2839\\\",\\\"treated\\\":false}\"\n    try:\n        test_json = requests.get(url, headers=header).json()['data'][0]\n        print(test_json)\n    except Exception:\n        print(\"The test element is not in the unpaywall dump collection \\\n                or it has already be processed\")\n        return\n    process_doi_unpaywall(test_json)", "fn_id": 4, "class_fn": false, "repo": "MinistereSupRecherche/bso", "file": "scripts/process_publications.py", "last_update_at": "2022-01-14T16:32:07+00:00", "question_id": "2ed480cdcfa28dd007f6db7b0731c529c83f70f0_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test():\n    url = APP_URL_DATA + '/dumps/unpaywall/?where={\"doi\":\"'\n    url += '10.4000/rechercheformation.2839\",\"treated\":false}'\n    try:\n        test_json = requests.get(url, headers=header).json()['data'][0]\n        print(test_json)\n    except Exception:\n        print('The test element is not in the unpaywall dump collection                 or it has already be processed')\n        return\n"]]}
{"hexsha": "88c9e3af4411569791cf77ecc1a737d4c4eee95d", "ext": "py", "lang": "Python", "content": "def load_yaml(stream):\n    \"\"\"load yaml from a file-like object; used to make it easier to cater to\n    API changes in ruamel.yaml\n    \"\"\"\n    from ruamel.yaml import YAML\n\n    yaml = YAML(typ=\"safe\", pure=True)\n    return yaml.load(stream)", "fn_id": 1, "class_fn": false, "repo": "TomArrow/ebu_adm_renderer", "file": "da_ear/compatibility.py", "last_update_at": "2022-02-03T08:59:48+00:00", "question_id": "88c9e3af4411569791cf77ecc1a737d4c4eee95d_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_yaml(stream):\n    \"\"\"load yaml from a file-like object; used to make it easier to cater to\n    API changes in ruamel.yaml\n    \"\"\"\n    from ruamel.yaml import YAML\n    yaml = YAML(typ='safe', pure=True)\n"]]}
{"hexsha": "d91f10e4333abe30ae20c29c7c6ad3bcd4a827c6", "ext": "py", "lang": "Python", "content": "def test_init(ws):\n    \"\"\"\n    Test that :meth:`.Connection.__init__` draws attributes from the passed session and websocket handler.\n\n    \"\"\"\n    assert ws.user_id == \"test\"\n    assert ws.groups == [\"admin\", \"test\"]\n    assert ws.permissions == [\"create_sample\"]", "fn_id": 0, "class_fn": false, "repo": "ReeceHoffmann/virtool", "file": "tests/dispatcher/test_connection.py", "last_update_at": "2022-01-15T00:00:42+00:00", "question_id": "d91f10e4333abe30ae20c29c7c6ad3bcd4a827c6_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_init(ws):\n    \"\"\"\n    Test that :meth:`.Connection.__init__` draws attributes from the passed session and websocket handler.\n\n    \"\"\"\n    assert ws.user_id == 'test'\n    assert ws.groups == ['admin', 'test']\n"]]}
{"hexsha": "34492ccf6d8eb8df4f400edd6f8c5fd4455ea9a6", "ext": "py", "lang": "Python", "content": "def prepare_dataloaders(data, opt):\n    train_loader = torch.utils.data.DataLoader(\n        TedDataset(\n            src_word2idx=data['dict'],\n            src_insts=data['train']['src'],\n            tgt_insts=data['train']['tgt']\n            ),\n            num_workers=opt.n_workers,\n            batch_size=opt.batch_size,\n            collate_fn=partial(collate_fn, opt=opt),\n            shuffle=True)\n\n    valid_loader = torch.utils.data.DataLoader(\n        TedDataset(\n            src_word2idx=data['dict'],\n            src_insts=data['valid']['src'],\n            tgt_insts=data['valid']['tgt']\n            ),\n            num_workers=opt.n_workers,\n            batch_size=opt.batch_size,\n            collate_fn=partial(collate_fn, opt=opt))\n\n    return train_loader, valid_loader", "fn_id": 5, "class_fn": false, "repo": "ehwa009/Co-Speech_Gesture_Generation", "file": "train.py", "last_update_at": "2022-02-21T16:55:55+00:00", "question_id": "34492ccf6d8eb8df4f400edd6f8c5fd4455ea9a6_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def prepare_dataloaders(data, opt):\n    train_loader = torch.utils.data.DataLoader(TedDataset(src_word2idx=data['dict'], src_insts=data['train']['src'], tgt_insts=data['train']['tgt']), num_workers=opt.n_workers, batch_size=opt.batch_size, collate_fn=partial(collate_fn, opt=opt), shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(TedDataset(src_word2idx=data['dict'], src_insts=data['valid']['src'], tgt_insts=data['valid']['tgt']), num_workers=opt.n_workers, batch_size=opt.batch_size, collate_fn=partial(collate_fn, opt=opt))\n"]]}
{"hexsha": "abedbb71dc900007c5b4a1250241071aab24da8c", "ext": "py", "lang": "Python", "content": "@make_symbolic\ndef str_locate(string, pattern):\n    \"\"\"\n    Find the indices of all pattern matches.\n    \"\"\"\n    try:\n        if isinstance(string, str):\n            raise TypeError\n        return Series([[m.start(0) for m in re.finditer(pattern, s)] for s in string])\n\n    except TypeError:\n        return [m.start(0) for m in re.finditer(pattern, string)]", "fn_id": 1, "class_fn": false, "repo": "OscarDeGar/py_grama", "file": "grama/dfply/string_helpers.py", "last_update_at": "2022-03-30T18:56:55+00:00", "question_id": "abedbb71dc900007c5b4a1250241071aab24da8c_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@make_symbolic\ndef str_locate(string, pattern):\n    \"\"\"\n    Find the indices of all pattern matches.\n    \"\"\"\n    try:\n        if isinstance(string, str):\n            raise TypeError\n        return Series([[m.start(0) for m in re.finditer(pattern, s)] for s in string])\n    except TypeError:\n"]]}
{"hexsha": "91ed15839cd6e88d40d65fba2750b5249cff81c1", "ext": "py", "lang": "Python", "content": "def parse_options():\n    \"\"\" This parses the CLI arguments and validates the arguments\n        Returns:\n            the path where the LCT directory is located\n    \"\"\"\n    input_path = \"\"\n    \n    # read in flags passed in with command line argument\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], \"hf:\", \"help\")\n    except getopt.GetoptError as err:\n        print(err)\n        sys.exit(2)\n\n    # make sure that options which need an argument (namely -f for the input file path) have them\n    for opt, arg in opts:\n        if opt in (\"-h\", \"--help\"):\n            print(\"use -f to specify the directory of LVT dataset\")\n            sys.exit(2)\n        elif opt == \"-f\": # and len(opts) == 2:\n            input_path = arg\n        else:\n            # only reach here if the the arguments were incorrect\n            print(\"Invalid set of arguments entered. Please refer to -h flag for more information.\")\n            sys.exit(2)\n\n    return input_path  ", "fn_id": 0, "class_fn": false, "repo": "nicorev/LVT", "file": "src/lct.py", "last_update_at": "2022-03-08T23:41:54+00:00", "question_id": "91ed15839cd6e88d40d65fba2750b5249cff81c1_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def parse_options():\n    \"\"\" This parses the CLI arguments and validates the arguments\n        Returns:\n            the path where the LCT directory is located\n    \"\"\"\n    input_path = ''\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], 'hf:', 'help')\n    except getopt.GetoptError as err:\n        print(err)\n        sys.exit(2)\n    for opt, arg in opts:\n        if opt in ('-h', '--help'):\n            print('use -f to specify the directory of LVT dataset')\n            sys.exit(2)\n        elif opt == '-f':\n            input_path = arg\n        else:\n            print('Invalid set of arguments entered. Please refer to -h flag for more information.')\n            sys.exit(2)\n"]]}
{"hexsha": "0701be6728fd240c75a3fa2b0538b52e286bb1f8", "ext": "py", "lang": "Python", "content": "def stripes(num_images_per_class=100, img_shape=(16, 16), num_classes=4,\n            class_type=('vertical', 'horizontal', 'main_diagonal', 'off_diagonal')):\n\n    if num_classes > 4 or num_classes < 2:\n        raise ValueError('stripes data has minimum of 2 classes and maximum of 4 classes')\n\n    data = np.zeros([*img_shape, num_classes * num_images_per_class])\n\n    for i in range(num_images_per_class):\n\n        for j in range(num_classes):\n            if class_type[j] == 'vertical':\n                data[:, :, i + j * num_images_per_class] = np.kron(np.ones([img_shape[0], 1]), randn(1, img_shape[1]))\n\n            elif class_type[j] == 'horizontal':\n                data[:, :, i + j * num_images_per_class] = np.kron(np.ones([1, img_shape[0]]), randn(img_shape[1], 1))\n\n            elif class_type[j] == 'main_diagonal':\n                # diagonal (top left to bottom right)\n                data[:, :, i + j * num_images_per_class] = toeplitz(randn(img_shape[0]), randn(img_shape[1]))\n\n            elif class_type[j] == 'off_diagonal':\n                # diagonal (top right to bottom left)\n                data[:, :, i + j * num_images_per_class] = np.rot90(toeplitz(randn(img_shape[0]), randn(img_shape[1])),\n                                                                    1)\n\n    data = rescale(data)\n    labels = np.kron(np.arange(num_classes), np.ones([num_images_per_class]))\n\n    return data, labels", "fn_id": 0, "class_fn": false, "repo": "elizabethnewman/tensor-fmri", "file": "data/synthetic_data.py", "last_update_at": "2022-01-13T04:09:00+00:00", "question_id": "0701be6728fd240c75a3fa2b0538b52e286bb1f8_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def stripes(num_images_per_class=100, img_shape=(16, 16), num_classes=4, class_type=('vertical', 'horizontal', 'main_diagonal', 'off_diagonal')):\n    if num_classes > 4 or num_classes < 2:\n        raise ValueError('stripes data has minimum of 2 classes and maximum of 4 classes')\n    data = np.zeros([*img_shape, num_classes * num_images_per_class])\n    for i in range(num_images_per_class):\n        for j in range(num_classes):\n            if class_type[j] == 'vertical':\n                data[:, :, i + j * num_images_per_class] = np.kron(np.ones([img_shape[0], 1]), randn(1, img_shape[1]))\n            elif class_type[j] == 'horizontal':\n                data[:, :, i + j * num_images_per_class] = np.kron(np.ones([1, img_shape[0]]), randn(img_shape[1], 1))\n            elif class_type[j] == 'main_diagonal':\n                data[:, :, i + j * num_images_per_class] = toeplitz(randn(img_shape[0]), randn(img_shape[1]))\n            elif class_type[j] == 'off_diagonal':\n                data[:, :, i + j * num_images_per_class] = np.rot90(toeplitz(randn(img_shape[0]), randn(img_shape[1])), 1)\n    data = rescale(data)\n    labels = np.kron(np.arange(num_classes), np.ones([num_images_per_class]))\n"]]}
{"hexsha": "d4b6194eda92abd0e3e181ef505f7919702f6e19", "ext": "py", "lang": "Python", "content": "def one_cone(blue, yellow):\n    if len(blue)>0:\n        angle = STEER_MIN\n    elif len(yellow)>0:\n        angle = STEER_MAX\n    return angle", "fn_id": 4, "class_fn": false, "repo": "Syed-Sherjeel/NFST_Driverless_2019", "file": "controller2.py", "last_update_at": "2022-03-02T15:44:58+00:00", "question_id": "d4b6194eda92abd0e3e181ef505f7919702f6e19_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def one_cone(blue, yellow):\n    if len(blue) > 0:\n        angle = STEER_MIN\n    elif len(yellow) > 0:\n        angle = STEER_MAX\n"]]}
{"hexsha": "22cb200f94f200cb58be445266a937b6c09f6a50", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize(\n    \"command\",\n    [\n        \"iothub setup --update-dotenv\",\n        \"\"\n    ]\n)\ndef test_is_terse_command_true(command):\n    envvars = EnvVars(Output())\n    assert envvars.is_terse_command(command)", "fn_id": 8, "class_fn": false, "repo": "vikas0212git/iotedgedev", "file": "tests/test_envvars.py", "last_update_at": "2022-03-29T12:12:50+00:00", "question_id": "22cb200f94f200cb58be445266a937b6c09f6a50_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('command', ['iothub setup --update-dotenv', ''])\ndef test_is_terse_command_true(command):\n    envvars = EnvVars(Output())\n"]]}
{"hexsha": "0f205ff8b8cc8ce39ca7af0ebd51f8180a7e8acd", "ext": "py", "lang": "Python", "content": "def plot_1D_pics(k, est_csd, est_pots, tp, Fs, cut=9):\n    plt.figure(figsize=(12, 8))\n    # plt.suptitle('plane: '+str(k.estm_x[cut,0])+' $\\mu$m '+' $\\lambda$ : '+str(k.lambd)+\n                 # '  R: '+ str(k.R))\n    ax1 = plt.subplot(122)\n    set_axis(ax1, -0.05, 1.05, letter= 'D')\n    make_plot_spacetime(ax1, k.estm_x, k.estm_y, est_csd[cut,:,:], Fs,\n              title='Estimated CSD', cmap='bwr')\n    for lvl, name in zip([-500,-850,-2000], ['II/III', 'IV', 'V/VI']):\n        plt.axhline(lvl, ls='--', color='grey')\n        plt.text(340, lvl+20, name)\n    plt.xlim(250, 400)\n    plt.xticks([250, 300, 350, 400], [-50, 0, 50, 100])\n    ax2 = plt.subplot(121)\n    set_axis(ax2, -0.05, 1.05, letter= 'C')\n    make_plot_spacetime(ax2, k.estm_x, k.estm_y, est_pots[cut,:,:],\n              title='Estimated LFP', cmap='PRGn')\n    plt.axvline(tp/Fs*1000, ls='--', color ='grey', lw=2)\n    plt.xlim(250, 400)\n    plt.xticks([250, 300, 350, 400], [-50, 0, 50, 100])\n    plt.tight_layout()\n    plt.savefig('figure_1D_pics', dpi=300)", "fn_id": 5, "class_fn": false, "repo": "rdarie/kCSD-python", "file": "figures/npx/kCSD2D_reconstruction_from_npx.py", "last_update_at": "2022-02-07T21:17:13+00:00", "question_id": "0f205ff8b8cc8ce39ca7af0ebd51f8180a7e8acd_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def plot_1D_pics(k, est_csd, est_pots, tp, Fs, cut=9):\n    plt.figure(figsize=(12, 8))\n    ax1 = plt.subplot(122)\n    set_axis(ax1, -0.05, 1.05, letter='D')\n    make_plot_spacetime(ax1, k.estm_x, k.estm_y, est_csd[cut, :, :], Fs, title='Estimated CSD', cmap='bwr')\n    for lvl, name in zip([-500, -850, -2000], ['II/III', 'IV', 'V/VI']):\n        plt.axhline(lvl, ls='--', color='grey')\n        plt.text(340, lvl + 20, name)\n    plt.xlim(250, 400)\n    plt.xticks([250, 300, 350, 400], [-50, 0, 50, 100])\n    ax2 = plt.subplot(121)\n    set_axis(ax2, -0.05, 1.05, letter='C')\n    make_plot_spacetime(ax2, k.estm_x, k.estm_y, est_pots[cut, :, :], title='Estimated LFP', cmap='PRGn')\n    plt.axvline(tp / Fs * 1000, ls='--', color='grey', lw=2)\n    plt.xlim(250, 400)\n    plt.xticks([250, 300, 350, 400], [-50, 0, 50, 100])\n    plt.tight_layout()\n"]]}
{"hexsha": "0657de93d1e5b30e794f21550ae68d6cc11d1a46", "ext": "py", "lang": "Python", "content": "def ask():\n  options = ['Add User To The Spammer','Remove User From Spammer','Remove User From All Groupchats','Delete Groupchats','Rename Groupchats','Spam User','Change group icon','Help']\n  print(\"\\033[H\\033[J\", end=\"\")\n  print('[ ', end='', flush=True)\n  cprint('1', 'green', end='', flush=True)\n  print(' ] ', end='', flush=True)\n  print('Create Groupchats', end='', flush=True)\n  for x, i in enumerate(options):\n    print()\n    print()\n    print('[ ', end='', flush=True)\n    cprint(x + 2, 'green', end='', flush=True)\n    print(' ] ', end='', flush=True)\n    print(i, end='', flush=True)\n  print()\n  print()\n  choice = input(\"> \")\n  try:\n    if int(choice) == 1:\n      cprint('Creating Groupchats...', 'green')\n      create()\n      time.sleep(1)\n      ask()\n    elif int(choice) == 2:\n      cprint('Enter user id below', 'yellow')\n      id = input(\"\")\n      addUser(id)\n      cprint('Added user!', 'green')\n      time.sleep(1)\n      ask()\n    elif int(choice) == 3:\n      cprint('Enter user id below', 'yellow')\n      id = input(\"\")\n      removeUser(id)\n      time.sleep(1)\n      ask()\n    elif int(choice) == 4:\n      cprint('Enter user id below', 'yellow')\n      id = input(\"\")\n      remove(id)\n      cprint('Removed from all groupchats', 'green')\n      time.sleep(1)\n      ask()\n    elif int(choice) == 5:\n      delete()\n      cprint('Deleted all groupchats', 'green')\n      time.sleep(1)\n      ask()\n    elif int(choice) == 6:\n      cprint('Enter group chat names below', 'yellow')\n      name = input(\"\")\n      rename(str(name))\n      time.sleep(1)\n      ask()\n    elif int(choice) == 7:\n      cprint('Enter user id below', 'yellow')\n      id = input(\"\")\n      spam(id)\n      for char in 'Done spamming':\n        time.sleep(0.1)\n        cprint(char, 'magenta', end='', flush=True)\n      time.sleep(5)\n      ask()\n    elif int(choice) == 8:\n      cprint('Enter url to image below', 'yellow')\n      url = input(\"\")\n      try:\n        base64.b64encode(requests.get(url).content)\n      except:\n        cprint('Invalid Url', 'red')\n        time.sleep(1.5)\n        ask()\n      url = str(base64.b64encode(requests.get(url).content)).replace(\"b'\", '')\n      changeImg(f'data:image/png;base64,{url}')\n      ask()\n    elif int(choice) == 9:\n      cprint(\"1.)To create groupchats is to prepare groupchats to add them to. The more groupchats the more pings for them. \\n\\n2.)To add someone to the spammer is to add them to the list of people who get removed and added to increase the amount of pings. MAKE SURE YOU HAVE THEM ADDED And to remove them is vice versa. You don't need to have them added to remove them.\\n\\n3.)To delete the group chats is self explanatory. To remove someone from all the groupchats is just to kick them from the groups you just added them to. To spam them is to start the spammer.\", 'green')\n      input(\"Press Enter To Exit\")\n      ask()\n    else:\n      ask()\n  except:\n    ask()", "fn_id": 12, "class_fn": false, "repo": "LucobIsGreat/Discord-Group-Chat-Spammer", "file": "main.py", "last_update_at": "2022-03-26T01:24:15+00:00", "question_id": "0657de93d1e5b30e794f21550ae68d6cc11d1a46_12", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ask():\n    options = ['Add User To The Spammer', 'Remove User From Spammer', 'Remove User From All Groupchats', 'Delete Groupchats', 'Rename Groupchats', 'Spam User', 'Change group icon', 'Help']\n    print('\\x1b[H\\x1b[J', end='')\n    print('[ ', end='', flush=True)\n    cprint('1', 'green', end='', flush=True)\n    print(' ] ', end='', flush=True)\n    print('Create Groupchats', end='', flush=True)\n    for x, i in enumerate(options):\n        print()\n        print()\n        print('[ ', end='', flush=True)\n        cprint(x + 2, 'green', end='', flush=True)\n        print(' ] ', end='', flush=True)\n        print(i, end='', flush=True)\n    print()\n    print()\n    choice = input('> ')\n    try:\n        if int(choice) == 1:\n            cprint('Creating Groupchats...', 'green')\n            create()\n            time.sleep(1)\n            ask()\n        elif int(choice) == 2:\n            cprint('Enter user id below', 'yellow')\n            id = input('')\n            addUser(id)\n            cprint('Added user!', 'green')\n            time.sleep(1)\n            ask()\n        elif int(choice) == 3:\n            cprint('Enter user id below', 'yellow')\n            id = input('')\n            removeUser(id)\n            time.sleep(1)\n            ask()\n        elif int(choice) == 4:\n            cprint('Enter user id below', 'yellow')\n            id = input('')\n            remove(id)\n            cprint('Removed from all groupchats', 'green')\n            time.sleep(1)\n            ask()\n        elif int(choice) == 5:\n            delete()\n            cprint('Deleted all groupchats', 'green')\n            time.sleep(1)\n            ask()\n        elif int(choice) == 6:\n            cprint('Enter group chat names below', 'yellow')\n            name = input('')\n            rename(str(name))\n            time.sleep(1)\n            ask()\n        elif int(choice) == 7:\n            cprint('Enter user id below', 'yellow')\n            id = input('')\n            spam(id)\n            for char in 'Done spamming':\n                time.sleep(0.1)\n                cprint(char, 'magenta', end='', flush=True)\n            time.sleep(5)\n            ask()\n        elif int(choice) == 8:\n            cprint('Enter url to image below', 'yellow')\n            url = input('')\n            try:\n                base64.b64encode(requests.get(url).content)\n            except:\n                cprint('Invalid Url', 'red')\n                time.sleep(1.5)\n                ask()\n            url = str(base64.b64encode(requests.get(url).content)).replace(\"b'\", '')\n            changeImg(f'data:image/png;base64,{url}')\n            ask()\n        elif int(choice) == 9:\n            cprint(\"1.)To create groupchats is to prepare groupchats to add them to. The more groupchats the more pings for them. \\n\\n2.)To add someone to the spammer is to add them to the list of people who get removed and added to increase the amount of pings. MAKE SURE YOU HAVE THEM ADDED And to remove them is vice versa. You don't need to have them added to remove them.\\n\\n3.)To delete the group chats is self explanatory. To remove someone from all the groupchats is just to kick them from the groups you just added them to. To spam them is to start the spammer.\", 'green')\n            input('Press Enter To Exit')\n            ask()\n        else:\n            ask()\n    except:\n"]]}
{"hexsha": "4c73435f64aa998e77846a2f2119fef0969e7063", "ext": "py", "lang": "Python", "content": "def ensure_ordering(\n    expression: Expression,\n    *,\n    ordering: OrderingHint = None,\n) -> Sequence[Variable]:\n    \"\"\"Get a canonical ordering of the variables in the expression, or pass one through.\n\n    The canonical ordering of the variables in a given expression is based on the alphabetical\n    sort order of the variables based on their names.\n\n    :param expression: The expression to get a canonical ordering from.\n    :param ordering: A given ordering to pass through if not none, otherwise calculate it.\n    :returns: The ordering\n    \"\"\"\n    if ordering is not None:\n        return _upgrade_ordering(ordering)\n    # use alphabetical ordering\n    return _sorted_variables(expression.get_variables())", "fn_id": 2, "class_fn": false, "repo": "y0-causal-inference/y0", "file": "src/y0/dsl.py", "last_update_at": "2022-03-29T17:26:52+00:00", "question_id": "4c73435f64aa998e77846a2f2119fef0969e7063_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ensure_ordering(expression: Expression, *, ordering: OrderingHint=None) -> Sequence[Variable]:\n    \"\"\"Get a canonical ordering of the variables in the expression, or pass one through.\n\n    The canonical ordering of the variables in a given expression is based on the alphabetical\n    sort order of the variables based on their names.\n\n    :param expression: The expression to get a canonical ordering from.\n    :param ordering: A given ordering to pass through if not none, otherwise calculate it.\n    :returns: The ordering\n    \"\"\"\n    if ordering is not None:\n        return _upgrade_ordering(ordering)\n"]]}
{"hexsha": "0c7590281c52f053295c7c2733b2977deee1453a", "ext": "py", "lang": "Python", "content": "def logout(request):\n    if request.method == 'POST' or not request.user.is_authenticated():\n        auth.logout(request)\n        return redirect('static:landing')\n\n    return render(request, 'account/logout.html')", "fn_id": 1, "class_fn": false, "repo": "takeyourmeds/takeyourmeds-web", "file": "takeyourmeds/account/views.py", "last_update_at": "2022-03-01T01:20:58+00:00", "question_id": "0c7590281c52f053295c7c2733b2977deee1453a_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def logout(request):\n    if request.method == 'POST' or not request.user.is_authenticated():\n        auth.logout(request)\n        return redirect('static:landing')\n"]]}
{"hexsha": "324ac65784fbe6385a130a28d05fe4f86fa50f06", "ext": "py", "lang": "Python", "content": "def test_ode_coupled_mNone_generator():\n    # coupled equations\n    m = None\n    k = np.array([0.0, 6.0e5, 6.0e5, 6.0e5])  # diagonal of stiffness\n    zeta = np.array([0.0, 0.05, 1.0, 2.0])  # percent damping\n    b = 2.0 * zeta * np.sqrt(k)  # diagonal of damping\n    k = np.diag(k)\n    b = np.diag(b)\n\n    k[1:, 1:] += np.random.randn(3, 3) * 1000\n    b[1:, 1:] += np.random.randn(3, 3)\n\n    h = 0.001  # time step\n    t = np.arange(0, 0.3001, h)  # time vector\n    c = 2 * np.pi\n    f = (\n        np.vstack(\n            (\n                3 * (1 - np.cos(c * 2 * t)),  # forcing function\n                4 * (np.cos(np.sqrt(6e5 / 30) * t)),\n                5 * (np.cos(np.sqrt(6e5 / 30) * t)),\n                6 * (np.cos(np.sqrt(6e5 / 30) * t)),\n            )\n        )\n        * 1.0e4\n    )\n\n    for order in (0, 1):\n        for rf in (None, 3, np.array([1, 2, 3])):\n            for static_ic in (0, 1):\n                # su\n                tsu = ode.SolveUnc(m, b, k, h, order=order, rf=rf)\n                solu = tsu.tsolve(f, static_ic=static_ic)\n\n                nt = f.shape[1]\n                gen, d, v = tsu.generator(nt, f[:, 0], static_ic=static_ic)\n                for i in range(1, nt):\n                    gen.send((i, f[:, i]))\n                solu2 = tsu.finalize()\n\n                tsu0 = ode.SolveUnc(m, b, k, h=None, order=order, rf=rf)\n                solu0 = tsu0.tsolve(f[:, :1], static_ic=static_ic)\n                gen, d, v = tsu0.generator(1, f[:, 0], static_ic=static_ic)\n                solu20 = tsu0.finalize()\n\n                assert np.allclose(solu2.a, solu.a)\n                assert np.allclose(solu2.v, solu.v)\n                assert np.allclose(solu2.d, solu.d)\n\n                assert np.allclose(solu0.a, solu.a[:, :1])\n                assert np.allclose(solu0.v, solu.v[:, :1])\n                assert np.allclose(solu0.d, solu.d[:, :1])\n\n                assert np.allclose(solu20.a, solu2.a[:, :1])\n                assert np.allclose(solu20.v, solu2.v[:, :1])\n                assert np.allclose(solu20.d, solu2.d[:, :1])\n\n                # test the generator solver w/ partial update:\n                nt = f.shape[1]\n                gen, d, v = tsu.generator(nt, f[:, 0], static_ic=static_ic)\n                for i in range(1, nt):\n                    fi = f[:, i] / 2\n                    gen.send((i, fi))\n                    gen.send((-1, fi))\n                solu2 = tsu.finalize()\n\n                assert np.allclose(solu2.a, solu.a)\n                assert np.allclose(solu2.v, solu.v)\n                assert np.allclose(solu2.d, solu.d)\n\n                # se\n                tse = ode.SolveExp2(m, b, k, h, order=order, rf=rf)\n                sole = tse.tsolve(f, static_ic=static_ic)\n\n                nt = f.shape[1]\n                gen, d, v = tse.generator(nt, f[:, 0], static_ic=static_ic)\n                for i in range(1, nt):\n                    gen.send((i, f[:, i]))\n                sole2 = tse.finalize()\n\n                tse0 = ode.SolveExp2(m, b, k, h=None, order=order, rf=rf)\n                sole0 = tse0.tsolve(f[:, :1], static_ic=static_ic)\n                gen, d, v = tse0.generator(1, f[:, 0], static_ic=static_ic)\n                sole20 = tse0.finalize()\n\n                assert np.allclose(sole2.a, sole.a)\n                assert np.allclose(sole2.v, sole.v)\n                assert np.allclose(sole2.d, sole.d)\n\n                assert np.allclose(solu.a, sole.a)\n                assert np.allclose(solu.v, sole.v)\n                assert np.allclose(solu.d, sole.d)\n\n                assert np.allclose(sole0.a, sole.a[:, :1])\n                assert np.allclose(sole0.v, sole.v[:, :1])\n                assert np.allclose(sole0.d, sole.d[:, :1])\n\n                assert np.allclose(sole20.a, sole2.a[:, :1])\n                assert np.allclose(sole20.v, sole2.v[:, :1])\n                assert np.allclose(sole20.d, sole2.d[:, :1])\n\n                # test the generator solver w/ partial update:\n                nt = f.shape[1]\n                gen, d, v = tse.generator(nt, f[:, 0], static_ic=static_ic)\n                for i in range(1, nt):\n                    fi = f[:, i] / 2\n                    gen.send((i, fi))\n                    gen.send((-1, fi))\n                sole2 = tse.finalize()\n\n                assert np.allclose(sole2.a, sole.a)\n                assert np.allclose(sole2.v, sole.v)\n                assert np.allclose(sole2.d, sole.d)", "fn_id": 72, "class_fn": false, "repo": "twmacro/pyye", "file": "tests/test_ode.py", "last_update_at": "2022-03-18T08:41:56+00:00", "question_id": "324ac65784fbe6385a130a28d05fe4f86fa50f06_72", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_ode_coupled_mNone_generator():\n    m = None\n    k = np.array([0.0, 600000.0, 600000.0, 600000.0])\n    zeta = np.array([0.0, 0.05, 1.0, 2.0])\n    b = 2.0 * zeta * np.sqrt(k)\n    k = np.diag(k)\n    b = np.diag(b)\n    k[1:, 1:] += np.random.randn(3, 3) * 1000\n    b[1:, 1:] += np.random.randn(3, 3)\n    h = 0.001\n    t = np.arange(0, 0.3001, h)\n    c = 2 * np.pi\n    f = np.vstack((3 * (1 - np.cos(c * 2 * t)), 4 * np.cos(np.sqrt(600000.0 / 30) * t), 5 * np.cos(np.sqrt(600000.0 / 30) * t), 6 * np.cos(np.sqrt(600000.0 / 30) * t))) * 10000.0\n    for order in (0, 1):\n        for rf in (None, 3, np.array([1, 2, 3])):\n            for static_ic in (0, 1):\n                tsu = ode.SolveUnc(m, b, k, h, order=order, rf=rf)\n                solu = tsu.tsolve(f, static_ic=static_ic)\n                nt = f.shape[1]\n                gen, d, v = tsu.generator(nt, f[:, 0], static_ic=static_ic)\n                for i in range(1, nt):\n                    gen.send((i, f[:, i]))\n                solu2 = tsu.finalize()\n                tsu0 = ode.SolveUnc(m, b, k, h=None, order=order, rf=rf)\n                solu0 = tsu0.tsolve(f[:, :1], static_ic=static_ic)\n                gen, d, v = tsu0.generator(1, f[:, 0], static_ic=static_ic)\n                solu20 = tsu0.finalize()\n                assert np.allclose(solu2.a, solu.a)\n                assert np.allclose(solu2.v, solu.v)\n                assert np.allclose(solu2.d, solu.d)\n                assert np.allclose(solu0.a, solu.a[:, :1])\n                assert np.allclose(solu0.v, solu.v[:, :1])\n                assert np.allclose(solu0.d, solu.d[:, :1])\n                assert np.allclose(solu20.a, solu2.a[:, :1])\n                assert np.allclose(solu20.v, solu2.v[:, :1])\n                assert np.allclose(solu20.d, solu2.d[:, :1])\n                nt = f.shape[1]\n                gen, d, v = tsu.generator(nt, f[:, 0], static_ic=static_ic)\n                for i in range(1, nt):\n                    fi = f[:, i] / 2\n                    gen.send((i, fi))\n                    gen.send((-1, fi))\n                solu2 = tsu.finalize()\n                assert np.allclose(solu2.a, solu.a)\n                assert np.allclose(solu2.v, solu.v)\n                assert np.allclose(solu2.d, solu.d)\n                tse = ode.SolveExp2(m, b, k, h, order=order, rf=rf)\n                sole = tse.tsolve(f, static_ic=static_ic)\n                nt = f.shape[1]\n                gen, d, v = tse.generator(nt, f[:, 0], static_ic=static_ic)\n                for i in range(1, nt):\n                    gen.send((i, f[:, i]))\n                sole2 = tse.finalize()\n                tse0 = ode.SolveExp2(m, b, k, h=None, order=order, rf=rf)\n                sole0 = tse0.tsolve(f[:, :1], static_ic=static_ic)\n                gen, d, v = tse0.generator(1, f[:, 0], static_ic=static_ic)\n                sole20 = tse0.finalize()\n                assert np.allclose(sole2.a, sole.a)\n                assert np.allclose(sole2.v, sole.v)\n                assert np.allclose(sole2.d, sole.d)\n                assert np.allclose(solu.a, sole.a)\n                assert np.allclose(solu.v, sole.v)\n                assert np.allclose(solu.d, sole.d)\n                assert np.allclose(sole0.a, sole.a[:, :1])\n                assert np.allclose(sole0.v, sole.v[:, :1])\n                assert np.allclose(sole0.d, sole.d[:, :1])\n                assert np.allclose(sole20.a, sole2.a[:, :1])\n                assert np.allclose(sole20.v, sole2.v[:, :1])\n                assert np.allclose(sole20.d, sole2.d[:, :1])\n                nt = f.shape[1]\n                gen, d, v = tse.generator(nt, f[:, 0], static_ic=static_ic)\n                for i in range(1, nt):\n                    fi = f[:, i] / 2\n                    gen.send((i, fi))\n                    gen.send((-1, fi))\n                sole2 = tse.finalize()\n                assert np.allclose(sole2.a, sole.a)\n                assert np.allclose(sole2.v, sole.v)\n"]]}
{"hexsha": "3eeafb978c60879f7eee0dda34c20d1f046224cd", "ext": "py", "lang": "Python", "content": "def entities_recognition(dataset_id, domain_id, correspondances, fields, api_key, language):\n    records = records_v2(domain_id, dataset_id, rows=ROWS_NUMBER, api_key=api_key)\n    for name in fields:\n        class_correspondance = get_field_class(records['records'], fields[name], language)\n        if class_correspondance:\n            correspondances['classes'].append(class_correspondance)\n            update_field_class(fields, name, class_correspondance)", "fn_id": 2, "class_fn": false, "repo": "CoronaWhy/semantic-bot", "file": "chatbot/automatic.py", "last_update_at": "2022-03-01T19:12:32+00:00", "question_id": "3eeafb978c60879f7eee0dda34c20d1f046224cd_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def entities_recognition(dataset_id, domain_id, correspondances, fields, api_key, language):\n    records = records_v2(domain_id, dataset_id, rows=ROWS_NUMBER, api_key=api_key)\n    for name in fields:\n        class_correspondance = get_field_class(records['records'], fields[name], language)\n        if class_correspondance:\n            correspondances['classes'].append(class_correspondance)\n"]]}
{"hexsha": "4c7b73d5f42124d0da837b2071791b3974ff48c0", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize(\n    'status',\n    [\n        'released',\n        'in progress',\n        'deleted',\n    ]\n)\ndef test_analysis_step_run_valid_statuses(status, testapp, analysis_step_run):\n    testapp.patch_json(analysis_step_run['@id'], {'status': status})\n    res = testapp.get(analysis_step_run['@id'] + '@@embedded').json\n    assert res['status'] == status", "fn_id": 0, "class_fn": false, "repo": "KCL-ORG/encoded", "file": "src/encoded/tests/test_schema_analysis_step_run.py", "last_update_at": "2022-03-07T06:03:55+00:00", "question_id": "4c7b73d5f42124d0da837b2071791b3974ff48c0_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('status', ['released', 'in progress', 'deleted'])\ndef test_analysis_step_run_valid_statuses(status, testapp, analysis_step_run):\n    testapp.patch_json(analysis_step_run['@id'], {'status': status})\n    res = testapp.get(analysis_step_run['@id'] + '@@embedded').json\n"]]}
{"hexsha": "001f8cd2f55ca8406127d6220844b96209a6b15a", "ext": "py", "lang": "Python", "content": "def get_offsets(dtype, interleave, band, width, length, num_bands=1):\n    \"\"\"\n    From ISCE Image.py\n    \"\"\"\n    bytes_per_pix = np.dtype(dtype).itemsize\n    # In this single-band case, all choices are the same\n    if band == 0 and num_bands == 1:\n        return (\n            width * bytes_per_pix,  # ImageOffset\n            bytes_per_pix,  # PixelOffset\n            width * bytes_per_pix,  # LineOffset\n        )\n    # otherwise, get the specific interleave options\n    if interleave == \"BIL\":\n        return (\n            band * width * bytes_per_pix,  # ImageOffset\n            bytes_per_pix,  # PixelOffset\n            num_bands * width * bytes_per_pix,  # LineOffset\n        )\n    elif interleave == \"BIP\":\n        return (\n            band * bytes_per_pix,\n            num_bands * bytes_per_pix,\n            num_bands * width * bytes_per_pix,\n        )\n    elif interleave == \"BSQ\":\n        return (\n            band * width * length * bytes_per_pix,\n            bytes_per_pix,\n            width * bytes_per_pix,\n        )\n    else:\n        raise ValueError(\"Unknown interleave: %s\" % interleave)", "fn_id": 48, "class_fn": false, "repo": "scottstanie/apertools", "file": "apertools/sario.py", "last_update_at": "2022-02-16T02:49:57+00:00", "question_id": "001f8cd2f55ca8406127d6220844b96209a6b15a_48", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_offsets(dtype, interleave, band, width, length, num_bands=1):\n    \"\"\"\n    From ISCE Image.py\n    \"\"\"\n    bytes_per_pix = np.dtype(dtype).itemsize\n    if band == 0 and num_bands == 1:\n        return (width * bytes_per_pix, bytes_per_pix, width * bytes_per_pix)\n    if interleave == 'BIL':\n        return (band * width * bytes_per_pix, bytes_per_pix, num_bands * width * bytes_per_pix)\n    elif interleave == 'BIP':\n        return (band * bytes_per_pix, num_bands * bytes_per_pix, num_bands * width * bytes_per_pix)\n    elif interleave == 'BSQ':\n        return (band * width * length * bytes_per_pix, bytes_per_pix, width * bytes_per_pix)\n    else:\n"]]}
{"hexsha": "20f0862385b561d54b6eece64aaa00c25f04d309", "ext": "py", "lang": "Python", "content": "@pytest.fixture(scope='session')\ndef variables_datacenter():\n    variables = {}\n    variables['observation'] = DEFAULT_DATACENTER_OBSERVATION_VARIABLES\n    variables['action'] = DEFAULT_DATACENTER_ACTION_VARIABLES\n    return variables", "fn_id": 1, "class_fn": false, "repo": "jajimer/sinergym", "file": "tests/conftest.py", "last_update_at": "2022-03-29T13:27:39+00:00", "question_id": "20f0862385b561d54b6eece64aaa00c25f04d309_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.fixture(scope='session')\ndef variables_datacenter():\n    variables = {}\n    variables['observation'] = DEFAULT_DATACENTER_OBSERVATION_VARIABLES\n    variables['action'] = DEFAULT_DATACENTER_ACTION_VARIABLES\n"]]}
{"hexsha": "962cfe04a5d241bac7617ae5d86a5dc593741238", "ext": "py", "lang": "Python", "content": "def getItemsForPicsDownloading( iLimit = 50, iTooOldDays = 95 ):\n    #\n    # bGetPictures = True, not implemented yet\n    #\n    tTooOld = timezone.now() - timezone.timedelta( iTooOldDays )\n    #\n    qsGetPics = Keeper.objects.filter(\n                    tGotPictures__isnull = True,\n                    tTimeEnd__gt         = tTooOld,\n                    iBidCount__gt        = 0,\n                ).order_by( 'tTimeEnd'\n                ).values_list( 'iItemNumb', flat = True\n                )[ : iLimit ]\n    #\n    iWantPics = iLimit / 10\n    #\n    if iLimit > len( qsGetPics ):\n        #\n        iWantPics = iLimit - len( qsGetPics )\n        #\n    #\n    # qsZeroBids = Keeper.objects.filter(\n    #                 tGotPictures__isnull = True, iBidCount = 0\n    #                 ).values_list( 'iItemNumb', flat = True )\n    #\n    qsZeroBids = Keeper.objects.filter(\n                        iBidCount           = 0,\n                        tTimeEnd__gt        = tTooOld,\n                        cListingType        = 'FixedPriceItem',\n                        tGotPictures__isnull= True\n                    ).values_list( 'iItemNumb', flat = True\n                    )[ : iWantPics ]\n    #\n    return qsGetPics.union( qsZeroBids )", "fn_id": 15, "class_fn": false, "repo": "netvigator/auctions", "file": "keepers/utils.py", "last_update_at": "2022-03-07T12:59:27+00:00", "question_id": "962cfe04a5d241bac7617ae5d86a5dc593741238_15", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def getItemsForPicsDownloading(iLimit=50, iTooOldDays=95):\n    tTooOld = timezone.now() - timezone.timedelta(iTooOldDays)\n    qsGetPics = Keeper.objects.filter(tGotPictures__isnull=True, tTimeEnd__gt=tTooOld, iBidCount__gt=0).order_by('tTimeEnd').values_list('iItemNumb', flat=True)[:iLimit]\n    iWantPics = iLimit / 10\n    if iLimit > len(qsGetPics):\n        iWantPics = iLimit - len(qsGetPics)\n    qsZeroBids = Keeper.objects.filter(iBidCount=0, tTimeEnd__gt=tTooOld, cListingType='FixedPriceItem', tGotPictures__isnull=True).values_list('iItemNumb', flat=True)[:iWantPics]\n"]]}
{"hexsha": "8f93ab9ae8042a57f2d0acfbbf7ce9221e376616", "ext": "py", "lang": "Python", "content": "def restart():\n    file = initDreplace()\n    data = mw.execShell(file + ' restart')\n    if data[1] == '':\n        return 'ok'\n    return data[1]", "fn_id": 12, "class_fn": false, "repo": "midoks/mdserver-web", "file": "plugins/sphinx/index.py", "last_update_at": "2022-03-30T15:04:26+00:00", "question_id": "8f93ab9ae8042a57f2d0acfbbf7ce9221e376616_12", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def restart():\n    file = initDreplace()\n    data = mw.execShell(file + ' restart')\n    if data[1] == '':\n        return 'ok'\n"]]}
{"hexsha": "24ce4af8344af3405e9520095667560f594757a5", "ext": "py", "lang": "Python", "content": "def check_valid_value(value, valid_values):\n    \"\"\" Returns None if the received `value`\n    is not contained in the received `valid_values`\n    \"\"\"\n    if value in valid_values:\n        return value\n    return None", "fn_id": 0, "class_fn": false, "repo": "rafaelhn2021/proyecto", "file": "declaraciones/api/utils.py", "last_update_at": "2022-03-18T10:26:38+00:00", "question_id": "24ce4af8344af3405e9520095667560f594757a5_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def check_valid_value(value, valid_values):\n    \"\"\" Returns None if the received `value`\n    is not contained in the received `valid_values`\n    \"\"\"\n    if value in valid_values:\n        return value\n"]]}
{"hexsha": "d9bebef25663f593d56227239c5a3902d9ba6932", "ext": "py", "lang": "Python", "content": "def convert_norm_act(norm_layer, act_layer):\n    assert isinstance(norm_layer, (type, str,  types.FunctionType, functools.partial))\n    assert act_layer is None or isinstance(act_layer, (type, str, types.FunctionType, functools.partial))\n    norm_act_kwargs = {}\n\n    # unbind partial fn, so args can be rebound later\n    if isinstance(norm_layer, functools.partial):\n        norm_act_kwargs.update(norm_layer.keywords)\n        norm_layer = norm_layer.func\n\n    if isinstance(norm_layer, str):\n        norm_act_layer = get_norm_act_layer(norm_layer)\n    elif norm_layer in _NORM_ACT_TYPES:\n        norm_act_layer = norm_layer\n    elif isinstance(norm_layer,  types.FunctionType):\n        # if function type, must be a lambda/fn that creates a norm_act layer\n        norm_act_layer = norm_layer\n    else:\n        type_name = norm_layer.__name__.lower()\n        if type_name.startswith('batchnorm'):\n            norm_act_layer = BatchNormAct2d\n        elif type_name.startswith('groupnorm'):\n            norm_act_layer = GroupNormAct\n        else:\n            assert False, f\"No equivalent norm_act layer for {type_name}\"\n\n    if norm_act_layer in _NORM_ACT_REQUIRES_ARG:\n        # pass `act_layer` through for backwards compat where `act_layer=None` implies no activation.\n        # In the future, may force use of `apply_act` with `act_layer` arg bound to relevant NormAct types\n        norm_act_kwargs.setdefault('act_layer', act_layer)\n    if norm_act_kwargs:\n        norm_act_layer = functools.partial(norm_act_layer, **norm_act_kwargs)  # bind/rebind args\n    return norm_act_layer", "fn_id": 2, "class_fn": false, "repo": "Ascend/modelzoo", "file": "contrib/PyTorch/Official/cv/image_classification/SPNASNet_100_for_PyTorch/timm/models/layers/create_norm_act.py", "last_update_at": "2022-03-20T15:17:17+00:00", "question_id": "d9bebef25663f593d56227239c5a3902d9ba6932_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def convert_norm_act(norm_layer, act_layer):\n    assert isinstance(norm_layer, (type, str, types.FunctionType, functools.partial))\n    assert act_layer is None or isinstance(act_layer, (type, str, types.FunctionType, functools.partial))\n    norm_act_kwargs = {}\n    if isinstance(norm_layer, functools.partial):\n        norm_act_kwargs.update(norm_layer.keywords)\n        norm_layer = norm_layer.func\n    if isinstance(norm_layer, str):\n        norm_act_layer = get_norm_act_layer(norm_layer)\n    elif norm_layer in _NORM_ACT_TYPES:\n        norm_act_layer = norm_layer\n    elif isinstance(norm_layer, types.FunctionType):\n        norm_act_layer = norm_layer\n    else:\n        type_name = norm_layer.__name__.lower()\n        if type_name.startswith('batchnorm'):\n            norm_act_layer = BatchNormAct2d\n        elif type_name.startswith('groupnorm'):\n            norm_act_layer = GroupNormAct\n        else:\n            assert False, f'No equivalent norm_act layer for {type_name}'\n    if norm_act_layer in _NORM_ACT_REQUIRES_ARG:\n        norm_act_kwargs.setdefault('act_layer', act_layer)\n    if norm_act_kwargs:\n        norm_act_layer = functools.partial(norm_act_layer, **norm_act_kwargs)\n"]]}
{"hexsha": "c41a47e6b90f9ec11a16c23788a2988a40e6c2b6", "ext": "py", "lang": "Python", "content": "def rawmoment(slc, sqc, scp, vm, k):\n    \"\"\"\n    This is where the resultant distribution moments are calculated. MODIFY \n    THIS CODE AT YOUR OWN RISK. These equations have been verified with\n    published equations and example problems by N.D. Cox.\n    \n    Each of the derivative components need to be standardized prior to input\n    to this function. This means multiplying them by their respective\n    standard deviations, depending on the order of the derivative. Helper\n    functions have been defined for this purpose (standard_lc, standard_qc,\n    and standard_cp). However, this is only necessary if manually calling this\n    function rather than using soerp_numeric below.\n    \n    Parameters\n    ----------\n    slc : array\n        The standardized first derivative terms.\n    sqc : array\n        The standardized pure second derivative terms\n    scp : 2d-array\n        The standardized cross-product second derivative terms\n    vm : 2d-array\n        The first 9 (starting at 0) standardized distribution moments (one \n        row for each input variable, corresponding to the derivative array \n        order). See the documentation for ``soerp_numeric`` for more details.\n    k : int\n        The kth distribution moment to calculate.\n    \n    Returns\n    -------\n    rm : scalar\n        The kth raw distribution moment\n    \"\"\"\n    lc = copy(slc)\n    qc = copy(sqc)\n    cp = copy(scp)\n    n = len(lc)\n    \n    if assume_linear:\n        qc[:] = 0.0\n        cp[:,:] = 0.0\n        \n    ans = 0.0\n        \n    ############################\n    # The 0th raw moment\n    \n    if k==0:\n        ans = 1\n        \n    ############################\n    # The 1st raw moment\n    \n    elif k==1:\n        for i in range(n):\n            ans += qc[i]*vm[i,2]\n    \n    ############################\n    # The 2nd raw moment\n    \n    elif k==2:\n        for i in range(n):\n            ans += lc[i]**2*vm[i,2] + 2*lc[i]*qc[i]*vm[i,3] + qc[i]**2*vm[i,4]\n        \n        if n>=2:\n            for i in range(n-1):\n                for j in range(i+1, n):\n                    ans += (2*qc[i]*qc[j] + cp[i,j]**2)*vm[i,2]*vm[j,2]\n    \n    ############################\n    # The 3rd raw moment\n    \n    elif k==3:\n        for i in range(n):\n            ans += lc[i]**3*vm[i,3] + qc[i]**3*vm[i,6] + \\\n                   3*lc[i]**2*qc[i]*vm[i,4] + 3*lc[i]*qc[i]**2*vm[i,5]\n        \n        if n>=2:\n            for i in range(n-1):\n                for j in range(i+1, n):\n                    ans += cp[i,j]**3*vm[i,3]*vm[j,3] + \\\n                           6*lc[i]*lc[j]*cp[i,j]*vm[i,2]*vm[j,2] + \\\n                           6*qc[i]*qc[j]*cp[i,j]*vm[i,3]*vm[j,3]\n            for i in range(n):\n                for j in range(n):\n                    if j!=i:\n                        ans += 3*qc[i]**2*vm[i,4]*qc[j]*vm[j,2] + \\\n                               6*lc[i]*qc[j]*cp[i,j]*vm[i,2]*vm[j,3] +\\\n                               3*qc[i]*lc[j]**2*vm[i,2]*vm[j,2] + \\\n                               6*lc[i]*qc[i]*qc[j]*vm[i,3]*vm[j,2] + \\\n                               3*lc[i]*cp[i,j]**2*vm[i,3]*vm[j,2] + \\\n                               3*qc[i]*cp[i,j]**2*vm[i,4]*vm[j,2]\n        \n        if n>=3:\n            for i in range(n-2):\n                for j in range(i+1, n-1):\n                    for k in range(j+1, n):\n                        ans += (6*qc[i]*qc[j]*qc[k] + \\\n                                6*cp[i,j]*cp[i,k]*cp[j,k] + \n                                3*(qc[i]*cp[j,k]**2 + \\\n                                qc[j]*cp[i,k]**2 + \\\n                                qc[k]*cp[i,j]**2))*vm[i,2]*vm[j,2]*vm[k,1]\n    \n    ############################\n    # The 4th raw moment\n    \n    elif k==4:\n        for i in range(n):\n            ans += lc[i]**4*vm[i,4] + qc[i]**4*vm[i,8] + \\\n                   4*lc[i]**3*qc[i]*vm[i,5] + 4*lc[i]*qc[i]**3*vm[i,7] + \\\n                   6*lc[i]**2*qc[i]**2*vm[i,6]\n        \n        if n>=2:\n            for i in range(n-1):\n                for j in range(i+1, n):\n                    ans += 6*lc[i]**2*lc[j]**2*vm[i,2]*vm[j,2] + \\\n                           6*qc[i]**2*qc[j]**2*vm[i,4]*vm[j,4] + \\\n                           cp[i,j]**4*vm[i,4]*vm[j,4] + \\\n                           12*cp[i,j]*(lc[i]**2*lc[j]*vm[i,3]*vm[j,2] + \\\n                                       lc[i]*lc[j]**2*vm[i,2]*vm[j,3]) + \\\n                           12*cp[i,j]*qc[i]*qc[j]*(qc[i]*vm[i,5]*vm[j,3] + \\\n                                                   qc[j]*vm[i,3]*vm[j,6]) + \\\n                           12*qc[i]*qc[j]*(lc[i]**2*vm[i,4]*vm[j,2] + \\\n                                           lc[j]**2*vm[i,2]*vm[j,4] + \\\n                                           2*lc[i]*lc[j]*vm[i,3]*vm[j,3]) + \\\n                           6*cp[i,j]**2*(lc[i]**2*vm[i,4]*vm[j,2] + \\\n                                         lc[j]**2*vm[i,2]*vm[j,4] + \\\n                                         2*lc[i]*lc[j]*vm[i,3]*vm[j,3]) + \\\n                           6*cp[i,j]**2*(qc[i]**2*vm[i,6]*vm[j,2] + \\\n                                         qc[j]**2*vm[i,2]*vm[j,6] + \\\n                                         2*qc[i]*qc[j]*vm[i,4]*vm[j,4]) + \\\n                           12*cp[i,j]*(lc[j]*qc[i]*(lc[j]*vm[i,3]*vm[j,3] + \\\n                                                    2*lc[i]*vm[i,4]*vm[j,2]) + \\\n                                       lc[i]*qc[j]*(lc[i]*vm[i,3]*vm[j,3] + \\\n                                                    2*lc[j]*vm[i,2]*vm[j,4])) +\\\n                           12*cp[i,j]*(lc[i]*qc[j]*(qc[j]*vm[i,2]*vm[j,5] + \\\n                                                    2*qc[i]*vm[i,4]*vm[j,3]) + \\\n                                       lc[j]*qc[i]*(qc[i]*vm[i,5]*vm[j,2] + \\\n                                                    2*qc[j]*vm[i,3]*vm[j,4])) +\\\n                           12*cp[i,j]**2*(qc[i]*(lc[i]*vm[i,5]*vm[i,2] + \\\n                                                 lc[j]*vm[i,4]*vm[j,3]) + \\\n                                          qc[j]*(lc[i]*vm[i,3]*vm[j,4] + \\\n                                                 lc[j]*vm[i,2]*vm[j,5]))\n            for i in range(n):\n                for j in range(n):\n                    if i!=j:\n                        ans += 4*qc[i]**3*qc[j]*vm[i,6]*vm[j,2] + \\\n                               4*qc[i]*lc[j]**3*vm[i,2]*vm[j,3] + \\\n                               12*lc[i]*qc[i]*lc[j]**2*vm[i,3]*vm[i,2] + \\\n                               12*lc[i]*qc[i]**2*qc[j]*vm[i,5]*vm[i,2] + \\\n                               12*lc[i]*qc[i]*qc[j]**2*vm[i,3]*vm[j,4] + \\\n                               4*lc[i]*cp[i,j]**3*vm[i,4]*vm[j,3] + \\\n                               4*qc[i]*cp[i,j]**3*vm[i,5]*vm[j,3] + \\\n                               6*qc[i]**2*lc[j]**2*vm[i,4]*vm[j,2]\n            \n        if n>=3:\n            for i in range(n-2):\n                for j in range(i+1, n-1):\n                    for k in range(j+1, n):\n                        ans += (12*qc[i]**2*qc[j]*qc[k] + \\\n                               6*cp[i,j]**2*cp[i,k]**2 + \\\n                               12*qc[i]*(qc[k]*cp[i,j]**2 + qc[j]*cp[i,k]**2) + \\\n                               6*qc[i]**2*cp[j,k])*vm[i,4]*vm[j,2]*vm[k,2]\n                        ans += (12*qc[i]*qc[j]**2*qc[k] + \\\n                               6*cp[i,j]**2*cp[j,k]**2 + \\\n                               12*qc[j]*(qc[k]*cp[i,j]**2 + qc[i]*cp[j,k]**2) + \\\n                               6*qc[j]**2*cp[i,k]**2)*vm[i,2]*vm[j,4]*vm[k,2]\n                        ans += (12*qc[i]*qc[j]*qc[k]**2 + \\\n                               6*cp[i,k]**2*cp[j,k]**2 + \\\n                               12*qc[k]*(qc[i]*cp[j,k]**2 + qc[j]*cp[i,k]**2) + \\\n                               6*qc[k]**2*cp[i,j]**2)*vm[i,2]*vm[j,2]*vm[k,4]\n                        ans += (12*cp[i,j]**2*cp[i,k]*cp[j,k] + \\\n                               24*qc[i]*qc[j]*qc[k]*cp[i,j] + \\\n                               4*qc[k]*cp[i,j]**3 + \\\n                               24*qc[i]*qc[k]*cp[i,k]*cp[j,k])*vm[i,3]*vm[j,3]*vm[k,2]\n                        ans += (12*cp[i,j]*cp[i,k]**2*cp[j,k] + \\\n                               24*qc[i]*qc[j]*qc[k]*cp[i,k] + \\\n                               4*qc[j]*cp[i,k]**3 + \\\n                               24*qc[i]*qc[k]*cp[i,j]*cp[j,k])*vm[i,3]*vm[j,2]*vm[k,3]\n                        ans += (12*cp[i,j]*cp[i,k]*cp[j,k]**2 + \\\n                               24*qc[i]*qc[j]*qc[k]*cp[j,k] + \\\n                               4*qc[i]*cp[j,k]**3 + \\\n                               24*qc[j]*qc[j]*cp[i,j]*cp[i,k])*vm[i,2]*vm[j,3]*vm[k,3]\n                        ans += (12*cp[i,j]*cp[i,k]*cp[j,k] + \\\n                               24*qc[i]*qc[j]*qc[k]*cp[j,k] + \\\n                               4*qc[i]*cp[j,k]**3 + \\\n                               24*qc[j]*qc[k]*cp[i,j]*cp[i,k])*vm[i,2]*vm[j,3]*vm[k,3]\n                        ans += 24*(qc[i]*qc[j]*qc[k] + \\\n                               cp[i,j]*cp[i,k]*cp[j,k])*(lc[i]*vm[i,3]*vm[j,2]*vm[k,2] + \\\n                                                         lc[j]*vm[i,2]*vm[j,3]*vm[k,2] + \\\n                                                         lc[k]*vm[i,2]*vm[j,2]*vm[k,3])\n                        ans += 12*(lc[i]*cp[j,k]**2*vm[i,2]*(cp[i,j]*vm[j,3]*vm[k,2] + \\\n                               cp[i,k]*vm[j,2]*vm[k,3]) + lc[j]*cp[i,k]**2*vm[j,2]*(cp[i,j]*vm[i,3]*vm[k,2] + \\\n                               cp[j,k]*vm[i,2]*vm[k,3]) + \\\n                               lc[k]*cp[i,j]**2*vm[k,2]*(cp[i,k]*vm[i,3]*vm[j,2] + \\\n                               cp[j,k]*vm[i,2]*vm[j,3]))\n                        ans += 12*(qc[i]*cp[j,k]**2*vm[i,3]*(cp[i,j]*vm[j,3]*vm[k,2] + \\\n                               cp[i,k]*vm[j,2]*vm[k,3]) + qc[j]*cp[i,k]**2*vm[j,3]*(cp[i,j]*vm[i,3]*vm[k,2] + \\\n                               cp[j,k]*vm[i,2]*vm[k,3]) + \\\n                               qc[k]*cp[i,j]**2*vm[k,3]*(cp[i,k]*vm[i,3]*vm[j,2] + \\\n                               cp[j,k]*vm[i,2]*vm[j,3]))\n                        ans += 24*cp[i,j]*cp[i,k]*cp[j,k]*(qc[i]*vm[i,4]*vm[j,2]*vm[k,2] + \\\n                               qc[j]*vm[i,2]*vm[j,4]*vm[k,2] + qc[k]*vm[i,2]*vm[j,2]*vm[k,4])\n                        ans += vm[i,2]*vm[j,2]*vm[k,2]*(12*(qc[i]*qc[j]*lc[k]**2 + \\\n                               qc[i]*qc[k]*lc[j]**2 + qc[j]*qc[k]*lc[i]**2) + \\\n                               6*(lc[i]**2*cp[j,k]**2 + lc[j]**2*cp[i,k]**2 + \\\n                               lc[k]**2*cp[i,j]**2) + \\\n                               24*(cp[i,j]*cp[i,k]*lc[j]*lc[k] + \\\n                                   cp[i,j]*cp[j,k]*lc[i]*lc[k] + \\\n                                   cp[i,k]*cp[j,k]*lc[i]*lc[j]) + \\\n                               24*(lc[i]*lc[j]*qc[k]*cp[i,j] + \\\n                                   lc[i]*lc[k]*qc[j]*cp[i,k] + \\\n                                   lc[j]*lc[k]*qc[i]*cp[j,k]))\n                        ans += vm[i,3]*vm[j,2]*vm[k,2]*(24*lc[j]*cp[i,j]*qc[i]*qc[k] + \\\n                               24*lc[k]*cp[i,k]*qc[i]*qc[j] + \\\n                               12*lc[i]*cp[j,k]**2*qc[i] + \\\n                               24*lc[j]*cp[i,k]*cp[j,k]*qc[i] + \\\n                               24*lc[k]*cp[i,j]*cp[j,k]*qc[i] + \\\n                               12*lc[i]*cp[i,k]**2*qc[j] + \\\n                               12*lc[i]*cp[i,j]**2*qc[k])\n                        ans += vm[i,2]*vm[j,3]*vm[k,2]*(24*lc[i]*cp[i,j]*qc[j]*qc[k] + \\\n                               24*lc[k]*cp[j,k]*qc[i]*qc[j] + \\\n                               12*lc[j]*cp[i,k]**2*qc[j] + \\\n                               24*lc[i]*cp[i,k]*cp[j,k]*qc[j] + \\\n                               24*lc[k]*cp[i,j]*cp[i,k]*qc[j] + \\\n                               12*lc[j]*cp[j,k]**2*qc[i] + \\\n                               12*lc[j]*cp[i,j]**2*qc[k])\n                        ans += vm[i,2]*vm[j,2]*vm[k,3]*(24*lc[i]*cp[i,k]*qc[j]*qc[k] + \\\n                               24*lc[j]*cp[j,k]*qc[i]*qc[k] + \\\n                               12*lc[k]*cp[i,j]**2*qc[k] + \\\n                               24*lc[i]*cp[i,j]*cp[j,k]*qc[k] + \\\n                               24*lc[j]*cp[i,j]*cp[i,k]*qc[k] + \\\n                               12*lc[k]*cp[j,k]**2*qc[i] + \\\n                               12*lc[k]*cp[i,k]**2*qc[j])\n        \n        if n>=4:\n            for i in range(n-3):\n                for j in range(i+1, n-2):\n                    for k in range(j+1, n-1):\n                        for m in range(k+1, n):\n                            ans += vm[i,2]*vm[j,2]*vm[k,2]*vm[m,2]*(24*(qc[i]*qc[j]*qc[k]*qc[m] +\\\n                                   cp[i,j]*cp[i,k]*cp[j,m]*cp[k,m] + \\\n                                   cp[i,j]*cp[i,m]*cp[j,k]*cp[k,m] + \\\n                                   cp[i,k]*cp[i,m]*cp[j,k]*cp[j,m] + \\\n                                   qc[i]*cp[j,k]*cp[j,m]*cp[k,m] + \\\n                                   qc[j]*cp[i,k]*cp[i,m]*cp[i,m] + \\\n                                   qc[k]*cp[i,j]*cp[i,m]*cp[j,m] + \\\n                                   qc[m]*cp[i,j]*cp[i,k]*cp[j,k]) + \\\n                                   12*(qc[i]*qc[j]*cp[k,m]**2 + \\\n                                       qc[i]*qc[k]*cp[j,m]**2 + \\\n                                       qc[i]*qc[m]*cp[j,k]**2 + \\\n                                       qc[j]*qc[k]*cp[i,m]**2 + \\\n                                       qc[j]*qc[m]*cp[i,k]**2 + \\\n                                       qc[k]*qc[m]*cp[i,j]**2) + \\\n                                   6*(cp[i,j]**2*cp[k,m]**2 + \\\n                                      cp[i,k]**2*cp[j,m]**2 + \\\n                                      cp[i,m]**2*cp[j,k]**2))\n    \n    ############################\n    \n    else:\n        print('Can only calculate raw moments k = 0 to 4. Sorry.')\n        ans = None\n    \n    return ans", "fn_id": 4, "class_fn": false, "repo": "tisimst/soerp", "file": "soerp/method_of_moments.py", "last_update_at": "2022-01-10T01:20:38+00:00", "question_id": "c41a47e6b90f9ec11a16c23788a2988a40e6c2b6_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def rawmoment(slc, sqc, scp, vm, k):\n    \"\"\"\n    This is where the resultant distribution moments are calculated. MODIFY \n    THIS CODE AT YOUR OWN RISK. These equations have been verified with\n    published equations and example problems by N.D. Cox.\n    \n    Each of the derivative components need to be standardized prior to input\n    to this function. This means multiplying them by their respective\n    standard deviations, depending on the order of the derivative. Helper\n    functions have been defined for this purpose (standard_lc, standard_qc,\n    and standard_cp). However, this is only necessary if manually calling this\n    function rather than using soerp_numeric below.\n    \n    Parameters\n    ----------\n    slc : array\n        The standardized first derivative terms.\n    sqc : array\n        The standardized pure second derivative terms\n    scp : 2d-array\n        The standardized cross-product second derivative terms\n    vm : 2d-array\n        The first 9 (starting at 0) standardized distribution moments (one \n        row for each input variable, corresponding to the derivative array \n        order). See the documentation for ``soerp_numeric`` for more details.\n    k : int\n        The kth distribution moment to calculate.\n    \n    Returns\n    -------\n    rm : scalar\n        The kth raw distribution moment\n    \"\"\"\n    lc = copy(slc)\n    qc = copy(sqc)\n    cp = copy(scp)\n    n = len(lc)\n    if assume_linear:\n        qc[:] = 0.0\n        cp[:, :] = 0.0\n    ans = 0.0\n    if k == 0:\n        ans = 1\n    elif k == 1:\n        for i in range(n):\n            ans += qc[i] * vm[i, 2]\n    elif k == 2:\n        for i in range(n):\n            ans += lc[i] ** 2 * vm[i, 2] + 2 * lc[i] * qc[i] * vm[i, 3] + qc[i] ** 2 * vm[i, 4]\n        if n >= 2:\n            for i in range(n - 1):\n                for j in range(i + 1, n):\n                    ans += (2 * qc[i] * qc[j] + cp[i, j] ** 2) * vm[i, 2] * vm[j, 2]\n    elif k == 3:\n        for i in range(n):\n            ans += lc[i] ** 3 * vm[i, 3] + qc[i] ** 3 * vm[i, 6] + 3 * lc[i] ** 2 * qc[i] * vm[i, 4] + 3 * lc[i] * qc[i] ** 2 * vm[i, 5]\n        if n >= 2:\n            for i in range(n - 1):\n                for j in range(i + 1, n):\n                    ans += cp[i, j] ** 3 * vm[i, 3] * vm[j, 3] + 6 * lc[i] * lc[j] * cp[i, j] * vm[i, 2] * vm[j, 2] + 6 * qc[i] * qc[j] * cp[i, j] * vm[i, 3] * vm[j, 3]\n            for i in range(n):\n                for j in range(n):\n                    if j != i:\n                        ans += 3 * qc[i] ** 2 * vm[i, 4] * qc[j] * vm[j, 2] + 6 * lc[i] * qc[j] * cp[i, j] * vm[i, 2] * vm[j, 3] + 3 * qc[i] * lc[j] ** 2 * vm[i, 2] * vm[j, 2] + 6 * lc[i] * qc[i] * qc[j] * vm[i, 3] * vm[j, 2] + 3 * lc[i] * cp[i, j] ** 2 * vm[i, 3] * vm[j, 2] + 3 * qc[i] * cp[i, j] ** 2 * vm[i, 4] * vm[j, 2]\n        if n >= 3:\n            for i in range(n - 2):\n                for j in range(i + 1, n - 1):\n                    for k in range(j + 1, n):\n                        ans += (6 * qc[i] * qc[j] * qc[k] + 6 * cp[i, j] * cp[i, k] * cp[j, k] + 3 * (qc[i] * cp[j, k] ** 2 + qc[j] * cp[i, k] ** 2 + qc[k] * cp[i, j] ** 2)) * vm[i, 2] * vm[j, 2] * vm[k, 1]\n    elif k == 4:\n        for i in range(n):\n            ans += lc[i] ** 4 * vm[i, 4] + qc[i] ** 4 * vm[i, 8] + 4 * lc[i] ** 3 * qc[i] * vm[i, 5] + 4 * lc[i] * qc[i] ** 3 * vm[i, 7] + 6 * lc[i] ** 2 * qc[i] ** 2 * vm[i, 6]\n        if n >= 2:\n            for i in range(n - 1):\n                for j in range(i + 1, n):\n                    ans += 6 * lc[i] ** 2 * lc[j] ** 2 * vm[i, 2] * vm[j, 2] + 6 * qc[i] ** 2 * qc[j] ** 2 * vm[i, 4] * vm[j, 4] + cp[i, j] ** 4 * vm[i, 4] * vm[j, 4] + 12 * cp[i, j] * (lc[i] ** 2 * lc[j] * vm[i, 3] * vm[j, 2] + lc[i] * lc[j] ** 2 * vm[i, 2] * vm[j, 3]) + 12 * cp[i, j] * qc[i] * qc[j] * (qc[i] * vm[i, 5] * vm[j, 3] + qc[j] * vm[i, 3] * vm[j, 6]) + 12 * qc[i] * qc[j] * (lc[i] ** 2 * vm[i, 4] * vm[j, 2] + lc[j] ** 2 * vm[i, 2] * vm[j, 4] + 2 * lc[i] * lc[j] * vm[i, 3] * vm[j, 3]) + 6 * cp[i, j] ** 2 * (lc[i] ** 2 * vm[i, 4] * vm[j, 2] + lc[j] ** 2 * vm[i, 2] * vm[j, 4] + 2 * lc[i] * lc[j] * vm[i, 3] * vm[j, 3]) + 6 * cp[i, j] ** 2 * (qc[i] ** 2 * vm[i, 6] * vm[j, 2] + qc[j] ** 2 * vm[i, 2] * vm[j, 6] + 2 * qc[i] * qc[j] * vm[i, 4] * vm[j, 4]) + 12 * cp[i, j] * (lc[j] * qc[i] * (lc[j] * vm[i, 3] * vm[j, 3] + 2 * lc[i] * vm[i, 4] * vm[j, 2]) + lc[i] * qc[j] * (lc[i] * vm[i, 3] * vm[j, 3] + 2 * lc[j] * vm[i, 2] * vm[j, 4])) + 12 * cp[i, j] * (lc[i] * qc[j] * (qc[j] * vm[i, 2] * vm[j, 5] + 2 * qc[i] * vm[i, 4] * vm[j, 3]) + lc[j] * qc[i] * (qc[i] * vm[i, 5] * vm[j, 2] + 2 * qc[j] * vm[i, 3] * vm[j, 4])) + 12 * cp[i, j] ** 2 * (qc[i] * (lc[i] * vm[i, 5] * vm[i, 2] + lc[j] * vm[i, 4] * vm[j, 3]) + qc[j] * (lc[i] * vm[i, 3] * vm[j, 4] + lc[j] * vm[i, 2] * vm[j, 5]))\n            for i in range(n):\n                for j in range(n):\n                    if i != j:\n                        ans += 4 * qc[i] ** 3 * qc[j] * vm[i, 6] * vm[j, 2] + 4 * qc[i] * lc[j] ** 3 * vm[i, 2] * vm[j, 3] + 12 * lc[i] * qc[i] * lc[j] ** 2 * vm[i, 3] * vm[i, 2] + 12 * lc[i] * qc[i] ** 2 * qc[j] * vm[i, 5] * vm[i, 2] + 12 * lc[i] * qc[i] * qc[j] ** 2 * vm[i, 3] * vm[j, 4] + 4 * lc[i] * cp[i, j] ** 3 * vm[i, 4] * vm[j, 3] + 4 * qc[i] * cp[i, j] ** 3 * vm[i, 5] * vm[j, 3] + 6 * qc[i] ** 2 * lc[j] ** 2 * vm[i, 4] * vm[j, 2]\n        if n >= 3:\n            for i in range(n - 2):\n                for j in range(i + 1, n - 1):\n                    for k in range(j + 1, n):\n                        ans += (12 * qc[i] ** 2 * qc[j] * qc[k] + 6 * cp[i, j] ** 2 * cp[i, k] ** 2 + 12 * qc[i] * (qc[k] * cp[i, j] ** 2 + qc[j] * cp[i, k] ** 2) + 6 * qc[i] ** 2 * cp[j, k]) * vm[i, 4] * vm[j, 2] * vm[k, 2]\n                        ans += (12 * qc[i] * qc[j] ** 2 * qc[k] + 6 * cp[i, j] ** 2 * cp[j, k] ** 2 + 12 * qc[j] * (qc[k] * cp[i, j] ** 2 + qc[i] * cp[j, k] ** 2) + 6 * qc[j] ** 2 * cp[i, k] ** 2) * vm[i, 2] * vm[j, 4] * vm[k, 2]\n                        ans += (12 * qc[i] * qc[j] * qc[k] ** 2 + 6 * cp[i, k] ** 2 * cp[j, k] ** 2 + 12 * qc[k] * (qc[i] * cp[j, k] ** 2 + qc[j] * cp[i, k] ** 2) + 6 * qc[k] ** 2 * cp[i, j] ** 2) * vm[i, 2] * vm[j, 2] * vm[k, 4]\n                        ans += (12 * cp[i, j] ** 2 * cp[i, k] * cp[j, k] + 24 * qc[i] * qc[j] * qc[k] * cp[i, j] + 4 * qc[k] * cp[i, j] ** 3 + 24 * qc[i] * qc[k] * cp[i, k] * cp[j, k]) * vm[i, 3] * vm[j, 3] * vm[k, 2]\n                        ans += (12 * cp[i, j] * cp[i, k] ** 2 * cp[j, k] + 24 * qc[i] * qc[j] * qc[k] * cp[i, k] + 4 * qc[j] * cp[i, k] ** 3 + 24 * qc[i] * qc[k] * cp[i, j] * cp[j, k]) * vm[i, 3] * vm[j, 2] * vm[k, 3]\n                        ans += (12 * cp[i, j] * cp[i, k] * cp[j, k] ** 2 + 24 * qc[i] * qc[j] * qc[k] * cp[j, k] + 4 * qc[i] * cp[j, k] ** 3 + 24 * qc[j] * qc[j] * cp[i, j] * cp[i, k]) * vm[i, 2] * vm[j, 3] * vm[k, 3]\n                        ans += (12 * cp[i, j] * cp[i, k] * cp[j, k] + 24 * qc[i] * qc[j] * qc[k] * cp[j, k] + 4 * qc[i] * cp[j, k] ** 3 + 24 * qc[j] * qc[k] * cp[i, j] * cp[i, k]) * vm[i, 2] * vm[j, 3] * vm[k, 3]\n                        ans += 24 * (qc[i] * qc[j] * qc[k] + cp[i, j] * cp[i, k] * cp[j, k]) * (lc[i] * vm[i, 3] * vm[j, 2] * vm[k, 2] + lc[j] * vm[i, 2] * vm[j, 3] * vm[k, 2] + lc[k] * vm[i, 2] * vm[j, 2] * vm[k, 3])\n                        ans += 12 * (lc[i] * cp[j, k] ** 2 * vm[i, 2] * (cp[i, j] * vm[j, 3] * vm[k, 2] + cp[i, k] * vm[j, 2] * vm[k, 3]) + lc[j] * cp[i, k] ** 2 * vm[j, 2] * (cp[i, j] * vm[i, 3] * vm[k, 2] + cp[j, k] * vm[i, 2] * vm[k, 3]) + lc[k] * cp[i, j] ** 2 * vm[k, 2] * (cp[i, k] * vm[i, 3] * vm[j, 2] + cp[j, k] * vm[i, 2] * vm[j, 3]))\n                        ans += 12 * (qc[i] * cp[j, k] ** 2 * vm[i, 3] * (cp[i, j] * vm[j, 3] * vm[k, 2] + cp[i, k] * vm[j, 2] * vm[k, 3]) + qc[j] * cp[i, k] ** 2 * vm[j, 3] * (cp[i, j] * vm[i, 3] * vm[k, 2] + cp[j, k] * vm[i, 2] * vm[k, 3]) + qc[k] * cp[i, j] ** 2 * vm[k, 3] * (cp[i, k] * vm[i, 3] * vm[j, 2] + cp[j, k] * vm[i, 2] * vm[j, 3]))\n                        ans += 24 * cp[i, j] * cp[i, k] * cp[j, k] * (qc[i] * vm[i, 4] * vm[j, 2] * vm[k, 2] + qc[j] * vm[i, 2] * vm[j, 4] * vm[k, 2] + qc[k] * vm[i, 2] * vm[j, 2] * vm[k, 4])\n                        ans += vm[i, 2] * vm[j, 2] * vm[k, 2] * (12 * (qc[i] * qc[j] * lc[k] ** 2 + qc[i] * qc[k] * lc[j] ** 2 + qc[j] * qc[k] * lc[i] ** 2) + 6 * (lc[i] ** 2 * cp[j, k] ** 2 + lc[j] ** 2 * cp[i, k] ** 2 + lc[k] ** 2 * cp[i, j] ** 2) + 24 * (cp[i, j] * cp[i, k] * lc[j] * lc[k] + cp[i, j] * cp[j, k] * lc[i] * lc[k] + cp[i, k] * cp[j, k] * lc[i] * lc[j]) + 24 * (lc[i] * lc[j] * qc[k] * cp[i, j] + lc[i] * lc[k] * qc[j] * cp[i, k] + lc[j] * lc[k] * qc[i] * cp[j, k]))\n                        ans += vm[i, 3] * vm[j, 2] * vm[k, 2] * (24 * lc[j] * cp[i, j] * qc[i] * qc[k] + 24 * lc[k] * cp[i, k] * qc[i] * qc[j] + 12 * lc[i] * cp[j, k] ** 2 * qc[i] + 24 * lc[j] * cp[i, k] * cp[j, k] * qc[i] + 24 * lc[k] * cp[i, j] * cp[j, k] * qc[i] + 12 * lc[i] * cp[i, k] ** 2 * qc[j] + 12 * lc[i] * cp[i, j] ** 2 * qc[k])\n                        ans += vm[i, 2] * vm[j, 3] * vm[k, 2] * (24 * lc[i] * cp[i, j] * qc[j] * qc[k] + 24 * lc[k] * cp[j, k] * qc[i] * qc[j] + 12 * lc[j] * cp[i, k] ** 2 * qc[j] + 24 * lc[i] * cp[i, k] * cp[j, k] * qc[j] + 24 * lc[k] * cp[i, j] * cp[i, k] * qc[j] + 12 * lc[j] * cp[j, k] ** 2 * qc[i] + 12 * lc[j] * cp[i, j] ** 2 * qc[k])\n                        ans += vm[i, 2] * vm[j, 2] * vm[k, 3] * (24 * lc[i] * cp[i, k] * qc[j] * qc[k] + 24 * lc[j] * cp[j, k] * qc[i] * qc[k] + 12 * lc[k] * cp[i, j] ** 2 * qc[k] + 24 * lc[i] * cp[i, j] * cp[j, k] * qc[k] + 24 * lc[j] * cp[i, j] * cp[i, k] * qc[k] + 12 * lc[k] * cp[j, k] ** 2 * qc[i] + 12 * lc[k] * cp[i, k] ** 2 * qc[j])\n        if n >= 4:\n            for i in range(n - 3):\n                for j in range(i + 1, n - 2):\n                    for k in range(j + 1, n - 1):\n                        for m in range(k + 1, n):\n                            ans += vm[i, 2] * vm[j, 2] * vm[k, 2] * vm[m, 2] * (24 * (qc[i] * qc[j] * qc[k] * qc[m] + cp[i, j] * cp[i, k] * cp[j, m] * cp[k, m] + cp[i, j] * cp[i, m] * cp[j, k] * cp[k, m] + cp[i, k] * cp[i, m] * cp[j, k] * cp[j, m] + qc[i] * cp[j, k] * cp[j, m] * cp[k, m] + qc[j] * cp[i, k] * cp[i, m] * cp[i, m] + qc[k] * cp[i, j] * cp[i, m] * cp[j, m] + qc[m] * cp[i, j] * cp[i, k] * cp[j, k]) + 12 * (qc[i] * qc[j] * cp[k, m] ** 2 + qc[i] * qc[k] * cp[j, m] ** 2 + qc[i] * qc[m] * cp[j, k] ** 2 + qc[j] * qc[k] * cp[i, m] ** 2 + qc[j] * qc[m] * cp[i, k] ** 2 + qc[k] * qc[m] * cp[i, j] ** 2) + 6 * (cp[i, j] ** 2 * cp[k, m] ** 2 + cp[i, k] ** 2 * cp[j, m] ** 2 + cp[i, m] ** 2 * cp[j, k] ** 2))\n    else:\n        print('Can only calculate raw moments k = 0 to 4. Sorry.')\n        ans = None\n"]]}
{"hexsha": "0c255072b2541bb77d68beebe77203131f1c812a", "ext": "py", "lang": "Python", "content": "def slave_loop(communicator,\n               logger=None,\n               epd_to_deploy=None,\n               untar_directory='/tmp'):\n    status = MPI.Status()\n    rank = communicator.Get_rank()\n\n    if not logger:\n        logger = logging.getLogger(\"testMPI.slave\")\n    commands = {}\n\n    if epd_to_deploy != None:\n        lock_file_path = os.path.join(untar_directory, \"sw_deploy_lock\")\n        if not os.path.isfile(lock_file_path):\n            try:\n                lock_file = open(lock_file_path, \"w\")\n                lock_file.write(\"locked \\n\")\n                lock_file.close()\n                epd_tar = tarfile.open(epd_to_deploy)\n                epd_tar.extractall(path=untar_directory)\n                # logger.debug('extract %s' %(epd_to_deploy))\n            except IOError as e:\n                logger.error(\"Could not deploy epd: %s\" % (e))\n                pass\n\n    max_nb_jobs = 1\n\n    ret_value = 0\n    slave_on_hold = False\n    while True:\n        ended_jobs_info = {}  # job_id -> (job_status, job_exit_status)\n        t = None\n        if len(commands) < max_nb_jobs:\n            communicator.send('Requesting a job',\n                              dest=0,\n                              tag=MPIScheduler.JOB_REQUEST)\n\n            # logger.debug(\"Slave \" + repr(rank) + \" job request\")\n            communicator.Probe(source=0,\n                               tag=MPI.ANY_TAG, status=status)\n            # logger.debug(\"Slave \" + repr(rank) + \" job request answered\")\n            t = status.Get_tag()\n        elif communicator.Iprobe(source=0,\n                                 tag=MPI.ANY_TAG, status=status):\n            t = status.Get_tag()\n        if t != None:\n\n            if t == MPIScheduler.JOB_SENDING:\n                # logger.debug(\"Slave \" + repr(rank) + \" receiving job\")\n                job_list = communicator.recv(source=0, tag=t)\n                # logger.debug(\"Slave \" + repr(rank) + \" job list received\")\n                for j in job_list:\n                    # process = scheduler.LocalScheduler.create_process(j)\n                    separator = \" \"\n                    if not j.command:  # barrier job\n                        command = None\n                    else:\n                        # command = \"\"\n                        # for command_el in j.plain_command():\n                            # command = command + \"\\'\" + command_el + \"\\' \"\n                        command = (j.plain_command(),\n                                   j.plain_stdout(),\n                                   j.plain_stderr())\n                    # command = separator.join(j.plain_command())\n                    # logger.debug(\"[host: \" + socket.gethostname() + \"] \"\n                             #+ \"Slave \" + repr(rank) + \" RUNS JOB\"\n                             #+ repr(j.job_id) + \" \" + str(command))\n                    commands[j.job_id] = (command, j.env)\n            elif t == MPIScheduler.NO_JOB:\n                communicator.recv(source=0, tag=t)\n                # logger.debug(\"Slave \" + repr(rank) + \" \"\n                #             \"received no job \" + repr(commands))\n                # time.sleep(1)\n            elif t == MPIScheduler.EXIT_SIGNAL:\n                communicator.send('STOP', dest=0, tag=MPIScheduler.EXIT_SIGNAL)\n                logger.debug(\"[host: \" + socket.gethostname() + \"] \"\n                             + \"Slave \" + repr(rank) + \" STOP !!!!! received\")\n                break\n            elif t == MPIScheduler.JOB_KILL:\n                job_ids = communicator.recv(source=0, tag=t)\n                for job_id in job_ids:\n                    if job_id in commands.keys():\n                        # TO DO: relevant exception type and message\n                        raise Exception(\n                            \"The job \" + repr(job_id) + \" can not be killed\")\n\n            else:\n                raise Exception('Unknown tag')\n        for job_id, command_def in six.iteritems(commands):\n            command, env = command_def\n            if command == None:\n                # ended_jobs_info[job_id] = (constants.FAILED,\n                                           #(constants.EXIT_ABORTED, None,\n                                            # None, None))\n                # normally a barrier job\n                ended_jobs_info[job_id] = (constants.DONE,\n                                           (constants.FINISHED_REGULARLY, 0,\n                                            None, None))\n            else:\n\n                # if j.plain_stderr():\n                    # command = command + \" >> \" + \\\n                        # j.plain_stdout() + \" 2>> \" + j.plain_stderr()\n                # else:\n                    # command = command + \" >> \" + j.plain_stdout()\n\n                # ret_value = os.system(command)\n                plain_command, plain_stdout, plain_stderr = command\n                cmd_stdout, cmd_stderr = (None, None)\n\n                if plain_stdout:\n                    cmd_stdout = open(plain_stdout, 'w+')\n\n                if plain_stderr:\n                    cmd_stderr = open(plain_stderr, 'w+')\n\n                logger.debug(\"[host: \" + socket.gethostname() + \"] \"\n                             + \"Slave %s JOB%s STARTING SUBPROCESS COMMAND %s, \"\n                             \"stdout: %s, stderr: %s\"\n                             % (repr(rank), repr(job_id), plain_command,\n                                plain_stdout, plain_stderr))\n\n                if env is not None:\n                    env2 = dict(os.environ)\n                    env2.update(env)\n                    env = env2\n\n                try:\n                    ret_value = subprocess.call(plain_command,\n                                                stdout=cmd_stdout,\n                                                stderr=cmd_stderr,\n                                                env=env)\n                    # ret_value = subprocess.call(plain_command)\n                    logger.debug(\"[host: \" + socket.gethostname() + \"] \"\n                                 + \"Slave %s JOB%s ENDED REGULARLY \"\n                                 \"(ret value %d), stdout: %s, stderr: %s\"\n                                 % (repr(rank), repr(job_id), ret_value,\n                                    plain_stdout, plain_stderr))\n\n                except Exception as e:\n                    ret_value = None\n                    # import traceback\n                    # exc_type, exc_value, exc_traceback = sys.exc_info()\n                    # if hasattr(e, 'child_traceback'):\n                        # logger.debug(\n                        # traceback.print_tb(e.child_traceback)\n                    logger.debug(\"[host: \" + socket.gethostname() + \"] \"\n                                 + \"Slave %s JOB%s RAISED ERROR, \"\n                                 \"stdout: %s, stderr: %s\"\n                                 % (repr(rank), repr(job_id),\n                                    plain_stdout, plain_stderr),\n                                 exc_info=True)\n                    ended_jobs_info[job_id] = (constants.FAILED,\n                                               (constants.EXIT_ABORTED,\n                                                None, None, None))\n                    # traceback.format_exception(exc_type, exc_value,\n                    # exc_traceback)\n\n                finally:\n                    if cmd_stdout:\n                        cmd_stdout.close()\n\n                    if cmd_stderr:\n                        cmd_stderr.close()\n\n                if ret_value != None:\n                    ended_jobs_info[job_id] = (constants.DONE,\n                                               (constants.FINISHED_REGULARLY,\n                                                ret_value, None, None))\n\n        if ended_jobs_info:\n            for job_id in six.iterkeys(ended_jobs_info):\n                del commands[job_id]\n            logger.debug(\"[host: \" + socket.gethostname() + \"] \"\n                         + \"Slave \" + repr(rank) + \" send JOB_RESULT\")\n            communicator.send(ended_jobs_info, dest=0,\n                              tag=MPIScheduler.JOB_RESULT)\n        else:\n            pass\n            # TO DO send Slave is alive\n        if slave_on_hold:\n            logger.debug(\"[host: \" + socket.gethostname() + \"] \"\n                         + \"Slave %d was punished !!!\" % (rank))\n            # time.sleep(1200) #20 mins\n            break\n        else:\n            time.sleep(1)\n\n    if epd_to_deploy != None:\n        logger.debug(\"[host: \" + socket.gethostname() + \"] \"\n                     + \"Slave %d cleaning ... \\n\" % (rank))\n        if os.path.isfile(lock_file_path):\n            try:\n                os.remove(lock_file_path)\n                archive_dir_path = os.path.join(untar_directory, 'epd')\n                if os.path.isdir(archive_dir_path):\n                    shutil.rmtree(archive_dir_path)\n                    logger.debug(\"remove %s\" % (archive_dir_path))\n            except Exception as e:\n                pass\n        logger.debug(\"[host: \" + socket.gethostname() + \"] \"\n                     + \"Slave %d: end of cleaning! \\n\" % (rank))\n    logger.debug(\"[host: \" + socket.gethostname() + \"] \"\n                 + \"Slave %d END!!! \\n\" % (rank))", "fn_id": 0, "class_fn": false, "repo": "denisri/soma-workflow", "file": "python/soma_workflow/MPI_workflow_runner.py", "last_update_at": "2022-03-15T10:54:57+00:00", "question_id": "0c255072b2541bb77d68beebe77203131f1c812a_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def slave_loop(communicator, logger=None, epd_to_deploy=None, untar_directory='/tmp'):\n    status = MPI.Status()\n    rank = communicator.Get_rank()\n    if not logger:\n        logger = logging.getLogger('testMPI.slave')\n    commands = {}\n    if epd_to_deploy != None:\n        lock_file_path = os.path.join(untar_directory, 'sw_deploy_lock')\n        if not os.path.isfile(lock_file_path):\n            try:\n                lock_file = open(lock_file_path, 'w')\n                lock_file.write('locked \\n')\n                lock_file.close()\n                epd_tar = tarfile.open(epd_to_deploy)\n                epd_tar.extractall(path=untar_directory)\n            except IOError as e:\n                logger.error('Could not deploy epd: %s' % e)\n                pass\n    max_nb_jobs = 1\n    ret_value = 0\n    slave_on_hold = False\n    while True:\n        ended_jobs_info = {}\n        t = None\n        if len(commands) < max_nb_jobs:\n            communicator.send('Requesting a job', dest=0, tag=MPIScheduler.JOB_REQUEST)\n            communicator.Probe(source=0, tag=MPI.ANY_TAG, status=status)\n            t = status.Get_tag()\n        elif communicator.Iprobe(source=0, tag=MPI.ANY_TAG, status=status):\n            t = status.Get_tag()\n        if t != None:\n            if t == MPIScheduler.JOB_SENDING:\n                job_list = communicator.recv(source=0, tag=t)\n                for j in job_list:\n                    separator = ' '\n                    if not j.command:\n                        command = None\n                    else:\n                        command = (j.plain_command(), j.plain_stdout(), j.plain_stderr())\n                    commands[j.job_id] = (command, j.env)\n            elif t == MPIScheduler.NO_JOB:\n                communicator.recv(source=0, tag=t)\n            elif t == MPIScheduler.EXIT_SIGNAL:\n                communicator.send('STOP', dest=0, tag=MPIScheduler.EXIT_SIGNAL)\n                logger.debug('[host: ' + socket.gethostname() + '] ' + 'Slave ' + repr(rank) + ' STOP !!!!! received')\n                break\n            elif t == MPIScheduler.JOB_KILL:\n                job_ids = communicator.recv(source=0, tag=t)\n                for job_id in job_ids:\n                    if job_id in commands.keys():\n                        raise Exception('The job ' + repr(job_id) + ' can not be killed')\n            else:\n                raise Exception('Unknown tag')\n        for job_id, command_def in six.iteritems(commands):\n            command, env = command_def\n            if command == None:\n                ended_jobs_info[job_id] = (constants.DONE, (constants.FINISHED_REGULARLY, 0, None, None))\n            else:\n                plain_command, plain_stdout, plain_stderr = command\n                cmd_stdout, cmd_stderr = (None, None)\n                if plain_stdout:\n                    cmd_stdout = open(plain_stdout, 'w+')\n                if plain_stderr:\n                    cmd_stderr = open(plain_stderr, 'w+')\n                logger.debug('[host: ' + socket.gethostname() + '] ' + 'Slave %s JOB%s STARTING SUBPROCESS COMMAND %s, stdout: %s, stderr: %s' % (repr(rank), repr(job_id), plain_command, plain_stdout, plain_stderr))\n                if env is not None:\n                    env2 = dict(os.environ)\n                    env2.update(env)\n                    env = env2\n                try:\n                    ret_value = subprocess.call(plain_command, stdout=cmd_stdout, stderr=cmd_stderr, env=env)\n                    logger.debug('[host: ' + socket.gethostname() + '] ' + 'Slave %s JOB%s ENDED REGULARLY (ret value %d), stdout: %s, stderr: %s' % (repr(rank), repr(job_id), ret_value, plain_stdout, plain_stderr))\n                except Exception as e:\n                    ret_value = None\n                    logger.debug('[host: ' + socket.gethostname() + '] ' + 'Slave %s JOB%s RAISED ERROR, stdout: %s, stderr: %s' % (repr(rank), repr(job_id), plain_stdout, plain_stderr), exc_info=True)\n                    ended_jobs_info[job_id] = (constants.FAILED, (constants.EXIT_ABORTED, None, None, None))\n                finally:\n                    if cmd_stdout:\n                        cmd_stdout.close()\n                    if cmd_stderr:\n                        cmd_stderr.close()\n                if ret_value != None:\n                    ended_jobs_info[job_id] = (constants.DONE, (constants.FINISHED_REGULARLY, ret_value, None, None))\n        if ended_jobs_info:\n            for job_id in six.iterkeys(ended_jobs_info):\n                del commands[job_id]\n            logger.debug('[host: ' + socket.gethostname() + '] ' + 'Slave ' + repr(rank) + ' send JOB_RESULT')\n            communicator.send(ended_jobs_info, dest=0, tag=MPIScheduler.JOB_RESULT)\n        else:\n            pass\n        if slave_on_hold:\n            logger.debug('[host: ' + socket.gethostname() + '] ' + 'Slave %d was punished !!!' % rank)\n            break\n        else:\n            time.sleep(1)\n    if epd_to_deploy != None:\n        logger.debug('[host: ' + socket.gethostname() + '] ' + 'Slave %d cleaning ... \\n' % rank)\n        if os.path.isfile(lock_file_path):\n            try:\n                os.remove(lock_file_path)\n                archive_dir_path = os.path.join(untar_directory, 'epd')\n                if os.path.isdir(archive_dir_path):\n                    shutil.rmtree(archive_dir_path)\n                    logger.debug('remove %s' % archive_dir_path)\n            except Exception as e:\n                pass\n        logger.debug('[host: ' + socket.gethostname() + '] ' + 'Slave %d: end of cleaning! \\n' % rank)\n"]]}
{"hexsha": "b6a018eb47eec9d761a97a29fdd12239167c8579", "ext": "py", "lang": "Python", "content": "def get_eta_A_C_f_i(m_C_f_i, A_env_f_i):\n    \"\"\"\u968e\u5c64f\u306b\u304a\u3051\u308b\u5358\u4f4d\u4f4f\u6238i\u306e\u51b7\u623f\u671f\u306e\u5e73\u5747\u65e5\u5c04\u71b1\u53d6\u5f97\u7387\uff08(W/m)/(W/m2) (%) \u2026\u2026\u2026\u2026\u5f0f(6a)\n\n    Args:\n      m_C_f_i(float): \u968e\u5c64f\u306b\u304a\u3051\u308b\u5358\u4f4d\u4f4f\u6238i\u306e\u5358\u4f4d\u65e5\u5c04\u5f37\u5ea6\u5f53\u305f\u308a\u306e\u51b7\u623f\u671f\u306e\u65e5\u5c04\u71b1\u53d6\u5f97\u91cf\uff08W/(W/m2)\uff09\u2026\u2026\u2026\u2026\u5f0f(6b)/get_m_C_f_i\n      A_env_f_i(Decimal): \u968e\u5c64f\u306b\u304a\u3051\u308b\u5358\u4f4d\u4f4f\u6238i\u306e\u5916\u76ae\u306e\u90e8\u4f4d\u306e\u9762\u7a4d\u306e\u5408\u8a08\u2026\u2026\u2026\u2026\u5f0f(7)/get_A_env_f_i\n\n    Returns:\n      Decimal: \u968e\u5c64f\u306b\u304a\u3051\u308b\u5358\u4f4d\u4f4f\u6238i\u306e\u51b7\u623f\u671f\u306e\u5e73\u5747\u65e5\u5c04\u71b1\u53d6\u5f97\u7387\uff08(W/m)/(W/m2)\uff09\n\n    \"\"\"\n\n    # Decimal\u306e\u6709\u52b9\u6841\u6570\u3092\u5c0f\u6570\u70b9\u4ee5\u4e0b10\u6841\u3068\u3059\u308b\n    eta_A_C_f_i = Decimal((m_C_f_i / float(A_env_f_i)) * 100).quantize(Decimal(str(10**(-10))), rounding=ROUND_HALF_UP)\n    # \u5c0f\u6570\u70b9\u4ee5\u4e0b\u4e00\u4f4d\u672a\u6e80\u306e\u7aef\u6570\u3092\u5207\u308a\u4e0a\u3052\n    eta_A_C_f_i = eta_A_C_f_i.quantize(Decimal('0.1'), rounding=ROUND_CEILING)\n\n    return eta_A_C_f_i", "fn_id": 6, "class_fn": false, "repo": "jjj-design/pyhees", "file": "src/pyhees/section2_6.py", "last_update_at": "2022-03-19T08:02:51+00:00", "question_id": "b6a018eb47eec9d761a97a29fdd12239167c8579_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_eta_A_C_f_i(m_C_f_i, A_env_f_i):\n    \"\"\"\u968e\u5c64f\u306b\u304a\u3051\u308b\u5358\u4f4d\u4f4f\u6238i\u306e\u51b7\u623f\u671f\u306e\u5e73\u5747\u65e5\u5c04\u71b1\u53d6\u5f97\u7387\uff08(W/m)/(W/m2) (%) \u2026\u2026\u2026\u2026\u5f0f(6a)\n\n    Args:\n      m_C_f_i(float): \u968e\u5c64f\u306b\u304a\u3051\u308b\u5358\u4f4d\u4f4f\u6238i\u306e\u5358\u4f4d\u65e5\u5c04\u5f37\u5ea6\u5f53\u305f\u308a\u306e\u51b7\u623f\u671f\u306e\u65e5\u5c04\u71b1\u53d6\u5f97\u91cf\uff08W/(W/m2)\uff09\u2026\u2026\u2026\u2026\u5f0f(6b)/get_m_C_f_i\n      A_env_f_i(Decimal): \u968e\u5c64f\u306b\u304a\u3051\u308b\u5358\u4f4d\u4f4f\u6238i\u306e\u5916\u76ae\u306e\u90e8\u4f4d\u306e\u9762\u7a4d\u306e\u5408\u8a08\u2026\u2026\u2026\u2026\u5f0f(7)/get_A_env_f_i\n\n    Returns:\n      Decimal: \u968e\u5c64f\u306b\u304a\u3051\u308b\u5358\u4f4d\u4f4f\u6238i\u306e\u51b7\u623f\u671f\u306e\u5e73\u5747\u65e5\u5c04\u71b1\u53d6\u5f97\u7387\uff08(W/m)/(W/m2)\uff09\n\n    \"\"\"\n    eta_A_C_f_i = Decimal(m_C_f_i / float(A_env_f_i) * 100).quantize(Decimal(str(10 ** (-10))), rounding=ROUND_HALF_UP)\n    eta_A_C_f_i = eta_A_C_f_i.quantize(Decimal('0.1'), rounding=ROUND_CEILING)\n"]]}
{"hexsha": "0803d9380edff055d884703703cd7ded33c22cb8", "ext": "py", "lang": "Python", "content": "def sfm_loop(sfm_images, features_dir, baseline, wpSet, K, dist):\n    \"\"\"\n    Main Structure From Motion loop.\n    \n    :param sfm_images: Text file of image paths.\n    :param features_dir: Path of features directory.\n    :param baseline: baseline of first two views.\n    :param wpSet: World points data from the baseline.\n    :param K: Camera intrinsic matrix.\n    :param dist: Camera distortion parameters.\n    \"\"\"\n\n    # Get new view. Perform linear PnP on new view to get pose information.\n    view1 = baseline.view1\n    view2 = baseline.view2\n    completed_views = [view1, view2]\n\n    def update_3d_points(view, completed_views, K):\n        \"\"\"\n        Updates 3D points with the current view.\n        :param view: View with which to update world 3D coordinates.\n        :param completed_views: Views from which 3D points have been triangulated.\n        :param K: Camera intrinsic matrix\n        \"\"\"\n        view.rotation, view.translation = compute_pose(view, completed_views, K, img_matches)\n\n        for view_n in completed_views:\n            if view_n.id in view.tracked_pts:\n                # Before triangulating, outliers should be removed using F matrix/RANSAC between x1 and x2.\n                x1, x2 = remove_outliers(view, view_n)\n                print(\"Number of points to triangulate:\", x1.shape[0])\n                X = triangulate_points(K, t1=view.translation, R1=view.rotation,\n                                       t2=view_n.translation, R2=view_n.rotation, x1=x1, x2=x2,\n                                       print_error=True)\n                # add correspondences to world coordinates only if reproj errors are low for img pair\n                if X is not None:\n                    X = store_3Dpoints_to_views(X, view_n, view, K, error_threshold=2.0)\n                    print(f\"Found {len(X)} 3D points between image{view_n.name} and image{view.name}.\")\n                    view.reproject_view(K, print_error=True)\n                    wpSet.add_correspondences(X, view, view_n)\n        print(f\"Found {len(view.world_points)} 3D points for new image{view.name}.\")\n\n    for i, image in enumerate(sfm_images[2:]):\n        # extract features of a view\n        view = ImageView(image, features_dir)\n        view.read_features()\n        if view.descriptors is None:\n            view.extract_features(write_to_file=True)\n        if view not in completed_views:\n            update_3d_points(view, completed_views, K, dist)\n            completed_views.append(view)\n        \n        # Perform bundle adjustment on new view and existing views -> Update 3D points dictionary\n        wpSet.correspondences.to_csv(f'points\\\\point_correspondences_{i + 1}.csv')\n        ba = BundleAdjustment(wpSet, K, dist, completed_views)\n        poses, wpSet.world_points = ba.optimize()\n        for j, view in enumerate(completed_views):\n            rot_mat = (poses[j, :9]).reshape(3, 3)\n            t_vec = poses[j, 9:].reshape(3, 1)\n            view.rotation = rot_mat\n            view.translation = t_vec\n            view.update_world_points(wpSet)\n            view.reproject_view(K, print_error=True)\n        np.savez(f'points\\\\points3d_{i}', point_cloud=wpSet.world_points)\n\n    wpSet.correspondences.to_csv('\\\\point_correspondences.csv')\n    np.savetxt(\"points_3d.csv\", wpSet.world_points, delimiter=\",\")\n    return wpSet.world_points", "fn_id": 1, "class_fn": false, "repo": "patel-nisarg/SFM_from_CAD_Model", "file": "main.py", "last_update_at": "2022-03-14T12:18:52+00:00", "question_id": "0803d9380edff055d884703703cd7ded33c22cb8_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def sfm_loop(sfm_images, features_dir, baseline, wpSet, K, dist):\n    \"\"\"\n    Main Structure From Motion loop.\n    \n    :param sfm_images: Text file of image paths.\n    :param features_dir: Path of features directory.\n    :param baseline: baseline of first two views.\n    :param wpSet: World points data from the baseline.\n    :param K: Camera intrinsic matrix.\n    :param dist: Camera distortion parameters.\n    \"\"\"\n    view1 = baseline.view1\n    view2 = baseline.view2\n    completed_views = [view1, view2]\n\n    def update_3d_points(view, completed_views, K):\n        \"\"\"\n        Updates 3D points with the current view.\n        :param view: View with which to update world 3D coordinates.\n        :param completed_views: Views from which 3D points have been triangulated.\n        :param K: Camera intrinsic matrix\n        \"\"\"\n        view.rotation, view.translation = compute_pose(view, completed_views, K, img_matches)\n        for view_n in completed_views:\n            if view_n.id in view.tracked_pts:\n                x1, x2 = remove_outliers(view, view_n)\n                print('Number of points to triangulate:', x1.shape[0])\n                X = triangulate_points(K, t1=view.translation, R1=view.rotation, t2=view_n.translation, R2=view_n.rotation, x1=x1, x2=x2, print_error=True)\n                if X is not None:\n                    X = store_3Dpoints_to_views(X, view_n, view, K, error_threshold=2.0)\n                    print(f'Found {len(X)} 3D points between image{view_n.name} and image{view.name}.')\n                    view.reproject_view(K, print_error=True)\n                    wpSet.add_correspondences(X, view, view_n)\n        print(f'Found {len(view.world_points)} 3D points for new image{view.name}.')\n    for i, image in enumerate(sfm_images[2:]):\n        view = ImageView(image, features_dir)\n        view.read_features()\n        if view.descriptors is None:\n            view.extract_features(write_to_file=True)\n        if view not in completed_views:\n            update_3d_points(view, completed_views, K, dist)\n            completed_views.append(view)\n        wpSet.correspondences.to_csv(f'points\\\\point_correspondences_{i + 1}.csv')\n        ba = BundleAdjustment(wpSet, K, dist, completed_views)\n        poses, wpSet.world_points = ba.optimize()\n        for j, view in enumerate(completed_views):\n            rot_mat = poses[j, :9].reshape(3, 3)\n            t_vec = poses[j, 9:].reshape(3, 1)\n            view.rotation = rot_mat\n            view.translation = t_vec\n            view.update_world_points(wpSet)\n            view.reproject_view(K, print_error=True)\n        np.savez(f'points\\\\points3d_{i}', point_cloud=wpSet.world_points)\n    wpSet.correspondences.to_csv('\\\\point_correspondences.csv')\n    np.savetxt('points_3d.csv', wpSet.world_points, delimiter=',')\n"]]}
{"hexsha": "3371c94e95dca7ace8a0ad2189b18574f1e55b17", "ext": "py", "lang": "Python", "content": "def pred_only(coco_json, add_score=None):\n    coco_json_path = path(coco_json)\n    coco_dict, setname = read_coco_json(coco_json_path)\n    if add_score is not None:\n        assert isinstance(add_score, float)\n        for annot in coco_dict[\"annotations\"]:\n            annot[\"score\"] = add_score\n    annotations = coco_dict[\"annotations\"]\n    out = coco_json_path.parent / f\"{coco_json_path.stem}_pred.json\"\n    write_json(out, annotations)", "fn_id": 0, "class_fn": false, "repo": "levan92/cocojson", "file": "cocojson/tools/pred_only.py", "last_update_at": "2022-03-13T10:44:49+00:00", "question_id": "3371c94e95dca7ace8a0ad2189b18574f1e55b17_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def pred_only(coco_json, add_score=None):\n    coco_json_path = path(coco_json)\n    coco_dict, setname = read_coco_json(coco_json_path)\n    if add_score is not None:\n        assert isinstance(add_score, float)\n        for annot in coco_dict['annotations']:\n            annot['score'] = add_score\n    annotations = coco_dict['annotations']\n    out = coco_json_path.parent / f'{coco_json_path.stem}_pred.json'\n"]]}
{"hexsha": "6cd976e98f8d397ebdfa4e36ed9da1883d4db478", "ext": "py", "lang": "Python", "content": "def average_gradients(tower_grads):\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        grads = []\n        for g, _ in grad_and_vars:\n            expanded_g = tf.expand_dims(g, 0)\n            grads.append(expanded_g)\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0)\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads", "fn_id": 1, "class_fn": false, "repo": "ShihaoZhaoZSH/Video-Backdoor-Attack", "file": "utils.py", "last_update_at": "2022-01-02T14:01:04+00:00", "question_id": "6cd976e98f8d397ebdfa4e36ed9da1883d4db478_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def average_gradients(tower_grads):\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        grads = []\n        for g, _ in grad_and_vars:\n            expanded_g = tf.expand_dims(g, 0)\n            grads.append(expanded_g)\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0)\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n"]]}
{"hexsha": "198ca08eaa928b53fdcdc57c8503851d20d28c99", "ext": "py", "lang": "Python", "content": "def logistic(x, L, k, x0):\n    \"\"\"\n    Logistic function\n\n    .. math::\n       L/(1+exp(-k(x-x0)))\n\n    Parameters\n    ----------\n    x : array_like\n        Independent variable to evalute logistic function\n    L : float\n        Maximum of logistic function\n    k : float\n        Steepness of logistic function\n    x0 : float\n        Inflection point of logistic function\n\n    Returns\n    -------\n    float or ndarray\n        Logistic function at *x* with maximum *L*, steepness *k* and inflection\n        point *x0*\n\n    \"\"\"\n    return L * sp.expit(k * (x - x0))", "fn_id": 0, "class_fn": false, "repo": "mcuntz/pyjams", "file": "src/pyjams/functions/logistic_function.py", "last_update_at": "2022-03-10T18:13:00+00:00", "question_id": "198ca08eaa928b53fdcdc57c8503851d20d28c99_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def logistic(x, L, k, x0):\n    \"\"\"\n    Logistic function\n\n    .. math::\n       L/(1+exp(-k(x-x0)))\n\n    Parameters\n    ----------\n    x : array_like\n        Independent variable to evalute logistic function\n    L : float\n        Maximum of logistic function\n    k : float\n        Steepness of logistic function\n    x0 : float\n        Inflection point of logistic function\n\n    Returns\n    -------\n    float or ndarray\n        Logistic function at *x* with maximum *L*, steepness *k* and inflection\n        point *x0*\n\n    \"\"\"\n"]]}
{"hexsha": "71ea046df258c106b4c8f2d6daf009537ec7c40c", "ext": "py", "lang": "Python", "content": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"N\", help=\"Number of complex samples.\",\n                    type=int)\n    parser.add_argument(\"mod\", help=\"Number of bits per sample.\",\n                    type=int, choices=[2,4,6,8,10], metavar='Mod')\n    parser.add_argument(\"--snr\", help=\"Signal to noise ratio (dB).\",\n                    type=float, nargs='?',default = 10, metavar='SNR')\n    parser.add_argument(\"--Nr\", help=\"Number of RX antennas.\",\n                    type=int, nargs='?',default = 1, metavar='Nr')\n    parser.add_argument(\"--Nt\", help=\"Number of TX antennas.\",\n                    type=int, nargs='?',default = 1, metavar='Nt')\n    parser.add_argument(\"--txmode\", help=\"Transmission moTX mode. Use multiple antennas for channel diversity or multiple streams.\",\n                    type=int, nargs='?', choices=[0,1], default = 0, metavar='TX mode')\n    args = parser.parse_args()\n    N = args.N\n    mod = args.mod\n    snr = args.snr\n    Nr = args.Nr\n    Nt = args.Nt\n    tx_mode = args.txmode\n    N0 = 1/np.power(10,snr/10)\n\n    #if Nt != Nr:\n    #    raise ValueError('Currently only Nr=Nt supported.')\n\n    NoS = min(Nr, Nt) # maximum number of possible streams\n    H = generateChMatrix(Nr,Nt,Channel.RAND_UNIT_GOOD)\n    print('Condition number of the generated channel: '+str(np.linalg.cond(H)))\n\n    # generate the baseband IQ signal\n    X = generateIQ(Nt, N, mod, tx_mode)\n    #plotConstell(x)\n\n    # Starting with diversity gain\n    # NOTE: Replicate same signal on all transmit antennas\n    Xin = X#np.asmatrix([x]*Nt)\n    Cx = np.var(X)*np.identity(Nt) #all antennas receiving same data\n    pltx = plotConstell(Xin)\n    plt.title('Transmit signal constellation')\n\n\n    # Pass through the channel\n    Xout = H*Xin\n\n    # Adding white gaussian noise\n    # The signal should have unit power. (?)\n    Y = awgnChannel(Xout,N0)\n    print('Size of received signal (Nr x N): '+str(Y.shape))\n    plty = plotConstell(Y)\n    plt.title('Received signal constellation')\n\n    # Covariance matrix of noise. Currently assuming uncorrelated across antennas.\n    Cz = np.identity(Nr)\n\n    Eq = getEqualizer(H, Cx, Cz, Equalizer.ZF)\n    t_Yhat = Eq*Y\n    print('Size of Equalized signal (Nt x N): '+str(t_Yhat.shape))\n\n    # NOTE: Following is done assuming all Nt antennas had the same data and RX\n    # diversity is being exploited\n    # Yhat = np.mean(t_Yhat,0)\n\n    Yhat = t_Yhat\n    print('Size of Equalized signal (Nt x N): '+str(Yhat.shape))\n    pltyhat = plotConstell(Yhat)\n    plt.title('Equalized signal constellation')\n\n    Xrec = mlDetectionIQ(Yhat, mod)\n    #plotConstell(Xrec)\n\n    nofsamp_err = getSER(X,Xrec)\n    evm = getEVM(X,Yhat,mod)\n    print('SER = '+str(nofsamp_err/N/Nt))\n    print('EVM = '+str(evm)+' dB')\n    #plt.show()\n    #sphereDecoder(H, X, Y, mod, 'basic', 4)\n    if Nt==1:\n        print('Using diversity gain.')\n        Ycomb = equalRatioCombine(Y, H)\n        Ycomb = maxRatioCombine(Y, H)\n        Ycomb = selectRatioCombine(Y, H)", "fn_id": 0, "class_fn": false, "repo": "r4tn3sh/MIMO_detection", "file": "src/main_detection.py", "last_update_at": "2022-01-19T09:37:37+00:00", "question_id": "71ea046df258c106b4c8f2d6daf009537ec7c40c_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('N', help='Number of complex samples.', type=int)\n    parser.add_argument('mod', help='Number of bits per sample.', type=int, choices=[2, 4, 6, 8, 10], metavar='Mod')\n    parser.add_argument('--snr', help='Signal to noise ratio (dB).', type=float, nargs='?', default=10, metavar='SNR')\n    parser.add_argument('--Nr', help='Number of RX antennas.', type=int, nargs='?', default=1, metavar='Nr')\n    parser.add_argument('--Nt', help='Number of TX antennas.', type=int, nargs='?', default=1, metavar='Nt')\n    parser.add_argument('--txmode', help='Transmission moTX mode. Use multiple antennas for channel diversity or multiple streams.', type=int, nargs='?', choices=[0, 1], default=0, metavar='TX mode')\n    args = parser.parse_args()\n    N = args.N\n    mod = args.mod\n    snr = args.snr\n    Nr = args.Nr\n    Nt = args.Nt\n    tx_mode = args.txmode\n    N0 = 1 / np.power(10, snr / 10)\n    NoS = min(Nr, Nt)\n    H = generateChMatrix(Nr, Nt, Channel.RAND_UNIT_GOOD)\n    print('Condition number of the generated channel: ' + str(np.linalg.cond(H)))\n    X = generateIQ(Nt, N, mod, tx_mode)\n    Xin = X\n    Cx = np.var(X) * np.identity(Nt)\n    pltx = plotConstell(Xin)\n    plt.title('Transmit signal constellation')\n    Xout = H * Xin\n    Y = awgnChannel(Xout, N0)\n    print('Size of received signal (Nr x N): ' + str(Y.shape))\n    plty = plotConstell(Y)\n    plt.title('Received signal constellation')\n    Cz = np.identity(Nr)\n    Eq = getEqualizer(H, Cx, Cz, Equalizer.ZF)\n    t_Yhat = Eq * Y\n    print('Size of Equalized signal (Nt x N): ' + str(t_Yhat.shape))\n    Yhat = t_Yhat\n    print('Size of Equalized signal (Nt x N): ' + str(Yhat.shape))\n    pltyhat = plotConstell(Yhat)\n    plt.title('Equalized signal constellation')\n    Xrec = mlDetectionIQ(Yhat, mod)\n    nofsamp_err = getSER(X, Xrec)\n    evm = getEVM(X, Yhat, mod)\n    print('SER = ' + str(nofsamp_err / N / Nt))\n    print('EVM = ' + str(evm) + ' dB')\n    if Nt == 1:\n        print('Using diversity gain.')\n        Ycomb = equalRatioCombine(Y, H)\n        Ycomb = maxRatioCombine(Y, H)\n"]]}
{"hexsha": "cd4e26fb528c0876b343d67b094aa0385193bb64", "ext": "py", "lang": "Python", "content": "def test_calling_at_cmd_raises_CommandFailure_when_NO_CARRIER_in_at_cmd_output_occurred(buffer_connection, at_cmd_test_class):\n    from moler.exceptions import CommandFailure\n    buffer_connection.remote_inject_response([\"AT+CMD\\ndata\\nNO CARRIER\\n\"])\n    at_cmd = at_cmd_test_class(\"AT+CMD\", connection=buffer_connection.moler_connection)\n    with pytest.raises(CommandFailure) as error:\n        at_cmd()\n    assert 'AT+CMD' in str(error.value)\n    assert \"failed with >>NO CARRIER<<\" in str(error.value)", "fn_id": 3, "class_fn": false, "repo": "jochenparm/moler", "file": "test/cmd/at/test_cmd_genericat.py", "last_update_at": "2022-03-28T10:36:57+00:00", "question_id": "cd4e26fb528c0876b343d67b094aa0385193bb64_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_calling_at_cmd_raises_CommandFailure_when_NO_CARRIER_in_at_cmd_output_occurred(buffer_connection, at_cmd_test_class):\n    from moler.exceptions import CommandFailure\n    buffer_connection.remote_inject_response(['AT+CMD\\ndata\\nNO CARRIER\\n'])\n    at_cmd = at_cmd_test_class('AT+CMD', connection=buffer_connection.moler_connection)\n    with pytest.raises(CommandFailure) as error:\n        at_cmd()\n    assert 'AT+CMD' in str(error.value)\n"]]}
{"hexsha": "492b01d0a2ff285db1f00f3fd18128d140552b31", "ext": "py", "lang": "Python", "content": "def main(args):\n    \"\"\" Transforms the problem file streamed in through the standard input using JSON the data file passed via command-line argument. \"\"\"\n    if len(args) < 1:\n        eprint(\"Usage: {0} <data.json>\".format(os.path.basename(sys.argv[0])))\n        exit(-1)\n\n    data_path = args[0]\n    # with open(data_path, mode='r', encoding=\"utf-8\") as fp:\n    with io.open(data_path, mode='r', encoding=\"utf-8\") as fp:\n        input_data = json.load(fp)\n\n    template = Template(template_text)\n    template.environment.filters['tif'] = tif_filter\n    template.environment.filters['mapattr'] = map_filter\n\n    rendered = template.render(data=input_data)\n\n    # output the rednered template to the standard output\n    print(rendered)\n    print(\"; This PDDL problem file was generated using Jinja2 template\")\n    print(\";    Python: \" + sys.version)\n    print(\";    Data: \" + data_path)\n    print(\";    Time: \" + str(datetime.datetime.now()))", "fn_id": 1, "class_fn": false, "repo": "ghmoore/vscode-pddl", "file": "scripts/transform_jinja2.py", "last_update_at": "2022-03-31T03:29:17+00:00", "question_id": "492b01d0a2ff285db1f00f3fd18128d140552b31_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main(args):\n    \"\"\" Transforms the problem file streamed in through the standard input using JSON the data file passed via command-line argument. \"\"\"\n    if len(args) < 1:\n        eprint('Usage: {0} <data.json>'.format(os.path.basename(sys.argv[0])))\n        exit(-1)\n    data_path = args[0]\n    with io.open(data_path, mode='r', encoding='utf-8') as fp:\n        input_data = json.load(fp)\n    template = Template(template_text)\n    template.environment.filters['tif'] = tif_filter\n    template.environment.filters['mapattr'] = map_filter\n    rendered = template.render(data=input_data)\n    print(rendered)\n    print('; This PDDL problem file was generated using Jinja2 template')\n    print(';    Python: ' + sys.version)\n    print(';    Data: ' + data_path)\n"]]}
{"hexsha": "5394a76ea2a59c9e92e104f1f4a85cc147468ebc", "ext": "py", "lang": "Python", "content": "def compute_precision_recall(cm: np.ndarray) -> Tuple[float, float]:\n    \"\"\" computes precision and recall from a confusion matrix \"\"\"\n    tn = cm[0, 0]\n    tp = cm[1, 1]\n    fp = cm[0, 1]\n    fn = cm[1, 0]\n\n    precision = tp / get_denominator(tp + fp)\n    recall = tp / get_denominator(tp + fn)\n    return precision, recall", "fn_id": 18, "class_fn": false, "repo": "beng-ern/deepethogram", "file": "deepethogram/metrics.py", "last_update_at": "2022-02-25T05:40:35+00:00", "question_id": "5394a76ea2a59c9e92e104f1f4a85cc147468ebc_18", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def compute_precision_recall(cm: np.ndarray) -> Tuple[float, float]:\n    \"\"\" computes precision and recall from a confusion matrix \"\"\"\n    tn = cm[0, 0]\n    tp = cm[1, 1]\n    fp = cm[0, 1]\n    fn = cm[1, 0]\n    precision = tp / get_denominator(tp + fp)\n    recall = tp / get_denominator(tp + fn)\n"]]}
{"hexsha": "c0ad2b0d7ce7ffcea0faf8560c9928107a634ddd", "ext": "py", "lang": "Python", "content": "def build_matrix(variants: Manager) -> list:\n    variant_colors = variants.filter().distinct('color').values('color')\n    sorted_colors = get_sorted_values(\n        Color,\n        variant_colors\n    )\n\n    variant_sizes = variants.filter().distinct('size').values('size')\n    sorted_sizes = get_sorted_values(\n        Size,\n        variant_sizes\n    )\n\n    matrix = build_structure(sorted_sizes.count(), sorted_colors.count())\n\n    matrix = place_variants_into_matrix(\n        matrix,\n        variants,\n        list(sorted_colors.values_list('id', flat=True)),\n        list(sorted_sizes.values_list('id', flat=True))\n    )\n\n    return matrix", "fn_id": 1, "class_fn": false, "repo": "falconsoft3d/clientportal_shop", "file": "store/services.py", "last_update_at": "2022-03-22T10:11:58+00:00", "question_id": "c0ad2b0d7ce7ffcea0faf8560c9928107a634ddd_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def build_matrix(variants: Manager) -> list:\n    variant_colors = variants.filter().distinct('color').values('color')\n    sorted_colors = get_sorted_values(Color, variant_colors)\n    variant_sizes = variants.filter().distinct('size').values('size')\n    sorted_sizes = get_sorted_values(Size, variant_sizes)\n    matrix = build_structure(sorted_sizes.count(), sorted_colors.count())\n    matrix = place_variants_into_matrix(matrix, variants, list(sorted_colors.values_list('id', flat=True)), list(sorted_sizes.values_list('id', flat=True)))\n"]]}
{"hexsha": "5850bb624fc311e1b4624e5a4555703ac8613620", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize(\n    \"start_block,end_block,batch_size,resource_group,web3_provider_type\",\n    [\n        (10324748, 10324748, 1, \"version_01a_block\", \"mock\"),\n        (12640760, 12640760, 1, \"version_03_block\", \"mock\"),\n        (14473621, 14473621, 1, \"version_04_block\", \"mock\"),\n        (14473622, 14473622, 1, \"version_05_block\", \"mock\"),\n        skip_if_slow_tests_disabled(\n            (10324748, 10324748, 1, \"version_01a_block\", \"public_endpoint\")\n        ),\n        skip_if_slow_tests_disabled(\n            (12640760, 12640760, 1, \"version_03_block\", \"public_endpoint\")\n        ),\n        skip_if_slow_tests_disabled(\n            (14473621, 14473621, 1, \"version_04_block\", \"public_endpoint\")\n        ),\n        skip_if_slow_tests_disabled(\n            (14473622, 14473622, 1, \"version_05_block\", \"public_endpoint\")\n        ),\n    ],\n)\ndef test_export_blocks_job(\n    tmpdir, start_block, end_block, batch_size, resource_group, web3_provider_type\n):\n    blocks_output_file = str(tmpdir.join(\"actual_blocks.csv\"))\n    transactions_output_file = str(tmpdir.join(\"actual_transactions.csv\"))\n\n    job = ExportBlocksJob(\n        start_block=start_block,\n        end_block=end_block,\n        batch_size=batch_size,\n        batch_web3_provider=ThreadLocalProxy(\n            lambda: get_web3_provider(\n                web3_provider_type,\n                lambda file: read_resource(resource_group, file),\n                batch=True,\n            )\n        ),\n        max_workers=5,\n        item_exporter=blocks_and_transactions_item_exporter(\n            blocks_output_file, transactions_output_file\n        ),\n        export_blocks=blocks_output_file is not None,\n        export_transactions=transactions_output_file is not None,\n    )\n    job.run()\n\n    compare_lines_ignore_order(\n        read_resource(resource_group, \"expected_blocks.csv\"),\n        read_file(blocks_output_file),\n    )\n\n    compare_lines_ignore_order(\n        read_resource(resource_group, \"expected_transactions.csv\"),\n        read_file(transactions_output_file),\n    )", "fn_id": 0, "class_fn": false, "repo": "geometry-labs/icon-etl", "file": "tests/iconetl/job/test_export_blocks_job.py", "last_update_at": "2022-02-28T23:22:35+00:00", "question_id": "5850bb624fc311e1b4624e5a4555703ac8613620_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('start_block,end_block,batch_size,resource_group,web3_provider_type', [(10324748, 10324748, 1, 'version_01a_block', 'mock'), (12640760, 12640760, 1, 'version_03_block', 'mock'), (14473621, 14473621, 1, 'version_04_block', 'mock'), (14473622, 14473622, 1, 'version_05_block', 'mock'), skip_if_slow_tests_disabled((10324748, 10324748, 1, 'version_01a_block', 'public_endpoint')), skip_if_slow_tests_disabled((12640760, 12640760, 1, 'version_03_block', 'public_endpoint')), skip_if_slow_tests_disabled((14473621, 14473621, 1, 'version_04_block', 'public_endpoint')), skip_if_slow_tests_disabled((14473622, 14473622, 1, 'version_05_block', 'public_endpoint'))])\ndef test_export_blocks_job(tmpdir, start_block, end_block, batch_size, resource_group, web3_provider_type):\n    blocks_output_file = str(tmpdir.join('actual_blocks.csv'))\n    transactions_output_file = str(tmpdir.join('actual_transactions.csv'))\n    job = ExportBlocksJob(start_block=start_block, end_block=end_block, batch_size=batch_size, batch_web3_provider=ThreadLocalProxy(lambda: get_web3_provider(web3_provider_type, lambda file: read_resource(resource_group, file), batch=True)), max_workers=5, item_exporter=blocks_and_transactions_item_exporter(blocks_output_file, transactions_output_file), export_blocks=blocks_output_file is not None, export_transactions=transactions_output_file is not None)\n    job.run()\n    compare_lines_ignore_order(read_resource(resource_group, 'expected_blocks.csv'), read_file(blocks_output_file))\n"]]}
{"hexsha": "8ed3f82bfc69879bbbfe3dc14099027b64445975", "ext": "py", "lang": "Python", "content": "@contextmanager\ndef session_scope():\n    \"\"\"Provide a transactional scope around a series of operations.\"\"\"\n    session = Session()\n    try:\n        yield session\n        session.commit()\n    except SQLAlchemyError:\n        session.rollback()\n        raise\n    finally:\n        session.close()", "fn_id": 0, "class_fn": false, "repo": "Im-zeus/Stickers", "file": "bot/database/base.py", "last_update_at": "2022-03-26T15:17:52+00:00", "question_id": "8ed3f82bfc69879bbbfe3dc14099027b64445975_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@contextmanager\ndef session_scope():\n    \"\"\"Provide a transactional scope around a series of operations.\"\"\"\n    session = Session()\n    try:\n        yield session\n        session.commit()\n    except SQLAlchemyError:\n        session.rollback()\n        raise\n    finally:\n"]]}
{"hexsha": "42e138c39602d365305963c0c6bd5d5d912f7af5", "ext": "py", "lang": "Python", "content": "@click.command()\n@click.argument('in_file', type=click.File(encoding='utf-8'))\n@click.option('--out_dir', '-o', default=os.getcwd(), type=click.Path())\ndef openiti2txt(in_file, out_dir):\n    \"\"\"Remove metadata from a text in OpenITI format.\n    \"\"\"\n    create_dirs(out_dir)\n\n    text = []\n    for line in in_file:\n        # Ignore metadata in the file and openITI header\n        if not line.startswith('#META#') and line != '######OpenITI#\\n':\n            text.append(line)\n\n    # TODO: optionally remove other openITI tags\n\n    text = u''.join(text)\n    fname = out_file_name(out_dir, in_file.name, ext='txt')\n    with codecs.open(fname, 'wb', encoding='utf-8') as f:\n        f.write(text)", "fn_id": 0, "class_fn": false, "repo": "arabic-digital-humanities/adhtools", "file": "adhtools/openiti2txt.py", "last_update_at": "2022-02-17T06:38:45+00:00", "question_id": "42e138c39602d365305963c0c6bd5d5d912f7af5_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@click.command()\n@click.argument('in_file', type=click.File(encoding='utf-8'))\n@click.option('--out_dir', '-o', default=os.getcwd(), type=click.Path())\ndef openiti2txt(in_file, out_dir):\n    \"\"\"Remove metadata from a text in OpenITI format.\n    \"\"\"\n    create_dirs(out_dir)\n    text = []\n    for line in in_file:\n        if not line.startswith('#META#') and line != '######OpenITI#\\n':\n            text.append(line)\n    text = u''.join(text)\n    fname = out_file_name(out_dir, in_file.name, ext='txt')\n    with codecs.open(fname, 'wb', encoding='utf-8') as f:\n"]]}
{"hexsha": "ff9b196d55a53480d03f206f97fbba23a16107fc", "ext": "py", "lang": "Python", "content": "def getMetaEbible():\n    try:\n        base_url = 'http://ebible.org/Scriptures/copyright.php'\n        soup = BeautifulSoup(requests.get(base_url).content)\n        tables=soup.select('table')\n        dfs=[]\n        for table in tables:\n            dfs.append(pd.read_html(table.prettify(), flavor='bs4',header=0)[0])\n        df=pd.concat(dfs, sort=True)\n        mask=(df['FCBH/DBS'].str.len() == 6) & (df['FCBH/DBS'].str.isupper())\n    except:\n        soup = BeautifulSoup(open(\"../meta/ebible.html\") )\n        tables=soup.select('table')\n        dfs=[]\n        for table in tables:\n            dfs.append(pd.read_html(table.prettify(), flavor='bs4',header=0)[0])\n        df=pd.concat(dfs, sort=True)\n        mask=(df['FCBH/DBS'].str.len() == 6) & (df['FCBH/DBS'].str.isupper())\n\n    df = df.loc[mask]\n    df['iso']=[x[0:3] for x in df['ID'].tolist()]\n    df=df[['iso','FCBH/DBS','Language in English', 'Year','Short Title']]\n    df=df.rename(index=str,columns={'iso':'language_iso','FCBH/DBS':'trans_ID','Language in English':'language_name','Short Title':'Description','Date':'Year'})\n    df=df[['trans_ID','language_iso','language_name','Description','Year']]\n    df.set_index('trans_ID')\n    return df", "fn_id": 1, "class_fn": false, "repo": "JHurricane96/1000Langs", "file": "metaAPI/metadata.py", "last_update_at": "2022-03-16T09:42:38+00:00", "question_id": "ff9b196d55a53480d03f206f97fbba23a16107fc_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def getMetaEbible():\n    try:\n        base_url = 'http://ebible.org/Scriptures/copyright.php'\n        soup = BeautifulSoup(requests.get(base_url).content)\n        tables = soup.select('table')\n        dfs = []\n        for table in tables:\n            dfs.append(pd.read_html(table.prettify(), flavor='bs4', header=0)[0])\n        df = pd.concat(dfs, sort=True)\n        mask = (df['FCBH/DBS'].str.len() == 6) & df['FCBH/DBS'].str.isupper()\n    except:\n        soup = BeautifulSoup(open('../meta/ebible.html'))\n        tables = soup.select('table')\n        dfs = []\n        for table in tables:\n            dfs.append(pd.read_html(table.prettify(), flavor='bs4', header=0)[0])\n        df = pd.concat(dfs, sort=True)\n        mask = (df['FCBH/DBS'].str.len() == 6) & df['FCBH/DBS'].str.isupper()\n    df = df.loc[mask]\n    df['iso'] = [x[0:3] for x in df['ID'].tolist()]\n    df = df[['iso', 'FCBH/DBS', 'Language in English', 'Year', 'Short Title']]\n    df = df.rename(index=str, columns={'iso': 'language_iso', 'FCBH/DBS': 'trans_ID', 'Language in English': 'language_name', 'Short Title': 'Description', 'Date': 'Year'})\n    df = df[['trans_ID', 'language_iso', 'language_name', 'Description', 'Year']]\n    df.set_index('trans_ID')\n"]]}
{"hexsha": "893e690ec93465ad034d4affcca8275c09a0ee45", "ext": "py", "lang": "Python", "content": "def find_files(src, src_ext_name, use_start=False):\n    \"\"\"\n    Method to find files with given extension\n    \"\"\"\n    result = []\n    for root, dirs, files in os.walk(src):\n        for file in files:\n            if file == src_ext_name or file.endswith(src_ext_name):\n                result.append(os.path.join(root, file))\n            elif use_start and file.startswith(src_ext_name):\n                result.append(os.path.join(root, file))\n    return result", "fn_id": 2, "class_fn": false, "repo": "deepu-james/sast-scan", "file": "lib/utils.py", "last_update_at": "2022-02-24T21:30:25+00:00", "question_id": "893e690ec93465ad034d4affcca8275c09a0ee45_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def find_files(src, src_ext_name, use_start=False):\n    \"\"\"\n    Method to find files with given extension\n    \"\"\"\n    result = []\n    for root, dirs, files in os.walk(src):\n        for file in files:\n            if file == src_ext_name or file.endswith(src_ext_name):\n                result.append(os.path.join(root, file))\n            elif use_start and file.startswith(src_ext_name):\n                result.append(os.path.join(root, file))\n"]]}
{"hexsha": "2f05ce920449b8ef9c0cc2a907d37b060f0acb6d", "ext": "py", "lang": "Python", "content": "def get_score(fname):\n    print(f'start processing {fname}', flush=True)\n    offset, size, nk = parse_retrieve_fname(fname)\n    print(f'offset: {offset}, size{size}, k{nk}', flush=True)\n    scores = np.zeros(args.dstore_size, dtype=np.float32)\n\n    ret = np.memmap(os.path.join(args.retrieval_dir, fname), dtype=np.int32, mode='r', shape=(size, nk))\n\n    # import pdb; pdb.set_trace()\n    for i, row in enumerate(ret):\n        if i % 100000 == 0:\n            print(f'processing {i} rows', flush=True)\n            # break\n        scores[row] = scores[row] + 1. / (np.arange(len(row)) + 1)\n        # if i == 50000:\n        #     break\n\n    return scores", "fn_id": 1, "class_fn": false, "repo": "ishine/efficient-knnlm", "file": "ef_knnlm/dstore_compression/filter_compression.py", "last_update_at": "2022-03-30T01:58:13+00:00", "question_id": "2f05ce920449b8ef9c0cc2a907d37b060f0acb6d_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_score(fname):\n    print(f'start processing {fname}', flush=True)\n    offset, size, nk = parse_retrieve_fname(fname)\n    print(f'offset: {offset}, size{size}, k{nk}', flush=True)\n    scores = np.zeros(args.dstore_size, dtype=np.float32)\n    ret = np.memmap(os.path.join(args.retrieval_dir, fname), dtype=np.int32, mode='r', shape=(size, nk))\n    for i, row in enumerate(ret):\n        if i % 100000 == 0:\n            print(f'processing {i} rows', flush=True)\n        scores[row] = scores[row] + 1.0 / (np.arange(len(row)) + 1)\n"]]}
{"hexsha": "7668d492e756b0a06bc64af07dea863e5e516702", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize(\n    \"discovery,username\",\n    [\n        (TEST_DISCOVERY, TEST_USERNAME),\n        (TEST_DISCOVERY2, TEST_USERNAME2),\n    ],\n)\nasync def test_zeroconf_flow(\n    hass: HomeAssistant,\n    mock_setup_entry: AsyncMock,\n    mock_smile_config_flow: MagicMock,\n    discovery: ZeroconfServiceInfo,\n    username: str,\n) -> None:\n    \"\"\"Test config flow for smile devices.\"\"\"\n    result = await hass.config_entries.flow.async_init(\n        DOMAIN,\n        context={CONF_SOURCE: SOURCE_ZEROCONF},\n        data=discovery,\n    )\n    assert result.get(\"type\") == RESULT_TYPE_FORM\n    assert result.get(\"errors\") == {}\n    assert result.get(\"step_id\") == \"user\"\n    assert \"flow_id\" in result\n\n    result2 = await hass.config_entries.flow.async_configure(\n        result[\"flow_id\"],\n        user_input={CONF_PASSWORD: TEST_PASSWORD},\n    )\n    await hass.async_block_till_done()\n\n    assert result2.get(\"type\") == RESULT_TYPE_CREATE_ENTRY\n    assert result2.get(\"title\") == \"Test Smile Name\"\n    assert result2.get(\"data\") == {\n        CONF_HOST: TEST_HOST,\n        CONF_PASSWORD: TEST_PASSWORD,\n        CONF_PORT: DEFAULT_PORT,\n        CONF_USERNAME: username,\n        PW_TYPE: API,\n    }\n\n    assert len(mock_setup_entry.mock_calls) == 1\n    assert len(mock_smile_config_flow.connect.mock_calls) == 1", "fn_id": 2, "class_fn": false, "repo": "MrDelik/core", "file": "tests/components/plugwise/test_config_flow.py", "last_update_at": "2022-03-17T00:55:28+00:00", "question_id": "7668d492e756b0a06bc64af07dea863e5e516702_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('discovery,username', [(TEST_DISCOVERY, TEST_USERNAME), (TEST_DISCOVERY2, TEST_USERNAME2)])\nasync def test_zeroconf_flow(hass: HomeAssistant, mock_setup_entry: AsyncMock, mock_smile_config_flow: MagicMock, discovery: ZeroconfServiceInfo, username: str) -> None:\n    \"\"\"Test config flow for smile devices.\"\"\"\n    result = await hass.config_entries.flow.async_init(DOMAIN, context={CONF_SOURCE: SOURCE_ZEROCONF}, data=discovery)\n    assert result.get('type') == RESULT_TYPE_FORM\n    assert result.get('errors') == {}\n    assert result.get('step_id') == 'user'\n    assert 'flow_id' in result\n    result2 = await hass.config_entries.flow.async_configure(result['flow_id'], user_input={CONF_PASSWORD: TEST_PASSWORD})\n    await hass.async_block_till_done()\n    assert result2.get('type') == RESULT_TYPE_CREATE_ENTRY\n    assert result2.get('title') == 'Test Smile Name'\n    assert result2.get('data') == {CONF_HOST: TEST_HOST, CONF_PASSWORD: TEST_PASSWORD, CONF_PORT: DEFAULT_PORT, CONF_USERNAME: username, PW_TYPE: API}\n    assert len(mock_setup_entry.mock_calls) == 1\n"]]}
{"hexsha": "99075f5a3278dc85eb59aab0c4c4a6c15d2f5e94", "ext": "py", "lang": "Python", "content": "def test_lease_keep(client):\n    ID = random.randint(10000, 100000)\n    TTL = 5  # min is 2sec\n    keep_cb = mock.Mock()\n    cancel_cb = mock.Mock()\n\n    lease = client.Lease(ttl=TTL, ID=ID)\n    lease.grant()\n    lease.keepalive(keep_cb=keep_cb, cancel_cb=cancel_cb)\n    with pytest.raises(RuntimeError):\n        lease.keepalive()\n    time.sleep(1)\n    lease.cancel_keepalive()\n    assert keep_cb.called\n    assert keep_cb.call_count < 2  # or it keep too fast\n    assert cancel_cb.called\n\n    lease.keepalive_once()\n    lease = client.Lease(ttl=TTL, ID=ID, new=False)\n    lease.grant()\n    assert lease.alive()\n    lease.revoke()\n\n    lease = client.Lease(ttl=TTL)\n    lease.grant()\n    assert lease.alive()\n    lease.revoke()", "fn_id": 2, "class_fn": false, "repo": "pjz/etcd3-py", "file": "tests/test_lease_util.py", "last_update_at": "2022-03-20T13:29:14+00:00", "question_id": "99075f5a3278dc85eb59aab0c4c4a6c15d2f5e94_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_lease_keep(client):\n    ID = random.randint(10000, 100000)\n    TTL = 5\n    keep_cb = mock.Mock()\n    cancel_cb = mock.Mock()\n    lease = client.Lease(ttl=TTL, ID=ID)\n    lease.grant()\n    lease.keepalive(keep_cb=keep_cb, cancel_cb=cancel_cb)\n    with pytest.raises(RuntimeError):\n        lease.keepalive()\n    time.sleep(1)\n    lease.cancel_keepalive()\n    assert keep_cb.called\n    assert keep_cb.call_count < 2\n    assert cancel_cb.called\n    lease.keepalive_once()\n    lease = client.Lease(ttl=TTL, ID=ID, new=False)\n    lease.grant()\n    assert lease.alive()\n    lease.revoke()\n    lease = client.Lease(ttl=TTL)\n    lease.grant()\n    assert lease.alive()\n"]]}
{"hexsha": "84f8fc5c7345ece989d9ee94ac87753dd2301118", "ext": "py", "lang": "Python", "content": "def test_name_error():\n    with pytest.raises(NameError):\n        iceberg(2, 3)\n    with pytest.raises(NameError):\n        chocolat()\n    with pytest.raises(NameError):\n        wishful_thinking()", "fn_id": 3, "class_fn": false, "repo": "mila-iqia/ptera", "file": "tests/test_selfless.py", "last_update_at": "2022-03-14T06:29:46+00:00", "question_id": "84f8fc5c7345ece989d9ee94ac87753dd2301118_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_name_error():\n    with pytest.raises(NameError):\n        iceberg(2, 3)\n    with pytest.raises(NameError):\n        chocolat()\n    with pytest.raises(NameError):\n"]]}
{"hexsha": "75f79ea2cbdc8f137ac84d9a747dff9ee3623a4d", "ext": "py", "lang": "Python", "content": "def _extract_nth_band(subdataset, xy_bbox, rad_coefs):\n    x0, y0, x1, y1 = xy_bbox\n    row_start, col_start = subdataset.index(x0, y0)\n    row_stop, col_stop = subdataset.index(x1, y1)\n    arr = subdataset.read(\n        1,\n        window=Window.from_slices(\n            (row_start, row_stop + 1),\n            (col_start, col_stop + 1)\n        )\n    )\n    # Turn DNs into TOA reflectances\n    band_array = _digital_number_to_reflectance(arr, *rad_coefs)\n    return band_array", "fn_id": 2, "class_fn": false, "repo": "inrae/SISPPEO", "file": "sisppeo/readers/L8_USGS_L1C1.py", "last_update_at": "2022-02-18T10:39:13+00:00", "question_id": "75f79ea2cbdc8f137ac84d9a747dff9ee3623a4d_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _extract_nth_band(subdataset, xy_bbox, rad_coefs):\n    x0, y0, x1, y1 = xy_bbox\n    row_start, col_start = subdataset.index(x0, y0)\n    row_stop, col_stop = subdataset.index(x1, y1)\n    arr = subdataset.read(1, window=Window.from_slices((row_start, row_stop + 1), (col_start, col_stop + 1)))\n    band_array = _digital_number_to_reflectance(arr, *rad_coefs)\n"]]}
{"hexsha": "f147fe2ce8778eb046f933fca8c16491d9307826", "ext": "py", "lang": "Python", "content": "def create_actor(cfg: dict) -> BaseActor:\n    import_module(cfg.actor.import_names)\n    if cfg.actor.actor_type not in actor_mapping.keys():\n        raise KeyError(\"not support actor type: {}\".format(cfg.actor.actor_type))\n    else:\n        return actor_mapping[cfg.actor.actor_type](cfg)", "fn_id": 1, "class_fn": false, "repo": "XinyuJing/DI-star", "file": "ctools/worker/actor/base_actor_controller.py", "last_update_at": "2022-03-02T11:37:33+00:00", "question_id": "f147fe2ce8778eb046f933fca8c16491d9307826_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_actor(cfg: dict) -> BaseActor:\n    import_module(cfg.actor.import_names)\n    if cfg.actor.actor_type not in actor_mapping.keys():\n        raise KeyError('not support actor type: {}'.format(cfg.actor.actor_type))\n    else:\n"]]}
{"hexsha": "7d4ec3b2ea9b93799340426971898142d97d90f5", "ext": "py", "lang": "Python", "content": "def verify_data(img1, img2, intrinsics1, extrinsics1, intrinsics2, extrinsics2):\n    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n\n    E, F, relative_pose = two_view_geometry(intrinsics1, extrinsics1,\n                                            intrinsics2, extrinsics2)\n\n    # sift = cv2.xfeatures2d.SIFT_create(nfeatures=20)\n    # kp1 = sift.detect(img1, mask=None)\n    # coord1 = np.array([[kp.pt[0], kp.pt[1]] for kp in kp1]).T\n\n    # Initiate ORB detector\n    orb = cv2.ORB_create()\n    # find the keypoints with ORB\n    kp1 = orb.detect(img1, None)\n    coord1 = np.array([[kp.pt[0], kp.pt[1]] for kp in kp1[:20]]).T\n    return epipolar(coord1, F, img1, img2)", "fn_id": 3, "class_fn": false, "repo": "QiuhongAnnaWei/IBRNet", "file": "ibrnet/data_loaders/data_verifier.py", "last_update_at": "2022-03-27T05:09:26+00:00", "question_id": "7d4ec3b2ea9b93799340426971898142d97d90f5_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def verify_data(img1, img2, intrinsics1, extrinsics1, intrinsics2, extrinsics2):\n    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n    E, F, relative_pose = two_view_geometry(intrinsics1, extrinsics1, intrinsics2, extrinsics2)\n    orb = cv2.ORB_create()\n    kp1 = orb.detect(img1, None)\n    coord1 = np.array([[kp.pt[0], kp.pt[1]] for kp in kp1[:20]]).T\n"]]}
{"hexsha": "f9adde7ecc876c2f551a9cf339a755bdb78d4250", "ext": "py", "lang": "Python", "content": "def make_cursor(widget, iconpath, x, y):\n    image = Gtk.Image()\n    image.set_from_file(iconpath)\n    pixbuf = image.get_pixbuf()\n    screen = widget.get_screen()\n    display = screen.get_display()\n    return Gdk.Cursor(display, pixbuf, x, y)", "fn_id": 5, "class_fn": false, "repo": "kyraikeda/ginga", "file": "ginga/gtk3w/GtkHelp.py", "last_update_at": "2022-03-23T04:10:54+00:00", "question_id": "f9adde7ecc876c2f551a9cf339a755bdb78d4250_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def make_cursor(widget, iconpath, x, y):\n    image = Gtk.Image()\n    image.set_from_file(iconpath)\n    pixbuf = image.get_pixbuf()\n    screen = widget.get_screen()\n    display = screen.get_display()\n"]]}
{"hexsha": "6b37f9565f143bb7d21132b392f07cc3ab7c806b", "ext": "py", "lang": "Python", "content": "def intersect(box_a, box_b):\n    n = box_a.size(0)\n    A = box_a.size(1)\n    B = box_b.size(1)\n    max_xy = torch.min(box_a[:, :, 2:].unsqueeze(2).expand(n, A, B, 2),\n                       box_b[:, :, 2:].unsqueeze(1).expand(n, A, B, 2))\n    min_xy = torch.max(box_a[:, :, :2].unsqueeze(2).expand(n, A, B, 2),\n                       box_b[:, :, :2].unsqueeze(1).expand(n, A, B, 2))\n    return torch.clamp(max_xy - min_xy, min=0).prod(3)", "fn_id": 0, "class_fn": false, "repo": "MCG-NKU/LD", "file": "mmdet/models/dense_heads/imitationv2_head.py", "last_update_at": "2022-03-27T05:24:46+00:00", "question_id": "6b37f9565f143bb7d21132b392f07cc3ab7c806b_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def intersect(box_a, box_b):\n    n = box_a.size(0)\n    A = box_a.size(1)\n    B = box_b.size(1)\n    max_xy = torch.min(box_a[:, :, 2:].unsqueeze(2).expand(n, A, B, 2), box_b[:, :, 2:].unsqueeze(1).expand(n, A, B, 2))\n    min_xy = torch.max(box_a[:, :, :2].unsqueeze(2).expand(n, A, B, 2), box_b[:, :, :2].unsqueeze(1).expand(n, A, B, 2))\n"]]}
{"hexsha": "abe83e0211a1980d4f506b8843b7188c6bd41fbd", "ext": "py", "lang": "Python", "content": "def parse_method_info(method):\n    sig = inspect.signature(method)\n    params = sig.parameters\n    return params", "fn_id": 0, "class_fn": false, "repo": "huimlight/SoftTeacher", "file": "ssod/utils/signature.py", "last_update_at": "2022-03-31T13:43:14+00:00", "question_id": "abe83e0211a1980d4f506b8843b7188c6bd41fbd_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def parse_method_info(method):\n    sig = inspect.signature(method)\n    params = sig.parameters\n"]]}
{"hexsha": "d18056486919411cbd22f586342d4119fc9b25d6", "ext": "py", "lang": "Python", "content": "@pytest.fixture(scope=\"class\")\ndef node_factory(request, bitcoind):\n    executor = futures.ThreadPoolExecutor(max_workers=20)\n    node_factory = NodeFactory(request._pyfuncitem.name, executor, bitcoind)\n    yield node_factory\n    node_factory.killall()\n    executor.shutdown(wait=False)", "fn_id": 4, "class_fn": false, "repo": "willcl-ark/lnd_grpc", "file": "tests/test_utils/fixtures.py", "last_update_at": "2022-03-17T00:04:00+00:00", "question_id": "d18056486919411cbd22f586342d4119fc9b25d6_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.fixture(scope='class')\ndef node_factory(request, bitcoind):\n    executor = futures.ThreadPoolExecutor(max_workers=20)\n    node_factory = NodeFactory(request._pyfuncitem.name, executor, bitcoind)\n    yield node_factory\n    node_factory.killall()\n"]]}
{"hexsha": "a4dfca8c84da006ebe66c8c1a14942cc8058e5f2", "ext": "py", "lang": "Python", "content": "def test_gzip():\n    mock = MagicMock(return_value=\"salt\")\n    with patch.dict(archive.__salt__, {\"cmd.run\": mock}):\n        with patch(\"salt.utils.path.which\", lambda exe: exe):\n            ret = archive.gzip(\"/tmp/something-to-compress\")\n            assert [\"salt\"] == ret\n            mock.assert_called_once_with(\n                [\"gzip\", \"/tmp/something-to-compress\"],\n                runas=None,\n                python_shell=False,\n                template=None,\n            )", "fn_id": 2, "class_fn": false, "repo": "babs/salt", "file": "tests/pytests/unit/modules/test_archive.py", "last_update_at": "2022-03-30T18:08:01+00:00", "question_id": "a4dfca8c84da006ebe66c8c1a14942cc8058e5f2_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_gzip():\n    mock = MagicMock(return_value='salt')\n    with patch.dict(archive.__salt__, {'cmd.run': mock}):\n        with patch('salt.utils.path.which', lambda exe: exe):\n            ret = archive.gzip('/tmp/something-to-compress')\n            assert ['salt'] == ret\n"]]}
{"hexsha": "cb25cf5c30ed70a963f0519eb7f6e0049109895a", "ext": "py", "lang": "Python", "content": "def cross_corr(a, b):\n    \"\"\"Cross-correlation\n\n    Calculate the cross correlation of array b against array a.\n\n    Args:\n        a (array): numpy vector. Reference against which cross\n            correlation is calculated.\n        b (array): numpy vector. The resulting cross-correlation function\n            will show how b should be shifted to line up with vector a.\n\n    Returns:\n        array: cross-correlation function\n    \"\"\"\n    # noramlize a & b\n    a = (a-np.min(a))/(np.max(a)-np.min(a))\n    b = (b-np.min(b))/(np.max(b)-np.min(b))\n\n    # compute the Fast Fourrier Transform\n    f_a = np.fft.fft(a)\n    f_b = np.fft.fft(b)\n\n    # get the complex conjugate\n    f_a_c = np.conj(f_a)\n\n    # Convolution Theorem: The Fourier transform of the convolution is\n    # the product of the two Fourier transforms\n\n    # Correlation theorem: multiplying the Fourier transform of\n    # one function by the complex conjugate of the Fourier transform of\n    # the other gives the Fourier transform of their correlation\n    # The inverse then brings us back to the original domain\n    c_corr = np.fft.ifft(f_a_c*f_b)\n\n    # FFT cross corr method gives the cyclic cross-correlation\n    # \"first n points in c_corr[0..2*n] stored in wrap-around order,\n    # i.e., correlations at increasingly negative lags are in c_corr[n]\n    # on down to c_corr[n/2+1], while correlations at increasingly\n    # positive lags are in c_corr[0] (zero lag) on up to c_corr[n/2].\"\n    # --> Numerical Recipes in C to get the linear correlation, need to\n    # circularly rotate the data this puts the peaks of the signal\n    # together\n    c_corr = np.abs(np.roll(c_corr, len(c_corr) // 2))\n    # above does the same as np.fft.fftshift\n    # note that the shift occurs on a pixel/array element level,\n    # so len/2 has to be an integer so enforce floor/int division here\n\n    # normalizing, may help peak fitting\n    c_corr = (c_corr-np.min(c_corr))/(np.max(c_corr)-np.min(c_corr))\n\n    return c_corr", "fn_id": 0, "class_fn": false, "repo": "rbentl/codeastro_workshop", "file": "exampledoc/correlate.py", "last_update_at": "2022-03-28T21:44:09+00:00", "question_id": "cb25cf5c30ed70a963f0519eb7f6e0049109895a_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def cross_corr(a, b):\n    \"\"\"Cross-correlation\n\n    Calculate the cross correlation of array b against array a.\n\n    Args:\n        a (array): numpy vector. Reference against which cross\n            correlation is calculated.\n        b (array): numpy vector. The resulting cross-correlation function\n            will show how b should be shifted to line up with vector a.\n\n    Returns:\n        array: cross-correlation function\n    \"\"\"\n    a = (a - np.min(a)) / (np.max(a) - np.min(a))\n    b = (b - np.min(b)) / (np.max(b) - np.min(b))\n    f_a = np.fft.fft(a)\n    f_b = np.fft.fft(b)\n    f_a_c = np.conj(f_a)\n    c_corr = np.fft.ifft(f_a_c * f_b)\n    c_corr = np.abs(np.roll(c_corr, len(c_corr) // 2))\n    c_corr = (c_corr - np.min(c_corr)) / (np.max(c_corr) - np.min(c_corr))\n"]]}
{"hexsha": "6f5814fe8730e00029487817530c9bc70c2de895", "ext": "py", "lang": "Python", "content": "def test_represent():\n    represent(sx) == Matrix([[0, 1], [1, 0]])\n    represent(sy) == Matrix([[0, -I], [I, 0]])\n    represent(sz) == Matrix([[1, 0], [0, -1]])\n    represent(sm) == Matrix([[0, 0], [1, 0]])\n    represent(sp) == Matrix([[0, 1], [0, 0]])", "fn_id": 11, "class_fn": false, "repo": "gum3ng/sympy", "file": "sympy/physics/quantum/tests/test_pauli.py", "last_update_at": "2022-01-17T12:38:24+00:00", "question_id": "6f5814fe8730e00029487817530c9bc70c2de895_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_represent():\n    represent(sx) == Matrix([[0, 1], [1, 0]])\n    represent(sy) == Matrix([[0, -I], [I, 0]])\n    represent(sz) == Matrix([[1, 0], [0, -1]])\n    represent(sm) == Matrix([[0, 0], [1, 0]])\n"]]}
{"hexsha": "7c077bf44e87cb409658d98030a2b416d9b98dbc", "ext": "py", "lang": "Python", "content": "def do_std(df, group_cols, counted, agg_name):\n    \"\"\"Add the standard deviation for each group for a given feature.\n\n    Arguments:\n        df: DataFrame to group and add the standard deviation feature.\n        group_cols: List with column or columns names to group by.\n        counted: Feature name to get the standard deviation (string).\n        agg_name: New feature name (string)\n\n    Returns:\n        df: Same DataFrame with the new feature\n    \"\"\"\n    gp = df[group_cols + [counted]].groupby(group_cols)[counted].std().reset_index().rename(\n        columns={counted: agg_name})\n    df = df.merge(gp, on=group_cols, how='left')\n    del gp\n    gc.collect()\n    return df", "fn_id": 5, "class_fn": false, "repo": "Sapphirine/Home_Credit_Default_Risk", "file": "utils.py", "last_update_at": "2022-03-06T08:11:59+00:00", "question_id": "7c077bf44e87cb409658d98030a2b416d9b98dbc_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def do_std(df, group_cols, counted, agg_name):\n    \"\"\"Add the standard deviation for each group for a given feature.\n\n    Arguments:\n        df: DataFrame to group and add the standard deviation feature.\n        group_cols: List with column or columns names to group by.\n        counted: Feature name to get the standard deviation (string).\n        agg_name: New feature name (string)\n\n    Returns:\n        df: Same DataFrame with the new feature\n    \"\"\"\n    gp = df[group_cols + [counted]].groupby(group_cols)[counted].std().reset_index().rename(columns={counted: agg_name})\n    df = df.merge(gp, on=group_cols, how='left')\n    del gp\n    gc.collect()\n"]]}
{"hexsha": "c765f106244ed67362c7c994dfe96cabfd5ecfe7", "ext": "py", "lang": "Python", "content": "def save_model(step, saver, sess):\n\tprint('Saving the model at step-%d'%step)\n\n\tsaved_model_path = saver.save(sess, FLAGS.model_dir+'model', global_step=step)\n\n\tprint('have saved model to ', saved_model_path)\n\tprint(\"rewriting the number of model to config.py\")\n\n\t#write the best checkpoint number back to the config.py file\n\tconfigFile=open(FLAGS.config_dir, \"r\")\n\tcontent=[line.strip(\"\\n\") for line in configFile]\n\tconfigFile.close()\n\n\tfor i in range(len(content)):\n\t\tif (\"checkpoint_path\" in content[i]):\n\t\t\tcontent[i]=\"tf.app.flags.DEFINE_string('checkpoint_path', './model/model-%d','The path to a checkpoint from which to fine-tune.')\"%step\n\t\n\tconfigFile=open(FLAGS.config_dir, \"w\")\n\tfor line in content:\n\t\tconfigFile.write(line+\"\\n\")\n\tconfigFile.close()", "fn_id": 3, "class_fn": false, "repo": "Buyun-Liang/DRNets-Nature-Machine-Intelligence", "file": "Crystal-Structure-Phase-Mapping/Bi-Cu-V-lib-100-I-compressed-new-307--solu/train_all.py", "last_update_at": "2022-03-09T16:27:58+00:00", "question_id": "c765f106244ed67362c7c994dfe96cabfd5ecfe7_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def save_model(step, saver, sess):\n    print('Saving the model at step-%d' % step)\n    saved_model_path = saver.save(sess, FLAGS.model_dir + 'model', global_step=step)\n    print('have saved model to ', saved_model_path)\n    print('rewriting the number of model to config.py')\n    configFile = open(FLAGS.config_dir, 'r')\n    content = [line.strip('\\n') for line in configFile]\n    configFile.close()\n    for i in range(len(content)):\n        if 'checkpoint_path' in content[i]:\n            content[i] = \"tf.app.flags.DEFINE_string('checkpoint_path', './model/model-%d','The path to a checkpoint from which to fine-tune.')\" % step\n    configFile = open(FLAGS.config_dir, 'w')\n    for line in content:\n        configFile.write(line + '\\n')\n"]]}
{"hexsha": "7b183455173c94b859ab17b3874decf7cbfd4efd", "ext": "py", "lang": "Python", "content": "def test_run_fixed_height(monkeypatch):\n    monkeypatch.setattr(SymbolsTable, \"__len__\", lambda _: 80)\n    model = run(\n        syms=None,\n        fixed_input_height=128,\n        save_model=False,\n        crnn=CreateCRNNArgs(\n            cnn_num_features=[16, 32, 48, 64, 80],\n            cnn_kernel_size=[3] * 5,\n            cnn_stride=[1] * 5,\n            cnn_dilation=[1] * 5,\n            cnn_activation=[\"LeakyReLU\"] * 5,\n            cnn_poolsize=[2] * 3 + [0] * 2,\n            cnn_dropout=[0] * 5,\n            cnn_batchnorm=[False] * 5,\n            rnn_layers=5,\n        ),\n    )\n    assert isinstance(model, LaiaCRNN)\n    assert sum(param.numel() for param in model.parameters()) == 9591248", "fn_id": 0, "class_fn": false, "repo": "eivtho/PyLaia", "file": "tests/scripts/htr/create_model_test.py", "last_update_at": "2022-02-03T09:04:21+00:00", "question_id": "7b183455173c94b859ab17b3874decf7cbfd4efd_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_run_fixed_height(monkeypatch):\n    monkeypatch.setattr(SymbolsTable, '__len__', lambda _: 80)\n    model = run(syms=None, fixed_input_height=128, save_model=False, crnn=CreateCRNNArgs(cnn_num_features=[16, 32, 48, 64, 80], cnn_kernel_size=[3] * 5, cnn_stride=[1] * 5, cnn_dilation=[1] * 5, cnn_activation=['LeakyReLU'] * 5, cnn_poolsize=[2] * 3 + [0] * 2, cnn_dropout=[0] * 5, cnn_batchnorm=[False] * 5, rnn_layers=5))\n    assert isinstance(model, LaiaCRNN)\n"]]}
{"hexsha": "723de0a5bbd627b16d9141116cd0d4472294a554", "ext": "py", "lang": "Python", "content": "def test_function(test_case):\n    output = rearrange_digits(test_case[0])\n    solution = test_case[1]\n    if sum(output) == sum(solution):\n        print(\"Pass\")\n    else:\n        print(\"Fail\")", "fn_id": 4, "class_fn": false, "repo": "ssi112/data-structures-algorithms", "file": "project3/3_problem.py", "last_update_at": "2022-03-25T22:35:56+00:00", "question_id": "723de0a5bbd627b16d9141116cd0d4472294a554_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_function(test_case):\n    output = rearrange_digits(test_case[0])\n    solution = test_case[1]\n    if sum(output) == sum(solution):\n        print('Pass')\n    else:\n"]]}
{"hexsha": "317e1c038ee3059652d5db98ce6cf7cc29317e97", "ext": "py", "lang": "Python", "content": "def create_get_directory(parent: Path, child: str) -> Path:\n    '''\n    A helper function that automatically creates a directory if it doesn't exist.\n    '''\n    path = parent / child\n    if not path.exists():\n        path.mkdir(parents=True)\n\n    return path", "fn_id": 3, "class_fn": false, "repo": "g4brielvs/wb-nlp-tools", "file": "src/wb_cleaning/utils/scripts.py", "last_update_at": "2022-03-26T20:49:48+00:00", "question_id": "317e1c038ee3059652d5db98ce6cf7cc29317e97_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_get_directory(parent: Path, child: str) -> Path:\n    \"\"\"\n    A helper function that automatically creates a directory if it doesn't exist.\n    \"\"\"\n    path = parent / child\n    if not path.exists():\n        path.mkdir(parents=True)\n"]]}
{"hexsha": "729c032a9d3db74487cfa10763d9949a1ed755e6", "ext": "py", "lang": "Python", "content": "def chkfflotrs(bbarr):\n    bubbleList = [col for col in range(len(bbarr[0]))\n                  if bbarr[0][col] != blank]\n\n    newbbList = []\n\n    for i in range(len(bubbleList)):\n        if i == 0:\n            newbbList.append(bubbleList[i])\n        elif bubbleList[i] > bubbleList[i - 1] + 1:\n            newbbList.append(bubbleList[i])\n\n    cpyofbrd = copy.deepcopy(bbarr)\n\n    for row in range(len(bbarr)):\n        for col in range(len(bbarr[0])):\n            bbarr[row][col] = blank\n\n    for col in newbbList:\n        popflotrs(bbarr, cpyofbrd, col)", "fn_id": 7, "class_fn": false, "repo": "avinashkranjan/PraticalPythonProjects", "file": "Bubble Shooter Game/bubbleshooter.py", "last_update_at": "2022-03-30T07:56:18+00:00", "question_id": "729c032a9d3db74487cfa10763d9949a1ed755e6_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def chkfflotrs(bbarr):\n    bubbleList = [col for col in range(len(bbarr[0])) if bbarr[0][col] != blank]\n    newbbList = []\n    for i in range(len(bubbleList)):\n        if i == 0:\n            newbbList.append(bubbleList[i])\n        elif bubbleList[i] > bubbleList[i - 1] + 1:\n            newbbList.append(bubbleList[i])\n    cpyofbrd = copy.deepcopy(bbarr)\n    for row in range(len(bbarr)):\n        for col in range(len(bbarr[0])):\n            bbarr[row][col] = blank\n    for col in newbbList:\n"]]}
{"hexsha": "69e1416f770975500d2de2c84833e658c2dcc755", "ext": "py", "lang": "Python", "content": "def create_pipeline(**kwargs):\n\n    unimp_full_pipeline = Pipeline(\n        [\n            node(\n                func=init_mlflow,\n                inputs=\"params:unimp_full_model_params\",\n                outputs=\"experiment_id\",\n            ),\n            node(\n                func=init_mlflow_run,\n                inputs=[\n                    \"params:unimp_full_model_params\",\n                    \"experiment_id\",\n                ],\n                outputs=\"active_run_id\",\n            ),\n            node(\n                func=define_unimp_full_model,\n                inputs=[\n                    \"data_engred_unimp_full\",\n                    \"params:unimp_full_model_params\",\n                    \"params:globals\",\n                    \"params:RANDOM_SEED\",\n                ],\n                outputs=\"untrained_unimp_full_model_pipeline\"\n            ),\n            node(\n                func=optimize_unimp_full_model,\n                inputs=[\n                    \"data_engred_unimp_full\",\n                    \"params:unimp_full_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                    \"params:RANDOM_SEED\",\n                    \"params:hypertune\",\n                    \"untrained_unimp_full_model_pipeline\"\n                ],\n                outputs=[\n                    \"best_model_params\",\n                    \"best_untrained_unimp_full_model_pipeline\",\n                ]\n                ),\n            node(\n                func=train_unimp_full_model,\n                inputs=[\n                    \"data_engred_unimp_full\",\n                    \"best_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                    \"params:RANDOM_SEED\",\n                    \"best_untrained_unimp_full_model_pipeline\"\n                ],\n                outputs=\"unimp_full_model_pipeline\",\n                ),\n            node(\n                func=predict,\n                inputs=[\n                    \"unimp_full_model_pipeline\",\n                    \"data_engred_unimp_full\",\n                    \"params:unimp_full_model_params\",\n                ],\n                outputs=\"data_predicted_full_taxi\",\n            ),\n            node(\n                func=train_unimp_full_baseline,\n                inputs=[\n                    \"data_engred_unimp_full\",\n                    \"params:unimp_full_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                ],\n                outputs=\"baseline_pipeline\",\n            ),\n            node(\n                func=predict_baseline,\n                inputs=[\n                    \"baseline_pipeline\",\n                    \"data_predicted_full_taxi\", # full_taxi because we want to add more preds to the dataframe\n                    \"params:unimp_full_model_params\",\n                ],\n                outputs=\"data_predicted_full_baseline\",\n            ),\n            node(\n                func=report_performance_metrics,\n                inputs=[\n                    \"data_predicted_full_baseline\",\n                    \"params:unimp_full_model_params\",\n                    \"active_run_id\",\n                ],\n                outputs=None,\n            ),\n            node(\n                func=visualization_caller,\n                inputs=[\n                    \"data_predicted_full_baseline\",\n                    \"baseline_pipeline\",\n                    \"params:unimp_full_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                ],\n                outputs=\"artifacts_ready\",\n            )\n            ,\n            node(\n                func=copy_artifacts_to_ntx,\n                inputs=[\n                    \"experiment_id\",\n                    \"active_run_id\",\n                    \"params:ntx_connection\",\n                    \"artifacts_ready\",\n                ],\n                outputs=None,\n            )\n        ]\n    )\n\n    unimp_ama_pipeline = Pipeline(\n        [\n            node(\n                func=init_mlflow,\n                inputs=\"params:unimp_ama_model_params\",\n                outputs=\"experiment_id\",\n            ),\n            node(\n                func=init_mlflow_run,\n                inputs=[\n                    \"params:unimp_ama_model_params\",\n                    \"experiment_id\",\n                ],\n                outputs=\"active_run_id\",\n            ),\n            node(\n                func=define_unimp_ama_model,\n                inputs=[\n                    \"data_engred_unimp_ama\",\n                    \"params:unimp_ama_model_params\",\n                    \"params:globals\",\n                    \"params:RANDOM_SEED\",\n                ],\n                outputs=\"untrained_unimp_ama_model_pipeline\"\n            ),\n            node(\n                func=optimize_unimp_ama_model,\n                inputs=[\n                    \"data_engred_unimp_ama\",\n                    \"params:unimp_ama_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                    \"params:RANDOM_SEED\",\n                    \"params:hypertune\",\n                    \"untrained_unimp_ama_model_pipeline\"\n                ],\n                outputs=[\n                    \"best_model_params\",\n                    \"best_untrained_unimp_ama_model_pipeline\",\n                ]\n            ),\n            node(\n                func=train_unimp_ama_model,\n                inputs=[\n                    \"data_engred_unimp_ama\",\n                    \"best_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                    \"params:RANDOM_SEED\",\n                    \"best_untrained_unimp_ama_model_pipeline\"\n                ],\n                outputs=\"unimp_ama_model_pipeline\",\n            ),\n            node(\n                func=predict,\n                inputs=[\n                    \"unimp_ama_model_pipeline\",\n                    \"data_engred_unimp_ama\",\n                    \"params:unimp_ama_model_params\",\n                ],\n                outputs=\"data_predicted_ama_taxi\",\n            ),\n            node(\n                func=train_unimp_ama_baseline,\n                inputs=[\n                    \"data_engred_unimp_ama\",\n                    \"params:unimp_ama_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                ],\n                outputs=\"baseline_pipeline\",\n            ),\n            node(\n                func=predict_baseline,\n                inputs=[\n                    \"baseline_pipeline\",\n                    \"data_predicted_ama_taxi\", \n                    \"params:unimp_ama_model_params\",\n                ],\n                outputs=\"data_predicted_ama_baseline\",\n            ),\n            node(\n                func=report_performance_metrics,\n                inputs=[\n                    \"data_predicted_ama_baseline\",\n                    \"params:unimp_ama_model_params\",\n                    \"active_run_id\",\n                ],\n                outputs=None,\n            ),\n            node(\n                func=visualization_caller,\n                inputs=[\n                    \"data_predicted_ama_baseline\",\n                    \"baseline_pipeline\",\n                    \"params:unimp_ama_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                ],\n                outputs=\"artifacts_ready\",\n            )\n            ,\n            node(\n                func=copy_artifacts_to_ntx,\n                inputs=[\n                    \"experiment_id\",\n                    \"active_run_id\",\n                    \"params:ntx_connection\",\n                    \"artifacts_ready\",\n                ],\n                outputs=None,\n            )\n             \n        ]\n    )\n\n\n    unimp_ramp_pipeline = Pipeline(\n        [\n            node(\n                func=init_mlflow,\n                inputs=\"params:unimp_ramp_model_params\",\n                outputs=\"experiment_id\",\n            ),\n            node(\n                func=init_mlflow_run,\n                inputs=[\n                    \"params:unimp_ramp_model_params\",\n                    \"experiment_id\",\n                ],\n                outputs=\"active_run_id\",\n            ),\n            node(\n                func=define_unimp_ramp_model,\n                inputs=[\n                    \"data_engred_unimp_ramp\",\n                    \"params:unimp_ramp_model_params\",\n                    \"params:globals\",\n                    \"params:RANDOM_SEED\",\n                ],\n                outputs=\"untrained_unimp_ramp_model_pipeline\"\n            ),\n            node(\n                func=optimize_unimp_ramp_model,\n                inputs=[\n                    \"data_engred_unimp_ramp\",\n                    \"params:unimp_ramp_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                    \"params:RANDOM_SEED\",\n                    \"params:hypertune\",\n                    \"untrained_unimp_ramp_model_pipeline\"\n                ],\n                outputs=[\n                    \"best_model_params\",\n                    \"best_untrained_unimp_ramp_model_pipeline\",\n                ]\n            ),\n            node(\n                func=train_unimp_ramp_model,\n                inputs=[\n                    \"data_engred_unimp_ramp\",\n                    \"best_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                    \"params:RANDOM_SEED\",\n                    \"best_untrained_unimp_ramp_model_pipeline\"\n                ],\n                outputs=\"unimp_ramp_model_pipeline\",\n            ),\n            node(\n                func=predict,\n                inputs=[\n                    \"unimp_ramp_model_pipeline\",\n                    \"data_engred_unimp_ramp\",\n                    \"params:unimp_ramp_model_params\",\n                ],\n                outputs=\"data_predicted_ramp_taxi\",\n            ),\n            node(\n                func=train_unimp_ramp_baseline,\n                inputs=[\n                    \"data_engred_unimp_ramp\",\n                    \"params:unimp_ramp_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                ],\n                outputs=\"baseline_pipeline\",\n            ),\n            node(\n                func=predict_baseline,\n                inputs=[\n                    \"baseline_pipeline\",\n                    \"data_predicted_ramp_taxi\", \n                    \"params:unimp_ramp_model_params\",\n                ],\n                outputs=\"data_predicted_ramp_baseline\",\n            ),\n            node(\n                func=report_performance_metrics,\n                inputs=[\n                    \"data_predicted_ramp_baseline\",\n                    \"params:unimp_ramp_model_params\",\n                    \"active_run_id\",\n                ],\n                outputs=None,\n            ),\n            node(\n                func=visualization_caller,\n                inputs=[\n                    \"data_predicted_ramp_baseline\",\n                    \"baseline_pipeline\",\n                    \"params:unimp_ramp_model_params\",\n                    \"params:globals\",\n                    \"active_run_id\",\n                ],\n                outputs=\"artifacts_ready\",\n            )\n            ,\n            node(\n                func=copy_artifacts_to_ntx,\n                inputs=[\n                    \"experiment_id\",\n                    \"active_run_id\",\n                    \"params:ntx_connection\",\n                    \"artifacts_ready\",\n                ],\n                outputs=None,\n            )             \n        ]\n    )\n\n    \n    return {\n        'unimp_full': unimp_full_pipeline,\n        'unimp_ama': unimp_ama_pipeline,\n        'unimp_ramp': unimp_ramp_pipeline        \n    }", "fn_id": 0, "class_fn": false, "repo": "nasa/ML-airport-taxi-out", "file": "src/taxi_out/pipelines/data_science/unimpeded_pipeline.py", "last_update_at": "2022-03-09T08:41:17+00:00", "question_id": "69e1416f770975500d2de2c84833e658c2dcc755_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_pipeline(**kwargs):\n    unimp_full_pipeline = Pipeline([node(func=init_mlflow, inputs='params:unimp_full_model_params', outputs='experiment_id'), node(func=init_mlflow_run, inputs=['params:unimp_full_model_params', 'experiment_id'], outputs='active_run_id'), node(func=define_unimp_full_model, inputs=['data_engred_unimp_full', 'params:unimp_full_model_params', 'params:globals', 'params:RANDOM_SEED'], outputs='untrained_unimp_full_model_pipeline'), node(func=optimize_unimp_full_model, inputs=['data_engred_unimp_full', 'params:unimp_full_model_params', 'params:globals', 'active_run_id', 'params:RANDOM_SEED', 'params:hypertune', 'untrained_unimp_full_model_pipeline'], outputs=['best_model_params', 'best_untrained_unimp_full_model_pipeline']), node(func=train_unimp_full_model, inputs=['data_engred_unimp_full', 'best_model_params', 'params:globals', 'active_run_id', 'params:RANDOM_SEED', 'best_untrained_unimp_full_model_pipeline'], outputs='unimp_full_model_pipeline'), node(func=predict, inputs=['unimp_full_model_pipeline', 'data_engred_unimp_full', 'params:unimp_full_model_params'], outputs='data_predicted_full_taxi'), node(func=train_unimp_full_baseline, inputs=['data_engred_unimp_full', 'params:unimp_full_model_params', 'params:globals', 'active_run_id'], outputs='baseline_pipeline'), node(func=predict_baseline, inputs=['baseline_pipeline', 'data_predicted_full_taxi', 'params:unimp_full_model_params'], outputs='data_predicted_full_baseline'), node(func=report_performance_metrics, inputs=['data_predicted_full_baseline', 'params:unimp_full_model_params', 'active_run_id'], outputs=None), node(func=visualization_caller, inputs=['data_predicted_full_baseline', 'baseline_pipeline', 'params:unimp_full_model_params', 'params:globals', 'active_run_id'], outputs='artifacts_ready'), node(func=copy_artifacts_to_ntx, inputs=['experiment_id', 'active_run_id', 'params:ntx_connection', 'artifacts_ready'], outputs=None)])\n    unimp_ama_pipeline = Pipeline([node(func=init_mlflow, inputs='params:unimp_ama_model_params', outputs='experiment_id'), node(func=init_mlflow_run, inputs=['params:unimp_ama_model_params', 'experiment_id'], outputs='active_run_id'), node(func=define_unimp_ama_model, inputs=['data_engred_unimp_ama', 'params:unimp_ama_model_params', 'params:globals', 'params:RANDOM_SEED'], outputs='untrained_unimp_ama_model_pipeline'), node(func=optimize_unimp_ama_model, inputs=['data_engred_unimp_ama', 'params:unimp_ama_model_params', 'params:globals', 'active_run_id', 'params:RANDOM_SEED', 'params:hypertune', 'untrained_unimp_ama_model_pipeline'], outputs=['best_model_params', 'best_untrained_unimp_ama_model_pipeline']), node(func=train_unimp_ama_model, inputs=['data_engred_unimp_ama', 'best_model_params', 'params:globals', 'active_run_id', 'params:RANDOM_SEED', 'best_untrained_unimp_ama_model_pipeline'], outputs='unimp_ama_model_pipeline'), node(func=predict, inputs=['unimp_ama_model_pipeline', 'data_engred_unimp_ama', 'params:unimp_ama_model_params'], outputs='data_predicted_ama_taxi'), node(func=train_unimp_ama_baseline, inputs=['data_engred_unimp_ama', 'params:unimp_ama_model_params', 'params:globals', 'active_run_id'], outputs='baseline_pipeline'), node(func=predict_baseline, inputs=['baseline_pipeline', 'data_predicted_ama_taxi', 'params:unimp_ama_model_params'], outputs='data_predicted_ama_baseline'), node(func=report_performance_metrics, inputs=['data_predicted_ama_baseline', 'params:unimp_ama_model_params', 'active_run_id'], outputs=None), node(func=visualization_caller, inputs=['data_predicted_ama_baseline', 'baseline_pipeline', 'params:unimp_ama_model_params', 'params:globals', 'active_run_id'], outputs='artifacts_ready'), node(func=copy_artifacts_to_ntx, inputs=['experiment_id', 'active_run_id', 'params:ntx_connection', 'artifacts_ready'], outputs=None)])\n    unimp_ramp_pipeline = Pipeline([node(func=init_mlflow, inputs='params:unimp_ramp_model_params', outputs='experiment_id'), node(func=init_mlflow_run, inputs=['params:unimp_ramp_model_params', 'experiment_id'], outputs='active_run_id'), node(func=define_unimp_ramp_model, inputs=['data_engred_unimp_ramp', 'params:unimp_ramp_model_params', 'params:globals', 'params:RANDOM_SEED'], outputs='untrained_unimp_ramp_model_pipeline'), node(func=optimize_unimp_ramp_model, inputs=['data_engred_unimp_ramp', 'params:unimp_ramp_model_params', 'params:globals', 'active_run_id', 'params:RANDOM_SEED', 'params:hypertune', 'untrained_unimp_ramp_model_pipeline'], outputs=['best_model_params', 'best_untrained_unimp_ramp_model_pipeline']), node(func=train_unimp_ramp_model, inputs=['data_engred_unimp_ramp', 'best_model_params', 'params:globals', 'active_run_id', 'params:RANDOM_SEED', 'best_untrained_unimp_ramp_model_pipeline'], outputs='unimp_ramp_model_pipeline'), node(func=predict, inputs=['unimp_ramp_model_pipeline', 'data_engred_unimp_ramp', 'params:unimp_ramp_model_params'], outputs='data_predicted_ramp_taxi'), node(func=train_unimp_ramp_baseline, inputs=['data_engred_unimp_ramp', 'params:unimp_ramp_model_params', 'params:globals', 'active_run_id'], outputs='baseline_pipeline'), node(func=predict_baseline, inputs=['baseline_pipeline', 'data_predicted_ramp_taxi', 'params:unimp_ramp_model_params'], outputs='data_predicted_ramp_baseline'), node(func=report_performance_metrics, inputs=['data_predicted_ramp_baseline', 'params:unimp_ramp_model_params', 'active_run_id'], outputs=None), node(func=visualization_caller, inputs=['data_predicted_ramp_baseline', 'baseline_pipeline', 'params:unimp_ramp_model_params', 'params:globals', 'active_run_id'], outputs='artifacts_ready'), node(func=copy_artifacts_to_ntx, inputs=['experiment_id', 'active_run_id', 'params:ntx_connection', 'artifacts_ready'], outputs=None)])\n"]]}
{"hexsha": "a6c79024ada304313c6b02d3d173236690737192", "ext": "py", "lang": "Python", "content": "def has_wireshark_env():\n    \"\"\"Test if wireshark GUI can run.\n\n    Returns:\n      (True, \"\") if wireshark can run in the environment.\n      (False, Error_String) otherwise.\n    \"\"\"\n    platform = os.uname()\n    print(platform)\n    if not platform:\n        return False, u\"Failed to get uname\"\n    if platform[0].lower() != u\"linux\":\n        return False, u\"Supported only on Linux\"\n\n    if not has_cmd(LINUX_BIN_WIRESHARK):\n        return False, u\"can\\'t find %s\" % LINUX_BIN_WIRESHARK\n\n    # All look good.\n    return True, u\"\"", "fn_id": 0, "class_fn": false, "repo": "sffc/fuchsia-clone", "file": "tools/devshell/contrib/sniff.py", "last_update_at": "2022-03-01T01:17:26+00:00", "question_id": "a6c79024ada304313c6b02d3d173236690737192_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def has_wireshark_env():\n    \"\"\"Test if wireshark GUI can run.\n\n    Returns:\n      (True, \"\") if wireshark can run in the environment.\n      (False, Error_String) otherwise.\n    \"\"\"\n    platform = os.uname()\n    print(platform)\n    if not platform:\n        return (False, u'Failed to get uname')\n    if platform[0].lower() != u'linux':\n        return (False, u'Supported only on Linux')\n    if not has_cmd(LINUX_BIN_WIRESHARK):\n        return (False, u\"can't find %s\" % LINUX_BIN_WIRESHARK)\n"]]}
{"hexsha": "86145ce6242f96d8541d77e28d22d17b889db316", "ext": "py", "lang": "Python", "content": "def profile_function(mod, dev, collectors, func_name=\"main\", warmup_iters=10):\n    \"\"\"Collect performance information of a function execution. Usually used with\n    a compiled PrimFunc.\n\n    This information can include performance counters like cache hits and FLOPs\n    that are useful in debugging performance issues of individual PrimFuncs.\n    Different metrics can be collected depending on which MetricCollector is\n    used.\n\n    Example\n    -------\n\n    .. code-block: python\n        f = tvm.build(my_func, target=\"llvm\", name=\"my_func\")\n        prof = tvm.runtime.profiling.profile_function(\n            f,\n            tvm.cpu(),\n            [tvm.runtime.profiling.PAPIMetricCollector({tvm.cpu(): [\"PAPI_FP_OPS\"]}),\n        )\n        counters = prof(*args)\n        print(counters)\n\n    Parameters\n    ----------\n    mod: Module\n        Module containing the function to profile.\n    dev: Device\n        Device to run the function on.\n\n    collectors: List[MetricCollector]\n        :py:class:`MetricCollector`s which will collect performance information.\n    func_name: str\n        Name of the function in `mod` to profile. Defaults to \"main\".\n    warmup_iters: int\n        Number of iterations to run the function before collecting performance\n        information. Recommended to set this larger than 0 for consistent cache\n        effects. Defaults to 10.\n\n    Returns\n    -------\n    prof: PackedFunc[args, Dict[str, ObjectRef]]\n        PackedFunc which takes the same arguments as the `mod[func_name]` and\n        returns performance metrics as a `Dict[str, ObjectRef]` where values\n        can be `CountNode`, `DurationNode`, `PercentNode`.\n    \"\"\"\n    return _ffi_api.ProfileFunction(\n        mod, func_name, dev.device_type, dev.device_id, warmup_iters, collectors\n    )", "fn_id": 0, "class_fn": false, "repo": "XiaoSong9905/tvm", "file": "python/tvm/runtime/profiling/__init__.py", "last_update_at": "2022-03-12T10:09:06+00:00", "question_id": "86145ce6242f96d8541d77e28d22d17b889db316_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def profile_function(mod, dev, collectors, func_name='main', warmup_iters=10):\n    \"\"\"Collect performance information of a function execution. Usually used with\n    a compiled PrimFunc.\n\n    This information can include performance counters like cache hits and FLOPs\n    that are useful in debugging performance issues of individual PrimFuncs.\n    Different metrics can be collected depending on which MetricCollector is\n    used.\n\n    Example\n    -------\n\n    .. code-block: python\n        f = tvm.build(my_func, target=\"llvm\", name=\"my_func\")\n        prof = tvm.runtime.profiling.profile_function(\n            f,\n            tvm.cpu(),\n            [tvm.runtime.profiling.PAPIMetricCollector({tvm.cpu(): [\"PAPI_FP_OPS\"]}),\n        )\n        counters = prof(*args)\n        print(counters)\n\n    Parameters\n    ----------\n    mod: Module\n        Module containing the function to profile.\n    dev: Device\n        Device to run the function on.\n\n    collectors: List[MetricCollector]\n        :py:class:`MetricCollector`s which will collect performance information.\n    func_name: str\n        Name of the function in `mod` to profile. Defaults to \"main\".\n    warmup_iters: int\n        Number of iterations to run the function before collecting performance\n        information. Recommended to set this larger than 0 for consistent cache\n        effects. Defaults to 10.\n\n    Returns\n    -------\n    prof: PackedFunc[args, Dict[str, ObjectRef]]\n        PackedFunc which takes the same arguments as the `mod[func_name]` and\n        returns performance metrics as a `Dict[str, ObjectRef]` where values\n        can be `CountNode`, `DurationNode`, `PercentNode`.\n    \"\"\"\n"]]}
{"hexsha": "43ecd69050205d1c564f3004cec59babbcd04602", "ext": "py", "lang": "Python", "content": "def wrapper_render_one_anno(dir_prefix, gameid, anno_id):\n    print('Running Scripts::Make_One_Annotation:wrapper_render_one_anno')\n    ### Load game\n    print ('Loading')\n    game_basename = gameid+'.pkl'\n\n    game_pkl = os.path.join(game_dir, game_basename)\n    with open(game_pkl,'rb') as f:\n        raw_data = pd.read_pickle(f)\n    game_str = \"{visitor}@{home}, on {date}\".format(\n        visitor=raw_data['events'][0]['visitor']['abbreviation'],\n        home=raw_data['events'][0]['home']['abbreviation'],\n        date=raw_data['gamedate']\n    )\n    print (game_str)\n\n\n    ### Create a new directory for videos\n    vid_dir =os.path.join(game_dir, 'video') # base dir that holds all the videos\n    if not os.path.exists(vid_dir):\n        os.makedirs(vid_dir)\n\n    new_dir = os.path.join(vid_dir, '{prefix}-{game_id}'.format(\n        prefix=dir_prefix,\n        game_id=game_basename.split('.')[0]\n    ))\n    previous_rendered_events = []\n    if not os.path.exists(new_dir):\n        os.makedirs(new_dir)\n    else: # already a directory exists, likely we've tried to do the same thing\n        print(new_dir)\n        print('Already exists, not rerunning events rendered and saved previously')\n\n    render_one_anno(\n        raw_data,\n        new_dir,\n        anno_id\n    )", "fn_id": 0, "class_fn": false, "repo": "dkStephanos/pnrRebuild", "file": "scripts/make_one_annotation.py", "last_update_at": "2022-02-09T23:39:36+00:00", "question_id": "43ecd69050205d1c564f3004cec59babbcd04602_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def wrapper_render_one_anno(dir_prefix, gameid, anno_id):\n    print('Running Scripts::Make_One_Annotation:wrapper_render_one_anno')\n    print('Loading')\n    game_basename = gameid + '.pkl'\n    game_pkl = os.path.join(game_dir, game_basename)\n    with open(game_pkl, 'rb') as f:\n        raw_data = pd.read_pickle(f)\n    game_str = '{visitor}@{home}, on {date}'.format(visitor=raw_data['events'][0]['visitor']['abbreviation'], home=raw_data['events'][0]['home']['abbreviation'], date=raw_data['gamedate'])\n    print(game_str)\n    vid_dir = os.path.join(game_dir, 'video')\n    if not os.path.exists(vid_dir):\n        os.makedirs(vid_dir)\n    new_dir = os.path.join(vid_dir, '{prefix}-{game_id}'.format(prefix=dir_prefix, game_id=game_basename.split('.')[0]))\n    previous_rendered_events = []\n    if not os.path.exists(new_dir):\n        os.makedirs(new_dir)\n    else:\n        print(new_dir)\n        print('Already exists, not rerunning events rendered and saved previously')\n"]]}
{"hexsha": "a2d49ebfc04c520a45f79375100a44409a55e260", "ext": "py", "lang": "Python", "content": "def compute_fairseq(ref_file, hyp_file, output_file):\n    # Get generate-tests\n    generate_test_path = os.path.join(os.path.dirname(hyp_file), \"generate-test.txt\")\n    if os.path.exists(generate_test_path):\n        # Read, parse and save lines\n        lines = [utils.read_file_lines(generate_test_path, autoclean=True)[-1]]\n        utils.write_file_lines(lines=lines, filename=output_file, insert_break_line=True)\n    else:\n        print(\"\\t- [INFO]: No 'generate-test.txt' was found.\")", "fn_id": 6, "class_fn": false, "repo": "salvacarrion/autonlp", "file": "autonmt/bundle/metrics.py", "last_update_at": "2022-01-14T01:02:52+00:00", "question_id": "a2d49ebfc04c520a45f79375100a44409a55e260_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def compute_fairseq(ref_file, hyp_file, output_file):\n    generate_test_path = os.path.join(os.path.dirname(hyp_file), 'generate-test.txt')\n    if os.path.exists(generate_test_path):\n        lines = [utils.read_file_lines(generate_test_path, autoclean=True)[-1]]\n        utils.write_file_lines(lines=lines, filename=output_file, insert_break_line=True)\n    else:\n"]]}
{"hexsha": "6a6ef29e9b45131efc43836479e4b4cc7e490cca", "ext": "py", "lang": "Python", "content": "@user_pait(\n    status=PaitStatus.release,\n    tag=(\"mock\",),\n    response_model_list=[UserSuccessRespModel2, FailRespModel],\n    enable_mock_response=True,\n)\ndef mock_route(\n    uid: int = Query.i(description=\"user id\", gt=10, lt=1000),\n    user_name: str = Query.i(description=\"user name\", min_length=2, max_length=4),\n    email: Optional[str] = Query.i(default=\"example@xxx.com\", description=\"user email\"),\n    multi_user_name: List[str] = MultiQuery.i(description=\"user name\", min_length=2, max_length=4),\n    age: int = Path.i(description=\"age\", gt=1, lt=100),\n    sex: SexEnum = Query.i(description=\"sex\"),\n) -> dict:\n    \"\"\"Test gen mock response\"\"\"\n    return {\n        \"code\": 0,\n        \"msg\": \"\",\n        \"data\": {\n            \"uid\": uid,\n            \"user_name\": user_name,\n            \"email\": email,\n            \"age\": age,\n            \"sex\": sex.value,\n            \"multi_user_name\": multi_user_name,\n        },\n    }", "fn_id": 7, "class_fn": false, "repo": "so1n/pait", "file": "example/param_verify/flask_example.py", "last_update_at": "2022-02-22T07:48:29+00:00", "question_id": "6a6ef29e9b45131efc43836479e4b4cc7e490cca_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@user_pait(status=PaitStatus.release, tag=('mock',), response_model_list=[UserSuccessRespModel2, FailRespModel], enable_mock_response=True)\ndef mock_route(uid: int=Query.i(description='user id', gt=10, lt=1000), user_name: str=Query.i(description='user name', min_length=2, max_length=4), email: Optional[str]=Query.i(default='example@xxx.com', description='user email'), multi_user_name: List[str]=MultiQuery.i(description='user name', min_length=2, max_length=4), age: int=Path.i(description='age', gt=1, lt=100), sex: SexEnum=Query.i(description='sex')) -> dict:\n    \"\"\"Test gen mock response\"\"\"\n"]]}
{"hexsha": "cbaf932aeb1cfe564a60a48222ac4dcaa380d543", "ext": "py", "lang": "Python", "content": "def read_scanline(band, yoff):\n    \"\"\" Read a band scanline (up to the y-offset), returning an array of values.\n\n    A raster (image) may consist of multiple bands (e.g. for a colour image, one may have a band for\n    red, green, blue, and alpha).\n    A scanline, is a single row of a band.\n\n    band, definition: https://gdal.org/user/raster_data_model.html#raster-band\n    fetching a raster band: https://gdal.org/tutorials/raster_api_tut.html#fetching-a-raster-band\n    \"\"\"\n    scanline = band.ReadRaster(xoff=0, yoff=yoff,\n                               xsize=band.XSize, ysize=1,\n                               buf_xsize=band.XSize, buf_ysize=1,\n                               buf_type=gdal.GDT_Float32)\n    return struct.unpack('f' * band.XSize, scanline)", "fn_id": 1, "class_fn": false, "repo": "bcgov/wps", "file": "api/app/c_haines/c_haines_index.py", "last_update_at": "2022-01-07T14:40:03+00:00", "question_id": "cbaf932aeb1cfe564a60a48222ac4dcaa380d543_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def read_scanline(band, yoff):\n    \"\"\" Read a band scanline (up to the y-offset), returning an array of values.\n\n    A raster (image) may consist of multiple bands (e.g. for a colour image, one may have a band for\n    red, green, blue, and alpha).\n    A scanline, is a single row of a band.\n\n    band, definition: https://gdal.org/user/raster_data_model.html#raster-band\n    fetching a raster band: https://gdal.org/tutorials/raster_api_tut.html#fetching-a-raster-band\n    \"\"\"\n    scanline = band.ReadRaster(xoff=0, yoff=yoff, xsize=band.XSize, ysize=1, buf_xsize=band.XSize, buf_ysize=1, buf_type=gdal.GDT_Float32)\n"]]}
{"hexsha": "0f1310d9efc0a4298e2fd3feb80cc1468f538147", "ext": "py", "lang": "Python", "content": "def _run(args, subset=None, append=None):\n    logging.info(\"Loading expressions\")\n    manager = FeatureMatrix.build_manager(args.expression_folder, filters = args.expression_filters, standardize=True, subset=subset)\n\n    logging.info(\"Saving\")\n    Utilities.ensure_requisite_folders(args.output)\n    manager.save_covariances(args.output, append=append)\n\n    logging.info(\"Ran.\")", "fn_id": 0, "class_fn": false, "repo": "adellanno/MetaXcan", "file": "software/BuildExpressionProduct.py", "last_update_at": "2022-03-28T17:02:39+00:00", "question_id": "0f1310d9efc0a4298e2fd3feb80cc1468f538147_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _run(args, subset=None, append=None):\n    logging.info('Loading expressions')\n    manager = FeatureMatrix.build_manager(args.expression_folder, filters=args.expression_filters, standardize=True, subset=subset)\n    logging.info('Saving')\n    Utilities.ensure_requisite_folders(args.output)\n    manager.save_covariances(args.output, append=append)\n"]]}
{"hexsha": "13090ccb72c3bc99b86692ede57bce6f59e560d8", "ext": "py", "lang": "Python", "content": "def test_disjuncts():\n    assert disjuncts(A | B | C) == {A, B, C}\n    assert disjuncts((A | B) & C) == {(A | B) & C}\n    assert disjuncts(A) == {A}\n    assert disjuncts(True) == {True}\n    assert disjuncts(False) == {False}", "fn_id": 18, "class_fn": false, "repo": "diofant/diofant", "file": "diofant/tests/logic/test_boolalg.py", "last_update_at": "2022-03-29T06:45:51+00:00", "question_id": "13090ccb72c3bc99b86692ede57bce6f59e560d8_18", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_disjuncts():\n    assert disjuncts(A | B | C) == {A, B, C}\n    assert disjuncts((A | B) & C) == {(A | B) & C}\n    assert disjuncts(A) == {A}\n    assert disjuncts(True) == {True}\n"]]}
{"hexsha": "7f0f344eccafaa0e224e4249c0f028bd8d1ed2a6", "ext": "py", "lang": "Python", "content": "def test_is_excluded_record():\n    # file\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"chapter_exclude_all/\",\n        rec_header_name=None,\n        to_exclude=[(\"chapter_exclude_all.md\", None)],\n    )\n    # file with multiple excluded\n    assert not ExcludeSearch.is_excluded_record(\n        rec_file_name=\"chapter_exclude_all/\",\n        rec_header_name=None,\n        to_exclude=[(\"chapter_exclude_all.md\", \"something.md\")],\n    )\n    # file + header (not specifically excluded)\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"chapter_exclude_all/\",\n        rec_header_name=\"header-chapter_exclude_all-aex\",\n        to_exclude=[(\"chapter_exclude_all.md\", None)],\n    )\n    # file + header (specifically excluded)\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"chapter_exclude_all/\",\n        rec_header_name=\"header-chapter_exclude_all-aex\",\n        to_exclude=[(\"chapter_exclude_all.md\", \"header-chapter_exclude_all-aex\")],\n    )\n    # file in dir\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"dir/dir_chapter_exclude_all/\",\n        rec_header_name=None,\n        to_exclude=[(\"dir/dir_chapter_exclude_all.md\", None)],\n    )\n    # all dir\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"all_dir/some-chapter/\",\n        rec_header_name=None,\n        to_exclude=[(\"all_dir/*\", None)],\n    )\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"all_dir/some-chapter/\",\n        rec_header_name=None,\n        to_exclude=[(\"all_dir/*\", None)],\n    )\n    # all dir + header\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"all_dir/some-chapter/\",\n        rec_header_name=\"all_dir/some-chapter-aex\",\n        to_exclude=[(\"all_dir/*\", None)],\n    )\n    # all subdir\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"all_dir_sub/all_dir_sub2/some-chapter/\",\n        rec_header_name=None,\n        to_exclude=[(\"all_dir_sub/all_dir_sub2/*\", None)],\n    )\n    # all subdir + header\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"all_dir_sub/all_dir_sub2/some-chapter/\",\n        rec_header_name=\"alldir-header-all_dir_sub2-aex\",\n        to_exclude=[(\"all_dir_sub/all_dir_sub2/*\", None)],\n    )\n    # file within subdir wildcard\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"all_dir_sub/all_dir_sub2/all_dir_sub2_1/\",\n        rec_header_name=None,\n        to_exclude=[(\"all_dir_sub/*/all_dir_sub2_1.md\", None)],\n    )\n    # file within multiple subdir wildcard\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"all_dir_sub/all_dir_sub2/all_dir_sub2_again/all_dir_sub2_1/\",\n        rec_header_name=None,\n        to_exclude=[(\"all_dir_sub/*/all_dir_sub2_1.md\", None)],\n    )\n    # file within multiple subdir wildcard + header\n    assert ExcludeSearch.is_excluded_record(\n        rec_file_name=\"all_dir_sub/all_dir_sub2/all_dir_sub2_again/all_dir_sub2_1/\",\n        rec_header_name=\"alldir-header-all_dir_sub2-aex\",\n        to_exclude=[\n            (\"all_dir_sub/*/all_dir_sub2_1.md\", \"alldir-header-all_dir_sub2-aex\")\n        ],\n    )", "fn_id": 4, "class_fn": false, "repo": "timmeinerzhagen/mkdocs-exclude-search", "file": "tests/test_plugin.py", "last_update_at": "2022-03-27T14:00:31+00:00", "question_id": "7f0f344eccafaa0e224e4249c0f028bd8d1ed2a6_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_is_excluded_record():\n    assert ExcludeSearch.is_excluded_record(rec_file_name='chapter_exclude_all/', rec_header_name=None, to_exclude=[('chapter_exclude_all.md', None)])\n    assert not ExcludeSearch.is_excluded_record(rec_file_name='chapter_exclude_all/', rec_header_name=None, to_exclude=[('chapter_exclude_all.md', 'something.md')])\n    assert ExcludeSearch.is_excluded_record(rec_file_name='chapter_exclude_all/', rec_header_name='header-chapter_exclude_all-aex', to_exclude=[('chapter_exclude_all.md', None)])\n    assert ExcludeSearch.is_excluded_record(rec_file_name='chapter_exclude_all/', rec_header_name='header-chapter_exclude_all-aex', to_exclude=[('chapter_exclude_all.md', 'header-chapter_exclude_all-aex')])\n    assert ExcludeSearch.is_excluded_record(rec_file_name='dir/dir_chapter_exclude_all/', rec_header_name=None, to_exclude=[('dir/dir_chapter_exclude_all.md', None)])\n    assert ExcludeSearch.is_excluded_record(rec_file_name='all_dir/some-chapter/', rec_header_name=None, to_exclude=[('all_dir/*', None)])\n    assert ExcludeSearch.is_excluded_record(rec_file_name='all_dir/some-chapter/', rec_header_name=None, to_exclude=[('all_dir/*', None)])\n    assert ExcludeSearch.is_excluded_record(rec_file_name='all_dir/some-chapter/', rec_header_name='all_dir/some-chapter-aex', to_exclude=[('all_dir/*', None)])\n    assert ExcludeSearch.is_excluded_record(rec_file_name='all_dir_sub/all_dir_sub2/some-chapter/', rec_header_name=None, to_exclude=[('all_dir_sub/all_dir_sub2/*', None)])\n    assert ExcludeSearch.is_excluded_record(rec_file_name='all_dir_sub/all_dir_sub2/some-chapter/', rec_header_name='alldir-header-all_dir_sub2-aex', to_exclude=[('all_dir_sub/all_dir_sub2/*', None)])\n    assert ExcludeSearch.is_excluded_record(rec_file_name='all_dir_sub/all_dir_sub2/all_dir_sub2_1/', rec_header_name=None, to_exclude=[('all_dir_sub/*/all_dir_sub2_1.md', None)])\n    assert ExcludeSearch.is_excluded_record(rec_file_name='all_dir_sub/all_dir_sub2/all_dir_sub2_again/all_dir_sub2_1/', rec_header_name=None, to_exclude=[('all_dir_sub/*/all_dir_sub2_1.md', None)])\n"]]}
{"hexsha": "5f4c2151b453c39162249241cade10ded72b6d5a", "ext": "py", "lang": "Python", "content": "def span_str(start = None, end = None):\n  assert(start is not None or end is not None)\n  if start is None:\n    return ' '  + str(end) + ')'\n  elif end is None:\n    return '(' + str(start) + ' '\n  else:\n    return ' (' + str(start) + ' ' + str(end) + ') '    ", "fn_id": 8, "class_fn": false, "repo": "brightp-py/li-rnng", "file": "utils.py", "last_update_at": "2022-03-07T02:29:26+00:00", "question_id": "5f4c2151b453c39162249241cade10ded72b6d5a_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def span_str(start=None, end=None):\n    assert start is not None or end is not None\n    if start is None:\n        return ' ' + str(end) + ')'\n    elif end is None:\n        return '(' + str(start) + ' '\n    else:\n"]]}
{"hexsha": "28fbfae961021e5d283c9a3c8c8f91cda4973696", "ext": "py", "lang": "Python", "content": "def preprocess_image(img,args):\n    \"\"\"\n        Processes image for input\n    \"\"\"\n    \n    composed_transforms = transforms.Compose([        \n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n    im_as_var = composed_transforms(img)\n    im_as_var = Variable(im_as_var.unsqueeze(0)).cuda().requires_grad_()\n    return im_as_var", "fn_id": 4, "class_fn": false, "repo": "YonghaoXu/UAE-RS", "file": "segmentation/utils/tools.py", "last_update_at": "2022-02-28T09:38:30+00:00", "question_id": "28fbfae961021e5d283c9a3c8c8f91cda4973696_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def preprocess_image(img, args):\n    \"\"\"\n        Processes image for input\n    \"\"\"\n    composed_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n    im_as_var = composed_transforms(img)\n    im_as_var = Variable(im_as_var.unsqueeze(0)).cuda().requires_grad_()\n"]]}
{"hexsha": "e6f6035a38d2a762a1a22dcfebc8a53bf471da7c", "ext": "py", "lang": "Python", "content": "def main(word):\n    word_mat = generate_sparse_mat(word.upper())\n    counter = 0\n    # Change date to first pixel of contribution chart\n    for row in word_mat:\n        row_str = \"\"\n        for val in row:\n            commit_date = FIRST_CONTRIB_DATE + datetime.timedelta(days=counter)\n            row_str = f\"{row_str} {commit_date}---{val}\"\n            execute_cmd(f\"\"\"git commit --allow-empty -m \"EMPTY COMMIT\" --date=\"{commit_date}\" \"\"\", val)\n            counter = counter + 1", "fn_id": 0, "class_fn": false, "repo": "steven-mi/github-contribution-text-drawer", "file": "main.py", "last_update_at": "2022-01-31T05:45:55+00:00", "question_id": "e6f6035a38d2a762a1a22dcfebc8a53bf471da7c_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main(word):\n    word_mat = generate_sparse_mat(word.upper())\n    counter = 0\n    for row in word_mat:\n        row_str = ''\n        for val in row:\n            commit_date = FIRST_CONTRIB_DATE + datetime.timedelta(days=counter)\n            row_str = f'{row_str} {commit_date}---{val}'\n            execute_cmd(f'git commit --allow-empty -m \"EMPTY COMMIT\" --date=\"{commit_date}\" ', val)\n"]]}
{"hexsha": "8e53c3c74c80b4de0ef275553982b145b9cda73c", "ext": "py", "lang": "Python", "content": "def get_caller_module_path(depth: int = 2) -> str:\n    \"\"\"Get the caller module path.\n\n    We use sys._getframe instead of inspect.stack(0) because the latter is way slower, since it iterates over\n    all the frames in the stack.\n    \"\"\"\n    frame = _getframe(depth)\n    return getframeinfo(frame, context=0).filename", "fn_id": 2, "class_fn": false, "repo": "therve/pytest-bdd", "file": "pytest_bdd/utils.py", "last_update_at": "2022-03-24T11:46:34+00:00", "question_id": "8e53c3c74c80b4de0ef275553982b145b9cda73c_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_caller_module_path(depth: int=2) -> str:\n    \"\"\"Get the caller module path.\n\n    We use sys._getframe instead of inspect.stack(0) because the latter is way slower, since it iterates over\n    all the frames in the stack.\n    \"\"\"\n    frame = _getframe(depth)\n"]]}
{"hexsha": "e1b31044e433b2058dc06849f93ff064a71f0653", "ext": "py", "lang": "Python", "content": "def serial_triple_dot_coulomb_symmetry_spin():\n    # Coulomb matrix elements\n    # Intradot terms: u, uex\n    # Interdot terms: un, udc, usc\n    u, uex, un, udc, usc = 10., 2., 3., -0.5, -0.2\n\n    # ----------- Spinless -----------\n    nsingle = 5\n    dotindex = [0, 0, 1, 1, 2]\n    # m, n, k, l\n    coulomb = []\n    for m, n, k, l in itertools.product(range(nsingle), repeat=4):\n        if m == n == k == l:\n            coulomb.append([m,m,m,m,u/2]) # Direct\n        if dotindex[m] == dotindex[k]:\n            if m == n and k == l and m != k:\n                coulomb.append([m,m,k,k,uex/2]) # Exchange\n        if m != n and k != l:\n            # Intradot\n            if dotindex[m] == dotindex[n]:\n                if m == l and n == k: coulomb.append([m,n,n,m,u/2])   # Direct\n                if m == k and n == l: coulomb.append([m,n,m,n,uex/2]) # Exchange\n            # Interdot\n            # Note that the pairs (n,k) and (m,l) are located at different dots\n            if (dotindex[m] == dotindex[l] and\n                dotindex[n] == dotindex[k] and\n                abs(dotindex[m]-dotindex[n]) == 1):\n                if m == l and n == k:\n                    coulomb.append([m,n,n,m,un/2])   # Direct\n                if n == k and m != l:\n                    sgn = dotindex[m]-dotindex[n]\n                    coulomb.append([m,n,n,l,udc/2*sgn])  # Charge-dipole\n                if n != k and m == l:\n                    sgn = dotindex[n]-dotindex[m]\n                    coulomb.append([m,n,k,m,udc/2*sgn])  # Charge-dipole\n                if n != k and m != l:\n                    coulomb.append([m,n,k,l,usc/2])  # Charge-quadrupole\n    coulomb0 = coulomb\n\n    coulomb1 = {(0,0,0,0):u, (1,1,1,1):u, (2,2,2,2):u, (3,3,3,3):u, (4,4,4,4):u,\n                (0,1,1,0):u, (2,3,3,2):u,\n                #\n                (0,0,1,1):uex, (2,2,3,3):uex,\n                (0,1,0,1):uex, (2,3,2,3):uex,\n                #\n                (0,2,2,0):un, (0,3,3,0):un,\n                (1,2,2,1):un, (1,3,3,1):un,\n                (2,4,4,2):un, (3,4,4,3):un,\n                #\n                (0,2,2,1):-udc, (0,3,3,1):-udc, (2,4,4,3):-udc,\n                (0,2,3,0):+udc, (1,2,3,1):+udc,\n                #\n                (0,2,3,1):usc, (0,3,2,1):usc,\n                # Conjugated terms\n                (1,1,0,0):uex, (3,3,2,2):uex,\n                #\n                (1,2,2,0):-udc, (1,3,3,0):-udc, (3,4,4,2):-udc,\n                (0,3,2,0):+udc, (1,3,2,1):+udc,\n                #\n                (1,3,2,0):usc, (1,2,3,0):usc\n                }\n\n    coulomb2 = {(0,0,0,0):u, (1,1,1,1):u, (2,2,2,2):u, (3,3,3,3):u, (4,4,4,4):u,\n                (0,1,1,0):u, (2,3,3,2):u,\n                #\n                (0,0,1,1):uex, (2,2,3,3):uex,\n                (0,1,0,1):uex, (2,3,2,3):uex,\n                #\n                (0,2,2,0):un, (0,3,3,0):un,\n                (1,2,2,1):un, (1,3,3,1):un,\n                (2,4,4,2):un, (3,4,4,3):un,\n                #\n                (0,2,2,1):-udc, (0,3,3,1):-udc, (2,4,4,3):-udc,\n                (0,2,3,0):+udc, (1,2,3,1):+udc,\n                #\n                (0,2,3,1):usc, (0,3,2,1):usc\n                }\n\n    sys0_spinless = qmeq.Builder(nsingle=5, coulomb=coulomb0, symmetry=None, m_less_n=False, herm_c=False)\n    sys0_spinless.solve(masterq=False)\n    sys1_spinless = qmeq.Builder(nsingle=5, coulomb=coulomb1, symmetry=None, m_less_n=True, herm_c=False)\n    sys1_spinless.solve(masterq=False)\n    sys2_spinless = qmeq.Builder(nsingle=5, coulomb=coulomb2, symmetry=None, m_less_n=True, herm_c=True)\n    sys2_spinless.solve(masterq=False)\n\n    assert sum(abs(sys1_spinless.Ea-sys0_spinless.Ea)) < EPS\n    assert sum(abs(sys2_spinless.Ea-sys0_spinless.Ea)) < EPS\n\n    # ----------- Spinful -----------\n    nsingle = 10\n    nssl = nsingle//2\n    dotindex = [0, 0, 1, 1, 2, 0, 0, 1, 1, 2]\n    # m, n, k, l\n    coulomb = []\n    for m, n, k, l in itertools.product(range(nsingle), repeat=4):\n        if m != n and k != l and m//nssl == l//nssl and n//nssl == k//nssl:\n            # Intradot\n            if dotindex[m] == dotindex[n]:\n                if m == l and n == k: coulomb.append([m,n,n,m,u/2]) # Direct\n                if m == k and n == l:\n                    coulomb.append([m,n,m,n,uex/2]) # Exchange\n                    if m+nssl < nsingle:\n                        coulomb.append([m,n+nssl,m+nssl,n,uex/2])\n                        coulomb.append([m+nssl,n,m,n+nssl,uex/2])\n                        coulomb.append([m,m+nssl,n+nssl,n,uex/2])\n                        coulomb.append([m+nssl,m,n,n+nssl,uex/2])\n            # Interdot\n            # Note that the pairs (n,k) and (m,l) are located at different dots\n            if (dotindex[m] == dotindex[l] and\n                dotindex[n] == dotindex[k] and\n                abs(dotindex[m]-dotindex[n]) == 1):\n                if m == l and n == k:\n                    coulomb.append([m,n,n,m,un/2])   # Direct\n                if n == k and m != l:\n                    sgn = dotindex[m]-dotindex[n]\n                    coulomb.append([m,n,n,l,udc/2*sgn])  # Charge-dipole\n                if n != k and m == l:\n                    sgn = dotindex[n]-dotindex[m]\n                    coulomb.append([m,n,k,m,udc/2*sgn])  # Charge-dipole\n                if n != k and m != l:\n                    coulomb.append([m,n,k,l,usc/2])  # Charge-quadrupole\n\n    indexing='ssq'\n    sys_ref_spinful = qmeq.Builder(nsingle=10, coulomb=coulomb, indexing=indexing)\n    sys_ref_spinful.solve(masterq=False)\n    sys0_spinful = qmeq.Builder(nsingle=10, coulomb=coulomb0, symmetry='spin', m_less_n=False, indexing=indexing)\n    sys0_spinful.solve(masterq=False)\n    sys1_spinful = qmeq.Builder(nsingle=10, coulomb=coulomb1, symmetry='spin', m_less_n=True, herm_c=False, indexing=indexing)\n    sys1_spinful.solve(masterq=False)\n    sys2_spinful = qmeq.Builder(nsingle=10, coulomb=coulomb2, symmetry='spin', m_less_n=True, herm_c=True, indexing=indexing)\n    sys2_spinful.solve(masterq=False)\n\n    assert sum(abs(sys0_spinful.Ea-sys_ref_spinful.Ea)) < 10*EPS\n    assert sum(abs(sys1_spinful.Ea-sys_ref_spinful.Ea)) < 10*EPS\n    assert sum(abs(sys2_spinful.Ea-sys_ref_spinful.Ea)) < 10*EPS", "fn_id": 6, "class_fn": false, "repo": "gedaskir/test-deploy-qmeq", "file": "qmeq/tests/test_builder.py", "last_update_at": "2022-02-23T08:01:45+00:00", "question_id": "e1b31044e433b2058dc06849f93ff064a71f0653_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def serial_triple_dot_coulomb_symmetry_spin():\n    u, uex, un, udc, usc = (10.0, 2.0, 3.0, -0.5, -0.2)\n    nsingle = 5\n    dotindex = [0, 0, 1, 1, 2]\n    coulomb = []\n    for m, n, k, l in itertools.product(range(nsingle), repeat=4):\n        if m == n == k == l:\n            coulomb.append([m, m, m, m, u / 2])\n        if dotindex[m] == dotindex[k]:\n            if m == n and k == l and (m != k):\n                coulomb.append([m, m, k, k, uex / 2])\n        if m != n and k != l:\n            if dotindex[m] == dotindex[n]:\n                if m == l and n == k:\n                    coulomb.append([m, n, n, m, u / 2])\n                if m == k and n == l:\n                    coulomb.append([m, n, m, n, uex / 2])\n            if dotindex[m] == dotindex[l] and dotindex[n] == dotindex[k] and (abs(dotindex[m] - dotindex[n]) == 1):\n                if m == l and n == k:\n                    coulomb.append([m, n, n, m, un / 2])\n                if n == k and m != l:\n                    sgn = dotindex[m] - dotindex[n]\n                    coulomb.append([m, n, n, l, udc / 2 * sgn])\n                if n != k and m == l:\n                    sgn = dotindex[n] - dotindex[m]\n                    coulomb.append([m, n, k, m, udc / 2 * sgn])\n                if n != k and m != l:\n                    coulomb.append([m, n, k, l, usc / 2])\n    coulomb0 = coulomb\n    coulomb1 = {(0, 0, 0, 0): u, (1, 1, 1, 1): u, (2, 2, 2, 2): u, (3, 3, 3, 3): u, (4, 4, 4, 4): u, (0, 1, 1, 0): u, (2, 3, 3, 2): u, (0, 0, 1, 1): uex, (2, 2, 3, 3): uex, (0, 1, 0, 1): uex, (2, 3, 2, 3): uex, (0, 2, 2, 0): un, (0, 3, 3, 0): un, (1, 2, 2, 1): un, (1, 3, 3, 1): un, (2, 4, 4, 2): un, (3, 4, 4, 3): un, (0, 2, 2, 1): -udc, (0, 3, 3, 1): -udc, (2, 4, 4, 3): -udc, (0, 2, 3, 0): +udc, (1, 2, 3, 1): +udc, (0, 2, 3, 1): usc, (0, 3, 2, 1): usc, (1, 1, 0, 0): uex, (3, 3, 2, 2): uex, (1, 2, 2, 0): -udc, (1, 3, 3, 0): -udc, (3, 4, 4, 2): -udc, (0, 3, 2, 0): +udc, (1, 3, 2, 1): +udc, (1, 3, 2, 0): usc, (1, 2, 3, 0): usc}\n    coulomb2 = {(0, 0, 0, 0): u, (1, 1, 1, 1): u, (2, 2, 2, 2): u, (3, 3, 3, 3): u, (4, 4, 4, 4): u, (0, 1, 1, 0): u, (2, 3, 3, 2): u, (0, 0, 1, 1): uex, (2, 2, 3, 3): uex, (0, 1, 0, 1): uex, (2, 3, 2, 3): uex, (0, 2, 2, 0): un, (0, 3, 3, 0): un, (1, 2, 2, 1): un, (1, 3, 3, 1): un, (2, 4, 4, 2): un, (3, 4, 4, 3): un, (0, 2, 2, 1): -udc, (0, 3, 3, 1): -udc, (2, 4, 4, 3): -udc, (0, 2, 3, 0): +udc, (1, 2, 3, 1): +udc, (0, 2, 3, 1): usc, (0, 3, 2, 1): usc}\n    sys0_spinless = qmeq.Builder(nsingle=5, coulomb=coulomb0, symmetry=None, m_less_n=False, herm_c=False)\n    sys0_spinless.solve(masterq=False)\n    sys1_spinless = qmeq.Builder(nsingle=5, coulomb=coulomb1, symmetry=None, m_less_n=True, herm_c=False)\n    sys1_spinless.solve(masterq=False)\n    sys2_spinless = qmeq.Builder(nsingle=5, coulomb=coulomb2, symmetry=None, m_less_n=True, herm_c=True)\n    sys2_spinless.solve(masterq=False)\n    assert sum(abs(sys1_spinless.Ea - sys0_spinless.Ea)) < EPS\n    assert sum(abs(sys2_spinless.Ea - sys0_spinless.Ea)) < EPS\n    nsingle = 10\n    nssl = nsingle // 2\n    dotindex = [0, 0, 1, 1, 2, 0, 0, 1, 1, 2]\n    coulomb = []\n    for m, n, k, l in itertools.product(range(nsingle), repeat=4):\n        if m != n and k != l and (m // nssl == l // nssl) and (n // nssl == k // nssl):\n            if dotindex[m] == dotindex[n]:\n                if m == l and n == k:\n                    coulomb.append([m, n, n, m, u / 2])\n                if m == k and n == l:\n                    coulomb.append([m, n, m, n, uex / 2])\n                    if m + nssl < nsingle:\n                        coulomb.append([m, n + nssl, m + nssl, n, uex / 2])\n                        coulomb.append([m + nssl, n, m, n + nssl, uex / 2])\n                        coulomb.append([m, m + nssl, n + nssl, n, uex / 2])\n                        coulomb.append([m + nssl, m, n, n + nssl, uex / 2])\n            if dotindex[m] == dotindex[l] and dotindex[n] == dotindex[k] and (abs(dotindex[m] - dotindex[n]) == 1):\n                if m == l and n == k:\n                    coulomb.append([m, n, n, m, un / 2])\n                if n == k and m != l:\n                    sgn = dotindex[m] - dotindex[n]\n                    coulomb.append([m, n, n, l, udc / 2 * sgn])\n                if n != k and m == l:\n                    sgn = dotindex[n] - dotindex[m]\n                    coulomb.append([m, n, k, m, udc / 2 * sgn])\n                if n != k and m != l:\n                    coulomb.append([m, n, k, l, usc / 2])\n    indexing = 'ssq'\n    sys_ref_spinful = qmeq.Builder(nsingle=10, coulomb=coulomb, indexing=indexing)\n    sys_ref_spinful.solve(masterq=False)\n    sys0_spinful = qmeq.Builder(nsingle=10, coulomb=coulomb0, symmetry='spin', m_less_n=False, indexing=indexing)\n    sys0_spinful.solve(masterq=False)\n    sys1_spinful = qmeq.Builder(nsingle=10, coulomb=coulomb1, symmetry='spin', m_less_n=True, herm_c=False, indexing=indexing)\n    sys1_spinful.solve(masterq=False)\n    sys2_spinful = qmeq.Builder(nsingle=10, coulomb=coulomb2, symmetry='spin', m_less_n=True, herm_c=True, indexing=indexing)\n    sys2_spinful.solve(masterq=False)\n    assert sum(abs(sys0_spinful.Ea - sys_ref_spinful.Ea)) < 10 * EPS\n    assert sum(abs(sys1_spinful.Ea - sys_ref_spinful.Ea)) < 10 * EPS\n"]]}
{"hexsha": "94bcdb86ef6b1e1256808a02f4d67adf2208cdea", "ext": "py", "lang": "Python", "content": "def _compare_traces(expected: Trace, received: Trace, ignored: Set[str]) -> None:\n    \"\"\"Compare two traces for differences.\n\n    The given traces are assumed to be in BFS order.\n    \"\"\"\n    if len(received) > len(expected):\n        names = [\"'%s'\" % s[\"name\"] for s in received[len(expected) - len(received) :]]\n        raise AssertionError(\n            f\"Received more spans ({len(received)}) than expected ({len(expected)}). Received unmatched spans: {', '.join(names)}\"\n        )\n    elif len(expected) > len(received):\n        names = [\"'%s'\" % s[\"name\"] for s in expected[len(received) - len(expected) :]]\n        raise AssertionError(\n            f\"Received fewer spans ({len(received)}) than expected ({len(expected)}). Expected unmatched spans: {', '.join(names)}\"\n        )\n\n    for s_exp, s_rec in zip(expected, received):\n        with CheckTrace.add_frame(\n            f\"snapshot compare of span '{s_exp['name']}' at position {s_exp['span_id']} in trace\"\n        ) as frame:\n            frame.add_item(f\"Expected span:\\n{pprint.pformat(s_exp)}\")\n            frame.add_item(f\"Received span:\\n{pprint.pformat(s_rec)}\")\n            top_level_diffs, meta_diffs, metrics_diffs = _diff_spans(\n                s_exp, s_rec, ignored\n            )\n\n            for diffs, diff_type, d_exp, d_rec in [\n                (top_level_diffs, \"span\", s_exp, s_rec),\n                (meta_diffs, \"meta\", s_exp[\"meta\"], s_rec[\"meta\"]),\n                (metrics_diffs, \"metrics\", s_exp[\"metrics\"], s_rec[\"metrics\"]),\n            ]:\n                for diff_key in diffs:\n                    if diff_key not in d_exp:\n                        raise AssertionError(\n                            f\"Span{' ' + diff_type if diff_type != 'span' else ''} value '{diff_key}' in received span but is not in the expected span.\"\n                        )\n                    elif diff_key not in d_rec:\n                        raise AssertionError(\n                            f\"Span{' ' + diff_type if diff_type != 'span' else ''} value '{diff_key}' in expected span but is not in the received span.\"\n                        )\n                    else:\n                        raise AssertionError(\n                            f\"{diff_type} mismatch on '{diff_key}': got '{d_rec[diff_key]}' which does not match expected '{d_exp[diff_key]}'.\"\n                        )", "fn_id": 7, "class_fn": false, "repo": "DataDog/dd-apm-test-agent", "file": "ddapm_test_agent/trace_snapshot.py", "last_update_at": "2022-02-10T19:00:45+00:00", "question_id": "94bcdb86ef6b1e1256808a02f4d67adf2208cdea_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _compare_traces(expected: Trace, received: Trace, ignored: Set[str]) -> None:\n    \"\"\"Compare two traces for differences.\n\n    The given traces are assumed to be in BFS order.\n    \"\"\"\n    if len(received) > len(expected):\n        names = [\"'%s'\" % s['name'] for s in received[len(expected) - len(received):]]\n        raise AssertionError(f\"Received more spans ({len(received)}) than expected ({len(expected)}). Received unmatched spans: {', '.join(names)}\")\n    elif len(expected) > len(received):\n        names = [\"'%s'\" % s['name'] for s in expected[len(received) - len(expected):]]\n        raise AssertionError(f\"Received fewer spans ({len(received)}) than expected ({len(expected)}). Expected unmatched spans: {', '.join(names)}\")\n    for s_exp, s_rec in zip(expected, received):\n        with CheckTrace.add_frame(f\"snapshot compare of span '{s_exp['name']}' at position {s_exp['span_id']} in trace\") as frame:\n            frame.add_item(f'Expected span:\\n{pprint.pformat(s_exp)}')\n            frame.add_item(f'Received span:\\n{pprint.pformat(s_rec)}')\n            top_level_diffs, meta_diffs, metrics_diffs = _diff_spans(s_exp, s_rec, ignored)\n            for diffs, diff_type, d_exp, d_rec in [(top_level_diffs, 'span', s_exp, s_rec), (meta_diffs, 'meta', s_exp['meta'], s_rec['meta']), (metrics_diffs, 'metrics', s_exp['metrics'], s_rec['metrics'])]:\n                for diff_key in diffs:\n                    if diff_key not in d_exp:\n                        raise AssertionError(f\"Span{(' ' + diff_type if diff_type != 'span' else '')} value '{diff_key}' in received span but is not in the expected span.\")\n                    elif diff_key not in d_rec:\n                        raise AssertionError(f\"Span{(' ' + diff_type if diff_type != 'span' else '')} value '{diff_key}' in expected span but is not in the received span.\")\n                    else:\n"]]}
{"hexsha": "fc9ed8e057689b4236908e0a26a6917db08253e9", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize('per_test_flask_app', ['dirbs_poweruser_login', 'dirbs_api_user', 'dirbs_catalog_user'],\n                         indirect=True)\ndef test_imei_api(per_test_flask_app, per_test_postgres, logger, mocked_statsd, tmpdir, request, mocked_config,\n                  api_version):\n    \"\"\"Test IMEI API call works with the security role created based on abstract role.\"\"\"\n    dsn = per_test_postgres.dsn()\n    db_config = DBConfig(ignore_env=True, **dsn)\n    with create_db_connection(db_config) as conn, \\\n            create_db_connection(db_config, autocommit=True) as metadata_conn:\n        with get_importer(OperatorDataImporter,\n                          conn,\n                          metadata_conn,\n                          db_config,\n                          tmpdir,\n                          logger,\n                          mocked_statsd,\n                          OperatorDataParams(\n                              filename='testData1-operator-operator1-anonymized_20161101_20161130.csv',\n                              operator='operator1',\n                              perform_unclean_checks=False,\n                              extract=False)) as imp:\n            imp.import_data()\n\n    current_user = request.node.callspec.params['per_test_flask_app']\n\n    if api_version == 'v1':\n        if current_user in ['dirbs_poweruser_login', 'dirbs_api_user']:\n            rv = per_test_flask_app.get(url_for('{0}.imei_api'.format(api_version),\n                                                imei='388260336982806', include_seen_with=1))\n            assert rv.status_code == 200\n            assert json.loads(rv.data.decode('utf-8'))['seen_with'] == \\\n                                                      [{'imsi': '11101400135251', 'msisdn': '22300825684694'},\n                                                       {'imsi': '11101400135252', 'msisdn': '22300825684692'}]\n            assert json.loads(rv.data.decode('utf-8'))['realtime_checks']['ever_observed_on_network'] is True\n\n        else:\n            with pytest.raises(DatabaseRoleCheckException):\n                per_test_flask_app.get(url_for('{0}.imei_api'.format(api_version),\n                                               imei='388260336982806', include_seen_with=1))\n    else:  # api version 2.0\n        if current_user in ['dirbs_poweruser_login', 'dirbs_api_user']:\n            rv = per_test_flask_app.get(url_for('{0}.imei_get_subscribers_api'.format(api_version),\n                                                imei='388260336982806'))\n            assert rv.status_code == 200\n            data = json.loads(rv.data.decode('utf-8'))\n            assert len(data['subscribers']) is not 0\n            assert data['subscribers'] == [\n                {\n                    'imsi': '11101400135251',\n                    'last_seen': '2016-11-01',\n                    'msisdn': '22300825684694'\n                },\n                {\n                    'imsi': '11101400135252',\n                    'last_seen': '2016-11-02',\n                    'msisdn': '22300825684692'\n                }]\n        else:\n            with pytest.raises(DatabaseRoleCheckException):\n                per_test_flask_app.get(url_for('{0}.imei_get_subscribers_api'.format(api_version),\n                                               imei='388260336982806'))", "fn_id": 12, "class_fn": false, "repo": "fossabot/DIRBS-Core-1", "file": "tests/privileges.py", "last_update_at": "2022-02-24T07:34:00+00:00", "question_id": "fc9ed8e057689b4236908e0a26a6917db08253e9_12", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('per_test_flask_app', ['dirbs_poweruser_login', 'dirbs_api_user', 'dirbs_catalog_user'], indirect=True)\ndef test_imei_api(per_test_flask_app, per_test_postgres, logger, mocked_statsd, tmpdir, request, mocked_config, api_version):\n    \"\"\"Test IMEI API call works with the security role created based on abstract role.\"\"\"\n    dsn = per_test_postgres.dsn()\n    db_config = DBConfig(ignore_env=True, **dsn)\n    with create_db_connection(db_config) as conn, create_db_connection(db_config, autocommit=True) as metadata_conn:\n        with get_importer(OperatorDataImporter, conn, metadata_conn, db_config, tmpdir, logger, mocked_statsd, OperatorDataParams(filename='testData1-operator-operator1-anonymized_20161101_20161130.csv', operator='operator1', perform_unclean_checks=False, extract=False)) as imp:\n            imp.import_data()\n    current_user = request.node.callspec.params['per_test_flask_app']\n    if api_version == 'v1':\n        if current_user in ['dirbs_poweruser_login', 'dirbs_api_user']:\n            rv = per_test_flask_app.get(url_for('{0}.imei_api'.format(api_version), imei='388260336982806', include_seen_with=1))\n            assert rv.status_code == 200\n            assert json.loads(rv.data.decode('utf-8'))['seen_with'] == [{'imsi': '11101400135251', 'msisdn': '22300825684694'}, {'imsi': '11101400135252', 'msisdn': '22300825684692'}]\n            assert json.loads(rv.data.decode('utf-8'))['realtime_checks']['ever_observed_on_network'] is True\n        else:\n            with pytest.raises(DatabaseRoleCheckException):\n                per_test_flask_app.get(url_for('{0}.imei_api'.format(api_version), imei='388260336982806', include_seen_with=1))\n    elif current_user in ['dirbs_poweruser_login', 'dirbs_api_user']:\n        rv = per_test_flask_app.get(url_for('{0}.imei_get_subscribers_api'.format(api_version), imei='388260336982806'))\n        assert rv.status_code == 200\n        data = json.loads(rv.data.decode('utf-8'))\n        assert len(data['subscribers']) is not 0\n        assert data['subscribers'] == [{'imsi': '11101400135251', 'last_seen': '2016-11-01', 'msisdn': '22300825684694'}, {'imsi': '11101400135252', 'last_seen': '2016-11-02', 'msisdn': '22300825684692'}]\n    else:\n        with pytest.raises(DatabaseRoleCheckException):\n"]]}
{"hexsha": "efd9f141956a681544d2a8c593883fc80424006a", "ext": "py", "lang": "Python", "content": "async def test_connect_error_second_attempt(hostname, unused_tcp_port):\n    client = SMTP(hostname=hostname, port=unused_tcp_port, timeout=1.0)\n\n    with pytest.raises(SMTPConnectError):\n        await client.connect()\n\n    with pytest.raises(SMTPConnectError):\n        await client.connect()", "fn_id": 19, "class_fn": false, "repo": "P-EB/aiosmtplib", "file": "tests/test_connect.py", "last_update_at": "2022-03-17T19:52:54+00:00", "question_id": "efd9f141956a681544d2a8c593883fc80424006a_19", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def test_connect_error_second_attempt(hostname, unused_tcp_port):\n    client = SMTP(hostname=hostname, port=unused_tcp_port, timeout=1.0)\n    with pytest.raises(SMTPConnectError):\n        await client.connect()\n    with pytest.raises(SMTPConnectError):\n"]]}
{"hexsha": "f8ab3865cbf8288a67f7508e2e4ba99bb11a4feb", "ext": "py", "lang": "Python", "content": "def get_reward(state):\n    \"\"\"randomly select an action, return next state and reward\"\"\"\n    rand = random.randint(0, 3)\n    action = actions[rand]\n\n    x = state[0] + action[0]\n    y = state[1] + action[1]\n\n    if x < 0 or x >= size or y < 0 or y >= size:  # out of gridworld\n        next_state = state  # state unchanged\n    else:\n        next_state = [x, y]\n    reward = -1\n\n    return next_state, reward", "fn_id": 1, "class_fn": false, "repo": "innovator-zero/CS489_RL_Assignment", "file": "Assignment2/main.py", "last_update_at": "2022-03-17T05:45:48+00:00", "question_id": "f8ab3865cbf8288a67f7508e2e4ba99bb11a4feb_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_reward(state):\n    \"\"\"randomly select an action, return next state and reward\"\"\"\n    rand = random.randint(0, 3)\n    action = actions[rand]\n    x = state[0] + action[0]\n    y = state[1] + action[1]\n    if x < 0 or x >= size or y < 0 or (y >= size):\n        next_state = state\n    else:\n        next_state = [x, y]\n    reward = -1\n"]]}
{"hexsha": "c3f2179f4bca2e52365357a7b8f67a9e86f4593b", "ext": "py", "lang": "Python", "content": "def gen_embeddings(word_dict, dim, in_file=None,\n                   init=lasagne.init.Uniform()):\n    \"\"\"\n        Generate an initial embedding matrix for `word_dict`.\n        If an embedding file is not given or a word is not in the embedding file,\n        a randomly initialized vector will be used.\n    \"\"\"\n\n    num_words = max(word_dict.values()) + 1\n    embeddings = init((num_words, dim))\n    logging.info('Embeddings: %d x %d' % (num_words, dim))\n\n    if in_file is not None:\n        logging.info('Loading embedding file: %s' % in_file)\n        pre_trained = 0\n        for line in open(in_file).readlines():\n            sp = line.split()\n            assert len(sp) == dim + 1\n            if sp[0] in word_dict:\n                pre_trained += 1\n                embeddings[word_dict[sp[0]]] = [float(x) for x in sp[1:]]\n        logging.info('Pre-trained: %d (%.2f%%)' %\n                     (pre_trained, pre_trained * 100.0 / num_words))\n    return embeddings", "fn_id": 5, "class_fn": false, "repo": "SimonSuster/rc-cnn-dailymail", "file": "code/utils.py", "last_update_at": "2022-03-26T13:34:39+00:00", "question_id": "c3f2179f4bca2e52365357a7b8f67a9e86f4593b_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def gen_embeddings(word_dict, dim, in_file=None, init=lasagne.init.Uniform()):\n    \"\"\"\n        Generate an initial embedding matrix for `word_dict`.\n        If an embedding file is not given or a word is not in the embedding file,\n        a randomly initialized vector will be used.\n    \"\"\"\n    num_words = max(word_dict.values()) + 1\n    embeddings = init((num_words, dim))\n    logging.info('Embeddings: %d x %d' % (num_words, dim))\n    if in_file is not None:\n        logging.info('Loading embedding file: %s' % in_file)\n        pre_trained = 0\n        for line in open(in_file).readlines():\n            sp = line.split()\n            assert len(sp) == dim + 1\n            if sp[0] in word_dict:\n                pre_trained += 1\n                embeddings[word_dict[sp[0]]] = [float(x) for x in sp[1:]]\n        logging.info('Pre-trained: %d (%.2f%%)' % (pre_trained, pre_trained * 100.0 / num_words))\n"]]}
{"hexsha": "ad475283b62d067b315d2540f0db797993998a45", "ext": "py", "lang": "Python", "content": "def write(path: Path) -> None:\n    # read methods up to start of tag enum values\n    lines: List[str] = []\n    with open(path, \"r\") as f:\n        for line in f.readlines():\n            lines.append(line)\n            if line.strip() == MARKER:\n                break\n\n    # write tag enum values\n    with open(path, \"w\") as f:\n        f.writelines(lines)\n        for tag, v in DicomDictionary.items():\n            keyword = v[-1]\n            if keyword:\n                f.write(f\"    {keyword} = {tag}\\n\")", "fn_id": 0, "class_fn": false, "repo": "medcognetics/dicom-utils", "file": "tag_enum.py", "last_update_at": "2022-03-09T20:33:55+00:00", "question_id": "ad475283b62d067b315d2540f0db797993998a45_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def write(path: Path) -> None:\n    lines: List[str] = []\n    with open(path, 'r') as f:\n        for line in f.readlines():\n            lines.append(line)\n            if line.strip() == MARKER:\n                break\n    with open(path, 'w') as f:\n        f.writelines(lines)\n        for tag, v in DicomDictionary.items():\n            keyword = v[-1]\n            if keyword:\n"]]}
{"hexsha": "b90abd4cbd8ae14e472891ad622a88c42d689980", "ext": "py", "lang": "Python", "content": "@given(numeric_arrays, numeric_arrays)\ndef test_pow_special_cases_two_args_equal__less_2(arg1, arg2):\n    \"\"\"\n    Special case test for `__pow__(self, other, /)`:\n\n        -   If `x1_i` is `+0` and `x2_i` is less than `0`, the result is `+infinity`.\n\n    \"\"\"\n    res = arg1.__pow__(arg2)\n    mask = logical_and(exactly_equal(arg1, zero(arg1.shape, arg1.dtype)), less(arg2, zero(arg2.shape, arg2.dtype)))\n    assert_exactly_equal(res[mask], (infinity(arg1.shape, arg1.dtype))[mask])", "fn_id": 14, "class_fn": false, "repo": "honno/array-api-tests", "file": "array_api_tests/special_cases/test_dunder_pow.py", "last_update_at": "2022-03-05T12:22:09+00:00", "question_id": "b90abd4cbd8ae14e472891ad622a88c42d689980_14", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@given(numeric_arrays, numeric_arrays)\ndef test_pow_special_cases_two_args_equal__less_2(arg1, arg2):\n    \"\"\"\n    Special case test for `__pow__(self, other, /)`:\n\n        -   If `x1_i` is `+0` and `x2_i` is less than `0`, the result is `+infinity`.\n\n    \"\"\"\n    res = arg1.__pow__(arg2)\n    mask = logical_and(exactly_equal(arg1, zero(arg1.shape, arg1.dtype)), less(arg2, zero(arg2.shape, arg2.dtype)))\n"]]}
{"hexsha": "3305c03ee328b6073934665c8c1bf2a4353f7017", "ext": "py", "lang": "Python", "content": "def test_linecomp_by_sorting():\n    unsorted = [\n        '\\t'.join(line)\n        for line in [\n            [r'\\N', r'\\N', r'\\N'],\n            [r'\\N', '', r'\\N'],\n            [r'\\N', r'\\N', ''],\n            ['', r'\\N', r'\\N'],\n            [r'\\N', '-.52', 'baz'],\n            [r'\\N', '42', r'\\N'],\n            [r'\\N', '.42', 'bar'],\n            [r'\\N', '-.4', 'foo'],\n            [r'\\N', 'foo', '.42'],\n        ]\n    ]\n    sorted_lines = unsorted[:]\n    sorted_lines.sort(key=cmp_to_key(linecomp))\n    result = [s.split('\\t') for s in sorted_lines]\n    assert result == [\n        ['', r'\\N', r'\\N'],\n        [r'\\N', '', r'\\N'],\n        [r'\\N', '-.52', 'baz'],\n        [r'\\N', '-.4', 'foo'],\n        [r'\\N', '.42', 'bar'],\n        [r'\\N', '42', r'\\N'],\n        [r'\\N', r'\\N', ''],\n        [r'\\N', r'\\N', r'\\N'],\n        [r'\\N', 'foo', '.42'],\n    ]", "fn_id": 2, "class_fn": false, "repo": "akaihola/pgtricks", "file": "pgtricks/tests/test_pg_dump_splitsort.py", "last_update_at": "2022-03-31T21:34:12+00:00", "question_id": "3305c03ee328b6073934665c8c1bf2a4353f7017_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_linecomp_by_sorting():\n    unsorted = ['\\t'.join(line) for line in [['\\\\N', '\\\\N', '\\\\N'], ['\\\\N', '', '\\\\N'], ['\\\\N', '\\\\N', ''], ['', '\\\\N', '\\\\N'], ['\\\\N', '-.52', 'baz'], ['\\\\N', '42', '\\\\N'], ['\\\\N', '.42', 'bar'], ['\\\\N', '-.4', 'foo'], ['\\\\N', 'foo', '.42']]]\n    sorted_lines = unsorted[:]\n    sorted_lines.sort(key=cmp_to_key(linecomp))\n    result = [s.split('\\t') for s in sorted_lines]\n"]]}
{"hexsha": "387fcf1f324b8306f23da251bcbbc77f05345086", "ext": "py", "lang": "Python", "content": "@pytest.fixture(params=params, ids=ids)\ndef attention_setup(request):\n    sl, bs = 3, 2\n    edq, edk = request.param\n\n    # query would be the hidden state of the decoder\n    keys = to_gpu(V(T(np.random.rand(sl, bs, edk))))\n    query = to_gpu(V(T(np.random.rand(bs, edq))))\n    return keys, query", "fn_id": 0, "class_fn": false, "repo": "jalajthanaki/quick-nlp", "file": "src/tests/test_attention.py", "last_update_at": "2022-03-22T02:05:40+00:00", "question_id": "387fcf1f324b8306f23da251bcbbc77f05345086_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.fixture(params=params, ids=ids)\ndef attention_setup(request):\n    sl, bs = (3, 2)\n    edq, edk = request.param\n    keys = to_gpu(V(T(np.random.rand(sl, bs, edk))))\n    query = to_gpu(V(T(np.random.rand(bs, edq))))\n"]]}
{"hexsha": "b39c0b56e981e84669201b598a62d33237035acb", "ext": "py", "lang": "Python", "content": "def test_match_view_to_dash():\n    content_results = [{'dashboard.id': 1, 'dashboard_element.id': 1, 'dashboard_element.type': 'vis', 'dashboard_element.result_source': 'Lookless', 'query.model': 'bq', 'query.view': 'order_items',\n                        'query.formatted_fields': '[\"order_items.created_month\", \"order_items.count\"]', 'query.id': 59, 'dashboard.title': 'dash_1', 'look.id': None, 'sql_joins': ['`looker-private-demo.ecomm.order_items`']}]\n    explore_results = ipe.fetch_view_files(project)\n    sql_table_name = sql_table_names\n    test = ipe.match_view_to_dash(content_results=content_results,\n                                  explore_results=explore_results, sql_table_name=sql_table_name, proj=project)\n    assert isinstance(test, list)\n    assert len(test[0]) == 6\n    assert isinstance(test[0]['fields_used'], str)\n    assert test[0]['element_id'] == 1", "fn_id": 16, "class_fn": false, "repo": "looker-open-source/lmanage", "file": "tests/test_get_content_with_views.py", "last_update_at": "2022-03-26T03:17:53+00:00", "question_id": "b39c0b56e981e84669201b598a62d33237035acb_16", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_match_view_to_dash():\n    content_results = [{'dashboard.id': 1, 'dashboard_element.id': 1, 'dashboard_element.type': 'vis', 'dashboard_element.result_source': 'Lookless', 'query.model': 'bq', 'query.view': 'order_items', 'query.formatted_fields': '[\"order_items.created_month\", \"order_items.count\"]', 'query.id': 59, 'dashboard.title': 'dash_1', 'look.id': None, 'sql_joins': ['`looker-private-demo.ecomm.order_items`']}]\n    explore_results = ipe.fetch_view_files(project)\n    sql_table_name = sql_table_names\n    test = ipe.match_view_to_dash(content_results=content_results, explore_results=explore_results, sql_table_name=sql_table_name, proj=project)\n    assert isinstance(test, list)\n    assert len(test[0]) == 6\n    assert isinstance(test[0]['fields_used'], str)\n"]]}
{"hexsha": "40e73ecf9cce1253294d0ab103fc9130c0e9b2fc", "ext": "py", "lang": "Python", "content": "def plant(q, dq, u, f_ext, prior=prior):\n    \"\"\"TODO: docstring.\"\"\"\n    H, C, g, B = prior(q, dq)\n    ddq = jax.scipy.linalg.solve(H, f_ext + B@u - C@dq - g, sym_pos=True)\n    return ddq", "fn_id": 1, "class_fn": false, "repo": "StanfordASL/Adaptive-Control-Oriented-Meta-Learning", "file": "dynamics.py", "last_update_at": "2022-03-23T14:31:33+00:00", "question_id": "40e73ecf9cce1253294d0ab103fc9130c0e9b2fc_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def plant(q, dq, u, f_ext, prior=prior):\n    \"\"\"TODO: docstring.\"\"\"\n    H, C, g, B = prior(q, dq)\n    ddq = jax.scipy.linalg.solve(H, f_ext + B @ u - C @ dq - g, sym_pos=True)\n"]]}
{"hexsha": "485df8d752e40e82b5d4887a67e26061574addfb", "ext": "py", "lang": "Python", "content": "def write_to_string(\n    input_otio,\n    adapter_name='otio_json',\n    **adapter_argument_map\n):\n    \"\"\"Return input_otio written to a string using adapter_name.\n\n    Example:\n        raw_text = otio.adapters.write_to_string(my_timeline, \"otio_json\")\n    \"\"\"\n\n    adapter = plugins.ActiveManifest().from_name(adapter_name)\n    return adapter.write_to_string(\n        input_otio=input_otio,\n        **adapter_argument_map\n    )", "fn_id": 7, "class_fn": false, "repo": "desruie/OpenTimelineIO", "file": "src/py-opentimelineio/opentimelineio/adapters/__init__.py", "last_update_at": "2022-03-28T16:53:28+00:00", "question_id": "485df8d752e40e82b5d4887a67e26061574addfb_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def write_to_string(input_otio, adapter_name='otio_json', **adapter_argument_map):\n    \"\"\"Return input_otio written to a string using adapter_name.\n\n    Example:\n        raw_text = otio.adapters.write_to_string(my_timeline, \"otio_json\")\n    \"\"\"\n    adapter = plugins.ActiveManifest().from_name(adapter_name)\n"]]}
{"hexsha": "9e6c24499a4baf2892321a7a09ed7821d8536c60", "ext": "py", "lang": "Python", "content": "def create_follicle_on_selection():\n    sel = pm.ls(os = True)\n    if len(sel) > 1:\n        src_obj = sel[0]\n        trgt_objs = sel[1:]\n        follicle_list = fmc.create_follicle_object_position(src_obj, trgt_objs)\n\n        for f, t in zip(follicle_list, trgt_objs):\n            f.rename('{}_foll'.format(t.nodeName()))\n            res = pm.parentConstraint(f, t, mo = True)\n            res.setParent(world = True)", "fn_id": 0, "class_fn": false, "repo": "muhammadfredo/FrMaya", "file": "MayaMenubar/FR_Tools/03_Rigging/Create_follicle_on_selection.py", "last_update_at": "2022-03-07T03:59:36+00:00", "question_id": "9e6c24499a4baf2892321a7a09ed7821d8536c60_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_follicle_on_selection():\n    sel = pm.ls(os=True)\n    if len(sel) > 1:\n        src_obj = sel[0]\n        trgt_objs = sel[1:]\n        follicle_list = fmc.create_follicle_object_position(src_obj, trgt_objs)\n        for f, t in zip(follicle_list, trgt_objs):\n            f.rename('{}_foll'.format(t.nodeName()))\n            res = pm.parentConstraint(f, t, mo=True)\n"]]}
{"hexsha": "906f2f0971cacd8845d8c8eaa239106deda3912d", "ext": "py", "lang": "Python", "content": "def checkInteger(s):\n    try:\n        int(s)\n        return True\n    except ValueError:\n        return False", "fn_id": 30, "class_fn": false, "repo": "EnjoyLifeFund/macHighSierra-py36-pkgs", "file": "ansible/module_utils/cnos.py", "last_update_at": "2022-01-25T22:52:58+00:00", "question_id": "906f2f0971cacd8845d8c8eaa239106deda3912d_30", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def checkInteger(s):\n    try:\n        int(s)\n        return True\n    except ValueError:\n"]]}
{"hexsha": "d01958d5bdff8057ec2a9bafa4da149c845a20eb", "ext": "py", "lang": "Python", "content": "@dispatch(Tensor)\ndef bgr_to_rgb(image: Tensor) -> Tensor:\n    \"\"\"Convert a BGR image to RGB.\n\n    Args:\n        image (Tensor[B, 3, H, W]):\n            BGR Image to be converted to BGR.\n\n    Returns:\n        rgb (Tensor[B, 3, H, W]):\n            RGB version of the image.\n    \"\"\"\n    # Flip image channels\n    rgb = image.flip(-3)\n    return rgb", "fn_id": 0, "class_fn": false, "repo": "phlong3105/onevision", "file": "src/onevision/cv/imgproc/color/rgb.py", "last_update_at": "2022-03-28T14:12:32+00:00", "question_id": "d01958d5bdff8057ec2a9bafa4da149c845a20eb_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@dispatch(Tensor)\ndef bgr_to_rgb(image: Tensor) -> Tensor:\n    \"\"\"Convert a BGR image to RGB.\n\n    Args:\n        image (Tensor[B, 3, H, W]):\n            BGR Image to be converted to BGR.\n\n    Returns:\n        rgb (Tensor[B, 3, H, W]):\n            RGB version of the image.\n    \"\"\"\n    rgb = image.flip(-3)\n"]]}
{"hexsha": "d1cf99c651cf0ffca23a43424ee2897df98d37ab", "ext": "py", "lang": "Python", "content": "def drawDetection(image,detections,colors=None,cost=None):\n    if image is None:\n        return image\n    for item in detections:\n        xmin = item[3]\n        ymin = item[4]\n        xmax = item[5]\n        ymax = item[6]\n        label = str(int(item[1]))\n        if colors is None:\n            cv2.putText(image,label,(xmin,ymin), 3,1,(255,0,255))\n            cv2.rectangle(image,(xmin,ymin),(xmax,ymax),(255,0,0))\n        else:\n            color=colors[int(round(item[1]))]\n            color=[c *256 for c in color]\n            cv2.putText(image,label,(xmin,ymin), 3,1,color)\n            cv2.rectangle(image,(xmin,ymin),(xmax,ymax),color)\n\n    if not cost is None:\n        cv2.putText(image,cost,(0,40),3,1,(0,0,255))\n\n    return image", "fn_id": 0, "class_fn": false, "repo": "imistyrain/ssd-models", "file": "python/demo.py", "last_update_at": "2022-03-06T06:18:33+00:00", "question_id": "d1cf99c651cf0ffca23a43424ee2897df98d37ab_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def drawDetection(image, detections, colors=None, cost=None):\n    if image is None:\n        return image\n    for item in detections:\n        xmin = item[3]\n        ymin = item[4]\n        xmax = item[5]\n        ymax = item[6]\n        label = str(int(item[1]))\n        if colors is None:\n            cv2.putText(image, label, (xmin, ymin), 3, 1, (255, 0, 255))\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 0, 0))\n        else:\n            color = colors[int(round(item[1]))]\n            color = [c * 256 for c in color]\n            cv2.putText(image, label, (xmin, ymin), 3, 1, color)\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color)\n    if not cost is None:\n        cv2.putText(image, cost, (0, 40), 3, 1, (0, 0, 255))\n"]]}
{"hexsha": "92fa0e2d3e968b63fba36a494cc3712bc8d88307", "ext": "py", "lang": "Python", "content": "def is_real(val: (int, float)) -> bool:\n    \"\"\"\n    Check if the provided argument is real\n\n    :param val: value to check\n    :return: boolean indicating value is real\n    \"\"\"\n    return False if isinstance(val, complex) else True", "fn_id": 1, "class_fn": false, "repo": "jkalish14/UserInputParser", "file": "inputparser/validators.py", "last_update_at": "2022-01-22T15:27:42+00:00", "question_id": "92fa0e2d3e968b63fba36a494cc3712bc8d88307_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def is_real(val: (int, float)) -> bool:\n    \"\"\"\n    Check if the provided argument is real\n\n    :param val: value to check\n    :return: boolean indicating value is real\n    \"\"\"\n"]]}
{"hexsha": "0aa4930e4d04e7af8a581272b2d5b47e2e5ed098", "ext": "py", "lang": "Python", "content": "def get_tests(config={}):\n    from common import make_hash_tests\n\n    tests = []\n\n    test_vectors = load_tests((\"Cryptodome\", \"SelfTest\", \"Hash\", \"test_vectors\", \"SHA3\"),\n                                \"ShortMsgKAT_SHA3-512.txt\",\n                                \"KAT SHA-3 512\",\n                                { \"len\" : lambda x: int(x) } )\n\n    test_data = []\n    for tv in test_vectors:\n        if tv.len == 0:\n            tv.msg = b(\"\")\n        test_data.append((hexlify(tv.md), tv.msg, tv.desc))\n\n    tests += make_hash_tests(SHA3, \"SHA3_512\", test_data,\n                             digest_size=SHA3.digest_size,\n                             oid=\"2.16.840.1.101.3.4.2.10\")\n    tests += list_test_cases(APITest)\n    return tests", "fn_id": 0, "class_fn": false, "repo": "skylex77/PokeMapGT", "file": "app/pylibs/win32/Cryptodome/SelfTest/Hash/test_SHA3_512.py", "last_update_at": "2022-01-25T10:53:35+00:00", "question_id": "0aa4930e4d04e7af8a581272b2d5b47e2e5ed098_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_tests(config={}):\n    from common import make_hash_tests\n    tests = []\n    test_vectors = load_tests(('Cryptodome', 'SelfTest', 'Hash', 'test_vectors', 'SHA3'), 'ShortMsgKAT_SHA3-512.txt', 'KAT SHA-3 512', {'len': lambda x: int(x)})\n    test_data = []\n    for tv in test_vectors:\n        if tv.len == 0:\n            tv.msg = b('')\n        test_data.append((hexlify(tv.md), tv.msg, tv.desc))\n    tests += make_hash_tests(SHA3, 'SHA3_512', test_data, digest_size=SHA3.digest_size, oid='2.16.840.1.101.3.4.2.10')\n    tests += list_test_cases(APITest)\n"]]}
{"hexsha": "1617669b70e1b30ab3c168158648e2ae751fb4e7", "ext": "py", "lang": "Python", "content": "def logout(driver):\n    mf_logout_url = \"https://moneyforward.com/sign_out\"\n    driver.get(mf_logout_url)\n    time.sleep(2)", "fn_id": 3, "class_fn": false, "repo": "mu373/paypaycard-fetcher", "file": "app/moneyforward.py", "last_update_at": "2022-03-26T16:06:22+00:00", "question_id": "1617669b70e1b30ab3c168158648e2ae751fb4e7_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def logout(driver):\n    mf_logout_url = 'https://moneyforward.com/sign_out'\n    driver.get(mf_logout_url)\n"]]}
{"hexsha": "3d242408af15e8590789d85d9b83bfbf77b90b2c", "ext": "py", "lang": "Python", "content": "@pytest.mark.usefixtures(\"dummyG\")\n@pytest.mark.parametrize(\"sort_by\", (\"value\", None))\n@pytest.mark.parametrize(\"group_by\", (\"group\", None))\ndef test_arc(dummyG, group_by, sort_by):\n    \"\"\"Test for arc layout.\n\n    Checks:\n\n    1. X-axis minimum is 0.\n    2. X-axis maximum is 2 * (num_nodes - 1)\n    3. Y-axis remains at 0 all the time.\n    \"\"\"\n\n    pos, nt = get_pos_df(dummyG, layouts.arc, group_by=group_by, sort_by=sort_by)\n    assert pos[\"x\"].min() == 0\n    assert pos[\"x\"].max() == 2 * (len(nt) - 1)\n    assert all(pos[\"y\"] == 0.0)", "fn_id": 3, "class_fn": false, "repo": "ericmjl/nxviz", "file": "tests/test_layouts.py", "last_update_at": "2022-03-28T16:02:08+00:00", "question_id": "3d242408af15e8590789d85d9b83bfbf77b90b2c_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.usefixtures('dummyG')\n@pytest.mark.parametrize('sort_by', ('value', None))\n@pytest.mark.parametrize('group_by', ('group', None))\ndef test_arc(dummyG, group_by, sort_by):\n    \"\"\"Test for arc layout.\n\n    Checks:\n\n    1. X-axis minimum is 0.\n    2. X-axis maximum is 2 * (num_nodes - 1)\n    3. Y-axis remains at 0 all the time.\n    \"\"\"\n    pos, nt = get_pos_df(dummyG, layouts.arc, group_by=group_by, sort_by=sort_by)\n    assert pos['x'].min() == 0\n    assert pos['x'].max() == 2 * (len(nt) - 1)\n"]]}
{"hexsha": "85f14149a4c507ac9aed029bb9abf76e6e40b575", "ext": "py", "lang": "Python", "content": "def pybind11_get_include():\n    \"\"\"Get pybind11 include paths if it's installed as a Python package.\"\"\"\n    try:\n        import pybind11\n        try:\n            return [pybind11.get_include(True), pybind11.get_include(False)]\n        except AttributeError:\n            return []\n    except ImportError:\n        return []", "fn_id": 3, "class_fn": false, "repo": "aldanor/ipybind", "file": "ipybind/common.py", "last_update_at": "2022-01-09T06:21:05+00:00", "question_id": "85f14149a4c507ac9aed029bb9abf76e6e40b575_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def pybind11_get_include():\n    \"\"\"Get pybind11 include paths if it's installed as a Python package.\"\"\"\n    try:\n        import pybind11\n        try:\n            return [pybind11.get_include(True), pybind11.get_include(False)]\n        except AttributeError:\n            return []\n    except ImportError:\n"]]}
{"hexsha": "86adb472c4f6dc145f0669ea4b05c823e270f608", "ext": "py", "lang": "Python", "content": "def _python_exit():\n    global _global_shutdown\n    _global_shutdown = True\n    items = list(_threads_wakeups.items())\n    mp.util.debug(\"Interpreter shutting down. Waking up queue_manager_threads \"\n                  \"{}\".format(items))\n    for thread, thread_wakeup in items:\n        if thread.is_alive():\n            thread_wakeup.wakeup()\n    for thread, _ in items:\n        thread.join()", "fn_id": 1, "class_fn": false, "repo": "deerajnagothu/pyenf_extraction", "file": "venv/Lib/site-packages/joblib/externals/loky/process_executor.py", "last_update_at": "2022-03-07T16:33:25+00:00", "question_id": "86adb472c4f6dc145f0669ea4b05c823e270f608_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _python_exit():\n    global _global_shutdown\n    _global_shutdown = True\n    items = list(_threads_wakeups.items())\n    mp.util.debug('Interpreter shutting down. Waking up queue_manager_threads {}'.format(items))\n    for thread, thread_wakeup in items:\n        if thread.is_alive():\n            thread_wakeup.wakeup()\n    for thread, _ in items:\n"]]}
{"hexsha": "82f76c0e9dc96c7d6f44e0739c0cbe4a767ef48d", "ext": "py", "lang": "Python", "content": "def sonde5(isTest=False):\n    TILE_ID = 'sp_ex'\n    print(f'{getTimeStr()} (+) Starting sensors 5', flush=True)\n    start_time = time.time()\n    data = executeScriptToGetData()\n    tipboardAnswer = sendDataToTipboard(tile_id=TILE_ID, data=data, tile_template='simple_percentage', isTest=isTest)\n    fade = False if not random.randrange(0, 1) else True\n    sendBVColor(tile_id=TILE_ID, color=BACKGROUND_TAB[random.randrange(0, 3)], fading=fade)\n    end(title=f'sensors5 -> {TILE_ID}', start_time=start_time, tipboardAnswer=tipboardAnswer, TILE_ID=TILE_ID)", "fn_id": 1, "class_fn": false, "repo": "adeo/benchmark-tipboard", "file": "src/sensors/sensors5_simplepercentage.py", "last_update_at": "2022-03-31T09:27:14+00:00", "question_id": "82f76c0e9dc96c7d6f44e0739c0cbe4a767ef48d_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def sonde5(isTest=False):\n    TILE_ID = 'sp_ex'\n    print(f'{getTimeStr()} (+) Starting sensors 5', flush=True)\n    start_time = time.time()\n    data = executeScriptToGetData()\n    tipboardAnswer = sendDataToTipboard(tile_id=TILE_ID, data=data, tile_template='simple_percentage', isTest=isTest)\n    fade = False if not random.randrange(0, 1) else True\n    sendBVColor(tile_id=TILE_ID, color=BACKGROUND_TAB[random.randrange(0, 3)], fading=fade)\n"]]}
{"hexsha": "9c3e946512f4af0c18b57892638f2c5ee3b757eb", "ext": "py", "lang": "Python", "content": "@app.route(\"/progress\") #Do Discovery\ndef progress():\n\n\n    global username, password, ip_network, ssh_enabled_ips\n    content=get_status()\n    render_template(\"/progress.html\", ip_network=ip_network, title=\"TCP Scan\", hosts=ssh_enabled_ips,status=content)\n    ssh_enabled_ips=[]\n    #form = DeviceDiscoveryForm()\n    ssh_enabled_ips=tcpscan(ip_network)\n    unique_ips = list(set(ssh_enabled_ips))\n    if len(unique_ips) == 0:\n        flash('No Devices to login via SSH', 'danger')\n        content=get_status()\n        return redirect(url_for('device_discovery'))\n    try_logon(unique_ips)\n    content=get_status()\n    return render_template(\"/progress_logon.html\",status=content)", "fn_id": 18, "class_fn": false, "repo": "edergernot/webnetworkdump", "file": "WebNetdump.py", "last_update_at": "2022-03-15T07:59:06+00:00", "question_id": "9c3e946512f4af0c18b57892638f2c5ee3b757eb_18", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.route('/progress')\ndef progress():\n    global username, password, ip_network, ssh_enabled_ips\n    content = get_status()\n    render_template('/progress.html', ip_network=ip_network, title='TCP Scan', hosts=ssh_enabled_ips, status=content)\n    ssh_enabled_ips = []\n    ssh_enabled_ips = tcpscan(ip_network)\n    unique_ips = list(set(ssh_enabled_ips))\n    if len(unique_ips) == 0:\n        flash('No Devices to login via SSH', 'danger')\n        content = get_status()\n        return redirect(url_for('device_discovery'))\n    try_logon(unique_ips)\n    content = get_status()\n"]]}
{"hexsha": "9907caf7dfd173e92aca5156c33e78088d92ebe9", "ext": "py", "lang": "Python", "content": "def _check_info(info, base_env=False, agent_ids=None):\n    if base_env:\n        for _, multi_agent_dict in info.items():\n            for agent_id, inf in multi_agent_dict.items():\n                if not isinstance(inf, dict):\n                    raise ValueError(\n                        \"Your step function must return infos that are a dict. \"\n                        f\"instead was a {type(inf)}: element: {inf}\"\n                    )\n                if not (agent_id in agent_ids or agent_id == \"__all__\"):\n                    error = (\n                        f\"Your dones dictionary must have agent ids that belong to \"\n                        f\"the environment. Agent_ids recieved from \"\n                        f\"env.get_agent_ids() are: {agent_ids}\"\n                    )\n                    raise ValueError(error)\n    elif not isinstance(info, dict):\n        error = (\n            \"Your step function must return a info that \"\n            f\"is a dict. element type: {type(info)}. element: {info}\"\n        )\n        raise ValueError(error)", "fn_id": 6, "class_fn": false, "repo": "jianoaix/ray", "file": "rllib/utils/pre_checks/env.py", "last_update_at": "2022-03-19T07:14:43+00:00", "question_id": "9907caf7dfd173e92aca5156c33e78088d92ebe9_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _check_info(info, base_env=False, agent_ids=None):\n    if base_env:\n        for _, multi_agent_dict in info.items():\n            for agent_id, inf in multi_agent_dict.items():\n                if not isinstance(inf, dict):\n                    raise ValueError(f'Your step function must return infos that are a dict. instead was a {type(inf)}: element: {inf}')\n                if not (agent_id in agent_ids or agent_id == '__all__'):\n                    error = f'Your dones dictionary must have agent ids that belong to the environment. Agent_ids recieved from env.get_agent_ids() are: {agent_ids}'\n                    raise ValueError(error)\n    elif not isinstance(info, dict):\n        error = f'Your step function must return a info that is a dict. element type: {type(info)}. element: {info}'\n"]]}
{"hexsha": "7cb3cf244a9104c3b80b10eee5a24952750d6153", "ext": "py", "lang": "Python", "content": "def download_dikhololo_night():  # pragma: no cover\n    \"\"\"Download and read the dikholo night hdr texture example.\n\n    Files hosted at https://polyhaven.com/\n\n    Returns\n    -------\n    pyvista.texture\n        HDR Texture.\n\n    Examples\n    --------\n    >>> import pyvista\n    >>> from pyvista import examples    # doctest:+SKIP\n    >>> gltf_file = examples.gltf.download_damaged_helmet()  # doctest:+SKIP\n    >>> texture = examples.hdr.download_dikhololo_night()  # doctest:+SKIP\n    >>> pl = pyvista.Plotter()  # doctest:+SKIP\n    >>> pl.import_gltf(gltf_file)  # doctest:+SKIP\n    >>> pl.set_environment_texture(texture)  # doctest:+SKIP\n    >>> pl.show()  # doctest:+SKIP\n\n    \"\"\"\n    texture = _download_and_read('dikhololo_night_4k.hdr', texture=True)\n    texture.SetColorModeToDirectScalars()\n    texture.SetMipmap(True)\n    texture.SetInterpolate(True)\n    return texture", "fn_id": 0, "class_fn": false, "repo": "eino/pyvista", "file": "pyvista/examples/hdr.py", "last_update_at": "2022-03-31T22:16:32+00:00", "question_id": "7cb3cf244a9104c3b80b10eee5a24952750d6153_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def download_dikhololo_night():\n    \"\"\"Download and read the dikholo night hdr texture example.\n\n    Files hosted at https://polyhaven.com/\n\n    Returns\n    -------\n    pyvista.texture\n        HDR Texture.\n\n    Examples\n    --------\n    >>> import pyvista\n    >>> from pyvista import examples    # doctest:+SKIP\n    >>> gltf_file = examples.gltf.download_damaged_helmet()  # doctest:+SKIP\n    >>> texture = examples.hdr.download_dikhololo_night()  # doctest:+SKIP\n    >>> pl = pyvista.Plotter()  # doctest:+SKIP\n    >>> pl.import_gltf(gltf_file)  # doctest:+SKIP\n    >>> pl.set_environment_texture(texture)  # doctest:+SKIP\n    >>> pl.show()  # doctest:+SKIP\n\n    \"\"\"\n    texture = _download_and_read('dikhololo_night_4k.hdr', texture=True)\n    texture.SetColorModeToDirectScalars()\n    texture.SetMipmap(True)\n    texture.SetInterpolate(True)\n"]]}
{"hexsha": "7c11b3b1eddd029e69bf5bba21e4c070a90bb2f9", "ext": "py", "lang": "Python", "content": "@pytest.fixture(scope=\"module\")\ndef model_data():\n    mnist = mx.test_utils.get_mnist()\n    train_data = array_module.array(mnist[\"train_data\"].reshape(-1, 784))\n    train_label = array_module.array(mnist[\"train_label\"])\n    test_data = array_module.array(mnist[\"test_data\"].reshape(-1, 784))\n    return train_data, train_label, test_data", "fn_id": 1, "class_fn": false, "repo": "jinzhang21/mlflow", "file": "tests/gluon/test_gluon_model_export.py", "last_update_at": "2022-02-13T09:40:56+00:00", "question_id": "7c11b3b1eddd029e69bf5bba21e4c070a90bb2f9_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.fixture(scope='module')\ndef model_data():\n    mnist = mx.test_utils.get_mnist()\n    train_data = array_module.array(mnist['train_data'].reshape(-1, 784))\n    train_label = array_module.array(mnist['train_label'])\n    test_data = array_module.array(mnist['test_data'].reshape(-1, 784))\n"]]}
{"hexsha": "12d317512cab1bead5d5cb8fdd36a466e600ab0f", "ext": "py", "lang": "Python", "content": "@patch(\"services.health_checker.CosmosClient\")\n@patch(\"services.health_checker.get_store_key\")\ndef test_get_state_store_status_other_exception(cosmos_client_mock, get_store_key_mock) -> None:\n    get_store_key_mock.return_value = None\n    cosmos_client_mock.return_value = None\n    cosmos_client_mock.side_effect = Exception()\n\n    status, message = health_checker.create_state_store_status()\n\n    assert status == StatusEnum.not_ok\n    assert message == strings.UNSPECIFIED_ERROR", "fn_id": 2, "class_fn": false, "repo": "damoodamoo/AzureTRE", "file": "api_app/tests_ma/test_services/test_health_checker.py", "last_update_at": "2022-03-29T16:37:37+00:00", "question_id": "12d317512cab1bead5d5cb8fdd36a466e600ab0f_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@patch('services.health_checker.CosmosClient')\n@patch('services.health_checker.get_store_key')\ndef test_get_state_store_status_other_exception(cosmos_client_mock, get_store_key_mock) -> None:\n    get_store_key_mock.return_value = None\n    cosmos_client_mock.return_value = None\n    cosmos_client_mock.side_effect = Exception()\n    status, message = health_checker.create_state_store_status()\n    assert status == StatusEnum.not_ok\n"]]}
{"hexsha": "a2da1fda6f5da272aa59b296808b668b20521756", "ext": "py", "lang": "Python", "content": "@caching.cached_api_call(in_memory=True)\ndef get_buckets(context: models.Context) -> Mapping[str, Bucket]:\n\n  buckets: Dict[str, Bucket] = {}\n  if not apis.is_enabled(context.project_id, 'storage'):\n    return buckets\n  gcs_api = apis.get_api('storage', 'v1', context.project_id)\n  logging.info('fetching list of GCS buckets in project %s', context.project_id)\n  query = gcs_api.buckets().list(project=context.project_id)\n  try:\n    resp = query.execute(num_retries=config.API_RETRIES)\n    if 'items' not in resp:\n      return buckets\n    for resp_b in resp['items']:\n      # verify that we have some minimal data that we expect\n      if 'id' not in resp_b:\n        raise RuntimeError('missing data in bucket response')\n      f = Bucket(project_id=context.project_id, resource_data=resp_b)\n      buckets[f.full_path] = f\n  except googleapiclient.errors.HttpError as err:\n    raise utils.GcpApiError(err) from err\n  return buckets", "fn_id": 1, "class_fn": false, "repo": "jorisfa/gcpdiag", "file": "gcpdiag/queries/gcs.py", "last_update_at": "2022-03-30T02:01:15+00:00", "question_id": "a2da1fda6f5da272aa59b296808b668b20521756_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@caching.cached_api_call(in_memory=True)\ndef get_buckets(context: models.Context) -> Mapping[str, Bucket]:\n    buckets: Dict[str, Bucket] = {}\n    if not apis.is_enabled(context.project_id, 'storage'):\n        return buckets\n    gcs_api = apis.get_api('storage', 'v1', context.project_id)\n    logging.info('fetching list of GCS buckets in project %s', context.project_id)\n    query = gcs_api.buckets().list(project=context.project_id)\n    try:\n        resp = query.execute(num_retries=config.API_RETRIES)\n        if 'items' not in resp:\n            return buckets\n        for resp_b in resp['items']:\n            if 'id' not in resp_b:\n                raise RuntimeError('missing data in bucket response')\n            f = Bucket(project_id=context.project_id, resource_data=resp_b)\n            buckets[f.full_path] = f\n    except googleapiclient.errors.HttpError as err:\n        raise utils.GcpApiError(err) from err\n"]]}
{"hexsha": "69d07fdea709287a39e1d335373a3ae770c37c00", "ext": "py", "lang": "Python", "content": "def ghostSlider(ghostControls, surface, sliderParent):\n    \"\"\"Modify the ghost control behaviour to slide on top of a surface\n\n    Args:\n        ghostControls (dagNode): The ghost control\n        surface (Surface): The NURBS surface\n        sliderParent (dagNode): The parent for the slider.\n    \"\"\"\n    if not isinstance(ghostControls, list):\n        ghostControls = [ghostControls]\n\n    # Seleccionamos los controles Ghost que queremos mover sobre el surface\n\n    surfaceShape = surface.getShape()\n\n    for ctlGhost in ghostControls:\n        ctl = pm.listConnections(ctlGhost, t=\"transform\")[-1]\n        t = ctl.getMatrix(worldSpace=True)\n\n        gDriver = primitive.addTransform(ctlGhost.getParent(),\n                                         ctl.name() + \"_slideDriver\",\n                                         t)\n\n        try:\n            pm.connectAttr(ctl + \".translate\", gDriver + \".translate\")\n            pm.disconnectAttr(ctl + \".translate\", ctlGhost + \".translate\")\n        except RuntimeError:\n            pass\n\n        try:\n            pm.connectAttr(ctl + \".scale\", gDriver + \".scale\")\n            pm.disconnectAttr(ctl + \".scale\", ctlGhost + \".scale\")\n        except RuntimeError:\n            pass\n\n        try:\n            pm.connectAttr(ctl + \".rotate\", gDriver + \".rotate\")\n            pm.disconnectAttr(ctl + \".rotate\", ctlGhost + \".rotate\")\n        except RuntimeError:\n            pass\n\n        oParent = ctlGhost.getParent()\n        npoName = \"_\".join(ctlGhost.name().split(\"_\")[:-1]) + \"_npo\"\n        oTra = pm.PyNode(pm.createNode(\"transform\",\n                                       n=npoName,\n                                       p=oParent,\n                                       ss=True))\n        oTra.setTransformation(ctlGhost.getMatrix())\n        pm.parent(ctlGhost, oTra)\n\n        slider = primitive.addTransform(sliderParent,\n                                        ctl.name() + \"_slideDriven\",\n                                        t)\n\n        # connexion\n\n        dm_node = node.createDecomposeMatrixNode(\n            gDriver.attr(\"worldMatrix[0]\"))\n        cps_node = pm.createNode(\"closestPointOnSurface\")\n        dm_node.attr(\"outputTranslate\") >> cps_node.attr(\"inPosition\")\n        surfaceShape.attr(\"worldSpace[0]\") >> cps_node.attr(\"inputSurface\")\n        cps_node.attr(\"position\") >> slider.attr(\"translate\")\n\n        pm.normalConstraint(surfaceShape,\n                            slider,\n                            aimVector=[0, 0, 1],\n                            upVector=[0, 1, 0],\n                            worldUpType=\"objectrotation\",\n                            worldUpVector=[0, 1, 0],\n                            worldUpObject=gDriver)\n\n        pm.parent(ctlGhost.getParent(), slider)", "fn_id": 2, "class_fn": false, "repo": "KRNKRS/mgear", "file": "scripts/mgear/maya/rigbits/ghost.py", "last_update_at": "2022-03-28T07:40:04+00:00", "question_id": "69d07fdea709287a39e1d335373a3ae770c37c00_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ghostSlider(ghostControls, surface, sliderParent):\n    \"\"\"Modify the ghost control behaviour to slide on top of a surface\n\n    Args:\n        ghostControls (dagNode): The ghost control\n        surface (Surface): The NURBS surface\n        sliderParent (dagNode): The parent for the slider.\n    \"\"\"\n    if not isinstance(ghostControls, list):\n        ghostControls = [ghostControls]\n    surfaceShape = surface.getShape()\n    for ctlGhost in ghostControls:\n        ctl = pm.listConnections(ctlGhost, t='transform')[-1]\n        t = ctl.getMatrix(worldSpace=True)\n        gDriver = primitive.addTransform(ctlGhost.getParent(), ctl.name() + '_slideDriver', t)\n        try:\n            pm.connectAttr(ctl + '.translate', gDriver + '.translate')\n            pm.disconnectAttr(ctl + '.translate', ctlGhost + '.translate')\n        except RuntimeError:\n            pass\n        try:\n            pm.connectAttr(ctl + '.scale', gDriver + '.scale')\n            pm.disconnectAttr(ctl + '.scale', ctlGhost + '.scale')\n        except RuntimeError:\n            pass\n        try:\n            pm.connectAttr(ctl + '.rotate', gDriver + '.rotate')\n            pm.disconnectAttr(ctl + '.rotate', ctlGhost + '.rotate')\n        except RuntimeError:\n            pass\n        oParent = ctlGhost.getParent()\n        npoName = '_'.join(ctlGhost.name().split('_')[:-1]) + '_npo'\n        oTra = pm.PyNode(pm.createNode('transform', n=npoName, p=oParent, ss=True))\n        oTra.setTransformation(ctlGhost.getMatrix())\n        pm.parent(ctlGhost, oTra)\n        slider = primitive.addTransform(sliderParent, ctl.name() + '_slideDriven', t)\n        dm_node = node.createDecomposeMatrixNode(gDriver.attr('worldMatrix[0]'))\n        cps_node = pm.createNode('closestPointOnSurface')\n        dm_node.attr('outputTranslate') >> cps_node.attr('inPosition')\n        surfaceShape.attr('worldSpace[0]') >> cps_node.attr('inputSurface')\n        cps_node.attr('position') >> slider.attr('translate')\n        pm.normalConstraint(surfaceShape, slider, aimVector=[0, 0, 1], upVector=[0, 1, 0], worldUpType='objectrotation', worldUpVector=[0, 1, 0], worldUpObject=gDriver)\n"]]}
{"hexsha": "a447bf1ee450b2312b15331c0e994fd894e68be9", "ext": "py", "lang": "Python", "content": "def get_data(subset):\n    # something that yields [[SHAPE, SHAPE, CHANNELS], [1]]\n    ds = FakeData([[SHAPE, SHAPE, CHANNELS], [1]], 1000, random=False,\n                  dtype=['float32', 'uint8'], domain=[(0, 255), (0, 10)])\n    ds = PrefetchDataZMQ(ds, 2)\n    ds = BatchData(ds, BATCH_SIZE)\n    return ds", "fn_id": 1, "class_fn": false, "repo": "PatWie/tensorpack-recipes", "file": "FaceRecognition/ssh.py", "last_update_at": "2022-02-08T18:29:03+00:00", "question_id": "a447bf1ee450b2312b15331c0e994fd894e68be9_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_data(subset):\n    ds = FakeData([[SHAPE, SHAPE, CHANNELS], [1]], 1000, random=False, dtype=['float32', 'uint8'], domain=[(0, 255), (0, 10)])\n    ds = PrefetchDataZMQ(ds, 2)\n    ds = BatchData(ds, BATCH_SIZE)\n"]]}
{"hexsha": "3f2c2e3a271362a615460cd3772d1b00e4fa9480", "ext": "py", "lang": "Python", "content": "def testKeyConversionFromEd25519ToCurve25519():\n    signer = Signer()\n    sk = signer.keyraw\n    vk = signer.verraw\n    # Check when keys are passed as raw bytes\n    secretKey = ed25519SkToCurve25519(sk)\n    publicKey = ed25519PkToCurve25519(vk)\n    assert PrivateKey(secretKey).public_key.__bytes__() == publicKey\n    assert ed25519PkToCurve25519(vk, toHex=True) == \\\n           hexlify(PrivateKey(secretKey).public_key.__bytes__())\n\n    # Check when keys are passed as hex\n    secretKey = ed25519SkToCurve25519(hexlify(sk))\n    publicKey = ed25519PkToCurve25519(hexlify(vk))\n    assert PrivateKey(secretKey).public_key.__bytes__() == publicKey", "fn_id": 5, "class_fn": false, "repo": "andkononykhin/plenum", "file": "plenum/test/test_crypto.py", "last_update_at": "2022-03-16T21:31:20+00:00", "question_id": "3f2c2e3a271362a615460cd3772d1b00e4fa9480_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def testKeyConversionFromEd25519ToCurve25519():\n    signer = Signer()\n    sk = signer.keyraw\n    vk = signer.verraw\n    secretKey = ed25519SkToCurve25519(sk)\n    publicKey = ed25519PkToCurve25519(vk)\n    assert PrivateKey(secretKey).public_key.__bytes__() == publicKey\n    assert ed25519PkToCurve25519(vk, toHex=True) == hexlify(PrivateKey(secretKey).public_key.__bytes__())\n    secretKey = ed25519SkToCurve25519(hexlify(sk))\n    publicKey = ed25519PkToCurve25519(hexlify(vk))\n"]]}
{"hexsha": "02c97ec5f60a2d69124d3e37bf0c239a96883532", "ext": "py", "lang": "Python", "content": "def build_vocab(path, raw_vocab, config, trans='transE'):\n\n    print(\"Creating word vocabulary...\")\n    vocab_list = ['_PAD','_GO', '_EOS', '_UNK', ] + sorted(raw_vocab, key=raw_vocab.get, reverse=True)\n    if len(vocab_list) > config.symbols:\n        vocab_list = vocab_list[:config.symbols]\n    \n    print(\"Creating entity vocabulary...\")\n    entity_list = ['_NONE', '_PAD_H', '_PAD_R', '_PAD_T', '_NAF_H', '_NAF_R', '_NAF_T'] \n    with open('%s/entity.txt' % path) as f:\n        for i, line in enumerate(f):\n            e = line.strip()\n            entity_list.append(e)\n    \n    print(\"Creating relation vocabulary...\")\n    relation_list = []\n    with open('%s/relation.txt' % path) as f:\n        for i, line in enumerate(f):\n            r = line.strip()\n            relation_list.append(r)\n\n    print(\"Loading word vectors...\")\n    vectors = {}\n    with open('%s/glove.840B.300d.txt' % path) as f:\n        for i, line in enumerate(f):\n            if i % 100000 == 0:\n                print(\"    processing line %d\" % i)\n            s = line.strip()\n            word = s[:s.find(' ')]\n            vector = s[s.find(' ')+1:]\n            vectors[word] = vector\n    \n    embed = []\n    for word in vocab_list:\n        if word in vectors:\n            #vector = map(float, vectors[word].split())\n            vector = vectors[word].split()\n        else:\n            vector = np.zeros((config.embed_units), dtype=np.float32) \n        embed.append(vector)\n    embed = np.array(embed, dtype=np.float32)\n            \n    print(\"Loading entity vectors...\")\n    entity_embed = []\n    with open('%s/entity_%s.txt' % (path, trans)) as f:\n        for i, line in enumerate(f):\n            s = line.strip().split('\\t')\n            #entity_embed.append(map(float, s))\n            entity_embed.append(s)\n\n    print(\"Loading relation vectors...\")\n    relation_embed = []\n    with open('%s/relation_%s.txt' % (path, trans)) as f:\n        for i, line in enumerate(f):\n            s = line.strip().split('\\t')\n            relation_embed.append(s)\n\n    entity_relation_embed = np.array(entity_embed+relation_embed, dtype=np.float32)\n    entity_embed = np.array(entity_embed, dtype=np.float32)\n    relation_embed = np.array(relation_embed, dtype=np.float32)\n\n    word2id = dict()\n    entity2id = dict()\n    for word in vocab_list:\n        word2id[word] = len(word2id)\n    for entity in entity_list + relation_list:\n        entity2id[entity] = len(entity2id)\n\n    return word2id, entity2id, vocab_list, embed, entity_list, entity_embed, relation_list, relation_embed, entity_relation_embed", "fn_id": 1, "class_fn": false, "repo": "Misterion777/ConceptFlow", "file": "preprocession.py", "last_update_at": "2022-02-23T07:36:20+00:00", "question_id": "02c97ec5f60a2d69124d3e37bf0c239a96883532_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def build_vocab(path, raw_vocab, config, trans='transE'):\n    print('Creating word vocabulary...')\n    vocab_list = ['_PAD', '_GO', '_EOS', '_UNK'] + sorted(raw_vocab, key=raw_vocab.get, reverse=True)\n    if len(vocab_list) > config.symbols:\n        vocab_list = vocab_list[:config.symbols]\n    print('Creating entity vocabulary...')\n    entity_list = ['_NONE', '_PAD_H', '_PAD_R', '_PAD_T', '_NAF_H', '_NAF_R', '_NAF_T']\n    with open('%s/entity.txt' % path) as f:\n        for i, line in enumerate(f):\n            e = line.strip()\n            entity_list.append(e)\n    print('Creating relation vocabulary...')\n    relation_list = []\n    with open('%s/relation.txt' % path) as f:\n        for i, line in enumerate(f):\n            r = line.strip()\n            relation_list.append(r)\n    print('Loading word vectors...')\n    vectors = {}\n    with open('%s/glove.840B.300d.txt' % path) as f:\n        for i, line in enumerate(f):\n            if i % 100000 == 0:\n                print('    processing line %d' % i)\n            s = line.strip()\n            word = s[:s.find(' ')]\n            vector = s[s.find(' ') + 1:]\n            vectors[word] = vector\n    embed = []\n    for word in vocab_list:\n        if word in vectors:\n            vector = vectors[word].split()\n        else:\n            vector = np.zeros(config.embed_units, dtype=np.float32)\n        embed.append(vector)\n    embed = np.array(embed, dtype=np.float32)\n    print('Loading entity vectors...')\n    entity_embed = []\n    with open('%s/entity_%s.txt' % (path, trans)) as f:\n        for i, line in enumerate(f):\n            s = line.strip().split('\\t')\n            entity_embed.append(s)\n    print('Loading relation vectors...')\n    relation_embed = []\n    with open('%s/relation_%s.txt' % (path, trans)) as f:\n        for i, line in enumerate(f):\n            s = line.strip().split('\\t')\n            relation_embed.append(s)\n    entity_relation_embed = np.array(entity_embed + relation_embed, dtype=np.float32)\n    entity_embed = np.array(entity_embed, dtype=np.float32)\n    relation_embed = np.array(relation_embed, dtype=np.float32)\n    word2id = dict()\n    entity2id = dict()\n    for word in vocab_list:\n        word2id[word] = len(word2id)\n    for entity in entity_list + relation_list:\n        entity2id[entity] = len(entity2id)\n"]]}
{"hexsha": "857eca2a556951ee9bddd2597f5c81031f266fe2", "ext": "py", "lang": "Python", "content": "def clickable(widget):\n    \"\"\"\n    Makes widget to be clickable.\n\n    :param widget: QLabel to receive clicks\n    :return: clickable widget\n    \"\"\"\n\n    class Filter(QObject):\n        clicked = pyqtSignal()\n\n        def eventFilter(self, obj, event):\n            if obj == widget:\n                if event.type() == QEvent.MouseButtonRelease:\n                    if obj.rect().contains(event.pos()):\n                        self.clicked.emit()\n                        return True\n            return False\n\n    filter = Filter(widget)\n    widget.installEventFilter(filter)\n    return filter.clicked", "fn_id": 0, "class_fn": false, "repo": "marik348/python-messenger", "file": "messenger/client/click_label.py", "last_update_at": "2022-03-17T10:41:48+00:00", "question_id": "857eca2a556951ee9bddd2597f5c81031f266fe2_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def clickable(widget):\n    \"\"\"\n    Makes widget to be clickable.\n\n    :param widget: QLabel to receive clicks\n    :return: clickable widget\n    \"\"\"\n\n    class Filter(QObject):\n        clicked = pyqtSignal()\n\n        def eventFilter(self, obj, event):\n            if obj == widget:\n                if event.type() == QEvent.MouseButtonRelease:\n                    if obj.rect().contains(event.pos()):\n                        self.clicked.emit()\n                        return True\n            return False\n    filter = Filter(widget)\n    widget.installEventFilter(filter)\n"]]}
{"hexsha": "c55a06a4a9e4cd06ad98ec382a568e6b037654e6", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize('test_input,migrate', [\n    ('IPv6:::1\\n', True),\n    ('IPv6:0:0:0:0:0:0:0:1\\n', False),\n])\ndef test_check_migration(tmpdir, monkeypatch, test_input, migrate):\n    test_cfg_path = text_type(tmpdir)\n    test_cfg_file = os.path.join(test_cfg_path, 'sendmail.cf')\n    with open(test_cfg_file, 'w') as file_out:\n        file_out.write(test_input)\n    monkeypatch.setattr(checksendmail, 'SendmailConfDir', test_cfg_path)\n    files = checksendmail.check_files_for_compressed_ipv6()\n    if migrate:\n        assert files == [test_cfg_file]\n    else:\n        assert files == []", "fn_id": 0, "class_fn": false, "repo": "sm00th/leapp-repository", "file": "repos/system_upgrade/el7toel8/actors/checksendmail/tests/unit_test_checksendmail.py", "last_update_at": "2022-03-15T19:57:24+00:00", "question_id": "c55a06a4a9e4cd06ad98ec382a568e6b037654e6_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('test_input,migrate', [('IPv6:::1\\n', True), ('IPv6:0:0:0:0:0:0:0:1\\n', False)])\ndef test_check_migration(tmpdir, monkeypatch, test_input, migrate):\n    test_cfg_path = text_type(tmpdir)\n    test_cfg_file = os.path.join(test_cfg_path, 'sendmail.cf')\n    with open(test_cfg_file, 'w') as file_out:\n        file_out.write(test_input)\n    monkeypatch.setattr(checksendmail, 'SendmailConfDir', test_cfg_path)\n    files = checksendmail.check_files_for_compressed_ipv6()\n    if migrate:\n        assert files == [test_cfg_file]\n    else:\n"]]}
{"hexsha": "87d69495cb6e86db0b07fa1502909e7e45ef8903", "ext": "py", "lang": "Python", "content": "@app.route('/mongodb/phone/<phone>/<sum>')\ndef send_data(phone, sum):\n\ttry:\n\t\tr = requests.get(URL + '/send_data/'+phone+'/'+sum)\n\t\tif(r.text == 'nice'):\n\t\t\tuser = users.find_one({'phone': phone, \"onTelegram\": False})\n\t\t\tprint(user)\n\t\t\tif(user != None):\n\t\t\t\tsend_to_whatsapp(phone, sum)\n\t\t\treturn 'good'\n\t\telif(r.text == 'bad'):\n\t\t\treturn 'bad'\n\texcept:\t\n\t\treturn 'server error'", "fn_id": 2, "class_fn": false, "repo": "nomomon/potato-cashback", "file": "py_modules/mongo.py", "last_update_at": "2022-01-23T10:12:27+00:00", "question_id": "87d69495cb6e86db0b07fa1502909e7e45ef8903_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.route('/mongodb/phone/<phone>/<sum>')\ndef send_data(phone, sum):\n    try:\n        r = requests.get(URL + '/send_data/' + phone + '/' + sum)\n        if r.text == 'nice':\n            user = users.find_one({'phone': phone, 'onTelegram': False})\n            print(user)\n            if user != None:\n                send_to_whatsapp(phone, sum)\n            return 'good'\n        elif r.text == 'bad':\n            return 'bad'\n    except:\n"]]}
{"hexsha": "0a0f367c1c51a056826cfa4caa983d4254925a8b", "ext": "py", "lang": "Python", "content": "def sh_chebyt(n, monic=False):\n    r\"\"\"Shifted Chebyshev polynomial of the first kind.\n\n    Defined as :math:`T^*_n(x) = T_n(2x - 1)` for :math:`T_n` the nth\n    Chebyshev polynomial of the first kind.\n\n    Parameters\n    ----------\n    n : int\n        Degree of the polynomial.\n    monic : bool, optional\n        If `True`, scale the leading coefficient to be 1. Default is\n        `False`.\n\n    Returns\n    -------\n    T : orthopoly1d\n        Shifted Chebyshev polynomial of the first kind.\n\n    Notes\n    -----\n    The polynomials :math:`T^*_n` are orthogonal over :math:`[0, 1]`\n    with weight function :math:`(x - x^2)^{-1/2}`.\n\n    \"\"\"\n    base = sh_jacobi(n, 0.0, 0.5, monic=monic)\n    if monic:\n        return base\n    if n > 0:\n        factor = 4**n / 2.0\n    else:\n        factor = 1.0\n    base._scale(factor)\n    return base", "fn_id": 31, "class_fn": false, "repo": "maxi-marufo/my-scipy", "file": "scipy/special/orthogonal.py", "last_update_at": "2022-03-16T21:20:15+00:00", "question_id": "0a0f367c1c51a056826cfa4caa983d4254925a8b_31", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def sh_chebyt(n, monic=False):\n    \"\"\"Shifted Chebyshev polynomial of the first kind.\n\n    Defined as :math:`T^*_n(x) = T_n(2x - 1)` for :math:`T_n` the nth\n    Chebyshev polynomial of the first kind.\n\n    Parameters\n    ----------\n    n : int\n        Degree of the polynomial.\n    monic : bool, optional\n        If `True`, scale the leading coefficient to be 1. Default is\n        `False`.\n\n    Returns\n    -------\n    T : orthopoly1d\n        Shifted Chebyshev polynomial of the first kind.\n\n    Notes\n    -----\n    The polynomials :math:`T^*_n` are orthogonal over :math:`[0, 1]`\n    with weight function :math:`(x - x^2)^{-1/2}`.\n\n    \"\"\"\n    base = sh_jacobi(n, 0.0, 0.5, monic=monic)\n    if monic:\n        return base\n    if n > 0:\n        factor = 4 ** n / 2.0\n    else:\n        factor = 1.0\n    base._scale(factor)\n"]]}
{"hexsha": "56786380a0d7d8275e172fb97f1f3f9d7e83b7a6", "ext": "py", "lang": "Python", "content": "def mapping_match_logical():\n    \"\"\" Test matching of mapping type\"\"\"\n    boats = [\n        {\"\ud83d\udc13\": 1, },\n        {\"\ud83e\udd8a\": 1, \"\ud83c\udf3d\": 1},\n        {\"\ud83d\udc13\": 1, \"\ud83c\udf3d\": 1},\n        {\"\ud83d\udc13\": 1, \"\ud83e\udd8a\": 1},\n    ]\n    problems = 0\n    valid_boats = 0\n    for _ in range(100_000):\n        for boat in boats:\n            if isinstance(boat, Mapping):\n                if \"\ud83d\udc13\" in boat and \"\ud83c\udf3d\" in boat: \n                    problems += 1\n                elif \"\ud83d\udc13\" in boat and \"\ud83e\udd8a\" in boat: \n                    problems += 1\n                else:\n                    valid_boats += 1\n                    \n    \n    assert valid_boats == 200_000\n    assert problems == 200_000", "fn_id": 4, "class_fn": false, "repo": "tonybaloney/anti-patterns", "file": "bench_match.py", "last_update_at": "2022-03-31T09:48:09+00:00", "question_id": "56786380a0d7d8275e172fb97f1f3f9d7e83b7a6_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def mapping_match_logical():\n    \"\"\" Test matching of mapping type\"\"\"\n    boats = [{'\ud83d\udc13': 1}, {'\ud83e\udd8a': 1, '\ud83c\udf3d': 1}, {'\ud83d\udc13': 1, '\ud83c\udf3d': 1}, {'\ud83d\udc13': 1, '\ud83e\udd8a': 1}]\n    problems = 0\n    valid_boats = 0\n    for _ in range(100000):\n        for boat in boats:\n            if isinstance(boat, Mapping):\n                if '\ud83d\udc13' in boat and '\ud83c\udf3d' in boat:\n                    problems += 1\n                elif '\ud83d\udc13' in boat and '\ud83e\udd8a' in boat:\n                    problems += 1\n                else:\n                    valid_boats += 1\n    assert valid_boats == 200000\n"]]}
{"hexsha": "1fad3b8ef3251f78078d3369644fbeef33711b60", "ext": "py", "lang": "Python", "content": "@login_required(login_url='/camp/login/')\ndef post_review(request, slug=None):\n    if request.method == \"POST\":\n        camp = get_object_or_404(Camp, slug=slug)\n        user = request.user\n\n        rating = request.POST.get('user-rating')\n        comment = request.POST.get('user-review')\n        \n        rate_obj = Rate(camp=camp, user=user, rating=rating, comment=comment)\n        rate_obj.save()\n\n        return redirect('camp_detail', slug=slug)\n\n    else:\n        return HttpResponse('NOT ALLOWED')", "fn_id": 4, "class_fn": false, "repo": "mayank-agarwal-96/go-camping", "file": "camp/views.py", "last_update_at": "2022-01-13T00:40:52+00:00", "question_id": "1fad3b8ef3251f78078d3369644fbeef33711b60_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@login_required(login_url='/camp/login/')\ndef post_review(request, slug=None):\n    if request.method == 'POST':\n        camp = get_object_or_404(Camp, slug=slug)\n        user = request.user\n        rating = request.POST.get('user-rating')\n        comment = request.POST.get('user-review')\n        rate_obj = Rate(camp=camp, user=user, rating=rating, comment=comment)\n        rate_obj.save()\n        return redirect('camp_detail', slug=slug)\n    else:\n"]]}
{"hexsha": "5dfae507c252975a6115736efcc167381500c9f5", "ext": "py", "lang": "Python", "content": "def check_status(url: str) -> List[Result]:\n    results: List[Result] = []\n    search = [\"status/\", \"stats/\"]\n\n    for path in search:\n        target = urljoin(url, path)\n\n        res = network.http_get(target, False)\n        body = res.text\n\n        if res.status_code == 200 and \"Active connections:\" in body:\n            results.append(\n                Result(\n                    f\"Nginx status page found: {target}\",\n                    Vulnerabilities.SERVER_NGINX_STATUS_EXPOSED,\n                    target,\n                    [\n                        network.http_build_raw_request(res.request),\n                        network.http_build_raw_response(res),\n                    ],\n                )\n            )\n\n        results += response_scanner.check_response(target, res)\n\n    return results", "fn_id": 2, "class_fn": false, "repo": "Prodject/yawast", "file": "yawast/scanner/plugins/http/servers/nginx.py", "last_update_at": "2022-03-14T00:50:25+00:00", "question_id": "5dfae507c252975a6115736efcc167381500c9f5_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def check_status(url: str) -> List[Result]:\n    results: List[Result] = []\n    search = ['status/', 'stats/']\n    for path in search:\n        target = urljoin(url, path)\n        res = network.http_get(target, False)\n        body = res.text\n        if res.status_code == 200 and 'Active connections:' in body:\n            results.append(Result(f'Nginx status page found: {target}', Vulnerabilities.SERVER_NGINX_STATUS_EXPOSED, target, [network.http_build_raw_request(res.request), network.http_build_raw_response(res)]))\n        results += response_scanner.check_response(target, res)\n"]]}
{"hexsha": "60be9dffe436cdf54010f0d4755f320c0cdfc886", "ext": "py", "lang": "Python", "content": "def create_model_optimizer_autoencoder(args, dataset_class):\n    model = ResnetAutoencoder(z_dim=args.autoencoder_z_dim, num_classes=dataset_class.num_classes,\n                              drop_rate=args.drop_rate, input_size=dataset_class.input_size)\n\n    model = model.cuda()\n\n    if args.autoencoder_resume:\n        model, _, _ = resume_model(args, model)\n        args.start_epoch = args.autoencoder_train_epochs\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n    return model, optimizer, args", "fn_id": 7, "class_fn": false, "repo": "AhmadQasim/MedAL", "file": "code/utils.py", "last_update_at": "2022-03-23T18:30:46+00:00", "question_id": "60be9dffe436cdf54010f0d4755f320c0cdfc886_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_model_optimizer_autoencoder(args, dataset_class):\n    model = ResnetAutoencoder(z_dim=args.autoencoder_z_dim, num_classes=dataset_class.num_classes, drop_rate=args.drop_rate, input_size=dataset_class.input_size)\n    model = model.cuda()\n    if args.autoencoder_resume:\n        model, _, _ = resume_model(args, model)\n        args.start_epoch = args.autoencoder_train_epochs\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n"]]}
{"hexsha": "c65ed685f87e62d0f2de0d657da543ea6bdb99eb", "ext": "py", "lang": "Python", "content": "def prod(val, axis=-1, keepdims=False):\n    \"\"\"The product.\"\"\"\n    if get_backend() == \"pytorch\":\n        import torch\n\n        return torch.prod(val, dim=axis, keepdim=keepdims)\n    else:\n        import tensorflow as tf\n\n        return tf.reduce_prod(val, axis=axis, keepdims=keepdims)", "fn_id": 11, "class_fn": false, "repo": "chiragnagpal/probflow", "file": "src/probflow/utils/ops.py", "last_update_at": "2022-03-26T22:17:34+00:00", "question_id": "c65ed685f87e62d0f2de0d657da543ea6bdb99eb_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def prod(val, axis=-1, keepdims=False):\n    \"\"\"The product.\"\"\"\n    if get_backend() == 'pytorch':\n        import torch\n        return torch.prod(val, dim=axis, keepdim=keepdims)\n    else:\n        import tensorflow as tf\n"]]}
{"hexsha": "021a6dc595fc0f9782bcb9027efb61934f4c3d13", "ext": "py", "lang": "Python", "content": "def calculate(a, b, c,):\n    discriminant = b ** 2 - 4 * a * c\n    if discriminant < 0:\n        return None, None\n    x1 = (2 * c) / (-b + math.sqrt(discriminant))\n    x2 = (2 * c) / (-b - math.sqrt(discriminant))\n    return x1, x2", "fn_id": 0, "class_fn": false, "repo": "vick-hub/plc", "file": "week2/problem5.py", "last_update_at": "2022-03-13T06:58:07+00:00", "question_id": "021a6dc595fc0f9782bcb9027efb61934f4c3d13_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def calculate(a, b, c):\n    discriminant = b ** 2 - 4 * a * c\n    if discriminant < 0:\n        return (None, None)\n    x1 = 2 * c / (-b + math.sqrt(discriminant))\n    x2 = 2 * c / (-b - math.sqrt(discriminant))\n"]]}
{"hexsha": "51abb3f92ca204e4930864bed6c039caaf5485a1", "ext": "py", "lang": "Python", "content": "def load_summarized_table(\n    args: argparse.Namespace\n) -> pd.DataFrame:\n    \"\"\"Load the DataFrame and keep only columns according to the selected metrics.\n\n    Parameters\n    ----------\n    args : argparse.Namespace\n        Arguments to control the loading.\n\n    Returns\n    -------\n    pd.DataFrame\n        The summarized DataFrame.\n    \"\"\"\n    df = pd.read_csv(args.metrics_path)\n    keep_cols = list(df.columns)[:2]\n    for col in df.columns[2:]:\n        for cmet in args.chosen_metrics:\n            if col.endswith(cmet):\n                keep_cols.append(col)\n    summ_df = df[keep_cols]\n    summ_df = summ_df.sort_values(args.sort_by)\n    summ_df = summ_df.round(3)\n    return summ_df", "fn_id": 1, "class_fn": false, "repo": "jasonbian97/ptlflow", "file": "summary_metrics.py", "last_update_at": "2022-03-25T05:51:25+00:00", "question_id": "51abb3f92ca204e4930864bed6c039caaf5485a1_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_summarized_table(args: argparse.Namespace) -> pd.DataFrame:\n    \"\"\"Load the DataFrame and keep only columns according to the selected metrics.\n\n    Parameters\n    ----------\n    args : argparse.Namespace\n        Arguments to control the loading.\n\n    Returns\n    -------\n    pd.DataFrame\n        The summarized DataFrame.\n    \"\"\"\n    df = pd.read_csv(args.metrics_path)\n    keep_cols = list(df.columns)[:2]\n    for col in df.columns[2:]:\n        for cmet in args.chosen_metrics:\n            if col.endswith(cmet):\n                keep_cols.append(col)\n    summ_df = df[keep_cols]\n    summ_df = summ_df.sort_values(args.sort_by)\n    summ_df = summ_df.round(3)\n"]]}
{"hexsha": "0a3845708a2f673a61b329be3525c6a6cb0e6529", "ext": "py", "lang": "Python", "content": "def discover_benchmarks(module_name, type_=BenchmarksType.TIME.value, repeat=10, number=1):\n    \"\"\"\n    Discover benchmarks in the module\n\n    :param module_name: benchmarks module\n    :param type_: benchmark type\n    :return: time benchmarks\n    \"\"\"\n    for module in discover_modules(module_name):\n        for attr_name, module_attr in module.__dict__.items():\n            if attr_name.startswith('_'):\n                # skip attributes which start with underscore\n                continue\n\n            if inspect.isclass(module_attr):\n                for name, class_attr in inspect.getmembers(module_attr):\n                    if not name.startswith(f'{type_}_'):\n                        continue\n\n                    name_parts = module.__name__.split('.', 1)[1:] + [module_attr.__name__, name]\n                    benchmark_name = '.'.join(name_parts)\n                    func = inspect.getattr_static(module_attr, name)\n                    params = inspect.getattr_static(module_attr, 'params', [[]])\n                    for param in itertools.product(*params):\n                        yield TimeBenchmark(benchmark_name, func, param, module_attr, repeat=repeat, number=number)", "fn_id": 2, "class_fn": false, "repo": "AlexanderKalistratov/hpat", "file": "tests_perf/runner.py", "last_update_at": "2022-02-21T06:49:03+00:00", "question_id": "0a3845708a2f673a61b329be3525c6a6cb0e6529_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def discover_benchmarks(module_name, type_=BenchmarksType.TIME.value, repeat=10, number=1):\n    \"\"\"\n    Discover benchmarks in the module\n\n    :param module_name: benchmarks module\n    :param type_: benchmark type\n    :return: time benchmarks\n    \"\"\"\n    for module in discover_modules(module_name):\n        for attr_name, module_attr in module.__dict__.items():\n            if attr_name.startswith('_'):\n                continue\n            if inspect.isclass(module_attr):\n                for name, class_attr in inspect.getmembers(module_attr):\n                    if not name.startswith(f'{type_}_'):\n                        continue\n                    name_parts = module.__name__.split('.', 1)[1:] + [module_attr.__name__, name]\n                    benchmark_name = '.'.join(name_parts)\n                    func = inspect.getattr_static(module_attr, name)\n                    params = inspect.getattr_static(module_attr, 'params', [[]])\n                    for param in itertools.product(*params):\n"]]}
{"hexsha": "2c2fdc20708a2da8654abb2a61d2e64bc88dbe1b", "ext": "py", "lang": "Python", "content": "@patch('logging.Logger.info')\n@patch('requests.post')\ndef test_rocket_chat_success(mock_request, mock_logger, mock_url, mock_messages):\n    Messages.send_rocketchat_message(mock_messages, mock_url)\n\n    mock_request.assert_called_once_with(mock_url, json={'text': mock_messages[0]})\n    mock_logger.assert_called_once_with('Rocket Chat message sent!')", "fn_id": 2, "class_fn": false, "repo": "Justintime50/pullbug", "file": "test/unit/test_messages.py", "last_update_at": "2022-01-04T07:10:12+00:00", "question_id": "2c2fdc20708a2da8654abb2a61d2e64bc88dbe1b_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@patch('logging.Logger.info')\n@patch('requests.post')\ndef test_rocket_chat_success(mock_request, mock_logger, mock_url, mock_messages):\n    Messages.send_rocketchat_message(mock_messages, mock_url)\n    mock_request.assert_called_once_with(mock_url, json={'text': mock_messages[0]})\n"]]}
{"hexsha": "527688881363ac531808f4c8fa7ed0c84a6eaedc", "ext": "py", "lang": "Python", "content": "def train():\n    img_input = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, 16, 112, 112, 3))\n    y_target = tf.placeholder(tf.float32, shape=(FLAGS.batch_size,FLAGS.dimension))\n\n    y = model.C3D(img_input, dimensions=FLAGS.dimension,dropout=False,regularizer=True) # not label!\n\n    global_step = tf.Variable(0, trainable=False)\n\n    varlist_weight = []\n    varlist_bias = []\n    trainable_variables = tf.trainable_variables()\n    for var in trainable_variables:\n        if 'weight' in var.name:\n            varlist_weight.append(var)\n        elif 'bias' in var.name:\n            varlist_bias.append(var)\n\n    lr_weight = tf.train.exponential_decay(FLAGS.base_lr, global_step, 20000, 0.1,\n                                           staircase=True)\n    lr_bias = tf.train.exponential_decay(FLAGS.base_lr * 2, global_step, 20000, 0.1,\n                                         staircase=True)\n\n    opt_weight = tf.train.MomentumOptimizer(lr_weight, momentum=momentum)\n    opt_bias = tf.train.MomentumOptimizer(lr_bias, momentum=momentum)\n\n    mse_loss = tf.reduce_mean(tf.squared_difference(y, y_target))\n\n    weight_decay_loss = tf.add_n(tf.get_collection('weight_decay_loss'))\n\n    loss = mse_loss + weight_decay_loss\n\n    tf.summary.scalar('mse_loss', mse_loss)\n    tf.summary.scalar('weight_decay_loss', weight_decay_loss)\n    tf.summary.scalar('total_loss', loss)\n\n    grad_weight = opt_weight.compute_gradients(loss, varlist_weight)\n    grad_bias = opt_bias.compute_gradients(loss, varlist_bias)\n    apply_gradient_op_weight = opt_weight.apply_gradients(grad_weight)\n    apply_gradient_op_bias = opt_bias.apply_gradients(grad_bias, global_step=global_step)\n    train_op = tf.group(apply_gradient_op_weight, apply_gradient_op_bias)\n\n    saver = tf.train.Saver()\n    merged = tf.summary.merge_all()\n\n    rgb_list = 'list/rgb_train_linux.list'\n    u_flow_list = 'list/u_flow_train_linux.list'\n    v_flow_list = 'list/v_flow_train_linux.list'\n\n\n    with tf.Session() as sess:\n        train_writer = tf.summary.FileWriter('./visual_logs/train', sess.graph)\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n        for i in range(FLAGS.max_iter):\n            start_time = time.time()\n\n            train_images, train_labels, next_batch_start = input_data.read_all(\n                rgb_filename=rgb_list,\n                u_flow_filename=u_flow_list,\n                v_flow_filename=v_flow_list,\n                batch_size=FLAGS.batch_size,\n                start_pos=-1,\n                shuffle=True,\n                cpu_num=FLAGS.cpu_num\n            )\n\n\n            duration = time.time() - start_time\n            print('read data time %.3f sec' % (duration))\n\n            summary, loss_value, ce_loss, _, old_weight = sess.run([\n                merged, loss, mse_loss, train_op, grad_weight], feed_dict={\n                img_input: train_images,\n                y_target: train_labels\n            })\n\n            if i % (FLAGS.display) == 0:\n                print(\"mse_loss:\", ce_loss)\n                print(\"loss:\", loss_value)\n                train_writer.add_summary(summary, i)\n            duration = time.time() - start_time\n            print('Step %d: %.3f sec' % (i, duration))\n\n\n            if i % 1000 == 0:\n                saver.save(sess, os.path.join(FLAGS.model_save_path, model_name), global_step=global_step)", "fn_id": 0, "class_fn": false, "repo": "laura-wang/video_repres_mas", "file": "train.py", "last_update_at": "2022-03-29T01:24:05+00:00", "question_id": "527688881363ac531808f4c8fa7ed0c84a6eaedc_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def train():\n    img_input = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, 16, 112, 112, 3))\n    y_target = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, FLAGS.dimension))\n    y = model.C3D(img_input, dimensions=FLAGS.dimension, dropout=False, regularizer=True)\n    global_step = tf.Variable(0, trainable=False)\n    varlist_weight = []\n    varlist_bias = []\n    trainable_variables = tf.trainable_variables()\n    for var in trainable_variables:\n        if 'weight' in var.name:\n            varlist_weight.append(var)\n        elif 'bias' in var.name:\n            varlist_bias.append(var)\n    lr_weight = tf.train.exponential_decay(FLAGS.base_lr, global_step, 20000, 0.1, staircase=True)\n    lr_bias = tf.train.exponential_decay(FLAGS.base_lr * 2, global_step, 20000, 0.1, staircase=True)\n    opt_weight = tf.train.MomentumOptimizer(lr_weight, momentum=momentum)\n    opt_bias = tf.train.MomentumOptimizer(lr_bias, momentum=momentum)\n    mse_loss = tf.reduce_mean(tf.squared_difference(y, y_target))\n    weight_decay_loss = tf.add_n(tf.get_collection('weight_decay_loss'))\n    loss = mse_loss + weight_decay_loss\n    tf.summary.scalar('mse_loss', mse_loss)\n    tf.summary.scalar('weight_decay_loss', weight_decay_loss)\n    tf.summary.scalar('total_loss', loss)\n    grad_weight = opt_weight.compute_gradients(loss, varlist_weight)\n    grad_bias = opt_bias.compute_gradients(loss, varlist_bias)\n    apply_gradient_op_weight = opt_weight.apply_gradients(grad_weight)\n    apply_gradient_op_bias = opt_bias.apply_gradients(grad_bias, global_step=global_step)\n    train_op = tf.group(apply_gradient_op_weight, apply_gradient_op_bias)\n    saver = tf.train.Saver()\n    merged = tf.summary.merge_all()\n    rgb_list = 'list/rgb_train_linux.list'\n    u_flow_list = 'list/u_flow_train_linux.list'\n    v_flow_list = 'list/v_flow_train_linux.list'\n    with tf.Session() as sess:\n        train_writer = tf.summary.FileWriter('./visual_logs/train', sess.graph)\n        init = tf.global_variables_initializer()\n        sess.run(init)\n        for i in range(FLAGS.max_iter):\n            start_time = time.time()\n            train_images, train_labels, next_batch_start = input_data.read_all(rgb_filename=rgb_list, u_flow_filename=u_flow_list, v_flow_filename=v_flow_list, batch_size=FLAGS.batch_size, start_pos=-1, shuffle=True, cpu_num=FLAGS.cpu_num)\n            duration = time.time() - start_time\n            print('read data time %.3f sec' % duration)\n            summary, loss_value, ce_loss, _, old_weight = sess.run([merged, loss, mse_loss, train_op, grad_weight], feed_dict={img_input: train_images, y_target: train_labels})\n            if i % FLAGS.display == 0:\n                print('mse_loss:', ce_loss)\n                print('loss:', loss_value)\n                train_writer.add_summary(summary, i)\n            duration = time.time() - start_time\n            print('Step %d: %.3f sec' % (i, duration))\n            if i % 1000 == 0:\n"]]}
{"hexsha": "bdda29eae654a32b64034abe00e31a7c5d9698a3", "ext": "py", "lang": "Python", "content": "def main(root, target, part_two=False):\n    \"\"\"Orchestrate the BFS returning steps taken when target is reached.\"\"\"\n    prev = set()\n    prev.add(root)\n\n    to_process = collections.deque(possible_moves(root))\n    [prev.add(node) for node in to_process]\n\n    steps = 1\n\n    while True:\n        children = []\n        while to_process:\n            [children.append(child) for child in possible_moves(to_process.pop())\n                if child not in prev]\n\n        steps += 1\n\n        for child in children:\n            if child == target and not part_two:\n                return steps\n\n            prev.add(child)\n\n        if part_two and steps == 50:\n            return len(prev)\n\n        to_process = collections.deque(children)", "fn_id": 2, "class_fn": false, "repo": "nabiirah/advent-of-code", "file": "2016/day_13.py", "last_update_at": "2022-01-18T20:08:06+00:00", "question_id": "bdda29eae654a32b64034abe00e31a7c5d9698a3_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main(root, target, part_two=False):\n    \"\"\"Orchestrate the BFS returning steps taken when target is reached.\"\"\"\n    prev = set()\n    prev.add(root)\n    to_process = collections.deque(possible_moves(root))\n    [prev.add(node) for node in to_process]\n    steps = 1\n    while True:\n        children = []\n        while to_process:\n            [children.append(child) for child in possible_moves(to_process.pop()) if child not in prev]\n        steps += 1\n        for child in children:\n            if child == target and (not part_two):\n                return steps\n            prev.add(child)\n        if part_two and steps == 50:\n            return len(prev)\n"]]}
{"hexsha": "0bd1e8099b37fdc953669c4b6d9e7016634cd84f", "ext": "py", "lang": "Python", "content": "def gen_vis_weight(path, weight_max=10.0, weight_min=0.1):\n    vismap = np.load(path)\n    weight = vismap\n    for i in range(0,32):\n        for j in range(0,32):\n            for k in range(0,100):\n                if vismap[k,i,j] == 1.0:\n                    weight[k,i,j] = weight_max\n                elif vismap[k,i,j] == 0.0:\n                    weight[k,i,j] = weight_min\n                else:\n                    print('There is something wrong!')\n    return weight", "fn_id": 3, "class_fn": false, "repo": "eric-yoo/HairNet", "file": "src/preprocessing.py", "last_update_at": "2022-03-21T13:08:39+00:00", "question_id": "0bd1e8099b37fdc953669c4b6d9e7016634cd84f_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def gen_vis_weight(path, weight_max=10.0, weight_min=0.1):\n    vismap = np.load(path)\n    weight = vismap\n    for i in range(0, 32):\n        for j in range(0, 32):\n            for k in range(0, 100):\n                if vismap[k, i, j] == 1.0:\n                    weight[k, i, j] = weight_max\n                elif vismap[k, i, j] == 0.0:\n                    weight[k, i, j] = weight_min\n                else:\n                    print('There is something wrong!')\n"]]}
{"hexsha": "d5a168a7ac63614a45963af3a2360ac1f8fa11b1", "ext": "py", "lang": "Python", "content": "def showblock(bx, by, shape, c):\n  for y in range(0, 4):\n    for x in range(0, 4):\n      if shape & (1<<(y*4+x)):\n        map[bx+x][by+y] = c", "fn_id": 1, "class_fn": false, "repo": "tomwei7/LA104", "file": "system/apps_featured/113_circuitpython/scripts/tetris2.py", "last_update_at": "2022-03-21T03:47:05+00:00", "question_id": "d5a168a7ac63614a45963af3a2360ac1f8fa11b1_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def showblock(bx, by, shape, c):\n    for y in range(0, 4):\n        for x in range(0, 4):\n            if shape & 1 << y * 4 + x:\n"]]}
{"hexsha": "e0f87cbc7956af9fc25348670e5b24203e4404ca", "ext": "py", "lang": "Python", "content": "@Ghost.command(name=\"help\", description=\"The help command.\", usage=\"help (command)\", aliases=[\"cmds\", \"commands\"])\nasync def help(ctx, *, command = None):\n    totalcmds = len(Ghost.commands)-len(scriptsList)\n    if command is None:\n        if __embedmode__:\n            embed = discord.Embed(title=f\"{__embedemoji__} **{__embedtitle__}** {__embedemoji__}\", color=__embedcolour__, description=f\"\"\"\nArguments in `[]` are required, arguments in `()` are optional.\n\n`{Ghost.command_prefix}`**text (page 1/2)** \u00bb Text commands.\n`{Ghost.command_prefix}`**fun (page 1)** \u00bb Fun commands.\n`{Ghost.command_prefix}`**image (page 1)** \u00bb Image commands.\n`{Ghost.command_prefix}`**moderation (page 1)** \u00bb Moderation commands.\n`{Ghost.command_prefix}`**info (page 1)** \u00bb Info commands.\n`{Ghost.command_prefix}`**user (page 1)** \u00bb User commands.\n`{Ghost.command_prefix}`**selfbot (page 1)** \u00bb Selfbot commands.\n`{Ghost.command_prefix}`**webhook (page 1)** \u00bb Webhook commands.\n`{Ghost.command_prefix}`**abuse (page 1)** \u00bb Abuse commands.\n`{Ghost.command_prefix}`**themes (page 1)** \u00bb Theme commands.\n`{Ghost.command_prefix}`**giveaway (page 1)** \u00bb Giveaway commands.\n`{Ghost.command_prefix}`**nsfw (page 1)** \u00bb NSFW commands.\n`{Ghost.command_prefix}`**proxy (page 1)** \u00bb Proxy commands.\n`{Ghost.command_prefix}`**tools (page 1)** \u00bb Discord and other tools.\n`{Ghost.command_prefix}`**customcommands** \u00bb Your custom commands.\n`{Ghost.command_prefix}`**customscripts** \u00bb Your scripts.\n\n`{Ghost.command_prefix}`**search [term]** \u00bb Search for a command.\n`{Ghost.command_prefix}`**help (command)** \u00bb Help for a specific command.\n\nThere is a total of `{totalcmds}` commands.\n        \"\"\")\n            embed.set_author(name=\"All Commands\")\n            embed.set_image(url=__embedlargeimage__)\n            embed.set_thumbnail(url=__embedimage__)\n            embed.set_footer(text=__embedfooter__, icon_url=__embedfooterimage__)\n            embed.timestamp = datetime.now()\n            await ctx.send(embed=embed, delete_after=__deletetimeout__)\n        else:\n            await ctx.send(f\"\"\"```ini\n[ {__embedtitle__} ]\n\nArguments in [] are required, arguments in () are optional.\n\n{Ghost.command_prefix}text (page 1/2) \u00bb Text commands.\n{Ghost.command_prefix}fun (page 1) \u00bb Fun commands.\n{Ghost.command_prefix}image (page 1) \u00bb Image commands.\n{Ghost.command_prefix}moderation (page 1) \u00bb Moderation commands.\n{Ghost.command_prefix}info (page 1) \u00bb Info commands.\n{Ghost.command_prefix}user (page 1) \u00bb User commands.\n{Ghost.command_prefix}selfbot (page 1) \u00bb Selfbot commands.\n{Ghost.command_prefix}webhook (page 1) \u00bb Webhook commands.\n{Ghost.command_prefix}abuse (page 1) \u00bb Abuse commands.\n{Ghost.command_prefix}themes (page 1) \u00bb Theme commands.\n{Ghost.command_prefix}giveaway (page 1) \u00bb Giveaway commands.\n{Ghost.command_prefix}nsfw (page 1) \u00bb NSFW commands.\n{Ghost.command_prefix}proxy (page 1) \u00bb Proxy commands.\n{Ghost.command_prefix}tools (page 1) \u00bb Discord and other tools.\n{Ghost.command_prefix}customcommands \u00bb Your custom commands.\n{Ghost.command_prefix}customscripts \u00bb Your scripts.\n\n{Ghost.command_prefix}search [term] \u00bb Search for a command.\n{Ghost.command_prefix}help (command) \u00bb Help for a specific command.\n\nThere is a total of {totalcmds} commands.\n\n# {__embedfooter__}```\"\"\", delete_after=__deletetimeout__)\n\n    else:\n        for cmd in Ghost.commands:\n            if command == cmd.name or command in cmd.aliases:\n                if not cmd.aliases:\n                    cmd.aliases.append(\"No aliases\")\n                if __embedmode__:\n                    embed = discord.Embed(title=f\"{cmd.name}\", color=__embedcolour__)\n                    embed.add_field(name=\"Usage\", value=f\"{cmd.usage}\", inline=False)\n                    embed.add_field(name=\"Description\", value=f\"{cmd.description}\", inline=False)\n                    embed.add_field(name=\"Aliases\", value=', '.join(cmd.aliases))\n                    embed.set_thumbnail(url=__embedimage__)\n                    embed.set_image(url=__embedlargeimage__)\n                    embed.set_footer(text=__embedfooter__, icon_url=__embedfooterimage__)\n                    embed.timestamp = datetime.now()\n                    await ctx.send(embed=embed, delete_after=__deletetimeout__)\n                else:\n                    await ctx.send(f\"\"\"```ini\n[ {cmd.name} ]\n\nUsage: {cmd.usage}\nDescription: {cmd.description}\n\n\n# {__embedfooter__}```\"\"\", delete_after=__deletetimeout__)\n", "fn_id": 31, "class_fn": false, "repo": "abashir7866/Ghost", "file": "Ghost.py", "last_update_at": "2022-02-07T14:42:22+00:00", "question_id": "e0f87cbc7956af9fc25348670e5b24203e4404ca_31", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@Ghost.command(name='help', description='The help command.', usage='help (command)', aliases=['cmds', 'commands'])\nasync def help(ctx, *, command=None):\n    totalcmds = len(Ghost.commands) - len(scriptsList)\n    if command is None:\n        if __embedmode__:\n            embed = discord.Embed(title=f'{__embedemoji__} **{__embedtitle__}** {__embedemoji__}', color=__embedcolour__, description=f'\\nArguments in `[]` are required, arguments in `()` are optional.\\n\\n`{Ghost.command_prefix}`**text (page 1/2)** \u00bb Text commands.\\n`{Ghost.command_prefix}`**fun (page 1)** \u00bb Fun commands.\\n`{Ghost.command_prefix}`**image (page 1)** \u00bb Image commands.\\n`{Ghost.command_prefix}`**moderation (page 1)** \u00bb Moderation commands.\\n`{Ghost.command_prefix}`**info (page 1)** \u00bb Info commands.\\n`{Ghost.command_prefix}`**user (page 1)** \u00bb User commands.\\n`{Ghost.command_prefix}`**selfbot (page 1)** \u00bb Selfbot commands.\\n`{Ghost.command_prefix}`**webhook (page 1)** \u00bb Webhook commands.\\n`{Ghost.command_prefix}`**abuse (page 1)** \u00bb Abuse commands.\\n`{Ghost.command_prefix}`**themes (page 1)** \u00bb Theme commands.\\n`{Ghost.command_prefix}`**giveaway (page 1)** \u00bb Giveaway commands.\\n`{Ghost.command_prefix}`**nsfw (page 1)** \u00bb NSFW commands.\\n`{Ghost.command_prefix}`**proxy (page 1)** \u00bb Proxy commands.\\n`{Ghost.command_prefix}`**tools (page 1)** \u00bb Discord and other tools.\\n`{Ghost.command_prefix}`**customcommands** \u00bb Your custom commands.\\n`{Ghost.command_prefix}`**customscripts** \u00bb Your scripts.\\n\\n`{Ghost.command_prefix}`**search [term]** \u00bb Search for a command.\\n`{Ghost.command_prefix}`**help (command)** \u00bb Help for a specific command.\\n\\nThere is a total of `{totalcmds}` commands.\\n            ')\n            embed.set_author(name='All Commands')\n            embed.set_image(url=__embedlargeimage__)\n            embed.set_thumbnail(url=__embedimage__)\n            embed.set_footer(text=__embedfooter__, icon_url=__embedfooterimage__)\n            embed.timestamp = datetime.now()\n            await ctx.send(embed=embed, delete_after=__deletetimeout__)\n        else:\n            await ctx.send(f'```ini\\n[ {__embedtitle__} ]\\n\\nArguments in [] are required, arguments in () are optional.\\n\\n{Ghost.command_prefix}text (page 1/2) \u00bb Text commands.\\n{Ghost.command_prefix}fun (page 1) \u00bb Fun commands.\\n{Ghost.command_prefix}image (page 1) \u00bb Image commands.\\n{Ghost.command_prefix}moderation (page 1) \u00bb Moderation commands.\\n{Ghost.command_prefix}info (page 1) \u00bb Info commands.\\n{Ghost.command_prefix}user (page 1) \u00bb User commands.\\n{Ghost.command_prefix}selfbot (page 1) \u00bb Selfbot commands.\\n{Ghost.command_prefix}webhook (page 1) \u00bb Webhook commands.\\n{Ghost.command_prefix}abuse (page 1) \u00bb Abuse commands.\\n{Ghost.command_prefix}themes (page 1) \u00bb Theme commands.\\n{Ghost.command_prefix}giveaway (page 1) \u00bb Giveaway commands.\\n{Ghost.command_prefix}nsfw (page 1) \u00bb NSFW commands.\\n{Ghost.command_prefix}proxy (page 1) \u00bb Proxy commands.\\n{Ghost.command_prefix}tools (page 1) \u00bb Discord and other tools.\\n{Ghost.command_prefix}customcommands \u00bb Your custom commands.\\n{Ghost.command_prefix}customscripts \u00bb Your scripts.\\n\\n{Ghost.command_prefix}search [term] \u00bb Search for a command.\\n{Ghost.command_prefix}help (command) \u00bb Help for a specific command.\\n\\nThere is a total of {totalcmds} commands.\\n\\n# {__embedfooter__}```', delete_after=__deletetimeout__)\n    else:\n        for cmd in Ghost.commands:\n            if command == cmd.name or command in cmd.aliases:\n                if not cmd.aliases:\n                    cmd.aliases.append('No aliases')\n                if __embedmode__:\n                    embed = discord.Embed(title=f'{cmd.name}', color=__embedcolour__)\n                    embed.add_field(name='Usage', value=f'{cmd.usage}', inline=False)\n                    embed.add_field(name='Description', value=f'{cmd.description}', inline=False)\n                    embed.add_field(name='Aliases', value=', '.join(cmd.aliases))\n                    embed.set_thumbnail(url=__embedimage__)\n                    embed.set_image(url=__embedlargeimage__)\n                    embed.set_footer(text=__embedfooter__, icon_url=__embedfooterimage__)\n                    embed.timestamp = datetime.now()\n                    await ctx.send(embed=embed, delete_after=__deletetimeout__)\n                else:\n"]]}
{"hexsha": "77323326a3a05ef5b71a17f3de780ad88fadd947", "ext": "py", "lang": "Python", "content": "def createPlugMatrixFromCells( cellPlugs ) :\n\n\tif not cellPlugs :\n\t\treturn []\n\n\trowsPlug = next( iter( cellPlugs ) ).ancestor( Gaffer.Spreadsheet.RowsPlug )\n\tassert( rowsPlug is not None )\n\n\t# Build a matrix of rows/columns in ascending order. We don't actually\n\t# care what the original row/column indices were, we just need them\n\t# to be ascending so the matrix represents the logical order of the cells.\n\n\tmatrix = []\n\n\t# First, group cells by row\n\trows = {}\n\tfor cell in cellPlugs :\n\t\trowPlug = cell.ancestor( Gaffer.Spreadsheet.RowPlug )\n\t\trows.setdefault( rowPlug, [] ).append( cell )\n\n\t# Then sort the rows, and their cells\n\tspreadsheetRows = rowsPlug.children()\n\tfor rowPlug, cells in sorted( rows.items(), key = lambda item : spreadsheetRows.index( item[0] ) ) :\n\t\trowCells = rowPlug[\"cells\"].children()\n\t\tmatrix.append( sorted( cells, key = rowCells.index ) )\n\n\treturn matrix", "fn_id": 6, "class_fn": false, "repo": "fbhradec/gaffer", "file": "python/GafferUI/SpreadsheetUI/_ClipboardAlgo.py", "last_update_at": "2022-02-08T13:54:05+00:00", "question_id": "77323326a3a05ef5b71a17f3de780ad88fadd947_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def createPlugMatrixFromCells(cellPlugs):\n    if not cellPlugs:\n        return []\n    rowsPlug = next(iter(cellPlugs)).ancestor(Gaffer.Spreadsheet.RowsPlug)\n    assert rowsPlug is not None\n    matrix = []\n    rows = {}\n    for cell in cellPlugs:\n        rowPlug = cell.ancestor(Gaffer.Spreadsheet.RowPlug)\n        rows.setdefault(rowPlug, []).append(cell)\n    spreadsheetRows = rowsPlug.children()\n    for rowPlug, cells in sorted(rows.items(), key=lambda item: spreadsheetRows.index(item[0])):\n        rowCells = rowPlug['cells'].children()\n        matrix.append(sorted(cells, key=rowCells.index))\n"]]}
{"hexsha": "577ebd038b06483a51fc1759c7dbed2554343823", "ext": "py", "lang": "Python", "content": "def readData(filename, NumDataSet):\n    ### Read CSV into raw_data dataframe ###\n    input_data = pd.read_csv(filename)\n    raw_data_header = input_data.columns[1:NumDataSet+1].tolist()\n    \n\n    ### Print Data headers\n    print('Input Data File Header:', raw_data_header)\n\n    ### number of columns and rows in raw_data (shape only for dataframe)\n    input_data_numrows = input_data.shape[0]\n    \n    input_data_numcols = input_data.shape[1]- NumDataSet #minus off the std deviation data\n    print('Input Data: Rows -', input_data_numrows, ', Columns -', input_data_numcols)\n\n    ### Saving raw_data columns into individual list (rows) in rfp_data numpy\n    # Flip row and col from raw_data\n    rfp_data = np.zeros((input_data_numcols,input_data_numrows))\n\n    for i in range(0, input_data_numcols): # Iterate through all columns\n        rfp_data[i, :] = input_data.iloc[:, i]\n    # print(rfp_data)\n\n    # Time + All Inducers\n    rfp_data_numrows = rfp_data.shape[0] # or = np.size(rfp_data,0)\n    # Number of RFP Data Per Inducer\n    rfp_data_numcols = rfp_data.shape[1]\n\n    ### Convert RFP/OD into (mol/OD) ###\n    # Starts from 1 because Row 0 is time\n    for i in range (1, rfp_data_numrows):\n        for j in range (0, rfp_data_numcols):\n            rfp_data[i][j] = ODconv(rfp_data[i][j])\n\n    #store standard deviation data\n    stddev_data = np.zeros((NumDataSet, input_data_numrows))\n\n    for i in range(input_data_numcols, input_data_numcols + NumDataSet): # Iterate through all columns\n        stddev_data[i-input_data_numcols, :] = input_data.iloc[:, i]\n\n    stddev_data_numrows = stddev_data.shape[0]\n    stddev_data_numcols = stddev_data.shape[1]\n\n    # convert RFP/OD into Molar/OD\n    for i in range(0, stddev_data_numrows):\n        for j in range(0, stddev_data_numcols):\n            stddev_data[i][j] = ODconv(stddev_data[i][j])\n\n    return raw_data_header, rfp_data, stddev_data", "fn_id": 0, "class_fn": false, "repo": "EngBioNUS/BMSSlib", "file": "BMSSlibmod/Read_Data.py", "last_update_at": "2022-02-20T04:58:09+00:00", "question_id": "577ebd038b06483a51fc1759c7dbed2554343823_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def readData(filename, NumDataSet):\n    input_data = pd.read_csv(filename)\n    raw_data_header = input_data.columns[1:NumDataSet + 1].tolist()\n    print('Input Data File Header:', raw_data_header)\n    input_data_numrows = input_data.shape[0]\n    input_data_numcols = input_data.shape[1] - NumDataSet\n    print('Input Data: Rows -', input_data_numrows, ', Columns -', input_data_numcols)\n    rfp_data = np.zeros((input_data_numcols, input_data_numrows))\n    for i in range(0, input_data_numcols):\n        rfp_data[i, :] = input_data.iloc[:, i]\n    rfp_data_numrows = rfp_data.shape[0]\n    rfp_data_numcols = rfp_data.shape[1]\n    for i in range(1, rfp_data_numrows):\n        for j in range(0, rfp_data_numcols):\n            rfp_data[i][j] = ODconv(rfp_data[i][j])\n    stddev_data = np.zeros((NumDataSet, input_data_numrows))\n    for i in range(input_data_numcols, input_data_numcols + NumDataSet):\n        stddev_data[i - input_data_numcols, :] = input_data.iloc[:, i]\n    stddev_data_numrows = stddev_data.shape[0]\n    stddev_data_numcols = stddev_data.shape[1]\n    for i in range(0, stddev_data_numrows):\n        for j in range(0, stddev_data_numcols):\n            stddev_data[i][j] = ODconv(stddev_data[i][j])\n"]]}
{"hexsha": "f6c78b9e1e685bcb9be5316b826a143242b8699c", "ext": "py", "lang": "Python", "content": "def train(graph, model):\n  tfg.conf.training = True\n  query_train = query(graph, gl.Mask.TRAIN)\n  dataset = tfg.Dataset(query_train, window=5)\n  data_dict = dataset.get_data_dict()\n  feature_handler = tfg.FeatureHandler('feature_handler',\n    query_train.get_node(\"train\").decoder.feature_spec)\n\n  x_list = reformat_node_feature(data_dict, query_train.list_alias(), feature_handler)\n  train_embeddings = model.forward(x_list, nbrs_num)\n  loss = supervised_loss(train_embeddings, data_dict['train'].labels)\n  return dataset.iterator, loss", "fn_id": 4, "class_fn": false, "repo": "amznero/graph-learn", "file": "examples/tf/ego_rgcn/train_supervised.py", "last_update_at": "2022-03-31T01:27:21+00:00", "question_id": "f6c78b9e1e685bcb9be5316b826a143242b8699c_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def train(graph, model):\n    tfg.conf.training = True\n    query_train = query(graph, gl.Mask.TRAIN)\n    dataset = tfg.Dataset(query_train, window=5)\n    data_dict = dataset.get_data_dict()\n    feature_handler = tfg.FeatureHandler('feature_handler', query_train.get_node('train').decoder.feature_spec)\n    x_list = reformat_node_feature(data_dict, query_train.list_alias(), feature_handler)\n    train_embeddings = model.forward(x_list, nbrs_num)\n    loss = supervised_loss(train_embeddings, data_dict['train'].labels)\n"]]}
{"hexsha": "391280629f45849718bf50a45a483389e94997dc", "ext": "py", "lang": "Python", "content": "def doConvertRc(oMgr, fromVersion):\n    if fromVersion < 210:\n        globs.log.write(globs.SEV_NOTICE, function='Convert', action='doConvertRc', msg='Updating .rc file from version {} to version 210.'.format(fromVersion))\n        # Start adding back in secitons\n        if oMgr.parser.has_section('main') is False:\n            globs.log.write(globs.SEV_DEBUG, function='Convert', action='convertRc', msg='Adding [main] section.')\n            oMgr.addRcSection('main')\n\n        if oMgr.parser.has_section('incoming') is False:\n            globs.log.write(globs.SEV_DEBUG, function='Convert', action='convertRc', msg='Adding [incoming] section.')\n            oMgr.addRcSection('incoming')\n\n        if oMgr.parser.has_section('outgoing') is False:\n            globs.log.write(globs.SEV_DEBUG, function='Convert', action='convertRc', msg='Adding [outgoing] section.')\n            oMgr.addRcSection('outgoing')\n\n        if oMgr.parser.has_section('report') is False:\n            globs.log.write(globs.SEV_DEBUG, function='Convert', action='convertRc', msg='Adding [report] section.')\n            oMgr.addRcSection('report')\n\n        if oMgr.parser.has_section('headings') is False:\n            globs.log.write(globs.SEV_DEBUG, function='Convert', action='convertRc', msg='Adding [headings] section.')\n            oMgr.addRcSection('headings')\n\n        for fromsection, fromoption, tosection, tooption in optList210:\n            moveOption(oMgr, fromsection, fromoption, tosection, tooption)\n\n        # Adjusted format of sizeDisplay in version 2.1\n        szDisp = oMgr.getRcOption('report', 'sizedisplay')\n        if szDisp == 'none':\n            oMgr.setRcOption('report', 'sizedisplay', 'byte')\n        oMgr.setRcOption('report', 'showsizedisplay', 'true')\n\n        oMgr.updateRc()\n        doConvertRc(oMgr, 210)\n    elif fromVersion < 300:\n        globs.log.write(globs.SEV_NOTICE, function='Convert', action='doConvertRc', msg='Updating .rc file from version {} to version 300.'.format(fromVersion))\n        # Remove deprecated options\n        if oMgr.parser.has_option('report', 'noactivitybg') == True:    # Deprecated in version 2.2.0\n            oMgr.clearRcOption('report', 'noactivitybg')\n\n        if oMgr.parser.has_option('main', 'version') == True:    # Deprecated in version 2.2.7 (renamed to 'rcversion')\n            oMgr.clearRcOption('main', 'version')\n\n        oMgr.updateRc()\n        doConvertRc(oMgr, 300)\n    elif fromVersion < 310:\n        globs.log.write(globs.SEV_NOTICE, function='Convert', action='doConvertRc', msg='Updating .rc file from version {} to version 310.'.format(fromVersion))\n        # Adjust size display option\n        value1 = oMgr.getRcOption('report', 'sizedisplay')\n        value2 = oMgr.getRcOption('report', 'showsizedisplay')\n        if value2.lower() == 'false':\n            value1 = 'none'\n        value1 = sizeTranslate[value1]\n        oMgr.setRcOption('report', 'sizedisplay', value1)\n        oMgr.clearRcOption('report', 'showsizedisplay')\n\n        # Change basic report options\n        reportTitle = oMgr.getRcOption('report', 'reporttitle')\n        oMgr.setRcOption('report', 'title', 'Duplicati Backup Summary Report')\n        oMgr.clearRcOption('report', 'reporttitle')\n        oMgr.setRcOption('report', 'columns', 'source:Source, destination:Destination, date: Date, time: Time, dupversion:Version, duration:Duration, examinedFiles:Files, examinedFilesDelta:+/-, sizeOfExaminedFiles:Size, fileSizeDelta:+/-, addedFiles:Added, deletedFiles:Deleted, modifiedFiles:Modified, filesWithError:Errors, parsedResult:Result, messages:Messages, warnings:Warnings, errors:Errors, logdata:Log Data')\n        oMgr.setRcOption('report', 'weminline', 'false')\n        moveOption(oMgr, 'report', 'subheadbg', 'report', 'groupheadingbg')\n       \n        # Set up report sections\n        mainReport = oMgr.getRcOption('report', 'style')    # This is the main report run\n        oMgr.clearRcOption('report', 'style')\n\n        # Add new sections using pre-defined defaults\n        oMgr.addRcSection('srcdest')\n        oMgr.addRcSection('bysrc')\n        oMgr.addRcSection('bydest')\n        oMgr.addRcSection('bydate')\n        oMgr.addRcSection('noactivity')\n        oMgr.addRcSection('lastseen')\n        for section, option, default, cancontinue in options.rcParts:\n            if section in ['srcdest','bysrc','bydest','bydate','noactivity','lastseen']:\n                oMgr.setRcOption(section, option, default)\n       \n        # Now, set the default report to mimic what was in the old format\n        oMgr.setRcOption(mainReport, 'title', reportTitle)\n        oMgr.setRcOption('report', 'layout', mainReport + ', noactivity')\n\n        # Update 'last seen' settings\n        value1 = oMgr.getRcOption('report', 'lastseensummary')\n        value2 = oMgr.getRcOption('report', 'lastseensummarytitle')\n        if value1.lower() != 'none':\n            oMgr.setRcOption('lastseen', 'title', value2)\n            value3 = oMgr.getRcOption('report', 'layout')\n            if value1.lower() == 'top':\n                oMgr.setRcOption('report', 'layout', 'lastseen, ' + value3)\n            else:\n                oMgr.setRcOption('report', 'layout', value3 + ', lastseen')\n        oMgr.clearRcOption('report', 'lastseensummary')\n        oMgr.clearRcOption('report', 'lastseensummarytitle')\n        \n        # Adjust field background colors\n        moveOption(oMgr, 'report', 'lastseenlow', 'report', 'normaldays')\n        moveOption(oMgr, 'report', 'lastseenlowcolor', 'report', 'normalbg')\n        moveOption(oMgr, 'report', 'lastseenmed', 'report', 'warningdays')\n        moveOption(oMgr, 'report', 'lastseenmedcolor', 'report', 'warningbg')\n        moveOption(oMgr, 'report', 'lastseenhighcolor', 'report', 'errorbg')\n\n        # Convert headings to new 'columns' format\n        headings = oMgr.getRcSection('headings')\n        columns = ''\n        colIndex = -1\n        for columnName in headings:\n            colIndex += 1\n            if headings[columnName] != '':\n                columns += v310Translate[columnName] + ':' + headings[columnName]\n                if colIndex < len(headings)-1:\n                    columns += ', '\n        if columns[-2:] == ', ':\n            columns = columns[:len(columns)-2:]\n        oMgr.setRcOption(mainReport, 'columns', columns)\n        oMgr.clearRcSection('headings')\n\n        # Change to new email server format\n        # Set [main]emailservers= option\n        oMgr.setRcOption('main', 'emailservers', 'incoming, outgoing')\n\n        # Move 'in*' to just '*'\n        protocol = oMgr.getRcOption('incoming', 'intransport')\n        oMgr.setRcOption('incoming', 'protocol', protocol)\n        oMgr.clearRcOption('incoming', 'intransport')        \n        oMgr.setRcOption('outgoing', 'protocol', 'smtp')\n\n        for option in ['inserver', 'inport', 'inencryption', 'inaccount', 'inpassword', 'infolder', 'inkeepalive']:\n            optVal = oMgr.getRcOption('incoming', option)\n            oMgr.setRcOption('incoming', option[2:], optVal)\n            oMgr.clearRcOption('incoming', option)\n        for option in ['outserver', 'outport', 'outencryption', 'outaccount', 'outpassword', 'outsender', 'outsendername', 'outreceiver', 'outkeepalive']:\n            optVal = oMgr.getRcOption('outgoing', option)\n            if optVal == None:\n                optVal = ''\n            oMgr.setRcOption('outgoing', option[3:], optVal)\n            oMgr.clearRcOption('outgoing', option)\n\n        # Move 'markread' to incoming\n        markread = oMgr.getRcOption('main', 'markread')\n        oMgr.setRcOption('incoming', 'markread', markread)\n        oMgr.clearRcOption('main', 'markread')\n\n        # Move logging levels\n        verbose = oMgr.getRcOption('main', 'verbose')\n        if verbose in ['1','2']:\n            verbose = '5'\n        elif verbose == '3':\n            verbose = '7'\n        oMgr.setRcOption('main', 'verbose', verbose)\n\n        # Add authentication methods\n        oMgr.setRcOption('incoming', 'authentication', 'basic')\n        oMgr.setRcOption('outgoing', 'authentication', 'basic')\n\n        # Update [apprise] section, if it exists. If it doesn't, default .rc routine will take care of it.\n        if oMgr.hasSection('apprise'):\n            oMgr.setRcOption('apprise', 'enabled', 'true')\n\n        oMgr.updateRc()\n        doConvertRc(oMgr, 310)\n    else:\n        pass\n\n    return None;", "fn_id": 2, "class_fn": false, "repo": "ekutner/dupReport", "file": "convert.py", "last_update_at": "2022-03-21T11:36:18+00:00", "question_id": "391280629f45849718bf50a45a483389e94997dc_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def doConvertRc(oMgr, fromVersion):\n    if fromVersion < 210:\n        globs.log.write(globs.SEV_NOTICE, function='Convert', action='doConvertRc', msg='Updating .rc file from version {} to version 210.'.format(fromVersion))\n        if oMgr.parser.has_section('main') is False:\n            globs.log.write(globs.SEV_DEBUG, function='Convert', action='convertRc', msg='Adding [main] section.')\n            oMgr.addRcSection('main')\n        if oMgr.parser.has_section('incoming') is False:\n            globs.log.write(globs.SEV_DEBUG, function='Convert', action='convertRc', msg='Adding [incoming] section.')\n            oMgr.addRcSection('incoming')\n        if oMgr.parser.has_section('outgoing') is False:\n            globs.log.write(globs.SEV_DEBUG, function='Convert', action='convertRc', msg='Adding [outgoing] section.')\n            oMgr.addRcSection('outgoing')\n        if oMgr.parser.has_section('report') is False:\n            globs.log.write(globs.SEV_DEBUG, function='Convert', action='convertRc', msg='Adding [report] section.')\n            oMgr.addRcSection('report')\n        if oMgr.parser.has_section('headings') is False:\n            globs.log.write(globs.SEV_DEBUG, function='Convert', action='convertRc', msg='Adding [headings] section.')\n            oMgr.addRcSection('headings')\n        for fromsection, fromoption, tosection, tooption in optList210:\n            moveOption(oMgr, fromsection, fromoption, tosection, tooption)\n        szDisp = oMgr.getRcOption('report', 'sizedisplay')\n        if szDisp == 'none':\n            oMgr.setRcOption('report', 'sizedisplay', 'byte')\n        oMgr.setRcOption('report', 'showsizedisplay', 'true')\n        oMgr.updateRc()\n        doConvertRc(oMgr, 210)\n    elif fromVersion < 300:\n        globs.log.write(globs.SEV_NOTICE, function='Convert', action='doConvertRc', msg='Updating .rc file from version {} to version 300.'.format(fromVersion))\n        if oMgr.parser.has_option('report', 'noactivitybg') == True:\n            oMgr.clearRcOption('report', 'noactivitybg')\n        if oMgr.parser.has_option('main', 'version') == True:\n            oMgr.clearRcOption('main', 'version')\n        oMgr.updateRc()\n        doConvertRc(oMgr, 300)\n    elif fromVersion < 310:\n        globs.log.write(globs.SEV_NOTICE, function='Convert', action='doConvertRc', msg='Updating .rc file from version {} to version 310.'.format(fromVersion))\n        value1 = oMgr.getRcOption('report', 'sizedisplay')\n        value2 = oMgr.getRcOption('report', 'showsizedisplay')\n        if value2.lower() == 'false':\n            value1 = 'none'\n        value1 = sizeTranslate[value1]\n        oMgr.setRcOption('report', 'sizedisplay', value1)\n        oMgr.clearRcOption('report', 'showsizedisplay')\n        reportTitle = oMgr.getRcOption('report', 'reporttitle')\n        oMgr.setRcOption('report', 'title', 'Duplicati Backup Summary Report')\n        oMgr.clearRcOption('report', 'reporttitle')\n        oMgr.setRcOption('report', 'columns', 'source:Source, destination:Destination, date: Date, time: Time, dupversion:Version, duration:Duration, examinedFiles:Files, examinedFilesDelta:+/-, sizeOfExaminedFiles:Size, fileSizeDelta:+/-, addedFiles:Added, deletedFiles:Deleted, modifiedFiles:Modified, filesWithError:Errors, parsedResult:Result, messages:Messages, warnings:Warnings, errors:Errors, logdata:Log Data')\n        oMgr.setRcOption('report', 'weminline', 'false')\n        moveOption(oMgr, 'report', 'subheadbg', 'report', 'groupheadingbg')\n        mainReport = oMgr.getRcOption('report', 'style')\n        oMgr.clearRcOption('report', 'style')\n        oMgr.addRcSection('srcdest')\n        oMgr.addRcSection('bysrc')\n        oMgr.addRcSection('bydest')\n        oMgr.addRcSection('bydate')\n        oMgr.addRcSection('noactivity')\n        oMgr.addRcSection('lastseen')\n        for section, option, default, cancontinue in options.rcParts:\n            if section in ['srcdest', 'bysrc', 'bydest', 'bydate', 'noactivity', 'lastseen']:\n                oMgr.setRcOption(section, option, default)\n        oMgr.setRcOption(mainReport, 'title', reportTitle)\n        oMgr.setRcOption('report', 'layout', mainReport + ', noactivity')\n        value1 = oMgr.getRcOption('report', 'lastseensummary')\n        value2 = oMgr.getRcOption('report', 'lastseensummarytitle')\n        if value1.lower() != 'none':\n            oMgr.setRcOption('lastseen', 'title', value2)\n            value3 = oMgr.getRcOption('report', 'layout')\n            if value1.lower() == 'top':\n                oMgr.setRcOption('report', 'layout', 'lastseen, ' + value3)\n            else:\n                oMgr.setRcOption('report', 'layout', value3 + ', lastseen')\n        oMgr.clearRcOption('report', 'lastseensummary')\n        oMgr.clearRcOption('report', 'lastseensummarytitle')\n        moveOption(oMgr, 'report', 'lastseenlow', 'report', 'normaldays')\n        moveOption(oMgr, 'report', 'lastseenlowcolor', 'report', 'normalbg')\n        moveOption(oMgr, 'report', 'lastseenmed', 'report', 'warningdays')\n        moveOption(oMgr, 'report', 'lastseenmedcolor', 'report', 'warningbg')\n        moveOption(oMgr, 'report', 'lastseenhighcolor', 'report', 'errorbg')\n        headings = oMgr.getRcSection('headings')\n        columns = ''\n        colIndex = -1\n        for columnName in headings:\n            colIndex += 1\n            if headings[columnName] != '':\n                columns += v310Translate[columnName] + ':' + headings[columnName]\n                if colIndex < len(headings) - 1:\n                    columns += ', '\n        if columns[-2:] == ', ':\n            columns = columns[:len(columns) - 2]\n        oMgr.setRcOption(mainReport, 'columns', columns)\n        oMgr.clearRcSection('headings')\n        oMgr.setRcOption('main', 'emailservers', 'incoming, outgoing')\n        protocol = oMgr.getRcOption('incoming', 'intransport')\n        oMgr.setRcOption('incoming', 'protocol', protocol)\n        oMgr.clearRcOption('incoming', 'intransport')\n        oMgr.setRcOption('outgoing', 'protocol', 'smtp')\n        for option in ['inserver', 'inport', 'inencryption', 'inaccount', 'inpassword', 'infolder', 'inkeepalive']:\n            optVal = oMgr.getRcOption('incoming', option)\n            oMgr.setRcOption('incoming', option[2:], optVal)\n            oMgr.clearRcOption('incoming', option)\n        for option in ['outserver', 'outport', 'outencryption', 'outaccount', 'outpassword', 'outsender', 'outsendername', 'outreceiver', 'outkeepalive']:\n            optVal = oMgr.getRcOption('outgoing', option)\n            if optVal == None:\n                optVal = ''\n            oMgr.setRcOption('outgoing', option[3:], optVal)\n            oMgr.clearRcOption('outgoing', option)\n        markread = oMgr.getRcOption('main', 'markread')\n        oMgr.setRcOption('incoming', 'markread', markread)\n        oMgr.clearRcOption('main', 'markread')\n        verbose = oMgr.getRcOption('main', 'verbose')\n        if verbose in ['1', '2']:\n            verbose = '5'\n        elif verbose == '3':\n            verbose = '7'\n        oMgr.setRcOption('main', 'verbose', verbose)\n        oMgr.setRcOption('incoming', 'authentication', 'basic')\n        oMgr.setRcOption('outgoing', 'authentication', 'basic')\n        if oMgr.hasSection('apprise'):\n            oMgr.setRcOption('apprise', 'enabled', 'true')\n        oMgr.updateRc()\n        doConvertRc(oMgr, 310)\n    else:\n        pass\n"]]}
{"hexsha": "d7a784e0be831f66b1bde2a4611d4ad199cc4143", "ext": "py", "lang": "Python", "content": "def showcreds():\n    \"\"\"Load and show credentials from db\"\"\"\n\n    conn = sqlite3.connect(dbfile)\n    with conn:\n        cur = conn.cursor()\n        cur.execute('SELECT * FROM Credentials ORDER BY url')\n        data = from_db_cursor(cur)\n\n    print()\n    print(colored(data, 'magenta'))\n    print()\n\n    cur.close()\n\n    while True:\n        response = input(colored(\" Back to menu or exit [menu/exit] \", 'yellow'))\n        if not response.isalpha():\n            continue\n        if response == 'menu' or response == 'exit':\n            break\n\n    if response == 'menu':\n        # Back to menu\n        os.system(\"clear\")\n        main()\n    else:\n        # Exit\n        os.system(\"clear\")\n        exit(0)", "fn_id": 2, "class_fn": false, "repo": "CalfCrusher/WPHunter", "file": "WPHunter.py", "last_update_at": "2022-03-16T12:20:31+00:00", "question_id": "d7a784e0be831f66b1bde2a4611d4ad199cc4143_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def showcreds():\n    \"\"\"Load and show credentials from db\"\"\"\n    conn = sqlite3.connect(dbfile)\n    with conn:\n        cur = conn.cursor()\n        cur.execute('SELECT * FROM Credentials ORDER BY url')\n        data = from_db_cursor(cur)\n    print()\n    print(colored(data, 'magenta'))\n    print()\n    cur.close()\n    while True:\n        response = input(colored(' Back to menu or exit [menu/exit] ', 'yellow'))\n        if not response.isalpha():\n            continue\n        if response == 'menu' or response == 'exit':\n            break\n    if response == 'menu':\n        os.system('clear')\n        main()\n    else:\n        os.system('clear')\n"]]}
{"hexsha": "640a2884cf23cb7d454440c26836b2a01294ac62", "ext": "py", "lang": "Python", "content": "def load_strains(db): \n  logger.info('Loading strains...')\n  andersen_strains = fetch_andersen_strains()\n  db.session.bulk_insert_mappings(Strain, andersen_strains)\n  db.session.commit()\n  logger.info(f\"Inserted {Strain.query.count()} strains\")", "fn_id": 0, "class_fn": false, "repo": "AndersenLab/CAENDR", "file": "src/pkg/caendr/caendr/services/sql/etl/strains.py", "last_update_at": "2022-03-11T02:46:35+00:00", "question_id": "640a2884cf23cb7d454440c26836b2a01294ac62_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_strains(db):\n    logger.info('Loading strains...')\n    andersen_strains = fetch_andersen_strains()\n    db.session.bulk_insert_mappings(Strain, andersen_strains)\n    db.session.commit()\n"]]}
{"hexsha": "09ab624fd34b487ce702e3a1bc93be04c9a92db1", "ext": "py", "lang": "Python", "content": "def list_rankings(wf):\n    # type: (Workflow3) -> ()\n\n    fiat = wf.settings.get('currency')\n\n    def _get():\n        api = CryptoCompareClient()\n        return api.get_top_market_cap(fiat, 10)\n\n    add_ticks_to_workflow(\n        wf,\n        wf.cached_data(\n            'market_cap_rankings_10', _get, max_age=3, session=True\n        ),\n    )", "fn_id": 6, "class_fn": false, "repo": "bskim45/alfred-coin-ticker", "file": "main.py", "last_update_at": "2022-03-23T11:13:32+00:00", "question_id": "09ab624fd34b487ce702e3a1bc93be04c9a92db1_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def list_rankings(wf):\n    fiat = wf.settings.get('currency')\n\n    def _get():\n        api = CryptoCompareClient()\n        return api.get_top_market_cap(fiat, 10)\n"]]}
{"hexsha": "5cd1dea68f2e347491dee7a10cd364506c37c0c3", "ext": "py", "lang": "Python", "content": "def testUserInput():\n    fileName = QtGui.QFileDialog.getOpenFileName(getMainWindow(), 'Open file',)\n    if fileName:\n        smp.OpenDataFile(fileName, guiName=os.path.basename(fileName))\n        smp.Show()\n        smp.ResetCamera()\n        smp.Render()", "fn_id": 3, "class_fn": false, "repo": "xj361685640/ParaView", "file": "Plugins/PythonQtPlugin/samples/demo.py", "last_update_at": "2022-03-26T07:48:07+00:00", "question_id": "5cd1dea68f2e347491dee7a10cd364506c37c0c3_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def testUserInput():\n    fileName = QtGui.QFileDialog.getOpenFileName(getMainWindow(), 'Open file')\n    if fileName:\n        smp.OpenDataFile(fileName, guiName=os.path.basename(fileName))\n        smp.Show()\n        smp.ResetCamera()\n"]]}
{"hexsha": "b75459388bec1684e8dba5668ad183ef8260c436", "ext": "py", "lang": "Python", "content": "def confirm(prompt='Confirm', default=False):\n    \"\"\"\n    https://code.activestate.com/recipes/541096-prompt-the-user-for-confirmation/\n    prompts for yes or no response from the user. Returns True for yes and False for no.\n\n    'resp' should be set to the default value assumed by the caller when user simply types ENTER.\n\n    >>> confirm(prompt='Create Directory?', default=True)\n    Create Directory? [y]|n:\n    True\n    >>> confirm(prompt='Create Directory?', default=False)\n    Create Directory? [n]|y:\n    False\n    >>> confirm(prompt='Create Directory?', default=False)\n    Create Directory? [n]|y: y\n    True\n\n    \"\"\"\n\n    if default:\n        prompt = '%s [%s]|%s: ' % (prompt, 'y', 'n')\n    else:\n        prompt = '%s [%s]|%s: ' % (prompt, 'n', 'y')\n\n    while True:\n        ans = input(prompt)\n        if not ans:\n            return default\n        if ans not in ['y', 'Y', 'n', 'N']:\n            print('please enter y or n.')\n            continue\n        if ans.lower() == 'y':\n            return True\n        if ans.lower() == 'n':\n            return False", "fn_id": 0, "class_fn": false, "repo": "unicef/un-partner-portal", "file": "backend/unpp_api/apps/common/utils.py", "last_update_at": "2022-02-12T16:51:48+00:00", "question_id": "b75459388bec1684e8dba5668ad183ef8260c436_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def confirm(prompt='Confirm', default=False):\n    \"\"\"\n    https://code.activestate.com/recipes/541096-prompt-the-user-for-confirmation/\n    prompts for yes or no response from the user. Returns True for yes and False for no.\n\n    'resp' should be set to the default value assumed by the caller when user simply types ENTER.\n\n    >>> confirm(prompt='Create Directory?', default=True)\n    Create Directory? [y]|n:\n    True\n    >>> confirm(prompt='Create Directory?', default=False)\n    Create Directory? [n]|y:\n    False\n    >>> confirm(prompt='Create Directory?', default=False)\n    Create Directory? [n]|y: y\n    True\n\n    \"\"\"\n    if default:\n        prompt = '%s [%s]|%s: ' % (prompt, 'y', 'n')\n    else:\n        prompt = '%s [%s]|%s: ' % (prompt, 'n', 'y')\n    while True:\n        ans = input(prompt)\n        if not ans:\n            return default\n        if ans not in ['y', 'Y', 'n', 'N']:\n            print('please enter y or n.')\n            continue\n        if ans.lower() == 'y':\n            return True\n        if ans.lower() == 'n':\n"]]}
{"hexsha": "62c05c106a7b4220e421a5e8dbd4ee6ac90c3b40", "ext": "py", "lang": "Python", "content": "def generate_ionstats_basecaller(\n    unmapped_bam_filenames, ionstats_basecaller_filename, library_key, histogram_length\n):\n\n    com = generate_ionstats_basecaller_cmd(\n        unmapped_bam_filenames,\n        ionstats_basecaller_filename,\n        library_key,\n        histogram_length,\n    )\n    try:\n        printtime(\"DEBUG: Calling '%s'\" % com)\n        subprocess.call(com, shell=True)\n    except Exception:\n        printtime(\"Failed ionstats basecaller\")\n        traceback.print_exc()", "fn_id": 1, "class_fn": false, "repo": "konradotto/TS", "file": "pipeline/python/ion/utils/ionstats.py", "last_update_at": "2022-03-22T17:15:59+00:00", "question_id": "62c05c106a7b4220e421a5e8dbd4ee6ac90c3b40_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def generate_ionstats_basecaller(unmapped_bam_filenames, ionstats_basecaller_filename, library_key, histogram_length):\n    com = generate_ionstats_basecaller_cmd(unmapped_bam_filenames, ionstats_basecaller_filename, library_key, histogram_length)\n    try:\n        printtime(\"DEBUG: Calling '%s'\" % com)\n        subprocess.call(com, shell=True)\n    except Exception:\n        printtime('Failed ionstats basecaller')\n"]]}
{"hexsha": "6f8ec2105f8f4269bedaa2795a1bb80004667fe2", "ext": "py", "lang": "Python", "content": "@celery_app.task(ignore_result=True, time_limit=600)\ndef do_refresh_config():\n    # \u5237\u65b0\u914d\u7f6e\n    inject.clear_and_configure(bind, bind_in_runtime=False)\n    config: Config = inject.instance(Config)\n    logger.info(f'run do_refresh_config done, config is {config}')", "fn_id": 0, "class_fn": false, "repo": "1995chen/jingdong_financial", "file": "app/tasks/async_tasks/refresh_config_task.py", "last_update_at": "2022-03-09T13:05:14+00:00", "question_id": "6f8ec2105f8f4269bedaa2795a1bb80004667fe2_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@celery_app.task(ignore_result=True, time_limit=600)\ndef do_refresh_config():\n    inject.clear_and_configure(bind, bind_in_runtime=False)\n    config: Config = inject.instance(Config)\n"]]}
{"hexsha": "953c1fb47142b706eed2dd69679c70b988f63382", "ext": "py", "lang": "Python", "content": "def setup_relu1():\n    # Construct a simple ReLU model with 2 hidden layers\n    dtype = torch.float64\n    relu_param = torch.tensor([\n        -1, 0.5, -0.3, 0.74, -2, 1.5, -0.5, 0.2, -1, -0.5, 1.5, 1.2, 2, -1.5,\n        2.6, 0.3, -2, -0.3, -.4, -0.1, 0.2, -0.5, 1.2, 1.3, -.4, .5, -.6, 0.3\n    ],\n                              dtype=dtype)\n    return setup_relu((2, 4, 4), relu_param, negative_gradient=0.1, bias=False)", "fn_id": 1, "class_fn": false, "repo": "hongkai-dai/neural-network-lyapunov-1", "file": "neural_network_lyapunov/test/train_discrete_linear_system_toy_lyapunov.py", "last_update_at": "2022-03-31T14:35:23+00:00", "question_id": "953c1fb47142b706eed2dd69679c70b988f63382_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def setup_relu1():\n    dtype = torch.float64\n    relu_param = torch.tensor([-1, 0.5, -0.3, 0.74, -2, 1.5, -0.5, 0.2, -1, -0.5, 1.5, 1.2, 2, -1.5, 2.6, 0.3, -2, -0.3, -0.4, -0.1, 0.2, -0.5, 1.2, 1.3, -0.4, 0.5, -0.6, 0.3], dtype=dtype)\n"]]}
{"hexsha": "d240999b626a346aa9ec07fef8022c82d7a2029d", "ext": "py", "lang": "Python", "content": "def _ensure_dict(item):\n    if type(item) == str:\n        return ast.literal_eval(item)\n    else:\n        return item", "fn_id": 4, "class_fn": false, "repo": "cogtoolslab/tools_block_construction", "file": "analysis/utils/analysis_helper.py", "last_update_at": "2022-02-19T00:04:14+00:00", "question_id": "d240999b626a346aa9ec07fef8022c82d7a2029d_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _ensure_dict(item):\n    if type(item) == str:\n        return ast.literal_eval(item)\n    else:\n"]]}
{"hexsha": "62466fb0cd5a3d0a4c3396d9d64dfc18c359c381", "ext": "py", "lang": "Python", "content": "def _f_fit_ ( func , histo , *args ) :\n    \"\"\"Fit histogram (Actually delegate to TH1::Fit method)\n    >>> func  = ...\n    >>> histo = ...\n    >>> func.Fit ( histo , .... )\n    \"\"\"\n    return histo.Fit( func , *args )", "fn_id": 1, "class_fn": false, "repo": "TatianaOvsiannikova/ostap", "file": "ostap/fitting/funcs.py", "last_update_at": "2022-02-21T05:00:57+00:00", "question_id": "62466fb0cd5a3d0a4c3396d9d64dfc18c359c381_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _f_fit_(func, histo, *args):\n    \"\"\"Fit histogram (Actually delegate to TH1::Fit method)\n    >>> func  = ...\n    >>> histo = ...\n    >>> func.Fit ( histo , .... )\n    \"\"\"\n"]]}
{"hexsha": "9b51df0f8e67c8296dac3ab781a3f6fcb65d76ed", "ext": "py", "lang": "Python", "content": "@Function\ndef check_pending_async(args):\n    if globals.pendingAsync is None or not len(globals.pendingAsync): return Object.fromBoolean(False)\n    return Object.fromBoolean(True)", "fn_id": 12, "class_fn": false, "repo": "soIu/rpython", "file": "javascript/emscripten.py", "last_update_at": "2022-01-23T18:04:42+00:00", "question_id": "9b51df0f8e67c8296dac3ab781a3f6fcb65d76ed_12", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@Function\ndef check_pending_async(args):\n    if globals.pendingAsync is None or not len(globals.pendingAsync):\n        return Object.fromBoolean(False)\n"]]}
{"hexsha": "7c32fc45129013b747e35793bb7d424a1908b65c", "ext": "py", "lang": "Python", "content": "def createMeetCloseHelper(pollType):\n    \n    jsonData =json.loads(request.get_data())\n    #GET DATA FROM FRONT END#\n    groupName = jsonData[\"groupName\"]\n    pollData = {}\n    pollData[\"pollCreator\"] = jsonData[\"pollCreator\"]\n    pollData[\"pollTitle\"] = jsonData[\"pollTitle\"]\n    pollData[\"pollPrompt\"] = jsonData[\"pollPrompt\"]\n    pollData[\"pollType\"] = pollType\n    pollData[\"uuid\"] = str(uuid.uuid4())\n    pollData[\"pollStatus\"] = \"ACTIVE\"\n    pollData[\"pollVoteOptionsList\"] = jsonData[\"pollVoteOptions\"]\n    pollVoteOptions = {}\n    for option in jsonData[\"pollVoteOptions\"]:\n        pollVoteOptions[option] = 0\n    pollData[\"pollVoteOptions\"] = pollVoteOptions\n    pollData[\"voters\"] = []\n    pollData[\"result\"] = None\n    #\n\n    #SQL CONNECTION\n    connection = sqlite3.connect(r\"./database.db\")\n    cursor = connection.cursor()\n\n    cursor.execute(\"SELECT * FROM  groups WHERE [groupName] = ?\",(groupName,))\n    groupData = list(cursor.fetchone())\n\n    groupPolls = json.loads(groupData[4])\n    groupPolls.append(pollData)\n    groupPolls = json.dumps(groupPolls)\n    groupData[4] = groupPolls\n\n    cursor.execute(\"DELETE FROM groups WHERE [groupName] = ?\",(groupName,))\n    cursor.execute(\"INSERT INTO groups (groupName,status,posts,memberpolls,groupPolls,members) VALUES(?,?,?,?,?,?)\",tuple(groupData))\n    connection.commit()\n    connection.close()", "fn_id": 12, "class_fn": false, "repo": "sajadgzd/softwareEngineeringProject", "file": "test.py", "last_update_at": "2022-01-12T22:14:08+00:00", "question_id": "7c32fc45129013b747e35793bb7d424a1908b65c_12", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def createMeetCloseHelper(pollType):\n    jsonData = json.loads(request.get_data())\n    groupName = jsonData['groupName']\n    pollData = {}\n    pollData['pollCreator'] = jsonData['pollCreator']\n    pollData['pollTitle'] = jsonData['pollTitle']\n    pollData['pollPrompt'] = jsonData['pollPrompt']\n    pollData['pollType'] = pollType\n    pollData['uuid'] = str(uuid.uuid4())\n    pollData['pollStatus'] = 'ACTIVE'\n    pollData['pollVoteOptionsList'] = jsonData['pollVoteOptions']\n    pollVoteOptions = {}\n    for option in jsonData['pollVoteOptions']:\n        pollVoteOptions[option] = 0\n    pollData['pollVoteOptions'] = pollVoteOptions\n    pollData['voters'] = []\n    pollData['result'] = None\n    connection = sqlite3.connect('./database.db')\n    cursor = connection.cursor()\n    cursor.execute('SELECT * FROM  groups WHERE [groupName] = ?', (groupName,))\n    groupData = list(cursor.fetchone())\n    groupPolls = json.loads(groupData[4])\n    groupPolls.append(pollData)\n    groupPolls = json.dumps(groupPolls)\n    groupData[4] = groupPolls\n    cursor.execute('DELETE FROM groups WHERE [groupName] = ?', (groupName,))\n    cursor.execute('INSERT INTO groups (groupName,status,posts,memberpolls,groupPolls,members) VALUES(?,?,?,?,?,?)', tuple(groupData))\n    connection.commit()\n"]]}
{"hexsha": "20aae0fcd0a8197d27c13553f3036d33b61e9213", "ext": "py", "lang": "Python", "content": "def _run_task(task):\n    \"\"\"Free function wrapping the Task runner.\n\n    It needs to be free because bound instancemethods can't be pickled for multiprocessing.\n    \"\"\"\n    task._run()", "fn_id": 4, "class_fn": false, "repo": "ska-sa/katsdpcal", "file": "katsdpcal/control.py", "last_update_at": "2022-01-12T13:19:05+00:00", "question_id": "20aae0fcd0a8197d27c13553f3036d33b61e9213_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _run_task(task):\n    \"\"\"Free function wrapping the Task runner.\n\n    It needs to be free because bound instancemethods can't be pickled for multiprocessing.\n    \"\"\"\n"]]}
{"hexsha": "cfb4f274b6427f7e0848af2f473761abab2a00fb", "ext": "py", "lang": "Python", "content": "def test_schemas():\n  modules = look_for_schemas_dirs_tests()\n  for (module, schema, files) in modules:\n    for (isValid, path, data) in files:\n      try:\n        validate(data, schema)\n        if not isValid:\n          raise Exception(\"Validation for {} should have failed\".format(path))\n      except ValidationError:\n        if isValid:\n          raise", "fn_id": 0, "class_fn": false, "repo": "rafaelcavazin/cloud-foundation-toolkit", "file": "dm/tests/templates/test_schemas.py", "last_update_at": "2022-03-30T14:44:18+00:00", "question_id": "cfb4f274b6427f7e0848af2f473761abab2a00fb_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_schemas():\n    modules = look_for_schemas_dirs_tests()\n    for module, schema, files in modules:\n        for isValid, path, data in files:\n            try:\n                validate(data, schema)\n                if not isValid:\n                    raise Exception('Validation for {} should have failed'.format(path))\n            except ValidationError:\n                if isValid:\n"]]}
{"hexsha": "406e17d5ac00499f0b9089fab72ff6712a066708", "ext": "py", "lang": "Python", "content": "def thread_run_query(query, barrier):\n    env = Env(decodeResponses=True)\n    conn = env.getConnection()\n    graph = Graph(GRAPH_ID, conn)\n\n    if barrier is not None:\n        barrier.wait()\n    \n    try:\n        result = graph.query(query)\n        return { \"result_set\": result.result_set, \n            \"nodes_created\": result.nodes_created, \n            \"properties_set\": result.properties_set }\n    except ResponseError as e:\n        return str(e)", "fn_id": 0, "class_fn": false, "repo": "LiorKogan/RedisGraph", "file": "tests/flow/test_concurrent_query.py", "last_update_at": "2022-03-31T19:36:19+00:00", "question_id": "406e17d5ac00499f0b9089fab72ff6712a066708_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def thread_run_query(query, barrier):\n    env = Env(decodeResponses=True)\n    conn = env.getConnection()\n    graph = Graph(GRAPH_ID, conn)\n    if barrier is not None:\n        barrier.wait()\n    try:\n        result = graph.query(query)\n        return {'result_set': result.result_set, 'nodes_created': result.nodes_created, 'properties_set': result.properties_set}\n    except ResponseError as e:\n"]]}
{"hexsha": "54de69c10ff19fc60274fe2184fd43c2dc2dcc57", "ext": "py", "lang": "Python", "content": "def test_band_edge_w_options():\n    parsed_args = parse_args([\"be\",\n                              \"--vasprun\", \"vasprun_1\",\n                              \"--outcar\", \"OUTCAR_1\",\n                              ])\n\n    expected = Namespace(\n        vasprun=Path(\"vasprun_1\"),\n        outcar=Path(\"OUTCAR_1\"),\n        func=parsed_args.func,\n    )\n\n    assert parsed_args == expected", "fn_id": 13, "class_fn": false, "repo": "kumagai-group/vise", "file": "vise/tests/cli/test_main.py", "last_update_at": "2022-03-04T13:39:30+00:00", "question_id": "54de69c10ff19fc60274fe2184fd43c2dc2dcc57_13", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_band_edge_w_options():\n    parsed_args = parse_args(['be', '--vasprun', 'vasprun_1', '--outcar', 'OUTCAR_1'])\n    expected = Namespace(vasprun=Path('vasprun_1'), outcar=Path('OUTCAR_1'), func=parsed_args.func)\n"]]}
{"hexsha": "51e25329c7334ff7517cd47e4485b252f4feee9d", "ext": "py", "lang": "Python", "content": "def make_encryptor(plaintext_type, pk_rcv_type, sk_snd_type):\n  @tff.tf_computation(plaintext_type, pk_rcv_type, sk_snd_type)\n  def encrypt_tensor(plaintext, pk_rcv, sk_snd):\n    pk_rcv = easy_box.PublicKey(pk_rcv)\n    sk_snd = easy_box.SecretKey(sk_snd)\n    nonce = easy_box.gen_nonce()\n    ciphertext, mac = easy_box.seal_detached(plaintext, nonce, pk_rcv, sk_snd)\n    return ciphertext.raw, mac.raw, nonce.raw\n\n  return encrypt_tensor", "fn_id": 0, "class_fn": false, "repo": "tf-encrypted/federated-aggregations", "file": "federated_aggregations/channels/computations.py", "last_update_at": "2022-01-08T20:32:07+00:00", "question_id": "51e25329c7334ff7517cd47e4485b252f4feee9d_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def make_encryptor(plaintext_type, pk_rcv_type, sk_snd_type):\n\n    @tff.tf_computation(plaintext_type, pk_rcv_type, sk_snd_type)\n    def encrypt_tensor(plaintext, pk_rcv, sk_snd):\n        pk_rcv = easy_box.PublicKey(pk_rcv)\n        sk_snd = easy_box.SecretKey(sk_snd)\n        nonce = easy_box.gen_nonce()\n        ciphertext, mac = easy_box.seal_detached(plaintext, nonce, pk_rcv, sk_snd)\n        return (ciphertext.raw, mac.raw, nonce.raw)\n"]]}
{"hexsha": "a0d20afe0549e14c55c4ce0b0a5034b779526488", "ext": "py", "lang": "Python", "content": "def sweep_render_dir(render_dir: str):\n    \"\"\"Checks to see if the rendering directory exists. If it does, it removes\n    it and recreates a new rendering directory.\n    \"\"\"\n    if os.path.exists(render_dir):\n        shutil.rmtree(render_dir)\n    os.mkdir(render_dir)", "fn_id": 6, "class_fn": false, "repo": "tedhtchang/modelmesh-performance", "file": "perf_test/scripts/renderer.py", "last_update_at": "2022-01-10T19:08:57+00:00", "question_id": "a0d20afe0549e14c55c4ce0b0a5034b779526488_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def sweep_render_dir(render_dir: str):\n    \"\"\"Checks to see if the rendering directory exists. If it does, it removes\n    it and recreates a new rendering directory.\n    \"\"\"\n    if os.path.exists(render_dir):\n        shutil.rmtree(render_dir)\n"]]}
{"hexsha": "efd9f141956a681544d2a8c593883fc80424006a", "ext": "py", "lang": "Python", "content": "async def test_connect_via_socket(smtp_client, hostname, smtpd_server_port):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n        sock.connect((hostname, smtpd_server_port))\n\n        await smtp_client.connect(hostname=None, port=None, sock=sock)\n        response = await smtp_client.ehlo()\n\n    assert response.code == SMTPStatus.completed", "fn_id": 22, "class_fn": false, "repo": "P-EB/aiosmtplib", "file": "tests/test_connect.py", "last_update_at": "2022-03-17T19:52:54+00:00", "question_id": "efd9f141956a681544d2a8c593883fc80424006a_22", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def test_connect_via_socket(smtp_client, hostname, smtpd_server_port):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n        sock.connect((hostname, smtpd_server_port))\n        await smtp_client.connect(hostname=None, port=None, sock=sock)\n        response = await smtp_client.ehlo()\n"]]}
{"hexsha": "9556a52a27831318174d1ec6ce8484e380df9eb4", "ext": "py", "lang": "Python", "content": "def train_model_lenet(num_convs, num_linear, l_rate, batch_size, device):\n    transform_train = transforms.Compose([transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n\n    net = LeNetFamily(num_convs, num_linear)\n    net = net.to(device)\n    criterion = nn.CrossEntropyLoss()\n\n    optimizer = optim.SGD(net.parameters(), lr=l_rate, momentum=0.9)\n    \n    model_name = \"nc_{}__nl_{}__bs_{}__lr_{}\".format(num_convs, num_linear, batch_size, l_rate)\n    path = \"{}{}\".format(\"./models/\", model_name)\n    \n    \n    EITER = 500\n    for epoch in range(51):\n        model_path = \"{}__epoch_{}.pth\".format(path, epoch)\n        running_loss = 0.0\n        for i,data in enumerate(trainloader, 0):\n            inputs, labels = data[0].to(device), data[1].to(device)\n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            last_path = \"{}__last_epoch__step_{}\".format(path, i)\n            if i % EITER == 1:\n                print(\"Loss @ [Epoch:{} Inter:{}] is {}\".format(epoch, i, running_loss/EITER))\n                running_loss = 0.0\n            if epoch > 49:\n                torch.save(net.state_dict(), last_path)\n        if epoch % 5 == 0:\n            torch.save(net.state_dict(), model_path)", "fn_id": 0, "class_fn": false, "repo": "makinzm/Hausdorff-Dimension-and-Generalization", "file": "src/train_lenet.py", "last_update_at": "2022-01-29T22:54:04+00:00", "question_id": "9556a52a27831318174d1ec6ce8484e380df9eb4_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def train_model_lenet(num_convs, num_linear, l_rate, batch_size, device):\n    transform_train = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.201))])\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n    net = LeNetFamily(num_convs, num_linear)\n    net = net.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=l_rate, momentum=0.9)\n    model_name = 'nc_{}__nl_{}__bs_{}__lr_{}'.format(num_convs, num_linear, batch_size, l_rate)\n    path = '{}{}'.format('./models/', model_name)\n    EITER = 500\n    for epoch in range(51):\n        model_path = '{}__epoch_{}.pth'.format(path, epoch)\n        running_loss = 0.0\n        for i, data in enumerate(trainloader, 0):\n            inputs, labels = (data[0].to(device), data[1].to(device))\n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            last_path = '{}__last_epoch__step_{}'.format(path, i)\n            if i % EITER == 1:\n                print('Loss @ [Epoch:{} Inter:{}] is {}'.format(epoch, i, running_loss / EITER))\n                running_loss = 0.0\n            if epoch > 49:\n                torch.save(net.state_dict(), last_path)\n        if epoch % 5 == 0:\n"]]}
{"hexsha": "4a9b46d66378f5fe1cb4b07da4884761faa854f9", "ext": "py", "lang": "Python", "content": "def save_adata_json(adata, schema, output_directory):\n    logger.info('Save adata')\n    os.makedirs(output_directory, exist_ok=True)\n    with open(os.path.join(output_directory, 'schema.json'), 'wt') as f:\n        # json.dump(result, f)\n        f.write(ujson.dumps(schema, double_precision=2, orient='values'))\n\n    save_adata_X(adata, output_directory)\n    save_data_obs(adata, output_directory)\n    save_data_obsm(adata, output_directory)", "fn_id": 1, "class_fn": false, "repo": "jkanche/cirrocumulus", "file": "cirrocumulus/json_io.py", "last_update_at": "2022-02-06T23:07:07+00:00", "question_id": "4a9b46d66378f5fe1cb4b07da4884761faa854f9_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def save_adata_json(adata, schema, output_directory):\n    logger.info('Save adata')\n    os.makedirs(output_directory, exist_ok=True)\n    with open(os.path.join(output_directory, 'schema.json'), 'wt') as f:\n        f.write(ujson.dumps(schema, double_precision=2, orient='values'))\n    save_adata_X(adata, output_directory)\n    save_data_obs(adata, output_directory)\n"]]}
{"hexsha": "0c34c74254adca1ea72841213ddd4c11d1ac144a", "ext": "py", "lang": "Python", "content": "def profile_cpu_bound_program():\n    real_dog = DogStatsApi()\n    real_dog.reporter = NullReporter()\n    fake_dog = NullDogStatsApi()\n    for type_, dog in [('real', real_dog), ('fake', fake_dog)]:\n        print('\\n\\n\\nTESTING %s\\n\\n' % type_)\n        dog.start()\n        program = CPUBoundProgram(dog)\n        yappi.start()\n        program.run()\n        yappi.print_stats(sort_type=yappi.SORTTYPE_TSUB, sort_order=yappi.SORTORDER_DESC)\n        yappi.stop()\n        yappi.clear_stats()", "fn_id": 0, "class_fn": false, "repo": "autrilla/dogapi", "file": "tests/performance/test_stats_api_performance.py", "last_update_at": "2022-01-31T03:49:54+00:00", "question_id": "0c34c74254adca1ea72841213ddd4c11d1ac144a_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def profile_cpu_bound_program():\n    real_dog = DogStatsApi()\n    real_dog.reporter = NullReporter()\n    fake_dog = NullDogStatsApi()\n    for type_, dog in [('real', real_dog), ('fake', fake_dog)]:\n        print('\\n\\n\\nTESTING %s\\n\\n' % type_)\n        dog.start()\n        program = CPUBoundProgram(dog)\n        yappi.start()\n        program.run()\n        yappi.print_stats(sort_type=yappi.SORTTYPE_TSUB, sort_order=yappi.SORTORDER_DESC)\n        yappi.stop()\n"]]}
{"hexsha": "ca1ace6fee57dd9d4e4378b1a46a68e1094fe02c", "ext": "py", "lang": "Python", "content": "def presidual_layer(tparams, state_below, options, prefix='presidual', mask=None,\n                    one_step=False, init_state=None, **kwargs):\n    '''\n    parametric residual layer (recurrent depth adjustable)\n    parametric vector on identity connection\n    '''\n    if one_step:\n        assert init_state, 'previous state must be provided'\n\n    # here state_below in x_emb\n    nsteps = state_below.shape[0]\n    depth = options['unit_depth']\n    dim = options['dim_proj']\n    if state_below.ndim == 3:\n        n_samples = state_below.shape[1]\n    else:\n        n_samples = 1\n\n    if mask is None:\n        mask = tensor.alloc(1., state_below.shape[0], 1)\n\n    def _slice(_x, n, dim):\n        if _x.ndim == 3:\n            return _x[:, :, n * dim:(n + 1) * dim]\n        return _x[:, n * dim:(n + 1) * dim]\n\n    # input mask, x(t), h(t-1)\n    def _presblock(m_, x_, h_):\n        y = h_\n        for idx in xrange(depth):\n            hy = tensor.dot(y, tparams[_p(prefix, 'U'+str(idx+1))])\n            y = tensor.nnet.sigmoid(_slice(x_, idx, dim) + hy)\n        # p = 2*sigmoid(wh(t-1)+b)-1\n        p = 2 * tensor.nnet.sigmoid(tensor.dot(h_, tparams[_p(prefix, 'w_res')]) + tparams[_p(prefix, 'b_res')]) - 1\n        p_vec = p.reshape(p.shape[0], 1)\n        # h(t) = tanh(ph(t-1)+y)\n        h = tensor.tanh(tensor.dot(tensor.nlinalg.alloc_diag(p_vec), h_) + y)\n        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n        return h\n\n    # state_below = W*x(t)+b (for all inter_state y)\n    state_below = (tensor.dot(state_below, tparams[_p(prefix, 'W')]) +\n                   tparams[_p(prefix, 'b')])\n\n    if init_state is None:\n        init_state = tensor.alloc(numpy_floatX(0.), n_samples, dim)\n\n    if one_step:\n        rval = _presblock(mask, state_below, init_state)\n    else:\n        rval, updates = theano.scan(_presblock,\n                                    sequences=[mask, state_below],\n                                    outputs_info=[init_state],\n                                    name=_p(prefix, 'layers'),\n                                    n_steps=nsteps)\n    # rval = [rval] # note: for consistency among model layers\n    return rval", "fn_id": 17, "class_fn": false, "repo": "Bhaskers-Blu-Org2/DualLearning", "file": "DSL_SentimentAnalysis/Classifier/Layers.py", "last_update_at": "2022-02-03T23:50:53+00:00", "question_id": "ca1ace6fee57dd9d4e4378b1a46a68e1094fe02c_17", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def presidual_layer(tparams, state_below, options, prefix='presidual', mask=None, one_step=False, init_state=None, **kwargs):\n    \"\"\"\n    parametric residual layer (recurrent depth adjustable)\n    parametric vector on identity connection\n    \"\"\"\n    if one_step:\n        assert init_state, 'previous state must be provided'\n    nsteps = state_below.shape[0]\n    depth = options['unit_depth']\n    dim = options['dim_proj']\n    if state_below.ndim == 3:\n        n_samples = state_below.shape[1]\n    else:\n        n_samples = 1\n    if mask is None:\n        mask = tensor.alloc(1.0, state_below.shape[0], 1)\n\n    def _slice(_x, n, dim):\n        if _x.ndim == 3:\n            return _x[:, :, n * dim:(n + 1) * dim]\n        return _x[:, n * dim:(n + 1) * dim]\n\n    def _presblock(m_, x_, h_):\n        y = h_\n        for idx in xrange(depth):\n            hy = tensor.dot(y, tparams[_p(prefix, 'U' + str(idx + 1))])\n            y = tensor.nnet.sigmoid(_slice(x_, idx, dim) + hy)\n        p = 2 * tensor.nnet.sigmoid(tensor.dot(h_, tparams[_p(prefix, 'w_res')]) + tparams[_p(prefix, 'b_res')]) - 1\n        p_vec = p.reshape(p.shape[0], 1)\n        h = tensor.tanh(tensor.dot(tensor.nlinalg.alloc_diag(p_vec), h_) + y)\n        h = m_[:, None] * h + (1.0 - m_)[:, None] * h_\n        return h\n    state_below = tensor.dot(state_below, tparams[_p(prefix, 'W')]) + tparams[_p(prefix, 'b')]\n    if init_state is None:\n        init_state = tensor.alloc(numpy_floatX(0.0), n_samples, dim)\n    if one_step:\n        rval = _presblock(mask, state_below, init_state)\n    else:\n        rval, updates = theano.scan(_presblock, sequences=[mask, state_below], outputs_info=[init_state], name=_p(prefix, 'layers'), n_steps=nsteps)\n"]]}
{"hexsha": "6cadce240fbf8b19411c5a0ff75b5b3799d0bbfa", "ext": "py", "lang": "Python", "content": "def _resnet(rng, arch, block, layers, pretrained, **kwargs):\n  resnet = ResNet(block=block, layers=layers, **kwargs)\n\n  if pretrained:\n    torch_params = utils.load_torch_params(model_urls[arch])\n    flax_params = FrozenDict(utils.torch_to_linen(torch_params, _get_flax_keys))\n  else:\n    init_batch = jnp.ones((1, 224, 224, 3), jnp.float32)\n    flax_params = ResNet(block=block, layers=layers, **kwargs).init(rng, init_batch)\n\n  return resnet, flax_params", "fn_id": 1, "class_fn": false, "repo": "rolandgvc/flaxvision", "file": "flaxvision/models/resnet.py", "last_update_at": "2022-03-25T10:01:18+00:00", "question_id": "6cadce240fbf8b19411c5a0ff75b5b3799d0bbfa_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _resnet(rng, arch, block, layers, pretrained, **kwargs):\n    resnet = ResNet(block=block, layers=layers, **kwargs)\n    if pretrained:\n        torch_params = utils.load_torch_params(model_urls[arch])\n        flax_params = FrozenDict(utils.torch_to_linen(torch_params, _get_flax_keys))\n    else:\n        init_batch = jnp.ones((1, 224, 224, 3), jnp.float32)\n        flax_params = ResNet(block=block, layers=layers, **kwargs).init(rng, init_batch)\n"]]}
{"hexsha": "149b81015b3e5cf297b77c6df214ea8f24d3192e", "ext": "py", "lang": "Python", "content": "def test_stream_forms_configured(requests_mock, config, form_response):\n    requests_mock.register_uri(\"GET\", TYPEFORM_BASE_URL + \"forms/u6nXL7\", form_response)\n    requests_mock.register_uri(\"GET\", TYPEFORM_BASE_URL + \"forms/k9xNV4\", form_response)\n\n    stream = Forms(authenticator=MagicMock(), **config)\n\n    merged_records = merge_records(stream, SyncMode.full_refresh)\n\n    assert len(merged_records) == 2", "fn_id": 1, "class_fn": false, "repo": "kattos-aws/airbyte", "file": "airbyte-integrations/connectors/source-typeform/unit_tests/test_streams.py", "last_update_at": "2022-03-16T22:53:06+00:00", "question_id": "149b81015b3e5cf297b77c6df214ea8f24d3192e_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_stream_forms_configured(requests_mock, config, form_response):\n    requests_mock.register_uri('GET', TYPEFORM_BASE_URL + 'forms/u6nXL7', form_response)\n    requests_mock.register_uri('GET', TYPEFORM_BASE_URL + 'forms/k9xNV4', form_response)\n    stream = Forms(authenticator=MagicMock(), **config)\n    merged_records = merge_records(stream, SyncMode.full_refresh)\n"]]}
{"hexsha": "b706e640a9917675466c2306fe364d4663b81823", "ext": "py", "lang": "Python", "content": "@pytest.mark.asyncio\nasync def test_get_status():\n    result = await test.get_status()\n    await handler.save_result('test_get_status', result)", "fn_id": 4, "class_fn": false, "repo": "opengsq/opengsq-python", "file": "tests/protocols/test_gamespy1.py", "last_update_at": "2022-01-10T17:15:51+00:00", "question_id": "b706e640a9917675466c2306fe364d4663b81823_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.asyncio\nasync def test_get_status():\n    result = await test.get_status()\n"]]}
{"hexsha": "885cd0f4c410108e3f69319f24eaa06c4e0d9ec6", "ext": "py", "lang": "Python", "content": "def run(args=None):\n    \"\"\" The main routine. \"\"\"\n    cfg = Config()\n    cfg.configure_logger()\n    devices = {device.name: device for device in [create_device(c) for c in cfg.config['accelerometers']]}\n    endpoint = TCP4ServerEndpoint(reactor, cfg.port)\n    logger.info(f\"Listening on port {cfg.port}\")\n    endpoint.listen(CommandFactory(devices))\n    reactor.run()", "fn_id": 1, "class_fn": false, "repo": "3ll3d00d/qvibe-recorder", "file": "qvibe/app.py", "last_update_at": "2022-02-11T20:40:10+00:00", "question_id": "885cd0f4c410108e3f69319f24eaa06c4e0d9ec6_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def run(args=None):\n    \"\"\" The main routine. \"\"\"\n    cfg = Config()\n    cfg.configure_logger()\n    devices = {device.name: device for device in [create_device(c) for c in cfg.config['accelerometers']]}\n    endpoint = TCP4ServerEndpoint(reactor, cfg.port)\n    logger.info(f'Listening on port {cfg.port}')\n    endpoint.listen(CommandFactory(devices))\n"]]}
{"hexsha": "3da6d404c3beb0b79e4ede613b6f84739f6231c5", "ext": "py", "lang": "Python", "content": "def test_set_vars():\n    candidate = CXStandardProfile()\n\n    with pytest.raises(TypeError):\n        candidate.vars = 123\n\n    candidate.vars = [\"a\", \"b\", \"c\"]\n    assert candidate.vars == [\"a\", \"b\", \"c\"]\n    assert candidate.y[VARS] == [\"a\", \"b\", \"c\"]\n\n    candidate.vars = None\n    assert candidate.vars == []\n    assert candidate.y[VARS] == []", "fn_id": 0, "class_fn": false, "repo": "docinfosci/canvasxpress-python", "file": "tests/unit/test_CXStandardProfile.py", "last_update_at": "2022-02-01T19:07:01+00:00", "question_id": "3da6d404c3beb0b79e4ede613b6f84739f6231c5_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_set_vars():\n    candidate = CXStandardProfile()\n    with pytest.raises(TypeError):\n        candidate.vars = 123\n    candidate.vars = ['a', 'b', 'c']\n    assert candidate.vars == ['a', 'b', 'c']\n    assert candidate.y[VARS] == ['a', 'b', 'c']\n    candidate.vars = None\n    assert candidate.vars == []\n"]]}
{"hexsha": "d1ead9ab375e92b80456d2755130fc686af9747a", "ext": "py", "lang": "Python", "content": "def run_main():\n\n    ap = argparse.ArgumentParser()\n    ap.add_argument('-d', '--dataset',    type=str, default='./dataset', help='root folder of dataset')\n    ap.add_argument('-g', '--graph',      type=str, default='./freeze/frozen_graph.pb', help='graph file (.pb) to be evaluated.')\n    ap.add_argument('-i', '--input_node', type=str, default='input_1', help='input node.')\n    ap.add_argument('-o', '--output_node',type=str, default='flatten_1/Reshape', help='output node.') \n    ap.add_argument('-b', '--batchsize',  type=int, default=1, help='Evaluation batchsize. Default is 1.') \n    args = ap.parse_args()\n\n    print('\\n'+DIVIDER)\n    print('Keras version      : ',tf.keras.__version__)\n    print('TensorFlow version : ',tf.__version__)\n    print(sys.version)\n    print(DIVIDER)\n    print(' Command line options:')\n    print ('--dataset     : ',args.dataset)\n    print ('--graph       : ',args.graph)\n    print ('--input_node  : ',args.input_node)\n    print ('--output_node : ',args.output_node)\n    print ('--batchsize   : ',args.batchsize)\n    print(DIVIDER)\n\n    input_graph_def = tf.Graph().as_graph_def()\n    input_graph_def.ParseFromString(tf.io.gfile.GFile(args.graph, \"rb\").read())\n\n    graph_eval(input_graph_def, args.input_node, args.output_node, args.dataset, args.batchsize)", "fn_id": 1, "class_fn": false, "repo": "mkolod/Vitis-Tutorials", "file": "Machine_Learning/Feature_Tutorials/03-edge-to-cloud/files/eval_graph.py", "last_update_at": "2022-03-15T22:07:18+00:00", "question_id": "d1ead9ab375e92b80456d2755130fc686af9747a_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def run_main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument('-d', '--dataset', type=str, default='./dataset', help='root folder of dataset')\n    ap.add_argument('-g', '--graph', type=str, default='./freeze/frozen_graph.pb', help='graph file (.pb) to be evaluated.')\n    ap.add_argument('-i', '--input_node', type=str, default='input_1', help='input node.')\n    ap.add_argument('-o', '--output_node', type=str, default='flatten_1/Reshape', help='output node.')\n    ap.add_argument('-b', '--batchsize', type=int, default=1, help='Evaluation batchsize. Default is 1.')\n    args = ap.parse_args()\n    print('\\n' + DIVIDER)\n    print('Keras version      : ', tf.keras.__version__)\n    print('TensorFlow version : ', tf.__version__)\n    print(sys.version)\n    print(DIVIDER)\n    print(' Command line options:')\n    print('--dataset     : ', args.dataset)\n    print('--graph       : ', args.graph)\n    print('--input_node  : ', args.input_node)\n    print('--output_node : ', args.output_node)\n    print('--batchsize   : ', args.batchsize)\n    print(DIVIDER)\n    input_graph_def = tf.Graph().as_graph_def()\n    input_graph_def.ParseFromString(tf.io.gfile.GFile(args.graph, 'rb').read())\n"]]}
{"hexsha": "cea9ae69659ebfb8a8404af6f11de3d5ed1fc82d", "ext": "py", "lang": "Python", "content": "def post_data_helper(grant_type=None, code=None, refresh_token=None):\n    data = {\n        \"client_id\": os.environ.get('STRAVA_CLIENT_ID'),\n        \"client_secret\": os.environ.get('STRAVA_CLIENT_SECRET')\n    }\n\n    if grant_type:\n        data['grant_type'] = grant_type\n\n    if refresh_token:\n        data['refresh_token'] = refresh_token\n\n    if code:\n        data['code'] = code\n\n    return data", "fn_id": 4, "class_fn": false, "repo": "jb1b84/es-strava", "file": "main.py", "last_update_at": "2022-02-01T21:43:59+00:00", "question_id": "cea9ae69659ebfb8a8404af6f11de3d5ed1fc82d_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def post_data_helper(grant_type=None, code=None, refresh_token=None):\n    data = {'client_id': os.environ.get('STRAVA_CLIENT_ID'), 'client_secret': os.environ.get('STRAVA_CLIENT_SECRET')}\n    if grant_type:\n        data['grant_type'] = grant_type\n    if refresh_token:\n        data['refresh_token'] = refresh_token\n    if code:\n        data['code'] = code\n"]]}
{"hexsha": "8389609a0770baef5ec3de86e8b1a0f846cd6521", "ext": "py", "lang": "Python", "content": "def random_shuffle(a):\n    keys = list(a.keys())\n    values = list(a.values())\n    random.shuffle(values)\n    return dict(zip(keys, values))", "fn_id": 0, "class_fn": false, "repo": "bonom/Quantum-Annealing-for-solving-QUBO-Problems", "file": "QA4QUBO/solver.py", "last_update_at": "2022-03-17T12:09:57+00:00", "question_id": "8389609a0770baef5ec3de86e8b1a0f846cd6521_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def random_shuffle(a):\n    keys = list(a.keys())\n    values = list(a.values())\n    random.shuffle(values)\n"]]}
{"hexsha": "35a867bf320b7148d51ea802a41172abbd344eef", "ext": "py", "lang": "Python", "content": "def start(run_type, n_runs=1, n_cores=-2, **kwargs):\n    \"\"\"run the simulation as a subprocess,\n    which makes it easier to manage\"\"\"\n\n    if is_running():\n        raise Exception('Simulation is already running.')\n\n    if run_type not in RUN_TYPES:\n        raise Exception('Not a valid run type: \"{}\".'.format(run_type))\n\n    if run_type == 'sensitivity':\n        params = [s.strip() for s in kwargs['sensitivity_params'].split('\\n') if s.strip()]\n    else:\n        params = []\n\n    cmd = [\n        'python',\n        'main.py',\n        '-n', str(n_runs),\n        '-c', str(n_cores),\n        '-p', kwargs['params'],\n        '-r', kwargs['config'],\n        run_type\n    ]\n    cmd.extend(params)\n    with open(LOG_FILE, 'w+') as f:\n        kwargs = {\n            'stdout': f,\n            'stderr': subprocess.STDOUT,\n        }\n        if WINDOWS:\n            kwargs['creationflags'] = subprocess.CREATE_NEW_PROCESS_GROUP\n        else:\n            kwargs['preexec_fn'] = os.setsid\n        ps = subprocess.Popen(cmd, **kwargs)\n    with open(PID_FILE, 'w') as f:\n        f.write(str(ps.pid))", "fn_id": 0, "class_fn": false, "repo": "BAFurtado/PolicySpace2", "file": "web/manager.py", "last_update_at": "2022-03-29T16:05:38+00:00", "question_id": "35a867bf320b7148d51ea802a41172abbd344eef_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def start(run_type, n_runs=1, n_cores=-2, **kwargs):\n    \"\"\"run the simulation as a subprocess,\n    which makes it easier to manage\"\"\"\n    if is_running():\n        raise Exception('Simulation is already running.')\n    if run_type not in RUN_TYPES:\n        raise Exception('Not a valid run type: \"{}\".'.format(run_type))\n    if run_type == 'sensitivity':\n        params = [s.strip() for s in kwargs['sensitivity_params'].split('\\n') if s.strip()]\n    else:\n        params = []\n    cmd = ['python', 'main.py', '-n', str(n_runs), '-c', str(n_cores), '-p', kwargs['params'], '-r', kwargs['config'], run_type]\n    cmd.extend(params)\n    with open(LOG_FILE, 'w+') as f:\n        kwargs = {'stdout': f, 'stderr': subprocess.STDOUT}\n        if WINDOWS:\n            kwargs['creationflags'] = subprocess.CREATE_NEW_PROCESS_GROUP\n        else:\n            kwargs['preexec_fn'] = os.setsid\n        ps = subprocess.Popen(cmd, **kwargs)\n    with open(PID_FILE, 'w') as f:\n"]]}
{"hexsha": "7e17cd30c7fdc3f405c26cbaec248033cac05183", "ext": "py", "lang": "Python", "content": "def __test_epoch(student_net, testloader, device, criterion):\n    student_net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = student_net(inputs)\n            # loss = criterion(outputs, targets)\n\n            # test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            progress_bar(batch_idx,\n                         len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n                         (test_loss / (batch_idx + 1), 100. * correct / total,\n                          correct, total))\n\n    # Save checkpoint.\n    acc = 100. * correct / total", "fn_id": 1, "class_fn": false, "repo": "SeoroMin/NLP_DL_pytorch", "file": "trainer.py", "last_update_at": "2022-03-20T05:29:58+00:00", "question_id": "7e17cd30c7fdc3f405c26cbaec248033cac05183_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def __test_epoch(student_net, testloader, device, criterion):\n    student_net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = (inputs.to(device), targets.to(device))\n            outputs = student_net(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss / (batch_idx + 1), 100.0 * correct / total, correct, total))\n"]]}
{"hexsha": "8a57a06e794da49cb6d24638c9cade0b5d7c551d", "ext": "py", "lang": "Python", "content": "def test_balancer_list_members(driver):\n    extra = {'pool_id': '4d360b1f-bc2c-4ab7-9884-1f03ba2768f7',\n             'network_domain_id': '1234'}\n    balancer = LoadBalancer(\n                            id='234',\n                            name='test',\n                            state=State.RUNNING,\n                            ip='1.2.3.4',\n                            port=1234,\n                            driver=driver,\n                            extra=extra\n                           )\n    members = driver.balancer_list_members(balancer)\n    assert 2 == len(members)\n    assert members[0].ip ==  '10.0.3.13'\n    assert members[0].id == '3dd806a2-c2c8-4c0c-9a4f-5219ea9266c0'\n    assert members[0].port == 9889", "fn_id": 7, "class_fn": false, "repo": "zimventures/libcloud", "file": "libcloud/test/loadbalancer/test_nttcis.py", "last_update_at": "2022-03-25T19:39:34+00:00", "question_id": "8a57a06e794da49cb6d24638c9cade0b5d7c551d_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_balancer_list_members(driver):\n    extra = {'pool_id': '4d360b1f-bc2c-4ab7-9884-1f03ba2768f7', 'network_domain_id': '1234'}\n    balancer = LoadBalancer(id='234', name='test', state=State.RUNNING, ip='1.2.3.4', port=1234, driver=driver, extra=extra)\n    members = driver.balancer_list_members(balancer)\n    assert 2 == len(members)\n    assert members[0].ip == '10.0.3.13'\n    assert members[0].id == '3dd806a2-c2c8-4c0c-9a4f-5219ea9266c0'\n"]]}
{"hexsha": "8db74457f0fea727e6021840d686343cb3d258f2", "ext": "py", "lang": "Python", "content": "def pam_waveform2(ak, TS,  \n                 samples = 10, tinitial = 0, tguard = 0.0):\n    \n    Dt = TS / samples                                # sampling period    \n    Nguard = np.round(tguard / Dt)                   # guard points                         \n    Ntot = 2 * Nguard + samples * ak.size            # total number of points   \n    \n    x = np.zeros( Ntot.astype(int) )\n    t = np.arange( tinitial, tinitial + Ntot * Dt, Dt )\n\n    i = np.floor( (t - tinitial) / TS).astype(int)\n    j = np.where( np.logical_and(i >= 0, i < ak.size ) )\n\n    x[j] = ak[ i[j] ]\n    return t, x", "fn_id": 7, "class_fn": false, "repo": "thomaskamalakis/pythoncommcourse", "file": "lecture5/commlib.py", "last_update_at": "2022-01-25T16:49:45+00:00", "question_id": "8db74457f0fea727e6021840d686343cb3d258f2_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def pam_waveform2(ak, TS, samples=10, tinitial=0, tguard=0.0):\n    Dt = TS / samples\n    Nguard = np.round(tguard / Dt)\n    Ntot = 2 * Nguard + samples * ak.size\n    x = np.zeros(Ntot.astype(int))\n    t = np.arange(tinitial, tinitial + Ntot * Dt, Dt)\n    i = np.floor((t - tinitial) / TS).astype(int)\n    j = np.where(np.logical_and(i >= 0, i < ak.size))\n    x[j] = ak[i[j]]\n"]]}
{"hexsha": "566b774d62472aa1932501a18ec7cae80e02726b", "ext": "py", "lang": "Python", "content": "def lowpass(data, f, fs):\n    wn = 2*f/fs\n    b, a = signal.butter(8, wn, 'lowpass')\n    filtedData = signal.filtfilt(b, a, data)\n    return filtedData", "fn_id": 6, "class_fn": false, "repo": "xzk8559/RecursiveLSTM", "file": "utils_data.py", "last_update_at": "2022-03-29T11:16:10+00:00", "question_id": "566b774d62472aa1932501a18ec7cae80e02726b_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def lowpass(data, f, fs):\n    wn = 2 * f / fs\n    b, a = signal.butter(8, wn, 'lowpass')\n    filtedData = signal.filtfilt(b, a, data)\n"]]}
{"hexsha": "bf6c7a3e1fd05895289e7302d31c3ba8727dd474", "ext": "py", "lang": "Python", "content": "@click.command()\n@click.pass_context\n@click.option('--model', 'model', help='which type of raster')\n@click.option('--forecast_hours_', 'forecast_hours_',\n              help='Forecast hours to extract from')\n@click.option('--model_run', 'model_run',\n              help='model run to use for the time series')\n@click.option('--input_geojson', 'input_geojson', help='shape to clip by')\ndef extract_raster(ctx, model, forecast_hours_, model_run, input_geojson):\n    output_geojson = extract_raster_main(\n        model, forecast_hours_, model_run, input_geojson)\n\n    return output_geojson\n\n    if output_geojson is not None:\n        click.echo(json.dumps(output_geojson))", "fn_id": 5, "class_fn": false, "repo": "Dukestep/msc-pygeoapi", "file": "msc_pygeoapi/process/weather/extract_raster.py", "last_update_at": "2022-01-04T19:00:22+00:00", "question_id": "bf6c7a3e1fd05895289e7302d31c3ba8727dd474_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@click.command()\n@click.pass_context\n@click.option('--model', 'model', help='which type of raster')\n@click.option('--forecast_hours_', 'forecast_hours_', help='Forecast hours to extract from')\n@click.option('--model_run', 'model_run', help='model run to use for the time series')\n@click.option('--input_geojson', 'input_geojson', help='shape to clip by')\ndef extract_raster(ctx, model, forecast_hours_, model_run, input_geojson):\n    output_geojson = extract_raster_main(model, forecast_hours_, model_run, input_geojson)\n    return output_geojson\n    if output_geojson is not None:\n"]]}
{"hexsha": "18ea7a4669d744d3db26b2a27f5324cb846eec65", "ext": "py", "lang": "Python", "content": "def test_workchain_build(sphinx_build_factory, xml_equal, reference_result):\n    \"\"\"Test building sphinx documentation for WorkChain.\n\n    Builds Sphinx documentation for workchain and compares against expected XML result.\n    \"\"\"\n    sphinx_build = sphinx_build_factory('workchain', buildername='xml')\n    sphinx_build.build(assert_pass=True)\n\n    index_file = sphinx_build.outdir / 'index.xml'\n    xml_equal(index_file, reference_result('workchain.xml'))", "fn_id": 0, "class_fn": false, "repo": "azadoks/aiida-core", "file": "tests/sphinxext/test_workchain.py", "last_update_at": "2022-03-22T13:16:57+00:00", "question_id": "18ea7a4669d744d3db26b2a27f5324cb846eec65_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_workchain_build(sphinx_build_factory, xml_equal, reference_result):\n    \"\"\"Test building sphinx documentation for WorkChain.\n\n    Builds Sphinx documentation for workchain and compares against expected XML result.\n    \"\"\"\n    sphinx_build = sphinx_build_factory('workchain', buildername='xml')\n    sphinx_build.build(assert_pass=True)\n    index_file = sphinx_build.outdir / 'index.xml'\n"]]}
{"hexsha": "625f5558fec6561ccce643c9dcca356d9fd867f3", "ext": "py", "lang": "Python", "content": "def initialize_variables(): # pragma: no cover\n    \"\"\"This function initializes the values for the rocket that will be used in the\n    simulation. Specifics are given alongside the value.\n    \"\"\"\n    STANDARD_GRAVITY = 9.80665\n    #current values for falcon 9 booster\n    thrust = float(\n        490000)  # Motor thrust in Newtons\n    motor_isp = float(\n        335)  # Motor ISP\n    mass_flow = thrust / (motor_isp * STANDARD_GRAVITY)\n    print(mass_flow)\n\n    dry_mass = float(\n        10000\n           )  # Dry mass in kg\n    wet_mass = float(\n        40000\n    )  # Wet mass in kg\n\n    reference_area = 3.14159 * 3**2  # This is the cross sectional profile of the rocket\n    return dry_mass, wet_mass, mass_flow, thrust, motor_isp, reference_area", "fn_id": 13, "class_fn": false, "repo": "bvermeulen/Rocket-and-gravity-turn", "file": "rocket_example.py", "last_update_at": "2022-02-07T18:39:18+00:00", "question_id": "625f5558fec6561ccce643c9dcca356d9fd867f3_13", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def initialize_variables():\n    \"\"\"This function initializes the values for the rocket that will be used in the\n    simulation. Specifics are given alongside the value.\n    \"\"\"\n    STANDARD_GRAVITY = 9.80665\n    thrust = float(490000)\n    motor_isp = float(335)\n    mass_flow = thrust / (motor_isp * STANDARD_GRAVITY)\n    print(mass_flow)\n    dry_mass = float(10000)\n    wet_mass = float(40000)\n    reference_area = 3.14159 * 3 ** 2\n"]]}
{"hexsha": "226bcc6fbccf1244851ea24b16eccb42a0dec529", "ext": "py", "lang": "Python", "content": "def run_job(spark, config, mode, s3_bucket=None, phase=None):\n    \"\"\" Runs Data Preparation job\"\"\"\n    raw_df = _read_data(spark, config, mode, phase, s3_bucket)\n    phase = 'train' if \"DEFAULT\" in raw_df.columns else 'test'\n    df = Pipe([\n        IF(IF.Predicate.has_column('DEFAULT'), then=[\n            RemoveDuplicates(config['id_col'])\n        ]),\n        ConvertStrToDate(config['date_str_cols']),\n        GetAge(config['age_cols']),\n        ExtractTimePeriodMths(config['tenure_cols']),\n        ReplaceStrRegex(config['str_replace_cols']),\n        ImputeCategoricalMissingVals(config['impute_cat_cols']),\n        DropColumns(config['drop_cols']),\n    ]).transform(raw_df)\n    if mode == \"local\":\n        df.write.parquet(config['processed_data_dir'] + f\"{phase}.parquet\", mode='overwrite')\n    else:\n        df.write.parquet(config['s3_processed_data_dir'].format(s3_bucket) + f\"{phase}.parquet\", \n                        mode='overwrite')", "fn_id": 1, "class_fn": false, "repo": "alanchn31/loan_default_prediction", "file": "app/src/jobs/preprocess_data.py", "last_update_at": "2022-03-12T20:44:38+00:00", "question_id": "226bcc6fbccf1244851ea24b16eccb42a0dec529_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def run_job(spark, config, mode, s3_bucket=None, phase=None):\n    \"\"\" Runs Data Preparation job\"\"\"\n    raw_df = _read_data(spark, config, mode, phase, s3_bucket)\n    phase = 'train' if 'DEFAULT' in raw_df.columns else 'test'\n    df = Pipe([IF(IF.Predicate.has_column('DEFAULT'), then=[RemoveDuplicates(config['id_col'])]), ConvertStrToDate(config['date_str_cols']), GetAge(config['age_cols']), ExtractTimePeriodMths(config['tenure_cols']), ReplaceStrRegex(config['str_replace_cols']), ImputeCategoricalMissingVals(config['impute_cat_cols']), DropColumns(config['drop_cols'])]).transform(raw_df)\n    if mode == 'local':\n        df.write.parquet(config['processed_data_dir'] + f'{phase}.parquet', mode='overwrite')\n    else:\n"]]}
{"hexsha": "f4baece3b1689b1dde25233a9ee3f6d169e536e6", "ext": "py", "lang": "Python", "content": "def render(image_id, requested_size):\n    print('image_id: \"{image_id}\", size: {requested_size}'.format(**locals()))\n\n    # width and height will be -1 if not set in QML\n    if requested_size == (-1, -1):\n        requested_size = (300, 300)\n\n    width, height = requested_size\n\n    # center for circle\n    cx, cy = width/2, 10\n\n    pixels = []\n    for y in range(height):\n        for x in range(width):\n            pixels.extend(reversed([\n                255, # alpha\n                int(10 + 10 * ((x - y * 0.5) % 20)), # red\n                20 + 10 * (y % 20), # green\n                int(255 * abs(math.sin(0.3*math.sqrt((cx-x)**2 + (cy-y)**2)))) # blue\n            ]))\n    return bytearray(pixels), (width, height), pyotherside.format_argb32", "fn_id": 0, "class_fn": false, "repo": "medxchange/pyotherside", "file": "examples/imageprovider_example.py", "last_update_at": "2022-03-28T08:13:10+00:00", "question_id": "f4baece3b1689b1dde25233a9ee3f6d169e536e6_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def render(image_id, requested_size):\n    print('image_id: \"{image_id}\", size: {requested_size}'.format(**locals()))\n    if requested_size == (-1, -1):\n        requested_size = (300, 300)\n    width, height = requested_size\n    cx, cy = (width / 2, 10)\n    pixels = []\n    for y in range(height):\n        for x in range(width):\n            pixels.extend(reversed([255, int(10 + 10 * ((x - y * 0.5) % 20)), 20 + 10 * (y % 20), int(255 * abs(math.sin(0.3 * math.sqrt((cx - x) ** 2 + (cy - y) ** 2))))]))\n"]]}
{"hexsha": "88ebf18f6afffd8fd603b85146cbf14f21d977a4", "ext": "py", "lang": "Python", "content": "@login_required\ndef events_list_mine(request):\n    context = {\"events_shown\": \"mine\"}\n    query = Q(bookings__person=request.user, bookings__cancelledOn=None)\n    query = query | Q(organisers=request.user)\n    query = query | Q(owner=request.user)\n    events = Event.objects.filter(query).distinct()\n    if events.count() > 0:\n        return events_list(request, events, context)\n    else:\n        messages.debug(request, \"You have no event yet\")\n        return redirect(\"events_list_all\")", "fn_id": 5, "class_fn": false, "repo": "gchazot/OneEvent", "file": "oneevent/views.py", "last_update_at": "2022-01-12T04:40:58+00:00", "question_id": "88ebf18f6afffd8fd603b85146cbf14f21d977a4_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@login_required\ndef events_list_mine(request):\n    context = {'events_shown': 'mine'}\n    query = Q(bookings__person=request.user, bookings__cancelledOn=None)\n    query = query | Q(organisers=request.user)\n    query = query | Q(owner=request.user)\n    events = Event.objects.filter(query).distinct()\n    if events.count() > 0:\n        return events_list(request, events, context)\n    else:\n        messages.debug(request, 'You have no event yet')\n"]]}
{"hexsha": "30f2b10652770dd23cf3be8e57fb27b746721c50", "ext": "py", "lang": "Python", "content": "def AOCDEFACEE():\n\tprint(w+\"AOCDEFACE\"+g+\" is Installing.....\\n\"+g)\n\tos.system(\"apt-get update -y;apt-get upgrade -y;apt-get install git -y;git clone https://github.com/Amriez/AOCDEFACE.git/;mv AOCDEFACE $HOME\")\n\tprint(w+\"\\nAOCDEFACE\"+g+\" is Installed and saved in\"+w+\" home \"+g+\"directory\")\n\tinput(b+\"\\npress ENTER to continue\")\n\tos.system(\"clear;python hacked.py\")", "fn_id": 546, "class_fn": false, "repo": "MrHacker-X/Hacked", "file": "hacked.py", "last_update_at": "2022-03-16T13:36:48+00:00", "question_id": "30f2b10652770dd23cf3be8e57fb27b746721c50_546", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def AOCDEFACEE():\n    print(w + 'AOCDEFACE' + g + ' is Installing.....\\n' + g)\n    os.system('apt-get update -y;apt-get upgrade -y;apt-get install git -y;git clone https://github.com/Amriez/AOCDEFACE.git/;mv AOCDEFACE $HOME')\n    print(w + '\\nAOCDEFACE' + g + ' is Installed and saved in' + w + ' home ' + g + 'directory')\n    input(b + '\\npress ENTER to continue')\n"]]}
{"hexsha": "4b89517d38a090a0880bc09b2ac0cea4be3c964a", "ext": "py", "lang": "Python", "content": "def main():\n    annotPath = r'.\\res\\PennFudanPed\\Annotation'\n    imgPath = r'.\\res\\PennFudanPed\\PNGImages'\n    dst = r'.\\res\\PennFudanPed\\Label'\n    if len(sys.argv)==2:\n        imgPath = sys.argv[1]\n    elif len(sys.argv)==3:\n        imgPath = sys.argv[1]\n        dst = sys.argv[2]\n    print('imagePath=',imgPath)\n    print('dst=',dst)\n        \n    generateImageLabel(imgPath,annotPath,dst)\n    '''\n    deleteFolder(dst)\n    createPath(dst)\n    for i in listFile(imgPath,'png'):\n        H,W = getImgHW(loadImg(i))\n        #print(H,W)\n        fAnnot = getImgAnnotFile(annotPath,i)\n        fAnnotName = getFileName(fAnnot)\n        dstFile = dst+ '\\\\' +  fAnnotName\n        print('fAnnot=', fAnnot)\n        print('dstFile=', dstFile)\n        \n        deleteFile(dstFile)\n        \n        coordinates = getFileCoordinates(fAnnot)\n        writeToAnnotFile(H,W,dstFile,coordinates)\n    '''", "fn_id": 5, "class_fn": false, "repo": "StevenHuang2020/Pedestrian-Segmentation", "file": "src/genImageBoxLabel.py", "last_update_at": "2022-01-12T01:38:11+00:00", "question_id": "4b89517d38a090a0880bc09b2ac0cea4be3c964a_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    annotPath = '.\\\\res\\\\PennFudanPed\\\\Annotation'\n    imgPath = '.\\\\res\\\\PennFudanPed\\\\PNGImages'\n    dst = '.\\\\res\\\\PennFudanPed\\\\Label'\n    if len(sys.argv) == 2:\n        imgPath = sys.argv[1]\n    elif len(sys.argv) == 3:\n        imgPath = sys.argv[1]\n        dst = sys.argv[2]\n    print('imagePath=', imgPath)\n    print('dst=', dst)\n    generateImageLabel(imgPath, annotPath, dst)\n"]]}
{"hexsha": "2618c160604c668f1d27543cfcb9010f90910789", "ext": "py", "lang": "Python", "content": "def get_coordinates_key_for_expression(\n        *,\n        x: Union[int, Int],\n        y: Union[int, Int]) -> ExpressionString:\n    \"\"\"\n    Get a key string for the expression from the x and y coordinates.\n\n    Parameters\n    ----------\n    x : int or Int\n        X-coordinate.\n    y : int or Int\n        Y-coordinate.\n\n    Returns\n    -------\n    key_exp_str : ExpressionString\n        Key expression string.\n    \"\"\"\n    from apysc._type.variable_name_interface import VariableNameInterface\n    if isinstance(x, VariableNameInterface):\n        x_str: str = x.variable_name\n    else:\n        x_str = str(x)\n    if isinstance(y, VariableNameInterface):\n        y_str: str = y.variable_name\n    else:\n        y_str = str(y)\n    key_exp_str: ExpressionString = ExpressionString(\n        value=f'String({x_str}) + \"_\" + String({y_str})')\n    return key_exp_str", "fn_id": 0, "class_fn": false, "repo": "ynsnf/apysc", "file": "apysc/_display/rotation_interface_helper.py", "last_update_at": "2022-01-01T08:53:49+00:00", "question_id": "2618c160604c668f1d27543cfcb9010f90910789_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_coordinates_key_for_expression(*, x: Union[int, Int], y: Union[int, Int]) -> ExpressionString:\n    \"\"\"\n    Get a key string for the expression from the x and y coordinates.\n\n    Parameters\n    ----------\n    x : int or Int\n        X-coordinate.\n    y : int or Int\n        Y-coordinate.\n\n    Returns\n    -------\n    key_exp_str : ExpressionString\n        Key expression string.\n    \"\"\"\n    from apysc._type.variable_name_interface import VariableNameInterface\n    if isinstance(x, VariableNameInterface):\n        x_str: str = x.variable_name\n    else:\n        x_str = str(x)\n    if isinstance(y, VariableNameInterface):\n        y_str: str = y.variable_name\n    else:\n        y_str = str(y)\n    key_exp_str: ExpressionString = ExpressionString(value=f'String({x_str}) + \"_\" + String({y_str})')\n"]]}
{"hexsha": "0afcd498a3e7bb94a538760ebf1ce831ce104976", "ext": "py", "lang": "Python", "content": "def tensors_to_vols(input_vols_th):\n    vols_np = [vol_unnormalize(input_vols_th[i].cpu().data.numpy()) for i in range(len(input_vols_th))]\n    vols_np = np.stack(vols_np, 0)\n    return vols_np", "fn_id": 34, "class_fn": false, "repo": "yuzhd/Text2Scene", "file": "lib/composites_utils.py", "last_update_at": "2022-01-23T16:03:17+00:00", "question_id": "0afcd498a3e7bb94a538760ebf1ce831ce104976_34", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def tensors_to_vols(input_vols_th):\n    vols_np = [vol_unnormalize(input_vols_th[i].cpu().data.numpy()) for i in range(len(input_vols_th))]\n    vols_np = np.stack(vols_np, 0)\n"]]}
{"hexsha": "0a5ee5468122bdbb8d95274ef795ea8e50563a93", "ext": "py", "lang": "Python", "content": "def branch_tica(centers, bandwidth, fusing_tolerance=1, logweights=None):\n    inf, sup = np.min(centers), np.max(centers)\n    padding = 1.1\n    init_points = np.arange(inf*padding, sup*padding, bandwidth)[:, np.newaxis]\n    fixed_points, kde = find_fes_fixed_points(centers[:, np.newaxis], bandwidth, init_points, return_kde = True, logweights=logweights)\n    state_bounds = get_state_bounds(fixed_points, kde, [inf, sup])\n    state_bounds = fuse_bounds(state_bounds, tol = fusing_tolerance)\n    _fes = -kde.logpdf(kde.dataset)\n    for d in state_bounds:\n        inf = d['bounds'][0]\n        sup = d['bounds'][1]\n        d['fes'] = _fes\n        d['mask'] = np.logical_and(centers >= inf, centers <= sup)\n    return state_bounds", "fn_id": 5, "class_fn": false, "repo": "luigibonati/md-stateinterpreter", "file": "stateinterpreter/utils/hierarchical.py", "last_update_at": "2022-02-14T10:06:45+00:00", "question_id": "0a5ee5468122bdbb8d95274ef795ea8e50563a93_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def branch_tica(centers, bandwidth, fusing_tolerance=1, logweights=None):\n    inf, sup = (np.min(centers), np.max(centers))\n    padding = 1.1\n    init_points = np.arange(inf * padding, sup * padding, bandwidth)[:, np.newaxis]\n    fixed_points, kde = find_fes_fixed_points(centers[:, np.newaxis], bandwidth, init_points, return_kde=True, logweights=logweights)\n    state_bounds = get_state_bounds(fixed_points, kde, [inf, sup])\n    state_bounds = fuse_bounds(state_bounds, tol=fusing_tolerance)\n    _fes = -kde.logpdf(kde.dataset)\n    for d in state_bounds:\n        inf = d['bounds'][0]\n        sup = d['bounds'][1]\n        d['fes'] = _fes\n        d['mask'] = np.logical_and(centers >= inf, centers <= sup)\n"]]}
{"hexsha": "312cf973c05f33f5b578e08da41d5d751803fb35", "ext": "py", "lang": "Python", "content": "def clear_mpls_counters(device):\n    \"\"\" Config ldp on Device\n\n        Args:\n            device (`obj`): Device object\n        Return:\n            None\n        Raise:\n            SubCommandFailure: Failed configuring interface\n    \"\"\"\n    log.info(\"clear mpls counters on {device}\".format(device=device.name))\n\n    try:\n        device.execute(\"clear mpls counters\")\n    except SubCommandFailure as e:\n        raise SubCommandFailure(\n            'Could not clear mpls counters on {device}, Error: {error}'.format(\n                device=device.name, error=e\n            )\n        )       ", "fn_id": 1, "class_fn": false, "repo": "CiscoTestAutomation/genielibs", "file": "pkgs/sdk-pkg/src/genie/libs/sdk/apis/iosxe/mpls/configure.py", "last_update_at": "2022-03-29T13:40:31+00:00", "question_id": "312cf973c05f33f5b578e08da41d5d751803fb35_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def clear_mpls_counters(device):\n    \"\"\" Config ldp on Device\n\n        Args:\n            device (`obj`): Device object\n        Return:\n            None\n        Raise:\n            SubCommandFailure: Failed configuring interface\n    \"\"\"\n    log.info('clear mpls counters on {device}'.format(device=device.name))\n    try:\n        device.execute('clear mpls counters')\n    except SubCommandFailure as e:\n"]]}
{"hexsha": "0ee471e238a34a202c3e1ef0b95db0b3d6eb1afd", "ext": "py", "lang": "Python", "content": "def create_custom_mesh2(objname, x0, x1, y0, y1, z0, z1, data):\n \n    # Define arrays for holding data    \n    myvertex = []\n    myfaces = []\n    \n    # Create all Vertices\n\n    # vertex 0\n    mypoint = [(x0, y0, z0)]\n    myvertex.extend(mypoint)\n\n    # vertex 1\n    mypoint = [(x1, y0, z0)]\n    myvertex.extend(mypoint)\n\n    # vertex 2\n    mypoint = [(x0, y1, z1)]\n    myvertex.extend(mypoint)\n\n    # vertex 3\n    mypoint = [(x1, y1, z1)]\n    myvertex.extend(mypoint)\n\n    # -------------------------------------\n    # Create all Faces\n    # -------------------------------------\n    myface = [(0, 1, 3, 2)]\n    myfaces.extend(myface)\n\n    print(\"A\", objname)\n    mymesh = bpy.data.meshes.new(objname)\n    print(\"B\")\n    myobject = bpy.data.objects.new(objname, mymesh)\n    print(\"C\")\n    bpy.context.scene.collection.objects.link(myobject)\n    print(\"D\")\n    # Generate mesh data\n    mymesh.from_pydata(myvertex, [], myfaces)\n    # Calculate the edges\n    print(\"E\")\n    mymesh.update(calc_edges=True)\n\n    # Set Location\n    #myobject.location.x = px\n    #myobject.location.y = py\n    #myobject.location.z = pz\n    print(\"F\")\n    myobject.location.x = 0\n    myobject.location.y = 0\n    myobject.location.z = 0\n    \n    tex = bpy.data.textures.new('%s-TEXTURE'%(objname), 'IMAGE')\n    print(data.shape)\n    mn = np.nanmin(data)\n    mx = np.nanmax(data)\n    d = (data - mn)/(mx - mn)\n    d2 = np.zeros((d.shape[0], d.shape[1], 4))\n    d2[:, :, 0] = d\n    d2[:, :, 1] = d\n    d2[:, :, 2] = d\n    d2[:, :, 3] = 1\n    print(d2.shape, d2.flatten().shape)\n    \n    image = bpy.data.images.new(\"%s-IMAGE\"%(objname), width=d.shape[0], height=d.shape[1])\n    image.pixels = d2.flatten()\n    tex.image = image\n    \n    mat = bpy.data.materials.new('%s-DATA'%(objname))\n    mat.texture_paint_images = [image]\n    #mat.texture_slots.add()\n    #ts = mat.texture_slots[0]\n    #ts.texture = tex\n    #ts.texture_coords = 'UV'\n    #ts.uv_layer = 'default'\n    #mtex.texture_coords = 'UV'\n    \n    myobject.data.materials = mat\n    \n\n    return myobject", "fn_id": 12, "class_fn": false, "repo": "johodges/pyfdstools", "file": "blender_examples.py", "last_update_at": "2022-01-25T23:02:18+00:00", "question_id": "0ee471e238a34a202c3e1ef0b95db0b3d6eb1afd_12", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_custom_mesh2(objname, x0, x1, y0, y1, z0, z1, data):\n    myvertex = []\n    myfaces = []\n    mypoint = [(x0, y0, z0)]\n    myvertex.extend(mypoint)\n    mypoint = [(x1, y0, z0)]\n    myvertex.extend(mypoint)\n    mypoint = [(x0, y1, z1)]\n    myvertex.extend(mypoint)\n    mypoint = [(x1, y1, z1)]\n    myvertex.extend(mypoint)\n    myface = [(0, 1, 3, 2)]\n    myfaces.extend(myface)\n    print('A', objname)\n    mymesh = bpy.data.meshes.new(objname)\n    print('B')\n    myobject = bpy.data.objects.new(objname, mymesh)\n    print('C')\n    bpy.context.scene.collection.objects.link(myobject)\n    print('D')\n    mymesh.from_pydata(myvertex, [], myfaces)\n    print('E')\n    mymesh.update(calc_edges=True)\n    print('F')\n    myobject.location.x = 0\n    myobject.location.y = 0\n    myobject.location.z = 0\n    tex = bpy.data.textures.new('%s-TEXTURE' % objname, 'IMAGE')\n    print(data.shape)\n    mn = np.nanmin(data)\n    mx = np.nanmax(data)\n    d = (data - mn) / (mx - mn)\n    d2 = np.zeros((d.shape[0], d.shape[1], 4))\n    d2[:, :, 0] = d\n    d2[:, :, 1] = d\n    d2[:, :, 2] = d\n    d2[:, :, 3] = 1\n    print(d2.shape, d2.flatten().shape)\n    image = bpy.data.images.new('%s-IMAGE' % objname, width=d.shape[0], height=d.shape[1])\n    image.pixels = d2.flatten()\n    tex.image = image\n    mat = bpy.data.materials.new('%s-DATA' % objname)\n    mat.texture_paint_images = [image]\n    myobject.data.materials = mat\n"]]}
{"hexsha": "6744c5a8846b111b0170e462d02bdfc5ecf29425", "ext": "py", "lang": "Python", "content": "def get_sub_package(packages_path: str) -> str:\n    package_cmd = \"--package\"\n    packages = os.listdir(packages_path)\n    available_packages = \", \".join(packages)\n\n    if package_cmd not in sys.argv:\n        raise RuntimeError(\n            f\"Specify which package to build with '{package_cmd} <PACKAGE NAME>'. \"\n            f\"Available packages are: {available_packages}\"\n        )\n\n    index = sys.argv.index(package_cmd)\n    sys.argv.pop(index)  # Removes the switch\n    package = sys.argv.pop(index)  # Returns the element after the switch\n    if package not in packages:\n        raise RuntimeError(\n            f\"Unknown package '{package}'. Available packages are: {available_packages}\"\n        )\n    return package", "fn_id": 0, "class_fn": false, "repo": "radeklat/namespaced-libraries-monorepo", "file": "setup.py", "last_update_at": "2022-02-05T08:11:45+00:00", "question_id": "6744c5a8846b111b0170e462d02bdfc5ecf29425_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_sub_package(packages_path: str) -> str:\n    package_cmd = '--package'\n    packages = os.listdir(packages_path)\n    available_packages = ', '.join(packages)\n    if package_cmd not in sys.argv:\n        raise RuntimeError(f\"Specify which package to build with '{package_cmd} <PACKAGE NAME>'. Available packages are: {available_packages}\")\n    index = sys.argv.index(package_cmd)\n    sys.argv.pop(index)\n    package = sys.argv.pop(index)\n    if package not in packages:\n        raise RuntimeError(f\"Unknown package '{package}'. Available packages are: {available_packages}\")\n"]]}
{"hexsha": "f571a3dc1cc6295799a0201b4c2ed4bebb3357b5", "ext": "py", "lang": "Python", "content": "def build_linux(workingPath):\n    arch = 'x86_64'\n    build_one_arch(workingPath, 'Debug', arch)\n    \n    build_one_arch(workingPath, 'Release', arch)", "fn_id": 2, "class_fn": false, "repo": "wanghaEMQ/kuma", "file": "bld/linux/build_linux.py", "last_update_at": "2022-03-26T05:49:40+00:00", "question_id": "f571a3dc1cc6295799a0201b4c2ed4bebb3357b5_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def build_linux(workingPath):\n    arch = 'x86_64'\n    build_one_arch(workingPath, 'Debug', arch)\n"]]}
{"hexsha": "69e8820ba11c4ccbf5c86951577451bb4f76b037", "ext": "py", "lang": "Python", "content": "def replace_class(full_path: str, new_module):\n    def replace_class_instance(func):\n        @functools.wraps(func)\n        async def with_replaced_class(*args, **kwargs):\n            class_name = full_path.split(\".\")[-1]\n            module_name = \".\".join(full_path.split(\".\")[:-1])\n            replaced_class = getattr(sys.modules[module_name], class_name)\n            setattr(sys.modules[module_name], class_name, new_module)\n\n            try:\n                if inspect.iscoroutinefunction(func):\n                    result = await func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n            finally:\n                setattr(sys.modules[module_name], class_name, replaced_class)\n\n            return result\n\n        return with_replaced_class\n\n    return replace_class_instance", "fn_id": 0, "class_fn": false, "repo": "software-mansion/protostar", "file": "protostar/utils/modules.py", "last_update_at": "2022-03-28T18:24:45+00:00", "question_id": "69e8820ba11c4ccbf5c86951577451bb4f76b037_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def replace_class(full_path: str, new_module):\n\n    def replace_class_instance(func):\n\n        @functools.wraps(func)\n        async def with_replaced_class(*args, **kwargs):\n            class_name = full_path.split('.')[-1]\n            module_name = '.'.join(full_path.split('.')[:-1])\n            replaced_class = getattr(sys.modules[module_name], class_name)\n            setattr(sys.modules[module_name], class_name, new_module)\n            try:\n                if inspect.iscoroutinefunction(func):\n                    result = await func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n            finally:\n                setattr(sys.modules[module_name], class_name, replaced_class)\n            return result\n        return with_replaced_class\n"]]}
{"hexsha": "568f18ef03007c0e220a26096430842fb006cf0f", "ext": "py", "lang": "Python", "content": "def _parse_single_line(line: str):\n    possible_separators = (\":\", \"_\", \"-\", \"\\t\", \" \", \";\", \",\", \".\")\n    idx = 0\n    parsed_line = (line,)\n    while idx < len(possible_separators):\n        res = line.split(possible_separators[idx])\n        if len(res) == 2:\n            parsed_line = res\n            break\n        idx += 1\n\n    # if we fail with the possible separators, let's see if\n    # we can split up on contiguous whitespace to two pieces.\n\n    if len(parsed_line) == 1:\n        ln = re.split(r'\\s+', *parsed_line)\n        if len(ln) == 2:\n            parsed_line = ln\n    rx = '[' + re.escape(''.join(possible_separators[2:-4])) + ']'\n\n    # remove the obvious unwanted chars\n    parsed_line = [element.strip(\"\\n\").strip(\"\\x00\") for element in parsed_line]\n\n    # remove the separator-like chars\n    parsed_line = [re.sub(rx, \"\", el) for el in parsed_line]\n\n    # remove whitespace around elements\n    parsed_line = [element.strip(\" \") for element in parsed_line]\n\n    return parsed_line", "fn_id": 0, "class_fn": false, "repo": "kingablgh/PySprint", "file": "pysprint/core/io/_parser.py", "last_update_at": "2022-02-09T17:29:19+00:00", "question_id": "568f18ef03007c0e220a26096430842fb006cf0f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _parse_single_line(line: str):\n    possible_separators = (':', '_', '-', '\\t', ' ', ';', ',', '.')\n    idx = 0\n    parsed_line = (line,)\n    while idx < len(possible_separators):\n        res = line.split(possible_separators[idx])\n        if len(res) == 2:\n            parsed_line = res\n            break\n        idx += 1\n    if len(parsed_line) == 1:\n        ln = re.split('\\\\s+', *parsed_line)\n        if len(ln) == 2:\n            parsed_line = ln\n    rx = '[' + re.escape(''.join(possible_separators[2:-4])) + ']'\n    parsed_line = [element.strip('\\n').strip('\\x00') for element in parsed_line]\n    parsed_line = [re.sub(rx, '', el) for el in parsed_line]\n    parsed_line = [element.strip(' ') for element in parsed_line]\n"]]}
{"hexsha": "5606c775e10a113b5417ebfffdb1eb92ea97e92f", "ext": "py", "lang": "Python", "content": "def make_deck(model, feature, words):\n    deckid = int(hashlib.sha256(feature.title.encode('utf-8')).hexdigest(), 16) % 10**8\n    deck = genanki.Deck(deckid, feature.title)\n\n    for key, g in itertools.groupby(sorted(words, key=feature.key), key=feature.key):\n        similar = list(g)\n        if len(similar) == 1:\n            continue\n\n        print(key)\n        similar = sorted(similar, key=lambda w:w['text'])\n        print(similar)\n\n        extra = '<br>'.join([f'''\n<div style=\"vertical-align: middle; font-size: 1em; font-family: Arial\">\n    [{s[\"text\"]}]&nbsp;\n    <audio style=\"vertical-align: middle;\" src=\"{s[\"audio\"]}\" controls></audio>\n</div>\n        ''' for s in similar])\n\n        for s in similar:\n            note = MinimalPairNote(model=model, fields=[s['text'], s['audio'], extra])\n            deck.add_note(note)\n\n    return deck", "fn_id": 0, "class_fn": false, "repo": "vitkyrka/nordict", "file": "tools/anki.py", "last_update_at": "2022-01-07T15:31:15+00:00", "question_id": "5606c775e10a113b5417ebfffdb1eb92ea97e92f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def make_deck(model, feature, words):\n    deckid = int(hashlib.sha256(feature.title.encode('utf-8')).hexdigest(), 16) % 10 ** 8\n    deck = genanki.Deck(deckid, feature.title)\n    for key, g in itertools.groupby(sorted(words, key=feature.key), key=feature.key):\n        similar = list(g)\n        if len(similar) == 1:\n            continue\n        print(key)\n        similar = sorted(similar, key=lambda w: w['text'])\n        print(similar)\n        extra = '<br>'.join([f'''\\n<div style=\"vertical-align: middle; font-size: 1em; font-family: Arial\">\\n    [{s['text']}]&nbsp;\\n    <audio style=\"vertical-align: middle;\" src=\"{s['audio']}\" controls></audio>\\n</div>\\n        ''' for s in similar])\n        for s in similar:\n            note = MinimalPairNote(model=model, fields=[s['text'], s['audio'], extra])\n            deck.add_note(note)\n"]]}
{"hexsha": "cf3a71a276988b7eff40f4617414c23a9c5cb292", "ext": "py", "lang": "Python", "content": "def get_bounds(geometry, north_up=True, transform=None):\n    \"\"\"Bounding box of a GeoJSON geometry, GeometryCollection, or FeatureCollection.\n    left, bottom, right, top\n    *not* xmin, ymin, xmax, ymax\n    If not north_up, y will be switched to guarantee the above.\n    Source code adapted from https://github.com/mapbox/rasterio/blob/master/rasterio/features.py#L361\n    \"\"\"\n\n    if 'bbox' in geometry:\n        return tuple(geometry['bbox'])\n\n    geometry = geometry.get('geometry') or geometry  \n\n    # geometry must be a geometry, GeometryCollection, or FeatureCollection\n    if not ('coordinates' in geometry or 'geometries' in geometry or 'features' in geometry):\n        raise ValueError(\n            \"geometry must be a GeoJSON-like geometry, GeometryCollection, \"\n            \"or FeatureCollection\"\n        )\n\n    if 'features' in geometry:\n        # Input is a FeatureCollection\n        xmins = []\n        ymins = []\n        xmaxs = []\n        ymaxs = []\n        for feature in geometry['features']:\n            xmin, ymin, xmax, ymax = get_bounds(feature['geometry'])\n            xmins.append(xmin)\n            ymins.append(ymin)\n            xmaxs.append(xmax)\n            ymaxs.append(ymax)\n        if north_up:\n            return min(xmins), min(ymins), max(xmaxs), max(ymaxs)\n        else:\n            return min(xmins), max(ymaxs), max(xmaxs), min(ymins)\n\n    elif 'geometries' in geometry:\n        # Input is a geometry collection\n        xmins = []\n        ymins = []\n        xmaxs = []\n        ymaxs = []\n        for geometry in geometry['geometries']:\n            xmin, ymin, xmax, ymax = get_bounds(geometry)\n            xmins.append(xmin)\n            ymins.append(ymin)\n            xmaxs.append(xmax)\n            ymaxs.append(ymax)\n        if north_up:\n            return min(xmins), min(ymins), max(xmaxs), max(ymaxs)\n        else:\n            return min(xmins), max(ymaxs), max(xmaxs), min(ymins)\n\n    elif 'coordinates' in geometry:\n        # Input is a singular geometry object\n        if transform is not None:\n            xyz = list(explode(geometry['coordinates']))\n            xyz_px = [transform * point for point in xyz]\n            xyz = tuple(zip(*xyz_px))\n            return min(xyz[0]), max(xyz[1]), max(xyz[0]), min(xyz[1])\n        else:\n            xyz = tuple(zip(*list(explode(geometry['coordinates']))))\n            if north_up:\n                return min(xyz[0]), min(xyz[1]), max(xyz[0]), max(xyz[1])\n            else:\n                return min(xyz[0]), max(xyz[1]), max(xyz[0]), min(xyz[1])\n\n    # all valid inputs returned above, so whatever falls through is an error\n    raise ValueError(\n            \"geometry must be a GeoJSON-like geometry, GeometryCollection, \"\n            \"or FeatureCollection\"\n        )", "fn_id": 71, "class_fn": false, "repo": "giswqs/eefolium", "file": "eefolium/common.py", "last_update_at": "2022-02-27T14:38:24+00:00", "question_id": "cf3a71a276988b7eff40f4617414c23a9c5cb292_71", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_bounds(geometry, north_up=True, transform=None):\n    \"\"\"Bounding box of a GeoJSON geometry, GeometryCollection, or FeatureCollection.\n    left, bottom, right, top\n    *not* xmin, ymin, xmax, ymax\n    If not north_up, y will be switched to guarantee the above.\n    Source code adapted from https://github.com/mapbox/rasterio/blob/master/rasterio/features.py#L361\n    \"\"\"\n    if 'bbox' in geometry:\n        return tuple(geometry['bbox'])\n    geometry = geometry.get('geometry') or geometry\n    if not ('coordinates' in geometry or 'geometries' in geometry or 'features' in geometry):\n        raise ValueError('geometry must be a GeoJSON-like geometry, GeometryCollection, or FeatureCollection')\n    if 'features' in geometry:\n        xmins = []\n        ymins = []\n        xmaxs = []\n        ymaxs = []\n        for feature in geometry['features']:\n            xmin, ymin, xmax, ymax = get_bounds(feature['geometry'])\n            xmins.append(xmin)\n            ymins.append(ymin)\n            xmaxs.append(xmax)\n            ymaxs.append(ymax)\n        if north_up:\n            return (min(xmins), min(ymins), max(xmaxs), max(ymaxs))\n        else:\n            return (min(xmins), max(ymaxs), max(xmaxs), min(ymins))\n    elif 'geometries' in geometry:\n        xmins = []\n        ymins = []\n        xmaxs = []\n        ymaxs = []\n        for geometry in geometry['geometries']:\n            xmin, ymin, xmax, ymax = get_bounds(geometry)\n            xmins.append(xmin)\n            ymins.append(ymin)\n            xmaxs.append(xmax)\n            ymaxs.append(ymax)\n        if north_up:\n            return (min(xmins), min(ymins), max(xmaxs), max(ymaxs))\n        else:\n            return (min(xmins), max(ymaxs), max(xmaxs), min(ymins))\n    elif 'coordinates' in geometry:\n        if transform is not None:\n            xyz = list(explode(geometry['coordinates']))\n            xyz_px = [transform * point for point in xyz]\n            xyz = tuple(zip(*xyz_px))\n            return (min(xyz[0]), max(xyz[1]), max(xyz[0]), min(xyz[1]))\n        else:\n            xyz = tuple(zip(*list(explode(geometry['coordinates']))))\n            if north_up:\n                return (min(xyz[0]), min(xyz[1]), max(xyz[0]), max(xyz[1]))\n            else:\n                return (min(xyz[0]), max(xyz[1]), max(xyz[0]), min(xyz[1]))\n"]]}
{"hexsha": "2299be24ed211bc0d95a878bff661c9590388b84", "ext": "py", "lang": "Python", "content": "def longest_match_size(str1, str2):\n    sq = SequenceMatcher(lambda x: x == \" \", str1, str2)\n    match = sq.find_longest_match(0, len(str1), 0, len(str2))\n    return match.size", "fn_id": 8, "class_fn": false, "repo": "shibing624/similarities", "file": "similarities/utils/distance.py", "last_update_at": "2022-03-29T07:35:33+00:00", "question_id": "2299be24ed211bc0d95a878bff661c9590388b84_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def longest_match_size(str1, str2):\n    sq = SequenceMatcher(lambda x: x == ' ', str1, str2)\n    match = sq.find_longest_match(0, len(str1), 0, len(str2))\n"]]}
{"hexsha": "4fccc1adaaa4e4ef0dee8bf26d632b6021c2780b", "ext": "py", "lang": "Python", "content": "@app.route(\"/<string:path>/\", methods = ['POST', 'GET'])\ndef path(path):\n\tlang = request.accept_languages.best_match(supported_languages)\n\tret = copy.deepcopy(page)\n\trtbdy = copy.deepcopy(body)\n\tstatus = rtbdy.add_MD(\"md/\"+path+\".\"+lang+\".md\")\n\tret.add_body(str(rtbdy))\n\treturn str(ret), status", "fn_id": 1, "class_fn": false, "repo": "ToCodeABluejay/Webbilangue-Py", "file": "index.py", "last_update_at": "2022-01-12T16:41:12+00:00", "question_id": "4fccc1adaaa4e4ef0dee8bf26d632b6021c2780b_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.route('/<string:path>/', methods=['POST', 'GET'])\ndef path(path):\n    lang = request.accept_languages.best_match(supported_languages)\n    ret = copy.deepcopy(page)\n    rtbdy = copy.deepcopy(body)\n    status = rtbdy.add_MD('md/' + path + '.' + lang + '.md')\n    ret.add_body(str(rtbdy))\n"]]}
{"hexsha": "870830ac6d3ca665607fc2adaf7f3480b48a6334", "ext": "py", "lang": "Python", "content": "def hash_to_point(*data):\n\tresult = ''\n\tfor datum in data:\n\t\tif datum is None:\n\t\t\traise TypeError\n\t\tresult += blake2s(str(datum).encode('utf-8')).hexdigest()\n\n\t# Continue hashing until we get a valid Point\n\twhile True:\n\t\tresult = blake2s(result.encode('utf-8')).hexdigest()\n\t\tif make_point(int(result,16)) is not None:\n\t\t\treturn make_point(int(result,16))*Scalar(cofactor)", "fn_id": 2, "class_fn": false, "repo": "firoorg/spark", "file": "dumb25519.py", "last_update_at": "2022-03-14T02:09:47+00:00", "question_id": "870830ac6d3ca665607fc2adaf7f3480b48a6334_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def hash_to_point(*data):\n    result = ''\n    for datum in data:\n        if datum is None:\n            raise TypeError\n        result += blake2s(str(datum).encode('utf-8')).hexdigest()\n    while True:\n        result = blake2s(result.encode('utf-8')).hexdigest()\n        if make_point(int(result, 16)) is not None:\n"]]}
{"hexsha": "22e6d0f2b2f477b57e84f0fd4e65d192146364a6", "ext": "py", "lang": "Python", "content": "def test_output_module():\n    # pylint: disable=no-value-for-parameter\n\n    @module()\n    def o1(wf: FugueWorkflow, df: WorkflowDataFrame) -> None:\n        pass\n\n    @module()\n    def o2(wf: FugueWorkflow, df: WorkflowDataFrame):\n        pass\n\n    @module()\n    def o3(df: WorkflowDataFrame):\n        pass\n\n    assert o1.has_input\n    assert o1.has_no_output\n    assert o2.has_no_output\n    assert o3.has_no_output\n\n    with FugueWorkflow() as dag:\n        df = dag.df([[0]], \"a:int\")\n        o1(df)\n        o1(dag, df)\n        o2(df=df)\n        o2(df=df, wf=dag)\n        o3(df)", "fn_id": 2, "class_fn": false, "repo": "fugue-project/fugue", "file": "tests/fugue/workflow/test_module.py", "last_update_at": "2022-03-30T23:11:05+00:00", "question_id": "22e6d0f2b2f477b57e84f0fd4e65d192146364a6_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_output_module():\n\n    @module()\n    def o1(wf: FugueWorkflow, df: WorkflowDataFrame) -> None:\n        pass\n\n    @module()\n    def o2(wf: FugueWorkflow, df: WorkflowDataFrame):\n        pass\n\n    @module()\n    def o3(df: WorkflowDataFrame):\n        pass\n    assert o1.has_input\n    assert o1.has_no_output\n    assert o2.has_no_output\n    assert o3.has_no_output\n    with FugueWorkflow() as dag:\n        df = dag.df([[0]], 'a:int')\n        o1(df)\n        o1(dag, df)\n        o2(df=df)\n        o2(df=df, wf=dag)\n"]]}
{"hexsha": "f2796ea144ad193df317596da903d1ebea84da24", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize('contour_plot_2d', [kde_contour_plot_2d,\n                                             fastkde_contour_plot_2d])\n@pytest.mark.parametrize('levels', [[0.9],\n                                    [0.9, 0.6],\n                                    [0.9, 0.6, 0.3],\n                                    [0.9, 0.7, 0.5, 0.3]])\ndef test_contour_plot_2d_levels(contour_plot_2d, levels):\n    try:\n        np.random.seed(42)\n        x = np.random.randn(1000)\n        y = np.random.randn(1000)\n        cmap = plt.cm.viridis\n\n        ax1 = plt.subplot(211)\n        contour_plot_2d(ax1, x, y, levels=levels, cmap=cmap)\n        ax2 = plt.subplot(212)\n        contour_plot_2d(ax2, x, y, levels=levels, cmap=cmap, fc=None)\n\n        # assert that color between filled and unfilled contours matches\n        # first level\n        color1 = ax1.get_children()[0].get_facecolor()  # filled face color\n        color2 = ax2.get_children()[0].get_edgecolor()  # unfilled line color\n        assert_array_equal(color1, color2)\n        # last level\n        color1 = ax1.get_children()[len(levels)-1].get_facecolor()\n        color2 = ax2.get_children()[len(levels)-1].get_edgecolor()\n        assert_array_equal(color1, color2)\n\n        plt.close(\"all\")\n\n    except ImportError:\n        if 'fastkde' not in sys.modules:\n            pass", "fn_id": 12, "class_fn": false, "repo": "Stefan-Heimersheim/anesthetic", "file": "tests/test_plot.py", "last_update_at": "2022-03-01T21:49:16+00:00", "question_id": "f2796ea144ad193df317596da903d1ebea84da24_12", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('contour_plot_2d', [kde_contour_plot_2d, fastkde_contour_plot_2d])\n@pytest.mark.parametrize('levels', [[0.9], [0.9, 0.6], [0.9, 0.6, 0.3], [0.9, 0.7, 0.5, 0.3]])\ndef test_contour_plot_2d_levels(contour_plot_2d, levels):\n    try:\n        np.random.seed(42)\n        x = np.random.randn(1000)\n        y = np.random.randn(1000)\n        cmap = plt.cm.viridis\n        ax1 = plt.subplot(211)\n        contour_plot_2d(ax1, x, y, levels=levels, cmap=cmap)\n        ax2 = plt.subplot(212)\n        contour_plot_2d(ax2, x, y, levels=levels, cmap=cmap, fc=None)\n        color1 = ax1.get_children()[0].get_facecolor()\n        color2 = ax2.get_children()[0].get_edgecolor()\n        assert_array_equal(color1, color2)\n        color1 = ax1.get_children()[len(levels) - 1].get_facecolor()\n        color2 = ax2.get_children()[len(levels) - 1].get_edgecolor()\n        assert_array_equal(color1, color2)\n        plt.close('all')\n    except ImportError:\n        if 'fastkde' not in sys.modules:\n"]]}
{"hexsha": "53e07e5c0b5e8eafa9b65a5cfecd85383fd56e1e", "ext": "py", "lang": "Python", "content": "def test_crosscorr_similarity_batchsize(random_seed=0):\n    rng = np.random.RandomState(random_seed)\n    X = rng.rand(100, 128)\n\n    results = [crosscorr_similarity(X, X, batch_size=batch_size)\n               for batch_size in [None, 50, 2500, 3000]]\n\n    for r1, r2 in zip(results, results[1:]):\n        assert_allclose(r1, r2)", "fn_id": 3, "class_fn": false, "repo": "tonygrey/klsh", "file": "klsh/tests/test_kernels.py", "last_update_at": "2022-03-23T23:18:14+00:00", "question_id": "53e07e5c0b5e8eafa9b65a5cfecd85383fd56e1e_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_crosscorr_similarity_batchsize(random_seed=0):\n    rng = np.random.RandomState(random_seed)\n    X = rng.rand(100, 128)\n    results = [crosscorr_similarity(X, X, batch_size=batch_size) for batch_size in [None, 50, 2500, 3000]]\n    for r1, r2 in zip(results, results[1:]):\n"]]}
{"hexsha": "fbfd24dcc59d1c71738c36857773857b2f53912d", "ext": "py", "lang": "Python", "content": "@pytest.mark.real_data\n@pytest.mark.regression\ndef test_legacy_sample_qc_table(real_data_cache):\n    comparison.assert_legacy_dev_sample_qc_equal(\n        real_data_cache / \"legacy_outputs/all_sample_qc.csv\",\n        real_data_cache / \"dev_outputs/sample_level/sample_qc.csv\",\n    )", "fn_id": 5, "class_fn": false, "repo": "Monia234/NCI-GwasQc", "file": "tests/workflow/sub_workflows/test_sample_qc.py", "last_update_at": "2022-03-16T20:26:55+00:00", "question_id": "fbfd24dcc59d1c71738c36857773857b2f53912d_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.real_data\n@pytest.mark.regression\ndef test_legacy_sample_qc_table(real_data_cache):\n"]]}
{"hexsha": "5e611139ab530bcf30171430af22d7d1c538a2b8", "ext": "py", "lang": "Python", "content": "def _read_image(img_path, normalize=False, dtype=np.float32):\n  img = np.array(Image.open(img_path))\n  if normalize:\n    img = img.astype(np.float32) * 2. / 255. - 1\n  if dtype is not None:\n    img = img.astype(dtype)\n  return img", "fn_id": 1, "class_fn": false, "repo": "MoustafaMeshry/lsr", "file": "data/voxceleb_data_provider_raw.py", "last_update_at": "2022-02-10T19:12:36+00:00", "question_id": "5e611139ab530bcf30171430af22d7d1c538a2b8_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _read_image(img_path, normalize=False, dtype=np.float32):\n    img = np.array(Image.open(img_path))\n    if normalize:\n        img = img.astype(np.float32) * 2.0 / 255.0 - 1\n    if dtype is not None:\n        img = img.astype(dtype)\n"]]}
{"hexsha": "b2e59251331de614416c333e4dce8ecc6593f543", "ext": "py", "lang": "Python", "content": "def _create_pyramid(video, pyramid_levels, pyramid_fn):\n    vid_pyramid = []\n    # frame_count, height, width, colors = video.shape\n    for frame_number, frame in enumerate(video):\n        frame_pyramid = pyramid_fn(frame, pyramid_levels)\n\n        for pyramid_level, pyramid_sub_frame in enumerate(frame_pyramid):\n            if frame_number == 0:\n                vid_pyramid.append(\n                    numpy.zeros((video.shape[0], pyramid_sub_frame.shape[0], pyramid_sub_frame.shape[1], 3),\n                                dtype=\"float\"))\n\n            vid_pyramid[pyramid_level][frame_number] = pyramid_sub_frame\n\n    return vid_pyramid", "fn_id": 2, "class_fn": false, "repo": "MarGetman/EM", "file": "eulerian_magnification/pyramid.py", "last_update_at": "2022-03-25T22:14:51+00:00", "question_id": "b2e59251331de614416c333e4dce8ecc6593f543_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _create_pyramid(video, pyramid_levels, pyramid_fn):\n    vid_pyramid = []\n    for frame_number, frame in enumerate(video):\n        frame_pyramid = pyramid_fn(frame, pyramid_levels)\n        for pyramid_level, pyramid_sub_frame in enumerate(frame_pyramid):\n            if frame_number == 0:\n                vid_pyramid.append(numpy.zeros((video.shape[0], pyramid_sub_frame.shape[0], pyramid_sub_frame.shape[1], 3), dtype='float'))\n            vid_pyramid[pyramid_level][frame_number] = pyramid_sub_frame\n"]]}
{"hexsha": "3f141512af9c79b9eb075660aeccebb06b995877", "ext": "py", "lang": "Python", "content": "def main():\n    global DWH_CLUSTER_IDENTIFIER, DWH_IAM_ROLE_NAME\n    global DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT\n    config_parser()\n\n    ec2 = aws_resource('ec2', \"us-west-2\")\n    s3 = aws_resource('s3', \"us-west-2\")\n    iam = aws_client('iam', \"us-west-2\")\n    redshift = aws_client('redshift', \"us-west-2\")\n\n    roleArn = iam_create_role(iam)\n\n    clusterCreationStarted = init_cluster_creation(redshift, roleArn)\n\n    if clusterCreationStarted:\n        print(\"The cluster is being created.\")\n\n        while True:\n            print(\"Checking if the cluster is created...\")\n\n            if redshift_cluster_status(redshift) == 'available':\n                config_update_cluster(redshift)\n                aws_open_redshift_port(ec2, redshift)\n                break\n            else:\n                print(\"Cluster is still being created. Please wait.\")\n\n            time.sleep(30)\n        print(\"Cluster creation successful.\\n\")\n\n        # Add Redshift to Airflow connections\n        myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n        DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n\n        conn = Connection(\n            conn_id='redshift',\n            conn_type='Postgres',\n            host=DWH_ENDPOINT,\n            login=DWH_DB_USER,\n            password=DWH_DB_PASSWORD,\n            port=DWH_PORT\n        )\n\n        session = settings.Session()\n        session.add(conn)\n        session.commit()\n\n        # Add role to Variables\n        Variable.set(\"aws_iam_role\", DWH_IAM_ROLE_NAME)", "fn_id": 8, "class_fn": false, "repo": "senthilvel-dev/UD_DE", "file": "Project 6 Capstone Project/airflow/aws_cluster_create.py", "last_update_at": "2022-03-28T14:51:13+00:00", "question_id": "3f141512af9c79b9eb075660aeccebb06b995877_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    global DWH_CLUSTER_IDENTIFIER, DWH_IAM_ROLE_NAME\n    global DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT\n    config_parser()\n    ec2 = aws_resource('ec2', 'us-west-2')\n    s3 = aws_resource('s3', 'us-west-2')\n    iam = aws_client('iam', 'us-west-2')\n    redshift = aws_client('redshift', 'us-west-2')\n    roleArn = iam_create_role(iam)\n    clusterCreationStarted = init_cluster_creation(redshift, roleArn)\n    if clusterCreationStarted:\n        print('The cluster is being created.')\n        while True:\n            print('Checking if the cluster is created...')\n            if redshift_cluster_status(redshift) == 'available':\n                config_update_cluster(redshift)\n                aws_open_redshift_port(ec2, redshift)\n                break\n            else:\n                print('Cluster is still being created. Please wait.')\n            time.sleep(30)\n        print('Cluster creation successful.\\n')\n        myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n        DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n        conn = Connection(conn_id='redshift', conn_type='Postgres', host=DWH_ENDPOINT, login=DWH_DB_USER, password=DWH_DB_PASSWORD, port=DWH_PORT)\n        session = settings.Session()\n        session.add(conn)\n        session.commit()\n"]]}
{"hexsha": "e380ab1cf72d434e091b2184598c6076464be8c6", "ext": "py", "lang": "Python", "content": "def process_docs(obj):\n    term_set = set()\n    for doc in obj:\n        if doc['fields']['title'] and doc['fields']['content']:\n            for term in clean_text(doc['fields']['title']):\n                term_set.add(term)\n            for term in clean_text(doc['fields']['content']):\n                term_set.add(term)\n    return term_set", "fn_id": 2, "class_fn": false, "repo": "appotry/sample-apps", "file": "incremental-search/search-suggestions/accepted_words.py", "last_update_at": "2022-03-19T23:26:08+00:00", "question_id": "e380ab1cf72d434e091b2184598c6076464be8c6_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def process_docs(obj):\n    term_set = set()\n    for doc in obj:\n        if doc['fields']['title'] and doc['fields']['content']:\n            for term in clean_text(doc['fields']['title']):\n                term_set.add(term)\n            for term in clean_text(doc['fields']['content']):\n                term_set.add(term)\n"]]}
{"hexsha": "930ab783927cac539ff32ddba9d858892d12dbea", "ext": "py", "lang": "Python", "content": "def test_get_preceding_sibling_by_svg_tag(html_doc):\n    rect_path = xh.get_element_by_svg_tag(\"path\", filter.attribute_contains(\"d\", \"m423\")).get_preceding_sibling_by_svg_tag(\n    \"rect\",\n    filter\n      .attribute_less_than(\"width\", 640)\n      .and_operator(filter.attribute_greater_than_or_equal_to(\"width\", 620))\n     )\n    elements = html_doc.xpath(str(rect_path))\n    assert len(elements) != 0", "fn_id": 29, "class_fn": false, "repo": "jrebecchi/xpath-helper", "file": "python/xpath-helper/tests/test_xpath_helper.py", "last_update_at": "2022-03-09T15:13:23+00:00", "question_id": "930ab783927cac539ff32ddba9d858892d12dbea_29", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_get_preceding_sibling_by_svg_tag(html_doc):\n    rect_path = xh.get_element_by_svg_tag('path', filter.attribute_contains('d', 'm423')).get_preceding_sibling_by_svg_tag('rect', filter.attribute_less_than('width', 640).and_operator(filter.attribute_greater_than_or_equal_to('width', 620)))\n    elements = html_doc.xpath(str(rect_path))\n"]]}
{"hexsha": "b7cb1896adbb88a7924c3d4e797379a7c6fc0e14", "ext": "py", "lang": "Python", "content": "def wait_for(success, timeout=TIMEOUT):\n    start_time = time.time()\n    interval = 0.25\n    while not success() and time.time() < start_time + timeout:\n        time.sleep(interval)\n        interval *= 2\n        if interval > 5:\n            interval = 5\n    if time.time() > start_time + timeout:\n        raise ValueError(\"Timeout waiting for {}\", success)", "fn_id": 0, "class_fn": false, "repo": "joemphilips/plugins", "file": "drain/utils.py", "last_update_at": "2022-03-27T12:14:00+00:00", "question_id": "b7cb1896adbb88a7924c3d4e797379a7c6fc0e14_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def wait_for(success, timeout=TIMEOUT):\n    start_time = time.time()\n    interval = 0.25\n    while not success() and time.time() < start_time + timeout:\n        time.sleep(interval)\n        interval *= 2\n        if interval > 5:\n            interval = 5\n    if time.time() > start_time + timeout:\n"]]}
{"hexsha": "42c0928e47b62e26ce976862771fd390bddeabb1", "ext": "py", "lang": "Python", "content": "def run_demo(cfg, frame_provider):\n    \"\"\"\n    Run visualization visualization.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        frame_provider (iterator): Python iterator that return task objects that are filled\n            with necessary information such as `frames`, `id` and `num_buffer_frames` for the\n            prediction and visualization pipeline.\n    \"\"\"\n    # Set random seed from configs.\n    np.random.seed(cfg.RNG_SEED)\n    torch.manual_seed(cfg.RNG_SEED)\n    # Setup logging format.\n    logging.setup_logging(cfg.OUTPUT_DIR)\n    # Print config.\n    logger.info(\"Run visualization with config:\")\n    logger.info(cfg)\n    common_classes = (\n        cfg.DEMO.COMMON_CLASS_NAMES\n        if len(cfg.DEMO.LABEL_FILE_PATH) != 0\n        else None\n    )\n\n    video_vis = VideoVisualizer(\n        num_classes=cfg.MODEL.HEAD.NUM_CLASSES,\n        class_names_path=cfg.DEMO.LABEL_FILE_PATH,\n        thres=cfg.DEMO.COMMON_CLASS_THRES,\n        lower_thres=cfg.DEMO.UNCOMMON_CLASS_THRES,\n        common_class_names=common_classes,\n        mode=cfg.DEMO.VIS_MODE,\n    )\n\n    async_vis = AsyncVis(video_vis, n_workers=cfg.DEMO.NUM_VIS_INSTANCES)\n\n    if cfg.NUM_GPUS <= 1:\n        model = ActionPredictor(cfg=cfg, async_vis=async_vis)\n    else:\n        model = AsyncActionPredictor(cfg=cfg, async_vis=async_vis)\n\n    seq_len = cfg.DATASETS.CLIP_LEN * cfg.DATASETS.NUM_CLIPS\n\n    assert (\n            cfg.DEMO.BUFFER_SIZE <= seq_len // 2\n    ), \"Buffer size cannot be greater than half of sequence length.\"\n    num_task = 0\n    # Start reading frames.\n    frame_provider.start()\n    for able_to_read, task in frame_provider:\n        if not able_to_read:\n            break\n        if task is None:\n            time.sleep(0.02)\n            continue\n        num_task += 1\n\n        model.put(task)\n        try:\n            task = model.get()\n            num_task -= 1\n            yield task\n        except IndexError:\n            continue\n\n    while num_task != 0:\n        try:\n            task = model.get()\n            num_task -= 1\n            yield task\n        except IndexError:\n            continue", "fn_id": 0, "class_fn": false, "repo": "ZJCV/TSN", "file": "demo/slowfast/demo_net.py", "last_update_at": "2022-03-22T02:39:44+00:00", "question_id": "42c0928e47b62e26ce976862771fd390bddeabb1_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def run_demo(cfg, frame_provider):\n    \"\"\"\n    Run visualization visualization.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        frame_provider (iterator): Python iterator that return task objects that are filled\n            with necessary information such as `frames`, `id` and `num_buffer_frames` for the\n            prediction and visualization pipeline.\n    \"\"\"\n    np.random.seed(cfg.RNG_SEED)\n    torch.manual_seed(cfg.RNG_SEED)\n    logging.setup_logging(cfg.OUTPUT_DIR)\n    logger.info('Run visualization with config:')\n    logger.info(cfg)\n    common_classes = cfg.DEMO.COMMON_CLASS_NAMES if len(cfg.DEMO.LABEL_FILE_PATH) != 0 else None\n    video_vis = VideoVisualizer(num_classes=cfg.MODEL.HEAD.NUM_CLASSES, class_names_path=cfg.DEMO.LABEL_FILE_PATH, thres=cfg.DEMO.COMMON_CLASS_THRES, lower_thres=cfg.DEMO.UNCOMMON_CLASS_THRES, common_class_names=common_classes, mode=cfg.DEMO.VIS_MODE)\n    async_vis = AsyncVis(video_vis, n_workers=cfg.DEMO.NUM_VIS_INSTANCES)\n    if cfg.NUM_GPUS <= 1:\n        model = ActionPredictor(cfg=cfg, async_vis=async_vis)\n    else:\n        model = AsyncActionPredictor(cfg=cfg, async_vis=async_vis)\n    seq_len = cfg.DATASETS.CLIP_LEN * cfg.DATASETS.NUM_CLIPS\n    assert cfg.DEMO.BUFFER_SIZE <= seq_len // 2, 'Buffer size cannot be greater than half of sequence length.'\n    num_task = 0\n    frame_provider.start()\n    for able_to_read, task in frame_provider:\n        if not able_to_read:\n            break\n        if task is None:\n            time.sleep(0.02)\n            continue\n        num_task += 1\n        model.put(task)\n        try:\n            task = model.get()\n            num_task -= 1\n            yield task\n        except IndexError:\n            continue\n    while num_task != 0:\n        try:\n            task = model.get()\n            num_task -= 1\n            yield task\n        except IndexError:\n"]]}
{"hexsha": "5f6bf97eb78b58ba7ed24d4217d65453b5da2324", "ext": "py", "lang": "Python", "content": "def _print_diff_cycle_list(cycles1: Set[count_cycles.Cycle],\n                           cycles2: Set[count_cycles.Cycle], label: str):\n    before_cycles: Set[str] = set(_cycle_str(cycle) for cycle in cycles1)\n    after_cycles: Set[str] = set(_cycle_str(cycle) for cycle in cycles2)\n    _print_set_diff(before_cycles, after_cycles, label)", "fn_id": 4, "class_fn": false, "repo": "zealoussnow/chromium", "file": "tools/android/dependency_analysis/diff_graphs.py", "last_update_at": "2022-03-31T23:33:32+00:00", "question_id": "5f6bf97eb78b58ba7ed24d4217d65453b5da2324_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _print_diff_cycle_list(cycles1: Set[count_cycles.Cycle], cycles2: Set[count_cycles.Cycle], label: str):\n    before_cycles: Set[str] = set((_cycle_str(cycle) for cycle in cycles1))\n    after_cycles: Set[str] = set((_cycle_str(cycle) for cycle in cycles2))\n"]]}
{"hexsha": "7ba7f87405482d6f2c4422d0b47f18f89b626bdb", "ext": "py", "lang": "Python", "content": "def getSettings():\n    if not os.path.isfile(settingsfile):\n        settings = {\n            'connections': {},\n            'max_history': 200000,\n            'max_scroll': 200000,\n            'smooth_scroll': True\n        }\n        saveSettings(settings)\n        return settings\n    with open(settingsfile, 'r') as f:\n        settings = yaml.load(f)\n    return settings", "fn_id": 0, "class_fn": false, "repo": "sinofool/screeps_console", "file": "screeps_console/settings.py", "last_update_at": "2022-01-08T01:27:59+00:00", "question_id": "7ba7f87405482d6f2c4422d0b47f18f89b626bdb_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def getSettings():\n    if not os.path.isfile(settingsfile):\n        settings = {'connections': {}, 'max_history': 200000, 'max_scroll': 200000, 'smooth_scroll': True}\n        saveSettings(settings)\n        return settings\n    with open(settingsfile, 'r') as f:\n        settings = yaml.load(f)\n"]]}
{"hexsha": "283f09dc637fa965bc8af894b623a1eca1b570f5", "ext": "py", "lang": "Python", "content": "def get_rgrids(max_value, rgrid_count):\n    \"\"\"Calculate radial grid steps and return it as a list\"\"\"\n\n    import math\n\n    digits = math.floor(math.log(max_value, 10))\n    scale = max_value / (10**digits)\n\n    ubound = math.ceil(scale) * (10**digits)\n    step = ubound / (rgrid_count+1)\n\n    return list(step * i for i in range(1, rgrid_count+1))", "fn_id": 2, "class_fn": false, "repo": "rahmanifard/lifelib", "file": "lifelib/projects/solvency2/draw_charts_radar.py", "last_update_at": "2022-03-26T20:29:59+00:00", "question_id": "283f09dc637fa965bc8af894b623a1eca1b570f5_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_rgrids(max_value, rgrid_count):\n    \"\"\"Calculate radial grid steps and return it as a list\"\"\"\n    import math\n    digits = math.floor(math.log(max_value, 10))\n    scale = max_value / 10 ** digits\n    ubound = math.ceil(scale) * 10 ** digits\n    step = ubound / (rgrid_count + 1)\n"]]}
{"hexsha": "0681ba6de6bfed398e974cebe20d735df21c208b", "ext": "py", "lang": "Python", "content": "def test_parse_kpoints(vasp_kpoints):\n    \"\"\"\n    Parse a reference KPOINTS file.\n\n    Using the KpointsParser and compare the result to a reference\n    kpoints-node.\n\n    \"\"\"\n\n    kpoints, _ = vasp_kpoints\n\n    try:\n        _ = kpoints.get_attribute('mesh')\n        file_path = data_path('kpoints', 'KPOINTS_mesh')\n        method = 'get_kpoints_mesh'\n        param = 'mesh'\n    except AttributeError:\n        pass\n\n    try:\n        _ = kpoints.get_attribute('array|kpoints')\n        file_path = data_path('kpoints', 'KPOINTS_list')\n        method = 'get_kpoints'\n        param = 'list'\n    except AttributeError:\n        pass\n\n    parser = KpointsParser(file_path=file_path)\n    result = parser.kpoints\n    if param == 'list':\n        assert getattr(result, method)().all() == getattr(kpoints, method)().all()\n    if param == 'mesh':\n        assert getattr(result, method)() == getattr(kpoints, method)()", "fn_id": 0, "class_fn": false, "repo": "muhrin/aiida-vasp", "file": "aiida_vasp/parsers/file_parsers/tests/test_kpoints_parser.py", "last_update_at": "2022-02-25T22:29:12+00:00", "question_id": "0681ba6de6bfed398e974cebe20d735df21c208b_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_parse_kpoints(vasp_kpoints):\n    \"\"\"\n    Parse a reference KPOINTS file.\n\n    Using the KpointsParser and compare the result to a reference\n    kpoints-node.\n\n    \"\"\"\n    kpoints, _ = vasp_kpoints\n    try:\n        _ = kpoints.get_attribute('mesh')\n        file_path = data_path('kpoints', 'KPOINTS_mesh')\n        method = 'get_kpoints_mesh'\n        param = 'mesh'\n    except AttributeError:\n        pass\n    try:\n        _ = kpoints.get_attribute('array|kpoints')\n        file_path = data_path('kpoints', 'KPOINTS_list')\n        method = 'get_kpoints'\n        param = 'list'\n    except AttributeError:\n        pass\n    parser = KpointsParser(file_path=file_path)\n    result = parser.kpoints\n    if param == 'list':\n        assert getattr(result, method)().all() == getattr(kpoints, method)().all()\n    if param == 'mesh':\n"]]}
{"hexsha": "16397b7afcef9d29b49b3213a411a6900817bbae", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize(\n    \"offset,expected\",\n    [\n        ((24 * 60 * 60), {\"id\": \"test\"}),\n        (-(24 * 60 * 60), None),\n    ],\n)\ndef test_actor_cookie_that_expires(app_client, offset, expected):\n    expires_at = int(time.time()) + offset\n    cookie = app_client.ds.sign(\n        {\"a\": {\"id\": \"test\"}, \"e\": baseconv.base62.encode(expires_at)}, \"actor\"\n    )\n    response = app_client.get(\"/\", cookies={\"ds_actor\": cookie})\n    assert expected == app_client.ds._last_request.scope[\"actor\"]", "fn_id": 3, "class_fn": false, "repo": "eyeseast/datasette", "file": "tests/test_auth.py", "last_update_at": "2022-02-05T16:28:49+00:00", "question_id": "16397b7afcef9d29b49b3213a411a6900817bbae_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('offset,expected', [(24 * 60 * 60, {'id': 'test'}), (-(24 * 60 * 60), None)])\ndef test_actor_cookie_that_expires(app_client, offset, expected):\n    expires_at = int(time.time()) + offset\n    cookie = app_client.ds.sign({'a': {'id': 'test'}, 'e': baseconv.base62.encode(expires_at)}, 'actor')\n    response = app_client.get('/', cookies={'ds_actor': cookie})\n"]]}
{"hexsha": "85adfc253bb4c960061f784c6672a7d58767047a", "ext": "py", "lang": "Python", "content": "@login_required\n@permission_required('students.delete_student', raise_exception=True)\ndef debug_delete_students(request: HttpRequest):\n    Student.objects.all().delete()\n    return redirect(\"core:debug\")", "fn_id": 1, "class_fn": false, "repo": "HarshNarayanJha/School-Management-Project", "file": "students/views.py", "last_update_at": "2022-03-15T12:43:58+00:00", "question_id": "85adfc253bb4c960061f784c6672a7d58767047a_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@login_required\n@permission_required('students.delete_student', raise_exception=True)\ndef debug_delete_students(request: HttpRequest):\n    Student.objects.all().delete()\n"]]}
{"hexsha": "ebec4e65e2536c8030d90c981434c0ab7bd826f4", "ext": "py", "lang": "Python", "content": "def get(PATH, mask=None, bg=False, cb=None):\n    if bg:\n        _thread.start_new_thread(\n            INTERNAL.get, [PATH, cb, mask])\n    else:\n        return INTERNAL.get(PATH, cb, mask)", "fn_id": 4, "class_fn": false, "repo": "WoolDoughnut310/micropython-firebase-firestore", "file": "ufirestore/ufirestore.py", "last_update_at": "2022-03-22T04:00:18+00:00", "question_id": "ebec4e65e2536c8030d90c981434c0ab7bd826f4_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get(PATH, mask=None, bg=False, cb=None):\n    if bg:\n        _thread.start_new_thread(INTERNAL.get, [PATH, cb, mask])\n    else:\n"]]}
{"hexsha": "53cb68c5ace8dc97c23abeaf69c885638d283f98", "ext": "py", "lang": "Python", "content": "def ODEs_mean_1(X, t, xi, Pert): \n    S, I, J,  D, H, R, sumI, sumH = X  \n    dSdt = -(BetaI_PINN[-1]  * (1+xi*Pert) *(I+eps1*J+eps2*H)/N)*S - V_1(t)/N*S\n    delay = (BetaI_PINN[-1]  * (1+xi*Pert) *(I+eps1*J+eps2*H)/N)*S  \n    dIdt = delta*delay - Gamma*I \n    dJdt = (1-delta)*delay - gammaA*J\n    dDdt = (q_mean*phiD)*H\n    dHdt = (p_mean*Gamma)*I - (q_mean*phiD) * H - ((1-q_mean)*phiR) * H\n    dRdt = gammaA*J + ((1-p_mean)*Gamma)*I + ((1-q_mean)*phiR)*H + V_1(t)/N*S\n    dsumIdt = delta*delay\n    dsumHdt = (p_mean*Gamma)*I   \n    return [dSdt, dIdt, dJdt, dDdt, dHdt, dRdt, dsumIdt, dsumHdt] ", "fn_id": 2, "class_fn": false, "repo": "ehsankharazmi/PINN-COVID", "file": "ModelUncertainty-NYCData/Integer_Order_Models_I1-I2-I3-T1/Model7_Prediction_All.py", "last_update_at": "2022-03-29T00:32:36+00:00", "question_id": "53cb68c5ace8dc97c23abeaf69c885638d283f98_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ODEs_mean_1(X, t, xi, Pert):\n    S, I, J, D, H, R, sumI, sumH = X\n    dSdt = -(BetaI_PINN[-1] * (1 + xi * Pert) * (I + eps1 * J + eps2 * H) / N) * S - V_1(t) / N * S\n    delay = BetaI_PINN[-1] * (1 + xi * Pert) * (I + eps1 * J + eps2 * H) / N * S\n    dIdt = delta * delay - Gamma * I\n    dJdt = (1 - delta) * delay - gammaA * J\n    dDdt = q_mean * phiD * H\n    dHdt = p_mean * Gamma * I - q_mean * phiD * H - (1 - q_mean) * phiR * H\n    dRdt = gammaA * J + (1 - p_mean) * Gamma * I + (1 - q_mean) * phiR * H + V_1(t) / N * S\n    dsumIdt = delta * delay\n    dsumHdt = p_mean * Gamma * I\n"]]}
