{"hexsha": "ff59713865db17e9ed5fd0305bb8a8400e06518d", "ext": "py", "lang": "Python", "content": "def display_cols(target_table, fields, show_disabled=True, order=None,\n                 output_format=None):\n    \"\"\"\n    Display the columns of data defined by the fields parameter.\n\n    This gets the data from the targets data based on the col_list and prepares\n    and displays a table based on those targets_tbl colums.\n\n    Parameters:\n      fields: list of strings defining the targets_data columns to be\n        displayed.\n\n      target_table: The targets table from the database\n\n      order (:term: `string`): None or name of field upon which the table will\n        be sorted for output\n\n      show_disabled(:class:`py:bool`)\n        If True, show disabled entries. If not True, entries marked disabled\n        are ignored\n\n    \"\"\"\n    if show_disabled:\n        if 'ScanEnabled' not in fields:\n            fields.append('ScanEnabled')\n\n    table_width = target_table.get_output_width(fields) + len(fields)\n    # TODO. the above is incorrect in that some fields are of indeterminate\n    # length.  The definition in targetstable is not correct.\n    fold = False if table_width < 80 else True\n\n    if show_disabled:\n        target_ids = sorted(target_table.keys())\n    else:\n        target_ids = sorted(target_table.get_enabled_targetids())\n\n    # If order defined check to see if valid field\n    if order:\n        if order not in target_table.get_field_list():\n            raise click.ClickException(\"--order option defines invalid field %s\"\n                                       % order)\n\n        # create dictionary with order value as key and list of targetids as\n        # value. List because the order fields are not unique\n        order_dict = defaultdict(list)\n        for targetid in target_ids:\n            order_dict[target_table[targetid][order]].append(targetid)\n        # order_dict = {target_table[targetid][order]: targetid\n        #               for targetid in target_ids}\n        # TODO this may be inefficient means to sort by keys and get values\n        # into list\n        target_ids = []\n        for key in sorted(order_dict.keys()):\n            target_ids.extend(order_dict[key])\n\n    rows = []\n    for targetid in target_ids:\n        rows.append(target_table.format_record(targetid, fields, fold))\n\n    headers = target_table.tbl_hdr(fields)\n    title = 'Target Providers Overview: %s:' % \\\n            datetime_display_str(datetime.datetime.now())\n    if show_disabled:\n        title = '%s including disabled targets' % title\n\n    print_table(rows, headers=headers, title=title,\n                table_format=output_format)", "fn_id": 12, "class_fn": false, "repo": "KSchopmeyer/smipyping", "file": "smicli/_cmd_targets.py", "last_update_at": "2020-03-04T19:31:59+00:00", "question_id": "ff59713865db17e9ed5fd0305bb8a8400e06518d_12", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def display_cols(target_table, fields, show_disabled=True, order=None, output_format=None):\n    \"\"\"\n    Display the columns of data defined by the fields parameter.\n\n    This gets the data from the targets data based on the col_list and prepares\n    and displays a table based on those targets_tbl colums.\n\n    Parameters:\n      fields: list of strings defining the targets_data columns to be\n        displayed.\n\n      target_table: The targets table from the database\n\n      order (:term: `string`): None or name of field upon which the table will\n        be sorted for output\n\n      show_disabled(:class:`py:bool`)\n        If True, show disabled entries. If not True, entries marked disabled\n        are ignored\n\n    \"\"\"\n    if show_disabled:\n        if 'ScanEnabled' not in fields:\n            fields.append('ScanEnabled')\n    table_width = target_table.get_output_width(fields) + len(fields)\n    fold = False if table_width < 80 else True\n    if show_disabled:\n        target_ids = sorted(target_table.keys())\n    else:\n        target_ids = sorted(target_table.get_enabled_targetids())\n    if order:\n        if order not in target_table.get_field_list():\n            raise click.ClickException('--order option defines invalid field %s' % order)\n        order_dict = defaultdict(list)\n        for targetid in target_ids:\n            order_dict[target_table[targetid][order]].append(targetid)\n        target_ids = []\n        for key in sorted(order_dict.keys()):\n            target_ids.extend(order_dict[key])\n    rows = []\n    for targetid in target_ids:\n        rows.append(target_table.format_record(targetid, fields, fold))\n    headers = target_table.tbl_hdr(fields)\n    title = 'Target Providers Overview: %s:' % datetime_display_str(datetime.datetime.now())\n    if show_disabled:\n        title = '%s including disabled targets' % title\n"]]}
{"hexsha": "6ca24807df6446ce04faf8cc3dba08f6450d2fe3", "ext": "py", "lang": "Python", "content": "def push_notification(request):\n    notification_type = request.get('type')\n    user_source = request.get('source')\n    user_target = request.get('target')\n    post_id = request.get('post_id')\n    return {\n        NotificationType.FOLLOW : lambda: send_follow_notification(user_target, user_source),\n        NotificationType.UNFOLLOW : lambda: remove_follow_notification(user_target, user_source),\n        NotificationType.LIKE : lambda: send_like_notification(user_target, user_source, post_id),\n        NotificationType.DISLIKE : lambda: remove_like_notification(user_target, user_source, post_id),\n        NotificationType.COMMENT : lambda: send_comment_notification(user_target, user_source, post_id)\n    }[notification_type]()", "fn_id": 0, "class_fn": false, "repo": "PNBenfica/Tipsters", "file": "src/backend/NotificationsManager.py", "last_update_at": "2020-12-08T02:56:28+00:00", "question_id": "6ca24807df6446ce04faf8cc3dba08f6450d2fe3_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def push_notification(request):\n    notification_type = request.get('type')\n    user_source = request.get('source')\n    user_target = request.get('target')\n    post_id = request.get('post_id')\n"]]}
{"hexsha": "18c51a1886214e619b1ff8291453dddf2f1a0263", "ext": "py", "lang": "Python", "content": "@borg.on(admin_cmd(\"speed ?(.*)\"))\nasync def _(event):\n    if event.fwd_from:\n        return\n    input_str = event.pattern_match.group(1)\n    as_text = True\n    as_document = False\n    if input_str == \"image\":\n        as_document = False\n    elif input_str == \"file\":\n        as_document = True\n    elif input_str == \"text\":\n        as_text = True\n    await event.edit(\"`Calculating my internet speed. Please wait!`\")\n    start = datetime.now()\n    s = speedtest.Speedtest()\n    s.get_best_server()\n    s.download()\n    s.upload()\n    end = datetime.now()\n    ms = (end - start).microseconds / 1000\n    response = s.results.dict()\n    download_speed = response.get(\"download\")\n    upload_speed = response.get(\"upload\")\n    ping_time = response.get(\"ping\")\n    client_infos = response.get(\"client\")\n    i_s_p = client_infos.get(\"isp\")\n    i_s_p_rating = client_infos.get(\"isprating\")\n    reply_msg_id = event.message.id\n    if event.reply_to_msg_id:\n        reply_msg_id = event.reply_to_msg_id\n    try:\n        response = s.results.share()\n        speedtest_image = response\n        if as_text:\n            await event.edit(\"\"\"`SpeedTest completed in {} seconds`\n\n`Download: {}`\n`Upload: {}`\n`Ping: {}`\n`Internet Service Provider: {}`\n`ISP Rating: {}`\"\"\".format(ms, convert_from_bytes(download_speed), convert_from_bytes(upload_speed), ping_time, i_s_p, i_s_p_rating))\n        else:\n            await borg.send_file(\n                event.chat_id,\n                speedtest_image,\n                caption=\"**SpeedTest** completed in {} seconds\".format(ms),\n                force_document=as_document,\n                reply_to=reply_msg_id,\n                allow_cache=False\n            )\n            await event.delete()\n    except Exception as exc:\n        await event.edit(\"\"\"**SpeedTest** completed in {} seconds\nDownload: {}\nUpload: {}\nPing: {}\n\n__With the Following ERRORs__\n{}\"\"\".format(ms, convert_from_bytes(download_speed), convert_from_bytes(upload_speed), ping_time, str(exc)))", "fn_id": 2, "class_fn": false, "repo": "kwkwkkw/ironbot", "file": "userbot/plugins/extra.py", "last_update_at": "2020-09-16T09:55:12+00:00", "question_id": "18c51a1886214e619b1ff8291453dddf2f1a0263_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@borg.on(admin_cmd('speed ?(.*)'))\nasync def _(event):\n    if event.fwd_from:\n        return\n    input_str = event.pattern_match.group(1)\n    as_text = True\n    as_document = False\n    if input_str == 'image':\n        as_document = False\n    elif input_str == 'file':\n        as_document = True\n    elif input_str == 'text':\n        as_text = True\n    await event.edit('`Calculating my internet speed. Please wait!`')\n    start = datetime.now()\n    s = speedtest.Speedtest()\n    s.get_best_server()\n    s.download()\n    s.upload()\n    end = datetime.now()\n    ms = (end - start).microseconds / 1000\n    response = s.results.dict()\n    download_speed = response.get('download')\n    upload_speed = response.get('upload')\n    ping_time = response.get('ping')\n    client_infos = response.get('client')\n    i_s_p = client_infos.get('isp')\n    i_s_p_rating = client_infos.get('isprating')\n    reply_msg_id = event.message.id\n    if event.reply_to_msg_id:\n        reply_msg_id = event.reply_to_msg_id\n    try:\n        response = s.results.share()\n        speedtest_image = response\n        if as_text:\n            await event.edit('`SpeedTest completed in {} seconds`\\n\\n`Download: {}`\\n`Upload: {}`\\n`Ping: {}`\\n`Internet Service Provider: {}`\\n`ISP Rating: {}`'.format(ms, convert_from_bytes(download_speed), convert_from_bytes(upload_speed), ping_time, i_s_p, i_s_p_rating))\n        else:\n            await borg.send_file(event.chat_id, speedtest_image, caption='**SpeedTest** completed in {} seconds'.format(ms), force_document=as_document, reply_to=reply_msg_id, allow_cache=False)\n            await event.delete()\n    except Exception as exc:\n"]]}
{"hexsha": "5c2ee2d33e6d6e2c2be0782ba6f20e9865d4008f", "ext": "py", "lang": "Python", "content": "def edit_distance(str1, str2, m, n):\n\n    # If either string is empty, the answer is length of the other string\n    # because we would have to insert that many\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n  \n    # If last two characters are same, just compare for the rest of the string\n    if str1[n-1] == str2[m-1]:\n        return edit_distance(str1, str2, m-1, n-1)\n  \n    # If characters aren't the same, just consider all the options\n    # these are relative to string 2\n    return 1 + min(edit_distance(str1, str2, m, n-1),       # insert\n                   edit_distance(str1, str2, m-1, n),       # remove\n                   edit_distance(str1, str2, m-1, n-1))     # replace", "fn_id": 0, "class_fn": false, "repo": "vsoch/algorithms", "file": "edit-distance/run.py", "last_update_at": "2020-07-04T15:45:15+00:00", "question_id": "5c2ee2d33e6d6e2c2be0782ba6f20e9865d4008f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def edit_distance(str1, str2, m, n):\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n    if str1[n - 1] == str2[m - 1]:\n        return edit_distance(str1, str2, m - 1, n - 1)\n"]]}
{"hexsha": "1ef274404dd1388e716e84806cab52c4e315fe83", "ext": "py", "lang": "Python", "content": "def status_robo(pytradebot_id):\n    \"\"\"\n    Met\u00f3do para retornar o estado do rob\u00f4 e atualizar o saldo da carteira\n    :param pytradebot_id: Par\u00e2metro de id do rob\u00f4 com o id do usu\u00e1rio\n    :return: Retorna um dicion\u00e1rio contendo o estado e a a\u00e7\u00e3o\n    \"\"\"\n    resultado = {\n        \"estado\": \"Offline\",\n        \"acao\": \"iniciar\"\n    }\n    # Chama o m\u00e9todo para executar o comando do rob\u00f4 para verificar se est\u00e1 online\n    process = comando_robo(pytradebot_id=pytradebot_id, comando='online')\n    # Se o rob\u00f4 estiver sendo executado o processo retorna c\u00f3digo 0\n    if process['code'] == 0:\n        # Remove os caracteres desnecess\u00e1rios\n        convert_output = str(process['out']).lstrip('(b\"[')\n        convert_output2 = convert_output.rstrip(f']\\\\n\"')\n        # Converte para uma tuple\n        json_tuple = ast.literal_eval(convert_output2)\n        # Verificar o estado do rob\u00f4\n        if json_tuple['status'] == 'stopped':\n            resultado['estado'] = 'Parado'\n            resultado['acao'] = 'retomar'\n        elif json_tuple['status'] == 'running':\n            resultado['estado'] = 'Operando'\n            resultado['acao'] = 'parar'\n        elif json_tuple['status'] == 'running-no-buy':\n            resultado['estado'] = 'Operando (Sem Comprar)'\n            resultado['acao'] = 'parar'\n\n    user_model = get_user_model()\n    usuario = user_model.objects.get(pk=pytradebot_id)\n    # Se o usu\u00e1rio tiver carteira cadastrada\n    if CarteiraCriptomoeda.objects.filter(usuario=usuario).exists():\n        # Verifica se o rob\u00f4 est\u00e1 operando\n        if resultado['estado'] == 'Operando (Sem Comprar)' or resultado['estado'] == 'Operando':\n            carteira = CarteiraCriptomoeda.objects.get(usuario=usuario)\n            # Executa um subprocesso com o comando para retornar o saldo da carteira\n            # com base do saldo na Exchange\n            process = comando_robo(pytradebot_id=pytradebot_id, comando='balance')\n            # Se o processo retornar c\u00f3digo 0 (sucesso)\n            if process['code'] == 0:\n                # Remove os caracteres desnecess\u00e1rios\n                convert_output = str(process['out']).lstrip('(b\"[')\n                convert_output2 = convert_output.rstrip(f']\\\\n\"')\n                # Converte para uma tuple\n                json_tuple = ast.literal_eval(convert_output2)\n                # Converte a tuple para String, trocando aspas simples para aspas duplas\n                # e depois converte a String para JSON\n                json_string = json.loads(str(json_tuple).replace(\"\\'\", \"\\\"\"))\n                # Pega o saldo total equivalente em Bitcoin e\n                # converte para float com 2 casas decimais\n                temp_btc = float(json_string['total'])\n                saldo_btc = format(temp_btc, '.8f')\n                carteira.saldo = saldo_btc\n                carteira.save()\n\n    return resultado", "fn_id": 3, "class_fn": false, "repo": "NatanNMB15/tcc-pytradebot", "file": "Back-End/pytradebot/mysite/views.py", "last_update_at": "2020-05-13T14:12:42+00:00", "question_id": "1ef274404dd1388e716e84806cab52c4e315fe83_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def status_robo(pytradebot_id):\n    \"\"\"\n    Met\u00f3do para retornar o estado do rob\u00f4 e atualizar o saldo da carteira\n    :param pytradebot_id: Par\u00e2metro de id do rob\u00f4 com o id do usu\u00e1rio\n    :return: Retorna um dicion\u00e1rio contendo o estado e a a\u00e7\u00e3o\n    \"\"\"\n    resultado = {'estado': 'Offline', 'acao': 'iniciar'}\n    process = comando_robo(pytradebot_id=pytradebot_id, comando='online')\n    if process['code'] == 0:\n        convert_output = str(process['out']).lstrip('(b\"[')\n        convert_output2 = convert_output.rstrip(f']\\\\n\"')\n        json_tuple = ast.literal_eval(convert_output2)\n        if json_tuple['status'] == 'stopped':\n            resultado['estado'] = 'Parado'\n            resultado['acao'] = 'retomar'\n        elif json_tuple['status'] == 'running':\n            resultado['estado'] = 'Operando'\n            resultado['acao'] = 'parar'\n        elif json_tuple['status'] == 'running-no-buy':\n            resultado['estado'] = 'Operando (Sem Comprar)'\n            resultado['acao'] = 'parar'\n    user_model = get_user_model()\n    usuario = user_model.objects.get(pk=pytradebot_id)\n    if CarteiraCriptomoeda.objects.filter(usuario=usuario).exists():\n        if resultado['estado'] == 'Operando (Sem Comprar)' or resultado['estado'] == 'Operando':\n            carteira = CarteiraCriptomoeda.objects.get(usuario=usuario)\n            process = comando_robo(pytradebot_id=pytradebot_id, comando='balance')\n            if process['code'] == 0:\n                convert_output = str(process['out']).lstrip('(b\"[')\n                convert_output2 = convert_output.rstrip(f']\\\\n\"')\n                json_tuple = ast.literal_eval(convert_output2)\n                json_string = json.loads(str(json_tuple).replace(\"'\", '\"'))\n                temp_btc = float(json_string['total'])\n                saldo_btc = format(temp_btc, '.8f')\n                carteira.saldo = saldo_btc\n                carteira.save()\n"]]}
{"hexsha": "29660d157f32a58dfbbbeb98f373bb3e4e42bc15", "ext": "py", "lang": "Python", "content": "@archives.route(\"/fs\", defaults={'req_path': ''})\n@archives.route(\"/fs/<path:req_path>\")\ndef dir_listing(req_path):\n    req_path = __resolve_path(req_path)\n    #print(\"archives\", divisor)\n    #trace('dir_listing')\n    abs_path = _get_req_absolute_path(req_path)\n    # Check if path is a file and serve\n    if os.path.isfile(abs_path):\n        return send_file(abs_path)\n    (files_info, locations, isRoot, folderEnv) = _dir_listing(req_path)\n    return render_template('archives_view.html', files_info=files_info, locations=locations, isRoot=isRoot, folderEnv=folderEnv)", "fn_id": 4, "class_fn": false, "repo": "Lukasavicus/WindMill", "file": "windmill/archives/routes.py", "last_update_at": "2020-09-04T17:28:09+00:00", "question_id": "29660d157f32a58dfbbbeb98f373bb3e4e42bc15_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@archives.route('/fs', defaults={'req_path': ''})\n@archives.route('/fs/<path:req_path>')\ndef dir_listing(req_path):\n    req_path = __resolve_path(req_path)\n    abs_path = _get_req_absolute_path(req_path)\n    if os.path.isfile(abs_path):\n        return send_file(abs_path)\n    files_info, locations, isRoot, folderEnv = _dir_listing(req_path)\n"]]}
{"hexsha": "6b6e4879a7f160c3a987091402db4e7bffcd3699", "ext": "py", "lang": "Python", "content": "def test_cd_dataframe():\n\tdf = pd.DataFrame({\"Voltage(V)\":[1,2,3], \"Smoothed_dQ/dV\": [4,5,6]})\n\tproper_V = df['Voltage(V)']\n\tproper_dqdv = df[\"Smoothed_dQ/dV\"]\n\ttry:\n\t\tdescriptors.fitters.cd_dataframe(proper_V,proper_dqdv, 'd')\n\t\t#this should fail because if dqdv is negative, it is probably discharge data and we want to flip to be positive for peak finding\n\texcept (AssertionError):\n\t\tpass\n\telse:\n\t\traise Exception (\"Exception not handled by asserts\")\n\n\treturn", "fn_id": 18, "class_fn": false, "repo": "tacohen125/chachies", "file": "chachies/tests/test_descriptors.py", "last_update_at": "2020-05-20T18:22:56+00:00", "question_id": "6b6e4879a7f160c3a987091402db4e7bffcd3699_18", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_cd_dataframe():\n    df = pd.DataFrame({'Voltage(V)': [1, 2, 3], 'Smoothed_dQ/dV': [4, 5, 6]})\n    proper_V = df['Voltage(V)']\n    proper_dqdv = df['Smoothed_dQ/dV']\n    try:\n        descriptors.fitters.cd_dataframe(proper_V, proper_dqdv, 'd')\n    except AssertionError:\n        pass\n    else:\n        raise Exception('Exception not handled by asserts')\n"]]}
{"hexsha": "c49375650ec8435cc5750eabedccb5b6e7df7aae", "ext": "py", "lang": "Python", "content": "def result_path_to_dict(result_path):\n    result_basename = os.path.basename(result_path).split('.')[0]\n    temp_result_dict = {}\n\n    with open(result_path,'r') as f:\n        for line in f:\n            if not line.startswith(\"#\"):\n                temp_line = [result_basename]\n                temp_line += split_str_rem_whitespace(line)[:22]\n                temp_line[5] = temp_line[5].split(\".\")[0]\n\n                if temp_line[1] in temp_result_dict.keys():\n                    temp_result_dict[temp_line[1]].append(temp_line)\n                else:\n                    temp_result_dict[temp_line[1]] = [temp_line]\n    return temp_result_dict", "fn_id": 0, "class_fn": false, "repo": "visanuwan/prodmx", "file": "prodmx/pfam_filter.py", "last_update_at": "2020-12-21T07:29:00+00:00", "question_id": "c49375650ec8435cc5750eabedccb5b6e7df7aae_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def result_path_to_dict(result_path):\n    result_basename = os.path.basename(result_path).split('.')[0]\n    temp_result_dict = {}\n    with open(result_path, 'r') as f:\n        for line in f:\n            if not line.startswith('#'):\n                temp_line = [result_basename]\n                temp_line += split_str_rem_whitespace(line)[:22]\n                temp_line[5] = temp_line[5].split('.')[0]\n                if temp_line[1] in temp_result_dict.keys():\n                    temp_result_dict[temp_line[1]].append(temp_line)\n                else:\n                    temp_result_dict[temp_line[1]] = [temp_line]\n"]]}
{"hexsha": "075b83f5bee7dd380ccf7b6290b97b9f0f35720b", "ext": "py", "lang": "Python", "content": "def test_get_next_with_no_child_and_some_open_neighbors_returns_first_available():\n    \"\"\"\n    Verifies that `get_next` returns the first available neighbor if some are open.\n    \"\"\"\n    grid = Grid(((3, -1, -1), (-1, 2, -1), (0, -1, -1)))\n    # Set partial path from \"3\"\n    draw_path(grid, ((0, 0), (0, 1), (0, 2), (1, 2)))\n    assert get_next(grid[1][1]).location == (2, 1)", "fn_id": 16, "class_fn": false, "repo": "andrewtbiehl/gaslines", "file": "tests/test_logic.py", "last_update_at": "2020-09-10T04:41:51+00:00", "question_id": "075b83f5bee7dd380ccf7b6290b97b9f0f35720b_16", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_get_next_with_no_child_and_some_open_neighbors_returns_first_available():\n    \"\"\"\n    Verifies that `get_next` returns the first available neighbor if some are open.\n    \"\"\"\n    grid = Grid(((3, -1, -1), (-1, 2, -1), (0, -1, -1)))\n    draw_path(grid, ((0, 0), (0, 1), (0, 2), (1, 2)))\n"]]}
{"hexsha": "e68a49671a938cba7d7700ecdb66d7a7f6d794d6", "ext": "py", "lang": "Python", "content": "def log(message):\n\tglobal args\n\ttry:args\n\texcept NameError:pass\n\telse:\n\t\tif args.verbose:\n\t\t\tstdout.write('%s\\n'%message)\n\t\t\tif args.debug:\n\t\t\t\tif message.startswith('[%s]'%args.debug.upper()):\n\t\t\t\t\texit(1)", "fn_id": 1, "class_fn": false, "repo": "Louae-Skript/ytviewer", "file": "main.py", "last_update_at": "2020-12-29T16:56:11+00:00", "question_id": "e68a49671a938cba7d7700ecdb66d7a7f6d794d6_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def log(message):\n    global args\n    try:\n        args\n    except NameError:\n        pass\n    else:\n        if args.verbose:\n            stdout.write('%s\\n' % message)\n            if args.debug:\n                if message.startswith('[%s]' % args.debug.upper()):\n"]]}
{"hexsha": "44e92408042831c3a6a471f99d878ae32e8d1431", "ext": "py", "lang": "Python", "content": "async def ses_webhook(request):\n    if not compare_digest(request.app['settings'].ses_url_token, request.match_info['token']):\n        raise JsonErrors.HTTPForbidden('invalid url')\n\n    # content type is plain text for SNS, so we have to decode json manually\n    try:\n        data = json.loads(await request.text())\n    except ValueError:\n        raise JsonErrors.HTTPBadRequest('invalid json')\n\n    # avoid keeping multiple copies of the request data in memory\n    request._read_byte = None\n\n    await verify_sns(request.app, data)\n\n    sns_type = data['Type']\n    if sns_type == 'SubscriptionConfirmation':\n        logger.info('confirming aws Subscription')\n        # TODO check we actually want this subscription\n        async with request.app['http_client'].head(data['SubscribeURL'], raise_for_status=True):\n            pass\n    else:\n        assert sns_type == 'Notification', sns_type\n        raw_msg = data['Message']\n        # TODO any other messages to ignore?\n        if raw_msg != 'Successfully validated SNS topic for Amazon SES event publishing.':\n            message = json.loads(raw_msg)\n            del data\n            if message.get('notificationType') == 'Received':\n                await asyncio.shield(_record_email_message(request, message))\n            else:\n                await asyncio.shield(_record_email_event(request, message))\n    return Response(status=204)", "fn_id": 0, "class_fn": false, "repo": "samuelcolvin/em2", "file": "em2/protocol/views/smtp_ses.py", "last_update_at": "2020-10-03T01:16:05+00:00", "question_id": "44e92408042831c3a6a471f99d878ae32e8d1431_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def ses_webhook(request):\n    if not compare_digest(request.app['settings'].ses_url_token, request.match_info['token']):\n        raise JsonErrors.HTTPForbidden('invalid url')\n    try:\n        data = json.loads(await request.text())\n    except ValueError:\n        raise JsonErrors.HTTPBadRequest('invalid json')\n    request._read_byte = None\n    await verify_sns(request.app, data)\n    sns_type = data['Type']\n    if sns_type == 'SubscriptionConfirmation':\n        logger.info('confirming aws Subscription')\n        async with request.app['http_client'].head(data['SubscribeURL'], raise_for_status=True):\n            pass\n    else:\n        assert sns_type == 'Notification', sns_type\n        raw_msg = data['Message']\n        if raw_msg != 'Successfully validated SNS topic for Amazon SES event publishing.':\n            message = json.loads(raw_msg)\n            del data\n            if message.get('notificationType') == 'Received':\n                await asyncio.shield(_record_email_message(request, message))\n            else:\n                await asyncio.shield(_record_email_event(request, message))\n"]]}
{"hexsha": "71a185bd30d18da2519ede069f3fb3bae056479b", "ext": "py", "lang": "Python", "content": "def Kaggle_IoU_Precision(y_true, y_pred, threshold=0.5):\n    y_pred = K.squeeze(tf.cast(y_pred > threshold, tf.int32), -1)\n    y_true = K.cast(y_true[..., 0], K.floatx())\n    y_pred = K.cast(y_pred, K.floatx())\n    truth_areas = K.sum(y_true, axis=[1, 2])\n    pred_areas = K.sum(y_pred, axis=[1, 2])\n    intersection = K.sum(y_true * y_pred, axis=[1, 2])\n    union = K.clip(truth_areas + pred_areas - intersection, 1e-9, 128 * 128)\n    check = K.map_fn(lambda x: K.equal(x, 0),\n                     truth_areas + pred_areas, dtype=tf.bool)\n    p = intersection / union\n    iou = K.switch(check, p + 1., p)\n\n    prec = K.map_fn(lambda x: K.mean(\n        K.greater(x, np.arange(0.5, 1.0, 0.05))), iou, dtype=tf.float32)\n\n    prec_iou = K.mean(prec)\n    return prec_iou", "fn_id": 1, "class_fn": false, "repo": "bo9zbo9z/MachineLearning", "file": "lib/losses_and_metrics/Losses_Babakhin.py", "last_update_at": "2020-06-25T01:48:18+00:00", "question_id": "71a185bd30d18da2519ede069f3fb3bae056479b_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def Kaggle_IoU_Precision(y_true, y_pred, threshold=0.5):\n    y_pred = K.squeeze(tf.cast(y_pred > threshold, tf.int32), -1)\n    y_true = K.cast(y_true[..., 0], K.floatx())\n    y_pred = K.cast(y_pred, K.floatx())\n    truth_areas = K.sum(y_true, axis=[1, 2])\n    pred_areas = K.sum(y_pred, axis=[1, 2])\n    intersection = K.sum(y_true * y_pred, axis=[1, 2])\n    union = K.clip(truth_areas + pred_areas - intersection, 1e-09, 128 * 128)\n    check = K.map_fn(lambda x: K.equal(x, 0), truth_areas + pred_areas, dtype=tf.bool)\n    p = intersection / union\n    iou = K.switch(check, p + 1.0, p)\n    prec = K.map_fn(lambda x: K.mean(K.greater(x, np.arange(0.5, 1.0, 0.05))), iou, dtype=tf.float32)\n    prec_iou = K.mean(prec)\n"]]}
{"hexsha": "66f57937dfe7f1fd6761886efdb63338d19ce62a", "ext": "py", "lang": "Python", "content": "def create_user_pack_roles(mycursor):\n    '''create table for admin roles for packs\n        there can be only one role per user-pack pair'''\n    mycursor.execute(\n        '''\n        create table if not exists user_pack_roles (\n            user_id int,\n            pack_id int,\n            role varchar(255),\n            user_granted_id int,\n            granted_dttm datetime\n            );\n        '''\n    )", "fn_id": 5, "class_fn": false, "repo": "greeshka/sticker-shortcut-bot", "file": "setup_database.py", "last_update_at": "2020-12-23T22:39:07+00:00", "question_id": "66f57937dfe7f1fd6761886efdb63338d19ce62a_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_user_pack_roles(mycursor):\n    \"\"\"create table for admin roles for packs\n        there can be only one role per user-pack pair\"\"\"\n"]]}
{"hexsha": "81d0a0210737aab52b87876253b5e3ba1dc52858", "ext": "py", "lang": "Python", "content": "def _get_unused_letters(used_letters):\n    doubles = [first + second for second in string.ascii_lowercase\n               for first in string.ascii_lowercase]\n    all_letters = set(list(string.ascii_lowercase) + doubles)\n    letters = list(all_letters - used_letters)\n    # NOTE(vish): prepend ` so all shorter sequences sort first\n    letters.sort(key=lambda x: x.rjust(2, '`'))\n    return letters[0]", "fn_id": 2, "class_fn": false, "repo": "bopopescu/stacklab-nova", "file": "debian/python-nova/usr/lib/python2.7/dist-packages/nova/compute/utils.py", "last_update_at": "2020-07-24T08:31:57+00:00", "question_id": "81d0a0210737aab52b87876253b5e3ba1dc52858_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _get_unused_letters(used_letters):\n    doubles = [first + second for second in string.ascii_lowercase for first in string.ascii_lowercase]\n    all_letters = set(list(string.ascii_lowercase) + doubles)\n    letters = list(all_letters - used_letters)\n    letters.sort(key=lambda x: x.rjust(2, '`'))\n"]]}
{"hexsha": "abc8b8aad9073f9b6658bee489bd7146e5d7c3ad", "ext": "py", "lang": "Python", "content": "def get_earliest_sct(xcert):\n    \"\"\"Calculate the earliest time this certificate was logged to a CT log.\n\n    If it was not logged by the CA, then the not_before time is returned.\n\n    Arguments:\n    xcert -- an x509 certificate object\n\n    Returns (datetime, bool):\n        datetime: the earliest calculated date.\n        bool: True if an SCT was used, False otherwise\n\n    \"\"\"\n    try:\n        earliest = datetime.max\n        scts = xcert.extensions.get_extension_for_class(\n            x509.PrecertificateSignedCertificateTimestamps\n        ).value\n        for sct in scts:\n            earliest = min(earliest, sct.timestamp)\n        return earliest, True\n    except x509.extensions.ExtensionNotFound:\n        return xcert.not_valid_before, False", "fn_id": 1, "class_fn": false, "repo": "mcdonnnj/admiral", "file": "src/admiral/model/cert.py", "last_update_at": "2020-01-20T00:49:34+00:00", "question_id": "abc8b8aad9073f9b6658bee489bd7146e5d7c3ad_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_earliest_sct(xcert):\n    \"\"\"Calculate the earliest time this certificate was logged to a CT log.\n\n    If it was not logged by the CA, then the not_before time is returned.\n\n    Arguments:\n    xcert -- an x509 certificate object\n\n    Returns (datetime, bool):\n        datetime: the earliest calculated date.\n        bool: True if an SCT was used, False otherwise\n\n    \"\"\"\n    try:\n        earliest = datetime.max\n        scts = xcert.extensions.get_extension_for_class(x509.PrecertificateSignedCertificateTimestamps).value\n        for sct in scts:\n            earliest = min(earliest, sct.timestamp)\n        return (earliest, True)\n    except x509.extensions.ExtensionNotFound:\n"]]}
{"hexsha": "4278fefc3d350168b06ea3d81a1c98f054b4d235", "ext": "py", "lang": "Python", "content": "def set_inputfile(hgrid, dico):\n    basicinput = \"\"\"\nlogfile: Yes\ndft:\n    ixc: PBE\n    ncong: 2\n    rmult: [10, 8]\n    itermax: 3\n    idsx: 0\n    gnrm_cv: 1e-8\n#Control the diagonalisation scheme\nmix:\n    iscf: 7\n    itrpmax: 200\n    rpnrm_cv: 1e-12\n    tel: 1e-3\n    alphamix: 0.5\n    norbsempty: 1000\n    alphadiis: 1.0\n\n#perf:\n#    accel: OCLGPU\n#    ocl_devices: Tesla K40c\n#    blas: Yes\n\n\"\"\"\n    import yaml\n    import os\n    var = yaml.load(basicinput)\n    # Spin parameters\n    var[\"dft\"].update(set_spin(dico[\"name\"], dico[\"nat\"]))\n    # K point parameters\n    var[\"kpt\"] = set_kpoints(dico[\"nat\"])\n    var[\"dft\"][\"hgrids\"] = (hgrid, hgrid, hgrid)\n    # Control the diagonalisation scheme\n    if dico[\"name\"] in (\"Cr\", ):\n        var[\"mix\"][\"iscf\"] = 3\n        var[\"mix\"][\"alphamix\"] = 0.9\n    if dico[\"name\"] in (\"Ba\", \"Ca\"):\n        var[\"mix\"][\"norbsempty\"] = 8\n    var[\"ig_occupation\"] = {dico[\"name\"]: {\"empty_shells\": (\"s\", \"p\", \"d\")}}\n    # Atoms\n    pspfile = \"psppar.\"+dico[\"name\"]\n    if not os.path.isfile(pspfile):\n        safe_print(\"WARNING: Using default PSP for atom\", dico[\"name\"])\n    else:\n        var[pspfile] = open(pspfile, 'r').read()\n    # var[\"posinp\"] = {\"positions\": [{dico[\"name\"]:\n    #                  map(float, dico[i + 1].split())}\n    #                  for i in range(dico[\"nat\"])],\n    #                  \"units\": \"reduced\",\n    #                  \"cell\": (dico[\"a\"], dico[\"b\"], dico[\"c\"])}\n    var[\"posinp\"] = {\"positions\": [{dico[\"name\"]: dico[i + 1]} for i in range(\n        dico[\"nat\"])], \"units\": \"reduced\",\n        \"cell\": (dico[\"a\"], dico[\"b\"], dico[\"c\"])}\n    # We round robin the igspins.\n    if \"mpol\" in var[\"dft\"]:\n        mpol = 0\n        while mpol < var[\"dft\"][\"mpol\"]:\n            for at in var[\"posinp\"][\"positions\"]:\n                if mpol < var[\"dft\"][\"mpol\"]:\n                    if \"IGSpin\" in at:\n                        at[\"IGSpin\"] += 1\n                    else:\n                        at[\"IGSpin\"] = 1\n                    mpol += 1\n    elif \"nspin\" in var[\"dft\"] and var[\"dft\"][\"nspin\"] == 2:\n        for (i, at) in enumerate(var[\"posinp\"][\"positions\"]):\n            at[\"IGSpin\"] = 1 - 2 * (i % 2)\n    return var", "fn_id": 1, "class_fn": false, "repo": "mikiec84/aiida-bigdft-plugin", "file": "aiida_bigdft/PyBigDFT/BigDFT/scripts/DeltaTest.py", "last_update_at": "2020-08-05T19:01:51+00:00", "question_id": "4278fefc3d350168b06ea3d81a1c98f054b4d235_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def set_inputfile(hgrid, dico):\n    basicinput = '\\nlogfile: Yes\\ndft:\\n    ixc: PBE\\n    ncong: 2\\n    rmult: [10, 8]\\n    itermax: 3\\n    idsx: 0\\n    gnrm_cv: 1e-8\\n#Control the diagonalisation scheme\\nmix:\\n    iscf: 7\\n    itrpmax: 200\\n    rpnrm_cv: 1e-12\\n    tel: 1e-3\\n    alphamix: 0.5\\n    norbsempty: 1000\\n    alphadiis: 1.0\\n\\n#perf:\\n#    accel: OCLGPU\\n#    ocl_devices: Tesla K40c\\n#    blas: Yes\\n\\n'\n    import yaml\n    import os\n    var = yaml.load(basicinput)\n    var['dft'].update(set_spin(dico['name'], dico['nat']))\n    var['kpt'] = set_kpoints(dico['nat'])\n    var['dft']['hgrids'] = (hgrid, hgrid, hgrid)\n    if dico['name'] in ('Cr',):\n        var['mix']['iscf'] = 3\n        var['mix']['alphamix'] = 0.9\n    if dico['name'] in ('Ba', 'Ca'):\n        var['mix']['norbsempty'] = 8\n    var['ig_occupation'] = {dico['name']: {'empty_shells': ('s', 'p', 'd')}}\n    pspfile = 'psppar.' + dico['name']\n    if not os.path.isfile(pspfile):\n        safe_print('WARNING: Using default PSP for atom', dico['name'])\n    else:\n        var[pspfile] = open(pspfile, 'r').read()\n    var['posinp'] = {'positions': [{dico['name']: dico[i + 1]} for i in range(dico['nat'])], 'units': 'reduced', 'cell': (dico['a'], dico['b'], dico['c'])}\n    if 'mpol' in var['dft']:\n        mpol = 0\n        while mpol < var['dft']['mpol']:\n            for at in var['posinp']['positions']:\n                if mpol < var['dft']['mpol']:\n                    if 'IGSpin' in at:\n                        at['IGSpin'] += 1\n                    else:\n                        at['IGSpin'] = 1\n                    mpol += 1\n    elif 'nspin' in var['dft'] and var['dft']['nspin'] == 2:\n        for i, at in enumerate(var['posinp']['positions']):\n            at['IGSpin'] = 1 - 2 * (i % 2)\n"]]}
{"hexsha": "b0555e848b5dc855d58ac8f786b92bf8e9a5e7e8", "ext": "py", "lang": "Python", "content": "def split_individual_events(\n        signal,\n        sampling_rate,\n        expected_call_max_duration=1.0,\n        max_tries=10,\n        scale_factor=1.25,\n        amp_env_mode=\"broadband\",\n    ):\n    \"\"\"Divide a signal interval into individual putative events\n\n    This function assumes that the input is a signal that already contains a lot\n    of sound (detected by thresholding) and wants to split it up into individual\n    events by creating a second, more conservative threshold.\n\n    It is recommended to include some padding in the signal so that the detector\n    can better find a baseline (e.g. include the period between 1.0s and 4.0s\n    for a vocal period detected at 2.0s to 3.0s)\n\n    TODO: do this across two channels?\n    \"\"\"\n    if signal.ndim == 1:\n        signal = signal - np.mean(signal)\n    else:\n        signal = (signal - np.mean(signal, axis=0))[:, 0]\n\n    amp_env = get_amplitude_envelope(\n        signal,\n        sampling_rate,\n        highpass=1000,\n        lowpass=8000,\n        mode=amp_env_mode,\n    )\n\n    threshold = compute_smart_threshold(amp_env, sampling_rate)\n    # Compute intervals within this period, preferentially separating sounds\n    # that are separated by more than 20ms of silence.\n    # Then, gradually raise the threshold until the longest period detected is\n    # no greater than the defined max_length (default 1s)\n\n    idx = 0\n    while idx < max_tries:\n        intervals = threshold_events(\n            amp_env,\n            threshold,\n            polarity=1,\n            sampling_rate=sampling_rate,\n            ignore_width=0.02,\n            min_size=0.01,\n            fuse_duration=0.02,\n        )\n        durations = [np.diff(x) / sampling_rate for x in intervals]\n        if len(durations) and np.max(durations) > expected_call_max_duration:\n            threshold *= scale_factor\n        else:\n            break\n\n        idx += 1\n\n    if not len(intervals):\n        return [[0, len(signal)]]\n    else:\n        return intervals", "fn_id": 3, "class_fn": false, "repo": "kevinyu/soundsep", "file": "src/main/python/detection/thresholding.py", "last_update_at": "2020-10-03T18:35:52+00:00", "question_id": "b0555e848b5dc855d58ac8f786b92bf8e9a5e7e8_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def split_individual_events(signal, sampling_rate, expected_call_max_duration=1.0, max_tries=10, scale_factor=1.25, amp_env_mode='broadband'):\n    \"\"\"Divide a signal interval into individual putative events\n\n    This function assumes that the input is a signal that already contains a lot\n    of sound (detected by thresholding) and wants to split it up into individual\n    events by creating a second, more conservative threshold.\n\n    It is recommended to include some padding in the signal so that the detector\n    can better find a baseline (e.g. include the period between 1.0s and 4.0s\n    for a vocal period detected at 2.0s to 3.0s)\n\n    TODO: do this across two channels?\n    \"\"\"\n    if signal.ndim == 1:\n        signal = signal - np.mean(signal)\n    else:\n        signal = (signal - np.mean(signal, axis=0))[:, 0]\n    amp_env = get_amplitude_envelope(signal, sampling_rate, highpass=1000, lowpass=8000, mode=amp_env_mode)\n    threshold = compute_smart_threshold(amp_env, sampling_rate)\n    idx = 0\n    while idx < max_tries:\n        intervals = threshold_events(amp_env, threshold, polarity=1, sampling_rate=sampling_rate, ignore_width=0.02, min_size=0.01, fuse_duration=0.02)\n        durations = [np.diff(x) / sampling_rate for x in intervals]\n        if len(durations) and np.max(durations) > expected_call_max_duration:\n            threshold *= scale_factor\n        else:\n            break\n        idx += 1\n    if not len(intervals):\n        return [[0, len(signal)]]\n    else:\n"]]}
{"hexsha": "e53074476cdd98e3b43185741b05edc305ab6e9b", "ext": "py", "lang": "Python", "content": "def test_build_examples():\n    mock_state = np.array([[0, 1], [2, 2]])\n    mock_next_state = np.array([[0, 1], [2, 3]])\n    mock_reward = 42\n    mock_action = 1  # -> right, second item in prediction array\n    mock_action_str = \"right\"\n    game_over = False\n    q_update = 1.234\n\n    class MockQNetworkGradientStep:\n        def predict(self, state):\n            assert (state == mock_state.reshape((1, 2, 2, 1))).all()\n            # return 2d array, because we are predicting a \"batch\" of 1 data item\n            return [[1.0,\n                     2.0,  # we picked this action\n                     -1.0,\n                     -1.5]]\n\n        def fit(self, state, q_values, verbose):\n            assert (state == mock_state).all()\n            assert q_values[0][mock_action] == q_update\n            assert q_values[0][0] == 1.0\n            assert q_values[0][2] == -1.0\n            assert q_values[0][3] == -1.5\n\n    mock_network = MockQNetworkGradientStep()\n    agent = dql_agent.DQLAgent(mock_config, mock_network)\n    agent.get_q_update = lambda *args: q_update\n\n    input, expected_output = agent.build_training_examples(\n        [(mock_state, mock_action_str, mock_reward, mock_next_state, game_over)])\n    assert input == [mock_state]\n    assert expected_output == [[1.0,\n                                q_update,  # we picked this action\n                                -1.0,\n                                -1.5]]", "fn_id": 6, "class_fn": false, "repo": "DiscoverAI/pungi", "file": "tests/agents/dql/test_dql_agent.py", "last_update_at": "2020-09-25T22:52:51+00:00", "question_id": "e53074476cdd98e3b43185741b05edc305ab6e9b_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_build_examples():\n    mock_state = np.array([[0, 1], [2, 2]])\n    mock_next_state = np.array([[0, 1], [2, 3]])\n    mock_reward = 42\n    mock_action = 1\n    mock_action_str = 'right'\n    game_over = False\n    q_update = 1.234\n\n    class MockQNetworkGradientStep:\n\n        def predict(self, state):\n            assert (state == mock_state.reshape((1, 2, 2, 1))).all()\n            return [[1.0, 2.0, -1.0, -1.5]]\n\n        def fit(self, state, q_values, verbose):\n            assert (state == mock_state).all()\n            assert q_values[0][mock_action] == q_update\n            assert q_values[0][0] == 1.0\n            assert q_values[0][2] == -1.0\n            assert q_values[0][3] == -1.5\n    mock_network = MockQNetworkGradientStep()\n    agent = dql_agent.DQLAgent(mock_config, mock_network)\n    agent.get_q_update = lambda *args: q_update\n    input, expected_output = agent.build_training_examples([(mock_state, mock_action_str, mock_reward, mock_next_state, game_over)])\n    assert input == [mock_state]\n"]]}
{"hexsha": "8ca7caaa0a55affaa1c1a80379fc6d3e4bbd965a", "ext": "py", "lang": "Python", "content": "async def test_if_fires_on_multiple_user_ids(hass, calls, context_with_user):\n    \"\"\"Test the firing of event when the trigger has multiple user ids.\"\"\"\n    assert await async_setup_component(\n        hass,\n        automation.DOMAIN,\n        {\n            automation.DOMAIN: {\n                \"trigger\": {\n                    \"platform\": \"event\",\n                    \"event_type\": \"test_event\",\n                    \"event_data\": {},\n                    \"context\": {\"user_id\": [context_with_user.user_id, \"another id\"]},\n                },\n                \"action\": {\"service\": \"test.automation\"},\n            }\n        },\n    )\n\n    hass.bus.async_fire(\"test_event\", {}, context=context_with_user)\n    await hass.async_block_till_done()\n    assert len(calls) == 1", "fn_id": 13, "class_fn": false, "repo": "MrDelik/core", "file": "tests/components/homeassistant/triggers/test_event.py", "last_update_at": "2020-03-02T12:56:31+00:00", "question_id": "8ca7caaa0a55affaa1c1a80379fc6d3e4bbd965a_13", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def test_if_fires_on_multiple_user_ids(hass, calls, context_with_user):\n    \"\"\"Test the firing of event when the trigger has multiple user ids.\"\"\"\n    assert await async_setup_component(hass, automation.DOMAIN, {automation.DOMAIN: {'trigger': {'platform': 'event', 'event_type': 'test_event', 'event_data': {}, 'context': {'user_id': [context_with_user.user_id, 'another id']}}, 'action': {'service': 'test.automation'}}})\n    hass.bus.async_fire('test_event', {}, context=context_with_user)\n    await hass.async_block_till_done()\n"]]}
{"hexsha": "00e841561e2b3c6e15cb5682b194723ed7aaacc5", "ext": "py", "lang": "Python", "content": "def segment_graph(num_vertices,num_edges,edges,c):\n    edges = sorted(edges,key=lambda edges:edges[2])\n    print(len(edges))\n    u = universe(num_vertices)\n    threshold = [0 for i in range(num_vertices)]\n    for i in range(num_vertices):\n        threshold[i] = THRESHOLD(1,c)\n    for i in range(num_edges):\n        pedge = edges[i]\n        # point a from list\n        a = u.find(pedge[0])\n        # point b from list\n        b = u.find(pedge[1])\n        # a!=b prevent to cause connected graph\n        if a != b:\n            if pedge[2] <= threshold[a] and pedge[2] <= threshold[b]:\n                u.join(a,b)\n                a = u.find(a)\n                threshold[a] = pedge[2] + THRESHOLD(u.get_size(a),c)\n\n    return u", "fn_id": 0, "class_fn": false, "repo": "ZhongHouyu/CVCode", "file": "Graph-Algorithm/GraphBasedImageSegmentation(C|Python|with or not OpenCV)/GraphBasedImageSegmentation(Python without OpenCV)/segment_graph.py", "last_update_at": "2020-10-21T05:17:07+00:00", "question_id": "00e841561e2b3c6e15cb5682b194723ed7aaacc5_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def segment_graph(num_vertices, num_edges, edges, c):\n    edges = sorted(edges, key=lambda edges: edges[2])\n    print(len(edges))\n    u = universe(num_vertices)\n    threshold = [0 for i in range(num_vertices)]\n    for i in range(num_vertices):\n        threshold[i] = THRESHOLD(1, c)\n    for i in range(num_edges):\n        pedge = edges[i]\n        a = u.find(pedge[0])\n        b = u.find(pedge[1])\n        if a != b:\n            if pedge[2] <= threshold[a] and pedge[2] <= threshold[b]:\n                u.join(a, b)\n                a = u.find(a)\n                threshold[a] = pedge[2] + THRESHOLD(u.get_size(a), c)\n"]]}
{"hexsha": "7c12bb92dca6e07d7eeebda40503fbdb25d98519", "ext": "py", "lang": "Python", "content": "def ATL06_2_gdf(ATL06_fn,dataset_dict):\n    \"\"\"\n    function to convert ATL06 hdf5 to geopandas dataframe, containing columns as passed in dataset dict\n    Used Ben's ATL06_to_dict function\n    \"\"\"\n    if ('latitude' in dataset_dict['land_ice_segments']) != True:\n        dataset_dict['land_ice_segments'].append('latitude')\n    if ('longitude' in dataset_dict['land_ice_segments']) != True:\n        dataset_dict['land_ice_segments'].append('longitude')\n    #use Ben's Scripts to convert to dict\n    data_dict = ATL06_to_dict(ATL06_fn,dataset_dict)\n    #this will give us 6 tracks\n    i = 0\n    for track in data_dict:\n        #1 track\n        #convert to datafrmae\n        df = pd.DataFrame(track)\n        df['p_b'] = str(track['pair'][0])+'_'+str(track['beam'][0])\n        df['geometry'] = df.apply(point_covert,axis=1)\n        if i==0:\n            df_final = df.copy()\n        else:\n            df_final = df_final.append(df)\n        i = i+1\n    gdf_final = gpd.GeoDataFrame(df_final,geometry='geometry',crs={'init':'epsg:4326'})\n    return gdf_final", "fn_id": 4, "class_fn": false, "repo": "ShashankBice/topohack", "file": "topolib/gda_lib.py", "last_update_at": "2020-07-01T18:50:45+00:00", "question_id": "7c12bb92dca6e07d7eeebda40503fbdb25d98519_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ATL06_2_gdf(ATL06_fn, dataset_dict):\n    \"\"\"\n    function to convert ATL06 hdf5 to geopandas dataframe, containing columns as passed in dataset dict\n    Used Ben's ATL06_to_dict function\n    \"\"\"\n    if ('latitude' in dataset_dict['land_ice_segments']) != True:\n        dataset_dict['land_ice_segments'].append('latitude')\n    if ('longitude' in dataset_dict['land_ice_segments']) != True:\n        dataset_dict['land_ice_segments'].append('longitude')\n    data_dict = ATL06_to_dict(ATL06_fn, dataset_dict)\n    i = 0\n    for track in data_dict:\n        df = pd.DataFrame(track)\n        df['p_b'] = str(track['pair'][0]) + '_' + str(track['beam'][0])\n        df['geometry'] = df.apply(point_covert, axis=1)\n        if i == 0:\n            df_final = df.copy()\n        else:\n            df_final = df_final.append(df)\n        i = i + 1\n    gdf_final = gpd.GeoDataFrame(df_final, geometry='geometry', crs={'init': 'epsg:4326'})\n"]]}
{"hexsha": "e9257c65339acddd1b1ca0c6a52f786edb4edf4a", "ext": "py", "lang": "Python", "content": "def get_playlist_tracks(playlist_id):\n    \"\"\"songname, id\"\"\"\n    playlist_tracks = sp.get_playlist_tracks(playlist_id)\n\n    tracks = []\n    offset = 0\n    for item in playlist_tracks['items']:\n        id = item['track']['id']\n        name = item['track']['name']\n\n        tracks.append(Track(name, id))\n\n        # if there are multiple pages\n        if playlist_tracks['next'] is not None:\n            playlist_tracks = sp.get_playlist_tracks(playlist_id, offset=offset)\n    return tracks", "fn_id": 3, "class_fn": false, "repo": "germaindudek/spotify-normalize-bpms", "file": "download_bpm.py", "last_update_at": "2020-08-02T08:54:53+00:00", "question_id": "e9257c65339acddd1b1ca0c6a52f786edb4edf4a_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_playlist_tracks(playlist_id):\n    \"\"\"songname, id\"\"\"\n    playlist_tracks = sp.get_playlist_tracks(playlist_id)\n    tracks = []\n    offset = 0\n    for item in playlist_tracks['items']:\n        id = item['track']['id']\n        name = item['track']['name']\n        tracks.append(Track(name, id))\n        if playlist_tracks['next'] is not None:\n            playlist_tracks = sp.get_playlist_tracks(playlist_id, offset=offset)\n"]]}
{"hexsha": "1bcf9d1d14f51f6cb18ecf75f8c60cb4cb91e48e", "ext": "py", "lang": "Python", "content": "def get_dict_descendants(p, level=0):\n    \"\"\"Returns a list of dictionaries, one for each family where p is\n    father or mother.\"\"\"\n\n    if level < 0:\n        return 1, level, None\n    if p is None:\n        return 0, 100, {}  # do not decrease level\n\n    data = [p_dict_descendants(p, '1')]\n    fams = p.get_children()\n    height = 0\n    new_level = level\n\n    def add_family(data, family):\n        height = 0\n        partner, children, _family = family\n        data[-1].update(p_dict_descendants(partner, '2'))\n        new_level = level\n\n        if level > 0:\n            data[-1]['parents'] = []\n            for ch in children:\n                h, nl, d = get_dict_descendants(ch, level-1)\n                height += h\n                new_level = min(new_level, nl)\n                data[-1]['parents'].extend(d)\n            else:\n                height += 1\n        return height, new_level\n\n    if fams:\n        ht, new_level = add_family(data, fams[0])\n        height += ht\n    for family in fams[1:]:\n        # Now handle other families\n        # Here, replace name of p by '...'\n        data.append(p_dict_descendants('...', '1'))\n        ht, nl = add_family(data, family)\n        height += ht\n        new_level = min(new_level, nl)\n\n    return height+2, new_level, data", "fn_id": 2, "class_fn": false, "repo": "ugoertz/django-familio", "file": "genealogio/views.py", "last_update_at": "2020-10-31T15:08:18+00:00", "question_id": "1bcf9d1d14f51f6cb18ecf75f8c60cb4cb91e48e_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_dict_descendants(p, level=0):\n    \"\"\"Returns a list of dictionaries, one for each family where p is\n    father or mother.\"\"\"\n    if level < 0:\n        return (1, level, None)\n    if p is None:\n        return (0, 100, {})\n    data = [p_dict_descendants(p, '1')]\n    fams = p.get_children()\n    height = 0\n    new_level = level\n\n    def add_family(data, family):\n        height = 0\n        partner, children, _family = family\n        data[-1].update(p_dict_descendants(partner, '2'))\n        new_level = level\n        if level > 0:\n            data[-1]['parents'] = []\n            for ch in children:\n                h, nl, d = get_dict_descendants(ch, level - 1)\n                height += h\n                new_level = min(new_level, nl)\n                data[-1]['parents'].extend(d)\n            else:\n                height += 1\n        return (height, new_level)\n    if fams:\n        ht, new_level = add_family(data, fams[0])\n        height += ht\n    for family in fams[1:]:\n        data.append(p_dict_descendants('...', '1'))\n        ht, nl = add_family(data, family)\n        height += ht\n        new_level = min(new_level, nl)\n"]]}
{"hexsha": "b08f9649ffcfb47313fcc0852c8df04ac13b239c", "ext": "py", "lang": "Python", "content": "def scrapePapago(language):\n    # Define the URL to be opened\n    url = 'https://papago.naver.com/'\n    # Define the driver for Selenium to use\n    driver = webdriver.Chrome('chromedriver.exe')\n    # Open the specfied URL\n    driver.get(url)\n    # Sleep to avoid errors\n    time.sleep(2)\n    # Find the Text Field and type the sentences\n    textFieldSource = driver.find_element_by_xpath('//*[@id=\"txtSource\"]')\n    test = source.loc[0:0].to_string(index = False, columns = None, header = False)\n    batch = 1\n    sentencesCleanBatches = {}\n    sentencesClean = []\n    for row in source['origin']:\n        textFieldSource.send_keys(row)\n        textFieldSource.send_keys(Keys.RETURN)\n        try:\n            char_usage = driver.find_element_by_xpath('//*[@id=\"root\"]/div/div[1]/section/div/div[1]/div[1]/div/p[1]').text\n            char_usage1 = char_usage[0:4]\n            char_usage2 = char_usage1.strip()\n            char_count = int(char_usage2)\n        except Exception:\n            char_count = 0\n            pass\n        if char_count >= 4500:\n            print(char_count)\n            # Pause to avoid errors\n            time.sleep(5)\n            # Push the Translate button\n            translate_button = driver.find_element_by_xpath('//*[@id=\"btnTranslate\"]').click()\n            # Pause to avoid errors\n            time.sleep(5)\n            # Store the translated text as a string\n            translatedText = driver.find_element_by_xpath('//*[@id=\"txtTarget\"]')\n            translatedText_content = translatedText.text\n            # Clean the output text and print\n            sentencesCleanBatches[batch] = translatedText_content.split(\"\\n\")\n            batch += 1\n            # Clear the input field\n            textFieldSource.clear()\n    # Pause to avoid errors\n    time.sleep(5)\n    # Push the Translate button\n    translate_button = driver.find_element_by_xpath('//*[@id=\"btnTranslate\"]').click()\n    # Pause to avoid errors\n    time.sleep(5)\n    # Store the translated text as a string\n    translatedText = driver.find_element_by_xpath('//*[@id=\"txtTarget\"]')\n    translatedText_content = translatedText.text\n    # Clean the output text and print\n    sentencesCleanBatches[batch] = translatedText_content.split(\"\\n\")\n    # Concatenate all the batches into once list\n    for i in range(1, batch + 1):\n        sentencesClean = sentencesClean + sentencesCleanBatches[i]\n    # Close the browser window\n    driver.quit()\n    # Return necessary objects\n    return sentencesClean", "fn_id": 4, "class_fn": false, "repo": "mikitz/machine-translation-scraping", "file": "scraper.py", "last_update_at": "2020-11-25T16:44:53+00:00", "question_id": "b08f9649ffcfb47313fcc0852c8df04ac13b239c_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def scrapePapago(language):\n    url = 'https://papago.naver.com/'\n    driver = webdriver.Chrome('chromedriver.exe')\n    driver.get(url)\n    time.sleep(2)\n    textFieldSource = driver.find_element_by_xpath('//*[@id=\"txtSource\"]')\n    test = source.loc[0:0].to_string(index=False, columns=None, header=False)\n    batch = 1\n    sentencesCleanBatches = {}\n    sentencesClean = []\n    for row in source['origin']:\n        textFieldSource.send_keys(row)\n        textFieldSource.send_keys(Keys.RETURN)\n        try:\n            char_usage = driver.find_element_by_xpath('//*[@id=\"root\"]/div/div[1]/section/div/div[1]/div[1]/div/p[1]').text\n            char_usage1 = char_usage[0:4]\n            char_usage2 = char_usage1.strip()\n            char_count = int(char_usage2)\n        except Exception:\n            char_count = 0\n            pass\n        if char_count >= 4500:\n            print(char_count)\n            time.sleep(5)\n            translate_button = driver.find_element_by_xpath('//*[@id=\"btnTranslate\"]').click()\n            time.sleep(5)\n            translatedText = driver.find_element_by_xpath('//*[@id=\"txtTarget\"]')\n            translatedText_content = translatedText.text\n            sentencesCleanBatches[batch] = translatedText_content.split('\\n')\n            batch += 1\n            textFieldSource.clear()\n    time.sleep(5)\n    translate_button = driver.find_element_by_xpath('//*[@id=\"btnTranslate\"]').click()\n    time.sleep(5)\n    translatedText = driver.find_element_by_xpath('//*[@id=\"txtTarget\"]')\n    translatedText_content = translatedText.text\n    sentencesCleanBatches[batch] = translatedText_content.split('\\n')\n    for i in range(1, batch + 1):\n        sentencesClean = sentencesClean + sentencesCleanBatches[i]\n    driver.quit()\n"]]}
{"hexsha": "0d35655989af14d192d0ac1f0d96cb5914890556", "ext": "py", "lang": "Python", "content": "def truncate_treat_sample(y, t, time, policy, rng, final=False, stack=False):\n    y_rx = np.array(y)\n    t_rx = np.array(t)\n    treated = np.zeros(len(t), dtype=bool)\n    rx = np.zeros(len(t))\n\n    for i, t0 in enumerate(t):\n        if t0 > time: # only treat before `time`\n            break\n\n        rx[i] = policy.sample_treatment(y_rx[:(i+1)], t_rx[:(i+1)], rng)\n\n        if rx[i] == 1:\n            y_rx, t_rx, treated = policy.treat(y_rx, t_rx, treated, t0, stack)\n\n    # Add a final treatment at `time`.\n    if final:\n        rx[t_rx == time] = 1.0\n        y_rx, t_rx, treated = policy.treat(y_rx, t_rx, treated, time, stack)\n\n    return y_rx, (t_rx, rx)", "fn_id": 4, "class_fn": false, "repo": "llja0112/counterfactual-gp", "file": "counterfactualgp/simulation.py", "last_update_at": "2020-09-04T14:12:26+00:00", "question_id": "0d35655989af14d192d0ac1f0d96cb5914890556_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def truncate_treat_sample(y, t, time, policy, rng, final=False, stack=False):\n    y_rx = np.array(y)\n    t_rx = np.array(t)\n    treated = np.zeros(len(t), dtype=bool)\n    rx = np.zeros(len(t))\n    for i, t0 in enumerate(t):\n        if t0 > time:\n            break\n        rx[i] = policy.sample_treatment(y_rx[:i + 1], t_rx[:i + 1], rng)\n        if rx[i] == 1:\n            y_rx, t_rx, treated = policy.treat(y_rx, t_rx, treated, t0, stack)\n    if final:\n        rx[t_rx == time] = 1.0\n        y_rx, t_rx, treated = policy.treat(y_rx, t_rx, treated, time, stack)\n"]]}
{"hexsha": "bc39e97d66c908e8a4ea6cf66ecdff3dc41c0bcf", "ext": "py", "lang": "Python", "content": "@login_required\ndef eventoCreate(request):\n    form = EventoForm()\n    if request.method == 'POST':\n        form = EventoForm(request.POST)\n        if form.is_valid():\n            form.save()\n            return redirect('dashboard')\n\n    context = {'form': form}\n    return render(request, 'model_form.html', context)", "fn_id": 1, "class_fn": false, "repo": "Rodp63/bugevents", "file": "bugevents/configuracion/views.py", "last_update_at": "2020-11-28T03:01:07+00:00", "question_id": "bc39e97d66c908e8a4ea6cf66ecdff3dc41c0bcf_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@login_required\ndef eventoCreate(request):\n    form = EventoForm()\n    if request.method == 'POST':\n        form = EventoForm(request.POST)\n        if form.is_valid():\n            form.save()\n            return redirect('dashboard')\n    context = {'form': form}\n"]]}
{"hexsha": "6f167e5d880e5593a503c1848335d997ce05c074", "ext": "py", "lang": "Python", "content": "def start():\n    register_tasks()\n\n    connection = pika.BlockingConnection(get_config())\n    channel = connection.channel()\n\n    rpc_server = RPCServer(channel, rpc_methods)\n    rpc_server.rpc_register()\n\n    job_server = JobServer(channel, job_methods)\n    job_server.job_register()\n\n    channel.start_consuming()", "fn_id": 1, "class_fn": false, "repo": "kentuck13/bgtasks", "file": "bgtasks/amqp.py", "last_update_at": "2020-04-22T11:53:02+00:00", "question_id": "6f167e5d880e5593a503c1848335d997ce05c074_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def start():\n    register_tasks()\n    connection = pika.BlockingConnection(get_config())\n    channel = connection.channel()\n    rpc_server = RPCServer(channel, rpc_methods)\n    rpc_server.rpc_register()\n    job_server = JobServer(channel, job_methods)\n    job_server.job_register()\n"]]}
{"hexsha": "d9aae88f68e5810fa9375636cd67128ca2d2b315", "ext": "py", "lang": "Python", "content": "def parseStrictToken(token):\n    \"\"\"given a string in the the format '<sign><number><unit>' return a number of seconds\"\"\"\n    # get rid of the equals sign if it exists\n    # print token\n    if token[0] == '=':\n        token = token[1:]\n    # get the unit\n    unit = token[-1]\n    # chop the unit off the token\n    token = token[:-1]\n\n    # use python's eval to turn the possibly signed number string into an integer\n    number = float(token)\n\n    # get the unit multiplier and if necessary raise an invalid unit error\n    if not unit in units:\n        raise MalformedTimeString('Invalid Unit')\n\n    unitMult = units[unit]\n\n    return int(unitMult * number)", "fn_id": 1, "class_fn": false, "repo": "fdChasm/spyd", "file": "src/spyd/utils/timestring.py", "last_update_at": "2020-10-27T09:45:23+00:00", "question_id": "d9aae88f68e5810fa9375636cd67128ca2d2b315_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def parseStrictToken(token):\n    \"\"\"given a string in the the format '<sign><number><unit>' return a number of seconds\"\"\"\n    if token[0] == '=':\n        token = token[1:]\n    unit = token[-1]\n    token = token[:-1]\n    number = float(token)\n    if not unit in units:\n        raise MalformedTimeString('Invalid Unit')\n    unitMult = units[unit]\n"]]}
{"hexsha": "c7e217a46f347647a01730ba6c2f44adea4ea3c1", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize(\n    \"config_name, expect\",\n    [\n        (\"JAVA_GATEWAY_ADDRESS\", \"127.0.0.1\"),\n        (\"JAVA_GATEWAY_PORT\", 25333),\n        (\"JAVA_GATEWAY_AUTO_CONVERT\", True),\n        (\"USER_NAME\", \"userPythonGateway\"),\n        (\"USER_PASSWORD\", \"userPythonGateway\"),\n        (\"USER_EMAIL\", \"userPythonGateway@dolphinscheduler.com\"),\n        (\"USER_PHONE\", \"11111111111\"),\n        (\"USER_STATE\", 1),\n        (\"WORKFLOW_PROJECT\", \"project-pydolphin\"),\n        (\"WORKFLOW_TENANT\", \"tenant_pydolphin\"),\n        (\"WORKFLOW_USER\", \"userPythonGateway\"),\n        (\"WORKFLOW_QUEUE\", \"queuePythonGateway\"),\n        (\"WORKFLOW_WORKER_GROUP\", \"default\"),\n        (\"WORKFLOW_TIME_ZONE\", \"Asia/Shanghai\"),\n        (\"WORKFLOW_WARNING_TYPE\", \"NONE\"),\n    ],\n)\ndef test_get_configuration(config_name: str, expect: Any):\n    \"\"\"Test get exists attribute in :mod:`configuration`.\"\"\"\n    assert expect == getattr(configuration, config_name)", "fn_id": 10, "class_fn": false, "repo": "SbloodyS/dolphinscheduler", "file": "dolphinscheduler-python/pydolphinscheduler/tests/core/test_configuration.py", "last_update_at": "2020-01-05T15:43:39+00:00", "question_id": "c7e217a46f347647a01730ba6c2f44adea4ea3c1_10", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('config_name, expect', [('JAVA_GATEWAY_ADDRESS', '127.0.0.1'), ('JAVA_GATEWAY_PORT', 25333), ('JAVA_GATEWAY_AUTO_CONVERT', True), ('USER_NAME', 'userPythonGateway'), ('USER_PASSWORD', 'userPythonGateway'), ('USER_EMAIL', 'userPythonGateway@dolphinscheduler.com'), ('USER_PHONE', '11111111111'), ('USER_STATE', 1), ('WORKFLOW_PROJECT', 'project-pydolphin'), ('WORKFLOW_TENANT', 'tenant_pydolphin'), ('WORKFLOW_USER', 'userPythonGateway'), ('WORKFLOW_QUEUE', 'queuePythonGateway'), ('WORKFLOW_WORKER_GROUP', 'default'), ('WORKFLOW_TIME_ZONE', 'Asia/Shanghai'), ('WORKFLOW_WARNING_TYPE', 'NONE')])\ndef test_get_configuration(config_name: str, expect: Any):\n    \"\"\"Test get exists attribute in :mod:`configuration`.\"\"\"\n"]]}
{"hexsha": "a4381e7060c53f60e6c99da88e2a53670cdff3db", "ext": "py", "lang": "Python", "content": "def model_ensemble_classification(X,\n                                  y,\n                                  feature_trans,\n                                  estimator_list,\n                                  score_eval,\n                                  model_perf_tuning_df):\n\n    stacked_model_eval_dict = {}\n\n    with open('config.yaml') as f:\n        config_data = yaml.load(f, Loader=yaml.FullLoader)\n\n    tree_models_short_name_dict = config_data['tree_models_short_name']\n    df = model_perf_tuning_df.copy()\n    df.set_index('Model', inplace=True)\n\n    ll = estimator_list\n    model_combinations_list = []\n    for ii in range(2, len(ll)+1):\n        if ii == 2:\n            res = [list(ele) for ele in list(itertools.permutations(ll, ii))]\n            model_combinations_list.extend(res)\n        else:\n            res = [list(ele) for ele in list(itertools.permutations(ll, ii))]\n            final_list = []\n            for i in res:\n                pop_item = i.pop()\n                sorted_list = sorted(i)\n                final_list.append(sorted_list+[pop_item])\n\n            final_list.sort()\n            complete_list = list(final_list for final_list,_ in itertools.groupby(final_list))\n            model_combinations_list.extend(complete_list)\n\n    # model_combinations_tuple = permutations(estimator_list)\n    for models_list in model_combinations_list:\n        level0 = list()\n        model_params_dict = {}\n        for i in models_list[:-1]:\n            level0.append((tree_models_short_name_dict[i][0],  globals()[i]()))\n\n            for key, val in eval(df.loc[i, 'Best_Params']).items():\n                key = key.replace('__', '__'+tree_models_short_name_dict[i][0]+'__')\n                model_params_dict[key] = val\n\n        # define meta learner model\n        level1 = globals()[models_list[-1]]()\n        final_model_short_name = tree_models_short_name_dict[models_list[-1]][0]\n\n        for key, val in eval(df.loc[models_list[-1], 'Best_Params']).items():\n            # key = key.replace('__', '__'+tree_models_short_name_dict[models_list[-1]][0]+'__')\n            key = key.replace('__', '__final_estimator__')\n            model_params_dict[key] = val\n\n        # define the stacking ensemble\n        model_stack = StackingClassifier(estimators=level0, final_estimator=level1)\n\n        model_pipeline = Pipeline([('feat_trans', feature_trans),\n                                   ('estimator', model_stack)])\n\n        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n        model_pipeline.set_params(**model_params_dict)\n        oof_preds = cross_val_predict(estimator=model_pipeline,\n                                      X=X,\n                                      y=y,\n                                      cv=cv,\n                                      n_jobs=-1,\n                                      method='predict',\n                                      # fit_params=clf.best_params_,\n                                      verbose=1)\n\n        # oof_roc_auc_score = roc_auc_score(y, oof_preds)\n        oof_eval_score = score_eval(y, oof_preds)\n\n        stacking_str = ''\n        for j in level0:\n            stacking_str = stacking_str+j[0]+'+'\n        stacking_str = stacking_str[:-1]\n        stacking_str = stacking_str+'-->'+final_model_short_name\n\n        stacked_model_eval_dict[stacking_str] = {'OOF_'+score_eval.__name__: round(oof_eval_score, 5),\n                                          'Best_Params': str(model_params_dict)}\n\n    stacked_model_eval_df = pd.DataFrame.from_dict(stacked_model_eval_dict,\n                                                  orient='index').reset_index().rename(columns={'index': 'Model'})\n\n\n    # stacked_model_eval_df = stacked_model_eval_df.sort_values(by='OOF_ROC_AUC_Score', ascending=False)\n    #=========================================================================\n    # Below is the code for pickle the best stacked model\n    stacked_model_eval_df = stacked_model_eval_df.sort_values(by='OOF_'+score_eval.__name__, ascending=False)\n    best_stacked_model_best_params = eval(stacked_model_eval_df.iloc[0].to_dict()['Best_Params'])\n    # logger.info(f'Best Stacked')\n    models_split = stacked_model_eval_df.iloc[0].to_dict()['Model'].split('-->')\n    logger.info(f\"Best Stacked Tree Model - {stacked_model_eval_df.iloc[0].to_dict()['Model']}\")\n    logger.info(f\"\\nBest Stacked Tree Model - {stacked_model_eval_df.iloc[0].to_dict()['Model']} Params:\\n{pformat(best_stacked_model_best_params)}\")\n    final_esti = [key for key, value in tree_models_short_name_dict.items() if value[0] == models_split[-1]]\n    level1 = globals()[final_esti[0]]()\n\n    level0 = []\n    for i in models_split[0].split('+'):\n        level0_esti = [key for key, value in tree_models_short_name_dict.items() if value[0] == i]\n        level0.append((i,  globals()[level0_esti[0]]()))\n\n    model_stack = StackingClassifier(estimators=level0, final_estimator=level1)\n    model_pipeline = Pipeline([('feat_trans', feature_trans),\n                               ('estimator', model_stack)])\n    model_pipeline.set_params(**best_stacked_model_best_params)\n    with open('best_stacked_tree_model.plk', 'wb') as f:\n        pickle.dump(model_pipeline, f)\n    #=========================================================================\n\n    # stacked_model_eval_df = stacked_model_eval_df.append(model_perf_tuning_df[['Model', 'OOF_ROC_AUC_Score', 'Best_Params']])\n    all_tree_model_eval_df = stacked_model_eval_df.append(model_perf_tuning_df[['Model', 'OOF_'+score_eval.__name__, 'Best_Params']])\n\n\n    all_tree_model_eval_df['Model'] = all_tree_model_eval_df['Model'].map(lambda x: tree_models_short_name_dict[x][0] if x in tree_models_short_name_dict else x)\n    all_tree_model_eval_df = all_tree_model_eval_df.sort_values(by='OOF_'+score_eval.__name__, ascending=False)\n    all_tree_model_eval_df.reset_index(drop=True, inplace=True)\n    logger.info(f\"\\nAll Tree Models Evaluation Dataframe\\n{all_tree_model_eval_df[['Model', 'OOF_'+score_eval.__name__]].to_string()}\")\n    best_tree_model = all_tree_model_eval_df.iloc[0].to_dict()['Model']\n    best_tree_model_params = eval(all_tree_model_eval_df.iloc[0].to_dict()['Best_Params'])\n    logger.info(f\"Best Tree Model (Single+Stacked) : {best_tree_model}\")\n    logger.info(f\"Best Tree Model (Single+Stacked) - {best_tree_model} Params\\n{pformat(best_tree_model_params)}\")\n    # breakpoint()\n    return all_tree_model_eval_df, best_tree_model", "fn_id": 8, "class_fn": false, "repo": "rvalusa2108/Scikit-Learn_Pipeline-House_Price_Prediction-Kaggle", "file": "Custom_Transformers_Methods.py", "last_update_at": "2020-11-02T19:09:50+00:00", "question_id": "a4381e7060c53f60e6c99da88e2a53670cdff3db_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def model_ensemble_classification(X, y, feature_trans, estimator_list, score_eval, model_perf_tuning_df):\n    stacked_model_eval_dict = {}\n    with open('config.yaml') as f:\n        config_data = yaml.load(f, Loader=yaml.FullLoader)\n    tree_models_short_name_dict = config_data['tree_models_short_name']\n    df = model_perf_tuning_df.copy()\n    df.set_index('Model', inplace=True)\n    ll = estimator_list\n    model_combinations_list = []\n    for ii in range(2, len(ll) + 1):\n        if ii == 2:\n            res = [list(ele) for ele in list(itertools.permutations(ll, ii))]\n            model_combinations_list.extend(res)\n        else:\n            res = [list(ele) for ele in list(itertools.permutations(ll, ii))]\n            final_list = []\n            for i in res:\n                pop_item = i.pop()\n                sorted_list = sorted(i)\n                final_list.append(sorted_list + [pop_item])\n            final_list.sort()\n            complete_list = list((final_list for final_list, _ in itertools.groupby(final_list)))\n            model_combinations_list.extend(complete_list)\n    for models_list in model_combinations_list:\n        level0 = list()\n        model_params_dict = {}\n        for i in models_list[:-1]:\n            level0.append((tree_models_short_name_dict[i][0], globals()[i]()))\n            for key, val in eval(df.loc[i, 'Best_Params']).items():\n                key = key.replace('__', '__' + tree_models_short_name_dict[i][0] + '__')\n                model_params_dict[key] = val\n        level1 = globals()[models_list[-1]]()\n        final_model_short_name = tree_models_short_name_dict[models_list[-1]][0]\n        for key, val in eval(df.loc[models_list[-1], 'Best_Params']).items():\n            key = key.replace('__', '__final_estimator__')\n            model_params_dict[key] = val\n        model_stack = StackingClassifier(estimators=level0, final_estimator=level1)\n        model_pipeline = Pipeline([('feat_trans', feature_trans), ('estimator', model_stack)])\n        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n        model_pipeline.set_params(**model_params_dict)\n        oof_preds = cross_val_predict(estimator=model_pipeline, X=X, y=y, cv=cv, n_jobs=-1, method='predict', verbose=1)\n        oof_eval_score = score_eval(y, oof_preds)\n        stacking_str = ''\n        for j in level0:\n            stacking_str = stacking_str + j[0] + '+'\n        stacking_str = stacking_str[:-1]\n        stacking_str = stacking_str + '-->' + final_model_short_name\n        stacked_model_eval_dict[stacking_str] = {'OOF_' + score_eval.__name__: round(oof_eval_score, 5), 'Best_Params': str(model_params_dict)}\n    stacked_model_eval_df = pd.DataFrame.from_dict(stacked_model_eval_dict, orient='index').reset_index().rename(columns={'index': 'Model'})\n    stacked_model_eval_df = stacked_model_eval_df.sort_values(by='OOF_' + score_eval.__name__, ascending=False)\n    best_stacked_model_best_params = eval(stacked_model_eval_df.iloc[0].to_dict()['Best_Params'])\n    models_split = stacked_model_eval_df.iloc[0].to_dict()['Model'].split('-->')\n    logger.info(f\"Best Stacked Tree Model - {stacked_model_eval_df.iloc[0].to_dict()['Model']}\")\n    logger.info(f\"\\nBest Stacked Tree Model - {stacked_model_eval_df.iloc[0].to_dict()['Model']} Params:\\n{pformat(best_stacked_model_best_params)}\")\n    final_esti = [key for key, value in tree_models_short_name_dict.items() if value[0] == models_split[-1]]\n    level1 = globals()[final_esti[0]]()\n    level0 = []\n    for i in models_split[0].split('+'):\n        level0_esti = [key for key, value in tree_models_short_name_dict.items() if value[0] == i]\n        level0.append((i, globals()[level0_esti[0]]()))\n    model_stack = StackingClassifier(estimators=level0, final_estimator=level1)\n    model_pipeline = Pipeline([('feat_trans', feature_trans), ('estimator', model_stack)])\n    model_pipeline.set_params(**best_stacked_model_best_params)\n    with open('best_stacked_tree_model.plk', 'wb') as f:\n        pickle.dump(model_pipeline, f)\n    all_tree_model_eval_df = stacked_model_eval_df.append(model_perf_tuning_df[['Model', 'OOF_' + score_eval.__name__, 'Best_Params']])\n    all_tree_model_eval_df['Model'] = all_tree_model_eval_df['Model'].map(lambda x: tree_models_short_name_dict[x][0] if x in tree_models_short_name_dict else x)\n    all_tree_model_eval_df = all_tree_model_eval_df.sort_values(by='OOF_' + score_eval.__name__, ascending=False)\n    all_tree_model_eval_df.reset_index(drop=True, inplace=True)\n    logger.info(f\"\\nAll Tree Models Evaluation Dataframe\\n{all_tree_model_eval_df[['Model', 'OOF_' + score_eval.__name__]].to_string()}\")\n    best_tree_model = all_tree_model_eval_df.iloc[0].to_dict()['Model']\n    best_tree_model_params = eval(all_tree_model_eval_df.iloc[0].to_dict()['Best_Params'])\n    logger.info(f'Best Tree Model (Single+Stacked) : {best_tree_model}')\n    logger.info(f'Best Tree Model (Single+Stacked) - {best_tree_model} Params\\n{pformat(best_tree_model_params)}')\n"]]}
{"hexsha": "ad1bfd456a20de9b88243f631836c545f7b4696f", "ext": "py", "lang": "Python", "content": "def symm2str(r,t=np.zeros([3, 1], dtype=float)):\n    xyz = \"xyz\"\n    string_all = []\n    for i in range(3):\n        string_row = \"\"\n        for j in range(3):\n            rval = r[i,j]\n            if rval == -1:\n                string_row += \"-{}\".format(xyz[j])\n            if rval == 1:\n                if string_row != \"\":\n                    string_row += \"+\"\n                string_row += \"{}\".format(xyz[j])\n        tval = t[i,0]\n        if tval%1 == 0.5:\n            tval = \"+1/2\"\n        elif tval%1 == 0.25:\n            tval = \"+1/4\"\n        elif tval%1 == 0.75:\n            tval = \"+3/4\"\n        elif tval%1 == 1.0/3.0:\n            tval = \"+1/3\"\n        elif tval%1 == 2.0/3.0:\n            tval = \"+2/3\"\n        elif tval%1 == 1.0/6.0:\n            tval = \"+1/6\"\n        elif tval%1 == 5.0/6.0:\n            tval = \"+5/6\"\n        else:\n            tval = \"\"\n        string_row += \"{}\".format(tval)\n        if string_row == \"\":\n            string_row = \"0\"\n        string_all.append(string_row)\n    return \", \".join(string_all)", "fn_id": 18, "class_fn": false, "repo": "stefsmeets/problematic", "file": "problematic/spacegroup.py", "last_update_at": "2020-08-08T17:38:40+00:00", "question_id": "ad1bfd456a20de9b88243f631836c545f7b4696f_18", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def symm2str(r, t=np.zeros([3, 1], dtype=float)):\n    xyz = 'xyz'\n    string_all = []\n    for i in range(3):\n        string_row = ''\n        for j in range(3):\n            rval = r[i, j]\n            if rval == -1:\n                string_row += '-{}'.format(xyz[j])\n            if rval == 1:\n                if string_row != '':\n                    string_row += '+'\n                string_row += '{}'.format(xyz[j])\n        tval = t[i, 0]\n        if tval % 1 == 0.5:\n            tval = '+1/2'\n        elif tval % 1 == 0.25:\n            tval = '+1/4'\n        elif tval % 1 == 0.75:\n            tval = '+3/4'\n        elif tval % 1 == 1.0 / 3.0:\n            tval = '+1/3'\n        elif tval % 1 == 2.0 / 3.0:\n            tval = '+2/3'\n        elif tval % 1 == 1.0 / 6.0:\n            tval = '+1/6'\n        elif tval % 1 == 5.0 / 6.0:\n            tval = '+5/6'\n        else:\n            tval = ''\n        string_row += '{}'.format(tval)\n        if string_row == '':\n            string_row = '0'\n        string_all.append(string_row)\n"]]}
{"hexsha": "42a6acec6cd2a414251e61ed9b36a5f1d2cbed99", "ext": "py", "lang": "Python", "content": "def test_clarification_edit_diff():\n    question = {\n        \"OwnerUserId\": \"1\",\n        \"Comments\": [{\n            \"Id\": \"1\",\n            \"Text\": \"is this a question? some unrelated text.\",\n            \"UserId\": \"2\",\n            \"CreationDate\": \"2010-07-22T07:00:00.000\",\n        }],\n        \"Edits\": initial_edits + [{\n            \"Id\": \"5\",\n            \"PostHistoryTypeId\": \"5\",  # edit body\n            \"RevisionGUID\": \"guid3\",\n            \"CreationDate\": \"2010-07-22T08:00:00.000\",\n            \"UserId\": \"1\",\n            \"Text\": \"question body, answer to question\"\n        }]\n    }\n\n    cq = ClarificationQuestion(\"is this a question ?\",\n                               parse_time(\"2010-07-22T07:00:00.000\"), entity_id=\"1\")\n    ce = annotator._clarification_edit(question, cq)\n\n    assert ce.text == \"question body , answer to question\"\n    assert ce.diff.insert == \" , answer to question\"\n    assert ce.diff.start_offset == 13\n    assert ce.diff.end_offset == 34", "fn_id": 7, "class_fn": false, "repo": "jantrienes/ecir2019-qac", "file": "qac/test/test_annotation.py", "last_update_at": "2020-01-16T06:53:54+00:00", "question_id": "42a6acec6cd2a414251e61ed9b36a5f1d2cbed99_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_clarification_edit_diff():\n    question = {'OwnerUserId': '1', 'Comments': [{'Id': '1', 'Text': 'is this a question? some unrelated text.', 'UserId': '2', 'CreationDate': '2010-07-22T07:00:00.000'}], 'Edits': initial_edits + [{'Id': '5', 'PostHistoryTypeId': '5', 'RevisionGUID': 'guid3', 'CreationDate': '2010-07-22T08:00:00.000', 'UserId': '1', 'Text': 'question body, answer to question'}]}\n    cq = ClarificationQuestion('is this a question ?', parse_time('2010-07-22T07:00:00.000'), entity_id='1')\n    ce = annotator._clarification_edit(question, cq)\n    assert ce.text == 'question body , answer to question'\n    assert ce.diff.insert == ' , answer to question'\n    assert ce.diff.start_offset == 13\n"]]}
{"hexsha": "d9d06e270311a385c2b952d8f3d78341b877e635", "ext": "py", "lang": "Python", "content": "def make_batched_features_dataset(file_pattern,\n                                  batch_size,\n                                  features,\n                                  reader=core_readers.TFRecordDataset,\n                                  label_key=None,\n                                  reader_args=None,\n                                  num_epochs=None,\n                                  shuffle=True,\n                                  shuffle_buffer_size=10000,\n                                  shuffle_seed=None,\n                                  prefetch_buffer_size=optimization.AUTOTUNE,\n                                  reader_num_threads=1,\n                                  parser_num_threads=2,\n                                  sloppy_ordering=False,\n                                  drop_final_batch=False):\n  \"\"\"Returns a `Dataset` of feature dictionaries from `Example` protos.\n\n  If label_key argument is provided, returns a `Dataset` of tuple\n  comprising of feature dictionaries and label.\n\n  Example:\n\n  ```\n  serialized_examples = [\n    features {\n      feature { key: \"age\" value { int64_list { value: [ 0 ] } } }\n      feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n      feature { key: \"kws\" value { bytes_list { value: [ \"code\", \"art\" ] } } }\n    },\n    features {\n      feature { key: \"age\" value { int64_list { value: [] } } }\n      feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n      feature { key: \"kws\" value { bytes_list { value: [ \"sports\" ] } } }\n    }\n  ]\n  ```\n\n  We can use arguments:\n\n  ```\n  features: {\n    \"age\": FixedLenFeature([], dtype=tf.int64, default_value=-1),\n    \"gender\": FixedLenFeature([], dtype=tf.string),\n    \"kws\": VarLenFeature(dtype=tf.string),\n  }\n  ```\n\n  And the expected output is:\n\n  ```python\n  {\n    \"age\": [[0], [-1]],\n    \"gender\": [[\"f\"], [\"f\"]],\n    \"kws\": SparseTensor(\n      indices=[[0, 0], [0, 1], [1, 0]],\n      values=[\"code\", \"art\", \"sports\"]\n      dense_shape=[2, 2]),\n  }\n  ```\n\n  Args:\n    file_pattern: List of files or patterns of file paths containing\n      `Example` records. See `tf.gfile.Glob` for pattern rules.\n    batch_size: An int representing the number of records to combine\n      in a single batch.\n    features: A `dict` mapping feature keys to `FixedLenFeature` or\n      `VarLenFeature` values. See `tf.parse_example`.\n    reader: A function or class that can be\n      called with a `filenames` tensor and (optional) `reader_args` and returns\n      a `Dataset` of `Example` tensors. Defaults to `tf.data.TFRecordDataset`.\n    label_key: (Optional) A string corresponding to the key labels are stored in\n      `tf.Examples`. If provided, it must be one of the `features` key,\n      otherwise results in `ValueError`.\n    reader_args: Additional arguments to pass to the reader class.\n    num_epochs: Integer specifying the number of times to read through the\n      dataset. If None, cycles through the dataset forever. Defaults to `None`.\n    shuffle: A boolean, indicates whether the input should be shuffled. Defaults\n      to `True`.\n    shuffle_buffer_size: Buffer size of the ShuffleDataset. A large capacity\n      ensures better shuffling but would increase memory usage and startup time.\n    shuffle_seed: Randomization seed to use for shuffling.\n    prefetch_buffer_size: Number of feature batches to prefetch in order to\n      improve performance. Recommended value is the number of batches consumed\n      per training step. Defaults to auto-tune.\n    reader_num_threads: Number of threads used to read `Example` records. If >1,\n      the results will be interleaved.\n    parser_num_threads: Number of threads to use for parsing `Example` tensors\n      into a dictionary of `Feature` tensors.\n    sloppy_ordering: If `True`, reading performance will be improved at\n      the cost of non-deterministic ordering. If `False`, the order of elements\n      produced is deterministic prior to shuffling (elements are still\n      randomized if `shuffle=True`. Note that if the seed is set, then order\n      of elements after shuffling is deterministic). Defaults to `False`.\n    drop_final_batch: If `True`, and the batch size does not evenly divide the\n      input dataset size, the final smaller batch will be dropped. Defaults to\n      `False`.\n\n  Returns:\n    A dataset of `dict` elements, (or a tuple of `dict` elements and label).\n    Each `dict` maps feature keys to `Tensor` or `SparseTensor` objects.\n\n  Raises:\n    ValueError: If `label_key` is not one of the `features` keys.\n  \"\"\"\n  # Create dataset of all matching filenames\n  filenames = _get_file_names(file_pattern, False)\n  dataset = dataset_ops.Dataset.from_tensor_slices(filenames)\n  if shuffle:\n    dataset = dataset.shuffle(len(filenames), shuffle_seed)\n\n  # Read `Example` records from files as tensor objects.\n  if reader_args is None:\n    reader_args = []\n\n  # Read files sequentially (if reader_num_threads=1) or in parallel\n  dataset = dataset.apply(\n      interleave_ops.parallel_interleave(\n          lambda filename: reader(filename, *reader_args),\n          cycle_length=reader_num_threads,\n          sloppy=sloppy_ordering))\n\n  # Extract values if the `Example` tensors are stored as key-value tuples.\n  if dataset.output_types == (dtypes.string, dtypes.string):\n    dataset = dataset_ops.MapDataset(\n        dataset, lambda _, v: v, use_inter_op_parallelism=False)\n\n  # Apply dataset repeat and shuffle transformations.\n  dataset = _maybe_shuffle_and_repeat(\n      dataset, num_epochs, shuffle, shuffle_buffer_size, shuffle_seed)\n\n  # NOTE(mrry): We set `drop_remainder=True` when `num_epochs is None` to\n  # improve the shape inference, because it makes the batch dimension static.\n  # It is safe to do this because in that case we are repeating the input\n  # indefinitely, and all batches will be full-sized.\n  dataset = dataset.batch(\n      batch_size, drop_remainder=drop_final_batch or num_epochs is None)\n\n  # Parse `Example` tensors to a dictionary of `Feature` tensors.\n  dataset = dataset.apply(\n      parsing_ops.parse_example_dataset(\n          features, num_parallel_calls=parser_num_threads))\n\n  if label_key:\n    if label_key not in features:\n      raise ValueError(\n          \"The `label_key` provided (%r) must be one of the `features` keys.\" %\n          label_key)\n    dataset = dataset.map(lambda x: (x, x.pop(label_key)))\n\n  dataset = dataset.prefetch(prefetch_buffer_size)\n  return dataset", "fn_id": 11, "class_fn": false, "repo": "zpreisler/tensorflow", "file": "tensorflow/contrib/data/python/ops/readers.py", "last_update_at": "2020-10-06T14:53:32+00:00", "question_id": "d9d06e270311a385c2b952d8f3d78341b877e635_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def make_batched_features_dataset(file_pattern, batch_size, features, reader=core_readers.TFRecordDataset, label_key=None, reader_args=None, num_epochs=None, shuffle=True, shuffle_buffer_size=10000, shuffle_seed=None, prefetch_buffer_size=optimization.AUTOTUNE, reader_num_threads=1, parser_num_threads=2, sloppy_ordering=False, drop_final_batch=False):\n    \"\"\"Returns a `Dataset` of feature dictionaries from `Example` protos.\n\n  If label_key argument is provided, returns a `Dataset` of tuple\n  comprising of feature dictionaries and label.\n\n  Example:\n\n  ```\n  serialized_examples = [\n    features {\n      feature { key: \"age\" value { int64_list { value: [ 0 ] } } }\n      feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n      feature { key: \"kws\" value { bytes_list { value: [ \"code\", \"art\" ] } } }\n    },\n    features {\n      feature { key: \"age\" value { int64_list { value: [] } } }\n      feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n      feature { key: \"kws\" value { bytes_list { value: [ \"sports\" ] } } }\n    }\n  ]\n  ```\n\n  We can use arguments:\n\n  ```\n  features: {\n    \"age\": FixedLenFeature([], dtype=tf.int64, default_value=-1),\n    \"gender\": FixedLenFeature([], dtype=tf.string),\n    \"kws\": VarLenFeature(dtype=tf.string),\n  }\n  ```\n\n  And the expected output is:\n\n  ```python\n  {\n    \"age\": [[0], [-1]],\n    \"gender\": [[\"f\"], [\"f\"]],\n    \"kws\": SparseTensor(\n      indices=[[0, 0], [0, 1], [1, 0]],\n      values=[\"code\", \"art\", \"sports\"]\n      dense_shape=[2, 2]),\n  }\n  ```\n\n  Args:\n    file_pattern: List of files or patterns of file paths containing\n      `Example` records. See `tf.gfile.Glob` for pattern rules.\n    batch_size: An int representing the number of records to combine\n      in a single batch.\n    features: A `dict` mapping feature keys to `FixedLenFeature` or\n      `VarLenFeature` values. See `tf.parse_example`.\n    reader: A function or class that can be\n      called with a `filenames` tensor and (optional) `reader_args` and returns\n      a `Dataset` of `Example` tensors. Defaults to `tf.data.TFRecordDataset`.\n    label_key: (Optional) A string corresponding to the key labels are stored in\n      `tf.Examples`. If provided, it must be one of the `features` key,\n      otherwise results in `ValueError`.\n    reader_args: Additional arguments to pass to the reader class.\n    num_epochs: Integer specifying the number of times to read through the\n      dataset. If None, cycles through the dataset forever. Defaults to `None`.\n    shuffle: A boolean, indicates whether the input should be shuffled. Defaults\n      to `True`.\n    shuffle_buffer_size: Buffer size of the ShuffleDataset. A large capacity\n      ensures better shuffling but would increase memory usage and startup time.\n    shuffle_seed: Randomization seed to use for shuffling.\n    prefetch_buffer_size: Number of feature batches to prefetch in order to\n      improve performance. Recommended value is the number of batches consumed\n      per training step. Defaults to auto-tune.\n    reader_num_threads: Number of threads used to read `Example` records. If >1,\n      the results will be interleaved.\n    parser_num_threads: Number of threads to use for parsing `Example` tensors\n      into a dictionary of `Feature` tensors.\n    sloppy_ordering: If `True`, reading performance will be improved at\n      the cost of non-deterministic ordering. If `False`, the order of elements\n      produced is deterministic prior to shuffling (elements are still\n      randomized if `shuffle=True`. Note that if the seed is set, then order\n      of elements after shuffling is deterministic). Defaults to `False`.\n    drop_final_batch: If `True`, and the batch size does not evenly divide the\n      input dataset size, the final smaller batch will be dropped. Defaults to\n      `False`.\n\n  Returns:\n    A dataset of `dict` elements, (or a tuple of `dict` elements and label).\n    Each `dict` maps feature keys to `Tensor` or `SparseTensor` objects.\n\n  Raises:\n    ValueError: If `label_key` is not one of the `features` keys.\n  \"\"\"\n    filenames = _get_file_names(file_pattern, False)\n    dataset = dataset_ops.Dataset.from_tensor_slices(filenames)\n    if shuffle:\n        dataset = dataset.shuffle(len(filenames), shuffle_seed)\n    if reader_args is None:\n        reader_args = []\n    dataset = dataset.apply(interleave_ops.parallel_interleave(lambda filename: reader(filename, *reader_args), cycle_length=reader_num_threads, sloppy=sloppy_ordering))\n    if dataset.output_types == (dtypes.string, dtypes.string):\n        dataset = dataset_ops.MapDataset(dataset, lambda _, v: v, use_inter_op_parallelism=False)\n    dataset = _maybe_shuffle_and_repeat(dataset, num_epochs, shuffle, shuffle_buffer_size, shuffle_seed)\n    dataset = dataset.batch(batch_size, drop_remainder=drop_final_batch or num_epochs is None)\n    dataset = dataset.apply(parsing_ops.parse_example_dataset(features, num_parallel_calls=parser_num_threads))\n    if label_key:\n        if label_key not in features:\n            raise ValueError('The `label_key` provided (%r) must be one of the `features` keys.' % label_key)\n        dataset = dataset.map(lambda x: (x, x.pop(label_key)))\n    dataset = dataset.prefetch(prefetch_buffer_size)\n"]]}
{"hexsha": "0a4ee7ec3cf7e8ba6da8726fac8e19bee3434dcf", "ext": "py", "lang": "Python", "content": "def decompose_tokens(tokens, shuffle):\n    decomposed = list()\n    for i, token in enumerate(tokens):\n        decomposed.append(tokens[:i+1])\n    if shuffle:\n        random.shuffle(decomposed)\n    return decomposed", "fn_id": 2, "class_fn": false, "repo": "Kleping/nlp-ml.neural-machine-translation", "file": "classes/auxiliary.py", "last_update_at": "2020-09-07T09:54:12+00:00", "question_id": "0a4ee7ec3cf7e8ba6da8726fac8e19bee3434dcf_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def decompose_tokens(tokens, shuffle):\n    decomposed = list()\n    for i, token in enumerate(tokens):\n        decomposed.append(tokens[:i + 1])\n    if shuffle:\n        random.shuffle(decomposed)\n"]]}
{"hexsha": "7a8e28da299122c36d383ba559ef8dae560f1c9b", "ext": "py", "lang": "Python", "content": "async def run_all(cmds, timeout=None, retry=0, loop=None, output=None, **kwargs):\n    output = _get_output(output)\n    ps = []\n    for cmd in cmds: ps.append(await Process.create(cmd, timeout, retry, loop, **kwargs))\n    mp = MutiProcesses(ps)\n    await mp.wait()\n    results = []\n    for p in mp.proccesses:\n        if p.returncode != 0:\n            results.append(CmdRunError(p.cmd, p.returncode, await _readstr(p.stderr), await _readstr(p.stdout)))\n        else:\n            results.append(await output(p.stdout))\n    return results", "fn_id": 2, "class_fn": false, "repo": "tornadoyi/pyplus", "file": "pyplus/async/shell.py", "last_update_at": "2020-04-02T13:44:53+00:00", "question_id": "7a8e28da299122c36d383ba559ef8dae560f1c9b_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def run_all(cmds, timeout=None, retry=0, loop=None, output=None, **kwargs):\n    output = _get_output(output)\n    ps = []\n    for cmd in cmds:\n        ps.append(await Process.create(cmd, timeout, retry, loop, **kwargs))\n    mp = MutiProcesses(ps)\n    await mp.wait()\n    results = []\n    for p in mp.proccesses:\n        if p.returncode != 0:\n            results.append(CmdRunError(p.cmd, p.returncode, await _readstr(p.stderr), await _readstr(p.stdout)))\n        else:\n            results.append(await output(p.stdout))\n"]]}
{"hexsha": "66cbba55d8f68c58d685cc45617c4365d6588dda", "ext": "py", "lang": "Python", "content": "@app.delete('/strava/athletes/{strava_athlete_id}')\nasync def delete_strava_athletes(strava_athlete_id: int, admin: bool = Depends(is_admin)):\n    strava_athlete = await StravaAthlete.objects.get(id=strava_athlete_id)\n\n    client = Client(strava_athlete.access_token)\n    try:\n        client.deauthorize()\n    except stravalib_exceptions.AccessUnauthorized:\n        strava_athlete = await refresh_access_token(strava_athlete)\n        client = Client(strava_athlete.access_token)\n        client.deauthorize()\n\n    await strava_athlete.delete()\n\n    return strava_athlete", "fn_id": 8, "class_fn": false, "repo": "AartGoossens/sweaty-reports", "file": "app/strava/views.py", "last_update_at": "2020-09-07T13:08:01+00:00", "question_id": "66cbba55d8f68c58d685cc45617c4365d6588dda_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.delete('/strava/athletes/{strava_athlete_id}')\nasync def delete_strava_athletes(strava_athlete_id: int, admin: bool=Depends(is_admin)):\n    strava_athlete = await StravaAthlete.objects.get(id=strava_athlete_id)\n    client = Client(strava_athlete.access_token)\n    try:\n        client.deauthorize()\n    except stravalib_exceptions.AccessUnauthorized:\n        strava_athlete = await refresh_access_token(strava_athlete)\n        client = Client(strava_athlete.access_token)\n        client.deauthorize()\n    await strava_athlete.delete()\n"]]}
{"hexsha": "d74cf4ec18d91885e973a541620f636e9ba6ebd5", "ext": "py", "lang": "Python", "content": "def checkGapPhase(ID, **kwargs):\n    \"\"\"\n    check ID gap, phase\n    return True if success, otherwise False\n    \"\"\"\n    gapMin, gapMax, gapStep, gapTol = kwargs.get(\"gap\",\n                                                 _params[ID.name][\"gap\"])\n    phaseMin, phaseMax, phaseStep, phaseTol = \\\n        kwargs.get(\"phase\",  _params[ID.name].get(\"phase\", (None, None, None, None)))\n    timeout = kwargs.get(\"timeout\", 150)\n    throw   = kwargs.get(\"throw\", True)\n    unitsys = kwargs.get(\"unitsys\", _params[ID.name][\"unitsys\"])\n    verbose = kwargs.get(\"verbose\", 0)\n    gapStep = kwargs.get(\"gapStep\", gapStep)\n    phaseStep = kwargs.get(\"phaseStep\", phaseStep)\n\n    flds = ID.fields()\n    if 'gap' in flds:\n        for gap in np.linspace(gapMin, gapMax, gapStep):\n            gapList = [['gap',gap, gapTol]]\n            gapStatus = putPar(ID,gapList,timeout=timeout,\n                               throw=throw,unitsys=unitsys,verbose=verbose)\n            if not gapStatus:\n                return False\n    if 'phase' in flds:\n        for phase in np.linspace(phaseMin,phaseMax,phaseStep):\n            phaseList = [['phase',phase,phaseTol]]\n            phaseStatus = putPar(ID,phaseList,timeout=timeout,\n                               throw=throw,unitsys=unitsys,verbose=verbose)\n            if not phaseStatus:\n                return False\n    return True", "fn_id": 7, "class_fn": false, "repo": "NSLS-II/aphla", "file": "aphla/nsls2id.py", "last_update_at": "2020-02-20T17:06:20+00:00", "question_id": "d74cf4ec18d91885e973a541620f636e9ba6ebd5_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def checkGapPhase(ID, **kwargs):\n    \"\"\"\n    check ID gap, phase\n    return True if success, otherwise False\n    \"\"\"\n    gapMin, gapMax, gapStep, gapTol = kwargs.get('gap', _params[ID.name]['gap'])\n    phaseMin, phaseMax, phaseStep, phaseTol = kwargs.get('phase', _params[ID.name].get('phase', (None, None, None, None)))\n    timeout = kwargs.get('timeout', 150)\n    throw = kwargs.get('throw', True)\n    unitsys = kwargs.get('unitsys', _params[ID.name]['unitsys'])\n    verbose = kwargs.get('verbose', 0)\n    gapStep = kwargs.get('gapStep', gapStep)\n    phaseStep = kwargs.get('phaseStep', phaseStep)\n    flds = ID.fields()\n    if 'gap' in flds:\n        for gap in np.linspace(gapMin, gapMax, gapStep):\n            gapList = [['gap', gap, gapTol]]\n            gapStatus = putPar(ID, gapList, timeout=timeout, throw=throw, unitsys=unitsys, verbose=verbose)\n            if not gapStatus:\n                return False\n    if 'phase' in flds:\n        for phase in np.linspace(phaseMin, phaseMax, phaseStep):\n            phaseList = [['phase', phase, phaseTol]]\n            phaseStatus = putPar(ID, phaseList, timeout=timeout, throw=throw, unitsys=unitsys, verbose=verbose)\n            if not phaseStatus:\n                return False\n"]]}
{"hexsha": "8769e337c40a0cdc510c62136493b4463c60c833", "ext": "py", "lang": "Python", "content": "def power(verbose, config=None, filename=None):\n    if config == None:\n        if filename == None:\n            print(\"config needed\")\n            exit(1)\n        else:\n            config = load_config(filename)\n\n    clock_power = clock(verbose,config)\n    logic_power = logic(verbose, config)\n    BRAM_power = BRAM(verbose, config)\n    DSP_power = DSP(verbose, config)\n    static_power = static(verbose, config)\n    \n    return round(clock_power + BRAM_power + DSP_power + logic_power + static_power, 3), clock_power, logic_power, BRAM_power, DSP_power, static_power", "fn_id": 0, "class_fn": false, "repo": "ShaoChongZhang/estimator", "file": "src/estimator.py", "last_update_at": "2020-07-30T16:54:40+00:00", "question_id": "8769e337c40a0cdc510c62136493b4463c60c833_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def power(verbose, config=None, filename=None):\n    if config == None:\n        if filename == None:\n            print('config needed')\n            exit(1)\n        else:\n            config = load_config(filename)\n    clock_power = clock(verbose, config)\n    logic_power = logic(verbose, config)\n    BRAM_power = BRAM(verbose, config)\n    DSP_power = DSP(verbose, config)\n    static_power = static(verbose, config)\n"]]}
{"hexsha": "5ad24b1552a54c6d7d1651eab5ec89d070518b50", "ext": "py", "lang": "Python", "content": "def upload(filepath):\n    payload = {}\n    files = [\n        ('file', open(filepath, 'rb'))\n    ]\n    response = requests.request(\"POST\", url, data=payload, files=files)\n    if response.ok:\n        return 1\n        print(response.json)\n    else:\n        print(123)", "fn_id": 0, "class_fn": false, "repo": "kxjko/bootcamp", "file": "solutions/video_similarity_search/search-video-demo/deploy/import_data.py", "last_update_at": "2020-04-03T05:24:47+00:00", "question_id": "5ad24b1552a54c6d7d1651eab5ec89d070518b50_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def upload(filepath):\n    payload = {}\n    files = [('file', open(filepath, 'rb'))]\n    response = requests.request('POST', url, data=payload, files=files)\n    if response.ok:\n        return 1\n        print(response.json)\n    else:\n"]]}
{"hexsha": "095ac223d811183eda37c0c4ac06bce2dd225277", "ext": "py", "lang": "Python", "content": "def find_2x2_matching(matrix):\n    matches = 0\n    for i in range(len(matrix) - 1):\n        for j in range(len(matrix[i]) - 1):\n            if matrix[i][j] == matrix[i][j + 1] == matrix[i + 1][j] == matrix[i + 1][j + 1]:\n                matches += 1\n    print(matches)", "fn_id": 0, "class_fn": false, "repo": "B3WD/python_advanced", "file": "Multidimensional Lists - Exercise/02. 2x2 Squares in Matrix.py", "last_update_at": "2020-10-28T07:52:17+00:00", "question_id": "095ac223d811183eda37c0c4ac06bce2dd225277_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def find_2x2_matching(matrix):\n    matches = 0\n    for i in range(len(matrix) - 1):\n        for j in range(len(matrix[i]) - 1):\n            if matrix[i][j] == matrix[i][j + 1] == matrix[i + 1][j] == matrix[i + 1][j + 1]:\n                matches += 1\n"]]}
{"hexsha": "ebbf2e42ee03772b9cfb6f2e07d80134666fd0de", "ext": "py", "lang": "Python", "content": "def save_model_to_hdf5_group(grp: h5py.Group, net: torch.nn.Module):\n    # this will work regardless whether\n    for x, y in net.named_parameters():\n        if x not in grp:\n            # currently, it's not big. So I don't do compression at all, for speed.\n            grp.create_dataset(x, data=y.data.cpu().numpy())\n            grp.file.flush()", "fn_id": 0, "class_fn": false, "repo": "leelabcnbc/tang_jcompneuro_revision", "file": "tang_jcompneuro_legacy/cnn.py", "last_update_at": "2020-12-25T03:10:30+00:00", "question_id": "ebbf2e42ee03772b9cfb6f2e07d80134666fd0de_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def save_model_to_hdf5_group(grp: h5py.Group, net: torch.nn.Module):\n    for x, y in net.named_parameters():\n        if x not in grp:\n            grp.create_dataset(x, data=y.data.cpu().numpy())\n"]]}
{"hexsha": "642b901db4811563f00b0cc99e370ef5f380dd0b", "ext": "py", "lang": "Python", "content": "def save(pi, sess, name):\n    saver = tf.train.Saver(pi.get_variables())\n    save_dir = os.path.join(logger.get_dir(), name)\n    saver.save(sess, save_dir)", "fn_id": 1, "class_fn": false, "repo": "shagunsodhani/consistent-dynamics", "file": "codes/model/expert_policy/train_mujoco.py", "last_update_at": "2020-05-25T20:32:47+00:00", "question_id": "642b901db4811563f00b0cc99e370ef5f380dd0b_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def save(pi, sess, name):\n    saver = tf.train.Saver(pi.get_variables())\n    save_dir = os.path.join(logger.get_dir(), name)\n"]]}
{"hexsha": "7271352807c2c5854d5b0bb5b3cd4fd112210bfc", "ext": "py", "lang": "Python", "content": "def readPower():\n    try:\n        requestP = modbus_client.read_holding_registers(10, 2, unit=1)\n        print(requestP.registers)\n        power_demand = requestP.registers[0]\n        power_supply = requestP.registers[1]\n    except Exception as ex:\n        # logger.exception(\"READPOWER Exception\")\n        print(ex)\n        print(\"Exception in Modbus Reading\")\n        requestP = modbus_client.read_holding_registers(10, 2, unit=1)\n        power_demand = requestP.registers[0]\n        power_supply = requestP.registers[1]\n    return power_demand, power_supply", "fn_id": 0, "class_fn": false, "repo": "m-yahya/test-snap", "file": "olibox_core/olibox_core/olibox_pkg/modbus_config.py", "last_update_at": "2020-02-17T01:37:33+00:00", "question_id": "7271352807c2c5854d5b0bb5b3cd4fd112210bfc_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def readPower():\n    try:\n        requestP = modbus_client.read_holding_registers(10, 2, unit=1)\n        print(requestP.registers)\n        power_demand = requestP.registers[0]\n        power_supply = requestP.registers[1]\n    except Exception as ex:\n        print(ex)\n        print('Exception in Modbus Reading')\n        requestP = modbus_client.read_holding_registers(10, 2, unit=1)\n        power_demand = requestP.registers[0]\n        power_supply = requestP.registers[1]\n"]]}
{"hexsha": "df8ffbba8b5e866c8e52242d4e473423a24171bf", "ext": "py", "lang": "Python", "content": "def add_follower(page_name=None):\n    \"\"\"\n\n    @param page_name:\n    @return: @rtype:\n    \"\"\"\n    error = list()\n\n    # get the id of the user you want to follow\n    if request.method == 'GET':\n        # via ajax\n        message = request.args.get('follow-user-id', '', type=str)\n    else:\n        # via no javascript post\n        message = request.form['follow-user-id']\n\n    if 'user' in session:\n        dbh = MessageHelper(StrictRedis())\n        message_object = Message(session['user']['id'], message)\n        dbh.post_message(message_object)\n\n    else:\n        return redirect(url_for('login'))\n\n    if request.method == 'GET':\n        return jsonify(post_time=message_object.posted_time,\n                       format_time=message_object.formatted_time,\n                       msg_id=message_object.id)\n\n    return redirect(url_for('dash'))", "fn_id": 0, "class_fn": false, "repo": "yareally/twitter-clone-python", "file": "controllers/retweet_msg.py", "last_update_at": "2020-05-22T22:13:48+00:00", "question_id": "df8ffbba8b5e866c8e52242d4e473423a24171bf_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def add_follower(page_name=None):\n    \"\"\"\n\n    @param page_name:\n    @return: @rtype:\n    \"\"\"\n    error = list()\n    if request.method == 'GET':\n        message = request.args.get('follow-user-id', '', type=str)\n    else:\n        message = request.form['follow-user-id']\n    if 'user' in session:\n        dbh = MessageHelper(StrictRedis())\n        message_object = Message(session['user']['id'], message)\n        dbh.post_message(message_object)\n    else:\n        return redirect(url_for('login'))\n    if request.method == 'GET':\n        return jsonify(post_time=message_object.posted_time, format_time=message_object.formatted_time, msg_id=message_object.id)\n"]]}
{"hexsha": "e900714ee7c1622a99ce0b8aa996f354ebce7e08", "ext": "py", "lang": "Python", "content": "def inception_C_block(name, input):\n\n    '''\n\n    :param name:\n    :param input:\n    :return:\n    '''\n\n    with tf.name_scope(name) as scope:\n        pool_1b1 = layers.pool_layer(scope+'pool_1b1', input, ksize=[1,1], kstep=[1,1], padding='SAME', pool_fun=tf.nn.avg_pool)\n        conv_1b1 = layers.conv_layer(scope+'conv_1b1', pool_1b1, ksize=[1,1], depth=256, kstep=[1,1],padding='SAME')\n\n        conv_2b2 = layers.conv_layer(scope+'conv_2b2', input, ksize=[1,1], depth=256, kstep=[1,1], padding='SAME')\n\n        conv_3b3 = layers.conv_layer(scope+'conv_3b3', input, ksize=[1,1], depth=384, kstep=[1,1], padding='SAME')\n        conv_4b3_1 = layers.conv_layer(scope+'conv_4b3_1',conv_3b3, ksize=[1,3], depth=256, kstep=[1,1], padding='SAME')\n        conv_5b3_2 = layers.conv_layer(scope+'conv_5b3_2',conv_3b3, ksize=[3,1], depth=256, kstep=[1,1], padding='SAME')\n\n        conv_6b4 = layers.conv_layer(scope+'conv_6b4', input, ksize=[1,1], depth=384, kstep=[1,1], padding='SAME')\n        conv_7b4 = layers.conv_layer(scope+'conv_7b4', conv_6b4, ksize=[1,3],depth=448, kstep=[1,1], padding='SAME')\n        conv_8b4 = layers.conv_layer(scope+'conv_8b4', conv_7b4, ksize=[3,1], depth=512, kstep=[1,1], padding='SAME')\n        conv_9b4_1 = layers.conv_layer(scope+'conv_9b4_1', conv_8b4, ksize=[3,1], depth=256, kstep=[1,1], padding='SAME')\n        conv_10b4_2 = layers.conv_layer(scope+'conv_10b4_2',conv_8b4,ksize=[1,3], depth=256, kstep=[1,1],padding='SAME')\n\n        concat_0 = tf.concat(3, [conv_1b1,conv_2b2,conv_4b3_1,conv_5b3_2,conv_9b4_1,conv_10b4_2],name=scope+'concat_0')\n        return concat_0", "fn_id": 5, "class_fn": false, "repo": "xiaocn/GTF", "file": "src/base/inception_net.py", "last_update_at": "2020-03-22T08:14:26+00:00", "question_id": "e900714ee7c1622a99ce0b8aa996f354ebce7e08_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def inception_C_block(name, input):\n    \"\"\"\n\n    :param name:\n    :param input:\n    :return:\n    \"\"\"\n    with tf.name_scope(name) as scope:\n        pool_1b1 = layers.pool_layer(scope + 'pool_1b1', input, ksize=[1, 1], kstep=[1, 1], padding='SAME', pool_fun=tf.nn.avg_pool)\n        conv_1b1 = layers.conv_layer(scope + 'conv_1b1', pool_1b1, ksize=[1, 1], depth=256, kstep=[1, 1], padding='SAME')\n        conv_2b2 = layers.conv_layer(scope + 'conv_2b2', input, ksize=[1, 1], depth=256, kstep=[1, 1], padding='SAME')\n        conv_3b3 = layers.conv_layer(scope + 'conv_3b3', input, ksize=[1, 1], depth=384, kstep=[1, 1], padding='SAME')\n        conv_4b3_1 = layers.conv_layer(scope + 'conv_4b3_1', conv_3b3, ksize=[1, 3], depth=256, kstep=[1, 1], padding='SAME')\n        conv_5b3_2 = layers.conv_layer(scope + 'conv_5b3_2', conv_3b3, ksize=[3, 1], depth=256, kstep=[1, 1], padding='SAME')\n        conv_6b4 = layers.conv_layer(scope + 'conv_6b4', input, ksize=[1, 1], depth=384, kstep=[1, 1], padding='SAME')\n        conv_7b4 = layers.conv_layer(scope + 'conv_7b4', conv_6b4, ksize=[1, 3], depth=448, kstep=[1, 1], padding='SAME')\n        conv_8b4 = layers.conv_layer(scope + 'conv_8b4', conv_7b4, ksize=[3, 1], depth=512, kstep=[1, 1], padding='SAME')\n        conv_9b4_1 = layers.conv_layer(scope + 'conv_9b4_1', conv_8b4, ksize=[3, 1], depth=256, kstep=[1, 1], padding='SAME')\n        conv_10b4_2 = layers.conv_layer(scope + 'conv_10b4_2', conv_8b4, ksize=[1, 3], depth=256, kstep=[1, 1], padding='SAME')\n        concat_0 = tf.concat(3, [conv_1b1, conv_2b2, conv_4b3_1, conv_5b3_2, conv_9b4_1, conv_10b4_2], name=scope + 'concat_0')\n"]]}
{"hexsha": "363e5efe809fc0b49d474572ce8794fb6dc275cd", "ext": "py", "lang": "Python", "content": "def access_ext(name):\n    #\u7528\u4e8e\u6269\u5c55\u540d\u79f0\u7684\u83b7\u53d6\n    ALLOW_SET = ['jpg', 'jpeg', 'png', 'webp', 'gif']\n    li = name.split('.')\n    if  len(li) > 1 and li[-1] in ALLOW_SET:\n        return \".\" + li[-1] #\u8fd4\u56de\u6269\u5c55\u540d\n\n    else:\n        #\u4e0d\u5408\u6cd5\u7684\u6587\u4ef6\u9ed8\u8ba4\u4fdd\u5b58\u4e3a\u65e0\u6269\u5c55\u7684\u6587\u4ef6\n        #\u8fd8\u9700\u8981\u5728\u524d\u7aef\u505a\u6587\u4ef6\u7c7b\u578b\u7684\u5224\u65ad\n        return ''", "fn_id": 1, "class_fn": false, "repo": "xk-wang/mgek_imgbed", "file": "app/utils/rename.py", "last_update_at": "2020-05-20T07:47:30+00:00", "question_id": "363e5efe809fc0b49d474572ce8794fb6dc275cd_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def access_ext(name):\n    ALLOW_SET = ['jpg', 'jpeg', 'png', 'webp', 'gif']\n    li = name.split('.')\n    if len(li) > 1 and li[-1] in ALLOW_SET:\n        return '.' + li[-1]\n    else:\n"]]}
{"hexsha": "0e313b0129840a9db7ca51c374e69f7035db7c71", "ext": "py", "lang": "Python", "content": "async def canvas_fetch(canvas_url, canvas_key, course_id, destination):\n    from canvasapi import Canvas\n\n    canvas = Canvas(canvas_url, canvas_key)\n    course = canvas.get_course(course_id)\n\n    for folder in course.get_folders():\n        for file in folder.get_files():\n            dest_path = os.path.join(destination, folder.full_name, file.filename)\n            if os.path.exists(dest_path):\n                file_mtime = datetime.fromtimestamp(os.path.getmtime(dest_path)).astimezone(pytz.utc)\n                if file.updated_at_date <= file_mtime:\n                    print(f'{dest_path} up to date, skipping')\n                    continue\n            # FIXME: protect against path traversal attacks here\n            os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n            file.download(dest_path)\n            print(f'downloaded {dest_path}')", "fn_id": 0, "class_fn": false, "repo": "yuvipanda/f2git", "file": "f2git.py", "last_update_at": "2020-09-06T12:16:27+00:00", "question_id": "0e313b0129840a9db7ca51c374e69f7035db7c71_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def canvas_fetch(canvas_url, canvas_key, course_id, destination):\n    from canvasapi import Canvas\n    canvas = Canvas(canvas_url, canvas_key)\n    course = canvas.get_course(course_id)\n    for folder in course.get_folders():\n        for file in folder.get_files():\n            dest_path = os.path.join(destination, folder.full_name, file.filename)\n            if os.path.exists(dest_path):\n                file_mtime = datetime.fromtimestamp(os.path.getmtime(dest_path)).astimezone(pytz.utc)\n                if file.updated_at_date <= file_mtime:\n                    print(f'{dest_path} up to date, skipping')\n                    continue\n            os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n            file.download(dest_path)\n"]]}
{"hexsha": "666971ee0009d0bafb0de9d0ae6601b14f216e6d", "ext": "py", "lang": "Python", "content": "@scheme\ndef main_path(source_path, download_yt_captions, convert_wav):\n    if convert_wav:\n        src = AudioFileClip(source_path)\n        filename = os.path.basename(source_path)\n        name, _ = os.path.splitext(filename)\n        wav_path = os.path.join(tempdir_path(), name + '.wav')\n        src.write_audiofile(wav_path)\n        return wav_path\n    else:\n        return source_path", "fn_id": 2, "class_fn": false, "repo": "yeonghoey/hew", "file": "hew/core.py", "last_update_at": "2020-01-27T16:23:21+00:00", "question_id": "666971ee0009d0bafb0de9d0ae6601b14f216e6d_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@scheme\ndef main_path(source_path, download_yt_captions, convert_wav):\n    if convert_wav:\n        src = AudioFileClip(source_path)\n        filename = os.path.basename(source_path)\n        name, _ = os.path.splitext(filename)\n        wav_path = os.path.join(tempdir_path(), name + '.wav')\n        src.write_audiofile(wav_path)\n        return wav_path\n    else:\n"]]}
{"hexsha": "32bec60d9a52bbdd54f6335566d470bc05e2e7aa", "ext": "py", "lang": "Python", "content": "def execute():\n    print(\"Initializing Stock Settings Custom Fields\")\n    doc = frappe.get_doc(\"Stock Settings\")\n    if not doc.get(\"cogs_on_invoice\"):\n        doc.cogs_on_invoice = 1\n    doc.save()", "fn_id": 0, "class_fn": false, "repo": "agritheory/aptronics", "file": "aptronics/patches/init_custom_stock_settings.py", "last_update_at": "2020-02-26T11:11:20+00:00", "question_id": "32bec60d9a52bbdd54f6335566d470bc05e2e7aa_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def execute():\n    print('Initializing Stock Settings Custom Fields')\n    doc = frappe.get_doc('Stock Settings')\n    if not doc.get('cogs_on_invoice'):\n        doc.cogs_on_invoice = 1\n"]]}
{"hexsha": "a07476cc01bd32fa4a5458dc36f7f4b9768fd5c4", "ext": "py", "lang": "Python", "content": "def build_ml_decoder_model(backbone, num_classes=80, num_of_groups=-1, decoder_embedding=768, **kwargs):\n    \"\"\"Create a model\n    \"\"\"\n    num_features = backbone.num_features\n    if num_classes == -1:\n        num_classes = backbone.num_classes\n    # loading ML decoder model\n    if hasattr(backbone, \"model\"): # timm models\n        backbone.model.classifier = None\n    else:\n        backbone.classifier = None\n    model = MLDecoder(backbone, num_classes=num_classes, initial_num_features=num_features, num_of_groups=num_of_groups,\n                      decoder_embedding=decoder_embedding, **kwargs)\n    return model", "fn_id": 0, "class_fn": false, "repo": "opencv/deep-person-reid", "file": "torchreid/models/ml_decoder.py", "last_update_at": "2020-07-07T19:22:17+00:00", "question_id": "a07476cc01bd32fa4a5458dc36f7f4b9768fd5c4_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def build_ml_decoder_model(backbone, num_classes=80, num_of_groups=-1, decoder_embedding=768, **kwargs):\n    \"\"\"Create a model\n    \"\"\"\n    num_features = backbone.num_features\n    if num_classes == -1:\n        num_classes = backbone.num_classes\n    if hasattr(backbone, 'model'):\n        backbone.model.classifier = None\n    else:\n        backbone.classifier = None\n    model = MLDecoder(backbone, num_classes=num_classes, initial_num_features=num_features, num_of_groups=num_of_groups, decoder_embedding=decoder_embedding, **kwargs)\n"]]}
{"hexsha": "7ca833a58757ca3c989bc7d016be73824d7526c6", "ext": "py", "lang": "Python", "content": "def get_all_text_(node):\n    if node.text is not None:\n        text = node.text\n    else:\n        text = ''\n    for child in node:\n        if child.tail is not None:\n            text += child.tail\n    return text", "fn_id": 6, "class_fn": false, "repo": "Escodoo/nfselib", "file": "nfselib/dsf/RetornoConsultaLote.py", "last_update_at": "2020-11-08T22:03:07+00:00", "question_id": "7ca833a58757ca3c989bc7d016be73824d7526c6_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_all_text_(node):\n    if node.text is not None:\n        text = node.text\n    else:\n        text = ''\n    for child in node:\n        if child.tail is not None:\n            text += child.tail\n"]]}
{"hexsha": "78899763761ba1ba1832a56a870e4aa4b4167d4e", "ext": "py", "lang": "Python", "content": "def move_border(old_center, old_border_diameter, center, border_diameter, delta_time, steps=1):\n    global server_ip, server_password, stats_control_ok\n    # waiting 10 seconds to avoid any problems with other orders at the round start\n    sleep(10)\n    # connect to server\n    with MCRcon(server_ip, server_password) as mcr:\n        # one step per second (starting by 1; ending by delta_time)\n        for n in range(1, delta_time + 1):\n            # end shrink when someone won\n            if not stats_control_ok:\n                break\n\n            # calculating new diameter with a linear equation\n            # y=m*x+n\n            # m=gradient=(delta y)/(delta x)=(border_diameter - old_border_diameter) / delta_time\n            # x=current step=n\n            # n=offset=old_border_diameter\n            temp_diameter = ((border_diameter - old_border_diameter) / delta_time) * n + old_border_diameter\n\n            # calculation a new temporary center, similar t temp_diameter\n            temp_x = ((center[0] - old_center[0]) / delta_time) * n + old_center[0]\n            temp_z = ((center[1] - old_center[1]) / delta_time) * n + old_center[1]\n\n            # apply new values\n            mcr.command(\"/worldborder center {} {}\".format(temp_x, temp_z))\n            mcr.command(\"/worldborder set {} {}\".format(str(temp_diameter), str(int(steps))))\n\n            # wait for one step; default is 1sec.\n            sleep(int(steps))", "fn_id": 3, "class_fn": false, "repo": "christopher-besch/mc_royale_supervisor", "file": "mc_royale.py", "last_update_at": "2020-07-13T08:45:08+00:00", "question_id": "78899763761ba1ba1832a56a870e4aa4b4167d4e_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def move_border(old_center, old_border_diameter, center, border_diameter, delta_time, steps=1):\n    global server_ip, server_password, stats_control_ok\n    sleep(10)\n    with MCRcon(server_ip, server_password) as mcr:\n        for n in range(1, delta_time + 1):\n            if not stats_control_ok:\n                break\n            temp_diameter = (border_diameter - old_border_diameter) / delta_time * n + old_border_diameter\n            temp_x = (center[0] - old_center[0]) / delta_time * n + old_center[0]\n            temp_z = (center[1] - old_center[1]) / delta_time * n + old_center[1]\n            mcr.command('/worldborder center {} {}'.format(temp_x, temp_z))\n            mcr.command('/worldborder set {} {}'.format(str(temp_diameter), str(int(steps))))\n"]]}
{"hexsha": "e7fe69ac6c25e8c5c536abbb50fb17f89cffa8f7", "ext": "py", "lang": "Python", "content": "def _interpret_nth(interpreter: Interpreter, actual_params: tp.List[ast.AST], idx: int) -> d.Data:\n    param_value = interpreter.visit(actual_params[0])\n    param_type = type(param_value)\n\n    if not issubclass(param_type, d.List) or len(param_value) <= idx:\n        raise err.ArgumentTypeError(\n            expected=d.List,\n            given=param_value,\n            min_length=idx+1,\n            max_length=None\n        )\n\n    result = param_value[idx]\n    return result", "fn_id": 0, "class_fn": false, "repo": "ZibingZhang/racket-interpreter", "file": "racketinterpreter/predefined/_list.py", "last_update_at": "2020-06-25T23:25:45+00:00", "question_id": "e7fe69ac6c25e8c5c536abbb50fb17f89cffa8f7_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _interpret_nth(interpreter: Interpreter, actual_params: tp.List[ast.AST], idx: int) -> d.Data:\n    param_value = interpreter.visit(actual_params[0])\n    param_type = type(param_value)\n    if not issubclass(param_type, d.List) or len(param_value) <= idx:\n        raise err.ArgumentTypeError(expected=d.List, given=param_value, min_length=idx + 1, max_length=None)\n    result = param_value[idx]\n"]]}
{"hexsha": "350a6c06372099ca7c1a9d8998b9726f2f877b99", "ext": "py", "lang": "Python", "content": "def template_contains_key(template, key):\n    if ContextParser.search_deep_keys(key, template, []):\n        return True\n    return False", "fn_id": 3, "class_fn": false, "repo": "rajkrishnamurthy/checkov", "file": "checkov/serverless/parsers/parser.py", "last_update_at": "2020-07-26T17:56:20+00:00", "question_id": "350a6c06372099ca7c1a9d8998b9726f2f877b99_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def template_contains_key(template, key):\n    if ContextParser.search_deep_keys(key, template, []):\n        return True\n"]]}
{"hexsha": "65fb11ef823908155f63a299cd3969cb1a761a0a", "ext": "py", "lang": "Python", "content": "def add_test_methods_to_database():\n    for content_type, method_dict in registry.items():\n        for method_name, defaults in method_dict.items():\n            TestMethod.objects.update_or_create(\n                content_type=content_type,\n                method_name=method_name,\n                defaults=defaults\n            )", "fn_id": 1, "class_fn": false, "repo": "andrewbird2/django-data-tests", "file": "data_tests/registry.py", "last_update_at": "2020-03-24T20:16:39+00:00", "question_id": "65fb11ef823908155f63a299cd3969cb1a761a0a_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def add_test_methods_to_database():\n    for content_type, method_dict in registry.items():\n        for method_name, defaults in method_dict.items():\n"]]}
{"hexsha": "3f75ec23874d95ee0b08b0fdd08aaf8b860c3f7a", "ext": "py", "lang": "Python", "content": "def mute_spotify(mute_val: bool):\n    '''\n    Finds spotify application and mutes or unmutes it based on the value of mute_val.\n    The input argument to the function sets the mute state of Spotify. The value\n    is True for muted False for unmuted.\n    Returns: nothing.\n    '''\n\n    if platform == 'linux' or platform == 'linux2':\n        _mute_spotify_linux(mute_val)\n    elif platform == 'darwin':\n        pass\n    elif platform == 'win32':\n        _mute_spotify_win32(mute_val)", "fn_id": 0, "class_fn": false, "repo": "sellicott/SpotifyMute", "file": "SpotifyCrossMute/__init__.py", "last_update_at": "2020-11-10T16:42:10+00:00", "question_id": "3f75ec23874d95ee0b08b0fdd08aaf8b860c3f7a_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def mute_spotify(mute_val: bool):\n    \"\"\"\n    Finds spotify application and mutes or unmutes it based on the value of mute_val.\n    The input argument to the function sets the mute state of Spotify. The value\n    is True for muted False for unmuted.\n    Returns: nothing.\n    \"\"\"\n    if platform == 'linux' or platform == 'linux2':\n        _mute_spotify_linux(mute_val)\n    elif platform == 'darwin':\n        pass\n    elif platform == 'win32':\n"]]}
{"hexsha": "c046cc3578bbb9a00d04c7eb1c123082d3a6fd77", "ext": "py", "lang": "Python", "content": "def GetCheckeredBitmap(blocksize=8,ntiles=4,rgb0=b'\\xFF', rgb1=b'\\xCC'):\n    \"\"\"Creates a square RGB checkered bitmap using the two specified colors.\n\n    Inputs:\n    - blocksize:  the number of pixels in each solid color square\n    - ntiles:  the number of tiles along width and height.  Each tile is 2x2 blocks.\n    - rbg0,rgb1:  the first and second colors, as 3-byte strings.\n                  If only 1 byte is provided, it is treated as a grey value.\n\n    The bitmap returned will have width = height = blocksize*ntiles*2\n    \"\"\"\n    size = blocksize*ntiles*2\n\n    if len(rgb0)==1:\n        rgb0 = rgb0 * 3\n    if len(rgb1)==1:\n        rgb1 = rgb1 * 3\n\n    strip0 = (rgb0*blocksize + rgb1*blocksize)*(ntiles*blocksize)\n    strip1 = (rgb1*blocksize + rgb0*blocksize)*(ntiles*blocksize)\n    band = strip0 + strip1\n    data = band * ntiles\n    return wx.Bitmap(data, size, size)", "fn_id": 1, "class_fn": false, "repo": "mbuckaway/CrossMgr", "file": "imagebrowser.py", "last_update_at": "2020-02-05T11:22:03+00:00", "question_id": "c046cc3578bbb9a00d04c7eb1c123082d3a6fd77_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def GetCheckeredBitmap(blocksize=8, ntiles=4, rgb0=b'\\xff', rgb1=b'\\xcc'):\n    \"\"\"Creates a square RGB checkered bitmap using the two specified colors.\n\n    Inputs:\n    - blocksize:  the number of pixels in each solid color square\n    - ntiles:  the number of tiles along width and height.  Each tile is 2x2 blocks.\n    - rbg0,rgb1:  the first and second colors, as 3-byte strings.\n                  If only 1 byte is provided, it is treated as a grey value.\n\n    The bitmap returned will have width = height = blocksize*ntiles*2\n    \"\"\"\n    size = blocksize * ntiles * 2\n    if len(rgb0) == 1:\n        rgb0 = rgb0 * 3\n    if len(rgb1) == 1:\n        rgb1 = rgb1 * 3\n    strip0 = (rgb0 * blocksize + rgb1 * blocksize) * (ntiles * blocksize)\n    strip1 = (rgb1 * blocksize + rgb0 * blocksize) * (ntiles * blocksize)\n    band = strip0 + strip1\n    data = band * ntiles\n"]]}
{"hexsha": "e1fcc40719c6de647b5c2d78a3453d599b57a0c4", "ext": "py", "lang": "Python", "content": "def df_to_dataset(df, predictor,  batch_size=32):\n    df = df.copy()\n    labels = df.pop(predictor)\n    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n    ds = ds.shuffle(buffer_size=len(df))\n    ds = ds.batch(batch_size)\n    return ds", "fn_id": 2, "class_fn": false, "repo": "phthaloc/nd320-c1-emr-data-project", "file": "utils.py", "last_update_at": "2020-11-21T13:12:56+00:00", "question_id": "e1fcc40719c6de647b5c2d78a3453d599b57a0c4_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def df_to_dataset(df, predictor, batch_size=32):\n    df = df.copy()\n    labels = df.pop(predictor)\n    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n    ds = ds.shuffle(buffer_size=len(df))\n    ds = ds.batch(batch_size)\n"]]}
{"hexsha": "1049046d27bf69fb8f572aeb210b4d9a4a45cca2", "ext": "py", "lang": "Python", "content": "def has_body(name):\n    try:\n        body_from_name(name)\n    except ValueError:\n        return False\n    return True", "fn_id": 1, "class_fn": false, "repo": "anthonyn2121/manipulator_sim", "file": "collision_utils.py", "last_update_at": "2020-11-17T07:41:43+00:00", "question_id": "1049046d27bf69fb8f572aeb210b4d9a4a45cca2_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def has_body(name):\n    try:\n        body_from_name(name)\n    except ValueError:\n        return False\n"]]}
{"hexsha": "18f5db71e32cae282455bb290f90114661bab366", "ext": "py", "lang": "Python", "content": "def main():\n    fields = {\n        \"host\": {\"required\": True, \"type\": \"str\"},\n        \"username\": {\"required\": True, \"type\": \"str\"},\n        \"password\": {\"required\": False, \"type\": \"str\", \"no_log\": True},\n        \"vdom\": {\"required\": False, \"type\": \"str\", \"default\": \"root\"},\n        \"https\": {\"required\": False, \"type\": \"bool\", \"default\": True},\n        \"system_dhcp_server\": {\n            \"required\": False, \"type\": \"dict\",\n            \"options\": {\n                \"state\": {\"required\": True, \"type\": \"str\",\n                          \"choices\": [\"present\", \"absent\"]},\n                \"auto-configuration\": {\"required\": False, \"type\": \"str\",\n                                       \"choices\": [\"disable\", \"enable\"]},\n                \"conflicted-ip-timeout\": {\"required\": False, \"type\": \"int\"},\n                \"ddns-auth\": {\"required\": False, \"type\": \"str\",\n                              \"choices\": [\"disable\", \"tsig\"]},\n                \"ddns-key\": {\"required\": False, \"type\": \"str\"},\n                \"ddns-keyname\": {\"required\": False, \"type\": \"str\"},\n                \"ddns-server-ip\": {\"required\": False, \"type\": \"str\"},\n                \"ddns-ttl\": {\"required\": False, \"type\": \"int\"},\n                \"ddns-update\": {\"required\": False, \"type\": \"str\",\n                                \"choices\": [\"disable\", \"enable\"]},\n                \"ddns-update-override\": {\"required\": False, \"type\": \"str\",\n                                         \"choices\": [\"disable\", \"enable\"]},\n                \"ddns-zone\": {\"required\": False, \"type\": \"str\"},\n                \"default-gateway\": {\"required\": False, \"type\": \"str\"},\n                \"dns-server1\": {\"required\": False, \"type\": \"str\"},\n                \"dns-server2\": {\"required\": False, \"type\": \"str\"},\n                \"dns-server3\": {\"required\": False, \"type\": \"str\"},\n                \"dns-service\": {\"required\": False, \"type\": \"str\",\n                                \"choices\": [\"local\", \"default\", \"specify\"]},\n                \"domain\": {\"required\": False, \"type\": \"str\"},\n                \"exclude-range\": {\"required\": False, \"type\": \"list\",\n                                  \"options\": {\n                                      \"end-ip\": {\"required\": False, \"type\": \"str\"},\n                                      \"id\": {\"required\": True, \"type\": \"int\"},\n                                      \"start-ip\": {\"required\": False, \"type\": \"str\"}\n                                  }},\n                \"filename\": {\"required\": False, \"type\": \"str\"},\n                \"forticlient-on-net-status\": {\"required\": False, \"type\": \"str\",\n                                              \"choices\": [\"disable\", \"enable\"]},\n                \"id\": {\"required\": True, \"type\": \"int\"},\n                \"interface\": {\"required\": False, \"type\": \"str\"},\n                \"ip-mode\": {\"required\": False, \"type\": \"str\",\n                            \"choices\": [\"range\", \"usrgrp\"]},\n                \"ip-range\": {\"required\": False, \"type\": \"list\",\n                             \"options\": {\n                                 \"end-ip\": {\"required\": False, \"type\": \"str\"},\n                                 \"id\": {\"required\": True, \"type\": \"int\"},\n                                 \"start-ip\": {\"required\": False, \"type\": \"str\"}\n                             }},\n                \"ipsec-lease-hold\": {\"required\": False, \"type\": \"int\"},\n                \"lease-time\": {\"required\": False, \"type\": \"int\"},\n                \"mac-acl-default-action\": {\"required\": False, \"type\": \"str\",\n                                           \"choices\": [\"assign\", \"block\"]},\n                \"netmask\": {\"required\": False, \"type\": \"str\"},\n                \"next-server\": {\"required\": False, \"type\": \"str\"},\n                \"ntp-server1\": {\"required\": False, \"type\": \"str\"},\n                \"ntp-server2\": {\"required\": False, \"type\": \"str\"},\n                \"ntp-server3\": {\"required\": False, \"type\": \"str\"},\n                \"ntp-service\": {\"required\": False, \"type\": \"str\",\n                                \"choices\": [\"local\", \"default\", \"specify\"]},\n                \"options\": {\"required\": False, \"type\": \"list\",\n                            \"options\": {\n                                \"code\": {\"required\": False, \"type\": \"int\"},\n                                \"id\": {\"required\": True, \"type\": \"int\"},\n                                \"ip\": {\"required\": False, \"type\": \"str\"},\n                                \"type\": {\"required\": False, \"type\": \"str\",\n                                         \"choices\": [\"hex\", \"string\", \"ip\"]},\n                                \"value\": {\"required\": False, \"type\": \"str\"}\n                            }},\n                \"reserved-address\": {\"required\": False, \"type\": \"list\",\n                                     \"options\": {\n                                         \"action\": {\"required\": False, \"type\": \"str\",\n                                                    \"choices\": [\"assign\", \"block\", \"reserved\"]},\n                                         \"description\": {\"required\": False, \"type\": \"str\"},\n                                         \"id\": {\"required\": True, \"type\": \"int\"},\n                                         \"ip\": {\"required\": False, \"type\": \"str\"},\n                                         \"mac\": {\"required\": False, \"type\": \"str\"}\n                                     }},\n                \"server-type\": {\"required\": False, \"type\": \"str\",\n                                \"choices\": [\"regular\", \"ipsec\"]},\n                \"status\": {\"required\": False, \"type\": \"str\",\n                           \"choices\": [\"disable\", \"enable\"]},\n                \"tftp-server\": {\"required\": False, \"type\": \"list\",\n                                \"options\": {\n                                    \"tftp-server\": {\"required\": True, \"type\": \"str\"}\n                                }},\n                \"timezone\": {\"required\": False, \"type\": \"str\",\n                             \"choices\": [\"01\", \"02\", \"03\",\n                                         \"04\", \"05\", \"81\",\n                                         \"06\", \"07\", \"08\",\n                                         \"09\", \"10\", \"11\",\n                                         \"12\", \"13\", \"74\",\n                                         \"14\", \"77\", \"15\",\n                                         \"87\", \"16\", \"17\",\n                                         \"18\", \"19\", \"20\",\n                                         \"75\", \"21\", \"22\",\n                                         \"23\", \"24\", \"80\",\n                                         \"79\", \"25\", \"26\",\n                                         \"27\", \"28\", \"78\",\n                                         \"29\", \"30\", \"31\",\n                                         \"32\", \"33\", \"34\",\n                                         \"35\", \"36\", \"37\",\n                                         \"38\", \"83\", \"84\",\n                                         \"40\", \"85\", \"41\",\n                                         \"42\", \"43\", \"39\",\n                                         \"44\", \"46\", \"47\",\n                                         \"51\", \"48\", \"45\",\n                                         \"49\", \"50\", \"52\",\n                                         \"53\", \"54\", \"55\",\n                                         \"56\", \"57\", \"58\",\n                                         \"59\", \"60\", \"62\",\n                                         \"63\", \"61\", \"64\",\n                                         \"65\", \"66\", \"67\",\n                                         \"68\", \"69\", \"70\",\n                                         \"71\", \"72\", \"00\",\n                                         \"82\", \"73\", \"86\",\n                                         \"76\"]},\n                \"timezone-option\": {\"required\": False, \"type\": \"str\",\n                                    \"choices\": [\"disable\", \"default\", \"specify\"]},\n                \"vci-match\": {\"required\": False, \"type\": \"str\",\n                              \"choices\": [\"disable\", \"enable\"]},\n                \"vci-string\": {\"required\": False, \"type\": \"list\",\n                               \"options\": {\n                                   \"vci-string\": {\"required\": True, \"type\": \"str\"}\n                               }},\n                \"wifi-ac1\": {\"required\": False, \"type\": \"str\"},\n                \"wifi-ac2\": {\"required\": False, \"type\": \"str\"},\n                \"wifi-ac3\": {\"required\": False, \"type\": \"str\"},\n                \"wins-server1\": {\"required\": False, \"type\": \"str\"},\n                \"wins-server2\": {\"required\": False, \"type\": \"str\"}\n\n            }\n        }\n    }\n\n    module = AnsibleModule(argument_spec=fields,\n                           supports_check_mode=False)\n    try:\n        from fortiosapi import FortiOSAPI\n    except ImportError:\n        module.fail_json(msg=\"fortiosapi module is required\")\n\n    global fos\n    fos = FortiOSAPI()\n\n    is_error, has_changed, result = fortios_system_dhcp(module.params, fos)\n\n    if not is_error:\n        module.exit_json(changed=has_changed, meta=result)\n    else:\n        module.fail_json(msg=\"Error in repo\", meta=result)", "fn_id": 5, "class_fn": false, "repo": "lakhlaifi/RedHat-Ansible", "file": "virt/ansible-latest/lib/python2.7/site-packages/ansible/modules/network/fortios/fortios_system_dhcp_server.py", "last_update_at": "2020-03-22T01:04:39+00:00", "question_id": "18f5db71e32cae282455bb290f90114661bab366_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    fields = {'host': {'required': True, 'type': 'str'}, 'username': {'required': True, 'type': 'str'}, 'password': {'required': False, 'type': 'str', 'no_log': True}, 'vdom': {'required': False, 'type': 'str', 'default': 'root'}, 'https': {'required': False, 'type': 'bool', 'default': True}, 'system_dhcp_server': {'required': False, 'type': 'dict', 'options': {'state': {'required': True, 'type': 'str', 'choices': ['present', 'absent']}, 'auto-configuration': {'required': False, 'type': 'str', 'choices': ['disable', 'enable']}, 'conflicted-ip-timeout': {'required': False, 'type': 'int'}, 'ddns-auth': {'required': False, 'type': 'str', 'choices': ['disable', 'tsig']}, 'ddns-key': {'required': False, 'type': 'str'}, 'ddns-keyname': {'required': False, 'type': 'str'}, 'ddns-server-ip': {'required': False, 'type': 'str'}, 'ddns-ttl': {'required': False, 'type': 'int'}, 'ddns-update': {'required': False, 'type': 'str', 'choices': ['disable', 'enable']}, 'ddns-update-override': {'required': False, 'type': 'str', 'choices': ['disable', 'enable']}, 'ddns-zone': {'required': False, 'type': 'str'}, 'default-gateway': {'required': False, 'type': 'str'}, 'dns-server1': {'required': False, 'type': 'str'}, 'dns-server2': {'required': False, 'type': 'str'}, 'dns-server3': {'required': False, 'type': 'str'}, 'dns-service': {'required': False, 'type': 'str', 'choices': ['local', 'default', 'specify']}, 'domain': {'required': False, 'type': 'str'}, 'exclude-range': {'required': False, 'type': 'list', 'options': {'end-ip': {'required': False, 'type': 'str'}, 'id': {'required': True, 'type': 'int'}, 'start-ip': {'required': False, 'type': 'str'}}}, 'filename': {'required': False, 'type': 'str'}, 'forticlient-on-net-status': {'required': False, 'type': 'str', 'choices': ['disable', 'enable']}, 'id': {'required': True, 'type': 'int'}, 'interface': {'required': False, 'type': 'str'}, 'ip-mode': {'required': False, 'type': 'str', 'choices': ['range', 'usrgrp']}, 'ip-range': {'required': False, 'type': 'list', 'options': {'end-ip': {'required': False, 'type': 'str'}, 'id': {'required': True, 'type': 'int'}, 'start-ip': {'required': False, 'type': 'str'}}}, 'ipsec-lease-hold': {'required': False, 'type': 'int'}, 'lease-time': {'required': False, 'type': 'int'}, 'mac-acl-default-action': {'required': False, 'type': 'str', 'choices': ['assign', 'block']}, 'netmask': {'required': False, 'type': 'str'}, 'next-server': {'required': False, 'type': 'str'}, 'ntp-server1': {'required': False, 'type': 'str'}, 'ntp-server2': {'required': False, 'type': 'str'}, 'ntp-server3': {'required': False, 'type': 'str'}, 'ntp-service': {'required': False, 'type': 'str', 'choices': ['local', 'default', 'specify']}, 'options': {'required': False, 'type': 'list', 'options': {'code': {'required': False, 'type': 'int'}, 'id': {'required': True, 'type': 'int'}, 'ip': {'required': False, 'type': 'str'}, 'type': {'required': False, 'type': 'str', 'choices': ['hex', 'string', 'ip']}, 'value': {'required': False, 'type': 'str'}}}, 'reserved-address': {'required': False, 'type': 'list', 'options': {'action': {'required': False, 'type': 'str', 'choices': ['assign', 'block', 'reserved']}, 'description': {'required': False, 'type': 'str'}, 'id': {'required': True, 'type': 'int'}, 'ip': {'required': False, 'type': 'str'}, 'mac': {'required': False, 'type': 'str'}}}, 'server-type': {'required': False, 'type': 'str', 'choices': ['regular', 'ipsec']}, 'status': {'required': False, 'type': 'str', 'choices': ['disable', 'enable']}, 'tftp-server': {'required': False, 'type': 'list', 'options': {'tftp-server': {'required': True, 'type': 'str'}}}, 'timezone': {'required': False, 'type': 'str', 'choices': ['01', '02', '03', '04', '05', '81', '06', '07', '08', '09', '10', '11', '12', '13', '74', '14', '77', '15', '87', '16', '17', '18', '19', '20', '75', '21', '22', '23', '24', '80', '79', '25', '26', '27', '28', '78', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '83', '84', '40', '85', '41', '42', '43', '39', '44', '46', '47', '51', '48', '45', '49', '50', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '63', '61', '64', '65', '66', '67', '68', '69', '70', '71', '72', '00', '82', '73', '86', '76']}, 'timezone-option': {'required': False, 'type': 'str', 'choices': ['disable', 'default', 'specify']}, 'vci-match': {'required': False, 'type': 'str', 'choices': ['disable', 'enable']}, 'vci-string': {'required': False, 'type': 'list', 'options': {'vci-string': {'required': True, 'type': 'str'}}}, 'wifi-ac1': {'required': False, 'type': 'str'}, 'wifi-ac2': {'required': False, 'type': 'str'}, 'wifi-ac3': {'required': False, 'type': 'str'}, 'wins-server1': {'required': False, 'type': 'str'}, 'wins-server2': {'required': False, 'type': 'str'}}}}\n    module = AnsibleModule(argument_spec=fields, supports_check_mode=False)\n    try:\n        from fortiosapi import FortiOSAPI\n    except ImportError:\n        module.fail_json(msg='fortiosapi module is required')\n    global fos\n    fos = FortiOSAPI()\n    is_error, has_changed, result = fortios_system_dhcp(module.params, fos)\n    if not is_error:\n        module.exit_json(changed=has_changed, meta=result)\n    else:\n"]]}
{"hexsha": "55e32f3c41a24b89029d05342314cf2f54db12f5", "ext": "py", "lang": "Python", "content": "def get_data(req):\n\n    # humens = Humen.objects.filter(money__gt=10050)\n    # avg_age = humens.aggregate(Avg('age'))\n    # return  HttpResponse(avg_age.get('age__avg'))\n\n    humens = Humen.objects.filter(money__gt=10050)\n    avg_age = humens.aggregate(Sum('age'))\n    return  HttpResponse(avg_age.get('age__sum'))", "fn_id": 0, "class_fn": false, "repo": "General-ITer/Django-Introduction", "file": "day03/app03/views.py", "last_update_at": "2020-12-09T18:26:36+00:00", "question_id": "55e32f3c41a24b89029d05342314cf2f54db12f5_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_data(req):\n    humens = Humen.objects.filter(money__gt=10050)\n    avg_age = humens.aggregate(Sum('age'))\n"]]}
{"hexsha": "d9da01dae503bd93e661ea5caa90fa1d369938c4", "ext": "py", "lang": "Python", "content": "def get_namespace_to_patch(namespace_target: NamespaceTarget) -> object:\n    if namespace_target == NamespaceTarget.TORCH_NN_FUNCTIONAL:\n        return torch.nn.functional\n    if namespace_target == NamespaceTarget.TORCH_TENSOR:\n        return TracedTensor\n    if namespace_target == NamespaceTarget.TORCH:\n        return torch\n    raise RuntimeError(\"{} namespace wasn't found in {}\".format(namespace_target, NamespaceTarget))", "fn_id": 0, "class_fn": false, "repo": "MaximProshin/nncf", "file": "nncf/torch/dynamic_graph/patch_pytorch.py", "last_update_at": "2020-10-28T06:10:50+00:00", "question_id": "d9da01dae503bd93e661ea5caa90fa1d369938c4_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_namespace_to_patch(namespace_target: NamespaceTarget) -> object:\n    if namespace_target == NamespaceTarget.TORCH_NN_FUNCTIONAL:\n        return torch.nn.functional\n    if namespace_target == NamespaceTarget.TORCH_TENSOR:\n        return TracedTensor\n    if namespace_target == NamespaceTarget.TORCH:\n        return torch\n"]]}
{"hexsha": "afba808c2a3b5e03497dcb564c6ed1b603da8646", "ext": "py", "lang": "Python", "content": "def main():\n    args = parser.parse_args()\n\n    # Fetch the image glob\n    image_filenames = sorted(glob.glob(args.image_glob))\n\n    # Parse original info from the experiment root and add new ones.\n    args_file = os.path.join(args.experiment_root, 'args.json')\n    if not os.path.isfile(args_file):\n        raise IOError('`args.json` not found in {}'.format(args_file))\n    print('Loading args from {}.'.format(args_file))\n    with open(args_file, 'r') as f:\n        args_resumed = json.load(f)\n    for key, value in args_resumed.items():\n        if key not in args.__dict__:\n            args.__dict__[key] = value\n\n    # In case no dataset config was specified, we need to fix the argument here\n    # since there will be a list of configs.\n    if args.dataset_config is None:\n        args.dataset_config = args_resumed['dataset_config'][0]\n\n    # Load the config for the dataset.\n    with open(args.dataset_config, 'r') as f:\n        dataset_config = json.load(f)\n\n    # Compute the label to color map\n    id_to_rgb = np.asarray(\n        dataset_config['rgb_colors'] + [(0, 0, 0)], dtype=np.uint8)[:,::-1]\n\n    # Setup the input\n    image_file_tensor = tf.data.Dataset.from_tensor_slices(image_filenames)\n\n    dataset = image_file_tensor.map(\n        lambda fn: tf.image.decode_png(tf.read_file(fn), channels=3))\n\n    dataset = dataset.map(lambda x: ((tf.to_float(x) - 128.0) / 128.0))\n\n    dataset = tf.data.Dataset.zip((dataset, image_file_tensor))\n\n    dataset = dataset.batch(1)\n\n    dataset = dataset.prefetch(1)\n\n    image_input, image_filename = dataset.make_one_shot_iterator().get_next()\n\n    if args.rescale_h is not None or args.rescale_w is not None:\n        if args.rescale_h is not None and args.rescale_w is not None:\n            image_input_resized = tf.image.resize_images(\n                image_input, (args.rescale_h, args.rescale_w))\n        else:\n            raise ValueError('Either both rescale_h and rescale_w should be '\n                             'left undefined or both should be set. Got {} and '\n                             '{}'.format(args.rescale_h, args.rescale_w))\n    else:\n        image_input_resized = image_input\n\n    # Determine the checkpoint location.\n    if args.checkpoint_iteration == -1:\n        # The default TF way to do this fails when moving folders.\n        checkpoint = os.path.join(\n            args.experiment_root,\n            'checkpoint-{}'.format(args.train_iterations))\n    else:\n        checkpoint = os.path.join(\n            args.experiment_root,\n            'checkpoint-{}'.format(args.checkpoint_iteration))\n    iteration = int(checkpoint.split('-')[-1])\n    print('Restoring from checkpoint: {}'.format(checkpoint))\n\n    # Check if the checkpoint contains a specifically named output_conv. This is\n    # needed for models trained with the older single dataset code.\n    reader = tf.train.NewCheckpointReader(checkpoint)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n    output_conv_name = 'output_conv_{}'.format(\n        dataset_config.get('dataset_name'))\n    output_conv_name_found = False\n\n    for k in var_to_shape_map.keys():\n        output_conv_name_found = output_conv_name in k\n        if output_conv_name_found:\n            break\n\n    if not output_conv_name_found:\n        print('Warning: An output for the specific dataset could not be found '\n              'in the checkpoint. This likely means it\\'s an old checkpoint. '\n              'Revertig to the old default output name. This could cause '\n              'issues if the dataset class count and output classes of the '\n              'network do not match.')\n        output_conv_name = 'output_conv'\n\n    # Setup the network for simple forward passing.\n    model = import_module('networks.' + args.model_type)\n    with tf.name_scope('model'):\n        net = model.network(image_input_resized, is_training=False,\n            **args.model_params)\n        logits = slim.conv2d(net, len(dataset_config['class_names']),\n            [3,3], scope=output_conv_name, activation_fn=None,\n            weights_initializer=slim.variance_scaling_initializer(),\n            biases_initializer=tf.zeros_initializer())\n        predictions = tf.nn.softmax(logits)\n\n        predictions_full = tf.image.resize_images(\n            predictions, tf.shape(image_input)[1:3])\n\n    with tf.Session() as sess:\n        checkpoint_loader = tf.train.Saver()\n        checkpoint_loader.restore(sess, checkpoint)\n\n        # Loop over all images\n        timings = []\n        print()\n        while True:\n            try:\n                start = time.time()\n                preds, fn = sess.run([predictions_full, image_filename])\n                timings.append(time.time() - start)\n                pred_class = np.argmax(preds[0], -1)\n                pred_out = id_to_rgb[pred_class]\n                if len(timings) > 1:\n                    print('Time for loading, resizing and forwarding per frame:'\n                          ' {:7.4f}s\u00b1{:7.4f}s'.format(\n                                np.mean(timings[-100:]),\n                                np.std(timings[-100:])),\n                          end='\\r')\n                in_filename = fn[0].decode(\"utf-8\")\n                base_dir = os.path.dirname(in_filename)\n                out_filename = in_filename.replace(\n                    base_dir, args.result_directory)\n                extension = os.path.splitext(out_filename)[1]\n                out_filename = out_filename.replace(extension, '.png')\n                if not os.path.isdir(args.result_directory):\n                    os.makedirs(args.result_directory)\n                cv2.imwrite(out_filename, pred_out)\n\n            except tf.errors.OutOfRangeError:\n                # Done!\n                break\n\n    # For the timings we skip the first frame since this is where Tensorflow\n    # hides the compilation time.\n    if len(timings) > 1:\n        timings = timings[-100:]\n        print('Time for loading, resizing and forwarding per frame: '\n              '{:7.4f}s\u00b1{:7.4f}s'.format(np.mean(timings), np.std(timings)))\n    else:\n        print('Loading and forwarding took {:7.4f}s. '\n              'This includes compilation'.format(timings[0]))", "fn_id": 0, "class_fn": false, "repo": "VisualComputingInstitute/PARIS-sem-seg", "file": "predict.py", "last_update_at": "2020-07-16T08:36:16+00:00", "question_id": "afba808c2a3b5e03497dcb564c6ed1b603da8646_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    args = parser.parse_args()\n    image_filenames = sorted(glob.glob(args.image_glob))\n    args_file = os.path.join(args.experiment_root, 'args.json')\n    if not os.path.isfile(args_file):\n        raise IOError('`args.json` not found in {}'.format(args_file))\n    print('Loading args from {}.'.format(args_file))\n    with open(args_file, 'r') as f:\n        args_resumed = json.load(f)\n    for key, value in args_resumed.items():\n        if key not in args.__dict__:\n            args.__dict__[key] = value\n    if args.dataset_config is None:\n        args.dataset_config = args_resumed['dataset_config'][0]\n    with open(args.dataset_config, 'r') as f:\n        dataset_config = json.load(f)\n    id_to_rgb = np.asarray(dataset_config['rgb_colors'] + [(0, 0, 0)], dtype=np.uint8)[:, ::-1]\n    image_file_tensor = tf.data.Dataset.from_tensor_slices(image_filenames)\n    dataset = image_file_tensor.map(lambda fn: tf.image.decode_png(tf.read_file(fn), channels=3))\n    dataset = dataset.map(lambda x: (tf.to_float(x) - 128.0) / 128.0)\n    dataset = tf.data.Dataset.zip((dataset, image_file_tensor))\n    dataset = dataset.batch(1)\n    dataset = dataset.prefetch(1)\n    image_input, image_filename = dataset.make_one_shot_iterator().get_next()\n    if args.rescale_h is not None or args.rescale_w is not None:\n        if args.rescale_h is not None and args.rescale_w is not None:\n            image_input_resized = tf.image.resize_images(image_input, (args.rescale_h, args.rescale_w))\n        else:\n            raise ValueError('Either both rescale_h and rescale_w should be left undefined or both should be set. Got {} and {}'.format(args.rescale_h, args.rescale_w))\n    else:\n        image_input_resized = image_input\n    if args.checkpoint_iteration == -1:\n        checkpoint = os.path.join(args.experiment_root, 'checkpoint-{}'.format(args.train_iterations))\n    else:\n        checkpoint = os.path.join(args.experiment_root, 'checkpoint-{}'.format(args.checkpoint_iteration))\n    iteration = int(checkpoint.split('-')[-1])\n    print('Restoring from checkpoint: {}'.format(checkpoint))\n    reader = tf.train.NewCheckpointReader(checkpoint)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n    output_conv_name = 'output_conv_{}'.format(dataset_config.get('dataset_name'))\n    output_conv_name_found = False\n    for k in var_to_shape_map.keys():\n        output_conv_name_found = output_conv_name in k\n        if output_conv_name_found:\n            break\n    if not output_conv_name_found:\n        print(\"Warning: An output for the specific dataset could not be found in the checkpoint. This likely means it's an old checkpoint. Revertig to the old default output name. This could cause issues if the dataset class count and output classes of the network do not match.\")\n        output_conv_name = 'output_conv'\n    model = import_module('networks.' + args.model_type)\n    with tf.name_scope('model'):\n        net = model.network(image_input_resized, is_training=False, **args.model_params)\n        logits = slim.conv2d(net, len(dataset_config['class_names']), [3, 3], scope=output_conv_name, activation_fn=None, weights_initializer=slim.variance_scaling_initializer(), biases_initializer=tf.zeros_initializer())\n        predictions = tf.nn.softmax(logits)\n        predictions_full = tf.image.resize_images(predictions, tf.shape(image_input)[1:3])\n    with tf.Session() as sess:\n        checkpoint_loader = tf.train.Saver()\n        checkpoint_loader.restore(sess, checkpoint)\n        timings = []\n        print()\n        while True:\n            try:\n                start = time.time()\n                preds, fn = sess.run([predictions_full, image_filename])\n                timings.append(time.time() - start)\n                pred_class = np.argmax(preds[0], -1)\n                pred_out = id_to_rgb[pred_class]\n                if len(timings) > 1:\n                    print('Time for loading, resizing and forwarding per frame: {:7.4f}s\u00b1{:7.4f}s'.format(np.mean(timings[-100:]), np.std(timings[-100:])), end='\\r')\n                in_filename = fn[0].decode('utf-8')\n                base_dir = os.path.dirname(in_filename)\n                out_filename = in_filename.replace(base_dir, args.result_directory)\n                extension = os.path.splitext(out_filename)[1]\n                out_filename = out_filename.replace(extension, '.png')\n                if not os.path.isdir(args.result_directory):\n                    os.makedirs(args.result_directory)\n                cv2.imwrite(out_filename, pred_out)\n            except tf.errors.OutOfRangeError:\n                break\n    if len(timings) > 1:\n        timings = timings[-100:]\n        print('Time for loading, resizing and forwarding per frame: {:7.4f}s\u00b1{:7.4f}s'.format(np.mean(timings), np.std(timings)))\n    else:\n"]]}
{"hexsha": "c4c34a0832f4b7fc813d99283c4adf2da0eecb2b", "ext": "py", "lang": "Python", "content": "@pytest.mark.asyncio\nasync def test_os(redis_conn, small_graph):\n    subjects = await graph.os(redis_conn, small_graph[\"b\"])\n    assert len(subjects) == 2\n    assert small_graph[\"a\"] in subjects\n    assert small_graph[\"d\"] in subjects", "fn_id": 5, "class_fn": false, "repo": "mildewey/redgraph", "file": "tests/integration/test_graph.py", "last_update_at": "2020-09-05T01:51:33+00:00", "question_id": "c4c34a0832f4b7fc813d99283c4adf2da0eecb2b_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.asyncio\nasync def test_os(redis_conn, small_graph):\n    subjects = await graph.os(redis_conn, small_graph['b'])\n    assert len(subjects) == 2\n    assert small_graph['a'] in subjects\n"]]}
{"hexsha": "3a47b724c63d217caa739ccda16a1fc5ef189af3", "ext": "py", "lang": "Python", "content": "@cocotb.test(skip = False)\ndef write_char_test(dut):\n    \"\"\"\n    Description:\n\n    Test ID: 7\n\n    Expected Results:\n        **\n    \"\"\"\n    dut.rst <= 1\n    dut.test_id <= 7\n    axim = AXI4LiteMaster(dut, \"AXIML\", dut.clk)\n    video_in = AXI4StreamSlave(dut, \"AXISS\", dut.clk, width=24)\n\n    setup_dut(dut)\n    yield Timer(CLK_PERIOD * 10)\n    dut.rst <= 0\n    yield Timer(CLK_PERIOD * 10)\n    dut.log.info(\"Ready\")\n    yield Timer(CLK_PERIOD * 300)\n\n    control = 0x00\n    control |= 1 << BIT_CTRL_EN\n    yield axim.write(REG_CONTROL, control)\n    yield Timer(CLK_PERIOD * 10)\n\n    #Write a characer down\n    char_val = 0x0101\n    yield axim.write(REG_CONSOLE_CHAR, char_val)\n    yield Timer(CLK_PERIOD * 10)", "fn_id": 8, "class_fn": false, "repo": "CospanDesign/nysa-verilog", "file": "verilog/axi/slave/axi_on_screen_display/cocotb/test_dut.py", "last_update_at": "2020-11-26T15:19:24+00:00", "question_id": "3a47b724c63d217caa739ccda16a1fc5ef189af3_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@cocotb.test(skip=False)\ndef write_char_test(dut):\n    \"\"\"\n    Description:\n\n    Test ID: 7\n\n    Expected Results:\n        **\n    \"\"\"\n    dut.rst <= 1\n    dut.test_id <= 7\n    axim = AXI4LiteMaster(dut, 'AXIML', dut.clk)\n    video_in = AXI4StreamSlave(dut, 'AXISS', dut.clk, width=24)\n    setup_dut(dut)\n    yield Timer(CLK_PERIOD * 10)\n    dut.rst <= 0\n    yield Timer(CLK_PERIOD * 10)\n    dut.log.info('Ready')\n    yield Timer(CLK_PERIOD * 300)\n    control = 0\n    control |= 1 << BIT_CTRL_EN\n    yield axim.write(REG_CONTROL, control)\n    yield Timer(CLK_PERIOD * 10)\n    char_val = 257\n    yield axim.write(REG_CONSOLE_CHAR, char_val)\n"]]}
{"hexsha": "c4e262116e7f8784cfb7bfd6854ce533f7849843", "ext": "py", "lang": "Python", "content": "def appendToProjects():\n    global projects\n    state = State()\n\n    sort = usfm_verses.verseCounts[state.ID][\"sort\"]\n    testament = 'nt'\n    if sort < 40:\n        testament = 'ot'\n    project = { \"title\": state.toc2, \"id\": state.ID.lower(), \"sort\": sort, \\\n                \"path\": \"./\" + makeUsfmFilename(state.ID), \\\n                \"categories\": \"[ 'bible-\" + testament + \"' ]\" }\n    projects.append(project)", "fn_id": 7, "class_fn": false, "repo": "unfoldingWord-dev/tools", "file": "usfm/alignment2rc.py", "last_update_at": "2020-06-25T14:32:35+00:00", "question_id": "c4e262116e7f8784cfb7bfd6854ce533f7849843_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def appendToProjects():\n    global projects\n    state = State()\n    sort = usfm_verses.verseCounts[state.ID]['sort']\n    testament = 'nt'\n    if sort < 40:\n        testament = 'ot'\n    project = {'title': state.toc2, 'id': state.ID.lower(), 'sort': sort, 'path': './' + makeUsfmFilename(state.ID), 'categories': \"[ 'bible-\" + testament + \"' ]\"}\n"]]}
{"hexsha": "e9e8e6f9f31ce619785842ccb9a485d745c27002", "ext": "py", "lang": "Python", "content": "@pytest.fixture\ndef csv_file_with_image(tmp_path, image_file):\n    filename = tmp_path / \"csv_with_image.csv\"\n    data = textwrap.dedent(\n        f\"\"\"\\\n        image\n        {image_file}\n        \"\"\"\n    )\n    with open(filename, \"w\") as f:\n        f.write(data)\n    return str(filename)", "fn_id": 2, "class_fn": false, "repo": "rpatil524/datasets", "file": "tests/packaged_modules/test_csv.py", "last_update_at": "2020-09-10T14:36:50+00:00", "question_id": "e9e8e6f9f31ce619785842ccb9a485d745c27002_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.fixture\ndef csv_file_with_image(tmp_path, image_file):\n    filename = tmp_path / 'csv_with_image.csv'\n    data = textwrap.dedent(f'        image\\n        {image_file}\\n        ')\n    with open(filename, 'w') as f:\n        f.write(data)\n"]]}
{"hexsha": "f75e85b6be66063f5f251a3924187e4cef4564ab", "ext": "py", "lang": "Python", "content": "def createVirtualEnv():\n    print(f\"{Text.HEADER}*** CREATING VIRTUAL ENVIRONMENT ***{Text.ENDC}\")\n    os.chdir('PiirBlaster')\n    cmdResult = execCommand(Commands.CREATE_VRITUAL_ENV)\n    if cmdResult != 0:\n        print(f\"{Text.FAIL}CREATING VIRTUAL ENVIRONEMENT FAILED!!!{Text.ENDC}\")\n        return False\n    print(f\"{Text.SUCCESS}CREATING VIRTUAL ENVIRONMENT DONE{Text.ENDC}\")\n    return True", "fn_id": 11, "class_fn": false, "repo": "Electronya/PirBlaster", "file": "scripts/install/PiirBlaster_install.py", "last_update_at": "2020-11-02T09:37:43+00:00", "question_id": "f75e85b6be66063f5f251a3924187e4cef4564ab_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def createVirtualEnv():\n    print(f'{Text.HEADER}*** CREATING VIRTUAL ENVIRONMENT ***{Text.ENDC}')\n    os.chdir('PiirBlaster')\n    cmdResult = execCommand(Commands.CREATE_VRITUAL_ENV)\n    if cmdResult != 0:\n        print(f'{Text.FAIL}CREATING VIRTUAL ENVIRONEMENT FAILED!!!{Text.ENDC}')\n        return False\n    print(f'{Text.SUCCESS}CREATING VIRTUAL ENVIRONMENT DONE{Text.ENDC}')\n"]]}
{"hexsha": "d829f5ac7f3264f41e3c6bc6f67d722b5f56d2d1", "ext": "py", "lang": "Python", "content": "def exp_firstrun(tas):\n    def func(group):\n        deb = rl.first_run(group.where(group.time.dt.month < 7) > thresh, window, 'time')\n        fin = rl.first_run(group.where(group.time.dt.month >= 7) < thresh, window, 'time')\n        return fin - deb\n\n    return tas.resample(time='YS').apply(func)", "fn_id": 2, "class_fn": false, "repo": "Ouranosinc/xclim-benchmark", "file": "scripts/bench_gsl.py", "last_update_at": "2020-06-05T14:42:41+00:00", "question_id": "d829f5ac7f3264f41e3c6bc6f67d722b5f56d2d1_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def exp_firstrun(tas):\n\n    def func(group):\n        deb = rl.first_run(group.where(group.time.dt.month < 7) > thresh, window, 'time')\n        fin = rl.first_run(group.where(group.time.dt.month >= 7) < thresh, window, 'time')\n        return fin - deb\n"]]}
{"hexsha": "d9c423c56af9c8224d38bf22c0671b06cd27bee9", "ext": "py", "lang": "Python", "content": "def ParseNolintSuppressions(filename, raw_line, linenum, error):\n  \"\"\"Updates the global list of line error-suppressions.\n\n  Parses any NOLINT comments on the current line, updating the global\n  error_suppressions store.  Reports an error if the NOLINT comment\n  was malformed.\n\n  Args:\n    filename: str, the name of the input file.\n    raw_line: str, the line of input text, with comments.\n    linenum: int, the number of the current line.\n    error: function, an error handler.\n  \"\"\"\n  matched = Search(r'\\bNOLINT(NEXTLINE)?\\b(\\([^)]+\\))?', raw_line)\n  if matched:\n    if matched.group(1):\n      suppressed_line = linenum + 1\n    else:\n      suppressed_line = linenum\n    category = matched.group(2)\n    if category in (None, '(*)'):  # => \"suppress all\"\n      _error_suppressions.setdefault(None, set()).add(suppressed_line)\n    else:\n      if category.startswith('(') and category.endswith(')'):\n        category = category[1:-1]\n        if category in _ERROR_CATEGORIES:\n          _error_suppressions.setdefault(category, set()).add(suppressed_line)\n        elif category not in _LEGACY_ERROR_CATEGORIES:\n          error(filename, linenum, 'readability/nolint', 5,\n                'Unknown NOLINT error category: %s' % category)", "fn_id": 1, "class_fn": false, "repo": "hangbogu/SJSU-Dev2", "file": "tools/cpplint/cpplint.py", "last_update_at": "2020-07-04T18:20:44+00:00", "question_id": "d9c423c56af9c8224d38bf22c0671b06cd27bee9_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ParseNolintSuppressions(filename, raw_line, linenum, error):\n    \"\"\"Updates the global list of line error-suppressions.\n\n  Parses any NOLINT comments on the current line, updating the global\n  error_suppressions store.  Reports an error if the NOLINT comment\n  was malformed.\n\n  Args:\n    filename: str, the name of the input file.\n    raw_line: str, the line of input text, with comments.\n    linenum: int, the number of the current line.\n    error: function, an error handler.\n  \"\"\"\n    matched = Search('\\\\bNOLINT(NEXTLINE)?\\\\b(\\\\([^)]+\\\\))?', raw_line)\n    if matched:\n        if matched.group(1):\n            suppressed_line = linenum + 1\n        else:\n            suppressed_line = linenum\n        category = matched.group(2)\n        if category in (None, '(*)'):\n            _error_suppressions.setdefault(None, set()).add(suppressed_line)\n        elif category.startswith('(') and category.endswith(')'):\n            category = category[1:-1]\n            if category in _ERROR_CATEGORIES:\n                _error_suppressions.setdefault(category, set()).add(suppressed_line)\n            elif category not in _LEGACY_ERROR_CATEGORIES:\n"]]}
{"hexsha": "ccf044473bec01f2984b4d2acf41dc342e0e7bd6", "ext": "py", "lang": "Python", "content": "def parse_quote_page(xml: minidom.Document, start_tag: str, cats: list, title_tag=TITLE_TAG) -> list:\n    \"\"\"\n    Reads through the page with quotes and parses them out\n    @param xml: the xml node list to parse\n    @param start_tag: xml tag to start reading elements from\n    @param cats: category list for the quotes on this page\n    @param title_tag: tag to parse the page title from\n    @return: list of quote objects\n    \"\"\"\n\n    quote_area = xml.getElementsByTagName(start_tag)\n    title_elem = minidom.NodeList(xml.getElementsByTagName(title_tag))\n    author = title_elem.item(0).getAttribute('title')\n    page_data = quote_area[0].firstChild.data.split('\\n')\n\n    i = 0\n    quotes = []\n\n    for line in page_data:\n        # remove the denotation chars for a quote  {\\{citat\\ |\n        matches = re.match('\\* ([\\S ]+)', line) \\\n                  or re.match('# ([^\\']+)', line) \\\n                  or re.match('\\*([^*]+)', line) \\\n                  or re.match('\\{\\{citat(?:ion)?\\|([\\S ]+)', line)\n\n        # crap to remove in other pages/languages\n        # or re.match('# ([^\\']+)', line) or re.match('\\*([^*]+)', line)\n\n        if matches:\n            quotes.append(format_quote(quote_line=matches.group(1), quote_id=i, author=author, cats=cats))\n            i += 1\n        else:\n            matches = re.match('\\*\\*([^*]+)', line)\n\n            if matches:\n                last_quote = quotes.pop()\n                last_quote.ref = matches.group(1).strip()\\\n                    .replace('[', '').replace(']', '')  # .replace('\"', '\"\"')\n                quotes.append(last_quote)\n\n        if re.match('\\{\\{misattributed|\\{\\{disputed', line, re.IGNORECASE):\n            break  # don't care about getting anything in misattributed and below\n\n    return quotes", "fn_id": 5, "class_fn": false, "repo": "yareally/wikiquote-parser", "file": "wikiparser.py", "last_update_at": "2020-09-18T23:33:47+00:00", "question_id": "ccf044473bec01f2984b4d2acf41dc342e0e7bd6_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def parse_quote_page(xml: minidom.Document, start_tag: str, cats: list, title_tag=TITLE_TAG) -> list:\n    \"\"\"\n    Reads through the page with quotes and parses them out\n    @param xml: the xml node list to parse\n    @param start_tag: xml tag to start reading elements from\n    @param cats: category list for the quotes on this page\n    @param title_tag: tag to parse the page title from\n    @return: list of quote objects\n    \"\"\"\n    quote_area = xml.getElementsByTagName(start_tag)\n    title_elem = minidom.NodeList(xml.getElementsByTagName(title_tag))\n    author = title_elem.item(0).getAttribute('title')\n    page_data = quote_area[0].firstChild.data.split('\\n')\n    i = 0\n    quotes = []\n    for line in page_data:\n        matches = re.match('\\\\* ([\\\\S ]+)', line) or re.match(\"# ([^']+)\", line) or re.match('\\\\*([^*]+)', line) or re.match('\\\\{\\\\{citat(?:ion)?\\\\|([\\\\S ]+)', line)\n        if matches:\n            quotes.append(format_quote(quote_line=matches.group(1), quote_id=i, author=author, cats=cats))\n            i += 1\n        else:\n            matches = re.match('\\\\*\\\\*([^*]+)', line)\n            if matches:\n                last_quote = quotes.pop()\n                last_quote.ref = matches.group(1).strip().replace('[', '').replace(']', '')\n                quotes.append(last_quote)\n        if re.match('\\\\{\\\\{misattributed|\\\\{\\\\{disputed', line, re.IGNORECASE):\n            break\n"]]}
{"hexsha": "aa9c59ada03bc4b765d0b0464fdd75b2a97d8859", "ext": "py", "lang": "Python", "content": "def generate_ios_json():\n    result = []\n\n    for key, value in ios_icon_mapping.items():\n        for multiplier in value:\n            effective_size = int(float(key) * float(multiplier))\n            result.append({\n                \"absoluteSize\": effective_size,\n                \"fileFormat\": \"png\",\n                \"name\": f'_{key}@{multiplier}x',\n                \"namingScheme\": 0,\n                \"scale\": 0,\n                \"visibleScaleType\": 1\n            })\n    return [{\n        \"name\": \"iOS App Icon (noahgilmore.com)\",\n        \"shouldApplyAutomatically\": True,\n        \"exportFormats\": result\n    }]", "fn_id": 0, "class_fn": false, "repo": "noahsark769/sketch-preset-icons-generator", "file": "generate.py", "last_update_at": "2020-01-17T07:40:42+00:00", "question_id": "aa9c59ada03bc4b765d0b0464fdd75b2a97d8859_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def generate_ios_json():\n    result = []\n    for key, value in ios_icon_mapping.items():\n        for multiplier in value:\n            effective_size = int(float(key) * float(multiplier))\n            result.append({'absoluteSize': effective_size, 'fileFormat': 'png', 'name': f'_{key}@{multiplier}x', 'namingScheme': 0, 'scale': 0, 'visibleScaleType': 1})\n"]]}
{"hexsha": "71eb6a1a78e7e9a50cf137956f87e7e06f2d3af1", "ext": "py", "lang": "Python", "content": "def get_param(entry,paramname):\n    '''\n    Returns parameter value.\n    Takes TaskEntry object.\n    '''\n    return json.loads(entry.params)[paramname]", "fn_id": 10, "class_fn": false, "repo": "aolabNeuro/brain-python-interface", "file": "db/dbfunctions.py", "last_update_at": "2020-09-11T01:34:50+00:00", "question_id": "71eb6a1a78e7e9a50cf137956f87e7e06f2d3af1_10", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_param(entry, paramname):\n    \"\"\"\n    Returns parameter value.\n    Takes TaskEntry object.\n    \"\"\"\n"]]}
{"hexsha": "10185d3f9e286470c2b937976eaee587dbf057bc", "ext": "py", "lang": "Python", "content": "def add_default_location_to_dataset(dataset_folder, location):\n    metadata = read_dataset_metadata(dataset_folder)\n    metadata[\"defaultLocation\"] = location\n    validate_with_schema(metadata, \"dataset\")\n    write_dataset_metadata(dataset_folder, metadata)", "fn_id": 2, "class_fn": false, "repo": "platybrowser/mobie-python", "file": "mobie/metadata/dataset_metadata.py", "last_update_at": "2020-03-03T01:33:06+00:00", "question_id": "10185d3f9e286470c2b937976eaee587dbf057bc_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def add_default_location_to_dataset(dataset_folder, location):\n    metadata = read_dataset_metadata(dataset_folder)\n    metadata['defaultLocation'] = location\n    validate_with_schema(metadata, 'dataset')\n"]]}
{"hexsha": "c82e7167822ec2d042db7a5b1c4ad889f7a70e6d", "ext": "py", "lang": "Python", "content": "def formatString(value, length, fLen):\n    fString = '{0:0.'+str(fLen) +'f}'\n    string = fString.format(value)\n    padding = length - len(string)\n    if padding != 0:\n        for i in range(padding):\n            string = ' ' + string\n    return string    ", "fn_id": 1, "class_fn": false, "repo": "ncthompson/inverter_monitor", "file": "lcdDisplay/lcdDriver.py", "last_update_at": "2020-06-18T21:33:57+00:00", "question_id": "c82e7167822ec2d042db7a5b1c4ad889f7a70e6d_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def formatString(value, length, fLen):\n    fString = '{0:0.' + str(fLen) + 'f}'\n    string = fString.format(value)\n    padding = length - len(string)\n    if padding != 0:\n        for i in range(padding):\n            string = ' ' + string\n"]]}
{"hexsha": "42067e8baf858341c173b42c0f164a10dc6a1734", "ext": "py", "lang": "Python", "content": "def bitmask_from_text(mask, text):\n    \"\"\"Initialize a bitmask from text.\n\n    Builds an integer value from text containing bit names that should be set. The\n    complement of :func:`decode_bitmask`. For example::\n\n        >>> COLORS = define_bitmask('COLORS','Primary colors',RED=0,BLUE=1,GREEN=4)\n        >>> '{0:b}'.format(bitmask_from_text(COLORS,'GREEN|BLUE'))\n        '10010'\n\n    Args:\n        mask: A bitmask type, normally created with :func:`create_bitmask`, that defines\n            the symbolic bit names that are allowed.\n        text: A list of bit names separated by '|'.\n\n    Returns:\n        int: Integer with bits set for each bit name appearing in the text.\n\n    Raises:\n        ValueError: invalid text specification.\n    \"\"\"\n    if not hasattr(mask, '__dict__'):\n        raise ValueError('Invalid bitmask.')\n    value = int(0)\n    for bit_name in text.split('|'):\n        if bit_name not in mask.__dict__:\n            raise ValueError('Invalid bit name: {0}.'.format(bit_name))\n        value = value | mask.__dict__[bit_name]\n    return value", "fn_id": 3, "class_fn": false, "repo": "dkirkby/bossdata", "file": "bossdata/bits.py", "last_update_at": "2020-04-08T10:00:31+00:00", "question_id": "42067e8baf858341c173b42c0f164a10dc6a1734_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def bitmask_from_text(mask, text):\n    \"\"\"Initialize a bitmask from text.\n\n    Builds an integer value from text containing bit names that should be set. The\n    complement of :func:`decode_bitmask`. For example::\n\n        >>> COLORS = define_bitmask('COLORS','Primary colors',RED=0,BLUE=1,GREEN=4)\n        >>> '{0:b}'.format(bitmask_from_text(COLORS,'GREEN|BLUE'))\n        '10010'\n\n    Args:\n        mask: A bitmask type, normally created with :func:`create_bitmask`, that defines\n            the symbolic bit names that are allowed.\n        text: A list of bit names separated by '|'.\n\n    Returns:\n        int: Integer with bits set for each bit name appearing in the text.\n\n    Raises:\n        ValueError: invalid text specification.\n    \"\"\"\n    if not hasattr(mask, '__dict__'):\n        raise ValueError('Invalid bitmask.')\n    value = int(0)\n    for bit_name in text.split('|'):\n        if bit_name not in mask.__dict__:\n            raise ValueError('Invalid bit name: {0}.'.format(bit_name))\n        value = value | mask.__dict__[bit_name]\n"]]}
{"hexsha": "a3d9a8469493ee5f200d6a9d8f927097f95bfa6e", "ext": "py", "lang": "Python", "content": "def start(player):\n    clear()\n    lawan = random_lawan()\n    hp_player = player.hp\n    menang = True\n    turn = 1\n\n    while player.hp > 0:\n        bar()\n        print('[ Turn %d ]\\n' % turn)\n\n        # player menyerang lawan\n        hp_awal_lawan = lawan.hp\n        player.attack(lawan)\n        \n        print('> %s menyerang lawan! (Damage: %d)' % (player.name, player.atk))\n        print('HP lawan: %d -> %d' % (hp_awal_lawan, lawan.hp))\n\n        if lawan.hp == 0:\n            menang = True\n            print('\\nKamu menang!'); bar();\n            break\n\n        print()\n\n        # lawan menyerang player\n        hp_awal_player = player.hp\n        lawan.attack(player)\n\n        print('< Lawan menyerang %s! (Damage: %d)' % (player.name, lawan.atk))\n        print('HP %s: %d -> %d' % \n            (player.name, hp_awal_player, player.hp))\n        \n        if player.hp == 0:\n            menang = False\n            print('\\nKamu kalah!'); bar()\n            break\n\n        turn += 1\n        bar()\n        wait()\n\n    player.hp = hp_player\n    wait()\n    clear()\n    if menang:\n        player.gain_exp(10)", "fn_id": 1, "class_fn": false, "repo": "cacadosman/Do-Whatever", "file": "Mini-RPG/module/battle.py", "last_update_at": "2020-10-14T08:14:49+00:00", "question_id": "a3d9a8469493ee5f200d6a9d8f927097f95bfa6e_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def start(player):\n    clear()\n    lawan = random_lawan()\n    hp_player = player.hp\n    menang = True\n    turn = 1\n    while player.hp > 0:\n        bar()\n        print('[ Turn %d ]\\n' % turn)\n        hp_awal_lawan = lawan.hp\n        player.attack(lawan)\n        print('> %s menyerang lawan! (Damage: %d)' % (player.name, player.atk))\n        print('HP lawan: %d -> %d' % (hp_awal_lawan, lawan.hp))\n        if lawan.hp == 0:\n            menang = True\n            print('\\nKamu menang!')\n            bar()\n            break\n        print()\n        hp_awal_player = player.hp\n        lawan.attack(player)\n        print('< Lawan menyerang %s! (Damage: %d)' % (player.name, lawan.atk))\n        print('HP %s: %d -> %d' % (player.name, hp_awal_player, player.hp))\n        if player.hp == 0:\n            menang = False\n            print('\\nKamu kalah!')\n            bar()\n            break\n        turn += 1\n        bar()\n        wait()\n    player.hp = hp_player\n    wait()\n    clear()\n    if menang:\n"]]}
{"hexsha": "4654a5cbc3f0ec0f604f2153bb14165d5526a3b9", "ext": "py", "lang": "Python", "content": "def _regenerate(\n    file_h,\n    h,\n    pabot_args,\n    outs_dir,\n    datasources,\n    options,\n    lines): # type: (Optional[Hashes], Hashes, Dict[str, str], str, List[str], Dict[str, str], List[ExecutionItem]) -> List[ExecutionItem]\n    assert(all(isinstance(s, ExecutionItem) for s in lines))\n    if (file_h is None or file_h.suitesfrom != h.suitesfrom) \\\n        and 'suitesfrom' in pabot_args \\\n        and os.path.isfile(pabot_args['suitesfrom']):\n        suites = _suites_from_outputxml(pabot_args['suitesfrom'])\n        if file_h is None or file_h.dirs != h.dirs:\n            all_suites = generate_suite_names_with_builder(outs_dir, datasources, options)\n        else:\n            all_suites = [suite for suite in lines if suite]\n        suites = _preserve_order(all_suites, suites)\n    else:\n        suites = generate_suite_names_with_builder(outs_dir, datasources, options)\n        if pabot_args.get('testlevelsplit'):\n            tests = [] # type: List[TestItem]\n            for s in suites:\n                tests.extend(s.tests)\n            suites = tests\n        suites = _preserve_order(suites, [suite for suite in lines if suite])\n    if suites:\n        store_suite_names(h, suites)\n    assert(all(isinstance(s, ExecutionItem) for s in suites))\n    return suites", "fn_id": 28, "class_fn": false, "repo": "amochin/pabot", "file": "pabot/pabot.py", "last_update_at": "2020-04-30T07:55:59+00:00", "question_id": "4654a5cbc3f0ec0f604f2153bb14165d5526a3b9_28", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _regenerate(file_h, h, pabot_args, outs_dir, datasources, options, lines):\n    assert all((isinstance(s, ExecutionItem) for s in lines))\n    if (file_h is None or file_h.suitesfrom != h.suitesfrom) and 'suitesfrom' in pabot_args and os.path.isfile(pabot_args['suitesfrom']):\n        suites = _suites_from_outputxml(pabot_args['suitesfrom'])\n        if file_h is None or file_h.dirs != h.dirs:\n            all_suites = generate_suite_names_with_builder(outs_dir, datasources, options)\n        else:\n            all_suites = [suite for suite in lines if suite]\n        suites = _preserve_order(all_suites, suites)\n    else:\n        suites = generate_suite_names_with_builder(outs_dir, datasources, options)\n        if pabot_args.get('testlevelsplit'):\n            tests = []\n            for s in suites:\n                tests.extend(s.tests)\n            suites = tests\n        suites = _preserve_order(suites, [suite for suite in lines if suite])\n    if suites:\n        store_suite_names(h, suites)\n    assert all((isinstance(s, ExecutionItem) for s in suites))\n"]]}
{"hexsha": "22b29b34b674dc37e0c08ea058b6d7d9ad9291d6", "ext": "py", "lang": "Python", "content": "def test_decode_text_unicode():\n    value = u'\\uffff'\n    decoded = decode_text(value)\n    assert decoded == value", "fn_id": 0, "class_fn": false, "repo": "nanoric/flask-debugtoolbar", "file": "test/test_utils.py", "last_update_at": "2020-02-23T01:14:37+00:00", "question_id": "22b29b34b674dc37e0c08ea058b6d7d9ad9291d6_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_decode_text_unicode():\n    value = u'\\uffff'\n    decoded = decode_text(value)\n"]]}
{"hexsha": "882bbc1addb5a30394d5fa8f5a1cffe9fb2931e4", "ext": "py", "lang": "Python", "content": "def read_csv(fn):\n    '''reads a csv file and returns a pandas dataframe'''\n    temp = pd.read_csv(fn)\n    return pd.DataFrame(temp)", "fn_id": 0, "class_fn": false, "repo": "arjuns2020/pycateda", "file": "pycateda/pycateda/__init__.py", "last_update_at": "2020-06-14T13:00:06+00:00", "question_id": "882bbc1addb5a30394d5fa8f5a1cffe9fb2931e4_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def read_csv(fn):\n    \"\"\"reads a csv file and returns a pandas dataframe\"\"\"\n    temp = pd.read_csv(fn)\n"]]}
{"hexsha": "09d742f8da13770052a2bbd09a2bb6e30f034e1a", "ext": "py", "lang": "Python", "content": "def __make_daybyday_interactive_timeline(\n    df: pd.DataFrame,\n    *,\n    geo_df: geopandas.GeoDataFrame,\n    value_col: str,\n    transform_df_func: Callable[[pd.DataFrame], pd.DataFrame] = None,\n    stage: Union[DiseaseStage, Literal[Select.ALL]] = Select.ALL,\n    count: Union[Counting, Literal[Select.ALL]] = Select.ALL,\n    out_file_basename: str,\n    subplot_title_prefix: str,\n    plot_aspect_ratio: float = None,\n    cmap=None,\n    n_cbar_buckets: int = None,\n    n_buckets_btwn_major_ticks: int = None,\n    n_minor_ticks_btwn_major_ticks: int = None,\n    per_capita_denominator: int = None,\n    x_range: Tuple[float, float],\n    y_range: Tuple[float, float],\n    min_visible_y_range: float,\n    should_make_video: bool,\n) -> InfoForAutoload:\n    \"\"\"Create the bokeh interactive timeline plot(s)\n\n    This function takes the given DataFrame, which must contain COVID data for locations\n    on different dates, and a GeoDataFrame, which contains the long/lat coords for those\n    locations, and creates an interactive choropleth of the COVID data over time.\n\n    :param df: The COVID data DataFrame\n    :type df: pd.DataFrame\n    :param geo_df: The geometry GeoDataFrame for the locations in `df`\n    :type geo_df: geopandas.GeoDataFrame\n    :param value_col: The column of `df` containing the values to plot in the\n    choropleth; should be something like \"Case_Counts\" or \"Case_Diff_From_Prev_Day\"\n    :type value_col: str\n    :param stage: The DiseaseStage to plot, defaults to Select.ALL. If ALL, then all\n    stages are plotted and are stacked vertically.\n    :type stage: Union[DiseaseStage, Literal[Select.ALL]], optional\n    :param count: The Counting to plot, defaults to Select.ALL. If ALL, then all\n    count types are plotted and are stacked horizontally.\n    :type count: Union[Counting, Literal[Select.ALL]], optional\n    :param out_file_basename: The basename of the file to save the interactive plots to\n    (there are two components, the JS script and the HTML <div>)\n    :type out_file_basename: str\n    :param subplot_title_prefix: What the first part of the subplot title should be;\n    probably a function of `value_col` (if value_col is \"Case_Counts\" then this param\n    might be \"Cases\" or \"# of Cases\")\n    :type subplot_title_prefix: str\n    :param x_range: The range of the x-axis as (min, max)\n    :type x_range: Tuple[float, float]\n    :param y_range: The range of the y-axis as (min, max)\n    :type y_range: Tuple[float, float]\n    :param min_visible_y_range: The minimum height (in axis units) of the y-axis; it\n    will not be possible to zoom in farther than this on the choropleth.\n    :type min_visible_y_range: float\n    :param should_make_video: Optionally run through the timeline day by day, capture\n    a screenshot for each day, and then stitch the screenshots into a video. The video\n    shows the same info as the interactive plots, but not interactively. This easily\n    takes 20x as long as just making the graphs themselves, so use with caution.\n    :type should_make_video: bool\n    :param transform_df_func: This function expects data in a certain format, and does\n    a bunch of preprocessing (expected to be common) before plotting. This gives you a\n    chance to do any customization on the postprocessed df before it's plotted. Defaults\n    to None, in which case no additional transformation is performed.\n    :type transform_df_func: Callable[[pd.DataFrame], pd.DataFrame], optional\n    :param plot_aspect_ratio: The aspect ratio of the plot as width/height; if set, the\n    aspect ratio will be fixed to this. Defaults to None, in which case the aspect ratio\n    is determined from the x_range and y_range arguments\n    :type plot_aspect_ratio: float, optional\n    :param cmap: The colormap to use as either a matplotlib-compatible colormap or a\n    list of hex strings (e.g., [\"#ae8f1c\", ...]). Defaults to None in which case a\n    reasonable default is used.\n    :type cmap: Matplotlib-compatible colormap or List[str], optional\n    :param n_cbar_buckets: How many colorbar buckets to use. Has little effect if the\n    colormap is continuous, but always works in conjunction with\n    n_buckets_btwn_major_ticks to determine the number of major ticks. Defaults to 6.\n    :type n_cbar_buckets: int, optional\n    :param n_buckets_btwn_major_ticks: How many buckets are to lie between colorbar\n    major ticks, determining how many major ticks are drawn. Defaults to 1.\n    :type n_buckets_btwn_major_ticks: int, optional\n    :param n_minor_ticks_btwn_major_ticks: How many minor ticks to draw between colorbar\n    major ticks. Defaults to 8 (which means each pair of major ticks has 10 ticks\n    total).\n    :type n_minor_ticks_btwn_major_ticks: int, optional\n    :param per_capita_denominator: When describing per-capita numbers, what to use as\n    the denominator (e.g., cases per 100,000 people). If None, it is automatically\n    computed per plot to be appropriately scaled for the data.\n    :type per_capita_denominator: int, optional\n    :raises ValueError: [description]\n    :return: The two pieces of info required to make a Bokeh autoloading HTML+JS plot:\n    the HTML div to be inserted somewhere in the HTML body, and the JS file that will\n    load the plot into that div.\n    :rtype: InfoForAutoload\n    \"\"\"\n\n    Counting.verify(count, allow_select=True)\n    DiseaseStage.verify(stage, allow_select=True)\n\n    # The date as a string, so that bokeh can use it as a column name\n    STRING_DATE_COL = \"String_Date_\"\n    # A column whose sole purpose is to be a (the same) date associated with each\n    # location\n    FAKE_DATE_COL = \"Fake_Date_\"\n    # The column we'll actually use for the colors; it's computed from value_col\n    COLOR_COL = \"Color_\"\n\n    # Under no circumstances may you change this date format\n    # It's not just a pretty date representation; it actually has to match up with the\n    # date strings computed in JS\n    DATE_FMT = r\"%Y-%m-%d\"\n\n    ID_COLS = [\n        REGION_NAME_COL,\n        Columns.DATE,\n        Columns.STAGE,\n        Columns.COUNT_TYPE,\n    ]\n\n    if cmap is None:\n        cmap = cmocean.cm.matter\n\n    if n_cbar_buckets is None:\n        n_cbar_buckets = 6\n\n    if n_buckets_btwn_major_ticks is None:\n        n_buckets_btwn_major_ticks = 1\n\n    if n_minor_ticks_btwn_major_ticks is None:\n        n_minor_ticks_btwn_major_ticks = 8\n\n    n_cbar_major_ticks = n_cbar_buckets // n_buckets_btwn_major_ticks + 1\n\n    try:\n        color_list = [\n            # Convert matplotlib colormap to bokeh (list of hex strings)\n            # https://stackoverflow.com/a/49934218\n            RGB(*rgb).to_hex()\n            for i, rgb in enumerate((255 * cmap(range(256))).astype(\"int\"))\n        ]\n    except TypeError:\n        color_list = cmap\n\n    color_list: List[BokehColor]\n\n    if stage is Select.ALL:\n        stage_list = list(DiseaseStage)\n    else:\n        stage_list = [stage]\n\n    if count is Select.ALL:\n        count_list = list(Counting)\n    else:\n        count_list = [count]\n\n    stage_list: List[DiseaseStage]\n    count_list: List[Counting]\n\n    stage_count_list: List[Tuple[DiseaseStage, Counting]] = list(\n        itertools.product(stage_list, count_list)\n    )\n\n    df = df.copy()\n\n    # Unadjust dates (see SaveFormats._adjust_dates)\n    normalized_dates = df[Columns.DATE].dt.normalize()\n    is_at_midnight = df[Columns.DATE] == normalized_dates\n    df.loc[is_at_midnight, Columns.DATE] -= pd.Timedelta(days=1)\n    df.loc[~is_at_midnight, Columns.DATE] = normalized_dates[~is_at_midnight]\n\n    min_date, max_date = df[Columns.DATE].agg([\"min\", \"max\"])\n    dates: List[pd.Timestamp] = pd.date_range(start=min_date, end=max_date, freq=\"D\")\n    max_date_str = max_date.strftime(DATE_FMT)\n\n    # Get day-by-day case diffs per location, date, stage, count-type\n\n    # Make sure data exists for every date for every state so that the entire country is\n    # plotted each day; fill missing data with 0 (missing really *is* as good as 0)\n    # enums will be replaced by their name (kind of important)\n    id_cols_product: pd.MultiIndex = pd.MultiIndex.from_product(\n        [\n            df[REGION_NAME_COL].unique(),\n            dates,\n            [s.name for s in DiseaseStage],\n            [c.name for c in Counting],\n        ],\n        names=ID_COLS,\n    )\n\n    df = (\n        id_cols_product.to_frame(index=False)\n        .merge(df, how=\"left\", on=ID_COLS,)\n        .sort_values(ID_COLS)\n    )\n\n    df[STRING_DATE_COL] = df[Columns.DATE].dt.strftime(DATE_FMT)\n    df[Columns.CASE_COUNT] = df[Columns.CASE_COUNT].fillna(0)\n\n    if transform_df_func is not None:\n        df = transform_df_func(df)\n\n    df = geo_df.merge(df, how=\"inner\", on=REGION_NAME_COL)[\n        [\n            REGION_NAME_COL,\n            Columns.DATE,\n            STRING_DATE_COL,\n            Columns.STAGE,\n            Columns.COUNT_TYPE,\n            value_col,\n        ]\n    ]\n\n    dates: List[pd.Timestamp] = [pd.Timestamp(d) for d in df[Columns.DATE].unique()]\n\n    values_mins_maxs = (\n        df[df[value_col] > 0]\n        .groupby([Columns.STAGE, Columns.COUNT_TYPE])[value_col]\n        .agg([\"min\", \"max\"])\n    )\n\n    vmins: pd.Series = values_mins_maxs[\"min\"]\n    vmaxs: pd.Series = values_mins_maxs[\"max\"]\n\n    pow10s_series: pd.Series = vmaxs.map(lambda x: int(10 ** (-np.floor(np.log10(x)))))\n\n    # _pow_10s_series_dict = {}\n    # for stage in DiseaseStage:\n    #     _pow_10s_series_dict.update(\n    #         {\n    #             (stage.name, Counting.TOTAL_CASES.name): 100000,\n    #             (stage.name, Counting.PER_CAPITA.name): 10000,\n    #         }\n    #     )\n\n    # pow10s_series = pd.Series(_pow_10s_series_dict)\n\n    vmins: dict = vmins.to_dict()\n    vmaxs: dict = vmaxs.to_dict()\n\n    for stage in DiseaseStage:\n        _value_key = (stage.name, Counting.PER_CAPITA.name)\n        if per_capita_denominator is None:\n            _max_pow10 = pow10s_series.loc[\n                (slice(None), Counting.PER_CAPITA.name)\n            ].max()\n        else:\n            _max_pow10 = per_capita_denominator\n\n        vmins[_value_key] *= _max_pow10\n        vmaxs[_value_key] *= _max_pow10\n        pow10s_series[_value_key] = _max_pow10\n\n    percap_pow10s: pd.Series = df.apply(\n        lambda row: pow10s_series[(row[Columns.STAGE], row[Columns.COUNT_TYPE])],\n        axis=1,\n    )\n\n    _per_cap_rows = df[Columns.COUNT_TYPE] == Counting.PER_CAPITA.name\n    df.loc[_per_cap_rows, value_col] *= percap_pow10s.loc[_per_cap_rows]\n\n    # Ideally we wouldn't have to pivot, and we could do a JIT join of state longs/lats\n    # after filtering the data. Unfortunately this is not possible, and a long data\n    # format leads to duplication of the very large long/lat lists; pivoting is how we\n    # avoid that. (This seems to be one downside of bokeh when compared to plotly)\n    df = (\n        df.pivot_table(\n            index=[REGION_NAME_COL, Columns.STAGE, Columns.COUNT_TYPE],\n            columns=STRING_DATE_COL,\n            values=value_col,\n            aggfunc=\"first\",\n        )\n        .reset_index()\n        .merge(\n            geo_df[[REGION_NAME_COL, LONG_COL, LAT_COL]],\n            how=\"inner\",\n            on=REGION_NAME_COL,\n        )\n    )\n\n    # All three oclumns are just initial values; they'll change with the date slider\n    df[value_col] = df[max_date_str]\n    df[FAKE_DATE_COL] = max_date_str\n    df[COLOR_COL] = np.where(df[value_col] > 0, df[value_col], \"NaN\")\n\n    # Technically takes a df but we don't need the index\n    bokeh_data_source = ColumnDataSource(\n        {k: v.tolist() for k, v in df.to_dict(orient=\"series\").items()}\n    )\n\n    filters = [\n        [\n            GroupFilter(column_name=Columns.STAGE, group=stage.name),\n            GroupFilter(column_name=Columns.COUNT_TYPE, group=count.name),\n        ]\n        for stage, count in stage_count_list\n    ]\n\n    figures = []\n\n    for subplot_index, (stage, count) in enumerate(stage_count_list):\n        # fig = bplotting.figure()\n        # ax: plt.Axes = fig.add_subplot(\n        #     len(stage_list), len(count_list), subplot_index\n        # )\n\n        # # Add timestamp to top right axis\n        # if subplot_index == 2:\n        #     ax.text(\n        #         1.25,  # Coords are arbitrary magic numbers\n        #         1.23,\n        #         f\"Last updated {NOW_STR}\",\n        #         horizontalalignment=\"right\",\n        #         fontsize=\"small\",\n        #         transform=ax.transAxes,\n        #     )\n\n        view = CDSView(source=bokeh_data_source, filters=filters[subplot_index])\n\n        vmin = vmins[(stage.name, count.name)]\n        vmax = vmaxs[(stage.name, count.name)]\n\n        # Compute and set axes titles\n        if stage is DiseaseStage.CONFIRMED:\n            fig_stage_name = \"Cases\"\n        elif stage is DiseaseStage.DEATH:\n            fig_stage_name = \"Deaths\"\n        else:\n            raise ValueError\n\n        fig_title_components: List[str] = []\n        if subplot_title_prefix is not None:\n            fig_title_components.append(subplot_title_prefix)\n\n        fig_title_components.append(fig_stage_name)\n\n        if count is Counting.PER_CAPITA:\n            _per_cap_denom = pow10s_series[(stage.name, count.name)]\n            fig_title_components.append(f\"Per {_per_cap_denom:,d} people\")\n            formatter = PrintfTickFormatter(format=r\"%2.3f\")\n            label_standoff = 12\n            tooltip_fmt = \"{0.000}\"\n        else:\n            formatter = NumeralTickFormatter(format=\"0.0a\")\n            label_standoff = 10\n            tooltip_fmt = \"{0}\"\n\n        color_mapper = LogColorMapper(\n            color_list, low=vmin, high=vmax, nan_color=\"#f2f2f2\",\n        )\n\n        fig_title = \" \".join(fig_title_components)\n\n        if plot_aspect_ratio is None:\n            if x_range is None or y_range is None:\n                raise ValueError(\n                    \"Must provide both `x_range` and `y_range`\"\n                    + \" when `plot_aspect_ratio` is None\"\n                )\n            plot_aspect_ratio = (x_range[1] - x_range[0]) / (y_range[1] - y_range[0])\n\n        # Create figure object\n        p = bplotting.figure(\n            title=fig_title,\n            title_location=\"above\",\n            tools=[\n                HoverTool(\n                    tooltips=[\n                        (\"Date\", f\"@{{{FAKE_DATE_COL}}}\"),\n                        (\"State\", f\"@{{{REGION_NAME_COL}}}\"),\n                        (\"Count\", f\"@{{{value_col}}}{tooltip_fmt}\"),\n                    ],\n                    toggleable=False,\n                ),\n                PanTool(),\n                BoxZoomTool(match_aspect=True),\n                ZoomInTool(),\n                ZoomOutTool(),\n                ResetTool(),\n            ],\n            active_drag=None,\n            aspect_ratio=plot_aspect_ratio,\n            output_backend=\"webgl\",\n            lod_factor=4,\n            lod_interval=400,\n            lod_threshold=1000,\n            lod_timeout=300,\n        )\n\n        p.xgrid.grid_line_color = None\n        p.ygrid.grid_line_color = None\n        # Finally, add the actual choropleth data we care about\n        p.patches(\n            LONG_COL,\n            LAT_COL,\n            source=bokeh_data_source,\n            view=view,\n            fill_color={\"field\": COLOR_COL, \"transform\": color_mapper},\n            line_color=\"black\",\n            line_width=0.25,\n            fill_alpha=1,\n        )\n\n        # Add evenly spaced ticks and their labels to the colorbar\n        # First major, then minor\n        # Adapted from https://stackoverflow.com/a/50314773\n        bucket_size = (vmax / vmin) ** (1 / n_cbar_buckets)\n        tick_dist = bucket_size ** n_buckets_btwn_major_ticks\n\n        # Simple log scale math\n        major_tick_locs = (\n            vmin\n            * (tick_dist ** np.arange(0, n_cbar_major_ticks))\n            # * (bucket_size ** 0.5) # Use this if centering ticks on buckets\n        )\n        # Get minor locs by linearly interpolating between major ticks\n        minor_tick_locs = []\n        for major_tick_index, this_major_tick in enumerate(major_tick_locs[:-1]):\n            next_major_tick = major_tick_locs[major_tick_index + 1]\n\n            # Get minor ticks as numbers in range [this_major_tick, next_major_tick]\n            # and exclude the major ticks themselves (once we've used them to\n            # compute the minor tick locs)\n            minor_tick_locs.extend(\n                np.linspace(\n                    this_major_tick,\n                    next_major_tick,\n                    n_minor_ticks_btwn_major_ticks + 2,\n                )[1:-1]\n            )\n\n        color_bar = ColorBar(\n            color_mapper=color_mapper,\n            ticker=FixedTicker(ticks=major_tick_locs, minor_ticks=minor_tick_locs),\n            formatter=formatter,\n            label_standoff=label_standoff,\n            major_tick_out=0,\n            major_tick_in=13,\n            major_tick_line_color=\"white\",\n            major_tick_line_width=1,\n            minor_tick_out=0,\n            minor_tick_in=5,\n            minor_tick_line_color=\"white\",\n            minor_tick_line_width=1,\n            location=(0, 0),\n            border_line_color=None,\n            bar_line_color=None,\n            orientation=\"vertical\",\n        )\n\n        p.add_layout(color_bar, \"right\")\n        p.hover.point_policy = \"follow_mouse\"\n\n        # Bokeh axes (and most other things) are splattable\n        p.axis.visible = False\n\n        figures.append(p)\n\n    # Make all figs pan and zoom together by setting their axes equal to each other\n    # Also fix the plots' aspect ratios\n    figs_iter = iter(np.ravel(figures))\n    anchor_fig = next(figs_iter)\n\n    if x_range is not None and y_range is not None:\n        data_aspect_ratio = (x_range[1] - x_range[0]) / (y_range[1] - y_range[0])\n    else:\n        data_aspect_ratio = plot_aspect_ratio\n\n    if x_range is not None:\n        anchor_fig.x_range = Range1d(\n            *x_range,\n            bounds=\"auto\",\n            min_interval=min_visible_y_range * data_aspect_ratio,\n        )\n\n    if y_range is not None:\n        anchor_fig.y_range = Range1d(\n            *y_range, bounds=\"auto\", min_interval=min_visible_y_range\n        )\n\n    for fig in figs_iter:\n        fig.x_range = anchor_fig.x_range\n        fig.y_range = anchor_fig.y_range\n\n    # 2x2 grid (for now)\n    gp = gridplot(\n        figures,\n        ncols=len(count_list),\n        sizing_mode=\"scale_both\",\n        toolbar_location=\"above\",\n    )\n    plot_layout = [gp]\n\n    # Ok, pause\n    # Now we're going into a whole other thing: we're doing all the JS logic behind a\n    # date slider that changes which date is shown on the graphs. The structure of the\n    # data is one column per date, one row per location, and a few extra columns to\n    # store the data the graph will use. When we adjust the date of the slider, we copy\n    # the relevant column of the df into the columns the graphs are looking at.\n    # That's the easy part; the hard part is handling the \"play button\" functionality,\n    # whereby the user can click one button and the date slider will periodically\n    # advance itself. That requires a fair bit of logic to schedule and cancel the\n    # timers and make it all feel right.\n\n    # Create unique ID for the JS playback info object for this plot (since it'll be on\n    # the webpage with other plots, and their playback info isn't shared)\n    _THIS_PLOT_ID = uuid.uuid4().hex\n\n    __TIMER = \"'timer'\"\n    __IS_ACTIVE = \"'isActive'\"\n    __SELECTED_INDEX = \"'selectedIndex'\"\n    __BASE_INTERVAL_MS = \"'BASE_INTERVAL'\"  # Time (in MS) btwn frames when speed==1\n    __TIMER_START_DATE = \"'startDate'\"\n    __TIMER_ELAPSED_TIME_MS = \"'elapsedTimeMS'\"\n    __TIMER_ELAPSED_TIME_PROPORTION = \"'elapsedTimeProportion'\"\n    __SPEEDS_KEY = \"'SPEEDS'\"\n    __PLAYBACK_INFO = f\"window._playbackInfo_{_THIS_PLOT_ID}\"\n\n    _PBI_TIMER = f\"{__PLAYBACK_INFO}[{__TIMER}]\"\n    _PBI_IS_ACTIVE = f\"{__PLAYBACK_INFO}[{__IS_ACTIVE}]\"\n    _PBI_SELECTED_INDEX = f\"{__PLAYBACK_INFO}[{__SELECTED_INDEX}]\"\n    _PBI_TIMER_START_DATE = f\"{__PLAYBACK_INFO}[{__TIMER_START_DATE}]\"\n    _PBI_TIMER_ELAPSED_TIME_MS = f\"{__PLAYBACK_INFO}[{__TIMER_ELAPSED_TIME_MS}]\"\n    _PBI_TIMER_ELAPSED_TIME_PROPORTION = (\n        f\"{__PLAYBACK_INFO}[{__TIMER_ELAPSED_TIME_PROPORTION}]\"\n    )\n    _PBI_BASE_INTERVAL = f\"{__PLAYBACK_INFO}[{__BASE_INTERVAL_MS}]\"\n    _PBI_SPEEDS = f\"{__PLAYBACK_INFO}[{__SPEEDS_KEY}]\"\n    _PBI_CURR_INTERVAL_MS = (\n        f\"{_PBI_BASE_INTERVAL} / {_PBI_SPEEDS}[{_PBI_SELECTED_INDEX}]\"\n    )\n\n    _SPEED_OPTIONS = [0.25, 0.5, 1.0, 2.0]\n    _DEFAULT_SPEED = 1.0\n    _DEFAULT_SELECTED_INDEX = _SPEED_OPTIONS.index(_DEFAULT_SPEED)\n\n    _SETUP_WINDOW_PLAYBACK_INFO = f\"\"\"\n        if (typeof({__PLAYBACK_INFO}) === 'undefined') {{\n            {__PLAYBACK_INFO} = {{\n                {__TIMER}: null,\n                {__IS_ACTIVE}: false,\n                {__SELECTED_INDEX}: {_DEFAULT_SELECTED_INDEX},\n                {__TIMER_START_DATE}: null,\n                {__TIMER_ELAPSED_TIME_MS}: 0,\n                {__TIMER_ELAPSED_TIME_PROPORTION}: 0,\n                {__BASE_INTERVAL_MS}: 1000,\n                {__SPEEDS_KEY}: {_SPEED_OPTIONS}\n            }};\n        }}\n\n    \"\"\"\n\n    _DEFFUN_INCR_DATE = f\"\"\"\n        // See this link for why this works (it's an undocumented feature?)\n        // https://discourse.bokeh.org/t/5254\n        // Tl;dr we need this to automatically update the hover as the play button plays\n        // Without this, the hover tooltip only updates when we jiggle the mouse\n        // slightly\n\n        let prev_val = null;\n        source.inspect.connect(v => prev_val = v);\n\n        function updateDate() {{\n            {_PBI_TIMER_START_DATE} = new Date();\n            {_PBI_TIMER_ELAPSED_TIME_MS} = 0\n            if (dateSlider.value < maxDate) {{\n                dateSlider.value += 86400000;\n            }}\n\n            if (dateSlider.value >= maxDate) {{\n                console.log(dateSlider.value, maxDate)\n                console.log('reached end')\n                clearInterval({_PBI_TIMER});\n                {_PBI_IS_ACTIVE} = false;\n                playPauseButton.active = false;\n                playPauseButton.change.emit();\n                playPauseButton.label = 'Restart';\n            }}\n\n            dateSlider.change.emit();\n\n            // This is pt. 2 of the prev_val/inspect stuff above\n            if (prev_val !== null) {{\n                source.inspect.emit(prev_val);\n            }}\n        }}\n    \"\"\"\n\n    _DO_START_TIMER = f\"\"\"\n        function startLoopTimer() {{\n            updateDate();\n            if ({_PBI_IS_ACTIVE}) {{\n                {_PBI_TIMER} = setInterval(updateDate, {_PBI_CURR_INTERVAL_MS})\n            }}\n\n        }}\n\n        {_PBI_TIMER_START_DATE} = new Date();\n\n        // Should never be <0 or >1 but I am being very defensive here\n        const proportionRemaining = 1 - (\n            {_PBI_TIMER_ELAPSED_TIME_PROPORTION} <= 0\n            ? 0\n            : {_PBI_TIMER_ELAPSED_TIME_PROPORTION} >= 1\n            ? 1\n            : {_PBI_TIMER_ELAPSED_TIME_PROPORTION}\n        );\n        const remainingTimeMS = (\n            {_PBI_CURR_INTERVAL_MS} * proportionRemaining\n        );\n        const initialInterval = (\n            {_PBI_TIMER_ELAPSED_TIME_MS} === 0\n            ? 0\n            : remainingTimeMS\n        );\n\n        {_PBI_TIMER} = setTimeout(\n            startLoopTimer,\n            initialInterval\n        );\n    \"\"\"\n\n    _DO_STOP_TIMER = f\"\"\"\n        const now = new Date();\n        {_PBI_TIMER_ELAPSED_TIME_MS} += (\n            now.getTime() - {_PBI_TIMER_START_DATE}.getTime()\n        );\n        {_PBI_TIMER_ELAPSED_TIME_PROPORTION} = (\n            {_PBI_TIMER_ELAPSED_TIME_MS} / {_PBI_CURR_INTERVAL_MS}\n        );\n        clearInterval({_PBI_TIMER});\n    \"\"\"\n\n    update_on_date_change_callback = CustomJS(\n        args={\"source\": bokeh_data_source},\n        code=f\"\"\"\n\n        {_SETUP_WINDOW_PLAYBACK_INFO}\n\n        const sliderValue = cb_obj.value;\n        const sliderDate = new Date(sliderValue)\n        // Ugh, actually requiring the date to be YYYY-MM-DD (matching DATE_FMT)\n        const dateStr = sliderDate.toISOString().split('T')[0]\n\n        const data = source.data;\n\n        {_PBI_TIMER_ELAPSED_TIME_MS} = 0\n\n        if (typeof(data[dateStr]) !== 'undefined') {{\n            data['{value_col}'] = data[dateStr]\n\n            const valueCol = data['{value_col}'];\n            const colorCol = data['{COLOR_COL}'];\n            const fakeDateCol = data['{FAKE_DATE_COL}']\n\n            for (var i = 0; i < data['{value_col}'].length; i++) {{\n                const value = valueCol[i]\n                if (value == 0) {{\n                    colorCol[i] = 'NaN';\n                }} else {{\n                    colorCol[i] = value;\n                }}\n\n                fakeDateCol[i] = dateStr;\n            }}\n\n            source.change.emit();\n\n        }}\n\n        \"\"\",\n    )\n\n    # Taking day-over-day diffs means the min slider day is one more than the min data\n    # date (might be off by 1 if not using day over diffs but in practice not an issue)\n    min_slider_date = min_date + pd.Timedelta(days=1)\n    date_slider = DateSlider(\n        start=min_slider_date,\n        end=max_date,\n        value=max_date,\n        step=1,\n        sizing_mode=\"stretch_width\",\n        width_policy=\"fit\",\n    )\n    date_slider.js_on_change(\"value\", update_on_date_change_callback)\n\n    play_pause_button = Toggle(\n        label=\"Start playing\",\n        button_type=\"success\",\n        active=False,\n        sizing_mode=\"stretch_width\",\n    )\n\n    animate_playback_callback = CustomJS(\n        args={\n            \"source\": bokeh_data_source,\n            \"dateSlider\": date_slider,\n            \"playPauseButton\": play_pause_button,\n            \"maxDate\": max_date,\n            \"minDate\": min_slider_date,\n        },\n        code=f\"\"\"\n\n        {_SETUP_WINDOW_PLAYBACK_INFO}\n        {_DEFFUN_INCR_DATE}\n\n        if (dateSlider.value >= maxDate) {{\n            if (playPauseButton.active) {{\n                dateSlider.value = minDate;\n                dateSlider.change.emit();\n\n                // Hack to get timer to wait after date slider wraps; any positive\n                // number works but the smaller the better\n                {_PBI_TIMER_ELAPSED_TIME_MS} = 1;\n            }}\n        }}\n\n        const active = cb_obj.active;\n        {_PBI_IS_ACTIVE} = active;\n\n        if (active) {{\n            playPauseButton.label = 'Playing \u2013 Click/tap to pause'\n            {_DO_START_TIMER}\n        }} else {{\n            playPauseButton.label = 'Paused \u2013 Click/tap to play'\n            {_DO_STOP_TIMER}\n        }}\n\n        \"\"\",\n    )\n\n    play_pause_button.js_on_click(animate_playback_callback)\n\n    change_playback_speed_callback = CustomJS(\n        args={\n            \"source\": bokeh_data_source,\n            \"dateSlider\": date_slider,\n            \"playPauseButton\": play_pause_button,\n            \"maxDate\": max_date,\n        },\n        code=f\"\"\"\n\n        {_SETUP_WINDOW_PLAYBACK_INFO}\n        {_DEFFUN_INCR_DATE}\n\n        // Must stop timer before handling changing the speed, as stopping the timer\n        // saves values based on the current (unchaged) speed selection\n        if ({_PBI_TIMER} !== null) {{\n            {_DO_STOP_TIMER}\n        }}\n\n        const selectedIndex = cb_obj.active;\n        {_PBI_SELECTED_INDEX} = selectedIndex;\n\n        if ({_PBI_IS_ACTIVE}) {{\n            {_DO_START_TIMER}\n        }} else {{\n            {_PBI_TIMER_ELAPSED_TIME_MS} = 0\n        }}\n\n        console.log({__PLAYBACK_INFO})\n\n    \"\"\",\n    )\n\n    playback_speed_radio = RadioButtonGroup(\n        labels=[f\"{speed:.2g}x speed\" for speed in _SPEED_OPTIONS],\n        active=_DEFAULT_SELECTED_INDEX,\n        sizing_mode=\"stretch_width\",\n    )\n    playback_speed_radio.js_on_click(change_playback_speed_callback)\n\n    plot_layout.append(\n        layout_column(\n            [\n                date_slider,\n                layout_row(\n                    [play_pause_button, playback_speed_radio], height_policy=\"min\",\n                ),\n            ],\n            width_policy=\"fit\",\n            height_policy=\"min\",\n        )\n    )\n    plot_layout = layout_column(plot_layout, sizing_mode=\"scale_both\")\n\n    # grid = gridplot(figures, ncols=len(count_list), sizing_mode=\"stretch_both\")\n\n    # Create the autoloading bokeh plot info (HTML + JS)\n    js_path = str(Path(out_file_basename + \"_autoload\").with_suffix(\".js\"))\n    tag_html_path = str(Path(out_file_basename + \"_div_tag\").with_suffix(\".html\"))\n\n    js_code, tag_code = autoload_static(plot_layout, CDN, js_path)\n    tag_uuid = re.search(r'id=\"([^\"]+)\"', tag_code).group(1)\n    tag_code = re.sub(r'src=\"([^\"]+)\"', f'src=\"\\\\1?uuid={tag_uuid}\"', tag_code)\n\n    with open(Paths.DOCS / js_path, \"w\") as f_js, open(\n        Paths.DOCS / tag_html_path, \"w\"\n    ) as f_html:\n        f_js.write(js_code)\n        f_html.write(tag_code)\n\n    # Create the video by creating stills of the graphs for each date and then stitching\n    # the images into a video\n    if should_make_video:\n        save_dir: Path = PNG_SAVE_ROOT_DIR / out_file_basename\n        save_dir.mkdir(parents=True, exist_ok=True)\n\n        STILL_WIDTH = 1500\n        STILL_HEIGHT = int(\n            np.ceil(STILL_WIDTH / plot_aspect_ratio) * 1.05\n        )  # Unclear why *1.05 is necessary\n        gp.height = STILL_HEIGHT\n        gp.width = STILL_WIDTH\n        gp.sizing_mode = \"fixed\"\n        orig_title = anchor_fig.title.text\n\n        for date in dates:\n            date_str = date.strftime(DATE_FMT)\n            anchor_fig.title = Title(text=f\"{orig_title} {date_str}\")\n\n            for p in figures:\n                p.title = Title(text=p.title.text, text_font_size=\"20px\")\n\n            # Just a reimplementation of the JS code in the date slider's callback\n            data = bokeh_data_source.data\n            data[value_col] = data[date_str]\n\n            for i, value in enumerate(data[value_col]):\n                if value == 0:\n                    data[COLOR_COL][i] = \"NaN\"\n                else:\n                    data[COLOR_COL][i] = value\n\n                data[FAKE_DATE_COL][i] = date_str\n\n            save_path: Path = (save_dir / date_str).with_suffix(\".png\")\n            export_png(gp, filename=save_path)\n            resize_to_even_dims(save_path, pad_bottom=0.08)\n\n            if date == max(dates):\n                poster_path: Path = (\n                    PNG_SAVE_ROOT_DIR / (out_file_basename + \"_poster\")\n                ).with_suffix(\".png\")\n                poster_path.write_bytes(save_path.read_bytes())\n\n        make_video(save_dir, out_file_basename, 0.9)\n\n    print(f\"Did interactive {out_file_basename}\")\n\n    return (js_code, tag_code)", "fn_id": 3, "class_fn": false, "repo": "rben01/covid19", "file": "src/plot_timeline_interactive.py", "last_update_at": "2020-04-24T01:40:01+00:00", "question_id": "09d742f8da13770052a2bbd09a2bb6e30f034e1a_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def __make_daybyday_interactive_timeline(df: pd.DataFrame, *, geo_df: geopandas.GeoDataFrame, value_col: str, transform_df_func: Callable[[pd.DataFrame], pd.DataFrame]=None, stage: Union[DiseaseStage, Literal[Select.ALL]]=Select.ALL, count: Union[Counting, Literal[Select.ALL]]=Select.ALL, out_file_basename: str, subplot_title_prefix: str, plot_aspect_ratio: float=None, cmap=None, n_cbar_buckets: int=None, n_buckets_btwn_major_ticks: int=None, n_minor_ticks_btwn_major_ticks: int=None, per_capita_denominator: int=None, x_range: Tuple[float, float], y_range: Tuple[float, float], min_visible_y_range: float, should_make_video: bool) -> InfoForAutoload:\n    \"\"\"Create the bokeh interactive timeline plot(s)\n\n    This function takes the given DataFrame, which must contain COVID data for locations\n    on different dates, and a GeoDataFrame, which contains the long/lat coords for those\n    locations, and creates an interactive choropleth of the COVID data over time.\n\n    :param df: The COVID data DataFrame\n    :type df: pd.DataFrame\n    :param geo_df: The geometry GeoDataFrame for the locations in `df`\n    :type geo_df: geopandas.GeoDataFrame\n    :param value_col: The column of `df` containing the values to plot in the\n    choropleth; should be something like \"Case_Counts\" or \"Case_Diff_From_Prev_Day\"\n    :type value_col: str\n    :param stage: The DiseaseStage to plot, defaults to Select.ALL. If ALL, then all\n    stages are plotted and are stacked vertically.\n    :type stage: Union[DiseaseStage, Literal[Select.ALL]], optional\n    :param count: The Counting to plot, defaults to Select.ALL. If ALL, then all\n    count types are plotted and are stacked horizontally.\n    :type count: Union[Counting, Literal[Select.ALL]], optional\n    :param out_file_basename: The basename of the file to save the interactive plots to\n    (there are two components, the JS script and the HTML <div>)\n    :type out_file_basename: str\n    :param subplot_title_prefix: What the first part of the subplot title should be;\n    probably a function of `value_col` (if value_col is \"Case_Counts\" then this param\n    might be \"Cases\" or \"# of Cases\")\n    :type subplot_title_prefix: str\n    :param x_range: The range of the x-axis as (min, max)\n    :type x_range: Tuple[float, float]\n    :param y_range: The range of the y-axis as (min, max)\n    :type y_range: Tuple[float, float]\n    :param min_visible_y_range: The minimum height (in axis units) of the y-axis; it\n    will not be possible to zoom in farther than this on the choropleth.\n    :type min_visible_y_range: float\n    :param should_make_video: Optionally run through the timeline day by day, capture\n    a screenshot for each day, and then stitch the screenshots into a video. The video\n    shows the same info as the interactive plots, but not interactively. This easily\n    takes 20x as long as just making the graphs themselves, so use with caution.\n    :type should_make_video: bool\n    :param transform_df_func: This function expects data in a certain format, and does\n    a bunch of preprocessing (expected to be common) before plotting. This gives you a\n    chance to do any customization on the postprocessed df before it's plotted. Defaults\n    to None, in which case no additional transformation is performed.\n    :type transform_df_func: Callable[[pd.DataFrame], pd.DataFrame], optional\n    :param plot_aspect_ratio: The aspect ratio of the plot as width/height; if set, the\n    aspect ratio will be fixed to this. Defaults to None, in which case the aspect ratio\n    is determined from the x_range and y_range arguments\n    :type plot_aspect_ratio: float, optional\n    :param cmap: The colormap to use as either a matplotlib-compatible colormap or a\n    list of hex strings (e.g., [\"#ae8f1c\", ...]). Defaults to None in which case a\n    reasonable default is used.\n    :type cmap: Matplotlib-compatible colormap or List[str], optional\n    :param n_cbar_buckets: How many colorbar buckets to use. Has little effect if the\n    colormap is continuous, but always works in conjunction with\n    n_buckets_btwn_major_ticks to determine the number of major ticks. Defaults to 6.\n    :type n_cbar_buckets: int, optional\n    :param n_buckets_btwn_major_ticks: How many buckets are to lie between colorbar\n    major ticks, determining how many major ticks are drawn. Defaults to 1.\n    :type n_buckets_btwn_major_ticks: int, optional\n    :param n_minor_ticks_btwn_major_ticks: How many minor ticks to draw between colorbar\n    major ticks. Defaults to 8 (which means each pair of major ticks has 10 ticks\n    total).\n    :type n_minor_ticks_btwn_major_ticks: int, optional\n    :param per_capita_denominator: When describing per-capita numbers, what to use as\n    the denominator (e.g., cases per 100,000 people). If None, it is automatically\n    computed per plot to be appropriately scaled for the data.\n    :type per_capita_denominator: int, optional\n    :raises ValueError: [description]\n    :return: The two pieces of info required to make a Bokeh autoloading HTML+JS plot:\n    the HTML div to be inserted somewhere in the HTML body, and the JS file that will\n    load the plot into that div.\n    :rtype: InfoForAutoload\n    \"\"\"\n    Counting.verify(count, allow_select=True)\n    DiseaseStage.verify(stage, allow_select=True)\n    STRING_DATE_COL = 'String_Date_'\n    FAKE_DATE_COL = 'Fake_Date_'\n    COLOR_COL = 'Color_'\n    DATE_FMT = '%Y-%m-%d'\n    ID_COLS = [REGION_NAME_COL, Columns.DATE, Columns.STAGE, Columns.COUNT_TYPE]\n    if cmap is None:\n        cmap = cmocean.cm.matter\n    if n_cbar_buckets is None:\n        n_cbar_buckets = 6\n    if n_buckets_btwn_major_ticks is None:\n        n_buckets_btwn_major_ticks = 1\n    if n_minor_ticks_btwn_major_ticks is None:\n        n_minor_ticks_btwn_major_ticks = 8\n    n_cbar_major_ticks = n_cbar_buckets // n_buckets_btwn_major_ticks + 1\n    try:\n        color_list = [RGB(*rgb).to_hex() for i, rgb in enumerate((255 * cmap(range(256))).astype('int'))]\n    except TypeError:\n        color_list = cmap\n    color_list: List[BokehColor]\n    if stage is Select.ALL:\n        stage_list = list(DiseaseStage)\n    else:\n        stage_list = [stage]\n    if count is Select.ALL:\n        count_list = list(Counting)\n    else:\n        count_list = [count]\n    stage_list: List[DiseaseStage]\n    count_list: List[Counting]\n    stage_count_list: List[Tuple[DiseaseStage, Counting]] = list(itertools.product(stage_list, count_list))\n    df = df.copy()\n    normalized_dates = df[Columns.DATE].dt.normalize()\n    is_at_midnight = df[Columns.DATE] == normalized_dates\n    df.loc[is_at_midnight, Columns.DATE] -= pd.Timedelta(days=1)\n    df.loc[~is_at_midnight, Columns.DATE] = normalized_dates[~is_at_midnight]\n    min_date, max_date = df[Columns.DATE].agg(['min', 'max'])\n    dates: List[pd.Timestamp] = pd.date_range(start=min_date, end=max_date, freq='D')\n    max_date_str = max_date.strftime(DATE_FMT)\n    id_cols_product: pd.MultiIndex = pd.MultiIndex.from_product([df[REGION_NAME_COL].unique(), dates, [s.name for s in DiseaseStage], [c.name for c in Counting]], names=ID_COLS)\n    df = id_cols_product.to_frame(index=False).merge(df, how='left', on=ID_COLS).sort_values(ID_COLS)\n    df[STRING_DATE_COL] = df[Columns.DATE].dt.strftime(DATE_FMT)\n    df[Columns.CASE_COUNT] = df[Columns.CASE_COUNT].fillna(0)\n    if transform_df_func is not None:\n        df = transform_df_func(df)\n    df = geo_df.merge(df, how='inner', on=REGION_NAME_COL)[[REGION_NAME_COL, Columns.DATE, STRING_DATE_COL, Columns.STAGE, Columns.COUNT_TYPE, value_col]]\n    dates: List[pd.Timestamp] = [pd.Timestamp(d) for d in df[Columns.DATE].unique()]\n    values_mins_maxs = df[df[value_col] > 0].groupby([Columns.STAGE, Columns.COUNT_TYPE])[value_col].agg(['min', 'max'])\n    vmins: pd.Series = values_mins_maxs['min']\n    vmaxs: pd.Series = values_mins_maxs['max']\n    pow10s_series: pd.Series = vmaxs.map(lambda x: int(10 ** (-np.floor(np.log10(x)))))\n    vmins: dict = vmins.to_dict()\n    vmaxs: dict = vmaxs.to_dict()\n    for stage in DiseaseStage:\n        _value_key = (stage.name, Counting.PER_CAPITA.name)\n        if per_capita_denominator is None:\n            _max_pow10 = pow10s_series.loc[slice(None), Counting.PER_CAPITA.name].max()\n        else:\n            _max_pow10 = per_capita_denominator\n        vmins[_value_key] *= _max_pow10\n        vmaxs[_value_key] *= _max_pow10\n        pow10s_series[_value_key] = _max_pow10\n    percap_pow10s: pd.Series = df.apply(lambda row: pow10s_series[row[Columns.STAGE], row[Columns.COUNT_TYPE]], axis=1)\n    _per_cap_rows = df[Columns.COUNT_TYPE] == Counting.PER_CAPITA.name\n    df.loc[_per_cap_rows, value_col] *= percap_pow10s.loc[_per_cap_rows]\n    df = df.pivot_table(index=[REGION_NAME_COL, Columns.STAGE, Columns.COUNT_TYPE], columns=STRING_DATE_COL, values=value_col, aggfunc='first').reset_index().merge(geo_df[[REGION_NAME_COL, LONG_COL, LAT_COL]], how='inner', on=REGION_NAME_COL)\n    df[value_col] = df[max_date_str]\n    df[FAKE_DATE_COL] = max_date_str\n    df[COLOR_COL] = np.where(df[value_col] > 0, df[value_col], 'NaN')\n    bokeh_data_source = ColumnDataSource({k: v.tolist() for k, v in df.to_dict(orient='series').items()})\n    filters = [[GroupFilter(column_name=Columns.STAGE, group=stage.name), GroupFilter(column_name=Columns.COUNT_TYPE, group=count.name)] for stage, count in stage_count_list]\n    figures = []\n    for subplot_index, (stage, count) in enumerate(stage_count_list):\n        view = CDSView(source=bokeh_data_source, filters=filters[subplot_index])\n        vmin = vmins[stage.name, count.name]\n        vmax = vmaxs[stage.name, count.name]\n        if stage is DiseaseStage.CONFIRMED:\n            fig_stage_name = 'Cases'\n        elif stage is DiseaseStage.DEATH:\n            fig_stage_name = 'Deaths'\n        else:\n            raise ValueError\n        fig_title_components: List[str] = []\n        if subplot_title_prefix is not None:\n            fig_title_components.append(subplot_title_prefix)\n        fig_title_components.append(fig_stage_name)\n        if count is Counting.PER_CAPITA:\n            _per_cap_denom = pow10s_series[stage.name, count.name]\n            fig_title_components.append(f'Per {_per_cap_denom:,d} people')\n            formatter = PrintfTickFormatter(format='%2.3f')\n            label_standoff = 12\n            tooltip_fmt = '{0.000}'\n        else:\n            formatter = NumeralTickFormatter(format='0.0a')\n            label_standoff = 10\n            tooltip_fmt = '{0}'\n        color_mapper = LogColorMapper(color_list, low=vmin, high=vmax, nan_color='#f2f2f2')\n        fig_title = ' '.join(fig_title_components)\n        if plot_aspect_ratio is None:\n            if x_range is None or y_range is None:\n                raise ValueError('Must provide both `x_range` and `y_range`' + ' when `plot_aspect_ratio` is None')\n            plot_aspect_ratio = (x_range[1] - x_range[0]) / (y_range[1] - y_range[0])\n        p = bplotting.figure(title=fig_title, title_location='above', tools=[HoverTool(tooltips=[('Date', f'@{{{FAKE_DATE_COL}}}'), ('State', f'@{{{REGION_NAME_COL}}}'), ('Count', f'@{{{value_col}}}{tooltip_fmt}')], toggleable=False), PanTool(), BoxZoomTool(match_aspect=True), ZoomInTool(), ZoomOutTool(), ResetTool()], active_drag=None, aspect_ratio=plot_aspect_ratio, output_backend='webgl', lod_factor=4, lod_interval=400, lod_threshold=1000, lod_timeout=300)\n        p.xgrid.grid_line_color = None\n        p.ygrid.grid_line_color = None\n        p.patches(LONG_COL, LAT_COL, source=bokeh_data_source, view=view, fill_color={'field': COLOR_COL, 'transform': color_mapper}, line_color='black', line_width=0.25, fill_alpha=1)\n        bucket_size = (vmax / vmin) ** (1 / n_cbar_buckets)\n        tick_dist = bucket_size ** n_buckets_btwn_major_ticks\n        major_tick_locs = vmin * tick_dist ** np.arange(0, n_cbar_major_ticks)\n        minor_tick_locs = []\n        for major_tick_index, this_major_tick in enumerate(major_tick_locs[:-1]):\n            next_major_tick = major_tick_locs[major_tick_index + 1]\n            minor_tick_locs.extend(np.linspace(this_major_tick, next_major_tick, n_minor_ticks_btwn_major_ticks + 2)[1:-1])\n        color_bar = ColorBar(color_mapper=color_mapper, ticker=FixedTicker(ticks=major_tick_locs, minor_ticks=minor_tick_locs), formatter=formatter, label_standoff=label_standoff, major_tick_out=0, major_tick_in=13, major_tick_line_color='white', major_tick_line_width=1, minor_tick_out=0, minor_tick_in=5, minor_tick_line_color='white', minor_tick_line_width=1, location=(0, 0), border_line_color=None, bar_line_color=None, orientation='vertical')\n        p.add_layout(color_bar, 'right')\n        p.hover.point_policy = 'follow_mouse'\n        p.axis.visible = False\n        figures.append(p)\n    figs_iter = iter(np.ravel(figures))\n    anchor_fig = next(figs_iter)\n    if x_range is not None and y_range is not None:\n        data_aspect_ratio = (x_range[1] - x_range[0]) / (y_range[1] - y_range[0])\n    else:\n        data_aspect_ratio = plot_aspect_ratio\n    if x_range is not None:\n        anchor_fig.x_range = Range1d(*x_range, bounds='auto', min_interval=min_visible_y_range * data_aspect_ratio)\n    if y_range is not None:\n        anchor_fig.y_range = Range1d(*y_range, bounds='auto', min_interval=min_visible_y_range)\n    for fig in figs_iter:\n        fig.x_range = anchor_fig.x_range\n        fig.y_range = anchor_fig.y_range\n    gp = gridplot(figures, ncols=len(count_list), sizing_mode='scale_both', toolbar_location='above')\n    plot_layout = [gp]\n    _THIS_PLOT_ID = uuid.uuid4().hex\n    __TIMER = \"'timer'\"\n    __IS_ACTIVE = \"'isActive'\"\n    __SELECTED_INDEX = \"'selectedIndex'\"\n    __BASE_INTERVAL_MS = \"'BASE_INTERVAL'\"\n    __TIMER_START_DATE = \"'startDate'\"\n    __TIMER_ELAPSED_TIME_MS = \"'elapsedTimeMS'\"\n    __TIMER_ELAPSED_TIME_PROPORTION = \"'elapsedTimeProportion'\"\n    __SPEEDS_KEY = \"'SPEEDS'\"\n    __PLAYBACK_INFO = f'window._playbackInfo_{_THIS_PLOT_ID}'\n    _PBI_TIMER = f'{__PLAYBACK_INFO}[{__TIMER}]'\n    _PBI_IS_ACTIVE = f'{__PLAYBACK_INFO}[{__IS_ACTIVE}]'\n    _PBI_SELECTED_INDEX = f'{__PLAYBACK_INFO}[{__SELECTED_INDEX}]'\n    _PBI_TIMER_START_DATE = f'{__PLAYBACK_INFO}[{__TIMER_START_DATE}]'\n    _PBI_TIMER_ELAPSED_TIME_MS = f'{__PLAYBACK_INFO}[{__TIMER_ELAPSED_TIME_MS}]'\n    _PBI_TIMER_ELAPSED_TIME_PROPORTION = f'{__PLAYBACK_INFO}[{__TIMER_ELAPSED_TIME_PROPORTION}]'\n    _PBI_BASE_INTERVAL = f'{__PLAYBACK_INFO}[{__BASE_INTERVAL_MS}]'\n    _PBI_SPEEDS = f'{__PLAYBACK_INFO}[{__SPEEDS_KEY}]'\n    _PBI_CURR_INTERVAL_MS = f'{_PBI_BASE_INTERVAL} / {_PBI_SPEEDS}[{_PBI_SELECTED_INDEX}]'\n    _SPEED_OPTIONS = [0.25, 0.5, 1.0, 2.0]\n    _DEFAULT_SPEED = 1.0\n    _DEFAULT_SELECTED_INDEX = _SPEED_OPTIONS.index(_DEFAULT_SPEED)\n    _SETUP_WINDOW_PLAYBACK_INFO = f\"\\n        if (typeof({__PLAYBACK_INFO}) === 'undefined') {{\\n            {__PLAYBACK_INFO} = {{\\n                {__TIMER}: null,\\n                {__IS_ACTIVE}: false,\\n                {__SELECTED_INDEX}: {_DEFAULT_SELECTED_INDEX},\\n                {__TIMER_START_DATE}: null,\\n                {__TIMER_ELAPSED_TIME_MS}: 0,\\n                {__TIMER_ELAPSED_TIME_PROPORTION}: 0,\\n                {__BASE_INTERVAL_MS}: 1000,\\n                {__SPEEDS_KEY}: {_SPEED_OPTIONS}\\n            }};\\n        }}\\n\\n    \"\n    _DEFFUN_INCR_DATE = f\"\\n        // See this link for why this works (it's an undocumented feature?)\\n        // https://discourse.bokeh.org/t/5254\\n        // Tl;dr we need this to automatically update the hover as the play button plays\\n        // Without this, the hover tooltip only updates when we jiggle the mouse\\n        // slightly\\n\\n        let prev_val = null;\\n        source.inspect.connect(v => prev_val = v);\\n\\n        function updateDate() {{\\n            {_PBI_TIMER_START_DATE} = new Date();\\n            {_PBI_TIMER_ELAPSED_TIME_MS} = 0\\n            if (dateSlider.value < maxDate) {{\\n                dateSlider.value += 86400000;\\n            }}\\n\\n            if (dateSlider.value >= maxDate) {{\\n                console.log(dateSlider.value, maxDate)\\n                console.log('reached end')\\n                clearInterval({_PBI_TIMER});\\n                {_PBI_IS_ACTIVE} = false;\\n                playPauseButton.active = false;\\n                playPauseButton.change.emit();\\n                playPauseButton.label = 'Restart';\\n            }}\\n\\n            dateSlider.change.emit();\\n\\n            // This is pt. 2 of the prev_val/inspect stuff above\\n            if (prev_val !== null) {{\\n                source.inspect.emit(prev_val);\\n            }}\\n        }}\\n    \"\n    _DO_START_TIMER = f'\\n        function startLoopTimer() {{\\n            updateDate();\\n            if ({_PBI_IS_ACTIVE}) {{\\n                {_PBI_TIMER} = setInterval(updateDate, {_PBI_CURR_INTERVAL_MS})\\n            }}\\n\\n        }}\\n\\n        {_PBI_TIMER_START_DATE} = new Date();\\n\\n        // Should never be <0 or >1 but I am being very defensive here\\n        const proportionRemaining = 1 - (\\n            {_PBI_TIMER_ELAPSED_TIME_PROPORTION} <= 0\\n            ? 0\\n            : {_PBI_TIMER_ELAPSED_TIME_PROPORTION} >= 1\\n            ? 1\\n            : {_PBI_TIMER_ELAPSED_TIME_PROPORTION}\\n        );\\n        const remainingTimeMS = (\\n            {_PBI_CURR_INTERVAL_MS} * proportionRemaining\\n        );\\n        const initialInterval = (\\n            {_PBI_TIMER_ELAPSED_TIME_MS} === 0\\n            ? 0\\n            : remainingTimeMS\\n        );\\n\\n        {_PBI_TIMER} = setTimeout(\\n            startLoopTimer,\\n            initialInterval\\n        );\\n    '\n    _DO_STOP_TIMER = f'\\n        const now = new Date();\\n        {_PBI_TIMER_ELAPSED_TIME_MS} += (\\n            now.getTime() - {_PBI_TIMER_START_DATE}.getTime()\\n        );\\n        {_PBI_TIMER_ELAPSED_TIME_PROPORTION} = (\\n            {_PBI_TIMER_ELAPSED_TIME_MS} / {_PBI_CURR_INTERVAL_MS}\\n        );\\n        clearInterval({_PBI_TIMER});\\n    '\n    update_on_date_change_callback = CustomJS(args={'source': bokeh_data_source}, code=f\"\\n\\n        {_SETUP_WINDOW_PLAYBACK_INFO}\\n\\n        const sliderValue = cb_obj.value;\\n        const sliderDate = new Date(sliderValue)\\n        // Ugh, actually requiring the date to be YYYY-MM-DD (matching DATE_FMT)\\n        const dateStr = sliderDate.toISOString().split('T')[0]\\n\\n        const data = source.data;\\n\\n        {_PBI_TIMER_ELAPSED_TIME_MS} = 0\\n\\n        if (typeof(data[dateStr]) !== 'undefined') {{\\n            data['{value_col}'] = data[dateStr]\\n\\n            const valueCol = data['{value_col}'];\\n            const colorCol = data['{COLOR_COL}'];\\n            const fakeDateCol = data['{FAKE_DATE_COL}']\\n\\n            for (var i = 0; i < data['{value_col}'].length; i++) {{\\n                const value = valueCol[i]\\n                if (value == 0) {{\\n                    colorCol[i] = 'NaN';\\n                }} else {{\\n                    colorCol[i] = value;\\n                }}\\n\\n                fakeDateCol[i] = dateStr;\\n            }}\\n\\n            source.change.emit();\\n\\n        }}\\n\\n        \")\n    min_slider_date = min_date + pd.Timedelta(days=1)\n    date_slider = DateSlider(start=min_slider_date, end=max_date, value=max_date, step=1, sizing_mode='stretch_width', width_policy='fit')\n    date_slider.js_on_change('value', update_on_date_change_callback)\n    play_pause_button = Toggle(label='Start playing', button_type='success', active=False, sizing_mode='stretch_width')\n    animate_playback_callback = CustomJS(args={'source': bokeh_data_source, 'dateSlider': date_slider, 'playPauseButton': play_pause_button, 'maxDate': max_date, 'minDate': min_slider_date}, code=f\"\\n\\n        {_SETUP_WINDOW_PLAYBACK_INFO}\\n        {_DEFFUN_INCR_DATE}\\n\\n        if (dateSlider.value >= maxDate) {{\\n            if (playPauseButton.active) {{\\n                dateSlider.value = minDate;\\n                dateSlider.change.emit();\\n\\n                // Hack to get timer to wait after date slider wraps; any positive\\n                // number works but the smaller the better\\n                {_PBI_TIMER_ELAPSED_TIME_MS} = 1;\\n            }}\\n        }}\\n\\n        const active = cb_obj.active;\\n        {_PBI_IS_ACTIVE} = active;\\n\\n        if (active) {{\\n            playPauseButton.label = 'Playing \u2013 Click/tap to pause'\\n            {_DO_START_TIMER}\\n        }} else {{\\n            playPauseButton.label = 'Paused \u2013 Click/tap to play'\\n            {_DO_STOP_TIMER}\\n        }}\\n\\n        \")\n    play_pause_button.js_on_click(animate_playback_callback)\n    change_playback_speed_callback = CustomJS(args={'source': bokeh_data_source, 'dateSlider': date_slider, 'playPauseButton': play_pause_button, 'maxDate': max_date}, code=f'\\n\\n        {_SETUP_WINDOW_PLAYBACK_INFO}\\n        {_DEFFUN_INCR_DATE}\\n\\n        // Must stop timer before handling changing the speed, as stopping the timer\\n        // saves values based on the current (unchaged) speed selection\\n        if ({_PBI_TIMER} !== null) {{\\n            {_DO_STOP_TIMER}\\n        }}\\n\\n        const selectedIndex = cb_obj.active;\\n        {_PBI_SELECTED_INDEX} = selectedIndex;\\n\\n        if ({_PBI_IS_ACTIVE}) {{\\n            {_DO_START_TIMER}\\n        }} else {{\\n            {_PBI_TIMER_ELAPSED_TIME_MS} = 0\\n        }}\\n\\n        console.log({__PLAYBACK_INFO})\\n\\n    ')\n    playback_speed_radio = RadioButtonGroup(labels=[f'{speed:.2g}x speed' for speed in _SPEED_OPTIONS], active=_DEFAULT_SELECTED_INDEX, sizing_mode='stretch_width')\n    playback_speed_radio.js_on_click(change_playback_speed_callback)\n    plot_layout.append(layout_column([date_slider, layout_row([play_pause_button, playback_speed_radio], height_policy='min')], width_policy='fit', height_policy='min'))\n    plot_layout = layout_column(plot_layout, sizing_mode='scale_both')\n    js_path = str(Path(out_file_basename + '_autoload').with_suffix('.js'))\n    tag_html_path = str(Path(out_file_basename + '_div_tag').with_suffix('.html'))\n    js_code, tag_code = autoload_static(plot_layout, CDN, js_path)\n    tag_uuid = re.search('id=\"([^\"]+)\"', tag_code).group(1)\n    tag_code = re.sub('src=\"([^\"]+)\"', f'src=\"\\\\1?uuid={tag_uuid}\"', tag_code)\n    with open(Paths.DOCS / js_path, 'w') as f_js, open(Paths.DOCS / tag_html_path, 'w') as f_html:\n        f_js.write(js_code)\n        f_html.write(tag_code)\n    if should_make_video:\n        save_dir: Path = PNG_SAVE_ROOT_DIR / out_file_basename\n        save_dir.mkdir(parents=True, exist_ok=True)\n        STILL_WIDTH = 1500\n        STILL_HEIGHT = int(np.ceil(STILL_WIDTH / plot_aspect_ratio) * 1.05)\n        gp.height = STILL_HEIGHT\n        gp.width = STILL_WIDTH\n        gp.sizing_mode = 'fixed'\n        orig_title = anchor_fig.title.text\n        for date in dates:\n            date_str = date.strftime(DATE_FMT)\n            anchor_fig.title = Title(text=f'{orig_title} {date_str}')\n            for p in figures:\n                p.title = Title(text=p.title.text, text_font_size='20px')\n            data = bokeh_data_source.data\n            data[value_col] = data[date_str]\n            for i, value in enumerate(data[value_col]):\n                if value == 0:\n                    data[COLOR_COL][i] = 'NaN'\n                else:\n                    data[COLOR_COL][i] = value\n                data[FAKE_DATE_COL][i] = date_str\n            save_path: Path = (save_dir / date_str).with_suffix('.png')\n            export_png(gp, filename=save_path)\n            resize_to_even_dims(save_path, pad_bottom=0.08)\n            if date == max(dates):\n                poster_path: Path = (PNG_SAVE_ROOT_DIR / (out_file_basename + '_poster')).with_suffix('.png')\n                poster_path.write_bytes(save_path.read_bytes())\n        make_video(save_dir, out_file_basename, 0.9)\n    print(f'Did interactive {out_file_basename}')\n"]]}
{"hexsha": "d81a96d9bba7bb461e51e7310f24c9106ee37c39", "ext": "py", "lang": "Python", "content": "def build_rowid_blocks(Zvr):\n    A = np.asarray(Zvr).T\n    U = map(tuple, A)\n    return {u:np.where(np.all(A==u, axis=1))[0] for u in U}", "fn_id": 19, "class_fn": false, "repo": "vishalbelsare/cgpm", "file": "src/utils/general.py", "last_update_at": "2020-10-28T14:16:42+00:00", "question_id": "d81a96d9bba7bb461e51e7310f24c9106ee37c39_19", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def build_rowid_blocks(Zvr):\n    A = np.asarray(Zvr).T\n    U = map(tuple, A)\n"]]}
{"hexsha": "153d99f089c9ae1ca9c52bc4b10adfc64105fd5b", "ext": "py", "lang": "Python", "content": "def generate_initial_random_policy(env):\n    policy = dict()\n\n    for i in range(GRID_HEIGHT):\n        for j in range(GRID_WIDTH):\n            if (i, j) not in env.TERMINAL_STATES:\n                actions = []\n                prob = []\n                for action in env.ACTIONS:\n                    actions.append(action)\n                    prob.append(0.25)\n\n                policy[(i, j)] = (actions, prob)\n\n    return policy", "fn_id": 2, "class_fn": false, "repo": "linklab/e_learning_rl", "file": "basic2/practice_4/cliff_dyna_q.py", "last_update_at": "2020-11-22T06:30:45+00:00", "question_id": "153d99f089c9ae1ca9c52bc4b10adfc64105fd5b_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def generate_initial_random_policy(env):\n    policy = dict()\n    for i in range(GRID_HEIGHT):\n        for j in range(GRID_WIDTH):\n            if (i, j) not in env.TERMINAL_STATES:\n                actions = []\n                prob = []\n                for action in env.ACTIONS:\n                    actions.append(action)\n                    prob.append(0.25)\n                policy[i, j] = (actions, prob)\n"]]}
{"hexsha": "87fd0a978d1cf015b580c9ad87216b134879573c", "ext": "py", "lang": "Python", "content": "def storagepool_register(args):\n    obj = StoragePool(args.ip, args.port)\n\n    try:\n        res = obj.storagepool_register_main(args.serialnumber,\n                                            args.type, args.name)\n\n    except SOSError as e:\n        if(e.err_code == SOSError.NOT_FOUND_ERR):\n            raise SOSError(SOSError.NOT_FOUND_ERR,\n                           \"Storagepool register failed: \"\n                           + e.err_text)\n        else:\n            raise e", "fn_id": 3, "class_fn": false, "repo": "CoprHD/sds-controller", "file": "cli/src/storagepool.py", "last_update_at": "2020-11-24T07:26:40+00:00", "question_id": "87fd0a978d1cf015b580c9ad87216b134879573c_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def storagepool_register(args):\n    obj = StoragePool(args.ip, args.port)\n    try:\n        res = obj.storagepool_register_main(args.serialnumber, args.type, args.name)\n    except SOSError as e:\n        if e.err_code == SOSError.NOT_FOUND_ERR:\n            raise SOSError(SOSError.NOT_FOUND_ERR, 'Storagepool register failed: ' + e.err_text)\n        else:\n"]]}
{"hexsha": "8c553947537e9aee480fe515a38baf2b5377262a", "ext": "py", "lang": "Python", "content": "def publish(temp,hum):\n\tAPI_KEY = 'D9EBWUZRNK5VQRPS' \n\tHOST = 'api.thingspeak.com'\n\tdata = b\"api_key=\"+ API_KEY + \"&field1=\" + str(temp) + \"&field2=\" + str(hum)\n\ts=_socket.socket()\n\tai = _socket.getaddrinfo(HOST, 443)\n\taddr = ai[0][-1]\n\ts.connect(addr)\n\ts = ssl.wrap_socket(s)  \n\ts.write(\"POST /update HTTP/1.0\\r\\n\")\n\ts.write(\"Host: \" + HOST + \"\\r\\n\")\n\ts.write(\"Content-Length: \" + str(len(data)) + \"\\r\\n\\r\\n\")\n\ts.write(data)\n\ts.close()", "fn_id": 2, "class_fn": false, "repo": "miguel5612/Medidor-de-temperatura-ambiente-", "file": "publishThingspeak.py", "last_update_at": "2020-03-08T02:26:59+00:00", "question_id": "8c553947537e9aee480fe515a38baf2b5377262a_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def publish(temp, hum):\n    API_KEY = 'D9EBWUZRNK5VQRPS'\n    HOST = 'api.thingspeak.com'\n    data = b'api_key=' + API_KEY + '&field1=' + str(temp) + '&field2=' + str(hum)\n    s = _socket.socket()\n    ai = _socket.getaddrinfo(HOST, 443)\n    addr = ai[0][-1]\n    s.connect(addr)\n    s = ssl.wrap_socket(s)\n    s.write('POST /update HTTP/1.0\\r\\n')\n    s.write('Host: ' + HOST + '\\r\\n')\n    s.write('Content-Length: ' + str(len(data)) + '\\r\\n\\r\\n')\n    s.write(data)\n"]]}
{"hexsha": "817fda110457850c41c112c92508235db682bda2", "ext": "py", "lang": "Python", "content": "def test_beam_with_open_issues_should_not_be_vacuumed(\n    eager_celery, db_session, create_beam, expired_beam_date, issue\n):\n    beam = create_beam(start=expired_beam_date, completed=True)\n    beam.issues.append(issue)\n    db_session.commit()\n    vacuum.delay()\n    assert not is_vacuumed(db_session, beam)", "fn_id": 5, "class_fn": false, "repo": "getslash/scotty", "file": "unittests/test_tasks.py", "last_update_at": "2020-06-05T19:10:23+00:00", "question_id": "817fda110457850c41c112c92508235db682bda2_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_beam_with_open_issues_should_not_be_vacuumed(eager_celery, db_session, create_beam, expired_beam_date, issue):\n    beam = create_beam(start=expired_beam_date, completed=True)\n    beam.issues.append(issue)\n    db_session.commit()\n    vacuum.delay()\n"]]}
{"hexsha": "76e22b95c378d69dee27d9c9b5652c34f4c41680", "ext": "py", "lang": "Python", "content": "def supports_c_sizeof(conf, mandatory=True):\n  '''\n     Check for F2008 c_sizeof support.\n  '''\n  fcenv = conf.env.derive()\n  fcenv.detach()\n\n  conf.check_fc( fragment = '''\nprogram check_c_sizeof\n  use, intrinsic :: iso_c_binding, only: c_sizeof, c_float\n  implicit none\n  real(kind=c_float) :: a_real\n  integer :: realsize\n  realsize = c_sizeof(a_real)\n  write(*,*) realsize\nend program check_c_sizeof\n''',\n                 msg = 'Checking for F2008 c_sizeof support',\n                 mandatory = mandatory, define_name='c_sizeof')\n  fcenv['fortsupp_c_sizeof'] = conf.is_defined('c_sizeof')\n\n  conf.env = fcenv", "fn_id": 4, "class_fn": false, "repo": "haraldkl/Aotus", "file": "fortran_language.py", "last_update_at": "2020-11-28T21:47:13+00:00", "question_id": "76e22b95c378d69dee27d9c9b5652c34f4c41680_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def supports_c_sizeof(conf, mandatory=True):\n    \"\"\"\n     Check for F2008 c_sizeof support.\n  \"\"\"\n    fcenv = conf.env.derive()\n    fcenv.detach()\n    conf.check_fc(fragment='\\nprogram check_c_sizeof\\n  use, intrinsic :: iso_c_binding, only: c_sizeof, c_float\\n  implicit none\\n  real(kind=c_float) :: a_real\\n  integer :: realsize\\n  realsize = c_sizeof(a_real)\\n  write(*,*) realsize\\nend program check_c_sizeof\\n', msg='Checking for F2008 c_sizeof support', mandatory=mandatory, define_name='c_sizeof')\n    fcenv['fortsupp_c_sizeof'] = conf.is_defined('c_sizeof')\n"]]}
{"hexsha": "fe824b9c43aecc8116bb749db31107476d4f2f42", "ext": "py", "lang": "Python", "content": "def generate_batches(dataset, batch_size, shuffle=True,\n                     drop_last=True, device=\"cpu\"):\n    \"\"\"\n    A generator function which wraps the PyTorch DataLoader. It will \n      ensure each tensor is on the write device location.\n    \"\"\"\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n                            shuffle=shuffle, drop_last=drop_last)\n\n    for data_dict in dataloader:\n        out_data_dict = {}\n        for name, tensor in data_dict.items():\n            out_data_dict[name] = data_dict[name].to(device)\n        yield out_data_dict", "fn_id": 0, "class_fn": false, "repo": "MarcosMota/SentimentAnalysisYelp", "file": "model/train.py", "last_update_at": "2020-10-31T22:24:31+00:00", "question_id": "fe824b9c43aecc8116bb749db31107476d4f2f42_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n    \"\"\"\n    A generator function which wraps the PyTorch DataLoader. It will \n      ensure each tensor is on the write device location.\n    \"\"\"\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n    for data_dict in dataloader:\n        out_data_dict = {}\n        for name, tensor in data_dict.items():\n            out_data_dict[name] = data_dict[name].to(device)\n"]]}
{"hexsha": "73de06f4ec2d77ed4ca06cc5612ef4cb7bdec7ac", "ext": "py", "lang": "Python", "content": "@tf.function\ndef wavegan_loss(gen, disc, x, z):\n    G_z = gen(z)\n    D_x = disc(x)\n    D_G_z = disc(G_z)\n    gen_loss_one = -tf.reduce_mean(D_G_z) #Expected value\n    gen_loss = tf.reduce_mean(D_x) - tf.reduce_mean(D_G_z)\n    disc_loss = -gen_loss\n\n    # Gradient penalty\n    epsilon = tf.random.uniform([hyperparams['batch_size'], 1, 1], minval=0., maxval=1.)\n    x_hat = epsilon * x + (1 - epsilon) * G_z #batch x 16384 x 1\n    with tf.GradientTape() as tape:\n        tape.watch(x_hat)\n        y = disc(x_hat) # 64 x 1\n    dy_d_x_hat = tape.gradient(y, x_hat) #batch x 16384 x 1\n    slopes = tf.sqrt(tf.compat.v1.reduce_sum(tf.square(dy_d_x_hat), reduction_indices=[1, 2]))\n    gp = hyperparams['wgan_gp_lambda'] * tf.reduce_mean((slopes - 1.) ** 2)\n    return gen_loss, disc_loss + gp, gen_loss_one", "fn_id": 2, "class_fn": false, "repo": "MaxHolmberg96/WaveGAN", "file": "run.py", "last_update_at": "2020-11-22T15:05:32+00:00", "question_id": "73de06f4ec2d77ed4ca06cc5612ef4cb7bdec7ac_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@tf.function\ndef wavegan_loss(gen, disc, x, z):\n    G_z = gen(z)\n    D_x = disc(x)\n    D_G_z = disc(G_z)\n    gen_loss_one = -tf.reduce_mean(D_G_z)\n    gen_loss = tf.reduce_mean(D_x) - tf.reduce_mean(D_G_z)\n    disc_loss = -gen_loss\n    epsilon = tf.random.uniform([hyperparams['batch_size'], 1, 1], minval=0.0, maxval=1.0)\n    x_hat = epsilon * x + (1 - epsilon) * G_z\n    with tf.GradientTape() as tape:\n        tape.watch(x_hat)\n        y = disc(x_hat)\n    dy_d_x_hat = tape.gradient(y, x_hat)\n    slopes = tf.sqrt(tf.compat.v1.reduce_sum(tf.square(dy_d_x_hat), reduction_indices=[1, 2]))\n    gp = hyperparams['wgan_gp_lambda'] * tf.reduce_mean((slopes - 1.0) ** 2)\n"]]}
{"hexsha": "2a556969e43c4a0ef11b142419ac0b2f9e28455d", "ext": "py", "lang": "Python", "content": "def fold_lines(lines, cols, linespace, word_warp):\n    filtered = [line for line in lines if linespace == False or len(lines) > 0]\n    out = []\n    init = True\n    for line in filtered:\n        if init:\n            init = False\n        elif linespace:\n            out.append(to_attributed_line(''))\n        for l in fold_line(line, cols, word_warp):\n            out.append(to_attributed_line(l))\n    return out", "fn_id": 5, "class_fn": false, "repo": "yhirose/immersion", "file": "immersion.py", "last_update_at": "2020-01-30T11:17:59+00:00", "question_id": "2a556969e43c4a0ef11b142419ac0b2f9e28455d_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def fold_lines(lines, cols, linespace, word_warp):\n    filtered = [line for line in lines if linespace == False or len(lines) > 0]\n    out = []\n    init = True\n    for line in filtered:\n        if init:\n            init = False\n        elif linespace:\n            out.append(to_attributed_line(''))\n        for l in fold_line(line, cols, word_warp):\n            out.append(to_attributed_line(l))\n"]]}
{"hexsha": "3861303222e76fe1f05361773256708f65fd81c8", "ext": "py", "lang": "Python", "content": "def elems_in_dir(path):\n    if not os.path.exists(path):\n        return 0\n    return len(os.listdir(path))", "fn_id": 0, "class_fn": false, "repo": "project-azorian/deploy-ostree", "file": "tests/integration/test_sysroot.py", "last_update_at": "2020-11-19T08:06:47+00:00", "question_id": "3861303222e76fe1f05361773256708f65fd81c8_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def elems_in_dir(path):\n    if not os.path.exists(path):\n        return 0\n"]]}
{"hexsha": "a27cc4ec14928624b14756f50405a6e45c0b9530", "ext": "py", "lang": "Python", "content": "def ask(message, strict=False, quiet=False):\n    if quiet:\n        return True\n    yesno = 'YES/NO' if strict else 'y/N'\n    negatives = ('NO', 'N', 'n', 'no', '')\n    affirmatives = ('YES',) if strict else ('y', 'Y', 'yes')\n    acceptable_options = affirmatives + negatives\n\n    response = ask_option(message, yesno.split('/'), acceptable_options)\n    return response in affirmatives", "fn_id": 0, "class_fn": false, "repo": "dirkdejager/commcare-cloud", "file": "src/commcare_cloud/cli_utils.py", "last_update_at": "2020-08-17T13:39:06+00:00", "question_id": "a27cc4ec14928624b14756f50405a6e45c0b9530_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ask(message, strict=False, quiet=False):\n    if quiet:\n        return True\n    yesno = 'YES/NO' if strict else 'y/N'\n    negatives = ('NO', 'N', 'n', 'no', '')\n    affirmatives = ('YES',) if strict else ('y', 'Y', 'yes')\n    acceptable_options = affirmatives + negatives\n    response = ask_option(message, yesno.split('/'), acceptable_options)\n"]]}
{"hexsha": "489067cb013ca441b8482a649f2bc350bc5740d6", "ext": "py", "lang": "Python", "content": "def printBanner():\n    print(BLUE + BOLD + '                    ____     ____')\n    print(\"                  /'    |   |    \\\\\")\n    print('                /    /  |   | \\   \\\\')\n    print('              /    / |  |   |  \\   \\\\')\n    print('             (   /   |  \"\"\"\"   |\\   \\ ' + END + BOLD + '   Ahhh HA!!!')\n    print(BLUE + BOLD + '             | /   / /^\\    /^\\  \\  _| ' + END + BOLD + '       I love big big carrot!!!')\n    print(BLUE + BOLD + '              ~   | |   |  |   | | ~')\n    print('                  | |__' + END + BOLD + 'O' + BLUE + BOLD + '|__|' + END + BOLD + 'O' + BLUE + BOLD + '__| |')\n    print('                /~~      \\/     ~~\\\\')\n    print('               /   (      |      )  \\\\')\n    print(\"         _--_  /,   \\____/^\\___/'   \\  _--_\")\n    print('       /~    ~\\ / -____-|_|_|-____-\\ /~    ~\\\\')\n    print('     /________|___/~~~~\\___/~~~~\\ __|________\\\\')\n    print('--~~~          ^ |     |   |     |  -     :  ~~~~~:~-_     ___-----~~~~~~~~|')\n    print(\"   /             `^-^-^'   `^-^-^'                  :  ~\\ /'   ____/--------|\")\n    print(' ;                                    :              :    |----------/--------|')\n    print(':          ,   ' + VIOLET + BOLD + 'ChubbyListener:' + RED + BOLD + ' v1.0.0' + BLUE + BOLD + '             ;    .  |---\\\\--------------|')\n    print(' :     -      ' + VIOLET + BOLD + 'Writen by:' + RED + BOLD + ' 0BL1V10N V01D' + BLUE + BOLD + ' .              : : |______________-__|')\n    print(\"  :              ,                 ,                :   /'~----___________|\")\n    print('__  \\\\\\        ^                          ,, ;; ;; ;._-~')\n    print('  ~~~-----____________________________________----~~~')", "fn_id": 0, "class_fn": false, "repo": "0BL1V10N-V01D/ChubbyBunny", "file": "server.py", "last_update_at": "2020-05-18T21:59:38+00:00", "question_id": "489067cb013ca441b8482a649f2bc350bc5740d6_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def printBanner():\n    print(BLUE + BOLD + '                    ____     ____')\n    print(\"                  /'    |   |    \\\\\")\n    print('                /    /  |   | \\\\   \\\\')\n    print('              /    / |  |   |  \\\\   \\\\')\n    print('             (   /   |  \"\"\"\"   |\\\\   \\\\ ' + END + BOLD + '   Ahhh HA!!!')\n    print(BLUE + BOLD + '             | /   / /^\\\\    /^\\\\  \\\\  _| ' + END + BOLD + '       I love big big carrot!!!')\n    print(BLUE + BOLD + '              ~   | |   |  |   | | ~')\n    print('                  | |__' + END + BOLD + 'O' + BLUE + BOLD + '|__|' + END + BOLD + 'O' + BLUE + BOLD + '__| |')\n    print('                /~~      \\\\/     ~~\\\\')\n    print('               /   (      |      )  \\\\')\n    print(\"         _--_  /,   \\\\____/^\\\\___/'   \\\\  _--_\")\n    print('       /~    ~\\\\ / -____-|_|_|-____-\\\\ /~    ~\\\\')\n    print('     /________|___/~~~~\\\\___/~~~~\\\\ __|________\\\\')\n    print('--~~~          ^ |     |   |     |  -     :  ~~~~~:~-_     ___-----~~~~~~~~|')\n    print(\"   /             `^-^-^'   `^-^-^'                  :  ~\\\\ /'   ____/--------|\")\n    print(' ;                                    :              :    |----------/--------|')\n    print(':          ,   ' + VIOLET + BOLD + 'ChubbyListener:' + RED + BOLD + ' v1.0.0' + BLUE + BOLD + '             ;    .  |---\\\\--------------|')\n    print(' :     -      ' + VIOLET + BOLD + 'Writen by:' + RED + BOLD + ' 0BL1V10N V01D' + BLUE + BOLD + ' .              : : |______________-__|')\n    print(\"  :              ,                 ,                :   /'~----___________|\")\n    print('__  \\\\\\\\        ^                          ,, ;; ;; ;._-~')\n"]]}
{"hexsha": "335382bd5fe33266872070ff2a061a4b3d32104c", "ext": "py", "lang": "Python", "content": "def _info_from_first_line(line):\n    \"\"\"\n    Gets the info from the first line of landmarks' txt. The format of the file\n    is hardcoded for now, e.g. the expected numbers and fields.\n    It returns a dictionary, which enables future extensions of what is returned.\n\n    Along with the functions _from_line_to_vec, from_txt_to_numpy_points, and\n    access_ln_frame they are the functions to access the single txt (with sparse\n    landmarks) per video.\n    Unless you know how to call the function, please avoid calling it directly, it is used\n    internally by the from_txt_to_numpy_points().\n    :param line:\n    :return: Dict with meta-data.\n    \"\"\"\n    info = {}\n    # # get the framename of the first (assumed int!)\n    info['init_framename'] = int(line[line.find(':') + 1: line.find('\\t')])\n    # # get the number of frames.\n    line1 = line[line.find('n_frames:'):]\n    info['n_frames'] = int(line1[9: line1.find('\\t')])\n    # # get the number of landmarks.\n    line2 = line[line.find('n_landmarks:'):]\n    info['n_landm'] = int(line2[12: line2.find('\\n')])\n    return info", "fn_id": 9, "class_fn": false, "repo": "grigorisg9gr/pyutils", "file": "research_pyutils/menpo_related.py", "last_update_at": "2020-01-08T10:37:06+00:00", "question_id": "335382bd5fe33266872070ff2a061a4b3d32104c_9", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _info_from_first_line(line):\n    \"\"\"\n    Gets the info from the first line of landmarks' txt. The format of the file\n    is hardcoded for now, e.g. the expected numbers and fields.\n    It returns a dictionary, which enables future extensions of what is returned.\n\n    Along with the functions _from_line_to_vec, from_txt_to_numpy_points, and\n    access_ln_frame they are the functions to access the single txt (with sparse\n    landmarks) per video.\n    Unless you know how to call the function, please avoid calling it directly, it is used\n    internally by the from_txt_to_numpy_points().\n    :param line:\n    :return: Dict with meta-data.\n    \"\"\"\n    info = {}\n    info['init_framename'] = int(line[line.find(':') + 1:line.find('\\t')])\n    line1 = line[line.find('n_frames:'):]\n    info['n_frames'] = int(line1[9:line1.find('\\t')])\n    line2 = line[line.find('n_landmarks:'):]\n    info['n_landm'] = int(line2[12:line2.find('\\n')])\n"]]}
{"hexsha": "648e3527d02cb41be6ee1553226a814bc0e72991", "ext": "py", "lang": "Python", "content": "def crt_table_dbs(db_name, sql):\n    \"\"\"\n    \u8fde\u63a5\u6570\u636e\u5e93\uff0c\u82e5\u4e0d\u5b58\u5728\u5219\u521b\u5efa\uff0c\u521b\u5efa\u8868\n    :param db_name: 'test.db'\n    :param sql: CREATE TABLE COMPANY\n                (ID INT PRIMARY KEY     NOT NULL,\n                NAME           TEXT    NOT NULL);\n    :return: void\n    \"\"\"\n    debug_print(\"crt_table_dbs\")\n    conn = sqlite3.connect(db_name)\n    c = conn.cursor()\n    c.execute(sql)\n    debug_print(\"Table created successfully\")\n    conn.commit()\n    conn.close()", "fn_id": 4, "class_fn": false, "repo": "StuRuby/python-starter", "file": "dataGet.py", "last_update_at": "2020-09-28T17:31:13+00:00", "question_id": "648e3527d02cb41be6ee1553226a814bc0e72991_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def crt_table_dbs(db_name, sql):\n    \"\"\"\n    \u8fde\u63a5\u6570\u636e\u5e93\uff0c\u82e5\u4e0d\u5b58\u5728\u5219\u521b\u5efa\uff0c\u521b\u5efa\u8868\n    :param db_name: 'test.db'\n    :param sql: CREATE TABLE COMPANY\n                (ID INT PRIMARY KEY     NOT NULL,\n                NAME           TEXT    NOT NULL);\n    :return: void\n    \"\"\"\n    debug_print('crt_table_dbs')\n    conn = sqlite3.connect(db_name)\n    c = conn.cursor()\n    c.execute(sql)\n    debug_print('Table created successfully')\n    conn.commit()\n"]]}
{"hexsha": "656a48b4f8e679db86c0772a59cd008ea8429109", "ext": "py", "lang": "Python", "content": "@dirichlet\ndef _zero_boundary(t, locations):\n  del t, locations\n  return 0.0", "fn_id": 0, "class_fn": false, "repo": "mirca/tf-quant-finance", "file": "tf_quant_finance/experimental/pde_v2/steppers/multidim_parabolic_equation_stepper_test.py", "last_update_at": "2020-02-20T07:31:42+00:00", "question_id": "656a48b4f8e679db86c0772a59cd008ea8429109_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@dirichlet\ndef _zero_boundary(t, locations):\n    del t, locations\n"]]}
{"hexsha": "f7c307658c3455c313677d269a8648f6697b4a82", "ext": "py", "lang": "Python", "content": "def encode_key(key: str) -> str:\n    \"\"\" convert to a file-stystem safe representation of a URL \"\"\"\n\n    result = re.sub(\"https?:.+/\", \"\", key)\n    result = re.sub(\"[/?=&]\", \"_\", result)\n    result = result.replace(\".aspx\", \".html\")\n    return result", "fn_id": 0, "class_fn": false, "repo": "jasonlcrane/urlwatch-proxies", "file": "cache.py", "last_update_at": "2020-11-04T16:54:57+00:00", "question_id": "f7c307658c3455c313677d269a8648f6697b4a82_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def encode_key(key: str) -> str:\n    \"\"\" convert to a file-stystem safe representation of a URL \"\"\"\n    result = re.sub('https?:.+/', '', key)\n    result = re.sub('[/?=&]', '_', result)\n    result = result.replace('.aspx', '.html')\n"]]}
{"hexsha": "7c2914bd86e1bf64d5c886500d73b7b98ed085a2", "ext": "py", "lang": "Python", "content": "@pytest.mark.unit\ndef test_check_url_good(client):\n    response = client.post(\n        fileinfo_url,\n        json={\n            \"url\": \"https://s3.amazonaws.com/testfiles.oceanprotocol.com/info.0.json\",\n            \"type\": \"url\",\n            \"method\": \"GET\",\n        },\n    )\n    result = response.get_json()\n    assert response.status == \"200 OK\"\n    for file_info in result:\n        assert file_info[\"contentLength\"] == \"1161\"\n        assert file_info[\"contentType\"] == \"application/json\"\n        assert file_info[\"valid\"] is True", "fn_id": 1, "class_fn": false, "repo": "oceanprotocol/provider-service-py", "file": "tests/test_fileinfo.py", "last_update_at": "2020-10-27T07:30:06+00:00", "question_id": "7c2914bd86e1bf64d5c886500d73b7b98ed085a2_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.unit\ndef test_check_url_good(client):\n    response = client.post(fileinfo_url, json={'url': 'https://s3.amazonaws.com/testfiles.oceanprotocol.com/info.0.json', 'type': 'url', 'method': 'GET'})\n    result = response.get_json()\n    assert response.status == '200 OK'\n    for file_info in result:\n        assert file_info['contentLength'] == '1161'\n        assert file_info['contentType'] == 'application/json'\n"]]}
{"hexsha": "45ada3a47f33c093e6660ca1de89f92fa8c1c7ca", "ext": "py", "lang": "Python", "content": "def _switch(db, dbid, row, keys, **kwargs):\n    j = _entity(db, dbid, row, entity_keys)\n    j.update(_copy_kv(row, keys))\n    return j, None", "fn_id": 76, "class_fn": false, "repo": "Vadman97/HawkProxy", "file": "entity/entity_manager.py", "last_update_at": "2020-07-24T00:31:05+00:00", "question_id": "45ada3a47f33c093e6660ca1de89f92fa8c1c7ca_76", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _switch(db, dbid, row, keys, **kwargs):\n    j = _entity(db, dbid, row, entity_keys)\n    j.update(_copy_kv(row, keys))\n"]]}
{"hexsha": "4aa4c0e26f8b9c3e062654d27b8693d160af92d1", "ext": "py", "lang": "Python", "content": "def _render_option(option, create_fun):\n    if option.render_context is None:\n        create_fun(option.source)\n    rendered = option.value\n    return rendered", "fn_id": 4, "class_fn": false, "repo": "VDBWRAIR/pyjip", "file": "jip/pipelines.py", "last_update_at": "2020-11-30T10:51:36+00:00", "question_id": "4aa4c0e26f8b9c3e062654d27b8693d160af92d1_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _render_option(option, create_fun):\n    if option.render_context is None:\n        create_fun(option.source)\n    rendered = option.value\n"]]}
{"hexsha": "dbdec1ad3f0ea63990d793307077e1b18001ef46", "ext": "py", "lang": "Python", "content": "def print_mat(n):\n    maxlen = 0\n    for row in n:\n        for f in row:\n            if len(str(f)) > maxlen:\n                maxlen = len(str(f))\n    for row in n:\n        row_str = \"\"\n        for f in row:\n            difflen = maxlen - len(str(f))\n            sep = \" \"\n            for i in range(difflen):\n                sep = sep + \" \"\n            row_str = row_str + sep + str(f)\n        print(row_str)", "fn_id": 2, "class_fn": false, "repo": "JazzzFM/Latticed-Manim", "file": "manimlib/Laticed.py", "last_update_at": "2020-03-31T19:03:08+00:00", "question_id": "dbdec1ad3f0ea63990d793307077e1b18001ef46_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def print_mat(n):\n    maxlen = 0\n    for row in n:\n        for f in row:\n            if len(str(f)) > maxlen:\n                maxlen = len(str(f))\n    for row in n:\n        row_str = ''\n        for f in row:\n            difflen = maxlen - len(str(f))\n            sep = ' '\n            for i in range(difflen):\n                sep = sep + ' '\n            row_str = row_str + sep + str(f)\n"]]}
{"hexsha": "680b55d43b4cf5b4b13423a4802a180b441ec7a4", "ext": "py", "lang": "Python", "content": "def test_no_sample():\n    \"\"\"Expect element to pass empty sample to next element.\"\"\"\n    config = _fall_detect_config()\n    result = 'Something'\n\n    def sample_callback(image=None, inference_result=None, **kwargs):\n        nonlocal result\n        result = image is None and inference_result is None\n    fall_detector = FallDetector(**config)\n    output = _OutPipeElement(sample_callback=sample_callback)\n    fall_detector.connect_to_next_element(output)\n    fall_detector.receive_next_sample()\n    assert result is True", "fn_id": 6, "class_fn": false, "repo": "will300/ambianic-edge", "file": "tests/pipeline/ai/test_fall_detect.py", "last_update_at": "2020-09-18T00:42:00+00:00", "question_id": "680b55d43b4cf5b4b13423a4802a180b441ec7a4_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_no_sample():\n    \"\"\"Expect element to pass empty sample to next element.\"\"\"\n    config = _fall_detect_config()\n    result = 'Something'\n\n    def sample_callback(image=None, inference_result=None, **kwargs):\n        nonlocal result\n        result = image is None and inference_result is None\n    fall_detector = FallDetector(**config)\n    output = _OutPipeElement(sample_callback=sample_callback)\n    fall_detector.connect_to_next_element(output)\n    fall_detector.receive_next_sample()\n"]]}
{"hexsha": "79347a15010591124840a4abc3e1eface25a91c5", "ext": "py", "lang": "Python", "content": "def make_command(list, module_index, routine_count=-1, make_file=False):\n    if routine_count > 0:  # routine \uc758 \uacbd\uc6b0\n        commands = []\n        for item in list:\n            data = item.split(\", \")\n            command_type = data[0]\n            if module_index == 0:  # Zigbee HA\n                if command_type == \"connect\":\n                    print(\"connect\")\n                elif command_type == \"on/off\":\n                    value = data[1]\n                    if value == \"on\" or value == \"0x01\":  # on\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\on.json\")\n                    elif value == \"off\" or value == \"0x00\":  # off\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\off.json\")\n                    elif value == \"toggle\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\toggle.json\")\n                    elif value == \"regular random\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\onoff_random.json\")\n                    elif value == \"irregular random\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\onoff_random.json\")\n                    else:\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\onoff_random.json\")\n                elif command_type == \"color\":\n                    value = data[1]\n                    if value == \"regular random\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\color_random.json\")\n                    elif value == \"irregular random\":\n                        commands.append(\n                            \"resource\\\\command\\\\Zigbee\\\\color_random.json\")\n                    elif value == \"random\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\color_random.json\")\n                    elif value == \"cw\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\color_cw.json\")\n                    elif value == \"dl\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\color_dl.json\")\n                    elif value == \"nw\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\color_nw.json\")\n                    elif value == \"sw\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\color_sw.json\")\n                    elif value == \"ww\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\color_ww.json\")\n                elif command_type == \"level\":\n                    value = data[1]\n                    if value == \"regular random\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\level_random.json\")\n                    elif value == \"irregular random\":\n                        commands.append(\n                            \"resource\\\\command\\\\Zigbee\\\\level_random.json\")\n                    elif value == \"random\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\level_random.json\")\n                    elif value == \"10\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\level_10.json\")\n                    elif value == \"50\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\level_50.json\")\n                    elif value == \"100\":\n                        commands.append(\"resource\\\\command\\\\Zigbee\\\\level_100.json\")\n                elif command_type == \"disconnect\":\n                    print(\"disconnect\")\n        file_data = OrderedDict()\n        file_data[\"task_list\"] = commands\n        file_data[\"iteration\"] = routine_count\n        if make_file:\n            with open('command_routine.json', 'w', encoding='utf-8') as make_file:\n                json.dump(file_data, make_file, ensure_ascii=False, indent=\"\\t\")\n        return file_data\n    else:  # single command \uc640 read attribute\uc758 \uacbd\uc6b0\n        commands = []\n        for item in list:\n            data = item.split(\", \")\n            command_type = data[0]\n            if module_index == 0:  # Zigbee HA\n                if command_type == \"connect\":\n                    print(\"connect\")\n                elif command_type == \"on/off\":\n                    value = data[1]\n                    cluster = ON_OFF_CLUSTER\n                    payloads = None\n                    duration = 0.51\n                    if value == \"on\" or value == \"0x01\" or value == \"1\":  # on\n                        commands.append(get_zigbee_command(cluster, ON_OFF_ON_CMD, payloads, duration, task_kind = COMMAND_TASK))\n                    elif value == \"off\" or value == \"0x00\" or value == \"0\":  # off\n                        commands.append(get_zigbee_command(cluster, ON_OFF_OFF_CMD, payloads, duration, task_kind = COMMAND_TASK))\n                    elif value == \"toggle\":  # toggle\n                        commands.append(get_zigbee_command(cluster, ON_OFF_TOGGLE_CMD, payloads, duration, task_kind = COMMAND_TASK))\n                    elif value == \"regular random\":\n                        command = 0x00\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))\n                    elif value == \"irregular random\":\n                        command = 0x00\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))\n                    else:\n                        command = 0x00\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))\n                elif command_type == \"color\":\n                    value = data[1]\n                    cluster = COLOR_CTRL_CLUSTER\n                    command = 0x0a\n                    duration = 0.51\n                    if value == \"regular random\":\n                        payloads = \"random\"\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))\n                    elif value == \"irregular random\":\n                        payloads = \"random\"\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))\n                    elif value == \"random\":\n                        payloads = \"random\"\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))\n                    else:\n                        payloads = [[int(value), 0x21], [0, 0x21]]\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))\n                elif command_type == \"level\":\n                    value = data[1]\n                    cluster = LVL_CTRL_CLUSTER\n                    command = 0x04\n                    duration = 0.51\n                    if value == \"regular random\":\n                        payloads = \"random\"\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))\n                    elif value == \"irregular random\":\n                        payloads = \"random\"\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))\n                    elif value == \"random\":\n                        payloads = \"random\"\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))\n                    else:\n                        payloads = [[int(value), 0x20], [0, 0x21]]\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))\n                elif command_type == \"disconnect\":\n                    print(\"disconnect\")\n                elif command_type == \"read attribute\":\n                    task_kind = 1\n                    attribute = data[1]\n                    duration = 0.51\n                    if \"ONOFF\" in attribute:\n                        cluster = ON_OFF_CLUSTER\n                        attr_id = zigbee_str_to_attr[cluster][attribute]\n                        command = get_zigbee_command(cluster, attr_id=attr_id, duration=duration, task_kind=task_kind)\n                        commands.append(command)\n                    elif \"COLOR\" in attribute:\n                        cluster = COLOR_CTRL_CLUSTER\n                        attr_id = zigbee_str_to_attr[cluster][attribute]\n                        command = get_zigbee_command(cluster, attr_id=attr_id, duration=duration, task_kind=task_kind)\n                        commands.append(command)\n                    elif \"LVL\" in attribute:\n                        cluster = LVL_CTRL_CLUSTER\n                        attr_id = zigbee_str_to_attr[cluster][attribute]\n                        command = get_zigbee_command(cluster, attr_id=attr_id, duration=duration, task_kind=task_kind)\n                        commands.append(command)\n        file_data = OrderedDict()\n        file_data[\"tasks\"] = commands\n        if make_file:\n            with open('sample_command.json', 'w', encoding='utf-8') as make_file:\n                json.dump(file_data, make_file, ensure_ascii=False, indent=\"\\t\")\n        return file_data", "fn_id": 1, "class_fn": false, "repo": "ninima0323/TestGUI", "file": "command_parser.py", "last_update_at": "2020-06-25T02:14:27+00:00", "question_id": "79347a15010591124840a4abc3e1eface25a91c5_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def make_command(list, module_index, routine_count=-1, make_file=False):\n    if routine_count > 0:\n        commands = []\n        for item in list:\n            data = item.split(', ')\n            command_type = data[0]\n            if module_index == 0:\n                if command_type == 'connect':\n                    print('connect')\n                elif command_type == 'on/off':\n                    value = data[1]\n                    if value == 'on' or value == '0x01':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\on.json')\n                    elif value == 'off' or value == '0x00':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\off.json')\n                    elif value == 'toggle':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\toggle.json')\n                    elif value == 'regular random':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\onoff_random.json')\n                    elif value == 'irregular random':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\onoff_random.json')\n                    else:\n                        commands.append('resource\\\\command\\\\Zigbee\\\\onoff_random.json')\n                elif command_type == 'color':\n                    value = data[1]\n                    if value == 'regular random':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\color_random.json')\n                    elif value == 'irregular random':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\color_random.json')\n                    elif value == 'random':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\color_random.json')\n                    elif value == 'cw':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\color_cw.json')\n                    elif value == 'dl':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\color_dl.json')\n                    elif value == 'nw':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\color_nw.json')\n                    elif value == 'sw':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\color_sw.json')\n                    elif value == 'ww':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\color_ww.json')\n                elif command_type == 'level':\n                    value = data[1]\n                    if value == 'regular random':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\level_random.json')\n                    elif value == 'irregular random':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\level_random.json')\n                    elif value == 'random':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\level_random.json')\n                    elif value == '10':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\level_10.json')\n                    elif value == '50':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\level_50.json')\n                    elif value == '100':\n                        commands.append('resource\\\\command\\\\Zigbee\\\\level_100.json')\n                elif command_type == 'disconnect':\n                    print('disconnect')\n        file_data = OrderedDict()\n        file_data['task_list'] = commands\n        file_data['iteration'] = routine_count\n        if make_file:\n            with open('command_routine.json', 'w', encoding='utf-8') as make_file:\n                json.dump(file_data, make_file, ensure_ascii=False, indent='\\t')\n        return file_data\n    else:\n        commands = []\n        for item in list:\n            data = item.split(', ')\n            command_type = data[0]\n            if module_index == 0:\n                if command_type == 'connect':\n                    print('connect')\n                elif command_type == 'on/off':\n                    value = data[1]\n                    cluster = ON_OFF_CLUSTER\n                    payloads = None\n                    duration = 0.51\n                    if value == 'on' or value == '0x01' or value == '1':\n                        commands.append(get_zigbee_command(cluster, ON_OFF_ON_CMD, payloads, duration, task_kind=COMMAND_TASK))\n                    elif value == 'off' or value == '0x00' or value == '0':\n                        commands.append(get_zigbee_command(cluster, ON_OFF_OFF_CMD, payloads, duration, task_kind=COMMAND_TASK))\n                    elif value == 'toggle':\n                        commands.append(get_zigbee_command(cluster, ON_OFF_TOGGLE_CMD, payloads, duration, task_kind=COMMAND_TASK))\n                    elif value == 'regular random':\n                        command = 0\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind=COMMAND_TASK))\n                    elif value == 'irregular random':\n                        command = 0\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind=COMMAND_TASK))\n                    else:\n                        command = 0\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind=COMMAND_TASK))\n                elif command_type == 'color':\n                    value = data[1]\n                    cluster = COLOR_CTRL_CLUSTER\n                    command = 10\n                    duration = 0.51\n                    if value == 'regular random':\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind=COMMAND_TASK))\n                    elif value == 'irregular random':\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind=COMMAND_TASK))\n                    elif value == 'random':\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind=COMMAND_TASK))\n                    else:\n                        payloads = [[int(value), 33], [0, 33]]\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind=COMMAND_TASK))\n                elif command_type == 'level':\n                    value = data[1]\n                    cluster = LVL_CTRL_CLUSTER\n                    command = 4\n                    duration = 0.51\n                    if value == 'regular random':\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind=COMMAND_TASK))\n                    elif value == 'irregular random':\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind=COMMAND_TASK))\n                    elif value == 'random':\n                        payloads = 'random'\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind=COMMAND_TASK))\n                    else:\n                        payloads = [[int(value), 32], [0, 33]]\n                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind=COMMAND_TASK))\n                elif command_type == 'disconnect':\n                    print('disconnect')\n                elif command_type == 'read attribute':\n                    task_kind = 1\n                    attribute = data[1]\n                    duration = 0.51\n                    if 'ONOFF' in attribute:\n                        cluster = ON_OFF_CLUSTER\n                        attr_id = zigbee_str_to_attr[cluster][attribute]\n                        command = get_zigbee_command(cluster, attr_id=attr_id, duration=duration, task_kind=task_kind)\n                        commands.append(command)\n                    elif 'COLOR' in attribute:\n                        cluster = COLOR_CTRL_CLUSTER\n                        attr_id = zigbee_str_to_attr[cluster][attribute]\n                        command = get_zigbee_command(cluster, attr_id=attr_id, duration=duration, task_kind=task_kind)\n                        commands.append(command)\n                    elif 'LVL' in attribute:\n                        cluster = LVL_CTRL_CLUSTER\n                        attr_id = zigbee_str_to_attr[cluster][attribute]\n                        command = get_zigbee_command(cluster, attr_id=attr_id, duration=duration, task_kind=task_kind)\n                        commands.append(command)\n        file_data = OrderedDict()\n        file_data['tasks'] = commands\n        if make_file:\n            with open('sample_command.json', 'w', encoding='utf-8') as make_file:\n                json.dump(file_data, make_file, ensure_ascii=False, indent='\\t')\n"]]}
{"hexsha": "a0aeaa987fec12bfa0ace9c55e26b264dab3bd52", "ext": "py", "lang": "Python", "content": "def register_font_path(font_path):\n    # Caching seems to cause problems. Disable for now\n    ff = fontfinder.FontFinder(useCache=False)\n    ff.addDirectory(font_path, recur=True)\n\n    try:\n        ff.search()\n    except KeyError as ke:\n        logging.warning(\"Problem parsing font: {}\".format(ke))\n    except Exception as e:\n        logging.warning(e)\n\n    for family_name in ff.getFamilyNames():\n        fonts_in_family = ff.getFontsInFamily(family_name)\n        for font in fonts_in_family:\n            if len(fonts_in_family) == 1:\n                try:\n                    ttfont = TTFont(family_name.decode(\"utf-8\"), font.fileName)\n                    pdfmetrics.registerFont(ttfont)\n                    pdfmetrics.registerFontFamily(family_name)\n                except TTFError as e:\n                    logging.warning(\"Could not register {}, {}\".format(family_name, e))\n                    continue\n            elif len(fonts_in_family) > 1:\n                '''If font family has multiple weights/styles'''\n                font_name = family_name + \"-\".encode() + font.styleName\n                font_name = font_name.decode(\"utf-8\")\n                try:\n                    ttfont = TTFont(font_name, font.fileName)\n                    pdfmetrics.registerFont(ttfont)\n                    addMapping(font.familyName, font.isBold, font.isItalic, font_name)\n                except TTFError as e:\n                    logging.warning(\"Could not register {}, {}\".format(family_name, e))\n                    continue", "fn_id": 1, "class_fn": false, "repo": "barudaret/kassia", "file": "font_reader.py", "last_update_at": "2020-04-24T07:40:05+00:00", "question_id": "a0aeaa987fec12bfa0ace9c55e26b264dab3bd52_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def register_font_path(font_path):\n    ff = fontfinder.FontFinder(useCache=False)\n    ff.addDirectory(font_path, recur=True)\n    try:\n        ff.search()\n    except KeyError as ke:\n        logging.warning('Problem parsing font: {}'.format(ke))\n    except Exception as e:\n        logging.warning(e)\n    for family_name in ff.getFamilyNames():\n        fonts_in_family = ff.getFontsInFamily(family_name)\n        for font in fonts_in_family:\n            if len(fonts_in_family) == 1:\n                try:\n                    ttfont = TTFont(family_name.decode('utf-8'), font.fileName)\n                    pdfmetrics.registerFont(ttfont)\n                    pdfmetrics.registerFontFamily(family_name)\n                except TTFError as e:\n                    logging.warning('Could not register {}, {}'.format(family_name, e))\n                    continue\n            elif len(fonts_in_family) > 1:\n                'If font family has multiple weights/styles'\n                font_name = family_name + '-'.encode() + font.styleName\n                font_name = font_name.decode('utf-8')\n                try:\n                    ttfont = TTFont(font_name, font.fileName)\n                    pdfmetrics.registerFont(ttfont)\n                    addMapping(font.familyName, font.isBold, font.isItalic, font_name)\n                except TTFError as e:\n                    logging.warning('Could not register {}, {}'.format(family_name, e))\n"]]}
{"hexsha": "5a75e5a14da8c33d6eba2f02a6e9aa18f45849f6", "ext": "py", "lang": "Python", "content": "@app.callback(\n    dash.dependencies.Output('logfield','children'),\n    [dash.dependencies.Input('interval_log','n_intervals')]\n)\n\ndef logupdate(n):\n    with open(\"datalogger.log\", \"r\") as file:\n        i=0\n        lines_size = 20\n        last_lines = []\n        for line in file:\n            if i < lines_size:\n                last_lines.append(line)\n            else:\n                last_lines[i%lines_size] = line\n            i = i + 1\n \n    last_lines = last_lines[(i%lines_size):] + last_lines[:(i%lines_size)]\n\n    output=\"\"\n\n    for line in last_lines:\n        output+=line\n       \n    \n    return output", "fn_id": 4, "class_fn": false, "repo": "martinlink00/loggerapp", "file": "loggerapp/app.py", "last_update_at": "2020-09-18T08:29:05+00:00", "question_id": "5a75e5a14da8c33d6eba2f02a6e9aa18f45849f6_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.callback(dash.dependencies.Output('logfield', 'children'), [dash.dependencies.Input('interval_log', 'n_intervals')])\ndef logupdate(n):\n    with open('datalogger.log', 'r') as file:\n        i = 0\n        lines_size = 20\n        last_lines = []\n        for line in file:\n            if i < lines_size:\n                last_lines.append(line)\n            else:\n                last_lines[i % lines_size] = line\n            i = i + 1\n    last_lines = last_lines[i % lines_size:] + last_lines[:i % lines_size]\n    output = ''\n    for line in last_lines:\n        output += line\n"]]}
{"hexsha": "79680950fe809ef8c16b2d6dbc028b141c8f2aa8", "ext": "py", "lang": "Python", "content": "@pandas_udf(\"boolean\", PandasUDFType.SCALAR)\ndef ST_Equals(left, right):\n    arr_left = pa.array(left, type='string')\n    arr_right = pa.array(right, type='string')\n    from arctern import ST_Equals\n    rs = ST_Equals(arr_left, arr_right)\n    return rs.to_pandas()", "fn_id": 6, "class_fn": false, "repo": "cydrain/arctern", "file": "spark/pyspark/arctern_pyspark/_wrapper_func.py", "last_update_at": "2020-04-25T04:21:01+00:00", "question_id": "79680950fe809ef8c16b2d6dbc028b141c8f2aa8_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pandas_udf('boolean', PandasUDFType.SCALAR)\ndef ST_Equals(left, right):\n    arr_left = pa.array(left, type='string')\n    arr_right = pa.array(right, type='string')\n    from arctern import ST_Equals\n    rs = ST_Equals(arr_left, arr_right)\n"]]}
{"hexsha": "9f0297ebd0788f0dccf07a7bd85a4c5f8e814f45", "ext": "py", "lang": "Python", "content": "def plot_angle_hist(vals, labels, dp):\n    nom_v  = vals[np.where(labels==0)[0]]\n    anom_v = vals[np.where(labels==1)[0]]\n    bins = np.arange(start=np.min(vals), stop=np.max(vals), step=(np.max(vals)-np.min(vals))/50)\n    font = {'xtick.labelsize': 16,\n            'ytick.labelsize': 16}\n    mpl.rc(font)\n    pl = dp.get_next_plot()\n    plt.xlabel(r\"angles from ${\\bf w}_{unif}$ (degrees)\", fontsize=20)\n    plt.ylabel(\"fraction of instances (%)\", fontsize=20)\n    logger.debug(\"\\n%s\" % str(list(nom_v)))\n    n1, bins1 = np.histogram(nom_v, bins=bins, normed=True)\n    n2, bins2 = np.histogram(anom_v, bins=bins, normed=True)\n    width = 0.7 * (bins[1] - bins[0])\n    center = (bins[:-1] + bins[1:]) / 2\n    plt.bar(center, n1, align='center', width=width, facecolor='green', alpha=0.50)\n    plt.bar(center, n2, align='center', width=width, facecolor='red', alpha=0.50)", "fn_id": 1, "class_fn": false, "repo": "snad-space/ad_examples", "file": "ad_examples/aad/test_hyperplane_angles.py", "last_update_at": "2020-04-23T18:12:57+00:00", "question_id": "9f0297ebd0788f0dccf07a7bd85a4c5f8e814f45_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def plot_angle_hist(vals, labels, dp):\n    nom_v = vals[np.where(labels == 0)[0]]\n    anom_v = vals[np.where(labels == 1)[0]]\n    bins = np.arange(start=np.min(vals), stop=np.max(vals), step=(np.max(vals) - np.min(vals)) / 50)\n    font = {'xtick.labelsize': 16, 'ytick.labelsize': 16}\n    mpl.rc(font)\n    pl = dp.get_next_plot()\n    plt.xlabel('angles from ${\\\\bf w}_{unif}$ (degrees)', fontsize=20)\n    plt.ylabel('fraction of instances (%)', fontsize=20)\n    logger.debug('\\n%s' % str(list(nom_v)))\n    n1, bins1 = np.histogram(nom_v, bins=bins, normed=True)\n    n2, bins2 = np.histogram(anom_v, bins=bins, normed=True)\n    width = 0.7 * (bins[1] - bins[0])\n    center = (bins[:-1] + bins[1:]) / 2\n    plt.bar(center, n1, align='center', width=width, facecolor='green', alpha=0.5)\n"]]}
{"hexsha": "02ded602e60b99c611e797ccb5c9cae95556d9be", "ext": "py", "lang": "Python", "content": "def get_coord(place):\n    \"\"\"\n    Returns the coordinates from the POI json\n    \"\"\"\n    coord = place.get_coord()\n    if coord:\n        lon = coord.get(\"lon\")\n        lat = coord.get(\"lat\")\n        return (lat, lon)\n    return None", "fn_id": 0, "class_fn": false, "repo": "QwantResearch/idunn", "file": "idunn/blocks/opening_hour.py", "last_update_at": "2020-11-07T01:53:07+00:00", "question_id": "02ded602e60b99c611e797ccb5c9cae95556d9be_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_coord(place):\n    \"\"\"\n    Returns the coordinates from the POI json\n    \"\"\"\n    coord = place.get_coord()\n    if coord:\n        lon = coord.get('lon')\n        lat = coord.get('lat')\n        return (lat, lon)\n"]]}
{"hexsha": "811e1b4c581229632441bd29abb0180fd80abbfa", "ext": "py", "lang": "Python", "content": "def load_entrypoint_plugins(entry_points, airflow_plugins):\n    \"\"\"\n    Load AirflowPlugin subclasses from the entrypoints\n    provided. The entry_point group should be 'airflow.plugins'.\n\n    :param entry_points: A collection of entrypoints to search for plugins\n    :type entry_points: Generator[setuptools.EntryPoint, None, None]\n    :param airflow_plugins: A collection of existing airflow plugins to\n        ensure we don't load duplicates\n    :type airflow_plugins: List[AirflowPlugin]\n    :return: List[Type[AirflowPlugin]]\n    \"\"\"\n    for entry_point in entry_points:\n        log.debug('Importing entry_point plugin %s', entry_point.name)\n        plugin_obj = entry_point.load()\n        if is_valid_plugin(plugin_obj, airflow_plugins):\n            if callable(getattr(plugin_obj, 'on_load', None)):\n                plugin_obj.on_load()\n                airflow_plugins.append(plugin_obj)\n    return airflow_plugins", "fn_id": 0, "class_fn": false, "repo": "andyh1203/airflow", "file": "airflow/plugins_manager.py", "last_update_at": "2020-10-14T16:40:36+00:00", "question_id": "811e1b4c581229632441bd29abb0180fd80abbfa_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_entrypoint_plugins(entry_points, airflow_plugins):\n    \"\"\"\n    Load AirflowPlugin subclasses from the entrypoints\n    provided. The entry_point group should be 'airflow.plugins'.\n\n    :param entry_points: A collection of entrypoints to search for plugins\n    :type entry_points: Generator[setuptools.EntryPoint, None, None]\n    :param airflow_plugins: A collection of existing airflow plugins to\n        ensure we don't load duplicates\n    :type airflow_plugins: List[AirflowPlugin]\n    :return: List[Type[AirflowPlugin]]\n    \"\"\"\n    for entry_point in entry_points:\n        log.debug('Importing entry_point plugin %s', entry_point.name)\n        plugin_obj = entry_point.load()\n        if is_valid_plugin(plugin_obj, airflow_plugins):\n            if callable(getattr(plugin_obj, 'on_load', None)):\n                plugin_obj.on_load()\n                airflow_plugins.append(plugin_obj)\n"]]}
{"hexsha": "1a94529672b7505165acc92ab8eef6c2a078d80f", "ext": "py", "lang": "Python", "content": "def rebatch_metadata_by_experiment(metadata):\n    normal, normal_rest = prioritize_normals(metadata)\n    batch = metadata[0][\"participant\"]\n    tumor_batch = [tz.assoc(x, \"batch\", batch) for x in metadata\n                   if x[\"sample_type\"] in PRIORITIZED_TUMOR_CODES.keys()]\n    normal = [tz.assoc(normal, \"batch\", batch)] if normal else []\n    # run each non priority normal as its own tumor sample with no control\n    normal_rest = [tz.assoc(x, \"batch\", batch + \"-\" + x[\"sample_type\"]) for x\n                   in normal_rest]\n    normal_rest = [tz.assoc(x, \"phenotype\", \"tumor\") for x in normal_rest]\n    all_batches = normal + normal_rest + tumor_batch\n    return all_batches", "fn_id": 4, "class_fn": false, "repo": "a113n/bcbio-nextgen", "file": "scripts/utils/tcga_to_bcbio.py", "last_update_at": "2020-07-21T19:45:24+00:00", "question_id": "1a94529672b7505165acc92ab8eef6c2a078d80f_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def rebatch_metadata_by_experiment(metadata):\n    normal, normal_rest = prioritize_normals(metadata)\n    batch = metadata[0]['participant']\n    tumor_batch = [tz.assoc(x, 'batch', batch) for x in metadata if x['sample_type'] in PRIORITIZED_TUMOR_CODES.keys()]\n    normal = [tz.assoc(normal, 'batch', batch)] if normal else []\n    normal_rest = [tz.assoc(x, 'batch', batch + '-' + x['sample_type']) for x in normal_rest]\n    normal_rest = [tz.assoc(x, 'phenotype', 'tumor') for x in normal_rest]\n    all_batches = normal + normal_rest + tumor_batch\n"]]}
{"hexsha": "4a241796f5fc6483411704dc8ffbed36257a96b5", "ext": "py", "lang": "Python", "content": "def get_window():\n    '''Returns windows in z-order (top first)'''\n    user32 = ctypes.windll.user32\n    lst = []\n    top = user32.GetTopWindow(None)\n    if not top:\n        return lst\n    lst.append(top)\n    while True:\n        next = user32.GetWindow(lst[-1], win32con.GW_HWNDNEXT)\n        if not next:\n            break\n        lst.append(next)\n    return lst", "fn_id": 0, "class_fn": false, "repo": "laashub-sua/picker", "file": "try/try2.py", "last_update_at": "2020-07-25T15:03:16+00:00", "question_id": "4a241796f5fc6483411704dc8ffbed36257a96b5_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_window():\n    \"\"\"Returns windows in z-order (top first)\"\"\"\n    user32 = ctypes.windll.user32\n    lst = []\n    top = user32.GetTopWindow(None)\n    if not top:\n        return lst\n    lst.append(top)\n    while True:\n        next = user32.GetWindow(lst[-1], win32con.GW_HWNDNEXT)\n        if not next:\n            break\n        lst.append(next)\n"]]}
{"hexsha": "b0fa61e7cbf4fc40d49289abb65d59ba663994d7", "ext": "py", "lang": "Python", "content": "def create_tables():\n\n    ''' Creates the table if it doesn't exists already.'''\n\n    try:\n\n        # Stablishes connection with our db\n\n\n\n        connection = psycopg2.connect(user=os.environ.get('db_user'),\n                                    password=os.environ.get('db_password'),\n                                    host=os.environ.get('db_host'),\n                                    port=os.environ.get('db_port'),\n                                    database=os.environ.get('db_name'))\n\n\n\n        # Create the cursor.\n\n        cursor = connection.cursor()\n\n\n        query_country_table='''\n                            CREATE TABLE IF NOT EXISTS countries (\n                                id SERIAL NOT NULL,\n                                country_code VARCHAR(3) NOT NULL UNIQUE,\n                                country_name CHAR(99),\n                                PRIMARY KEY(country_code)\n                            );\n        '''\n\n        cursor.execute(query_country_table)\n        connection.commit()\n\n        query_market_country_table= '''\n                            CREATE TABLE IF NOT EXISTS markets (\n                                id SERIAL,\n                                market_id VARCHAR(99),\n                                market_name VARCHAR(99),\n                                country_code VARCHAR(3) REFERENCES countries(country_code),\n                                PRIMARY KEY (market_id)\n                            );\n        '''\n\n        cursor.execute(query_market_country_table)\n        connection.commit()\n\n        query_currency_table = '''\n                            CREATE TABLE IF NOT EXISTS currencies (\n                                id SERIAL NOT NULL,\n                                currency_name VARCHAR(99),\n                                currency_code VARCHAR(3) NOT NULL UNIQUE,\n                                is_in_uganda BOOLEAN,\n                                is_in_kenya BOOLEAN,\n                                is_in_congo BOOLEAN,\n                                is_in_burundi BOOLEAN,\n                                is_in_tanzania BOOLEAN,\n                                is_in_south_sudan BOOLEAN,\n                                is_in_rwanda BOOLEAN,\n                                is_in_malawi BOOLEAN,\n                                PRIMARY KEY(currency_code)\n                            );\n        '''\n\n        cursor.execute(query_currency_table)\n        connection.commit()\n\n        query_sources_table = '''\n                            CREATE TABLE IF NOT EXISTS sources (\n                                id SERIAL NOT NULL,\n                                source_name VARCHAR(99) NOT NULL,\n                                is_in_uganda BOOLEAN,\n                                is_in_kenya BOOLEAN,\n                                is_in_congo BOOLEAN,\n                                is_in_burundi BOOLEAN,\n                                is_in_tanzania BOOLEAN,\n                                is_in_south_sudan BOOLEAN,\n                                is_in_rwanda BOOLEAN,\n                                is_in_malawi BOOLEAN,\n                                PRIMARY KEY (id)\n                            );\n        '''\n\n        cursor.execute(query_sources_table)\n        connection.commit()\n\n        query_categories_table = '''\n                            CREATE TABLE IF NOT EXISTS categories (\n                                id SERIAL NOT NULL,\n                                category_name VARCHAR(99) UNIQUE,\n                                PRIMARY KEY(id)\n                            );\n        '''\n\n        cursor.execute(query_categories_table)\n        connection.commit()\n\n        # To be discused. ####\n\n        # query_products_table = '''\n        #                     CREATE TABLE IF NOT EXISTS products (\n        #                         id SERIAL NOT NULL UNIQUE,\n        #                         product_name VARCHAR(99) UNIQUE,\n        #                         PRIMARY KEY(id, product_name)\n        #                     );\n        # '''\n\n        # cursor.execute(query_products_table)\n        # connection.commit()\n\n\n        query_product_category_pair_table = '''\n                            CREATE TABLE IF NOT EXISTS prod_cat_pair (\n                                id SERIAL NOT NULL,\n                                product_name VARCHAR(99) UNIQUE,\n                                category_id INT REFERENCES categories(id),\n                                PRIMARY KEY(product_name)\n                            );\n        '''\n\n        cursor.execute(query_product_category_pair_table)\n        connection.commit()\n\n        query_product_raw_info_table = '''\n                            CREATE TABLE IF NOT EXISTS product_raw_info (\n                                product_name VARCHAR(99) REFERENCES prod_cat_pair(product_name),\n                                market_id VARCHAR(99) REFERENCES markets(market_id),\n                                unit_scale VARCHAR(32),\n                                source_id INT REFERENCES sources(id),\n                                currency_code VARCHAR(3) REFERENCES currencies(currency_code),\n                                date_price DATE,\n                                retail_observed_price float4,\n                                wholesale_observed_price float4\n                            );\n        '''\n\n        cursor.execute(query_product_raw_info_table)\n        connection.commit()\n\n\n        query_product_clean_retail_info_table = '''\n                            CREATE TABLE IF NOT EXISTS product_clean_retail_info (\n                                product_name VARCHAR(99) REFERENCES prod_cat_pair(product_name),\n                                market_id VARCHAR(99) REFERENCES markets(market_id),\n                                source_id INT REFERENCES sources(id),\n                                currency_code VARCHAR(3) REFERENCES currencies(currency_code),\n                                date_price DATE,\n                                observed_price float4,\n                                observed_class VARCHAR(9),\n                                forecasted_price_1 float4,\n                                forecasted_class_1 VARCHAR(9),\n                                forecasted_price_2 float4,\n                                forecasted_class_2 VARCHAR(9),\n                                forecasted_price_3 float4,\n                                forecasted_class_3 VARCHAR(9),\n                                forecasted_price_4 float4,\n                                forecasted_class_4 VARCHAR(9),\n                                used_model VARCHAR(99),\n                                date_run_model DATE,\n                                normal_band_limit float8,\n                                stress_band_limit float8,\n                                alert_band_limit float8,\n                                stressness float8\n                            );\n        '''\n\n        cursor.execute(query_product_clean_retail_info_table)\n        connection.commit()\n\n        query_product_clean_wholesale_info_table = '''\n                            CREATE TABLE IF NOT EXISTS product_clean_wholesale_info (\n                                product_name VARCHAR(99) REFERENCES prod_cat_pair(product_name),\n                                market_id VARCHAR(99) REFERENCES markets(market_id),\n                                source_id INT REFERENCES sources(id),\n                                currency_code VARCHAR(3) REFERENCES currencies(currency_code),\n                                date_price DATE,\n                                observed_price float4,\n                                observed_class VARCHAR(9),\n                                forecasted_price_1 float4,\n                                forecasted_class_1 VARCHAR(9),\n                                forecasted_price_2 float4,\n                                forecasted_class_2 VARCHAR(9),\n                                forecasted_price_3 float4,\n                                forecasted_class_3 VARCHAR(9),\n                                forecasted_price_4 float4,\n                                forecasted_class_4 VARCHAR(9),\n                                used_model VARCHAR(99),\n                                date_run_model DATE,\n                                normal_band_limit float8,\n                                stress_band_limit float8,\n                                alert_band_limit float8,\n                                stressness float8\n                            );\n        '''\n\n        cursor.execute(query_product_clean_wholesale_info_table)\n        connection.commit()\n\n        return 'Success'\n\n    except (Exception, psycopg2.Error) as error:\n        print('Error verifying or creating the table.')\n\n    finally:\n\n        if (connection):\n            cursor.close()\n            connection.close()", "fn_id": 0, "class_fn": false, "repo": "CodingDuckmx/Sauti-Africa-Market-Monitoring", "file": "collect_data.py", "last_update_at": "2020-05-20T15:58:22+00:00", "question_id": "b0fa61e7cbf4fc40d49289abb65d59ba663994d7_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_tables():\n    \"\"\" Creates the table if it doesn't exists already.\"\"\"\n    try:\n        connection = psycopg2.connect(user=os.environ.get('db_user'), password=os.environ.get('db_password'), host=os.environ.get('db_host'), port=os.environ.get('db_port'), database=os.environ.get('db_name'))\n        cursor = connection.cursor()\n        query_country_table = '\\n                            CREATE TABLE IF NOT EXISTS countries (\\n                                id SERIAL NOT NULL,\\n                                country_code VARCHAR(3) NOT NULL UNIQUE,\\n                                country_name CHAR(99),\\n                                PRIMARY KEY(country_code)\\n                            );\\n        '\n        cursor.execute(query_country_table)\n        connection.commit()\n        query_market_country_table = '\\n                            CREATE TABLE IF NOT EXISTS markets (\\n                                id SERIAL,\\n                                market_id VARCHAR(99),\\n                                market_name VARCHAR(99),\\n                                country_code VARCHAR(3) REFERENCES countries(country_code),\\n                                PRIMARY KEY (market_id)\\n                            );\\n        '\n        cursor.execute(query_market_country_table)\n        connection.commit()\n        query_currency_table = '\\n                            CREATE TABLE IF NOT EXISTS currencies (\\n                                id SERIAL NOT NULL,\\n                                currency_name VARCHAR(99),\\n                                currency_code VARCHAR(3) NOT NULL UNIQUE,\\n                                is_in_uganda BOOLEAN,\\n                                is_in_kenya BOOLEAN,\\n                                is_in_congo BOOLEAN,\\n                                is_in_burundi BOOLEAN,\\n                                is_in_tanzania BOOLEAN,\\n                                is_in_south_sudan BOOLEAN,\\n                                is_in_rwanda BOOLEAN,\\n                                is_in_malawi BOOLEAN,\\n                                PRIMARY KEY(currency_code)\\n                            );\\n        '\n        cursor.execute(query_currency_table)\n        connection.commit()\n        query_sources_table = '\\n                            CREATE TABLE IF NOT EXISTS sources (\\n                                id SERIAL NOT NULL,\\n                                source_name VARCHAR(99) NOT NULL,\\n                                is_in_uganda BOOLEAN,\\n                                is_in_kenya BOOLEAN,\\n                                is_in_congo BOOLEAN,\\n                                is_in_burundi BOOLEAN,\\n                                is_in_tanzania BOOLEAN,\\n                                is_in_south_sudan BOOLEAN,\\n                                is_in_rwanda BOOLEAN,\\n                                is_in_malawi BOOLEAN,\\n                                PRIMARY KEY (id)\\n                            );\\n        '\n        cursor.execute(query_sources_table)\n        connection.commit()\n        query_categories_table = '\\n                            CREATE TABLE IF NOT EXISTS categories (\\n                                id SERIAL NOT NULL,\\n                                category_name VARCHAR(99) UNIQUE,\\n                                PRIMARY KEY(id)\\n                            );\\n        '\n        cursor.execute(query_categories_table)\n        connection.commit()\n        query_product_category_pair_table = '\\n                            CREATE TABLE IF NOT EXISTS prod_cat_pair (\\n                                id SERIAL NOT NULL,\\n                                product_name VARCHAR(99) UNIQUE,\\n                                category_id INT REFERENCES categories(id),\\n                                PRIMARY KEY(product_name)\\n                            );\\n        '\n        cursor.execute(query_product_category_pair_table)\n        connection.commit()\n        query_product_raw_info_table = '\\n                            CREATE TABLE IF NOT EXISTS product_raw_info (\\n                                product_name VARCHAR(99) REFERENCES prod_cat_pair(product_name),\\n                                market_id VARCHAR(99) REFERENCES markets(market_id),\\n                                unit_scale VARCHAR(32),\\n                                source_id INT REFERENCES sources(id),\\n                                currency_code VARCHAR(3) REFERENCES currencies(currency_code),\\n                                date_price DATE,\\n                                retail_observed_price float4,\\n                                wholesale_observed_price float4\\n                            );\\n        '\n        cursor.execute(query_product_raw_info_table)\n        connection.commit()\n        query_product_clean_retail_info_table = '\\n                            CREATE TABLE IF NOT EXISTS product_clean_retail_info (\\n                                product_name VARCHAR(99) REFERENCES prod_cat_pair(product_name),\\n                                market_id VARCHAR(99) REFERENCES markets(market_id),\\n                                source_id INT REFERENCES sources(id),\\n                                currency_code VARCHAR(3) REFERENCES currencies(currency_code),\\n                                date_price DATE,\\n                                observed_price float4,\\n                                observed_class VARCHAR(9),\\n                                forecasted_price_1 float4,\\n                                forecasted_class_1 VARCHAR(9),\\n                                forecasted_price_2 float4,\\n                                forecasted_class_2 VARCHAR(9),\\n                                forecasted_price_3 float4,\\n                                forecasted_class_3 VARCHAR(9),\\n                                forecasted_price_4 float4,\\n                                forecasted_class_4 VARCHAR(9),\\n                                used_model VARCHAR(99),\\n                                date_run_model DATE,\\n                                normal_band_limit float8,\\n                                stress_band_limit float8,\\n                                alert_band_limit float8,\\n                                stressness float8\\n                            );\\n        '\n        cursor.execute(query_product_clean_retail_info_table)\n        connection.commit()\n        query_product_clean_wholesale_info_table = '\\n                            CREATE TABLE IF NOT EXISTS product_clean_wholesale_info (\\n                                product_name VARCHAR(99) REFERENCES prod_cat_pair(product_name),\\n                                market_id VARCHAR(99) REFERENCES markets(market_id),\\n                                source_id INT REFERENCES sources(id),\\n                                currency_code VARCHAR(3) REFERENCES currencies(currency_code),\\n                                date_price DATE,\\n                                observed_price float4,\\n                                observed_class VARCHAR(9),\\n                                forecasted_price_1 float4,\\n                                forecasted_class_1 VARCHAR(9),\\n                                forecasted_price_2 float4,\\n                                forecasted_class_2 VARCHAR(9),\\n                                forecasted_price_3 float4,\\n                                forecasted_class_3 VARCHAR(9),\\n                                forecasted_price_4 float4,\\n                                forecasted_class_4 VARCHAR(9),\\n                                used_model VARCHAR(99),\\n                                date_run_model DATE,\\n                                normal_band_limit float8,\\n                                stress_band_limit float8,\\n                                alert_band_limit float8,\\n                                stressness float8\\n                            );\\n        '\n        cursor.execute(query_product_clean_wholesale_info_table)\n        connection.commit()\n        return 'Success'\n    except (Exception, psycopg2.Error) as error:\n        print('Error verifying or creating the table.')\n    finally:\n        if connection:\n            cursor.close()\n"]]}
{"hexsha": "ceee44d934e8b84acda78f0b31d76ab5898ee66e", "ext": "py", "lang": "Python", "content": "@cli.group()\n@click.pass_context\ndef env(ctx):\n    \"\"\"\n    super.AI Config operations\n    \"\"\"\n    pass", "fn_id": 1, "class_fn": false, "repo": "mysuperai/superai-sdk", "file": "superai/cli.py", "last_update_at": "2020-12-03T18:18:16+00:00", "question_id": "ceee44d934e8b84acda78f0b31d76ab5898ee66e_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@cli.group()\n@click.pass_context\ndef env(ctx):\n    \"\"\"\n    super.AI Config operations\n    \"\"\"\n"]]}
{"hexsha": "4b63530da8cc0deb58044c8935aabba9d812357c", "ext": "py", "lang": "Python", "content": "async def device_firmware_upgrade(dfu_addr, package):\n\n    async with DfuDevice(address=str(dfu_addr)) as dev:\n        imgpkg = DfuImagePkg(package)\n        await dev.send_image_package(imgpkg)", "fn_id": 8, "class_fn": false, "repo": "lohmega/jamble", "file": "bblogger/dfu.py", "last_update_at": "2020-09-29T12:42:23+00:00", "question_id": "4b63530da8cc0deb58044c8935aabba9d812357c_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def device_firmware_upgrade(dfu_addr, package):\n    async with DfuDevice(address=str(dfu_addr)) as dev:\n        imgpkg = DfuImagePkg(package)\n"]]}
{"hexsha": "c196b83429923b591360fe6626e240c60f7a76ac", "ext": "py", "lang": "Python", "content": "def scaleCallback(newValue):\n\tglobal previousValue\n\tprint('scaleCallback: The type is '+ str(type(newValue)))\n\t# setting of the inital value\n\tif previousValue == 0.0 and newValue > MIN_WEIGHT_THRESHOLD:\n\t\tpreviousValue = newValue\n\t\tprint('Set new value ' + str(previousValue))\n\t\tuploadValue(newValue)\n\t# handle case when user presses on the item on the scale (pump bottle)\n\telif (newValue - previousValue) > previousValue * 2 and (newValue - previousValue) >= 150:\n\t\tprint('User most likely consumed by pressing on the scale skipping')\n\t\treturn\n\t# should hit when a single usage is made\n\telif (previousValue - newValue) >= VAL_THRESHOLD:\n\t\tdiff = previousValue - newValue\n\t\tpreviousValue = newValue\n\t\tprint('Value changed to ' + str(previousValue) + ' with diff ' + str(diff))\n\t\tuploadValue(newValue)\n\telse:\n\t\tprint('Previous value is ' + str(previousValue) + ' new value is ' + str(newValue) + ' diff is too small, skipping')", "fn_id": 0, "class_fn": false, "repo": "robsel118/junction2020", "file": "Scale/data_service.py", "last_update_at": "2020-11-09T06:18:55+00:00", "question_id": "c196b83429923b591360fe6626e240c60f7a76ac_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def scaleCallback(newValue):\n    global previousValue\n    print('scaleCallback: The type is ' + str(type(newValue)))\n    if previousValue == 0.0 and newValue > MIN_WEIGHT_THRESHOLD:\n        previousValue = newValue\n        print('Set new value ' + str(previousValue))\n        uploadValue(newValue)\n    elif newValue - previousValue > previousValue * 2 and newValue - previousValue >= 150:\n        print('User most likely consumed by pressing on the scale skipping')\n        return\n    elif previousValue - newValue >= VAL_THRESHOLD:\n        diff = previousValue - newValue\n        previousValue = newValue\n        print('Value changed to ' + str(previousValue) + ' with diff ' + str(diff))\n        uploadValue(newValue)\n    else:\n"]]}
{"hexsha": "69b5d11bd9427c7424a01c1cf4dd0916e3330729", "ext": "py", "lang": "Python", "content": "def lr_decay(step):\n\tlr = 0.1\n\tstep = step/20000\n\tstep = tf.math.floor(step)\n\tstep = tf.math.pow(0.1, step)\n\tlr = lr * step \n\treturn lr ", "fn_id": 0, "class_fn": false, "repo": "ddddwee1/SULT", "file": "example/FaceResNet/train.py", "last_update_at": "2020-03-12T11:07:45+00:00", "question_id": "69b5d11bd9427c7424a01c1cf4dd0916e3330729_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def lr_decay(step):\n    lr = 0.1\n    step = step / 20000\n    step = tf.math.floor(step)\n    step = tf.math.pow(0.1, step)\n    lr = lr * step\n"]]}
{"hexsha": "8a2a6300d8cbd4857fddcb78bbb2c8e343a8dc20", "ext": "py", "lang": "Python", "content": "def _check_tensorflow_computation(label, comp):\n  py_typecheck.check_type(comp, computation_base.Computation, label)\n  comp_proto = computation_impl.ComputationImpl.get_proto(comp)\n  which_comp = comp_proto.WhichOneof('computation')\n  if which_comp != 'tensorflow':\n    raise TypeError('Expected all computations supplied as arguments to '\n                    'be plain TensorFlow, found {}.'.format(which_comp))", "fn_id": 0, "class_fn": false, "repo": "Queenmariehatcher/federated", "file": "tensorflow_federated/python/core/backends/mapreduce/forms.py", "last_update_at": "2020-12-28T19:20:04+00:00", "question_id": "8a2a6300d8cbd4857fddcb78bbb2c8e343a8dc20_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _check_tensorflow_computation(label, comp):\n    py_typecheck.check_type(comp, computation_base.Computation, label)\n    comp_proto = computation_impl.ComputationImpl.get_proto(comp)\n    which_comp = comp_proto.WhichOneof('computation')\n    if which_comp != 'tensorflow':\n"]]}
{"hexsha": "ff3e6273138416f8f635747b52c2637170ad6f94", "ext": "py", "lang": "Python", "content": "def test_user_title():\n\n    test_data = [2, \"test\"]\n    title = data.UserTitle(test_data)\n\n    assert title.id == 2\n    assert title.title == \"test\"\n    assert title.to_row() == test_data\n    assert title.table_name() == \"user_titles\"", "fn_id": 2, "class_fn": false, "repo": "sedezee/TalosBot", "file": "tests/test_discord/test_sql.py", "last_update_at": "2020-12-10T00:17:30+00:00", "question_id": "ff3e6273138416f8f635747b52c2637170ad6f94_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_user_title():\n    test_data = [2, 'test']\n    title = data.UserTitle(test_data)\n    assert title.id == 2\n    assert title.title == 'test'\n    assert title.to_row() == test_data\n"]]}
{"hexsha": "9efbc8fc3a21394fe8714b4e0acd8bb2d358a0cd", "ext": "py", "lang": "Python", "content": "def run_tests(test_config: config.TestConfig) -> None:\n    \"\"\"Run tests with specification given in config\"\"\"\n\n    print(\"Testing config:\")\n    print(test_config)\n    print()\n\n    if not os.path.isdir(test_config.test_dir):\n        cli.print_error(f\"No directory `{test_config.test_dir}`\")\n        return\n\n    if test_config.groups is None:\n        groups = get_groups_in_dir(test_config.test_dir)\n    else:\n        groups = set()\n        for pattern in test_config.groups:\n            for group in get_groups_in_dir_matching(test_config.test_dir,\n                                                    pattern):\n                groups.add(group)\n\n    for group in groups:\n        run_test_group(group, test_config)", "fn_id": 0, "class_fn": false, "repo": "mhorod/checker", "file": "simple_checker/run.py", "last_update_at": "2020-11-12T11:37:34+00:00", "question_id": "9efbc8fc3a21394fe8714b4e0acd8bb2d358a0cd_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def run_tests(test_config: config.TestConfig) -> None:\n    \"\"\"Run tests with specification given in config\"\"\"\n    print('Testing config:')\n    print(test_config)\n    print()\n    if not os.path.isdir(test_config.test_dir):\n        cli.print_error(f'No directory `{test_config.test_dir}`')\n        return\n    if test_config.groups is None:\n        groups = get_groups_in_dir(test_config.test_dir)\n    else:\n        groups = set()\n        for pattern in test_config.groups:\n            for group in get_groups_in_dir_matching(test_config.test_dir, pattern):\n                groups.add(group)\n    for group in groups:\n"]]}
{"hexsha": "13a7e5434cc83bfa96ea6615181e8a978e0e4ee0", "ext": "py", "lang": "Python", "content": "@commands.command(\"run-local\")\n@cli_args.MODEL_PATH\n@cli_args.RUN_ID\n@click.option(\"--port\", \"-p\", default=5000, help=\"Server port. [default: 5000]\")\n@click.option(\"--image\", \"-i\", default=IMAGE, help=\"Docker image name\")\n@click.option(\"--flavor\", \"-f\", default=None,\n              help=(\"The name of the flavor to use for local serving. Must be one of the following:\"\n                    \" {supported_flavors}. If unspecified, a flavor will be automatically selected\"\n                    \" from the model's available flavors.\".format(\n                        supported_flavors=mlflow.sagemaker.SUPPORTED_DEPLOYMENT_FLAVORS)))\ndef run_local(model_path, run_id, port, image, flavor):\n    \"\"\"\n    Serve model locally running in a Sagemaker-compatible Docker container.\n    \"\"\"\n    mlflow.sagemaker.run_local(\n        model_path=model_path, run_id=run_id, port=port, image=image, flavor=flavor)", "fn_id": 3, "class_fn": false, "repo": "yutannihilation/mlflow", "file": "mlflow/sagemaker/cli.py", "last_update_at": "2020-07-22T06:11:55+00:00", "question_id": "13a7e5434cc83bfa96ea6615181e8a978e0e4ee0_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@commands.command('run-local')\n@cli_args.MODEL_PATH\n@cli_args.RUN_ID\n@click.option('--port', '-p', default=5000, help='Server port. [default: 5000]')\n@click.option('--image', '-i', default=IMAGE, help='Docker image name')\n@click.option('--flavor', '-f', default=None, help=\"The name of the flavor to use for local serving. Must be one of the following: {supported_flavors}. If unspecified, a flavor will be automatically selected from the model's available flavors.\".format(supported_flavors=mlflow.sagemaker.SUPPORTED_DEPLOYMENT_FLAVORS))\ndef run_local(model_path, run_id, port, image, flavor):\n    \"\"\"\n    Serve model locally running in a Sagemaker-compatible Docker container.\n    \"\"\"\n"]]}
{"hexsha": "4768f9a389d6ec9e2fd064176285deb25c0d877d", "ext": "py", "lang": "Python", "content": "def demo_distance():\n    from ..datasets import public_dataset\n    data = public_dataset(name=\"iris\")\n    x = data.iloc[:, 0]\n    y = data.iloc[:, 1]\n    print(\"Minkowski distance of the 'sepal length(cm)' and 'sepal width(cm)' from the iris data:\")\n    for p_power in [-2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]:\n        p = 2**p_power\n        print(f\"order p={p: .3f}, Minkowski distance = {distance(p=p).Minkowski(x=x, y=y): .3f}\")\n    print(f\"Euclidean distance = {distance().Euclidean(x=x, y=y): .3f}\")", "fn_id": 1, "class_fn": false, "repo": "daniel-yj-yang/pyml", "file": "machlearn/math_and_stats/_stats.py", "last_update_at": "2020-11-18T13:25:27+00:00", "question_id": "4768f9a389d6ec9e2fd064176285deb25c0d877d_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def demo_distance():\n    from ..datasets import public_dataset\n    data = public_dataset(name='iris')\n    x = data.iloc[:, 0]\n    y = data.iloc[:, 1]\n    print(\"Minkowski distance of the 'sepal length(cm)' and 'sepal width(cm)' from the iris data:\")\n    for p_power in [-2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]:\n        p = 2 ** p_power\n        print(f'order p={p: .3f}, Minkowski distance = {distance(p=p).Minkowski(x=x, y=y): .3f}')\n"]]}
{"hexsha": "7b5770c3c7844ccbf260cf913a5169f92fc3d014", "ext": "py", "lang": "Python", "content": "def remove_ponto(cnpj):\n    cnpj_limpo = ''\n    for x in cnpj:\n        if x.isnumeric():\n            cnpj_limpo += x\n    return cnpj_limpo", "fn_id": 0, "class_fn": false, "repo": "Don616/pratica", "file": "python/programas simples/cnpj.py", "last_update_at": "2020-09-24T22:31:00+00:00", "question_id": "7b5770c3c7844ccbf260cf913a5169f92fc3d014_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def remove_ponto(cnpj):\n    cnpj_limpo = ''\n    for x in cnpj:\n        if x.isnumeric():\n            cnpj_limpo += x\n"]]}
{"hexsha": "f46a51432651c4e321748bc817ea07abcffb4985", "ext": "py", "lang": "Python", "content": "def print_graph(relationships, site_nodes):\n    project_keys = []\n    for node in site_nodes:\n        project_keys.append(node)\n\n    print(tabulate(relationships,\n                   headers=project_keys,\n                   showindex=project_keys,\n                   tablefmt='fancy_grid'))", "fn_id": 0, "class_fn": false, "repo": "huntertran/ptidej-wordcloud", "file": "nlp/Linker.py", "last_update_at": "2020-12-16T14:57:31+00:00", "question_id": "f46a51432651c4e321748bc817ea07abcffb4985_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def print_graph(relationships, site_nodes):\n    project_keys = []\n    for node in site_nodes:\n        project_keys.append(node)\n"]]}
{"hexsha": "ed887bb6049ff4625eb036c565dd9926a4a7d433", "ext": "py", "lang": "Python", "content": "async def test_form(hass: HomeAssistant) -> None:\n    \"\"\"Test we get the form.\"\"\"\n    result = await hass.config_entries.flow.async_init(\n        DOMAIN, context={\"source\": config_entries.SOURCE_USER}\n    )\n    assert result[\"type\"] == RESULT_TYPE_FORM\n    assert result[\"errors\"] == {}\n\n    with _patch_discovery(no_device=True), patch(\n        \"homeassistant.components.steamist.config_flow.Steamist.async_get_status\"\n    ), patch(\n        \"homeassistant.components.steamist.async_setup_entry\",\n        return_value=True,\n    ) as mock_setup_entry:\n        result2 = await hass.config_entries.flow.async_configure(\n            result[\"flow_id\"],\n            {\n                \"host\": \"127.0.0.1\",\n            },\n        )\n        await hass.async_block_till_done()\n\n    assert result2[\"type\"] == RESULT_TYPE_CREATE_ENTRY\n    assert result2[\"title\"] == \"127.0.0.1\"\n    assert result2[\"data\"] == {\n        \"host\": \"127.0.0.1\",\n    }\n    assert len(mock_setup_entry.mock_calls) == 1", "fn_id": 0, "class_fn": false, "repo": "MrDelik/core", "file": "tests/components/steamist/test_config_flow.py", "last_update_at": "2020-03-02T12:56:31+00:00", "question_id": "ed887bb6049ff4625eb036c565dd9926a4a7d433_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def test_form(hass: HomeAssistant) -> None:\n    \"\"\"Test we get the form.\"\"\"\n    result = await hass.config_entries.flow.async_init(DOMAIN, context={'source': config_entries.SOURCE_USER})\n    assert result['type'] == RESULT_TYPE_FORM\n    assert result['errors'] == {}\n    with _patch_discovery(no_device=True), patch('homeassistant.components.steamist.config_flow.Steamist.async_get_status'), patch('homeassistant.components.steamist.async_setup_entry', return_value=True) as mock_setup_entry:\n        result2 = await hass.config_entries.flow.async_configure(result['flow_id'], {'host': '127.0.0.1'})\n        await hass.async_block_till_done()\n    assert result2['type'] == RESULT_TYPE_CREATE_ENTRY\n    assert result2['title'] == '127.0.0.1'\n    assert result2['data'] == {'host': '127.0.0.1'}\n"]]}
{"hexsha": "e79fce9f9693f141e9b49b0e400cbc25938a6035", "ext": "py", "lang": "Python", "content": "def create_window():\n    # Pretend the process\n    window = tk.Tk()\n    window.title('My Window')\n    window.geometry('0x0')\n    window.mainloop()", "fn_id": 0, "class_fn": false, "repo": "dzikoysk/cefstream", "file": "tests/cefpython/cefpython_osr_test.py", "last_update_at": "2020-08-17T10:24:16+00:00", "question_id": "e79fce9f9693f141e9b49b0e400cbc25938a6035_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_window():\n    window = tk.Tk()\n    window.title('My Window')\n    window.geometry('0x0')\n"]]}
{"hexsha": "67db3baa9c7e7a9495b74e51cbbdea983ae6d547", "ext": "py", "lang": "Python", "content": "def get_agent(x, reuse=False):\n    \"\"\"\n    Generate the CNN agent\n    :param x: tensor, Input frames concatenated along axis 3\n    :param reuse: bool, True -> Reuse weight variables\n                        False -> Create new ones\n    :return: Tensor, logits for each valid action\n    \"\"\"\n    if reuse:\n        tf.get_variable_scope().reuse_variables()\n\n    x = tf.divide(x, 255.0, name='Normalize')\n    conv_1 = tf.nn.relu(ops.cnn_2d(x, weight_shape=mc.conv_1, strides=mc.stride_1, name='conv_1'))\n    conv_2 = tf.nn.relu(ops.cnn_2d(conv_1, weight_shape=mc.conv_2, strides=mc.stride_2, name='conv_2'))\n    conv_3 = tf.nn.relu(ops.cnn_2d(conv_2, weight_shape=mc.conv_3, strides=mc.stride_3, name='conv_3'))\n    conv_3_r = tf.reshape(conv_3, [-1, 7 * 7 * 64], name='reshape')\n    dense_1 = tf.nn.relu(ops.dense(conv_3_r, 7 * 7 * 64, mc.dense_1, name='dense_1'))\n    output = ops.dense(dense_1, mc.dense_1, mc.dense_2, name='dense_2')\n    return output", "fn_id": 0, "class_fn": false, "repo": "Naresh1318/Atari_using_RL", "file": "generate_dataset.py", "last_update_at": "2020-12-07T21:32:00+00:00", "question_id": "67db3baa9c7e7a9495b74e51cbbdea983ae6d547_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_agent(x, reuse=False):\n    \"\"\"\n    Generate the CNN agent\n    :param x: tensor, Input frames concatenated along axis 3\n    :param reuse: bool, True -> Reuse weight variables\n                        False -> Create new ones\n    :return: Tensor, logits for each valid action\n    \"\"\"\n    if reuse:\n        tf.get_variable_scope().reuse_variables()\n    x = tf.divide(x, 255.0, name='Normalize')\n    conv_1 = tf.nn.relu(ops.cnn_2d(x, weight_shape=mc.conv_1, strides=mc.stride_1, name='conv_1'))\n    conv_2 = tf.nn.relu(ops.cnn_2d(conv_1, weight_shape=mc.conv_2, strides=mc.stride_2, name='conv_2'))\n    conv_3 = tf.nn.relu(ops.cnn_2d(conv_2, weight_shape=mc.conv_3, strides=mc.stride_3, name='conv_3'))\n    conv_3_r = tf.reshape(conv_3, [-1, 7 * 7 * 64], name='reshape')\n    dense_1 = tf.nn.relu(ops.dense(conv_3_r, 7 * 7 * 64, mc.dense_1, name='dense_1'))\n    output = ops.dense(dense_1, mc.dense_1, mc.dense_2, name='dense_2')\n"]]}
{"hexsha": "842aa11162b5ae87e312df6f26e737f7209a554f", "ext": "py", "lang": "Python", "content": "async def notify_leave(room, name):\n    \"\"\"Notify users in [room] that someone [name] left.\n    [room]: list of websockets\n    [name]: name of the person leaving\n    \"\"\"\n    # if the user never identified themselves, don't mention\n    # anything to anyone\n    if name:\n        msg = json.dumps({\"content\": f\"{name} left the room.\"})\n        if room:\n            await asyncio.wait([user.send(msg) for user in room])", "fn_id": 2, "class_fn": false, "repo": "wsowens/accord-chat", "file": "server.py", "last_update_at": "2020-10-22T03:02:01+00:00", "question_id": "842aa11162b5ae87e312df6f26e737f7209a554f_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def notify_leave(room, name):\n    \"\"\"Notify users in [room] that someone [name] left.\n    [room]: list of websockets\n    [name]: name of the person leaving\n    \"\"\"\n    if name:\n        msg = json.dumps({'content': f'{name} left the room.'})\n        if room:\n"]]}
{"hexsha": "188e15d17c57132182d32d6babcbf40c8d59f90a", "ext": "py", "lang": "Python", "content": "def get_version(lang, default):\n    \"\"\"\n    function get_version -- gets language version from config\n\n    This function reads user-specified version of a language from config file.\n    If the version was not found, it fall backs to the default specified.\n\n    params:\n    lang: str -- the language to look up in the config file\n    default: str -- the value to return if version was not found\n    \"\"\"\n    if 'Version' in config[lang]:\n        return config[lang]['Version']\n    return default", "fn_id": 7, "class_fn": false, "repo": "sohnryang/boj-submit", "file": "boj/__init__.py", "last_update_at": "2020-07-09T13:21:08+00:00", "question_id": "188e15d17c57132182d32d6babcbf40c8d59f90a_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_version(lang, default):\n    \"\"\"\n    function get_version -- gets language version from config\n\n    This function reads user-specified version of a language from config file.\n    If the version was not found, it fall backs to the default specified.\n\n    params:\n    lang: str -- the language to look up in the config file\n    default: str -- the value to return if version was not found\n    \"\"\"\n    if 'Version' in config[lang]:\n        return config[lang]['Version']\n"]]}
{"hexsha": "2a8da1237d3594a5b0825382e5e9eee1aee55e77", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize(\n    \"endpoint\",\n    [\n        \"http://1.2.3.4\",\n        \"http://1.2.3.4:8080\",\n        \"http://host.com\",\n        \"http://host.com:8080\",\n    ],\n)\ndef test_external_endpoint_validation_valid(hooks_config, endpoint):\n    \"\"\"Test the validation of the external endpoint used in the \"complete\" hook with\n    URL that are valid.\n    \"\"\"\n    config_dict = hooks_config.serialize()\n\n    config_dict[\"complete\"][\"external_endpoint\"] = endpoint\n    HooksConfiguration.deserialize(config_dict)", "fn_id": 26, "class_fn": false, "repo": "rak-n-rok/Krake", "file": "krake/tests/data/test_serializable.py", "last_update_at": "2020-05-29T08:43:32+00:00", "question_id": "2a8da1237d3594a5b0825382e5e9eee1aee55e77_26", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('endpoint', ['http://1.2.3.4', 'http://1.2.3.4:8080', 'http://host.com', 'http://host.com:8080'])\ndef test_external_endpoint_validation_valid(hooks_config, endpoint):\n    \"\"\"Test the validation of the external endpoint used in the \"complete\" hook with\n    URL that are valid.\n    \"\"\"\n    config_dict = hooks_config.serialize()\n    config_dict['complete']['external_endpoint'] = endpoint\n"]]}
{"hexsha": "2de6b204572c28fb0c1a6c077e8c9cb0a8f35349", "ext": "py", "lang": "Python", "content": "def perc_thresh_n(connections):\n\n    n = len(connections)\n    previous = {}\n\n    for ind in [n-2, n-1]:\n        previous[ind] = {ind}\n\n    if connections[n-1, n-2]:\n        previous[n-1].add(n-2)\n\n    no_points = 2\n\n    \n    while (n-2) not in previous[n-1] and no_points<n:\n        \n        no_points += 1\n        \n        upto = n-no_points\n        previous[upto] = {upto}\n\n        for node_to_current in np.nonzero(connections[n-no_points, n-no_points:])[0]:\n            previous[upto].update(previous[upto + node_to_current])\n\n        nodes_from = set(upto + np.nonzero(connections[upto:, upto])[0])\n\n        for other_node in range(upto, n):\n            if previous[other_node].intersection(nodes_from):\n                previous[other_node].update(previous[upto])\n\n    return no_points", "fn_id": 21, "class_fn": false, "repo": "mwcotton/DAGmetrics", "file": "minkowskitools.py", "last_update_at": "2020-11-07T21:11:56+00:00", "question_id": "2de6b204572c28fb0c1a6c077e8c9cb0a8f35349_21", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def perc_thresh_n(connections):\n    n = len(connections)\n    previous = {}\n    for ind in [n - 2, n - 1]:\n        previous[ind] = {ind}\n    if connections[n - 1, n - 2]:\n        previous[n - 1].add(n - 2)\n    no_points = 2\n    while n - 2 not in previous[n - 1] and no_points < n:\n        no_points += 1\n        upto = n - no_points\n        previous[upto] = {upto}\n        for node_to_current in np.nonzero(connections[n - no_points, n - no_points:])[0]:\n            previous[upto].update(previous[upto + node_to_current])\n        nodes_from = set(upto + np.nonzero(connections[upto:, upto])[0])\n        for other_node in range(upto, n):\n            if previous[other_node].intersection(nodes_from):\n                previous[other_node].update(previous[upto])\n"]]}
{"hexsha": "16d3163ada87d80c9109401edea455d226b76fb9", "ext": "py", "lang": "Python", "content": "def ball_test():\n    cnf = Canvas3DFrame(None)\n    vts, fs, ns, cs = geoutil.build_ball((100,100,100),50, (1,0,0))\n    cnf.add_surf('ball', vts, fs, ns, cs)\n    cnf.Show()", "fn_id": 1, "class_fn": false, "repo": "cycleuser/imagepy", "file": "sciwx/demo/mesh3_geoutil.py", "last_update_at": "2020-06-17T05:16:46+00:00", "question_id": "16d3163ada87d80c9109401edea455d226b76fb9_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ball_test():\n    cnf = Canvas3DFrame(None)\n    vts, fs, ns, cs = geoutil.build_ball((100, 100, 100), 50, (1, 0, 0))\n    cnf.add_surf('ball', vts, fs, ns, cs)\n"]]}
{"hexsha": "3607b51f0527a413dfd012afbddd58e748fb41bb", "ext": "py", "lang": "Python", "content": "def chunks2bio(chunks, sent_len):\n    bio_tags = ['O'] * sent_len\n    for (start, end, label) in chunks:\n        bio_tags[start] = 'B-'+label\n        for j in range(start+1, end):\n            bio_tags[j] = 'I-'+label\n    return bio_tags", "fn_id": 2, "class_fn": false, "repo": "saxogrammaticus/danlp", "file": "danlp/models/spacy_models.py", "last_update_at": "2020-02-12T09:59:04+00:00", "question_id": "3607b51f0527a413dfd012afbddd58e748fb41bb_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def chunks2bio(chunks, sent_len):\n    bio_tags = ['O'] * sent_len\n    for start, end, label in chunks:\n        bio_tags[start] = 'B-' + label\n        for j in range(start + 1, end):\n            bio_tags[j] = 'I-' + label\n"]]}
{"hexsha": "2d5a95d03fe34251466cc2dffbed1aac34a8714b", "ext": "py", "lang": "Python", "content": "def parse_duration(duration: str, /) -> pendulum.Duration:\n    duration = duration.strip()\n\n    if not duration:\n        raise ValueError('No duration provided.')\n\n    args: dict[str, int] = {}\n    digits = ''\n\n    for char in duration:\n        if char.isdigit():\n            digits += char\n            continue\n\n        if char == ' ':\n            if len(digits) > 0:\n                raise ValueError('Invalid duration')\n\n            continue  # pragma: no cover\n\n        if char not in UNITS or not digits:\n            raise ValueError('Invalid duration')\n\n        args[UNITS[char]] = int(digits)\n        digits = ''\n\n    return pendulum.duration(**args)", "fn_id": 0, "class_fn": false, "repo": "gpontesss/botus_receptus", "file": "src/botus_receptus/util.py", "last_update_at": "2020-04-07T13:31:19+00:00", "question_id": "2d5a95d03fe34251466cc2dffbed1aac34a8714b_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def parse_duration(duration: str, /) -> pendulum.Duration:\n    duration = duration.strip()\n    if not duration:\n        raise ValueError('No duration provided.')\n    args: dict[str, int] = {}\n    digits = ''\n    for char in duration:\n        if char.isdigit():\n            digits += char\n            continue\n        if char == ' ':\n            if len(digits) > 0:\n                raise ValueError('Invalid duration')\n            continue\n        if char not in UNITS or not digits:\n            raise ValueError('Invalid duration')\n        args[UNITS[char]] = int(digits)\n        digits = ''\n"]]}
{"hexsha": "8e0c36be28eea50c71e90b501763b6ebed13be18", "ext": "py", "lang": "Python", "content": "def _get_columns(item):\n    column_map = {\n    }\n    inv_columns = ['']\n    return sdk_utils.get_osc_show_columns_for_sdk_resource(item, column_map,\n                                                           inv_columns)", "fn_id": 0, "class_fn": false, "repo": "zsoltn/python-otcextensions", "file": "otcextensions/osclient/ces/v1/alarm.py", "last_update_at": "2020-01-08T10:03:00+00:00", "question_id": "8e0c36be28eea50c71e90b501763b6ebed13be18_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _get_columns(item):\n    column_map = {}\n    inv_columns = ['']\n"]]}
{"hexsha": "222a892a1147c783fa0085707592bb7f4944af76", "ext": "py", "lang": "Python", "content": "def test_asset_key_not_in_list(setup: str) -> None:\n    target = setup\n    collection = Collection(\"fake_title\")\n    collection.survey = \"survey_id\"\n    collection.description = \"fake_description\"\n    collection.license = \"fake_license\"\n    item = Item(\"item_id\")\n    item.datetime = datetime.now()\n    item.linz_geospatial_type = \"black and white image\"\n    collection.add_item(item)\n    item.collection = collection\n\n    test_asset = Asset(\"./test_data/tiffs/SURVEY_1/CONTROL.tiff\")\n    test_asset.target = \"fake_title/fake_target.tiff\"\n    test_asset.key_name = None\n    item.add_asset(test_asset)\n\n    with pytest.raises(Exception, match=r\"No asset key set for asset ./item_id.tiff\"):\n        transfer_collection(item.collection, target, DataType(\"imagery.historic\"))", "fn_id": 1, "class_fn": false, "repo": "linz/processor-aerial-imagery", "file": "topo_processor/util/tests/transfer_collection_test.py", "last_update_at": "2020-09-21T06:28:57+00:00", "question_id": "222a892a1147c783fa0085707592bb7f4944af76_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_asset_key_not_in_list(setup: str) -> None:\n    target = setup\n    collection = Collection('fake_title')\n    collection.survey = 'survey_id'\n    collection.description = 'fake_description'\n    collection.license = 'fake_license'\n    item = Item('item_id')\n    item.datetime = datetime.now()\n    item.linz_geospatial_type = 'black and white image'\n    collection.add_item(item)\n    item.collection = collection\n    test_asset = Asset('./test_data/tiffs/SURVEY_1/CONTROL.tiff')\n    test_asset.target = 'fake_title/fake_target.tiff'\n    test_asset.key_name = None\n    item.add_asset(test_asset)\n    with pytest.raises(Exception, match='No asset key set for asset ./item_id.tiff'):\n"]]}
{"hexsha": "32bca387c950131122c8810c44812e28258f86fb", "ext": "py", "lang": "Python", "content": "def test_timer():\n    with util.time.timer() as t:\n        time.sleep(1e-6)\n    assert t['seconds'] > 0", "fn_id": 0, "class_fn": false, "repo": "nathants/s", "file": "tests/test_time.py", "last_update_at": "2020-03-01T03:00:36+00:00", "question_id": "32bca387c950131122c8810c44812e28258f86fb_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_timer():\n    with util.time.timer() as t:\n        time.sleep(1e-06)\n"]]}
{"hexsha": "c827dcf472f9c3a54a8d6e47ea5c2911bca311fc", "ext": "py", "lang": "Python", "content": "def cancelChainlinkRequest(sender, requestId, payment, callbackFunctionId, expiration):\n    RequireWitness(sender)\n    oracle = Get(GetContext(), concatKey(PENDING_REQUESTS_PREFIX, requestId))\n    params = [sender, requestId, payment, GetCallingScriptHash(), callbackFunctionId, expiration]\n    assert (DynamicCallFunction(bytearray_reverse(oracle), \"cancelOracleRequest\", params))\n    Delete(GetContext(), concatKey(PENDING_REQUESTS_PREFIX, requestId))\n    ChainlinkCancelledEvent(requestId)\n    return True", "fn_id": 2, "class_fn": false, "repo": "ontio/oracle-chainlink-smartcontract", "file": "ont-contracts/chainlink_client.py", "last_update_at": "2020-12-31T06:42:51+00:00", "question_id": "c827dcf472f9c3a54a8d6e47ea5c2911bca311fc_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def cancelChainlinkRequest(sender, requestId, payment, callbackFunctionId, expiration):\n    RequireWitness(sender)\n    oracle = Get(GetContext(), concatKey(PENDING_REQUESTS_PREFIX, requestId))\n    params = [sender, requestId, payment, GetCallingScriptHash(), callbackFunctionId, expiration]\n    assert DynamicCallFunction(bytearray_reverse(oracle), 'cancelOracleRequest', params)\n    Delete(GetContext(), concatKey(PENDING_REQUESTS_PREFIX, requestId))\n    ChainlinkCancelledEvent(requestId)\n"]]}
{"hexsha": "7156199fb832e7d8f6893c42b29e477a2c978f37", "ext": "py", "lang": "Python", "content": "@app.route('/adapters/<service_platform>/instantiations', methods=['POST'])\ndef serviceInstantiation(service_platform):\n    content = request.get_json()\n    LOG.debug(\"service_uuid : \"+content['service_uuid'])\n    service_uuid = content['service_uuid']\n    instantiate_str = \"{\\\"service_uuid\\\": \\\"\" + service_uuid + \"\\\"}\" \n    ad = adapter.Adapter(service_platform)      \n    return ad.instantiation(instantiate_str)", "fn_id": 34, "class_fn": false, "repo": "sonata-nfv/tng-vnv-platform-adapter", "file": "main.py", "last_update_at": "2020-01-08T15:53:14+00:00", "question_id": "7156199fb832e7d8f6893c42b29e477a2c978f37_34", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.route('/adapters/<service_platform>/instantiations', methods=['POST'])\ndef serviceInstantiation(service_platform):\n    content = request.get_json()\n    LOG.debug('service_uuid : ' + content['service_uuid'])\n    service_uuid = content['service_uuid']\n    instantiate_str = '{\"service_uuid\": \"' + service_uuid + '\"}'\n    ad = adapter.Adapter(service_platform)\n"]]}
{"hexsha": "b36010a1c05ae0de61fa1341d49cbcdd5a6a14c0", "ext": "py", "lang": "Python", "content": "def _get_build_number(build_folder: str, build_def: BuildDef) -> Optional[str]:\n    \"\"\"Get the build number of a completed build by inspecting the produced\n    assembly.\n\n    Will return None if an error occurred.\n    \"\"\"\n    assembly_path = _get_assembly_path(build_def)\n    if assembly_path is None:\n        return None\n\n    assembly_path = path.join(build_folder, assembly_path)\n\n    # we're going whole-hog here and loading the PE file itself to figure out\n    # the version number... writing it to a TXT file during the Unity build\n    # wasn't producing consistent version numbers as it seemed to be building\n    # the assembly twice and only writing the number after the first build\n    try:\n        pe = pefile.PE(assembly_path)\n        ver_info = pe.VS_FIXEDFILEINFO[0]\n        ver_ms = ver_info.ProductVersionMS\n        ver_ls = ver_info.ProductVersionLS\n        major = ver_ms >> 16  # version numbers are 2 bytes each - grab em like this\n        minor = ver_ms & 0xFFFF\n        build = ver_ls >> 16\n        patch = ver_ls & 0xFFFF\n        return f\"{major}.{minor}.{build}.{patch}\"\n    except OSError as err:\n        logging.error(f\"Could not open built assembly: {err}\")\n        return None", "fn_id": 1, "class_fn": false, "repo": "Figglewatts/toriicli", "file": "toriicli/build/build_data.py", "last_update_at": "2020-11-13T17:28:47+00:00", "question_id": "b36010a1c05ae0de61fa1341d49cbcdd5a6a14c0_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _get_build_number(build_folder: str, build_def: BuildDef) -> Optional[str]:\n    \"\"\"Get the build number of a completed build by inspecting the produced\n    assembly.\n\n    Will return None if an error occurred.\n    \"\"\"\n    assembly_path = _get_assembly_path(build_def)\n    if assembly_path is None:\n        return None\n    assembly_path = path.join(build_folder, assembly_path)\n    try:\n        pe = pefile.PE(assembly_path)\n        ver_info = pe.VS_FIXEDFILEINFO[0]\n        ver_ms = ver_info.ProductVersionMS\n        ver_ls = ver_info.ProductVersionLS\n        major = ver_ms >> 16\n        minor = ver_ms & 65535\n        build = ver_ls >> 16\n        patch = ver_ls & 65535\n        return f'{major}.{minor}.{build}.{patch}'\n    except OSError as err:\n        logging.error(f'Could not open built assembly: {err}')\n"]]}
{"hexsha": "913c1814ae960b7b49a7fa6851e3371d07acf33f", "ext": "py", "lang": "Python", "content": "def utility_for_consumers(\n    quality,\n    prev_usage,\n    usage_counter,\n    privacy_concern,\n    firm_privacy_score,\n    util_weight_dict,\n):\n    \"\"\"\n    Input:\n    - quality = (firm, category)\n    - prev_usage = (consumer, category, firm)\n    - usage_counter = (consumer, category, firm)\n    - privacy_concern = (consumer)\n    - firm_privacy_score = (firm)\n    - util_weight_dict has all the weights we need\n    returns (Customer, Category, Firm): utility of each product for the customer (whether or not it exists).\n    \"\"\"\n    n_consumers, n_categories, n_firms = prev_usage.shape\n    usage_company = usage_counter.sum(axis=1)\n    w_priv = util_weight_dict[\"w_priv\"]\n    w_qual = util_weight_dict[\"w_qual\"]\n    w_loyal_firm = util_weight_dict[\"w_loyal_firm\"]\n    w_loyal_category = util_weight_dict[\"w_loyal_category\"]\n    return (\n        w_qual * quality.T[None, :, :]\n        + w_loyal_category * usage_counter\n        + w_loyal_firm * usage_company[:, None, :]\n        - w_priv\n        * privacy_concern[:, None, None]\n        * (1 - firm_privacy_score)[None, None, :]\n    )", "fn_id": 0, "class_fn": false, "repo": "theodi/data-sharing-abm-model", "file": "model/utility.py", "last_update_at": "2020-03-06T03:06:18+00:00", "question_id": "913c1814ae960b7b49a7fa6851e3371d07acf33f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def utility_for_consumers(quality, prev_usage, usage_counter, privacy_concern, firm_privacy_score, util_weight_dict):\n    \"\"\"\n    Input:\n    - quality = (firm, category)\n    - prev_usage = (consumer, category, firm)\n    - usage_counter = (consumer, category, firm)\n    - privacy_concern = (consumer)\n    - firm_privacy_score = (firm)\n    - util_weight_dict has all the weights we need\n    returns (Customer, Category, Firm): utility of each product for the customer (whether or not it exists).\n    \"\"\"\n    n_consumers, n_categories, n_firms = prev_usage.shape\n    usage_company = usage_counter.sum(axis=1)\n    w_priv = util_weight_dict['w_priv']\n    w_qual = util_weight_dict['w_qual']\n    w_loyal_firm = util_weight_dict['w_loyal_firm']\n    w_loyal_category = util_weight_dict['w_loyal_category']\n"]]}
{"hexsha": "16d23cdf8171322b2fc9c1e4894149bde6d3cf03", "ext": "py", "lang": "Python", "content": "def validate_height(height: str):\n    if height[-2:] == 'cm':\n        nr = int(height[:height.index('cm')])\n        if 150 <= nr <= 193:\n            return True\n    if height[-2:] == 'in':\n        nr = int(height[:height.index('in')])\n        if 59 <= nr <= 76:\n            return True\n    return False", "fn_id": 6, "class_fn": false, "repo": "ChrisCh7/advent-of-code", "file": "2020/day4/day4.py", "last_update_at": "2020-12-03T23:20:53+00:00", "question_id": "16d23cdf8171322b2fc9c1e4894149bde6d3cf03_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def validate_height(height: str):\n    if height[-2:] == 'cm':\n        nr = int(height[:height.index('cm')])\n        if 150 <= nr <= 193:\n            return True\n    if height[-2:] == 'in':\n        nr = int(height[:height.index('in')])\n        if 59 <= nr <= 76:\n            return True\n"]]}
{"hexsha": "c57dccfa7699fdddff62f1aa8b166a2bff1de595", "ext": "py", "lang": "Python", "content": "def downgrade():\n    bind = op.get_bind()\n    session = db.Session(bind=bind)\n\n    for slc in session.query(Slice).filter(Slice.viz_type == 'pie').all():\n        try:\n            params = json.loads(slc.params)\n\n            if 'metric' in params:\n                if params['metric']:\n                    params['metrics'] = [params['metric']]\n\n                del params['metric']\n                slc.params = json.dumps(params, sort_keys=True)\n        except Exception:\n            pass\n\n    session.commit()\n    session.close()", "fn_id": 1, "class_fn": false, "repo": "MukaJiTrue/incubator-superset", "file": "superset/migrations/versions/80a67c5192fa_single_pie_chart_metric.py", "last_update_at": "2020-11-26T07:38:24+00:00", "question_id": "c57dccfa7699fdddff62f1aa8b166a2bff1de595_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def downgrade():\n    bind = op.get_bind()\n    session = db.Session(bind=bind)\n    for slc in session.query(Slice).filter(Slice.viz_type == 'pie').all():\n        try:\n            params = json.loads(slc.params)\n            if 'metric' in params:\n                if params['metric']:\n                    params['metrics'] = [params['metric']]\n                del params['metric']\n                slc.params = json.dumps(params, sort_keys=True)\n        except Exception:\n            pass\n    session.commit()\n"]]}
{"hexsha": "adbc181767aff9fc1b04daf91833d5790e1e9dca", "ext": "py", "lang": "Python", "content": "def extract_summaries(logdir: str):\n    \"\"\" extracts and pickles only relevant scalars\n\n    :param logdir: Path to the directory having event logs\n    \"\"\"\n    # Collect data : we recognize all files which have tfevents\n    scalars_info = defaultdict(dict)\n\n    for root, dirs, files in os.walk(logdir):\n        game_name = root.split('-v')[0].split('/')[-1]\n        event_files = [x for x in files if 'tfevents' in x]\n        if len(event_files) > 0:\n            assert len(event_files) == 1, 'only one tf file allowed per experiment.'\n\n            if game_name not in scalars_info:\n                scalars_info[game_name] = {'fixed': {}, 'variable': {}}\n\n            event_path = os.path.join(root, event_files[0])\n            acc = ea.EventAccumulator(event_path)\n            acc.Reload()  # load data\n\n            if 'fixed' in root:\n                repeat_mode = 'fixed'\n                repeat_count = root.split('action_repeat_')[1].split('/')[0]\n                if repeat_count not in scalars_info[game_name]['fixed']:\n                    scalars_info[game_name]['fixed'][repeat_count] = {'test': {'seed': {}},\n                                                                      'avg_action_repeat': {'seed': {}}}\n                dest = scalars_info[game_name]['fixed'][repeat_count]\n            elif 'variable' in root:\n                repeat_mode = 'variable'\n                repeat_count = ''\n                if 'test' not in scalars_info[game_name]['variable']:\n                    scalars_info[game_name]['variable'] = {'test': {'seed': {}},\n                                                           'avg_action_repeat': {'seed': {}}}\n                dest = scalars_info[game_name]['variable']\n            else:\n                raise NotImplementedError\n\n            seed = root.split('seed_')[1].split('/')[0]\n            x = [s.step for s in acc.Scalars(TEST_TAG['ref'])]\n            y = [s.value for s in acc.Scalars(TEST_TAG['ref'])]\n            dest['test']['seed'][seed] = {'x': x, 'y': y}\n\n            x = [s.step for s in acc.Scalars(AVG_ACTION_REPEAT_TAG['ref'])]\n            y = [s.value for s in acc.Scalars(AVG_ACTION_REPEAT_TAG['ref'])]\n            dest['avg_action_repeat']['seed'][seed] = {'x': x, 'y': y}\n\n            print('Processed {}, seed:{} , mode:{} , repeat: {}'.format(game_name, seed, repeat_mode, repeat_count))\n\n    return scalars_info", "fn_id": 0, "class_fn": false, "repo": "koulanurag/variable-td3", "file": "scripts/summary_graphs.py", "last_update_at": "2020-12-24T17:09:37+00:00", "question_id": "adbc181767aff9fc1b04daf91833d5790e1e9dca_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def extract_summaries(logdir: str):\n    \"\"\" extracts and pickles only relevant scalars\n\n    :param logdir: Path to the directory having event logs\n    \"\"\"\n    scalars_info = defaultdict(dict)\n    for root, dirs, files in os.walk(logdir):\n        game_name = root.split('-v')[0].split('/')[-1]\n        event_files = [x for x in files if 'tfevents' in x]\n        if len(event_files) > 0:\n            assert len(event_files) == 1, 'only one tf file allowed per experiment.'\n            if game_name not in scalars_info:\n                scalars_info[game_name] = {'fixed': {}, 'variable': {}}\n            event_path = os.path.join(root, event_files[0])\n            acc = ea.EventAccumulator(event_path)\n            acc.Reload()\n            if 'fixed' in root:\n                repeat_mode = 'fixed'\n                repeat_count = root.split('action_repeat_')[1].split('/')[0]\n                if repeat_count not in scalars_info[game_name]['fixed']:\n                    scalars_info[game_name]['fixed'][repeat_count] = {'test': {'seed': {}}, 'avg_action_repeat': {'seed': {}}}\n                dest = scalars_info[game_name]['fixed'][repeat_count]\n            elif 'variable' in root:\n                repeat_mode = 'variable'\n                repeat_count = ''\n                if 'test' not in scalars_info[game_name]['variable']:\n                    scalars_info[game_name]['variable'] = {'test': {'seed': {}}, 'avg_action_repeat': {'seed': {}}}\n                dest = scalars_info[game_name]['variable']\n            else:\n                raise NotImplementedError\n            seed = root.split('seed_')[1].split('/')[0]\n            x = [s.step for s in acc.Scalars(TEST_TAG['ref'])]\n            y = [s.value for s in acc.Scalars(TEST_TAG['ref'])]\n            dest['test']['seed'][seed] = {'x': x, 'y': y}\n            x = [s.step for s in acc.Scalars(AVG_ACTION_REPEAT_TAG['ref'])]\n            y = [s.value for s in acc.Scalars(AVG_ACTION_REPEAT_TAG['ref'])]\n            dest['avg_action_repeat']['seed'][seed] = {'x': x, 'y': y}\n            print('Processed {}, seed:{} , mode:{} , repeat: {}'.format(game_name, seed, repeat_mode, repeat_count))\n"]]}
{"hexsha": "03cbde7afaaf370dff1a69ad6713b70c2e5595e3", "ext": "py", "lang": "Python", "content": "def main():\n    # Prompt for a colour - TODO: add validation\n    colour = input(\"Please pick a Colour: \")\n    print()\n    # Print out the colour a letter per row, slowly\n    for c in colour:\n        print (str.upper(c) + \"...\")\n        time.sleep(0.5)\n\n    # Prompt for a number - TODO: add validation\n    n = get_positive_int()\n    print()\n    # Count out slowly\n    i = 0\n    for i in range(1, n+1):\n        print (i, end=\"\")\n        time.sleep(0.2)\n        print(\".\", end=\"\")\n        time.sleep(0.2)\n        print(\".\", end=\"\")\n        time.sleep(0.2)\n        print(\".\", end=\"\\n\")\n        i =+ 1\n\n    # Generate a number at random\n    final = random.randrange(1,8,1)\n    #print(\"\\nThe random number is: \" + str(final))\n    time.sleep(0.5)\n    # Build suspense\n    print (\"\\nI will now open the flap...\", end=\"\\n\\n\")\n    time.sleep(2.0)\n    # Return the instruction\n    if (final == 1) or (final == 5):\n        print(str.center(\"You must turn left.\", 30))\n    elif (final == 2) or (final == 6):\n        print(str.center(\"You must turn right.\", 30))\n    elif (final == 3) or (final == 8):\n        print(str.center(\"You must turn back.\", 30))\n    else:\n        print(str.center(\"You must continue ahead.\", 30))\n    print()", "fn_id": 0, "class_fn": false, "repo": "tj2904/childhood_games", "file": "picker.py", "last_update_at": "2020-11-25T17:08:39+00:00", "question_id": "03cbde7afaaf370dff1a69ad6713b70c2e5595e3_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    colour = input('Please pick a Colour: ')\n    print()\n    for c in colour:\n        print(str.upper(c) + '...')\n        time.sleep(0.5)\n    n = get_positive_int()\n    print()\n    i = 0\n    for i in range(1, n + 1):\n        print(i, end='')\n        time.sleep(0.2)\n        print('.', end='')\n        time.sleep(0.2)\n        print('.', end='')\n        time.sleep(0.2)\n        print('.', end='\\n')\n        i = +1\n    final = random.randrange(1, 8, 1)\n    time.sleep(0.5)\n    print('\\nI will now open the flap...', end='\\n\\n')\n    time.sleep(2.0)\n    if final == 1 or final == 5:\n        print(str.center('You must turn left.', 30))\n    elif final == 2 or final == 6:\n        print(str.center('You must turn right.', 30))\n    elif final == 3 or final == 8:\n        print(str.center('You must turn back.', 30))\n    else:\n        print(str.center('You must continue ahead.', 30))\n"]]}
{"hexsha": "750c4afc515bd1464204757357d9abb97ef72e1f", "ext": "py", "lang": "Python", "content": "def prog_bar(total, done, present=\"In Progress\", past=\"complete\"):\n    ratio = int((done / total) * 40)\n    black = ratio * \"\u25a0\"\n    white = (40 - ratio) * \" \"\n    print(f\"{present}... {black}{white}| {done}/{total} {past}\", flush=True, end=\"\\r\")", "fn_id": 0, "class_fn": false, "repo": "tna-webarchive/BX_tools", "file": "capture.py", "last_update_at": "2020-10-19T18:06:38+00:00", "question_id": "750c4afc515bd1464204757357d9abb97ef72e1f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def prog_bar(total, done, present='In Progress', past='complete'):\n    ratio = int(done / total * 40)\n    black = ratio * '\u25a0'\n    white = (40 - ratio) * ' '\n"]]}
{"hexsha": "f4e51510ce220d2328c476c7c1ebe7f2984840cd", "ext": "py", "lang": "Python", "content": "def get_kw(string):\n    \"\"\"Return all the keywords in a string.\"\"\"\n    try:\n        keys = keywords(string)\n        keys = keys['keywords']\n        key_list = []\n        for i in range(len(keys)):\n            key_list.append(keys[i]['keyword'])\n        return key_list\n\n    except:\n        print(\"Error in \", string)", "fn_id": 3, "class_fn": false, "repo": "danish17/litmus-ai-web", "file": "src/api_call.py", "last_update_at": "2020-04-08T15:20:08+00:00", "question_id": "f4e51510ce220d2328c476c7c1ebe7f2984840cd_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_kw(string):\n    \"\"\"Return all the keywords in a string.\"\"\"\n    try:\n        keys = keywords(string)\n        keys = keys['keywords']\n        key_list = []\n        for i in range(len(keys)):\n            key_list.append(keys[i]['keyword'])\n        return key_list\n    except:\n"]]}
{"hexsha": "3a3eac2f902dedabd9a07416e292de3a98f668b5", "ext": "py", "lang": "Python", "content": "def run_worker_build(client,\n    host,\n    artifact,\n    build_number,\n    environment,\n    dependency,\n    provider,\n    component,\n    command,\n    previous_outputs,\n    builds):\n    class Worker(Thread):\n        def run(self):\n            self.success = False\n            exit_code_path = get_exit_code_path(project_directory, environment, provider, component, command, build_number)\n            if os.path.exists(exit_code_path):\n                os.remove(exit_code_path)\n            self.error = False\n            lock.acquire()\n            builds_filename = get_builds_filename(environment, provider, component, command)\n            ensure_file(builds_filename)\n            build_data = json.loads(open(builds_filename).read())\n            this_build = {\n                \"success\": False,\n                \"remote\": True,\n                \"build_number\": build_number,\n                \"reference\": dependency,\n                \"status\": \"running\",\n                \"environment\": environment\n            }\n            state[\"running\"].append(this_build)\n            mark_dependency_as_running(dependency, environment)\n            build_data[\"builds\"].append(this_build)\n            write_builds_file(builds_filename, build_data)\n            lock.release()\n            last_successful_build = find_last_successful_build(builds)\n            if last_successful_build:\n                try:\n                    last_logfile = \"builds/logs/{:03d}-{}-{}-{}-{}.log\".format(last_successful_build[\"build_number\"],\n                        environment,\n                        provider, component,\n                        command)\n\n                    last_size = os.stat(last_logfile).st_size\n                    this_build[\"last_size\"] = last_size\n                    this_build[\"log_file\"] = log_filename\n                except Exception as e:\n                    pass\n\n\n            print(\"Creating remote directories\")\n            work_directory = \"builds\"\n            cmd = client.run_command(\"mkdir -p builds/exits\")\n            client.join(cmd)\n            cmd = client.run_command(\"mkdir -p builds/outputs\")\n            client.join(cmd)\n            cmd = client.run_command(\"mkdir -p builds/envs\")\n            client.join(cmd)\n            cmd = client.run_command(\"mkdir -p builds/logs\")\n            client.join(cmd)\n            cmd = client.run_command(\"mkdir -p builds/published\")\n            client.join(cmd)\n\n\n            env = construct_environment(\"\", build_number, environment, provider, component, command, previous_outputs)\n            work_path = \"builds/work/{}.{}.{}.{}\".format(environment, provider, component, command)\n            make_work_path = client.run_command(\"mkdir -p {}\".format(work_path))\n            client.join(make_work_path)\n\n            print(\"Unpacking artifact remotely\")\n            unpack_command = \"tar -xvf {} -C {}\".format(artifact, work_path)\n\n            extract = client.run_command(unpack_command)\n            client.join(extract)\n            env_file_name = \"{}.{}.{}.{}.{}\".format(environment, provider, component, command, build_number)\n            env_file_path = os.path.join(project_directory, \"builds/envs/\", env_file_name)\n            env_file = open(env_file_path, \"w\")\n            for key, value in env.items():\n                escaped_value = value\n                if \" \" in value:\n                    escaped_value = \"\\\"\" + value + \"\\\"\"\n                env_file.write(\"{}={}\".format(key, escaped_value))\n\n                env_file.write(\"\\n\")\n            env_file.flush()\n            env_file.close()\n            print(\"Sending envs file to worker\")\n            remote_env_path = os.path.join(\"builds/envs\", env_file_name)\n            cmds = client.scp_send(env_file_path, remote_env_path, True)\n            joinall(cmds, raise_error=True)\n            ran_remotely = False\n            build_stdout = None\n            build_stderr = None\n            test_command = \"test -f {}/{}/{}\".format(work_path, provider, command)\n            print(test_command)\n            print(work_path)\n            test_exists = client.run_command(test_command)\n            client.join(test_exists)\n\n            print(\"Running require\")\n            chosen_file = run_require(\"\", environment, provider, component, command)\n            if chosen_file:\n                 print(chosen_file)\n                 destination_path = os.path.join(work_path, os.path.basename(chosen_file))\n                 print(\"Copying {} to remote {}\".format(chosen_file, destination_path))\n\n                 # cmds = client.scp_send(chosen_file, destination_path, True)\n                 # joinall(cmds, raise_error=True)\n                 rsync_command = \"rsync -Pav -e \\\"ssh -i {}\\\" {} {}@{}:{}\".format(\n                    args.workers_key[0],\n                    chosen_file,\n                    args.workers_user,\n                    host,\n                    destination_path)\n                 print(rsync_command)\n                 rsync = Popen([\"bash\", \"-c\", rsync_command])\n                 rsync.communicate()\n\n                 unzip_command = \"cd {} ; tar -xf {}\".format(work_path, os.path.basename(chosen_file))\n                 unzip_artifact = client.run_command(unzip_command)\n                 print(unzip_command)\n                 client.join(unzip_artifact)\n\n\n            if test_exists[host][\"exit_code\"] == 0:\n                print(\"Running build remotely\")\n                ran_remotely = True\n                remote_log_filename = \"builds/logs/{}-{}.{}.{}.{}.log\".format(build_number, environment, provider, component, command)\n                build_command = client.run_command(\"\"\"set -a ;\n                    source {} ;\n                    export OUTPUT_PATH=$(readlink -f ${{OUTPUT_PATH}}) ;\n                    export EXIT_CODE_PATH=$(readlink -f ${{EXIT_CODE_PATH}}) ;\n                    export ARTIFACT_PATH=$(readlink -f ${{ARTIFACT_PATH}}) ;\n                    cd {}/{} ;\n                    ./{} {} {}\"\"\".format(remote_env_path, work_path, provider, command, environment, component, remote_log_filename))\n\n            else:\n                lock.acquire()\n                builds_filename = get_builds_filename(environment, provider, component, command)\n                ensure_file(builds_filename)\n                build_data = json.loads(open(builds_filename).read())\n                this_build = build_data[\"builds\"][-1]\n                print(\"Not implemented\")\n                this_build[\"success\"] = True\n                self.success = True\n                open(os.path.join(project_directory,\n                    \"builds/outputs/{}.{}.{}.{}.outputs.json\"\n                    .format(environment, provider, component, command)), 'w').write(\"{}\")\n                write_builds_file(builds_filename, build_data)\n                lock.release()\n                remove_from_running(dependency, environment)\n                open(os.path.join(get_last_run_path(environment, provider, component, command)), 'w').write(':)')\n                return\n            if ran_remotely:\n                print(\"Setting up logs...\")\n                logfile = open(\"builds/logs/{}-{}.{}.{}.{}.log\".format(build_number, environment, provider, component, command), \"w\")\n\n                client.join(build_command)\n                for line in build_command[host][\"stdout\"]:\n                     logfile.write(line + \"\\n\")\n                for line in build_command[host][\"stderr\"]:\n                    logfile.write(line + \"\\n\")\n                print(\"Remote build finished\")\n                remove_from_running(dependency, environment)\n\n                print(\"Downloading outputs...\")\n                dest_output = os.path.join(project_directory, \"builds/outputs/{}.{}.{}.{}.outputs.json\".format(environment, provider, component, command))\n\n                receive_outputs = client.copy_remote_file(os.path.join(\"builds/outputs/{}.{}.{}.{}.outputs.json\".format(environment, provider, component, command)), dest_output)\n\n                joinall(receive_outputs)\n                os.rename(\"{}_{}\".format(dest_output, host), dest_output)\n\n                print(\"Downloading exit code...\")\n                dest_exit_code = os.path.join(project_directory, \"builds/exits/{}.{}.{}.{}.{}.exitcode\".format(environment, provider, component, command, build_number))\n                receive_exit_code = client.copy_remote_file(os.path.join(\"builds/exits/{}.{}.{}.{}.{}.exitcode\".format(environment, provider, component, command, build_number)), dest_exit_code)\n\n                joinall(receive_exit_code, raise_error=True)\n                os.rename(\"{}_{}\".format(dest_exit_code, host), dest_exit_code)\n\n                print(\"Downloading artifact...\")\n                check_for_artifacts = client.run_command(\"test -f {}\".format(\"builds/published/{}.{}.{}.{}.{}.tgz\".format(environment, provider, component, command, build_number)))\n                client.join(check_for_artifacts)\n\n                if check_for_artifacts[host][\"exit_code\"] == 0:\n                    dest_artifact = os.path.join(project_directory, \"builds/published/{}.{}.{}.{}.{}.tgz\".format(environment, provider, component, command, build_number))\n                    receive_artifact = client.copy_remote_file(os.path.join(\"builds/published/{}.{}.{}.{}.{}.tgz\".format(environment, provider, component, command, build_number)), dest_artifact)\n                    joinall(receive_artifact, raise_error=True)\n                    os.rename(\"{}_{}\".format(dest_artifact, host), dest_artifact)\n\n                last_running_build = Component(dependency, environment, provider, component, command, args).calculate_state()\n                if last_running_build:\n                    self.success = last_running_build[\"success\"]\n\n    worker = Worker()\n    worker.start()\n    return worker", "fn_id": 27, "class_fn": false, "repo": "samsquire/devops-pipeline", "file": "mazzle/mazzle.py", "last_update_at": "2020-03-19T05:39:17+00:00", "question_id": "3a3eac2f902dedabd9a07416e292de3a98f668b5_27", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def run_worker_build(client, host, artifact, build_number, environment, dependency, provider, component, command, previous_outputs, builds):\n\n    class Worker(Thread):\n\n        def run(self):\n            self.success = False\n            exit_code_path = get_exit_code_path(project_directory, environment, provider, component, command, build_number)\n            if os.path.exists(exit_code_path):\n                os.remove(exit_code_path)\n            self.error = False\n            lock.acquire()\n            builds_filename = get_builds_filename(environment, provider, component, command)\n            ensure_file(builds_filename)\n            build_data = json.loads(open(builds_filename).read())\n            this_build = {'success': False, 'remote': True, 'build_number': build_number, 'reference': dependency, 'status': 'running', 'environment': environment}\n            state['running'].append(this_build)\n            mark_dependency_as_running(dependency, environment)\n            build_data['builds'].append(this_build)\n            write_builds_file(builds_filename, build_data)\n            lock.release()\n            last_successful_build = find_last_successful_build(builds)\n            if last_successful_build:\n                try:\n                    last_logfile = 'builds/logs/{:03d}-{}-{}-{}-{}.log'.format(last_successful_build['build_number'], environment, provider, component, command)\n                    last_size = os.stat(last_logfile).st_size\n                    this_build['last_size'] = last_size\n                    this_build['log_file'] = log_filename\n                except Exception as e:\n                    pass\n            print('Creating remote directories')\n            work_directory = 'builds'\n            cmd = client.run_command('mkdir -p builds/exits')\n            client.join(cmd)\n            cmd = client.run_command('mkdir -p builds/outputs')\n            client.join(cmd)\n            cmd = client.run_command('mkdir -p builds/envs')\n            client.join(cmd)\n            cmd = client.run_command('mkdir -p builds/logs')\n            client.join(cmd)\n            cmd = client.run_command('mkdir -p builds/published')\n            client.join(cmd)\n            env = construct_environment('', build_number, environment, provider, component, command, previous_outputs)\n            work_path = 'builds/work/{}.{}.{}.{}'.format(environment, provider, component, command)\n            make_work_path = client.run_command('mkdir -p {}'.format(work_path))\n            client.join(make_work_path)\n            print('Unpacking artifact remotely')\n            unpack_command = 'tar -xvf {} -C {}'.format(artifact, work_path)\n            extract = client.run_command(unpack_command)\n            client.join(extract)\n            env_file_name = '{}.{}.{}.{}.{}'.format(environment, provider, component, command, build_number)\n            env_file_path = os.path.join(project_directory, 'builds/envs/', env_file_name)\n            env_file = open(env_file_path, 'w')\n            for key, value in env.items():\n                escaped_value = value\n                if ' ' in value:\n                    escaped_value = '\"' + value + '\"'\n                env_file.write('{}={}'.format(key, escaped_value))\n                env_file.write('\\n')\n            env_file.flush()\n            env_file.close()\n            print('Sending envs file to worker')\n            remote_env_path = os.path.join('builds/envs', env_file_name)\n            cmds = client.scp_send(env_file_path, remote_env_path, True)\n            joinall(cmds, raise_error=True)\n            ran_remotely = False\n            build_stdout = None\n            build_stderr = None\n            test_command = 'test -f {}/{}/{}'.format(work_path, provider, command)\n            print(test_command)\n            print(work_path)\n            test_exists = client.run_command(test_command)\n            client.join(test_exists)\n            print('Running require')\n            chosen_file = run_require('', environment, provider, component, command)\n            if chosen_file:\n                print(chosen_file)\n                destination_path = os.path.join(work_path, os.path.basename(chosen_file))\n                print('Copying {} to remote {}'.format(chosen_file, destination_path))\n                rsync_command = 'rsync -Pav -e \"ssh -i {}\" {} {}@{}:{}'.format(args.workers_key[0], chosen_file, args.workers_user, host, destination_path)\n                print(rsync_command)\n                rsync = Popen(['bash', '-c', rsync_command])\n                rsync.communicate()\n                unzip_command = 'cd {} ; tar -xf {}'.format(work_path, os.path.basename(chosen_file))\n                unzip_artifact = client.run_command(unzip_command)\n                print(unzip_command)\n                client.join(unzip_artifact)\n            if test_exists[host]['exit_code'] == 0:\n                print('Running build remotely')\n                ran_remotely = True\n                remote_log_filename = 'builds/logs/{}-{}.{}.{}.{}.log'.format(build_number, environment, provider, component, command)\n                build_command = client.run_command('set -a ;\\n                    source {} ;\\n                    export OUTPUT_PATH=$(readlink -f ${{OUTPUT_PATH}}) ;\\n                    export EXIT_CODE_PATH=$(readlink -f ${{EXIT_CODE_PATH}}) ;\\n                    export ARTIFACT_PATH=$(readlink -f ${{ARTIFACT_PATH}}) ;\\n                    cd {}/{} ;\\n                    ./{} {} {}'.format(remote_env_path, work_path, provider, command, environment, component, remote_log_filename))\n            else:\n                lock.acquire()\n                builds_filename = get_builds_filename(environment, provider, component, command)\n                ensure_file(builds_filename)\n                build_data = json.loads(open(builds_filename).read())\n                this_build = build_data['builds'][-1]\n                print('Not implemented')\n                this_build['success'] = True\n                self.success = True\n                open(os.path.join(project_directory, 'builds/outputs/{}.{}.{}.{}.outputs.json'.format(environment, provider, component, command)), 'w').write('{}')\n                write_builds_file(builds_filename, build_data)\n                lock.release()\n                remove_from_running(dependency, environment)\n                open(os.path.join(get_last_run_path(environment, provider, component, command)), 'w').write(':)')\n                return\n            if ran_remotely:\n                print('Setting up logs...')\n                logfile = open('builds/logs/{}-{}.{}.{}.{}.log'.format(build_number, environment, provider, component, command), 'w')\n                client.join(build_command)\n                for line in build_command[host]['stdout']:\n                    logfile.write(line + '\\n')\n                for line in build_command[host]['stderr']:\n                    logfile.write(line + '\\n')\n                print('Remote build finished')\n                remove_from_running(dependency, environment)\n                print('Downloading outputs...')\n                dest_output = os.path.join(project_directory, 'builds/outputs/{}.{}.{}.{}.outputs.json'.format(environment, provider, component, command))\n                receive_outputs = client.copy_remote_file(os.path.join('builds/outputs/{}.{}.{}.{}.outputs.json'.format(environment, provider, component, command)), dest_output)\n                joinall(receive_outputs)\n                os.rename('{}_{}'.format(dest_output, host), dest_output)\n                print('Downloading exit code...')\n                dest_exit_code = os.path.join(project_directory, 'builds/exits/{}.{}.{}.{}.{}.exitcode'.format(environment, provider, component, command, build_number))\n                receive_exit_code = client.copy_remote_file(os.path.join('builds/exits/{}.{}.{}.{}.{}.exitcode'.format(environment, provider, component, command, build_number)), dest_exit_code)\n                joinall(receive_exit_code, raise_error=True)\n                os.rename('{}_{}'.format(dest_exit_code, host), dest_exit_code)\n                print('Downloading artifact...')\n                check_for_artifacts = client.run_command('test -f {}'.format('builds/published/{}.{}.{}.{}.{}.tgz'.format(environment, provider, component, command, build_number)))\n                client.join(check_for_artifacts)\n                if check_for_artifacts[host]['exit_code'] == 0:\n                    dest_artifact = os.path.join(project_directory, 'builds/published/{}.{}.{}.{}.{}.tgz'.format(environment, provider, component, command, build_number))\n                    receive_artifact = client.copy_remote_file(os.path.join('builds/published/{}.{}.{}.{}.{}.tgz'.format(environment, provider, component, command, build_number)), dest_artifact)\n                    joinall(receive_artifact, raise_error=True)\n                    os.rename('{}_{}'.format(dest_artifact, host), dest_artifact)\n                last_running_build = Component(dependency, environment, provider, component, command, args).calculate_state()\n                if last_running_build:\n                    self.success = last_running_build['success']\n    worker = Worker()\n    worker.start()\n"]]}
{"hexsha": "5960e0ee8becb1e1a225115427a9de9b595480fa", "ext": "py", "lang": "Python", "content": "def metric_to_image_radial_length(length, affine):\n    \"\"\"\n        Only useful for image + mesh cases. Not implemented yet.\n        \"\"\"\n    return length", "fn_id": 1, "class_fn": false, "repo": "EuroPOND/deformetrica", "file": "src/in_out/image_functions.py", "last_update_at": "2020-10-27T07:30:56+00:00", "question_id": "5960e0ee8becb1e1a225115427a9de9b595480fa_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def metric_to_image_radial_length(length, affine):\n    \"\"\"\n        Only useful for image + mesh cases. Not implemented yet.\n        \"\"\"\n"]]}
{"hexsha": "354aad468386d2ab99eadd09fc2be4ebde3051fe", "ext": "py", "lang": "Python", "content": "@app.route('/station_detail/station_id=<station_id>')\ndef station_detail(station_id: str):\n    url_back = url_for('index')\n    logging.debug(f\"station_id: {station_id}\")\n    station = stations.loc[station_id, :]\n    stlabel = utils.station_label(station)\n    rdf, cprcp, curr_drought_rate, curr_fillrate, curr_fillrate_cdf = \\\n        utils.drought_rate_data(station_id, current_year, engine=engine)\n    dft = totals.loc[totals['station'] == station_id, :]\n    f_prcp = utils.cum_prcp_plot(stlabel, rdf, cprcp, curr_drought_rate)\n    f_totals = utils.totals_barchart(dft)\n    script, div = components(f_prcp)\n    script_totals, div_totals = components(f_totals)\n    html = render_template(\n        'station_detail.html',\n        js_resources=js_resources,\n        css_resources=css_resources,\n        plot_script=script,\n        plot_script_totals=script_totals,\n        plot_div=div,\n        plot_totals=div_totals,\n        station=station,\n        url_back=url_back)\n    return html", "fn_id": 1, "class_fn": false, "repo": "ftrojan/aquarius", "file": "app.py", "last_update_at": "2020-03-30T13:45:35+00:00", "question_id": "354aad468386d2ab99eadd09fc2be4ebde3051fe_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.route('/station_detail/station_id=<station_id>')\ndef station_detail(station_id: str):\n    url_back = url_for('index')\n    logging.debug(f'station_id: {station_id}')\n    station = stations.loc[station_id, :]\n    stlabel = utils.station_label(station)\n    rdf, cprcp, curr_drought_rate, curr_fillrate, curr_fillrate_cdf = utils.drought_rate_data(station_id, current_year, engine=engine)\n    dft = totals.loc[totals['station'] == station_id, :]\n    f_prcp = utils.cum_prcp_plot(stlabel, rdf, cprcp, curr_drought_rate)\n    f_totals = utils.totals_barchart(dft)\n    script, div = components(f_prcp)\n    script_totals, div_totals = components(f_totals)\n    html = render_template('station_detail.html', js_resources=js_resources, css_resources=css_resources, plot_script=script, plot_script_totals=script_totals, plot_div=div, plot_totals=div_totals, station=station, url_back=url_back)\n"]]}
{"hexsha": "13d20c58618bae1fd9184241e64ff9b913dd727d", "ext": "py", "lang": "Python", "content": "def check_adb_alive():\n    try:\n        sess = ADBClientSession(config.ADB_SERVER)\n        version = int(sess.service('host:version').read_response().decode(), 16)\n        logger.debug('ADB server version %d', version)\n        return True\n    except ConnectionRefusedError:\n        return False\n    except RuntimeError:\n        return False", "fn_id": 1, "class_fn": false, "repo": "qiutongxue/ArknightsAutoHelper", "file": "connector/ADBConnector.py", "last_update_at": "2020-12-16T06:19:02+00:00", "question_id": "13d20c58618bae1fd9184241e64ff9b913dd727d_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def check_adb_alive():\n    try:\n        sess = ADBClientSession(config.ADB_SERVER)\n        version = int(sess.service('host:version').read_response().decode(), 16)\n        logger.debug('ADB server version %d', version)\n        return True\n    except ConnectionRefusedError:\n        return False\n    except RuntimeError:\n"]]}
{"hexsha": "8d78cb3a80d5c2001dda0cac594eb5af4a4f142b", "ext": "py", "lang": "Python", "content": "def methane_molecule():\n    symbols = ['C','H','H','H','H']\n    coordinates = np.array([\n    [1, 1, 1],\n    [2.4, 1, 1],\n    [-0.4, 1, 1],\n    [1, 1, 2.4],\n    [1, 1, 0.4],\n    ])\n    return symbols, coordinates", "fn_id": 0, "class_fn": false, "repo": "PierMorgante/molecool", "file": "molecool/tests/test_molecule.py", "last_update_at": "2020-05-15T18:14:40+00:00", "question_id": "8d78cb3a80d5c2001dda0cac594eb5af4a4f142b_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def methane_molecule():\n    symbols = ['C', 'H', 'H', 'H', 'H']\n    coordinates = np.array([[1, 1, 1], [2.4, 1, 1], [-0.4, 1, 1], [1, 1, 2.4], [1, 1, 0.4]])\n"]]}
{"hexsha": "8a00b7a1095424bbea3b3ede093816e290d1e3ff", "ext": "py", "lang": "Python", "content": "def get_target_extended_spec(s, target, logger):\n    \"\"\"Return target's spec extended with linked elements.\n\n    That is extend spec.endpoints with the selector of the\n    linkedVirtualService, and set spec.cluster.spec to spec.listener\n    of the linkedVirtualService.\n\n    \"\"\"\n    spec = deepcopy(target['spec'])\n    # 1. Find the linked VirtualService\n    try:\n        vsvc_name = spec['linkedVirtualService']\n    except KeyError:\n        return spec\n    del spec['linkedVirtualService']\n    vsvc = s['virtualservices'].get(vsvc_name)\n    if not vsvc:\n        for v in s['virtualservices'].values():\n            if v['metadata']['name'] == vsvc_name:\n                vsvc = v\n                break\n    if not vsvc:\n        return {}\n\n    # 2.\n    spec['cluster'] = spec.get('cluster', {})\n    eps = spec['cluster'].get('endpoints', [])\n    new_ep = vsvc['spec'].get('selector', [])\n    if new_ep:\n        eps.append({'selector': new_ep})\n        spec['cluster'].update({'endpoints': eps})\n\n    # 3.\n    spec['cluster']['spec'] = vsvc['spec']['listener']\n\n    # 4.\n    return spec", "fn_id": 2, "class_fn": false, "repo": "rg0now/l7mp", "file": "k8s-operator/l7mp.py", "last_update_at": "2020-10-12T12:45:25+00:00", "question_id": "8a00b7a1095424bbea3b3ede093816e290d1e3ff_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_target_extended_spec(s, target, logger):\n    \"\"\"Return target's spec extended with linked elements.\n\n    That is extend spec.endpoints with the selector of the\n    linkedVirtualService, and set spec.cluster.spec to spec.listener\n    of the linkedVirtualService.\n\n    \"\"\"\n    spec = deepcopy(target['spec'])\n    try:\n        vsvc_name = spec['linkedVirtualService']\n    except KeyError:\n        return spec\n    del spec['linkedVirtualService']\n    vsvc = s['virtualservices'].get(vsvc_name)\n    if not vsvc:\n        for v in s['virtualservices'].values():\n            if v['metadata']['name'] == vsvc_name:\n                vsvc = v\n                break\n    if not vsvc:\n        return {}\n    spec['cluster'] = spec.get('cluster', {})\n    eps = spec['cluster'].get('endpoints', [])\n    new_ep = vsvc['spec'].get('selector', [])\n    if new_ep:\n        eps.append({'selector': new_ep})\n        spec['cluster'].update({'endpoints': eps})\n    spec['cluster']['spec'] = vsvc['spec']['listener']\n"]]}
{"hexsha": "43ae8701be6db11e210380b41a09e72c18c9dfe8", "ext": "py", "lang": "Python", "content": "@commandWrap\ndef CreateEmptyUVSetOptions(*args, **kwargs):\n    u\"\"\":rtype: list|str|DagNode|AttrObject|ArrayAttrObject|Components1Base\"\"\"\n    return cmds.CreateEmptyUVSetOptions(*args, **kwargs)", "fn_id": 1306, "class_fn": false, "repo": "2921251087/CPMel", "file": "src/CPMel/cmds/static_cmds.py", "last_update_at": "2020-10-09T08:47:32+00:00", "question_id": "43ae8701be6db11e210380b41a09e72c18c9dfe8_1306", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@commandWrap\ndef CreateEmptyUVSetOptions(*args, **kwargs):\n    u\"\"\":rtype: list|str|DagNode|AttrObject|ArrayAttrObject|Components1Base\"\"\"\n"]]}
{"hexsha": "438d12f3e6ab73dd71324fca9771be402155918f", "ext": "py", "lang": "Python", "content": "def is_image(resp):\n    \"\"\"Returns true if html request response was an image\"\"\"\n    try:\n        return resp.headers['content-type'].split('/')[0] == 'image'\n    except:\n        return False", "fn_id": 2, "class_fn": false, "repo": "denschmitz/pins_tiles", "file": "util.py", "last_update_at": "2020-05-04T14:49:12+00:00", "question_id": "438d12f3e6ab73dd71324fca9771be402155918f_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def is_image(resp):\n    \"\"\"Returns true if html request response was an image\"\"\"\n    try:\n        return resp.headers['content-type'].split('/')[0] == 'image'\n    except:\n"]]}
{"hexsha": "366b0343ba359792a3cdf165d3849e31b0299bf2", "ext": "py", "lang": "Python", "content": "def get_data(file_obj):\n    row_0 = np.genfromtxt(file_obj, delimiter=',', usecols=(0))\n    row_1 = np.genfromtxt(file_obj, delimiter=',', usecols=(1))\n    row_2 = np.genfromtxt(file_obj, delimiter=',', usecols=(2))\n    row_3 = np.genfromtxt(file_obj, delimiter=',', usecols=(3))\n    return row_0, row_1, row_2, row_3", "fn_id": 0, "class_fn": false, "repo": "klevin92/covid19_moscow_cases", "file": "moscow_data.py", "last_update_at": "2020-03-25T12:00:00+00:00", "question_id": "366b0343ba359792a3cdf165d3849e31b0299bf2_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_data(file_obj):\n    row_0 = np.genfromtxt(file_obj, delimiter=',', usecols=0)\n    row_1 = np.genfromtxt(file_obj, delimiter=',', usecols=1)\n    row_2 = np.genfromtxt(file_obj, delimiter=',', usecols=2)\n    row_3 = np.genfromtxt(file_obj, delimiter=',', usecols=3)\n"]]}
{"hexsha": "50e165c10837556e835d32bc880e3e8dc4d29c62", "ext": "py", "lang": "Python", "content": "def secant(a, b, E, n):\n    k = 0\n    while k < n:\n        t_x = ((a * f(b)) - (b * f(a))) \\\n        / (f(b) - f(a))\n        if abs(b-a) < E or abs(f(b)) < E:\n            print (\"Raiz =  {}\".format(t_x))\n            break\n        a = b\n        b = t_x\n        \n        k += 1", "fn_id": 0, "class_fn": false, "repo": "bnahuz/zeto_funcoes_reais", "file": "real_zero/secant.py", "last_update_at": "2020-11-05T23:17:15+00:00", "question_id": "50e165c10837556e835d32bc880e3e8dc4d29c62_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def secant(a, b, E, n):\n    k = 0\n    while k < n:\n        t_x = (a * f(b) - b * f(a)) / (f(b) - f(a))\n        if abs(b - a) < E or abs(f(b)) < E:\n            print('Raiz =  {}'.format(t_x))\n            break\n        a = b\n        b = t_x\n"]]}
{"hexsha": "c4b59ea674aa8a31f87633b437e5863be80f3ef3", "ext": "py", "lang": "Python", "content": "def test_cmp():\n    p1 = PseudoMotor(5)\n    p2 = PseudoMotor(10)\n    assert AngledJoint(p1,p2) == AngledJoint(p1, p2)", "fn_id": 11, "class_fn": false, "repo": "slaclab/pystand", "file": "tests/test_joints.py", "last_update_at": "2020-12-13T00:35:01+00:00", "question_id": "c4b59ea674aa8a31f87633b437e5863be80f3ef3_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_cmp():\n    p1 = PseudoMotor(5)\n    p2 = PseudoMotor(10)\n"]]}
{"hexsha": "4264c8ca3040630f755480554f4cbc48f89bf3c4", "ext": "py", "lang": "Python", "content": "def make_train_loader(cifar_img_dim, shuffle=10000, batch_size=FLAGS.batch_size):\n    num_dataset_instances = xm.xrt_world_size() * FLAGS.num_workers\n    epoch_size = trainsize // num_dataset_instances\n\n    image_transform = transforms.Compose(\n        [\n            transforms.RandomCrop(cifar_img_dim, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]\n    )\n    \n    dataset = (\n        wds.WebDataset(FLAGS.wds_traindir,\n                       splitter=my_worker_splitter,\n                       nodesplitter=my_node_splitter,\n                       shardshuffle=True, length=epoch_size)\n        .shuffle(shuffle)\n        .decode(\"pil\")\n        .to_tuple(\"ppm;jpg;jpeg;png\", \"cls\")\n        .map_tuple(image_transform, identity)\n        .batched(batch_size, partial=True)\n        )\n\n    loader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, drop_last=False, num_workers=FLAGS.num_workers) # , worker_init_fn=worker_init_fn\n    return loader", "fn_id": 5, "class_fn": false, "repo": "mlexample/gcspytorchimagenet", "file": "pytorch_on_gcp/test_train_mp_wds_cifar.py", "last_update_at": "2020-12-03T18:00:16+00:00", "question_id": "4264c8ca3040630f755480554f4cbc48f89bf3c4_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def make_train_loader(cifar_img_dim, shuffle=10000, batch_size=FLAGS.batch_size):\n    num_dataset_instances = xm.xrt_world_size() * FLAGS.num_workers\n    epoch_size = trainsize // num_dataset_instances\n    image_transform = transforms.Compose([transforms.RandomCrop(cifar_img_dim, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize])\n    dataset = wds.WebDataset(FLAGS.wds_traindir, splitter=my_worker_splitter, nodesplitter=my_node_splitter, shardshuffle=True, length=epoch_size).shuffle(shuffle).decode('pil').to_tuple('ppm;jpg;jpeg;png', 'cls').map_tuple(image_transform, identity).batched(batch_size, partial=True)\n    loader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, drop_last=False, num_workers=FLAGS.num_workers)\n"]]}
{"hexsha": "2f5ecad524d0197183b72ab76a60c9f291d1a06f", "ext": "py", "lang": "Python", "content": "def load_base_config():\n    ''' Define initial config '''\n    for key in CONFIG.keys():\n        if key.upper() in os.environ:\n            CONFIG[key.lower()] = os.getenv(key.upper())\n        else:\n            print(\"ERROR: You need to set {} as environment variable or .env entry\".format(key.upper()))\n            exit(1)\n\n    CONFIG['session_id'] = str(uuid.uuid4())\n    CONFIG['oauth_token'] = get_oauth_token()", "fn_id": 0, "class_fn": false, "repo": "d-k-ivanov/helpers", "file": "git/bitbucket/list-members-in-org/list-members-in-org.py", "last_update_at": "2020-05-15T00:21:15+00:00", "question_id": "2f5ecad524d0197183b72ab76a60c9f291d1a06f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_base_config():\n    \"\"\" Define initial config \"\"\"\n    for key in CONFIG.keys():\n        if key.upper() in os.environ:\n            CONFIG[key.lower()] = os.getenv(key.upper())\n        else:\n            print('ERROR: You need to set {} as environment variable or .env entry'.format(key.upper()))\n            exit(1)\n    CONFIG['session_id'] = str(uuid.uuid4())\n"]]}
{"hexsha": "29c2d4519c1ca7534ac3b8ab14387d713f625c82", "ext": "py", "lang": "Python", "content": "@pytest.mark.install\n@pytest.mark.sanity\ndef test_vol_create():\n    # Verify px status before calling create px volume\n    px_status = px_utils.check_px_status()\n    assert px_status == 2, \"PORTWORX: Avoiding volume create, status returned: {}\".format(px_status)\n    sleep(120) # Observed failures while volume creation, so introducing sleep of 90 seconds here. \n\n    pod_count, pod_list = px_utils.get_px_pod_list()\n    if pod_count <= 0:\n        log.info(\"PORTWORX: Can't proceed with volume creation, Pod count is: {}\".format(pod_count))\n        raise\n    pod_name = pod_list[0]\n\n    px_utils.px_create_volume(pod_name, \"px_dcos_vol_1\", 5)\n    sleep(30) # Wait before immediatly calling vol size, it is observed that dcos needs few seconds to refresh\n    vol_size = px_utils.px_get_vol_size_in_gb(\"px_dcos_vol_1\")\n    if 5 != vol_size:\n        log.info(\"PORTWORX: Size of created volume if incorrect, provided: 5, obtained: {}\".format(vol_size))\n        raise", "fn_id": 10, "class_fn": false, "repo": "greggomann/dcos-commons", "file": "frameworks/portworx/tests/test_sanity.py", "last_update_at": "2020-01-27T19:33:52+00:00", "question_id": "29c2d4519c1ca7534ac3b8ab14387d713f625c82_10", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.install\n@pytest.mark.sanity\ndef test_vol_create():\n    px_status = px_utils.check_px_status()\n    assert px_status == 2, 'PORTWORX: Avoiding volume create, status returned: {}'.format(px_status)\n    sleep(120)\n    pod_count, pod_list = px_utils.get_px_pod_list()\n    if pod_count <= 0:\n        log.info(\"PORTWORX: Can't proceed with volume creation, Pod count is: {}\".format(pod_count))\n        raise\n    pod_name = pod_list[0]\n    px_utils.px_create_volume(pod_name, 'px_dcos_vol_1', 5)\n    sleep(30)\n    vol_size = px_utils.px_get_vol_size_in_gb('px_dcos_vol_1')\n    if 5 != vol_size:\n        log.info('PORTWORX: Size of created volume if incorrect, provided: 5, obtained: {}'.format(vol_size))\n"]]}
{"hexsha": "346ae55e96b95c36974792f3f6d58b71b72d1202", "ext": "py", "lang": "Python", "content": "def test_db_update(data):\n    db = data._fields.db_update\n    assert isinstance(db, list)\n    assert len(db) == 2\n    assert set(fld.name for fld in db) == set(('name', 'age'))", "fn_id": 3, "class_fn": false, "repo": "robertchase/spindrift", "file": "test/test_database_dao.py", "last_update_at": "2020-09-21T19:51:22+00:00", "question_id": "346ae55e96b95c36974792f3f6d58b71b72d1202_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_db_update(data):\n    db = data._fields.db_update\n    assert isinstance(db, list)\n    assert len(db) == 2\n"]]}
{"hexsha": "306e4a4f0346704936d6b27a3e2b1fb84cabd3e1", "ext": "py", "lang": "Python", "content": "def d_transform(img,gt_bboxes,gt_masks=None):\n    img=torch.tensor(img)\n    assert gt_bboxes.shape==(1,4)\n    box=gt_bboxes[0].astype(np.int32)\n    box_img=np.zeros((512,512),dtype=np.uint8)\n    box_img[box[1]:box[3],box[0]:box[2]]=1\n    img=TF.to_pil_image(img)\n    box_img=TF.to_pil_image(box_img)\n\n    angle=random.randint(-10,10)\n    #angle=90\n    m_rotate= lambda x: TF.rotate(x,angle)\n\n    if gt_masks is not None:\n        mask_img=TF.to_pil_image(torch.tensor(gt_masks))\n        r_img,r_box,r_mask=map(m_rotate,[img,box_img,mask_img])\n        return unpack_pil(r_img,r_box,r_mask)\n    else:\n        r_img,r_box=map(m_rotate,[img,box_img])\n        return unpack_pil(r_img,r_box)", "fn_id": 0, "class_fn": false, "repo": "daniel616/DL", "file": "mmdet/datasets/dl_coco.py", "last_update_at": "2020-06-20T02:35:14+00:00", "question_id": "306e4a4f0346704936d6b27a3e2b1fb84cabd3e1_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def d_transform(img, gt_bboxes, gt_masks=None):\n    img = torch.tensor(img)\n    assert gt_bboxes.shape == (1, 4)\n    box = gt_bboxes[0].astype(np.int32)\n    box_img = np.zeros((512, 512), dtype=np.uint8)\n    box_img[box[1]:box[3], box[0]:box[2]] = 1\n    img = TF.to_pil_image(img)\n    box_img = TF.to_pil_image(box_img)\n    angle = random.randint(-10, 10)\n    m_rotate = lambda x: TF.rotate(x, angle)\n    if gt_masks is not None:\n        mask_img = TF.to_pil_image(torch.tensor(gt_masks))\n        r_img, r_box, r_mask = map(m_rotate, [img, box_img, mask_img])\n        return unpack_pil(r_img, r_box, r_mask)\n    else:\n        r_img, r_box = map(m_rotate, [img, box_img])\n"]]}
{"hexsha": "a0490b32bcf53291170b831faf8cdb96ba01d501", "ext": "py", "lang": "Python", "content": "def xy_to_ABC(xy, xscale=1.0, yscale=1.0):\n    \"\"\"\n    Convert x-y coordinates within a triangle to compositional ternary coordinates.\n\n    Parameters\n    -----------\n    xy : :class:`numpy.ndarray`\n        XY array (:code:`samples, 2`).\n    xscale : :class:`float`\n        Scale for x-axis.\n    yscale : :class:`float`\n        Scale for y-axis.\n\n    Returns\n    --------\n    :class:`numpy.ndarray`\n        Array of ternary coordinates (:code:`samples, 3`)\n    \"\"\"\n    assert xy.shape[-1] == 2\n    # transform from xy cartesian to ternary\n    scale = affine_transform(\n        np.array([[1 / xscale, 0, 0], [0, 1 / yscale, 0], [0, 0, 1]])\n    )\n    shear = affine_transform(np.array([[1, -1 / 2, 0], [0, 1, 0], [0, 0, 1]]))\n    A, B = shear(scale(xy).T)\n    C = 1.0 - (A + B)  # + (xscale-1) + (yscale-1)\n    return np.vstack([A, B, C]).T", "fn_id": 24, "class_fn": false, "repo": "JustinGOSSES/pyrolite", "file": "pyrolite/util/plot.py", "last_update_at": "2020-03-13T07:11:47+00:00", "question_id": "a0490b32bcf53291170b831faf8cdb96ba01d501_24", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def xy_to_ABC(xy, xscale=1.0, yscale=1.0):\n    \"\"\"\n    Convert x-y coordinates within a triangle to compositional ternary coordinates.\n\n    Parameters\n    -----------\n    xy : :class:`numpy.ndarray`\n        XY array (:code:`samples, 2`).\n    xscale : :class:`float`\n        Scale for x-axis.\n    yscale : :class:`float`\n        Scale for y-axis.\n\n    Returns\n    --------\n    :class:`numpy.ndarray`\n        Array of ternary coordinates (:code:`samples, 3`)\n    \"\"\"\n    assert xy.shape[-1] == 2\n    scale = affine_transform(np.array([[1 / xscale, 0, 0], [0, 1 / yscale, 0], [0, 0, 1]]))\n    shear = affine_transform(np.array([[1, -1 / 2, 0], [0, 1, 0], [0, 0, 1]]))\n    A, B = shear(scale(xy).T)\n    C = 1.0 - (A + B)\n"]]}
{"hexsha": "aca7d17340fb56c81511cd04150bbc861e06ca70", "ext": "py", "lang": "Python", "content": "def test_mark_complete_existing_todo(repository: Repository) -> None:\n    id_1, text_1 = _insert_todo(repository, \"This is a Todo!\")\n    id_2, text_2 = _insert_todo(repository, \"This is a ANOTHER Todo!\")\n\n    result = repository.deactivate(id_2)\n    assert result\n\n    todos = repository.list()\n    assert todos == tuple(\n        sorted(\n            (\n                Todo(id=id_1, text=text_1, active=True),\n                Todo(id=id_2, text=text_2, active=False),\n            )\n        )\n    )", "fn_id": 9, "class_fn": false, "repo": "davidepedranz/speck-and-tech-monitoring-workshop", "file": "backend/tests/test_repository.py", "last_update_at": "2020-04-18T18:17:43+00:00", "question_id": "aca7d17340fb56c81511cd04150bbc861e06ca70_9", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_mark_complete_existing_todo(repository: Repository) -> None:\n    id_1, text_1 = _insert_todo(repository, 'This is a Todo!')\n    id_2, text_2 = _insert_todo(repository, 'This is a ANOTHER Todo!')\n    result = repository.deactivate(id_2)\n    assert result\n    todos = repository.list()\n"]]}
{"hexsha": "e7bbbbcf44242be8de62aa96ec4e01dbf0130092", "ext": "py", "lang": "Python", "content": "def ping(input, ping_time, ping_tries):\n    ping_string = \"ping -c {} -w {} {} > /dev/null 2>&1\"\\\n        .format(ping_tries, ping_time, input)\n    hookenv.log('Ping command: {}'.format(ping_string), 'DEBUG')\n    response = os.system(ping_string)\n    if response == 0:\n        return 0\n    else:\n        return 1", "fn_id": 1, "class_fn": false, "repo": "openstack/charm-magpie", "file": "src/lib/charms/layer/magpie_tools.py", "last_update_at": "2020-12-16T11:06:37+00:00", "question_id": "e7bbbbcf44242be8de62aa96ec4e01dbf0130092_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ping(input, ping_time, ping_tries):\n    ping_string = 'ping -c {} -w {} {} > /dev/null 2>&1'.format(ping_tries, ping_time, input)\n    hookenv.log('Ping command: {}'.format(ping_string), 'DEBUG')\n    response = os.system(ping_string)\n    if response == 0:\n        return 0\n    else:\n"]]}
{"hexsha": "5953f08bea7d8f1e7dd8178cf204d267bdabb64a", "ext": "py", "lang": "Python", "content": "def getJobList(url, jobStatus):\n    V100_task_list = []\n    P4_task_list = []\n    response = requests.get(url).json()['news']\n    for t in response:\n        #if t['jobname'] != 'PADDLE_DOCKER_BUILD': #\u9700\u4e0d\u9700\u8981\u628a\u6784\u5efa\u955c\u50cf\u53bb\u6389\uff1f\n        task = {}\n        task['CIName'] = t['name']\n        task[jobStatus] = t[jobStatus] if t[jobStatus] != None else 0\n        task['PR'] = str(t['prid'])\n        task['commitId'] = t['commit']\n        task['targetId'] = t['bid']\n        if jobStatus == 'running':\n            task['jobname'] = t['jobname']\n        if t['name'].startswith('PR-CI-Py35') or t['name'].startswith(\n                'PR-CI-Coverage'):\n            V100_task_list.append(task)\n        elif t['name'].startswith('PR-CI-CPU-Py2') or t['name'].startswith(\n                'PR-CI-Inference'):\n            P4_task_list.append(task)\n    return V100_task_list, P4_task_list", "fn_id": 2, "class_fn": false, "repo": "randytli/Paddle-bot", "file": "webservice/monitor/queueCIMonitor.py", "last_update_at": "2020-05-27T05:21:40+00:00", "question_id": "5953f08bea7d8f1e7dd8178cf204d267bdabb64a_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def getJobList(url, jobStatus):\n    V100_task_list = []\n    P4_task_list = []\n    response = requests.get(url).json()['news']\n    for t in response:\n        task = {}\n        task['CIName'] = t['name']\n        task[jobStatus] = t[jobStatus] if t[jobStatus] != None else 0\n        task['PR'] = str(t['prid'])\n        task['commitId'] = t['commit']\n        task['targetId'] = t['bid']\n        if jobStatus == 'running':\n            task['jobname'] = t['jobname']\n        if t['name'].startswith('PR-CI-Py35') or t['name'].startswith('PR-CI-Coverage'):\n            V100_task_list.append(task)\n        elif t['name'].startswith('PR-CI-CPU-Py2') or t['name'].startswith('PR-CI-Inference'):\n            P4_task_list.append(task)\n"]]}
{"hexsha": "305a54370db17c2200ccd40a8347199a7b89a58f", "ext": "py", "lang": "Python", "content": "def filter_files(predicate, files):\n    for filenames in files.values():\n        new_filenames = tuple(filter(predicate, filenames))\n        filenames.clear()\n        filenames.extend(new_filenames)", "fn_id": 1, "class_fn": false, "repo": "Rufflewind/config", "file": "home/sbin/find-duplicates-helper.py", "last_update_at": "2020-09-14T17:23:35+00:00", "question_id": "305a54370db17c2200ccd40a8347199a7b89a58f_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def filter_files(predicate, files):\n    for filenames in files.values():\n        new_filenames = tuple(filter(predicate, filenames))\n        filenames.clear()\n"]]}
{"hexsha": "fef0c05fbc3097742bed72246ffd798b20bd0fc8", "ext": "py", "lang": "Python", "content": "@contextlib.contextmanager\ndef lock_repository_write(repo_path):\n    with acquire_repo_lock(\"write\", repo_path):\n        repo = _WritableLocalRepository(repo_path)\n        try:\n            yield repo\n        finally:\n            repo.invalidate()", "fn_id": 3, "class_fn": false, "repo": "nokia/nokia-deployer", "file": "deployment/gitutils.py", "last_update_at": "2020-10-13T08:08:43+00:00", "question_id": "fef0c05fbc3097742bed72246ffd798b20bd0fc8_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@contextlib.contextmanager\ndef lock_repository_write(repo_path):\n    with acquire_repo_lock('write', repo_path):\n        repo = _WritableLocalRepository(repo_path)\n        try:\n            yield repo\n        finally:\n"]]}
{"hexsha": "c5af7336463d4fdf0b0c0ab9e39d2fca2711f16c", "ext": "py", "lang": "Python", "content": "def isint(s):\n  try: \n    int(s)\n    return True\n  except ValueError:\n    return False", "fn_id": 0, "class_fn": false, "repo": "kannix68/advent_of_code_2017", "file": "day18/day18a.py", "last_update_at": "2020-04-12T17:58:39+00:00", "question_id": "c5af7336463d4fdf0b0c0ab9e39d2fca2711f16c_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def isint(s):\n    try:\n        int(s)\n        return True\n    except ValueError:\n"]]}
{"hexsha": "c28d41e68dc39189efb6db08756cc4158587d229", "ext": "py", "lang": "Python", "content": "@app.route('/frame', methods=['POST','GET'])\ndef frame():\n    # os.system('python iPhoneFrame.py')\n    os.chdir(app.root_path)\n    import iPhoneFrame\n    outputpath = os.path.join(app.root_path, 'output', 'image.png') \n    if os.path.exists(outputpath):\n        print('exists')\n        return render_template('frame.html')\n    else:\n        return redirect(url_for('upload'))", "fn_id": 2, "class_fn": false, "repo": "bryanseah234/iphone-frame-code", "file": "main.py", "last_update_at": "2020-11-03T07:52:16+00:00", "question_id": "c28d41e68dc39189efb6db08756cc4158587d229_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.route('/frame', methods=['POST', 'GET'])\ndef frame():\n    os.chdir(app.root_path)\n    import iPhoneFrame\n    outputpath = os.path.join(app.root_path, 'output', 'image.png')\n    if os.path.exists(outputpath):\n        print('exists')\n        return render_template('frame.html')\n    else:\n"]]}
{"hexsha": "6e2c44a9819f248e0d8243bd043775df65f71567", "ext": "py", "lang": "Python", "content": "def ecs_init():\n    # Setup AWS connection\n    aws_region = os.getenv('REGION', 'us-east-1')\n    logger.info(\"Connecting ecs to region \\\"%s\\\"\", aws_region)\n    \n    global ecs, dynamodb_client, autoscaling\n    ecs = boto3.client('ecs', region_name=aws_region)\n    autoscaling = boto3.client(\"application-autoscaling\", region_name=aws_region)\n    dynamodb_client = boto3.client('dynamodb', region_name=aws_region)\n\n    logger.info(\"Connected ecs to region \\\"%s\\\"\", aws_region)", "fn_id": 5, "class_fn": false, "repo": "leonardobiffi/aws-schedule-instances", "file": "scheduler.py", "last_update_at": "2020-09-27T00:37:52+00:00", "question_id": "6e2c44a9819f248e0d8243bd043775df65f71567_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def ecs_init():\n    aws_region = os.getenv('REGION', 'us-east-1')\n    logger.info('Connecting ecs to region \"%s\"', aws_region)\n    global ecs, dynamodb_client, autoscaling\n    ecs = boto3.client('ecs', region_name=aws_region)\n    autoscaling = boto3.client('application-autoscaling', region_name=aws_region)\n    dynamodb_client = boto3.client('dynamodb', region_name=aws_region)\n"]]}
{"hexsha": "0b239bbc8646acaed661338bd322cb9b63d610ab", "ext": "py", "lang": "Python", "content": "def cache_data(fn, fun):\n    if os.path.exists(fn):\n        print(f'{fn} exists, using cached version')\n        return pd.read_csv(fn)\n    else:\n        print(f'{fn} does not exist, creating file')\n        df = fun()\n        df.to_csv(fn, index=False)\n        return df", "fn_id": 4, "class_fn": false, "repo": "danielrmt/dash-br-options", "file": "data_helpers.py", "last_update_at": "2020-07-06T04:05:04+00:00", "question_id": "0b239bbc8646acaed661338bd322cb9b63d610ab_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def cache_data(fn, fun):\n    if os.path.exists(fn):\n        print(f'{fn} exists, using cached version')\n        return pd.read_csv(fn)\n    else:\n        print(f'{fn} does not exist, creating file')\n        df = fun()\n        df.to_csv(fn, index=False)\n"]]}
{"hexsha": "5f9b67a19d8463a079a3e7e4abc256e61e5d35c6", "ext": "py", "lang": "Python", "content": "def groupdel(cmd, grp, loc_namenode):\n    \"\"\"Allow to execute groupdel command.\n    \n    Parameters\n    ----------\n    cmd --> str, the command\n    grp --> list, the list of groups to which the user belongs\n    loc_namenode --> str, the master namenode in the moment in which the command has been invoked\n    \n    Returns\n    -------\n    None\n    \"\"\"\n    f, required_by, group = cmd.split()\n    #call the groupdel command with a rpc\n    with xmlrpc.client.ServerProxy(loc_namenode) as proxy:\n        try:\n            res = proxy.groupdel(required_by, group) #no print\n        except xmlrpc.client.Fault as err:\n            #the user is not allowed to delete the group\n            if 'RootNecessaryException' in err.faultString:\n                logging.warning(err.faultString)\n            #the group the user want to delete does not exist\n            if 'GroupNotFoundException' in err.faultString:\n                logging.warning(err.faultString)\n            #the group the user want to delete is the main group of a user (must delete the user first)\n            if 'MainUserGroupException' in err.faultString:\n                logging.warning(err.faultString)\n    return", "fn_id": 22, "class_fn": false, "repo": "Erca94/HandMadeDistributedFileSystem", "file": "src/commands_interpreter.py", "last_update_at": "2020-11-01T16:54:24+00:00", "question_id": "5f9b67a19d8463a079a3e7e4abc256e61e5d35c6_22", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def groupdel(cmd, grp, loc_namenode):\n    \"\"\"Allow to execute groupdel command.\n    \n    Parameters\n    ----------\n    cmd --> str, the command\n    grp --> list, the list of groups to which the user belongs\n    loc_namenode --> str, the master namenode in the moment in which the command has been invoked\n    \n    Returns\n    -------\n    None\n    \"\"\"\n    f, required_by, group = cmd.split()\n    with xmlrpc.client.ServerProxy(loc_namenode) as proxy:\n        try:\n            res = proxy.groupdel(required_by, group)\n        except xmlrpc.client.Fault as err:\n            if 'RootNecessaryException' in err.faultString:\n                logging.warning(err.faultString)\n            if 'GroupNotFoundException' in err.faultString:\n                logging.warning(err.faultString)\n            if 'MainUserGroupException' in err.faultString:\n                logging.warning(err.faultString)\n"]]}
{"hexsha": "2a34e489fa2b16cef9b2f528b6f6e9b0e4d2acb6", "ext": "py", "lang": "Python", "content": "def load_nli_file(data_path, num_par=2):\n  \"\"\"Build a tf.data.Data from a file of NLI examples.\"\"\"\n  tokenizer = tokenization.NltkTokenizer()\n  dataset = tf.data.TextLineDataset(data_path)\n  dataset = dataset.map(\n      functools.partial(_nli_line_to_tensors, tokenizer=tokenizer),\n      num_parallel_calls=num_par)\n  dataset = dataset.filter(lambda x: tf.greater_equal(x[\"label\"], 0))\n  return dataset", "fn_id": 2, "class_fn": false, "repo": "vidhishanair/language", "file": "language/boolq/run_recurrent_model_boolq.py", "last_update_at": "2020-05-27T15:48:09+00:00", "question_id": "2a34e489fa2b16cef9b2f528b6f6e9b0e4d2acb6_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_nli_file(data_path, num_par=2):\n    \"\"\"Build a tf.data.Data from a file of NLI examples.\"\"\"\n    tokenizer = tokenization.NltkTokenizer()\n    dataset = tf.data.TextLineDataset(data_path)\n    dataset = dataset.map(functools.partial(_nli_line_to_tensors, tokenizer=tokenizer), num_parallel_calls=num_par)\n    dataset = dataset.filter(lambda x: tf.greater_equal(x['label'], 0))\n"]]}
{"hexsha": "bc4dffea4962fa9f87f4fd8f0dccf1e491ba0a0f", "ext": "py", "lang": "Python", "content": "def write_shah_data(data, questiondict, targetpath):\n    corpus = {}\n\n    #write train.pos.txt and train.neg.txt\n    with io.open(targetpath + \"/train.pos.txt\", 'w', encoding=\"utf-8\") as pos:\n        with io.open(targetpath + \"/train.neg.txt\", 'w', encoding=\"utf-8\") as neg:\n            for d in data:\n                pos.write(d[0] + '\\t' + d[1] + '\\n')\n                for n in d[2]:\n                    neg.write(d[0] + '\\t' + n + '\\n')\n\n    # d question, answer, list(100 random neg answers)  \n    with gzip.open(targetpath + \"/corpus.tsv.gz\", 'wt', encoding='utf-8') as c:\n        for d in data:\n            if not d[0] in corpus:\n                title = prep(questiondict[d[0]]['TITLE'])\n                body = prep(questiondict[d[0]]['BODY'])\n                line = d[0] + '\\t' + title + '\\t' + body + '\\n'\n                c.write(line)\n                corpus[d[0]] = {'TITLE': title, 'BODY': body}\n\n            for n in d[2]:\n                if not n in corpus:\n                    title = prep(questiondict[n]['TITLE'])\n                    body = prep(questiondict[n]['BODY'])\n                    line = n + '\\t' + title + '\\t' + body + '\\n'\n                    c.write(line)\n                    corpus[n] = {'TITLE': title, 'BODY': body}", "fn_id": 3, "class_fn": false, "repo": "UKPLab/emnlp2019-duplicate_question_detection", "file": "reproducing/ws-tb/WS-TB.py", "last_update_at": "2020-07-17T07:32:17+00:00", "question_id": "bc4dffea4962fa9f87f4fd8f0dccf1e491ba0a0f_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def write_shah_data(data, questiondict, targetpath):\n    corpus = {}\n    with io.open(targetpath + '/train.pos.txt', 'w', encoding='utf-8') as pos:\n        with io.open(targetpath + '/train.neg.txt', 'w', encoding='utf-8') as neg:\n            for d in data:\n                pos.write(d[0] + '\\t' + d[1] + '\\n')\n                for n in d[2]:\n                    neg.write(d[0] + '\\t' + n + '\\n')\n    with gzip.open(targetpath + '/corpus.tsv.gz', 'wt', encoding='utf-8') as c:\n        for d in data:\n            if not d[0] in corpus:\n                title = prep(questiondict[d[0]]['TITLE'])\n                body = prep(questiondict[d[0]]['BODY'])\n                line = d[0] + '\\t' + title + '\\t' + body + '\\n'\n                c.write(line)\n                corpus[d[0]] = {'TITLE': title, 'BODY': body}\n            for n in d[2]:\n                if not n in corpus:\n                    title = prep(questiondict[n]['TITLE'])\n                    body = prep(questiondict[n]['BODY'])\n                    line = n + '\\t' + title + '\\t' + body + '\\n'\n                    c.write(line)\n"]]}
{"hexsha": "a0c54ce4eadc711ce3405e6d511725ad1980c2c1", "ext": "py", "lang": "Python", "content": "def create_replicated_pool(remote, name, pgnum, cluster_name=\"ceph\", application=None):\n    remote.run(args=[\n        'sudo', 'ceph', 'osd', 'pool', 'create', name, str(pgnum), str(pgnum), '--cluster', cluster_name\n        ])\n    if application:\n        remote.run(args=[\n            'sudo', 'ceph', 'osd', 'pool', 'application', 'enable', name, application, '--cluster', cluster_name\n        ], check_status=False)", "fn_id": 2, "class_fn": false, "repo": "rpratap-bot/ceph", "file": "qa/tasks/util/rados.py", "last_update_at": "2020-10-01T20:34:48+00:00", "question_id": "a0c54ce4eadc711ce3405e6d511725ad1980c2c1_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_replicated_pool(remote, name, pgnum, cluster_name='ceph', application=None):\n    remote.run(args=['sudo', 'ceph', 'osd', 'pool', 'create', name, str(pgnum), str(pgnum), '--cluster', cluster_name])\n    if application:\n"]]}
{"hexsha": "1161b1c262be58ef58ec2227e5c5e31125e1c5cc", "ext": "py", "lang": "Python", "content": "@vcr.use_cassette\ndef test_instruments_get_candles_with_instrument_only(cli_oandapy):\n    instrument = 'EUR_USD'\n    result = cli_oandapy.instrument.get_candles(instrument).as_dict()\n    assert 'candles' in result\n    assert result['instrument'] == instrument", "fn_id": 0, "class_fn": false, "repo": "rhenter/oandapy", "file": "tests/api/test_instruments.py", "last_update_at": "2020-07-15T15:04:58+00:00", "question_id": "1161b1c262be58ef58ec2227e5c5e31125e1c5cc_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@vcr.use_cassette\ndef test_instruments_get_candles_with_instrument_only(cli_oandapy):\n    instrument = 'EUR_USD'\n    result = cli_oandapy.instrument.get_candles(instrument).as_dict()\n    assert 'candles' in result\n"]]}
{"hexsha": "1f04cf2476169e88ee631d20646ec0fa03dac648", "ext": "py", "lang": "Python", "content": "@hooks.before_each\ndef auth_before_each_hook(transaction):\n    auth = generate_token()\n    if 'request' in transaction:\n        if 'headers' in transaction['request'] and 'Authorization' in transaction['request']['headers']:\n            transaction['request']['headers']['Authorization'] = auth", "fn_id": 0, "class_fn": false, "repo": "marianoleonardo/device-manager", "file": "tests/dredd-hooks/authentication_hook.py", "last_update_at": "2020-12-22T07:32:15+00:00", "question_id": "1f04cf2476169e88ee631d20646ec0fa03dac648_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@hooks.before_each\ndef auth_before_each_hook(transaction):\n    auth = generate_token()\n    if 'request' in transaction:\n        if 'headers' in transaction['request'] and 'Authorization' in transaction['request']['headers']:\n"]]}
{"hexsha": "b26d22aa84a8cfc772d57eb1f17690ed81271c6e", "ext": "py", "lang": "Python", "content": "def nested_if_pg_test():\n    s = '''\n    proctype p(){\n        bit x;\n        if\n        :: if\n           :: true\n           :: false\n           fi\n        :: x\n        fi\n    }\n    '''\n    tree = parser.parse(s)\n    g = tree[0].to_pg()\n    dump(g)\n    h = nx.MultiDiGraph()\n    h.add_edges_from([(0, 1), (0, 1), (0, 1)])\n    assert nx.is_isomorphic(g, h)", "fn_id": 7, "class_fn": false, "repo": "git-tcd/promela", "file": "tests/yacc_test.py", "last_update_at": "2020-03-17T22:51:17+00:00", "question_id": "b26d22aa84a8cfc772d57eb1f17690ed81271c6e_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def nested_if_pg_test():\n    s = '\\n    proctype p(){\\n        bit x;\\n        if\\n        :: if\\n           :: true\\n           :: false\\n           fi\\n        :: x\\n        fi\\n    }\\n    '\n    tree = parser.parse(s)\n    g = tree[0].to_pg()\n    dump(g)\n    h = nx.MultiDiGraph()\n    h.add_edges_from([(0, 1), (0, 1), (0, 1)])\n"]]}
{"hexsha": "5fde36e6a35655aa406e77611971ec0c4c646953", "ext": "py", "lang": "Python", "content": "def vectorize_array(f, array, *args):\n    \"\"\" Helper function to vectorize across the rows of a 2D numpy array\n\n    :params f: function to vectorize\n    :params array: 2darray to iterate over.\n    :params args: list of arguments to pass to the function f\n    :returns: ndarray of the results of applying the function to each row.\n    \"\"\"\n    return np.array([f(row, *args) for row in array])", "fn_id": 11, "class_fn": false, "repo": "samueljackson92/major-project", "file": "src/mia/utils.py", "last_update_at": "2020-03-17T00:57:42+00:00", "question_id": "5fde36e6a35655aa406e77611971ec0c4c646953_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def vectorize_array(f, array, *args):\n    \"\"\" Helper function to vectorize across the rows of a 2D numpy array\n\n    :params f: function to vectorize\n    :params array: 2darray to iterate over.\n    :params args: list of arguments to pass to the function f\n    :returns: ndarray of the results of applying the function to each row.\n    \"\"\"\n"]]}
{"hexsha": "2ff8007a69a858e48ee7fb5000d8bff0bc1af91a", "ext": "py", "lang": "Python", "content": "def parse_args(args):\n    parser = argparse.ArgumentParser(description='Process optional arguments.')\n    parser.add_argument('package', help='Specify the package name and version')\n    parser.add_argument('--json', dest=\"is_json\", action='store_true',\n                        help='Format the result as JSON (default: false)')\n\n    return parser.parse_args(args)", "fn_id": 0, "class_fn": false, "repo": "avishayil/python-snyk-test", "file": "src/snyk_test/cli.py", "last_update_at": "2020-11-18T09:40:10+00:00", "question_id": "2ff8007a69a858e48ee7fb5000d8bff0bc1af91a_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def parse_args(args):\n    parser = argparse.ArgumentParser(description='Process optional arguments.')\n    parser.add_argument('package', help='Specify the package name and version')\n    parser.add_argument('--json', dest='is_json', action='store_true', help='Format the result as JSON (default: false)')\n"]]}
{"hexsha": "d1d875c17a52a7299af027dd66b0ae884c4b58cc", "ext": "py", "lang": "Python", "content": "def main(args):\n    \"\"\"The main Command Line Interface for Segmentify\"\"\"\n\n    # parse in images\n    imgs = [util.parse_img(img) for img in args.images]\n\n    if len(imgs) > 1:\n        imgs = np.stack(imgs, axis=0)\n    else:\n        imgs = np.array(imgs)\n\n    with gui_qt():\n        viewer = Viewer(imgs, heatmap=args.heatmap)", "fn_id": 0, "class_fn": false, "repo": "kne42/segmentify", "file": "segmentify/main.py", "last_update_at": "2020-01-28T05:07:13+00:00", "question_id": "d1d875c17a52a7299af027dd66b0ae884c4b58cc_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main(args):\n    \"\"\"The main Command Line Interface for Segmentify\"\"\"\n    imgs = [util.parse_img(img) for img in args.images]\n    if len(imgs) > 1:\n        imgs = np.stack(imgs, axis=0)\n    else:\n        imgs = np.array(imgs)\n    with gui_qt():\n"]]}
{"hexsha": "4de4aba63f64dfbd07fe311092dd95e306a0d5a0", "ext": "py", "lang": "Python", "content": "@pytest.fixture(scope=\"module\")\ndef start_cluster():\n    try:\n        cluster.start()\n\n        yield cluster\n    finally:\n        cluster.shutdown()", "fn_id": 0, "class_fn": false, "repo": "emakarov/ClickHouse", "file": "tests/integration/test_default_compression_codec/test.py", "last_update_at": "2020-12-29T08:19:11+00:00", "question_id": "4de4aba63f64dfbd07fe311092dd95e306a0d5a0_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.fixture(scope='module')\ndef start_cluster():\n    try:\n        cluster.start()\n        yield cluster\n    finally:\n"]]}
{"hexsha": "26281d1a85f723050fb16e7013598656c1fc13ee", "ext": "py", "lang": "Python", "content": "def recieverStatus():\n    ser = openConnection()\n\n    # send random (but existing!) code end evaluate response to get power state\n    ser.write(formatCommand(\"07eb1\"))\n    response = ser.read(200)\n    if b'\\x02002B01\\x03' in response:\n        return True\n    elif b'\\x02310002\\x03' in response:\n        return False    \n    else:\n        raise(\"ERROR: Not recieved any response\")", "fn_id": 2, "class_fn": false, "repo": "FireFrei/Control-Yamaha-RX-V1500-via-Airplay", "file": "reciever_rs232.py", "last_update_at": "2020-09-10T04:05:02+00:00", "question_id": "26281d1a85f723050fb16e7013598656c1fc13ee_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def recieverStatus():\n    ser = openConnection()\n    ser.write(formatCommand('07eb1'))\n    response = ser.read(200)\n    if b'\\x02002B01\\x03' in response:\n        return True\n    elif b'\\x02310002\\x03' in response:\n        return False\n    else:\n"]]}
{"hexsha": "5bc57ff79acfe3428f9ddf5565f8a646f4c3ccbb", "ext": "py", "lang": "Python", "content": "def move_left():\n    x = player.xcor()\n    #y = player.ycor()\n    x = x - playerspeed  # x = x - playerspeed ( Changing value of x each time )\n    if x < -280:  # Blocking the player from crossing\n        x = - 280\n    player.setx(x)  # setting the player's location to new x", "fn_id": 0, "class_fn": false, "repo": "afrozchakure/Python-Games", "file": "Space_Invaders/space_invaders.py", "last_update_at": "2020-08-25T02:05:42+00:00", "question_id": "5bc57ff79acfe3428f9ddf5565f8a646f4c3ccbb_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def move_left():\n    x = player.xcor()\n    x = x - playerspeed\n    if x < -280:\n        x = -280\n"]]}
{"hexsha": "8a647be5c6a9ddba7a6bce39fd7e5b29c396fd4a", "ext": "py", "lang": "Python", "content": "@pytest.mark.asyncio\nasync def test_enable_proxy_unable_to_get_process_number(\n        mocker, mailchimp_list):\n    \"\"\"Tests the enable_proxy function when current_process().index\n    raises an AttributeError.\"\"\"\n    mocked_os = mocker.patch('app.lists.os')\n    mocked_os.environ.get.side_effect = [None, 'foo']\n    mocked_current_proccess = mocker.patch('app.lists.current_process')\n    del mocked_current_proccess.return_value.index\n    mocked_requests = mocker.patch('app.lists.requests')\n    mocked_requests.get.return_value.text = 'foo:bar:baz'\n    mocker.patch('app.lists.asyncio.sleep', new=CoroutineMock())\n    await mailchimp_list.enable_proxy()\n    mocked_requests.get.assert_called_with(\n        'http://us-proxies.com/api.php',\n        params=(\n            ('api', ''),\n            ('uid', '9557'),\n            ('pwd', 'foo'),\n            ('cmd', 'rotate'),\n            ('process', '1'),\n        ),\n    )", "fn_id": 3, "class_fn": false, "repo": "williamhakim10/Benchmarks-Program", "file": "tests/unit/test_lists.py", "last_update_at": "2020-12-18T03:03:59+00:00", "question_id": "8a647be5c6a9ddba7a6bce39fd7e5b29c396fd4a_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.asyncio\nasync def test_enable_proxy_unable_to_get_process_number(mocker, mailchimp_list):\n    \"\"\"Tests the enable_proxy function when current_process().index\n    raises an AttributeError.\"\"\"\n    mocked_os = mocker.patch('app.lists.os')\n    mocked_os.environ.get.side_effect = [None, 'foo']\n    mocked_current_proccess = mocker.patch('app.lists.current_process')\n    del mocked_current_proccess.return_value.index\n    mocked_requests = mocker.patch('app.lists.requests')\n    mocked_requests.get.return_value.text = 'foo:bar:baz'\n    mocker.patch('app.lists.asyncio.sleep', new=CoroutineMock())\n    await mailchimp_list.enable_proxy()\n"]]}
{"hexsha": "25cf96ae6a5620e334caa61021d1a181071e0d66", "ext": "py", "lang": "Python", "content": "def get_opt_perf(folder, ext):\n    folder = Path(folder)\n    files = folder.rglob(f\"optim_performance*.{ext}\")\n\n    data = []\n    for i, f in enumerate(files):\n        # Divide by max hypervolume to get normalised\n        if ext == \"csv\":\n            run = pd.read_csv(f, sep=\",\")[\"hypervolume\"].values / 1e7\n        elif ext == \"txt\":\n            run = pd.read_csv(f, sep=\"\\t\")[\"hypervolume\"].values / 1e7\n        data.append(run)\n    mean = np.array(data).mean(0)\n    std = np.array(data).std(0)\n    stats = pd.DataFrame({\"gen\": range(mean.shape[0]), \"mean\": mean, \"std\": std})\n    stats.to_csv(folder / \"optim_hypervolume.csv\", index=False, sep=\",\")", "fn_id": 0, "class_fn": false, "repo": "fedepare/evolutionary", "file": "extra/plot_optimization_performance.py", "last_update_at": "2020-07-08T11:32:24+00:00", "question_id": "25cf96ae6a5620e334caa61021d1a181071e0d66_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_opt_perf(folder, ext):\n    folder = Path(folder)\n    files = folder.rglob(f'optim_performance*.{ext}')\n    data = []\n    for i, f in enumerate(files):\n        if ext == 'csv':\n            run = pd.read_csv(f, sep=',')['hypervolume'].values / 10000000.0\n        elif ext == 'txt':\n            run = pd.read_csv(f, sep='\\t')['hypervolume'].values / 10000000.0\n        data.append(run)\n    mean = np.array(data).mean(0)\n    std = np.array(data).std(0)\n    stats = pd.DataFrame({'gen': range(mean.shape[0]), 'mean': mean, 'std': std})\n"]]}
{"hexsha": "05ec34444bf0054b5764c6af03d7745638ec37c6", "ext": "py", "lang": "Python", "content": "def main():\n  \"\"\"main entry point for module execution\n  \"\"\"\n\n  argument_spec = dict(gather_subset=dict(default=['!config'], type='list'))\n\n  argument_spec.update(fujitsu_srs_argument_spec)\n\n  module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True)\n\n  gather_subset = module.params['gather_subset']\n\n  runable_subsets = set()\n  exclude_subsets = set()\n\n  for subset in gather_subset:\n    if subset == 'all':\n      runable_subsets.update(VALID_SUBSETS)\n      continue\n\n    if subset.startswith('!'):\n      subset = subset[1:]\n      if subset == 'all':\n        exclude_subsets.update(VALID_SUBSETS)\n        continue\n      exclude = True\n    else:\n      exclude = False\n\n    if subset not in VALID_SUBSETS:\n      module.fail_json(msg='Bad subset')\n\n    if exclude:\n      exclude_subsets.add(subset)\n    else:\n      runable_subsets.add(subset)\n\n  if not runable_subsets:\n    runable_subsets.update(VALID_SUBSETS)\n\n  runable_subsets.difference_update(exclude_subsets)\n  runable_subsets.add('default')\n\n  facts = dict()\n  facts['gather_subset'] = list(runable_subsets)\n\n  instances = list()\n  for key in runable_subsets:\n    instances.append(FACT_SUBSETS[key](module))\n\n  for inst in instances:\n    inst.populate()\n    facts.update(inst.facts)\n\n  # prepend 'ansible_net_' to the facts key\n  ansible_facts = dict()\n  for key, value in iteritems(facts):\n    key = 'ansible_net_%s' % key\n    ansible_facts[key] = value\n\n  warnings = list()\n  check_args(module, warnings)\n\n  module.exit_json(ansible_facts=ansible_facts, warnings=warnings)", "fn_id": 0, "class_fn": false, "repo": "takamitsu-iida/ansible-fujitsu-devices", "file": "plugins/modules/fujitsu_srs_facts.py", "last_update_at": "2020-04-04T05:45:55+00:00", "question_id": "05ec34444bf0054b5764c6af03d7745638ec37c6_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    \"\"\"main entry point for module execution\n  \"\"\"\n    argument_spec = dict(gather_subset=dict(default=['!config'], type='list'))\n    argument_spec.update(fujitsu_srs_argument_spec)\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True)\n    gather_subset = module.params['gather_subset']\n    runable_subsets = set()\n    exclude_subsets = set()\n    for subset in gather_subset:\n        if subset == 'all':\n            runable_subsets.update(VALID_SUBSETS)\n            continue\n        if subset.startswith('!'):\n            subset = subset[1:]\n            if subset == 'all':\n                exclude_subsets.update(VALID_SUBSETS)\n                continue\n            exclude = True\n        else:\n            exclude = False\n        if subset not in VALID_SUBSETS:\n            module.fail_json(msg='Bad subset')\n        if exclude:\n            exclude_subsets.add(subset)\n        else:\n            runable_subsets.add(subset)\n    if not runable_subsets:\n        runable_subsets.update(VALID_SUBSETS)\n    runable_subsets.difference_update(exclude_subsets)\n    runable_subsets.add('default')\n    facts = dict()\n    facts['gather_subset'] = list(runable_subsets)\n    instances = list()\n    for key in runable_subsets:\n        instances.append(FACT_SUBSETS[key](module))\n    for inst in instances:\n        inst.populate()\n        facts.update(inst.facts)\n    ansible_facts = dict()\n    for key, value in iteritems(facts):\n        key = 'ansible_net_%s' % key\n        ansible_facts[key] = value\n    warnings = list()\n    check_args(module, warnings)\n"]]}
{"hexsha": "4a6d1498b256f25d29bcfb889fa36e24e388e3b6", "ext": "py", "lang": "Python", "content": "@projects.command(\"delete\")\n@click.argument(\"project_identifier\")\n@click.option(\"--identifier_type\", type=click.Choice([\"name\", \"id\"]), default=\"name\")\ndef cli_project_delete(project_identifier, identifier_type):\n    \"\"\"Delete project with provided name. Returns total deleted projects\"\"\"\n    project_identifier_arg = project_identifier\n    logger.info(\"about to delete project by {0} = {1}\".format(identifier_type, project_identifier))\n    if identifier_type == \"id\":\n        project_identifier_arg = int(project_identifier)\n    project_delete(project_identifier_arg)\n    print_success(\"Successfuly deleted project name: {0}\".format(project_identifier))", "fn_id": 4, "class_fn": false, "repo": "witrdotnet/gather2gether", "file": "src/gather2gether/cli_project.py", "last_update_at": "2020-02-22T22:54:05+00:00", "question_id": "4a6d1498b256f25d29bcfb889fa36e24e388e3b6_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@projects.command('delete')\n@click.argument('project_identifier')\n@click.option('--identifier_type', type=click.Choice(['name', 'id']), default='name')\ndef cli_project_delete(project_identifier, identifier_type):\n    \"\"\"Delete project with provided name. Returns total deleted projects\"\"\"\n    project_identifier_arg = project_identifier\n    logger.info('about to delete project by {0} = {1}'.format(identifier_type, project_identifier))\n    if identifier_type == 'id':\n        project_identifier_arg = int(project_identifier)\n    project_delete(project_identifier_arg)\n"]]}
{"hexsha": "7336f50c7ae17c7ac9743931d9def55fd899e436", "ext": "py", "lang": "Python", "content": "def image_resize_aspectratio(arImage, nMinDim = 256):\n    \"\"\"\n    Resize aspect ratio of image.\n    Rescale height to 256.\n\n    Keyword arguments:\n    arImage -- np.array\n    nMinDim -- Minimum Dimension (default 256)\n\n    returns np.array of floats\n    \"\"\"\n    if (len(arImage.shape) < 2 ): raise ValueError(\"Image doesnot exist\")\n    nHeigth, nWidth, _ = arImage.shape\n\n    if nWidth >= nHeigth:\n        # wider than high => map heigth to 224\n        fRatio = nMinDim / nHeigth\n    else: \n        fRatio = nMinDim / nWidth\n\n    if fRatio != 1.0:\n        arImage = cv2.resize(arImage, dsize = (0,0), fx = fRatio, fy = fRatio, interpolation=cv2.INTER_LINEAR)\n\n    return arImage", "fn_id": 0, "class_fn": false, "repo": "gudlucsam/Project_DHH", "file": "model_lib/utility_functions.py", "last_update_at": "2020-08-13T16:36:29+00:00", "question_id": "7336f50c7ae17c7ac9743931d9def55fd899e436_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def image_resize_aspectratio(arImage, nMinDim=256):\n    \"\"\"\n    Resize aspect ratio of image.\n    Rescale height to 256.\n\n    Keyword arguments:\n    arImage -- np.array\n    nMinDim -- Minimum Dimension (default 256)\n\n    returns np.array of floats\n    \"\"\"\n    if len(arImage.shape) < 2:\n        raise ValueError('Image doesnot exist')\n    nHeigth, nWidth, _ = arImage.shape\n    if nWidth >= nHeigth:\n        fRatio = nMinDim / nHeigth\n    else:\n        fRatio = nMinDim / nWidth\n    if fRatio != 1.0:\n        arImage = cv2.resize(arImage, dsize=(0, 0), fx=fRatio, fy=fRatio, interpolation=cv2.INTER_LINEAR)\n"]]}
{"hexsha": "36aaad06482e7c8d04fa1bb1da3d6a25d88ddc45", "ext": "py", "lang": "Python", "content": "def get_autoincrement(sqlite):\n    if (sqlite):\n        return \"AUTOINCREMENT\"\n    else:\n        return \"AUTO_INCREMENT\"", "fn_id": 0, "class_fn": false, "repo": "p76dub/23-discord-bot", "file": "src/python/src/commands.py", "last_update_at": "2020-02-17T18:23:54+00:00", "question_id": "36aaad06482e7c8d04fa1bb1da3d6a25d88ddc45_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_autoincrement(sqlite):\n    if sqlite:\n        return 'AUTOINCREMENT'\n    else:\n"]]}
{"hexsha": "9c92e3cfe0b484e62f56af56c09981bd06d65ca1", "ext": "py", "lang": "Python", "content": "def fetch_servers(conn, userId):\n    sql = \"SELECT server_id,server_name,server_icon FROM Servers WHERE owner_id=?\"\n    cur = conn.cursor()\n    cur.execute(sql, (userId,))\n    return cur.fetchall()", "fn_id": 26, "class_fn": false, "repo": "BrandonRBlank/Discord-Bot", "file": "database/DBHelper.py", "last_update_at": "2020-10-09T20:54:20+00:00", "question_id": "9c92e3cfe0b484e62f56af56c09981bd06d65ca1_26", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def fetch_servers(conn, userId):\n    sql = 'SELECT server_id,server_name,server_icon FROM Servers WHERE owner_id=?'\n    cur = conn.cursor()\n    cur.execute(sql, (userId,))\n"]]}
{"hexsha": "6dff4785e5f5d988d4b88df1f9376d4bfac1a614", "ext": "py", "lang": "Python", "content": "def _date_range(calendar_span, focal_date=None):\n    # returns (start_date, end_date)\n    # When calendar_span == week\n    # start_date is the latest monday that is less than or equal to today,\n    # end_date is start_date + seven days\n    # When calendar_span == month \n    # start_date is the first day of the new month\n    # end_date is start_date + length of the current month\n    # both values are then padded by 24 hours to allow for TZ differences\n    # throws ValueError if focal_date is not in ISO-8601 format\n\n    if focal_date:\n        initial_date = datetime.strptime(focal_date, ISO_8601_DATE_FORMAT)\n    else:\n        initial_date = datetime.now()\n\n    initial_date = utc.localize(initial_date)\n    if calendar_span == \"month\":\n        reducer = timedelta(int(initial_date.strftime(\"%d\")))\n        span = timedelta(31)\n    else:\n        # This calculation depends on the fact that monday == 0 in python\n        reducer = timedelta(initial_date.weekday())\n        span = ONE_WEEK\n\n    start_date = initial_date - reducer\n    end_date = start_date + span + ONE_DAY\n    adjusted_start_date = start_date - ONE_DAY\n    return adjusted_start_date, end_date", "fn_id": 0, "class_fn": false, "repo": "masschallenge/impact-api", "file": "web/impact/impact/v1/views/office_hours_calendar_view.py", "last_update_at": "2020-03-08T07:16:21+00:00", "question_id": "6dff4785e5f5d988d4b88df1f9376d4bfac1a614_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _date_range(calendar_span, focal_date=None):\n    if focal_date:\n        initial_date = datetime.strptime(focal_date, ISO_8601_DATE_FORMAT)\n    else:\n        initial_date = datetime.now()\n    initial_date = utc.localize(initial_date)\n    if calendar_span == 'month':\n        reducer = timedelta(int(initial_date.strftime('%d')))\n        span = timedelta(31)\n    else:\n        reducer = timedelta(initial_date.weekday())\n        span = ONE_WEEK\n    start_date = initial_date - reducer\n    end_date = start_date + span + ONE_DAY\n    adjusted_start_date = start_date - ONE_DAY\n"]]}
{"hexsha": "3357f1134ab6d6b6880995f4a2cd67cfb3a2b35a", "ext": "py", "lang": "Python", "content": "def convert_image_by_inkscape(inputimg, outputdir, output_imgname,\n                              outputname='', outputformat='png', overwrite=True,\n                              dpi=150):\n    outputimg = detect_output_file_exist(outputdir, output_imgname,\n                                         outputformat,\n                                         overwrite)\n    if not outputimg:\n        return None\n\n    if inputimg == outputimg:\n        raise FileExistsError\n\n    if outputformat == 'png':\n        outflag = 'e'\n    elif outputformat == 'pdf':\n        outflag = 'A'\n    elif outputformat == 'ps':\n        outflag = 'P'\n    elif outputformat == 'eps':\n        outflag = 'E'\n\n    if shutil.which('inkscape'):\n        process_cmd = ['inkscape', '-zC',\n                       '-f', inputimg, '-{0}'.format(outflag),\n                       outputimg, '-d', str(dpi)]\n        logger.debug(f'start call cmd {process_cmd}')\n        subprocess.check_call(process_cmd)\n        return outputimg  # only retcode is zero\n    else:\n        raise InkscapeProcessError(\"inkscape commond not found.\")", "fn_id": 1, "class_fn": false, "repo": "a358003542/ximage", "file": "ximage/convert_image.py", "last_update_at": "2020-10-30T08:55:17+00:00", "question_id": "3357f1134ab6d6b6880995f4a2cd67cfb3a2b35a_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def convert_image_by_inkscape(inputimg, outputdir, output_imgname, outputname='', outputformat='png', overwrite=True, dpi=150):\n    outputimg = detect_output_file_exist(outputdir, output_imgname, outputformat, overwrite)\n    if not outputimg:\n        return None\n    if inputimg == outputimg:\n        raise FileExistsError\n    if outputformat == 'png':\n        outflag = 'e'\n    elif outputformat == 'pdf':\n        outflag = 'A'\n    elif outputformat == 'ps':\n        outflag = 'P'\n    elif outputformat == 'eps':\n        outflag = 'E'\n    if shutil.which('inkscape'):\n        process_cmd = ['inkscape', '-zC', '-f', inputimg, '-{0}'.format(outflag), outputimg, '-d', str(dpi)]\n        logger.debug(f'start call cmd {process_cmd}')\n        subprocess.check_call(process_cmd)\n        return outputimg\n    else:\n"]]}
{"hexsha": "6c9e8429a08fe7cae9883a160beddb9681ecf991", "ext": "py", "lang": "Python", "content": "def get_specs(logger, conf, directory, specification_kinds):\n    \"\"\"\n    Get specification kinds descriptions and parse all JSON files separating them on the base of markets in\n    specification kinds.\n\n    :param logger:\n    :param conf:\n    :param directory:\n    :param specification_kinds:\n    :return:\n    \"\"\"\n    logger.info('Search for various EMG generators specifications in {}'.format(directory))\n    # Find all json files\n    file_candidates = set()\n    for root, dirs, files in os.walk(directory):\n        # Check only full paths to files\n        json_files = glob.glob('{}/*.json'.format(root))\n        file_candidates.update(json_files)\n\n    # Filter specifications\n    for file in file_candidates:\n        with open(file, encoding=\"utf8\") as fh:\n            try:\n                content = ujson.loads(fh.read())\n            except ValueError:\n                raise ValueError(\"Cannot parse EMG specification file {!r}\".format(os.path.abspath(file)))\n\n        if isinstance(content, dict):\n            __check_file(logger, file, content, specification_kinds)\n\n    # Merge specifications\n    for kind in specification_kinds:\n        spec = __merge_spec_versions(specification_kinds[kind]['specification'],\n                                     get_necessary_conf_property(conf, 'specifications set'))\n        specification_kinds[kind]['specification'] = spec\n        __save_collection(logger, spec, '{} spec.json'.format(kind))\n    return specification_kinds", "fn_id": 0, "class_fn": false, "repo": "mutilin/klever", "file": "core/core/vtg/emg/common/specifications.py", "last_update_at": "2020-02-20T13:15:18+00:00", "question_id": "6c9e8429a08fe7cae9883a160beddb9681ecf991_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_specs(logger, conf, directory, specification_kinds):\n    \"\"\"\n    Get specification kinds descriptions and parse all JSON files separating them on the base of markets in\n    specification kinds.\n\n    :param logger:\n    :param conf:\n    :param directory:\n    :param specification_kinds:\n    :return:\n    \"\"\"\n    logger.info('Search for various EMG generators specifications in {}'.format(directory))\n    file_candidates = set()\n    for root, dirs, files in os.walk(directory):\n        json_files = glob.glob('{}/*.json'.format(root))\n        file_candidates.update(json_files)\n    for file in file_candidates:\n        with open(file, encoding='utf8') as fh:\n            try:\n                content = ujson.loads(fh.read())\n            except ValueError:\n                raise ValueError('Cannot parse EMG specification file {!r}'.format(os.path.abspath(file)))\n        if isinstance(content, dict):\n            __check_file(logger, file, content, specification_kinds)\n    for kind in specification_kinds:\n        spec = __merge_spec_versions(specification_kinds[kind]['specification'], get_necessary_conf_property(conf, 'specifications set'))\n        specification_kinds[kind]['specification'] = spec\n        __save_collection(logger, spec, '{} spec.json'.format(kind))\n"]]}
{"hexsha": "48c9411b3fa428be33edb97fb59b28fd37cf9156", "ext": "py", "lang": "Python", "content": "def test_parse_rsa_algorithm_ps512():\n    (hash, padding) = parse_rsa_algorithm('PS512')\n    assert hash\n    assert hash.name == 'sha512'\n    assert padding.name == 'EMSA-PSS'", "fn_id": 51, "class_fn": false, "repo": "bcarroll/JWTConnect-Python-CryptoJWT", "file": "tests/test_06_jws.py", "last_update_at": "2020-09-30T13:07:42+00:00", "question_id": "48c9411b3fa428be33edb97fb59b28fd37cf9156_51", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_parse_rsa_algorithm_ps512():\n    hash, padding = parse_rsa_algorithm('PS512')\n    assert hash\n    assert hash.name == 'sha512'\n"]]}
{"hexsha": "084ffc6f53d15e2d7e6c129e64073616a4cc7ba3", "ext": "py", "lang": "Python", "content": "def show_warning(ser):\n    warning_bytes = []\n    for x in \"5757a4a4\".split():\n        warning_bytes.append(binascii.a2b_hex(x))\n        ser.writelines(warning_bytes)", "fn_id": 0, "class_fn": false, "repo": "kbdancer/QR-DETECTOR", "file": "QR-DETECTOR Pro V4.0.py", "last_update_at": "2020-12-15T08:35:32+00:00", "question_id": "084ffc6f53d15e2d7e6c129e64073616a4cc7ba3_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def show_warning(ser):\n    warning_bytes = []\n    for x in '5757a4a4'.split():\n        warning_bytes.append(binascii.a2b_hex(x))\n"]]}
{"hexsha": "b2ef62d79752c2a4f8feb0b371165d2f6dc4ee4b", "ext": "py", "lang": "Python", "content": "def test_mongo_config_with_data():\n    \"\"\"Test creation of the MongoConfig model with data.\"\"\"\n    res = MongoConfig(**MONGO_CONFIG)\n    assert isinstance(res, MongoConfig)", "fn_id": 23, "class_fn": false, "repo": "elixir-cloud-aai/flask-connexion-archetype", "file": "tests/models/test_config.py", "last_update_at": "2020-02-19T17:47:08+00:00", "question_id": "b2ef62d79752c2a4f8feb0b371165d2f6dc4ee4b_23", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_mongo_config_with_data():\n    \"\"\"Test creation of the MongoConfig model with data.\"\"\"\n    res = MongoConfig(**MONGO_CONFIG)\n"]]}
{"hexsha": "768927f522f0df61a37ddb309601dac8d35246dd", "ext": "py", "lang": "Python", "content": "@test\ndef plano_command():\n    if PYTHON2: # pragma: nocover\n        raise PlanoTestSkipped(\"The plano command is not supported on Python 2\")\n\n    with working_dir():\n        PlanoCommand().main([])\n\n    with working_dir():\n        write(\"Planofile\", \"garbage\")\n\n        with expect_system_exit():\n            PlanoCommand().main([])\n\n    with expect_system_exit():\n        PlanoCommand(\"no-such-file\").main([])\n\n    with expect_system_exit():\n        PlanoCommand().main([\"-f\", \"no-such-file\"])\n\n    def run_command(*args):\n        PlanoCommand().main([\"-f\", test_project_dir] + list(args))\n\n    with test_project():\n        run_command()\n        run_command(\"--help\")\n        run_command(\"--quiet\")\n        run_command(\"--init-only\")\n\n        run_command(\"build\")\n        run_command(\"install\")\n        run_command(\"clean\")\n\n        with expect_system_exit():\n            run_command(\"build\", \"--help\")\n\n        with expect_system_exit():\n            run_command(\"no-such-command\")\n\n        with expect_system_exit():\n            run_command(\"no-such-command\", \"--help\")\n\n        with expect_system_exit():\n            run_command(\"--help\", \"no-such-command\")\n\n        run_command(\"extended-command\", \"a\", \"b\", \"--omega\", \"z\")\n\n        with expect_system_exit():\n            run_command(\"echo\")\n\n        with expect_exception(contains=\"Trouble\"):\n            run_command(\"echo\", \"Hello\", \"--trouble\")\n\n        run_command(\"echo\", \"Hello\", \"--count\", \"5\")\n\n        with expect_system_exit():\n            run_command(\"echo\", \"Hello\", \"--count\", \"not-an-int\")\n\n        run_command(\"haberdash\", \"ballcap\", \"fedora\", \"hardhat\", \"--last\", \"turban\")\n        result = read_json(\"haberdash.json\")\n        assert result == [\"ballcap\", \"fedora\", \"hardhat\", \"turban\"], result\n\n        run_command(\"haberdash\", \"ballcap\", \"--last\", \"turban\")\n        result = read_json(\"haberdash.json\")\n        assert result == [\"ballcap\", \"turban\"], result\n\n        run_command(\"haberdash\", \"ballcap\")\n        result = read_json(\"haberdash.json\")\n        assert result == [\"ballcap\", \"bowler\"], result\n\n        run_command(\"balderdash\", \"bunk\", \"poppycock\")\n        result = read_json(\"balderdash.json\")\n        assert result == [\"bunk\", \"poppycock\", \"rubbish\"], result\n\n        run_command(\"balderdash\", \"bunk\")\n        result = read_json(\"balderdash.json\")\n        assert result == [\"bunk\", \"malarkey\", \"rubbish\"], result\n\n        run_command(\"balderdash\", \"bunk\", \"--other\", \"bollocks\")\n        result = read_json(\"balderdash.json\")\n        assert result == [\"bunk\", \"malarkey\", \"bollocks\"], result", "fn_id": 21, "class_fn": false, "repo": "ssorj/centralia", "file": "subrepos/transom/subrepos/plano/python/plano_tests.py", "last_update_at": "2020-07-17T17:45:36+00:00", "question_id": "768927f522f0df61a37ddb309601dac8d35246dd_21", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@test\ndef plano_command():\n    if PYTHON2:\n        raise PlanoTestSkipped('The plano command is not supported on Python 2')\n    with working_dir():\n        PlanoCommand().main([])\n    with working_dir():\n        write('Planofile', 'garbage')\n        with expect_system_exit():\n            PlanoCommand().main([])\n    with expect_system_exit():\n        PlanoCommand('no-such-file').main([])\n    with expect_system_exit():\n        PlanoCommand().main(['-f', 'no-such-file'])\n\n    def run_command(*args):\n        PlanoCommand().main(['-f', test_project_dir] + list(args))\n    with test_project():\n        run_command()\n        run_command('--help')\n        run_command('--quiet')\n        run_command('--init-only')\n        run_command('build')\n        run_command('install')\n        run_command('clean')\n        with expect_system_exit():\n            run_command('build', '--help')\n        with expect_system_exit():\n            run_command('no-such-command')\n        with expect_system_exit():\n            run_command('no-such-command', '--help')\n        with expect_system_exit():\n            run_command('--help', 'no-such-command')\n        run_command('extended-command', 'a', 'b', '--omega', 'z')\n        with expect_system_exit():\n            run_command('echo')\n        with expect_exception(contains='Trouble'):\n            run_command('echo', 'Hello', '--trouble')\n        run_command('echo', 'Hello', '--count', '5')\n        with expect_system_exit():\n            run_command('echo', 'Hello', '--count', 'not-an-int')\n        run_command('haberdash', 'ballcap', 'fedora', 'hardhat', '--last', 'turban')\n        result = read_json('haberdash.json')\n        assert result == ['ballcap', 'fedora', 'hardhat', 'turban'], result\n        run_command('haberdash', 'ballcap', '--last', 'turban')\n        result = read_json('haberdash.json')\n        assert result == ['ballcap', 'turban'], result\n        run_command('haberdash', 'ballcap')\n        result = read_json('haberdash.json')\n        assert result == ['ballcap', 'bowler'], result\n        run_command('balderdash', 'bunk', 'poppycock')\n        result = read_json('balderdash.json')\n        assert result == ['bunk', 'poppycock', 'rubbish'], result\n        run_command('balderdash', 'bunk')\n        result = read_json('balderdash.json')\n        assert result == ['bunk', 'malarkey', 'rubbish'], result\n        run_command('balderdash', 'bunk', '--other', 'bollocks')\n        result = read_json('balderdash.json')\n"]]}
{"hexsha": "e70a25a5bdee482509843e096b266c23d77d2875", "ext": "py", "lang": "Python", "content": "def load_data(train_data_location):\n    train_data = {};\n    lines = []\n\n    test_file = open(train_data_location, \"r\")\n    for line in test_file:\n        lines.append(unicode(line, \"utf-8\"))\n    test_file.close()\n    random.shuffle(lines)\n    lines = lines[-INTENT_LIMIT:]\n    train_data[\"intent_data\"] = get_intent_training_data(lines);\n    train_data[\"entity_data\"] = get_entity_training_data(lines);\n            \n    return train_data", "fn_id": 1, "class_fn": false, "repo": "AmeyKamat/ProjectJarvis", "file": "sementicAnalysis/training/ModelTrainer.py", "last_update_at": "2020-07-19T18:37:14+00:00", "question_id": "e70a25a5bdee482509843e096b266c23d77d2875_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_data(train_data_location):\n    train_data = {}\n    lines = []\n    test_file = open(train_data_location, 'r')\n    for line in test_file:\n        lines.append(unicode(line, 'utf-8'))\n    test_file.close()\n    random.shuffle(lines)\n    lines = lines[-INTENT_LIMIT:]\n    train_data['intent_data'] = get_intent_training_data(lines)\n    train_data['entity_data'] = get_entity_training_data(lines)\n"]]}
{"hexsha": "b3fbe2bd9224fb8aefbcc528ef7c2e8b818e74b7", "ext": "py", "lang": "Python", "content": "def all_blocks(session) -> Dict[str, Any]:\n    result = {}\n    for type_ in [Junction, BLK, BLM, Signal, Disconnector, Railway, Track_Section, Control_Area]:\n        result.update({block.id: block for block in session.query(type_).all()})\n    return result", "fn_id": 2, "class_fn": false, "repo": "kmzbrnoI/my2h_transform", "file": "my2h_transform/utils.py", "last_update_at": "2020-03-08T16:56:43+00:00", "question_id": "b3fbe2bd9224fb8aefbcc528ef7c2e8b818e74b7_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def all_blocks(session) -> Dict[str, Any]:\n    result = {}\n    for type_ in [Junction, BLK, BLM, Signal, Disconnector, Railway, Track_Section, Control_Area]:\n        result.update({block.id: block for block in session.query(type_).all()})\n"]]}
{"hexsha": "cc87b34f04ef1079888a151f9b233741b7d3bace", "ext": "py", "lang": "Python", "content": "@app.route('/response', methods=['GET'])\ndef response():\n    response_form = ResponseForm()\n    response_form.body.data = Model.get_response()\n    return render_template('response.html', form=response_form)", "fn_id": 1, "class_fn": false, "repo": "VinterMute/TrueTeam_clusterer", "file": "application/routes.py", "last_update_at": "2020-02-05T02:36:49+00:00", "question_id": "cc87b34f04ef1079888a151f9b233741b7d3bace_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.route('/response', methods=['GET'])\ndef response():\n    response_form = ResponseForm()\n    response_form.body.data = Model.get_response()\n"]]}
{"hexsha": "5a008166fc049653195ca50a0b6d46f3b8d5e955", "ext": "py", "lang": "Python", "content": "@main.group(cls=DYMGroup)\ndef run():\n    \"\"\"Run actions in an app\"\"\"\n    pass", "fn_id": 14, "class_fn": false, "repo": "tonyjames/calm-dsl", "file": "calm/dsl/cli/main.py", "last_update_at": "2020-02-13T02:56:58+00:00", "question_id": "5a008166fc049653195ca50a0b6d46f3b8d5e955_14", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@main.group(cls=DYMGroup)\ndef run():\n    \"\"\"Run actions in an app\"\"\"\n"]]}
{"hexsha": "4679dc674345cea16066de909faaff14eeb152bc", "ext": "py", "lang": "Python", "content": "def test_edge_end_node_not_in_graph():\n\n    graph = Graph()\n\n    end = Vertex('end')\n\n    start = graph.add_node('start')\n\n    with pytest.raises(KeyError):\n        graph.add_edge(start, end)", "fn_id": 2, "class_fn": false, "repo": "nastinsk/python-data-structures-and-algorithms", "file": "Data-Structures/graph/test_graph.py", "last_update_at": "2020-12-05T21:03:13+00:00", "question_id": "4679dc674345cea16066de909faaff14eeb152bc_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_edge_end_node_not_in_graph():\n    graph = Graph()\n    end = Vertex('end')\n    start = graph.add_node('start')\n    with pytest.raises(KeyError):\n"]]}
{"hexsha": "8a9b490b8dead69bfb2c9b3ca3c5e6dff05a38f7", "ext": "py", "lang": "Python", "content": "def log_zinb_positive(\n    x: torch.Tensor, mu: torch.Tensor, theta: torch.Tensor, pi: torch.Tensor, eps=1e-8\n):\n    \"\"\"\n    Log likelihood (scalar) of a minibatch according to a zinb model.\n\n    Parameters\n    ----------\n    x\n        Data\n    mu\n        mean of the negative binomial (has to be positive support) (shape: minibatch x vars)\n    theta\n        inverse dispersion parameter (has to be positive support) (shape: minibatch x vars)\n    pi\n        logit of the dropout parameter (real support) (shape: minibatch x vars)\n    eps\n        numerical stability constant\n\n    Notes\n    -----\n    We parametrize the bernoulli using the logits, hence the softplus functions appearing.\n    \"\"\"\n    # theta is the dispersion rate. If .ndimension() == 1, it is shared for all cells (regardless of batch or labels)\n    if theta.ndimension() == 1:\n        theta = theta.view(\n            1, theta.size(0)\n        )  # In this case, we reshape theta for broadcasting\n\n    softplus_pi = F.softplus(-pi)  # \u00a0uses log(sigmoid(x)) = -softplus(-x)\n    log_theta_eps = torch.log(theta + eps)\n    log_theta_mu_eps = torch.log(theta + mu + eps)\n    pi_theta_log = -pi + theta * (log_theta_eps - log_theta_mu_eps)\n\n    case_zero = F.softplus(pi_theta_log) - softplus_pi\n    mul_case_zero = torch.mul((x < eps).type(torch.float32), case_zero)\n\n    case_non_zero = (\n        -softplus_pi\n        + pi_theta_log\n        + x * (torch.log(mu + eps) - log_theta_mu_eps)\n        + torch.lgamma(x + theta)\n        - torch.lgamma(theta)\n        - torch.lgamma(x + 1)\n    )\n    mul_case_non_zero = torch.mul((x > eps).type(torch.float32), case_non_zero)\n\n    res = mul_case_zero + mul_case_non_zero\n\n    return res", "fn_id": 0, "class_fn": false, "repo": "YosefLab/scVI", "file": "scvi/distributions/_negative_binomial.py", "last_update_at": "2020-09-14T02:46:25+00:00", "question_id": "8a9b490b8dead69bfb2c9b3ca3c5e6dff05a38f7_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def log_zinb_positive(x: torch.Tensor, mu: torch.Tensor, theta: torch.Tensor, pi: torch.Tensor, eps=1e-08):\n    \"\"\"\n    Log likelihood (scalar) of a minibatch according to a zinb model.\n\n    Parameters\n    ----------\n    x\n        Data\n    mu\n        mean of the negative binomial (has to be positive support) (shape: minibatch x vars)\n    theta\n        inverse dispersion parameter (has to be positive support) (shape: minibatch x vars)\n    pi\n        logit of the dropout parameter (real support) (shape: minibatch x vars)\n    eps\n        numerical stability constant\n\n    Notes\n    -----\n    We parametrize the bernoulli using the logits, hence the softplus functions appearing.\n    \"\"\"\n    if theta.ndimension() == 1:\n        theta = theta.view(1, theta.size(0))\n    softplus_pi = F.softplus(-pi)\n    log_theta_eps = torch.log(theta + eps)\n    log_theta_mu_eps = torch.log(theta + mu + eps)\n    pi_theta_log = -pi + theta * (log_theta_eps - log_theta_mu_eps)\n    case_zero = F.softplus(pi_theta_log) - softplus_pi\n    mul_case_zero = torch.mul((x < eps).type(torch.float32), case_zero)\n    case_non_zero = -softplus_pi + pi_theta_log + x * (torch.log(mu + eps) - log_theta_mu_eps) + torch.lgamma(x + theta) - torch.lgamma(theta) - torch.lgamma(x + 1)\n    mul_case_non_zero = torch.mul((x > eps).type(torch.float32), case_non_zero)\n    res = mul_case_zero + mul_case_non_zero\n"]]}
{"hexsha": "e8262f6673f183045cb8601be3e6fddd719417be", "ext": "py", "lang": "Python", "content": "def wait_amd_device_plugin_ready(total_time=3600):\n    while pod_is_ready_or_not(\"name\", \"amdgpu-dp-ds\", \"AMD-Device-Plugin\") != True:\n        logger.info(\"AMD-Device-Plugin is not ready yet. Please wait for a moment!\")\n        time.sleep(10)\n        total_time = total_time - 10\n        if total_time < 0:\n            logger.error(\"An issue occure when starting up AMD-Device-Plugin\")\n            sys.exit(1)", "fn_id": 9, "class_fn": false, "repo": "guningbo/pai", "file": "contrib/kubespray/script/openpai-generator.py", "last_update_at": "2020-09-29T14:34:09+00:00", "question_id": "e8262f6673f183045cb8601be3e6fddd719417be_9", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def wait_amd_device_plugin_ready(total_time=3600):\n    while pod_is_ready_or_not('name', 'amdgpu-dp-ds', 'AMD-Device-Plugin') != True:\n        logger.info('AMD-Device-Plugin is not ready yet. Please wait for a moment!')\n        time.sleep(10)\n        total_time = total_time - 10\n        if total_time < 0:\n            logger.error('An issue occure when starting up AMD-Device-Plugin')\n"]]}
{"hexsha": "d826704b14ad9179eee070a894057980bda56405", "ext": "py", "lang": "Python", "content": "def main(argv):\n    # buffer hack\n    sys.stdout = Unbuffered(sys.stdout)\n\n    path = 'local/config.yml'\n\n    if len(argv) >= 1:\n        path = argv[0]\n\n    try:\n        cc.loadConfig(path)\n    except IOError as ioe:\n        print(\"{0}\".format(ioe))\n\n    for experiment in cc.cfg['experiments']:\n        # load configuration into global var and run it\n        cc.loadExperiment(experiment)\n\n\n        if cc.cfg['model'] == 'rnn':\n\n            if cc.cfg['grid']:\n\n                # REMOVEME: hardcoded grid for ratio\n                grid = [\n                    [1,1,1],\n                    # [1,1,2],\n                    # [1,2,1],\n                    # [2,1,1],\n                    # [1,2,2],\n                    # [2,1,2],\n                    [2,2,1]\n                ]\n                cc.exp['grid'] = {}\n\n                import rnn.rnn\n                reload(rnn.rnn)\n\n                comment = '[GRID][RATIO={}][A549][TDGRUGRU] performing grid search for ratio'\n                for ratios in grid:\n                    cc.exp['params']['rnn']['comment'] = comment.format(':'.join([str(x) for x in ratios]))\n                    cc.exp['grid']['ratios'] = ratios\n\n                    print(cc.cfg,cc.exp)\n                    rnn.rnn.run()\n            else:\n                import rnn.rnn\n                reload(rnn.rnn)\n\n                print(cc.cfg,cc.exp)\n                rnn.rnn.run()\n\n        elif cc.cfg['model'] == 'dnn':\n            import rnn.dnn\n            reload(rnn.dnn)\n\n            print(cc.cfg,cc.exp)\n            rnn.dnn.run()\n        else:\n            raise Exception('Run: unknown model')", "fn_id": 0, "class_fn": false, "repo": "PMitura/smiles-neural-network", "file": "run.py", "last_update_at": "2020-04-25T17:22:24+00:00", "question_id": "d826704b14ad9179eee070a894057980bda56405_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main(argv):\n    sys.stdout = Unbuffered(sys.stdout)\n    path = 'local/config.yml'\n    if len(argv) >= 1:\n        path = argv[0]\n    try:\n        cc.loadConfig(path)\n    except IOError as ioe:\n        print('{0}'.format(ioe))\n    for experiment in cc.cfg['experiments']:\n        cc.loadExperiment(experiment)\n        if cc.cfg['model'] == 'rnn':\n            if cc.cfg['grid']:\n                grid = [[1, 1, 1], [2, 2, 1]]\n                cc.exp['grid'] = {}\n                import rnn.rnn\n                reload(rnn.rnn)\n                comment = '[GRID][RATIO={}][A549][TDGRUGRU] performing grid search for ratio'\n                for ratios in grid:\n                    cc.exp['params']['rnn']['comment'] = comment.format(':'.join([str(x) for x in ratios]))\n                    cc.exp['grid']['ratios'] = ratios\n                    print(cc.cfg, cc.exp)\n                    rnn.rnn.run()\n            else:\n                import rnn.rnn\n                reload(rnn.rnn)\n                print(cc.cfg, cc.exp)\n                rnn.rnn.run()\n        elif cc.cfg['model'] == 'dnn':\n            import rnn.dnn\n            reload(rnn.dnn)\n            print(cc.cfg, cc.exp)\n            rnn.dnn.run()\n        else:\n"]]}
{"hexsha": "68b18f9cc116cc175112bf5e4f150f5fcec6e3e6", "ext": "py", "lang": "Python", "content": "def evaluate(model: nn.Module,\n             data: MolPairDataset,\n             loss_func: Callable,\n             num_tasks: int,\n             metric_func: Callable,\n             batch_size: int,\n             dataset_type: str,\n             scaler: StandardScaler = None,\n             logger: logging.Logger = None) -> List[float]:\n    \"\"\"\n    Evaluates an ensemble of models on a dataset.\n\n    :param model: A model.\n    :param data: A MolPairDataset.\n    :param loss_func: Loss function.\n    :param num_tasks: Number of tasks.\n    :param metric_func: Metric function which takes in a list of targets and a list of predictions.\n    :param batch_size: Batch size.\n    :param dataset_type: Dataset type.\n    :param scaler: A StandardScaler object fit on the training targets.\n    :param logger: Logger.\n    :return: A list with the score for each task based on `metric_func`.\n    \"\"\"\n    loss = val_loss(model, data, loss_func, batch_size, dataset_type, scaler)\n\n    preds = predict(\n        model=model,\n        data=data,\n        batch_size=batch_size,\n        scaler=scaler\n    )\n\n    targets = data.targets()\n\n    results = evaluate_predictions(\n        preds=preds,\n        targets=targets,\n        num_tasks=num_tasks,\n        metric_func=metric_func,\n        dataset_type=dataset_type,\n        logger=logger\n    )\n\n    return results, loss", "fn_id": 2, "class_fn": false, "repo": "cbilodeau2/chemprop", "file": "chemprop/train/evaluate.py", "last_update_at": "2020-04-02T13:10:34+00:00", "question_id": "68b18f9cc116cc175112bf5e4f150f5fcec6e3e6_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def evaluate(model: nn.Module, data: MolPairDataset, loss_func: Callable, num_tasks: int, metric_func: Callable, batch_size: int, dataset_type: str, scaler: StandardScaler=None, logger: logging.Logger=None) -> List[float]:\n    \"\"\"\n    Evaluates an ensemble of models on a dataset.\n\n    :param model: A model.\n    :param data: A MolPairDataset.\n    :param loss_func: Loss function.\n    :param num_tasks: Number of tasks.\n    :param metric_func: Metric function which takes in a list of targets and a list of predictions.\n    :param batch_size: Batch size.\n    :param dataset_type: Dataset type.\n    :param scaler: A StandardScaler object fit on the training targets.\n    :param logger: Logger.\n    :return: A list with the score for each task based on `metric_func`.\n    \"\"\"\n    loss = val_loss(model, data, loss_func, batch_size, dataset_type, scaler)\n    preds = predict(model=model, data=data, batch_size=batch_size, scaler=scaler)\n    targets = data.targets()\n    results = evaluate_predictions(preds=preds, targets=targets, num_tasks=num_tasks, metric_func=metric_func, dataset_type=dataset_type, logger=logger)\n"]]}
{"hexsha": "6757e77dbd602d533e36ec095f38398dacb4e6e3", "ext": "py", "lang": "Python", "content": "def _LogFeedback(try_job_id, email):\n  formatter = quick_logger.Formatter()\n  logger = quick_logger.QuickLogger('bad_bisect', 'report', formatter)\n  message = '%s marked try job %d.' % (email, try_job_id)\n  logger.Log(message)\n  logger.Save()", "fn_id": 0, "class_fn": false, "repo": "bopopescu/catapult-2", "file": "dashboard/dashboard/bad_bisect.py", "last_update_at": "2020-07-24T05:08:25+00:00", "question_id": "6757e77dbd602d533e36ec095f38398dacb4e6e3_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _LogFeedback(try_job_id, email):\n    formatter = quick_logger.Formatter()\n    logger = quick_logger.QuickLogger('bad_bisect', 'report', formatter)\n    message = '%s marked try job %d.' % (email, try_job_id)\n    logger.Log(message)\n"]]}
{"hexsha": "e1cc0ef5ec4e5be0789aef291bf492822295e7a1", "ext": "py", "lang": "Python", "content": "def convert_setplot(setplot_file='setplot.py'):\n\n    setplot_text = open(setplot_file).read()\n    setplot_text = setplot_text.replace('pyclaw.plotters','clawpack.visclaw')\n    setplot_text = setplot_text.replace('2d_grid','2d_patch')\n    setplot_text = setplot_text.replace('gridedges','patchedges')\n    setplot_text = setplot_text.replace('gridlines','celledges')\n    setplot_text = setplot_text.replace('grid_bgcolor','patch_bgcolor')\n\n    os.system(\"mv %s %s_4.x\" % (setplot_file,setplot_file))\n    print('Moved %s to %s_4.x ' % (setplot_file,setplot_file))\n    open(setplot_file,'w').write(setplot_text)\n    print('===> Created ', setplot_file)", "fn_id": 2, "class_fn": false, "repo": "ClimateImpactLab/clawutil", "file": "src/python/clawutil/conversion/convert46to50.py", "last_update_at": "2020-06-08T22:24:06+00:00", "question_id": "e1cc0ef5ec4e5be0789aef291bf492822295e7a1_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def convert_setplot(setplot_file='setplot.py'):\n    setplot_text = open(setplot_file).read()\n    setplot_text = setplot_text.replace('pyclaw.plotters', 'clawpack.visclaw')\n    setplot_text = setplot_text.replace('2d_grid', '2d_patch')\n    setplot_text = setplot_text.replace('gridedges', 'patchedges')\n    setplot_text = setplot_text.replace('gridlines', 'celledges')\n    setplot_text = setplot_text.replace('grid_bgcolor', 'patch_bgcolor')\n    os.system('mv %s %s_4.x' % (setplot_file, setplot_file))\n    print('Moved %s to %s_4.x ' % (setplot_file, setplot_file))\n    open(setplot_file, 'w').write(setplot_text)\n"]]}
{"hexsha": "6f3265a2804fcedfdd4236756f8a2aed5c157ffd", "ext": "py", "lang": "Python", "content": "def write_spectrum_out(R):\n    names = ['wavelength', 'F_lambda', 'e_F_lambda', 'Sky_lambda']\n    hdu = fits.PrimaryHDU(np.array([R.rect_wave, R.flux, R.fluxerr, R.skyflux],\n                                   dtype='float32'))\n    hdu.header['DWAVE'] = R.rect_wave[1] - R.rect_wave[0]\n    hdu.header['WAVE0'] = R.rect_wave[0]\n    hdu.header['WAVESOL'] = 'WAVE0 + DWAVE * linspace(0, NAXIS1)'\n    hdu.header['WAVEUNIT'] = 'A'\n    hdu.header['FLUXUNIT'] = 'ergs/s/cm2/A'\n    for i, name in enumerate(names):\n        hdu.header['ROW%i' % (i+1)] = name\n    for key in R.header.keys():\n        if key in hdu.header:\n            continue\n        hdu.header[key] = R.header[key]\n    hdu.writeto(op.join(R.path, 'spectrum_%s.fits' % R.side_name),\n                overwrite=True)", "fn_id": 21, "class_fn": false, "repo": "grzeimann/Panacea", "file": "combine_amp_reductions.py", "last_update_at": "2020-02-19T20:01:44+00:00", "question_id": "6f3265a2804fcedfdd4236756f8a2aed5c157ffd_21", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def write_spectrum_out(R):\n    names = ['wavelength', 'F_lambda', 'e_F_lambda', 'Sky_lambda']\n    hdu = fits.PrimaryHDU(np.array([R.rect_wave, R.flux, R.fluxerr, R.skyflux], dtype='float32'))\n    hdu.header['DWAVE'] = R.rect_wave[1] - R.rect_wave[0]\n    hdu.header['WAVE0'] = R.rect_wave[0]\n    hdu.header['WAVESOL'] = 'WAVE0 + DWAVE * linspace(0, NAXIS1)'\n    hdu.header['WAVEUNIT'] = 'A'\n    hdu.header['FLUXUNIT'] = 'ergs/s/cm2/A'\n    for i, name in enumerate(names):\n        hdu.header['ROW%i' % (i + 1)] = name\n    for key in R.header.keys():\n        if key in hdu.header:\n            continue\n        hdu.header[key] = R.header[key]\n"]]}
{"hexsha": "102f25fe83def843136195641c40352461bf2b68", "ext": "py", "lang": "Python", "content": "def main(use_stream=True):\n    # Ask for data\n    d.measure()\n    data = b\"{'t': \" + str(d.temperature()) + \", 'h': \" + str(d.humidity()) + \"}\"\n\n    s = socket.socket()\n\n    ai = socket.getaddrinfo(HOST, 443)\n    addr = ai[0][-1]\n    s.connect(addr)\n\n    # SSL wrap\n    s = ussl.wrap_socket(s)\n    \n    # Send POST request to Azure IoT Hub\n    s.write(\"POST /devices/\" + DEVICE + \"/messages/events?api-version=2016-02-03 HTTP/1.0\\r\\n\")\n    # HTTP Headers\n    s.write(\"Host: \" + HOST + \"\\r\\n\")\n    s.write(\"Authorization: \" + SAS + \"\\r\\n\")\n    s.write(\"Content-Type: application/json\\r\\n\")\n    s.write(\"Connection: close\\r\\n\")\n    s.write(\"Content-Length: \" + str(len(data)) + \"\\r\\n\\r\\n\")\n    # Data\n    s.write(data)\n    \n    # Print 128 bytes of response\n    print(s.read(128))\n\n    s.close()", "fn_id": 0, "class_fn": false, "repo": "bechynsky/Micropython", "file": "iothub.py", "last_update_at": "2020-10-07T12:23:58+00:00", "question_id": "102f25fe83def843136195641c40352461bf2b68_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main(use_stream=True):\n    d.measure()\n    data = b\"{'t': \" + str(d.temperature()) + \", 'h': \" + str(d.humidity()) + '}'\n    s = socket.socket()\n    ai = socket.getaddrinfo(HOST, 443)\n    addr = ai[0][-1]\n    s.connect(addr)\n    s = ussl.wrap_socket(s)\n    s.write('POST /devices/' + DEVICE + '/messages/events?api-version=2016-02-03 HTTP/1.0\\r\\n')\n    s.write('Host: ' + HOST + '\\r\\n')\n    s.write('Authorization: ' + SAS + '\\r\\n')\n    s.write('Content-Type: application/json\\r\\n')\n    s.write('Connection: close\\r\\n')\n    s.write('Content-Length: ' + str(len(data)) + '\\r\\n\\r\\n')\n    s.write(data)\n    print(s.read(128))\n"]]}
{"hexsha": "fc80231c6c6cfbc08f59cba6b6ed371e9b7a2a7f", "ext": "py", "lang": "Python", "content": "def decorator_blank_line_and_sleep(function):\n    \"\"\" This decorator just waits one second and prints a blank line after executing function\"\"\"\n    def inner_wrapper(*args, **kwargs):\n        input(\"\")\n        value = function(*args, **kwargs)\n        print()\n        return value\n    return inner_wrapper", "fn_id": 0, "class_fn": false, "repo": "xaviernguyen07/group_project_COMP_1800", "file": "Sandbox/jwt_Example/RUN_THIS.py", "last_update_at": "2020-04-05T20:32:16+00:00", "question_id": "fc80231c6c6cfbc08f59cba6b6ed371e9b7a2a7f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def decorator_blank_line_and_sleep(function):\n    \"\"\" This decorator just waits one second and prints a blank line after executing function\"\"\"\n\n    def inner_wrapper(*args, **kwargs):\n        input('')\n        value = function(*args, **kwargs)\n        print()\n        return value\n"]]}
{"hexsha": "ce2056f419905897c0583d459b2a3dedbe84d838", "ext": "py", "lang": "Python", "content": "@banner_item_api.route('/hide/<int:bid>', methods=['PUT'])\n@route_meta(auth='\u9690\u85cf\u6a2a\u5e45\u5b50\u9879', module='\u6a2a\u5e45\u5b50\u9879')\n@group_required\ndef hide(bid):\n    BannerItem.hide_model(bid, throw=True)\n    return Success(msg='\u6a2a\u5e45\u9690\u85cf\u6210\u529f')", "fn_id": 5, "class_fn": false, "repo": "zcxyun/snack-api-lin", "file": "app/api/cms/banner_item.py", "last_update_at": "2020-02-03T18:39:33+00:00", "question_id": "ce2056f419905897c0583d459b2a3dedbe84d838_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@banner_item_api.route('/hide/<int:bid>', methods=['PUT'])\n@route_meta(auth='\u9690\u85cf\u6a2a\u5e45\u5b50\u9879', module='\u6a2a\u5e45\u5b50\u9879')\n@group_required\ndef hide(bid):\n    BannerItem.hide_model(bid, throw=True)\n"]]}
{"hexsha": "fd8f516ba68ea0a417c745ad518f1d606a0edcf4", "ext": "py", "lang": "Python", "content": "def digraph_of_expectation(G, theta=0.005, iterations=1000, seed=None):\n\tif (seed == None):\n\t\tseed = G.nodes()\n\ttau = {}\n\tedge_list = []\n\tfor v in tqdm(seed):\n\t\tseed_v = {v}\n\t\texp = graph_of_expectation(G=G, iterations=iterations, seed=seed_v)\n\t\tremove = [v]\n\t\tfor i in exp:\n\t\t\texp[i] /= iterations\n\t\t\tif exp[i] < theta :\n\t\t\t\tremove.append(i)\n\t\tfor rem in remove:\n\t\t\tdel exp[rem]\n\t\ttau[v] = exp\n\t\tfor u in tau[v]:\n\t\t\tedge_list.append((v,u))\n\tDG = nx.DiGraph()\n\tDG.add_edges_from(edge_list)\n\treturn DG, tau", "fn_id": 0, "class_fn": false, "repo": "theReuben/sonar", "file": "expectation_digraph.py", "last_update_at": "2020-11-03T01:28:04+00:00", "question_id": "fd8f516ba68ea0a417c745ad518f1d606a0edcf4_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def digraph_of_expectation(G, theta=0.005, iterations=1000, seed=None):\n    if seed == None:\n        seed = G.nodes()\n    tau = {}\n    edge_list = []\n    for v in tqdm(seed):\n        seed_v = {v}\n        exp = graph_of_expectation(G=G, iterations=iterations, seed=seed_v)\n        remove = [v]\n        for i in exp:\n            exp[i] /= iterations\n            if exp[i] < theta:\n                remove.append(i)\n        for rem in remove:\n            del exp[rem]\n        tau[v] = exp\n        for u in tau[v]:\n            edge_list.append((v, u))\n    DG = nx.DiGraph()\n    DG.add_edges_from(edge_list)\n"]]}
{"hexsha": "212b3becef06107f0a212d0613552ab99907b0b8", "ext": "py", "lang": "Python", "content": "def test_build_autorest_options():\n    line = build_autorest_options({\"autorest_options\": {\"A\": \"value\"}}, {\"autorest_options\": {\"B\": \"value value\"}})\n    assert line == [\"--a=value\", \"--b='value value'\"]\n\n    line = build_autorest_options({\"autorest_options\": {\"A\": \"value\"}}, {\"autorest_options\": {\"B\": [\"value1\", \"value2\"]}})\n    assert line == [\"--a=value\", \"--b=value1\", \"--b=value2\"]\n\n    line = build_autorest_options({\"autorest_options\": {\"A\": \"value\"}}, {\"autorest_options\": {\"A\": \"newvalue\"}})\n    assert line == [\"--a=newvalue\"]\n\n    line = build_autorest_options({}, {})\n    assert line == []\n\n    line = build_autorest_options({\"autorest_options\": {\"A\": 12, \"B\": True, \"C\": ''}}, {})\n    assert line == [\"--a=12\", \"--b=True\", \"--c\"]", "fn_id": 1, "class_fn": false, "repo": "EvgenyAgafonchikov/swagger-to-sdk", "file": "tests/test_autorest_tools.py", "last_update_at": "2020-06-01T14:59:36+00:00", "question_id": "212b3becef06107f0a212d0613552ab99907b0b8_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_build_autorest_options():\n    line = build_autorest_options({'autorest_options': {'A': 'value'}}, {'autorest_options': {'B': 'value value'}})\n    assert line == ['--a=value', \"--b='value value'\"]\n    line = build_autorest_options({'autorest_options': {'A': 'value'}}, {'autorest_options': {'B': ['value1', 'value2']}})\n    assert line == ['--a=value', '--b=value1', '--b=value2']\n    line = build_autorest_options({'autorest_options': {'A': 'value'}}, {'autorest_options': {'A': 'newvalue'}})\n    assert line == ['--a=newvalue']\n    line = build_autorest_options({}, {})\n    assert line == []\n    line = build_autorest_options({'autorest_options': {'A': 12, 'B': True, 'C': ''}}, {})\n"]]}
{"hexsha": "6b468d698e3030ea2a8e970e69a4fccd67db2cb6", "ext": "py", "lang": "Python", "content": "def train(model_config, loader_config):    \n    '''\n        This function trains the model that is passed in the first argument,\n        using the arguments used afterwards.\n    '''\n    best_acc = 0.0\n    for epoch in range(0, model_config.epochs):\n        train_acc = parse_epoch(loader_config.trainloader, model_config.model, model_config.optimizer, model_config.criterion, model_config.device)\n        torch.cuda.empty_cache()\n        model_config.scheduler.step()\n        accuracy = parse_epoch(loader_config.testloader, model_config.model, model_config.optimizer, model_config.criterion, model_config.device, train=False)\n        \n        # if accuracy > best_acc:\n        model_config.save_model()\n        best_acc = accuracy  \n\n        if train_acc > 0.9:\n            break", "fn_id": 1, "class_fn": false, "repo": "LaurenceBont/Fairness--Accountability--Confidentiality-and-Transparency-in-AI", "file": "classifier.py", "last_update_at": "2020-01-31T14:16:32+00:00", "question_id": "6b468d698e3030ea2a8e970e69a4fccd67db2cb6_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def train(model_config, loader_config):\n    \"\"\"\n        This function trains the model that is passed in the first argument,\n        using the arguments used afterwards.\n    \"\"\"\n    best_acc = 0.0\n    for epoch in range(0, model_config.epochs):\n        train_acc = parse_epoch(loader_config.trainloader, model_config.model, model_config.optimizer, model_config.criterion, model_config.device)\n        torch.cuda.empty_cache()\n        model_config.scheduler.step()\n        accuracy = parse_epoch(loader_config.testloader, model_config.model, model_config.optimizer, model_config.criterion, model_config.device, train=False)\n        model_config.save_model()\n        best_acc = accuracy\n        if train_acc > 0.9:\n"]]}
{"hexsha": "8865cc07f43910d1429edc8b327be6a6081c800a", "ext": "py", "lang": "Python", "content": "def f1_internet2(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n\n\n    return f1_val", "fn_id": 9, "class_fn": false, "repo": "rubende/CrossFall", "file": "Auxiliary/Metrics.py", "last_update_at": "2020-10-27T09:05:05+00:00", "question_id": "8865cc07f43910d1429edc8b327be6a6081c800a_9", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def f1_internet2(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n"]]}
{"hexsha": "1d605b197cfb37e2bb233810d218c66618736ab9", "ext": "py", "lang": "Python", "content": "def parse_degmin(degmin,sign):\n    if sign in \"EW\":\n        if len(degmin) < 5:\n            return np.nan\n        dec=my_float(degmin[:3]) + my_float(degmin[3:])/60\n    else:\n        if len(degmin) < 4:\n            return np.nan\n        dec=my_float(degmin[:2]) + my_float(degmin[2:])/60\n    if sign in \"WS\":\n        dec*=-1\n    elif sign in 'NE':\n        pass\n    else:\n        return np.nan\n    return dec", "fn_id": 2, "class_fn": false, "repo": "oneconcern/stompy", "file": "stompy/io/nmea.py", "last_update_at": "2020-02-03T19:20:27+00:00", "question_id": "1d605b197cfb37e2bb233810d218c66618736ab9_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def parse_degmin(degmin, sign):\n    if sign in 'EW':\n        if len(degmin) < 5:\n            return np.nan\n        dec = my_float(degmin[:3]) + my_float(degmin[3:]) / 60\n    else:\n        if len(degmin) < 4:\n            return np.nan\n        dec = my_float(degmin[:2]) + my_float(degmin[2:]) / 60\n    if sign in 'WS':\n        dec *= -1\n    elif sign in 'NE':\n        pass\n    else:\n        return np.nan\n"]]}
{"hexsha": "12ce798abf33b127100da403940af825afc78b5b", "ext": "py", "lang": "Python", "content": "def test_document_where():\n    with patch(f\"{__name__}.Document1.collection_ref\") as mock_collection_ref:\n        stream = Mock()\n        stream.stream.return_value = snapshot_generator()\n        ref = Mock()\n        ref.where.return_value = stream\n        mock_collection_ref.return_value = ref\n        docs = list(Document1.where(\"str_attr\", \"==\", \"foo\", None).stream())\n        assert len(docs) == 1\n        doc = docs.pop()\n        assert doc.dict() == mock_data(with_id=True)", "fn_id": 8, "class_fn": false, "repo": "mgwilliams/pyrodantic", "file": "tests/test.py", "last_update_at": "2020-07-31T20:46:02+00:00", "question_id": "12ce798abf33b127100da403940af825afc78b5b_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_document_where():\n    with patch(f'{__name__}.Document1.collection_ref') as mock_collection_ref:\n        stream = Mock()\n        stream.stream.return_value = snapshot_generator()\n        ref = Mock()\n        ref.where.return_value = stream\n        mock_collection_ref.return_value = ref\n        docs = list(Document1.where('str_attr', '==', 'foo', None).stream())\n        assert len(docs) == 1\n        doc = docs.pop()\n"]]}
{"hexsha": "a95af293f0db3ab6a81d94fa0a9fc9af58b9e376", "ext": "py", "lang": "Python", "content": "@pytest.fixture\ndef setup_and_finalize(request, CTX):\n    # Clear City collection before test\n    _delete_all(collection_name=\"City\", CTX=CTX)\n\n    # Clear City collection after test\n    def fin():\n        _delete_all(collection_name=\"City\", CTX=CTX)\n\n    request.addfinalizer(fin)", "fn_id": 4, "class_fn": false, "repo": "billyrrr/onto", "file": "tests/test_domain_model.py", "last_update_at": "2020-10-04T10:01:45+00:00", "question_id": "a95af293f0db3ab6a81d94fa0a9fc9af58b9e376_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.fixture\ndef setup_and_finalize(request, CTX):\n    _delete_all(collection_name='City', CTX=CTX)\n\n    def fin():\n        _delete_all(collection_name='City', CTX=CTX)\n"]]}
{"hexsha": "e5aabded9df3715d8d66b20c171cae6935a88db0", "ext": "py", "lang": "Python", "content": "def update_hex_infowidget(infowidget):\n    observed_hex = infowidget.game_state.grid.observed_hex\n    font = pygame.font.SysFont(\"Calibri\", 20)\n    infowidget.image.fill(pr.WHITE_RGB)\n    if observed_hex is not None:\n        h, w = infowidget.height, infowidget.width\n        triplets = [(observed_hex, h // 4, str(observed_hex.hex_type))]\n        if observed_hex.object is not None:\n            triplets.append((observed_hex.object, 3 * h // 4, str(observed_hex.object.character_type)))\n        for obj, oh, tl in triplets:\n            texture = obj.texture.copy()\n            if obj.height > h // 2:\n                texture = pygame.transform.scale(texture,\n                                                 (obj.width, h // 2))\n            texture_rect = texture.get_rect(size=texture.get_size(),\n                                            center=(1 * w // 4, oh))\n            infowidget.image.blit(texture, texture_rect)\n            text = font.render(tl, True, pr.BLACK_RGB)\n            if text.get_width() > w // 2:\n                text = pygame.transform.scale(text, (w // 2, text.get_height()))\n            text_rect = text.get_rect(size=text.get_size(),\n                                      center=(3 * w // 4, oh))\n            infowidget.image.blit(text, text_rect)\n    else:\n        text = font.render(\"Nothing to observe\", True, pr.BLACK_RGB)\n        text_rect = text.get_rect(size=text.get_size(),\n                                  center=(infowidget.width // 2, infowidget.height // 2))\n        infowidget.image.blit(text, text_rect)", "fn_id": 2, "class_fn": false, "repo": "bangersNmash/smg", "file": "gui.py", "last_update_at": "2020-03-31T18:56:22+00:00", "question_id": "e5aabded9df3715d8d66b20c171cae6935a88db0_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def update_hex_infowidget(infowidget):\n    observed_hex = infowidget.game_state.grid.observed_hex\n    font = pygame.font.SysFont('Calibri', 20)\n    infowidget.image.fill(pr.WHITE_RGB)\n    if observed_hex is not None:\n        h, w = (infowidget.height, infowidget.width)\n        triplets = [(observed_hex, h // 4, str(observed_hex.hex_type))]\n        if observed_hex.object is not None:\n            triplets.append((observed_hex.object, 3 * h // 4, str(observed_hex.object.character_type)))\n        for obj, oh, tl in triplets:\n            texture = obj.texture.copy()\n            if obj.height > h // 2:\n                texture = pygame.transform.scale(texture, (obj.width, h // 2))\n            texture_rect = texture.get_rect(size=texture.get_size(), center=(1 * w // 4, oh))\n            infowidget.image.blit(texture, texture_rect)\n            text = font.render(tl, True, pr.BLACK_RGB)\n            if text.get_width() > w // 2:\n                text = pygame.transform.scale(text, (w // 2, text.get_height()))\n            text_rect = text.get_rect(size=text.get_size(), center=(3 * w // 4, oh))\n            infowidget.image.blit(text, text_rect)\n    else:\n        text = font.render('Nothing to observe', True, pr.BLACK_RGB)\n        text_rect = text.get_rect(size=text.get_size(), center=(infowidget.width // 2, infowidget.height // 2))\n"]]}
{"hexsha": "d6b0fb2d72085fe98785f68d017e42639962caa0", "ext": "py", "lang": "Python", "content": "def find_svf(n_states, trajectories):\n    \"\"\"\n    Find the state visitation frequency from trajectories.\n\n    n_states: Number of states. int.\n    trajectories: 3D array of state/action pairs. States are ints, actions\n        are ints. NumPy array with shape (T, L, 2) where T is the number of\n        trajectories and L is the trajectory length.\n    -> State visitation frequencies vector with shape (N,).\n    \"\"\"\n\n    svf = np.zeros(n_states)\n\n    for trajectory in trajectories:\n        for state, _, _ in trajectory:\n            svf[state] += 1\n\n    svf /= trajectories.shape[0]\n\n    return svf", "fn_id": 1, "class_fn": false, "repo": "yadrimz/Hierarchical-Inverse-Reinforcement-Learning", "file": "hierarchicalrl/sdp_maxent.py", "last_update_at": "2020-02-09T19:09:53+00:00", "question_id": "d6b0fb2d72085fe98785f68d017e42639962caa0_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def find_svf(n_states, trajectories):\n    \"\"\"\n    Find the state visitation frequency from trajectories.\n\n    n_states: Number of states. int.\n    trajectories: 3D array of state/action pairs. States are ints, actions\n        are ints. NumPy array with shape (T, L, 2) where T is the number of\n        trajectories and L is the trajectory length.\n    -> State visitation frequencies vector with shape (N,).\n    \"\"\"\n    svf = np.zeros(n_states)\n    for trajectory in trajectories:\n        for state, _, _ in trajectory:\n            svf[state] += 1\n    svf /= trajectories.shape[0]\n"]]}
{"hexsha": "a11c2ef4cf43b1ec2e0dd41218ccb654f28f840e", "ext": "py", "lang": "Python", "content": "def registration(request):\n\n    user_registered = False\n    error_flag = ''\n\n    if request.method == 'POST':\n        user_form = UserProfileForm(data=request.POST)\n        participants_form = UserParticipantsForm(data=request.POST)\n\n        #profile validation\n        if user_form.is_valid():\n\n            user = user_form.save()\n            user.set_password(user.password)\n            user.save()\n\n            participants = participants_form.save(commit = False)\n            participants.user = user\n\n            participants.save()\n\n            user_registered = True\n\n        else:\n            error_flag = 'Error occured during validation!'\n\n    else:\n        user_form = UserProfileForm()\n        participants_form = UserParticipantsForm()\n\n    return render(request,'user/registration.html',{'user_registered':user_registered,\n                                                    'error_flag':error_flag,\n                                                    'user_form':user_form,\n                                                    'participants_form':participants_form,})", "fn_id": 1, "class_fn": false, "repo": "sifrovacky-cz/kachna", "file": "crypto_duck/user/views.py", "last_update_at": "2020-04-21T19:59:38+00:00", "question_id": "a11c2ef4cf43b1ec2e0dd41218ccb654f28f840e_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def registration(request):\n    user_registered = False\n    error_flag = ''\n    if request.method == 'POST':\n        user_form = UserProfileForm(data=request.POST)\n        participants_form = UserParticipantsForm(data=request.POST)\n        if user_form.is_valid():\n            user = user_form.save()\n            user.set_password(user.password)\n            user.save()\n            participants = participants_form.save(commit=False)\n            participants.user = user\n            participants.save()\n            user_registered = True\n        else:\n            error_flag = 'Error occured during validation!'\n    else:\n        user_form = UserProfileForm()\n        participants_form = UserParticipantsForm()\n"]]}
{"hexsha": "8da77b89f2d73f87d82c9499b1d3cd7508e9c4c5", "ext": "py", "lang": "Python", "content": "def read(*paths, lines=False):\n    \"\"\"\n    Build a file path from *paths and return the contents.\n\n    Parameters:\n        lines - if True, return a list of lines. Defaults to\n            False (send back raw text).\n    \"\"\"\n    with open(os.path.join(*paths), 'r') as src:\n        return src.readlines() if lines else src.read()", "fn_id": 0, "class_fn": false, "repo": "jesserobertson/cogj", "file": "setup.py", "last_update_at": "2020-08-19T19:50:28+00:00", "question_id": "8da77b89f2d73f87d82c9499b1d3cd7508e9c4c5_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def read(*paths, lines=False):\n    \"\"\"\n    Build a file path from *paths and return the contents.\n\n    Parameters:\n        lines - if True, return a list of lines. Defaults to\n            False (send back raw text).\n    \"\"\"\n    with open(os.path.join(*paths), 'r') as src:\n"]]}
{"hexsha": "d7eda8a5af5b265f999a02095a8ce528f9ca6d94", "ext": "py", "lang": "Python", "content": "def chord_parse(rn):\n    str_chord = rn.figure\n    ret = ''\n    for char in str_chord:\n        if (char == 'i') or (char == 'v') or (char == 'I') or (char == 'V'):\n            ret += char\n    return ret", "fn_id": 0, "class_fn": false, "repo": "AD98/MIR-ROR", "file": "python/add_track.py", "last_update_at": "2020-08-31T18:30:58+00:00", "question_id": "d7eda8a5af5b265f999a02095a8ce528f9ca6d94_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def chord_parse(rn):\n    str_chord = rn.figure\n    ret = ''\n    for char in str_chord:\n        if char == 'i' or char == 'v' or char == 'I' or (char == 'V'):\n            ret += char\n"]]}
{"hexsha": "a982b089080491ccbb04052acf293edc7cb2a2f1", "ext": "py", "lang": "Python", "content": "def preview_element(my_app: \"sqlApp\"):\n    help_text = \"\"\"\n    Press Enter in the input box to page through the table.\n    Alternatively, enter a filtering SQL statement and then press Enter\n    to page through the results.\n    \"\"\"\n    formatter = TabularOutputFormatter()\n    input_buffer = Buffer(\n            name = \"previewbuffer\",\n            tempfile_suffix = \".sql\",\n            multiline = False\n            )\n\n    input_control = BufferControl(\n            buffer = input_buffer,\n            include_default_input_processors = False,\n            preview_search = False\n    )\n    input_window = Window(\n            input_control,\n        )\n\n    search_buffer = Buffer(name = \"previewsearchbuffer\")\n    search_field = SearchToolbar(search_buffer)\n    output_field = TextArea(style = \"class:preview-output-field\",\n            text = help_text,\n            height = D(preferred = 50),\n            search_field=search_field,\n            wrap_lines = False,\n            focusable = True,\n            read_only = True,\n            preview_search = True,\n            input_processors = [\n                ConditionalProcessor(\n                    processor=HighlightIncrementalSearchProcessor(),\n                    filter=has_focus(\"previewsearchbuffer\")\n                    | has_focus(search_field.control),\n                    ),\n                HighlightSelectionProcessor(),\n            ]\n            )\n\n    def refresh_results(window_height) -> bool:\n        sql_conn = my_app.selected_object.conn\n\n        if sql_conn.execution_status == executionStatus.FAIL:\n            # Let's display the error message to the user\n            output = sql_conn.execution_err\n        else:\n            crsr = sql_conn.cursor\n            if crsr.description:\n                cols = [col.name for col in crsr.description]\n            else:\n                cols = []\n            if len(cols):\n                sql_conn.status = connStatus.FETCHING\n                res = sql_conn.async_fetchmany(size = window_height - 4)\n                output = formatter.format_output(res, cols, format_name = \"psql\")\n                output = \"\\n\".join(output)\n            else:\n                sql_conn.status = connStatus.IDLE\n                output = \"No rows returned\\n\"\n\n        # Add text to output buffer.\n        output_field.buffer.set_document(Document(\n            text = output, cursor_position = 0), True)\n\n        return True\n\n    def accept(buff: Buffer) -> bool:\n        obj = my_app.selected_object\n        sql_conn = obj.conn\n        catalog = None\n        schema = None\n        # TODO: Verify connected\n        if obj.parent is not None:\n            if type(obj.parent).__name__ == \"myDBSchema\":\n                schema = obj.parent.name\n            elif type(obj.parent).__name__ == \"myDBCatalog\":\n                catalog = obj.parent.name\n            if obj.parent.parent is not None:\n                if type(obj.parent.parent).__name__ == \"myDBCatalog\":\n                    catalog = obj.parent.parent.name\n\n        if catalog:\n            catalog =  (sql_conn.quotechar + \"%s\" + sql_conn.quotechar) % catalog\n        if schema:\n            schema =  (sql_conn.quotechar + \"%s\" + sql_conn.quotechar) % schema\n        name = (sql_conn.quotechar + \"%s\" + sql_conn.quotechar) % obj.name\n        identifier = \".\".join(list(filter(None, [catalog, schema, obj.name])))\n        query = sql_conn.preview_query(table = identifier, filter_query = buff.text,\n                limit = my_app.preview_limit_rows)\n\n        func = partial(refresh_results,\n                window_height = output_field.window.render_info.window_height)\n        # If status is IDLE, this is the first time we are executing.\n        if sql_conn.query != query or sql_conn.status == connStatus.IDLE:\n            # Exit the app to execute the query\n            my_app.application.exit(result = [\"preview\", query])\n            my_app.application.pre_run_callables.append(func)\n        else:\n            # No need to exit let's just go and fetch\n            func()\n        return True # Keep filter text\n\n    input_buffer.accept_handler = accept\n\n    def cancel_handler() -> None:\n        sql_conn = my_app.selected_object.conn\n        sql_conn.close_cursor()\n        sql_conn.status = connStatus.IDLE\n        input_buffer.text = \"\"\n        output_field.buffer.set_document(Document(\n            text = help_text, cursor_position = 0\n        ), True)\n        my_app.show_preview = False\n        my_app.show_sidebar = True\n        my_app.application.layout.focus(input_buffer)\n        my_app.application.layout.focus(\"sidebarbuffer\")\n        return None\n\n    cancel_button = Button(text = \"Done\", handler = cancel_handler)\n\n    container = HSplit(\n            [\n                Box(\n                    body = VSplit(\n                        [input_window, cancel_button],\n                        padding=1\n                    ),\n                    padding=1,\n                    style=\"class:preview-input-field\"\n                ),\n                Window(height=1, char=\"-\", style=\"class:preview-divider-line\"),\n                output_field,\n                search_field,\n                ]\n            )\n\n    frame = Shadow(\n            body = Frame(\n                title = \"Table Preview\",\n                body = container,\n                style=\"class:dialog.body\",\n                width = D(preferred = 180, min = 30),\n                modal = True\n            )\n    )\n\n\n    return ConditionalContainer(\n            content = frame,\n            filter = ShowPreview(my_app) & ~is_done\n    )", "fn_id": 0, "class_fn": false, "repo": "detule/odbc-cli", "file": "odbcli/preview.py", "last_update_at": "2020-09-21T15:51:48+00:00", "question_id": "a982b089080491ccbb04052acf293edc7cb2a2f1_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def preview_element(my_app: 'sqlApp'):\n    help_text = '\\n    Press Enter in the input box to page through the table.\\n    Alternatively, enter a filtering SQL statement and then press Enter\\n    to page through the results.\\n    '\n    formatter = TabularOutputFormatter()\n    input_buffer = Buffer(name='previewbuffer', tempfile_suffix='.sql', multiline=False)\n    input_control = BufferControl(buffer=input_buffer, include_default_input_processors=False, preview_search=False)\n    input_window = Window(input_control)\n    search_buffer = Buffer(name='previewsearchbuffer')\n    search_field = SearchToolbar(search_buffer)\n    output_field = TextArea(style='class:preview-output-field', text=help_text, height=D(preferred=50), search_field=search_field, wrap_lines=False, focusable=True, read_only=True, preview_search=True, input_processors=[ConditionalProcessor(processor=HighlightIncrementalSearchProcessor(), filter=has_focus('previewsearchbuffer') | has_focus(search_field.control)), HighlightSelectionProcessor()])\n\n    def refresh_results(window_height) -> bool:\n        sql_conn = my_app.selected_object.conn\n        if sql_conn.execution_status == executionStatus.FAIL:\n            output = sql_conn.execution_err\n        else:\n            crsr = sql_conn.cursor\n            if crsr.description:\n                cols = [col.name for col in crsr.description]\n            else:\n                cols = []\n            if len(cols):\n                sql_conn.status = connStatus.FETCHING\n                res = sql_conn.async_fetchmany(size=window_height - 4)\n                output = formatter.format_output(res, cols, format_name='psql')\n                output = '\\n'.join(output)\n            else:\n                sql_conn.status = connStatus.IDLE\n                output = 'No rows returned\\n'\n        output_field.buffer.set_document(Document(text=output, cursor_position=0), True)\n        return True\n\n    def accept(buff: Buffer) -> bool:\n        obj = my_app.selected_object\n        sql_conn = obj.conn\n        catalog = None\n        schema = None\n        if obj.parent is not None:\n            if type(obj.parent).__name__ == 'myDBSchema':\n                schema = obj.parent.name\n            elif type(obj.parent).__name__ == 'myDBCatalog':\n                catalog = obj.parent.name\n            if obj.parent.parent is not None:\n                if type(obj.parent.parent).__name__ == 'myDBCatalog':\n                    catalog = obj.parent.parent.name\n        if catalog:\n            catalog = (sql_conn.quotechar + '%s' + sql_conn.quotechar) % catalog\n        if schema:\n            schema = (sql_conn.quotechar + '%s' + sql_conn.quotechar) % schema\n        name = (sql_conn.quotechar + '%s' + sql_conn.quotechar) % obj.name\n        identifier = '.'.join(list(filter(None, [catalog, schema, obj.name])))\n        query = sql_conn.preview_query(table=identifier, filter_query=buff.text, limit=my_app.preview_limit_rows)\n        func = partial(refresh_results, window_height=output_field.window.render_info.window_height)\n        if sql_conn.query != query or sql_conn.status == connStatus.IDLE:\n            my_app.application.exit(result=['preview', query])\n            my_app.application.pre_run_callables.append(func)\n        else:\n            func()\n        return True\n    input_buffer.accept_handler = accept\n\n    def cancel_handler() -> None:\n        sql_conn = my_app.selected_object.conn\n        sql_conn.close_cursor()\n        sql_conn.status = connStatus.IDLE\n        input_buffer.text = ''\n        output_field.buffer.set_document(Document(text=help_text, cursor_position=0), True)\n        my_app.show_preview = False\n        my_app.show_sidebar = True\n        my_app.application.layout.focus(input_buffer)\n        my_app.application.layout.focus('sidebarbuffer')\n        return None\n    cancel_button = Button(text='Done', handler=cancel_handler)\n    container = HSplit([Box(body=VSplit([input_window, cancel_button], padding=1), padding=1, style='class:preview-input-field'), Window(height=1, char='-', style='class:preview-divider-line'), output_field, search_field])\n    frame = Shadow(body=Frame(title='Table Preview', body=container, style='class:dialog.body', width=D(preferred=180, min=30), modal=True))\n"]]}
{"hexsha": "e7403d8ed4ba6476c7e5720395f5268978927c4e", "ext": "py", "lang": "Python", "content": "def convert(lines):\n    seen_noncomment = False\n    for line in lines:\n        if line[0] == '#' and not seen_noncomment:\n            # skip comments in beginning of file\n            continue\n        else:\n            seen_noncomment = True\n        c, w = line.rstrip('\\n').split(None, 1)\n        c = int(float(c)) + 1\n        yield c, w ", "fn_id": 0, "class_fn": false, "repo": "Waino/OpenNMT-py", "file": "tools/expected_to_vocab.py", "last_update_at": "2020-09-08T04:42:31+00:00", "question_id": "e7403d8ed4ba6476c7e5720395f5268978927c4e_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def convert(lines):\n    seen_noncomment = False\n    for line in lines:\n        if line[0] == '#' and (not seen_noncomment):\n            continue\n        else:\n            seen_noncomment = True\n        c, w = line.rstrip('\\n').split(None, 1)\n        c = int(float(c)) + 1\n"]]}
{"hexsha": "8ac7b233e97406cd0036cf8e9bc77b7e813edc5e", "ext": "py", "lang": "Python", "content": "def create_ioclass_config(\n        add_default_rule: bool = True, ioclass_config_path: str = default_config_file_path\n):\n    TestRun.LOGGER.info(f\"Creating config file {ioclass_config_path}\")\n    output = TestRun.executor.run(\n        f'echo {IO_CLASS_CONFIG_HEADER} > {ioclass_config_path}'\n    )\n    if output.exit_code != 0:\n        raise Exception(\n            \"Failed to create ioclass config file. \"\n            + f\"stdout: {output.stdout} \\n stderr :{output.stderr}\"\n        )\n    if add_default_rule:\n        output = TestRun.executor.run(\n            f'echo \"0,unclassified,22,1\" >> {ioclass_config_path}'\n        )\n        if output.exit_code != 0:\n            raise Exception(\n                \"Failed to create ioclass config file. \"\n                + f\"stdout: {output.stdout} \\n stderr :{output.stderr}\"\n            )", "fn_id": 0, "class_fn": false, "repo": "arutk/open-cas-linux", "file": "test/functional/api/cas/ioclass_config.py", "last_update_at": "2020-10-14T01:32:58+00:00", "question_id": "8ac7b233e97406cd0036cf8e9bc77b7e813edc5e_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_ioclass_config(add_default_rule: bool=True, ioclass_config_path: str=default_config_file_path):\n    TestRun.LOGGER.info(f'Creating config file {ioclass_config_path}')\n    output = TestRun.executor.run(f'echo {IO_CLASS_CONFIG_HEADER} > {ioclass_config_path}')\n    if output.exit_code != 0:\n        raise Exception('Failed to create ioclass config file. ' + f'stdout: {output.stdout} \\n stderr :{output.stderr}')\n    if add_default_rule:\n        output = TestRun.executor.run(f'echo \"0,unclassified,22,1\" >> {ioclass_config_path}')\n        if output.exit_code != 0:\n"]]}
{"hexsha": "dade6b7690cf9c0ec954b82fa6bd23cceaf873e5", "ext": "py", "lang": "Python", "content": "def prereq():\n    with open('configure.json') as configure_file:\n        json_text = json.load(configure_file)\n\n    # Create a Thing\n    thing_name = json_text['thing_name']\n    thing_obj = thing.Thing(thing_name)\n    if not thing_obj.create():\n\n        # Create a Certificate\n        cert_obj = certs.Certificate()\n        result = cert_obj.create()\n\n        # Store certId\n        cert_id = result['certificateId']\n        cert_id_filename = thing_name + '_cert_id_file.txt'\n        cert_id_file = open(cert_id_filename, 'w')\n        cert_id_file.write(cert_id)\n        cert_id_file_path = os.path.abspath(cert_id_filename)\n        os.chmod(cert_id_file_path, 0o444)\n        cert_id_file.close()\n\n        # Store cert_pem as file\n        cert_pem = result['certificatePem']\n        cert_pem_filename = thing_name + '_cert_pem_file.pem'\n        cert_pem_file = open(cert_pem_filename, 'w')\n        cert_pem_file.write(cert_pem)\n        cert_pem_file_path = os.path.abspath(cert_pem_filename)\n        os.chmod(cert_pem_file_path, 0o444)\n        cert_pem_file.close()\n\n        # Store private key PEM as file\n        private_key_pem = result['keyPair']['PrivateKey']\n        private_key_pem_filename = thing_name + '_private_key_pem_file.pem'\n        private_key_pem_file = open(private_key_pem_filename, 'w')\n        private_key_pem_file.write(private_key_pem)\n        private_key_pem_file_path = os.path.abspath(private_key_pem_filename)\n        os.chmod(private_key_pem_file_path, 0o444)\n        private_key_pem_file.close()\n\n        # Create a Policy\n        policy_document = misc.create_policy_document()\n        policy_name = thing_name + '_amazon_freertos_policy'\n        policy_obj = policy.Policy(policy_name, policy_document)\n        policy_obj.create()\n\n        # Attach certificate to Thing\n        cert_obj.attach_thing(thing_name)\n\n        # Attach policy to certificate\n        cert_obj.attach_policy(policy_name)", "fn_id": 1, "class_fn": false, "repo": "MicroEJ/FreeRTOS", "file": "FreeRTOS-Labs/Demo/FreeRTOS_IoT_Libraries/tools/aws_config_quick_start/SetupAWS.py", "last_update_at": "2020-11-23T02:02:43+00:00", "question_id": "dade6b7690cf9c0ec954b82fa6bd23cceaf873e5_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def prereq():\n    with open('configure.json') as configure_file:\n        json_text = json.load(configure_file)\n    thing_name = json_text['thing_name']\n    thing_obj = thing.Thing(thing_name)\n    if not thing_obj.create():\n        cert_obj = certs.Certificate()\n        result = cert_obj.create()\n        cert_id = result['certificateId']\n        cert_id_filename = thing_name + '_cert_id_file.txt'\n        cert_id_file = open(cert_id_filename, 'w')\n        cert_id_file.write(cert_id)\n        cert_id_file_path = os.path.abspath(cert_id_filename)\n        os.chmod(cert_id_file_path, 292)\n        cert_id_file.close()\n        cert_pem = result['certificatePem']\n        cert_pem_filename = thing_name + '_cert_pem_file.pem'\n        cert_pem_file = open(cert_pem_filename, 'w')\n        cert_pem_file.write(cert_pem)\n        cert_pem_file_path = os.path.abspath(cert_pem_filename)\n        os.chmod(cert_pem_file_path, 292)\n        cert_pem_file.close()\n        private_key_pem = result['keyPair']['PrivateKey']\n        private_key_pem_filename = thing_name + '_private_key_pem_file.pem'\n        private_key_pem_file = open(private_key_pem_filename, 'w')\n        private_key_pem_file.write(private_key_pem)\n        private_key_pem_file_path = os.path.abspath(private_key_pem_filename)\n        os.chmod(private_key_pem_file_path, 292)\n        private_key_pem_file.close()\n        policy_document = misc.create_policy_document()\n        policy_name = thing_name + '_amazon_freertos_policy'\n        policy_obj = policy.Policy(policy_name, policy_document)\n        policy_obj.create()\n        cert_obj.attach_thing(thing_name)\n"]]}
{"hexsha": "b3427a061396ce44eb1ffe4b3906a8a3af55e5a1", "ext": "py", "lang": "Python", "content": "def test_should_return_failure_when_find_stream_by_stream_id_fails(\n    get_stream_details_use_case: GetStreamDetailsUseCase, find_stream_by_stream_id: Mock\n) -> None:\n    stream_id = uuid4()\n    failure = FailureDetails(reason=\"TEST_FIND_STREAM_FAILS\")\n    find_stream_by_stream_id.return_value = Failure(failure)\n\n    actual = get_stream_details_use_case(stream_id)\n\n    find_stream_by_stream_id.assert_called_once()\n    find_stream_by_stream_id.assert_called_with(stream_id)\n    assert isinstance(actual, Result.failure_type)\n    assert isinstance(actual.failure(), FailureDetails)\n    assert failure == actual.failure()", "fn_id": 2, "class_fn": false, "repo": "thepabloaguilar/kamui", "file": "tests/core/usecase/stream/test_get_stream_details.py", "last_update_at": "2020-12-25T16:08:15+00:00", "question_id": "b3427a061396ce44eb1ffe4b3906a8a3af55e5a1_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_should_return_failure_when_find_stream_by_stream_id_fails(get_stream_details_use_case: GetStreamDetailsUseCase, find_stream_by_stream_id: Mock) -> None:\n    stream_id = uuid4()\n    failure = FailureDetails(reason='TEST_FIND_STREAM_FAILS')\n    find_stream_by_stream_id.return_value = Failure(failure)\n    actual = get_stream_details_use_case(stream_id)\n    find_stream_by_stream_id.assert_called_once()\n    find_stream_by_stream_id.assert_called_with(stream_id)\n    assert isinstance(actual, Result.failure_type)\n    assert isinstance(actual.failure(), FailureDetails)\n"]]}
{"hexsha": "a3bff75ac5e1b06534652cc7d1866fca3be20f91", "ext": "py", "lang": "Python", "content": "def deploy(\n        functions: ListOfCloudFunctionTypes,\n        project_id: str,\n        region: str,\n        main_py_filename: str = 'main.py',\n        labels: dict = None,\n        execute=False\n):\n    \"\"\"\n    - Generates `main.py` containing all the functions\n    - Generates and runs `gcloud functions deploy` commands for all the functions from the list\n    https://cloud.google.com/sdk/gcloud/reference/functions/deploy\n\n    :param functions: List of `CloudFunction` types\n    :param project_id: GCP Project ID\n    :param region: GCP Cloud Functions Region\n    :param main_py_filename: full name with path to `main.py` e.g. ```../main.py```\n    :param labels: dictionary with labels\n    :param execute: True if you also want to execute generated commands, False by default\n\n    :return: list of commands to execute\n    \"\"\"\n    main_py_generator.generate(functions, main_py_filename)\n    commands = generate_commands(project_id, functions, region, labels)\n    for command in commands:\n        print('Executing: {}'.format(command))\n        if execute:\n            os.system(command)\n    return commands", "fn_id": 3, "class_fn": false, "repo": "MaximBazarov/gcpy", "file": "gcpy/generators/deploy.py", "last_update_at": "2020-07-29T11:20:35+00:00", "question_id": "a3bff75ac5e1b06534652cc7d1866fca3be20f91_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def deploy(functions: ListOfCloudFunctionTypes, project_id: str, region: str, main_py_filename: str='main.py', labels: dict=None, execute=False):\n    \"\"\"\n    - Generates `main.py` containing all the functions\n    - Generates and runs `gcloud functions deploy` commands for all the functions from the list\n    https://cloud.google.com/sdk/gcloud/reference/functions/deploy\n\n    :param functions: List of `CloudFunction` types\n    :param project_id: GCP Project ID\n    :param region: GCP Cloud Functions Region\n    :param main_py_filename: full name with path to `main.py` e.g. ```../main.py```\n    :param labels: dictionary with labels\n    :param execute: True if you also want to execute generated commands, False by default\n\n    :return: list of commands to execute\n    \"\"\"\n    main_py_generator.generate(functions, main_py_filename)\n    commands = generate_commands(project_id, functions, region, labels)\n    for command in commands:\n        print('Executing: {}'.format(command))\n        if execute:\n            os.system(command)\n"]]}
{"hexsha": "226217e2534a16358b206e917692823728cf2c67", "ext": "py", "lang": "Python", "content": "def test_get_case_no_allegations():\n    \"\"\"\n    Test case detail without allegation info.\n    :return:\n    \"\"\"\n    case_info = get_case(\"02-RC-023360\")\n    assert_equal(len(case_info[\"allegations\"]), 0)", "fn_id": 6, "class_fn": false, "repo": "LexPredict/nlrb_data", "file": "nlrb_data/tests/test_scraper.py", "last_update_at": "2020-11-01T21:02:52+00:00", "question_id": "226217e2534a16358b206e917692823728cf2c67_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_get_case_no_allegations():\n    \"\"\"\n    Test case detail without allegation info.\n    :return:\n    \"\"\"\n    case_info = get_case('02-RC-023360')\n"]]}
{"hexsha": "02fe1b4f5a53f5cdc4d866927100e6f2eac23547", "ext": "py", "lang": "Python", "content": "def test_datetime_to_millis():\n    assert datetime_to_millis(DT_0) == 0\n    assert datetime_to_millis(DT) == 1542333064162\n    datetime_to_millis(DT_NO_TIMEZONE)  # Depends on system timezone", "fn_id": 9, "class_fn": false, "repo": "JabLuszko/fbchat", "file": "tests/test_util.py", "last_update_at": "2020-09-30T09:04:11+00:00", "question_id": "02fe1b4f5a53f5cdc4d866927100e6f2eac23547_9", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_datetime_to_millis():\n    assert datetime_to_millis(DT_0) == 0\n    assert datetime_to_millis(DT) == 1542333064162\n"]]}
{"hexsha": "2c97255ca89c177a0d423b27c58da90c9f3f60b9", "ext": "py", "lang": "Python", "content": "def testISSNAppending(filename):\n    '''\n    Test function that just uses a few random ISSNs to check\n    whether appending rows to the exisiting csv is working correctly\n    '''\n    issnRaws = [\"0002-2667\", \"1096-1216\",  \"1000-9361\"]\n    for issn in issnRaws:\n        infoList = getInfoFromISSN(issn)\n        addEntryToISSNLibrary(filename, infoList)", "fn_id": 4, "class_fn": false, "repo": "NickSarge/librarybuilder", "file": "Libraryify.py", "last_update_at": "2020-08-28T05:07:16+00:00", "question_id": "2c97255ca89c177a0d423b27c58da90c9f3f60b9_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def testISSNAppending(filename):\n    \"\"\"\n    Test function that just uses a few random ISSNs to check\n    whether appending rows to the exisiting csv is working correctly\n    \"\"\"\n    issnRaws = ['0002-2667', '1096-1216', '1000-9361']\n    for issn in issnRaws:\n        infoList = getInfoFromISSN(issn)\n"]]}
{"hexsha": "fa894e365b78fea6e59360041eb5a7771f2f7045", "ext": "py", "lang": "Python", "content": "@given(frame())\ndef test_grayscale(frame: Frame):\n    with patch(\"facelift.transform.cv2.cvtColor\") as mocked_cv2_cvtColor:\n        transform.grayscale(frame)\n\n    mocked_cv2_cvtColor.assert_called_once_with(src=frame, code=cv2.COLOR_BGR2GRAY)", "fn_id": 23, "class_fn": false, "repo": "stephen-bunn/facelift", "file": "tests/test_transforms.py", "last_update_at": "2020-12-25T16:50:40+00:00", "question_id": "fa894e365b78fea6e59360041eb5a7771f2f7045_23", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@given(frame())\ndef test_grayscale(frame: Frame):\n    with patch('facelift.transform.cv2.cvtColor') as mocked_cv2_cvtColor:\n        transform.grayscale(frame)\n"]]}
{"hexsha": "ac8b701ffc5ac7b2621a9dfad531c11bb42c6c0f", "ext": "py", "lang": "Python", "content": "def text_similarity(text_1, text_2):\n    #tokenize, get unique words and clean data.\n    text_1 = clean_text(text_1)\n    text_2 = clean_text(text_2)\n\n    #word-to-word translation\n    if args.translate:\n        print(\"Translating data\")\n        text_1 = translate(text_1)\n        text_2 = translate(text_2)\n    else:\n        print(\"Text already in english\")\n        text_1 = [[word] for word in text_1] #convert in an list of list\n        text_2 = [[word] for word in text_2] #convert in an list of list\n\n    score_wordnet = 0\n    score_yago = 0 \n    cnt_wordnet = 0\n    cnt_yago = 0\n    if args.debug:\n        print(len(text_1),len(text_2))\n    for xx, t in enumerate(text_1):\n        if args.debug:\n            print('-->', xx, t)\n        score_wordnet_r, score_yago_r, ok_wordnet, ok_yago = word_similarity(t, text_2)\n        if ok_wordnet:\n            if args.debug:\n                print(\"++\", cnt_wordnet)\n            score_wordnet += score_wordnet_r\n            cnt_wordnet += 1\n        if ok_yago:\n            if args.debug:\n                print(\"--\", cnt_yago)\n            score_yago += score_yago_r\n            cnt_yago += 1\n    if args.debug:\n        print(\"Done with text 1\")\n\n    for xx, t in enumerate(text_2):\n        if args.debug:\n            print('-->', xx, t)\n        score_wordnet_r, score_yago_r, ok_wordnet, ok_yago = word_similarity(t, text_1)\n        if ok_wordnet:\n            if args.debug:\n                print(\"++\", cnt_wordnet)\n            score_wordnet += score_wordnet_r\n            cnt_wordnet += 1\n        if ok_yago:\n            if args.debug:\n                print(\"--\", cnt_yago)\n            score_yago += score_yago_r\n            cnt_yago += 1\n    if args.debug:\n        print(\"Done with text 2\")\n\n    if cnt_wordnet != 0:\n        score_wordnet /= cnt_wordnet\n    if cnt_yago != 0:\n        score_yago /= cnt_yago\n    return score_wordnet, score_yago", "fn_id": 4, "class_fn": false, "repo": "RQuispeC/fake-news", "file": "text_similarity.py", "last_update_at": "2020-03-14T06:02:36+00:00", "question_id": "ac8b701ffc5ac7b2621a9dfad531c11bb42c6c0f_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def text_similarity(text_1, text_2):\n    text_1 = clean_text(text_1)\n    text_2 = clean_text(text_2)\n    if args.translate:\n        print('Translating data')\n        text_1 = translate(text_1)\n        text_2 = translate(text_2)\n    else:\n        print('Text already in english')\n        text_1 = [[word] for word in text_1]\n        text_2 = [[word] for word in text_2]\n    score_wordnet = 0\n    score_yago = 0\n    cnt_wordnet = 0\n    cnt_yago = 0\n    if args.debug:\n        print(len(text_1), len(text_2))\n    for xx, t in enumerate(text_1):\n        if args.debug:\n            print('-->', xx, t)\n        score_wordnet_r, score_yago_r, ok_wordnet, ok_yago = word_similarity(t, text_2)\n        if ok_wordnet:\n            if args.debug:\n                print('++', cnt_wordnet)\n            score_wordnet += score_wordnet_r\n            cnt_wordnet += 1\n        if ok_yago:\n            if args.debug:\n                print('--', cnt_yago)\n            score_yago += score_yago_r\n            cnt_yago += 1\n    if args.debug:\n        print('Done with text 1')\n    for xx, t in enumerate(text_2):\n        if args.debug:\n            print('-->', xx, t)\n        score_wordnet_r, score_yago_r, ok_wordnet, ok_yago = word_similarity(t, text_1)\n        if ok_wordnet:\n            if args.debug:\n                print('++', cnt_wordnet)\n            score_wordnet += score_wordnet_r\n            cnt_wordnet += 1\n        if ok_yago:\n            if args.debug:\n                print('--', cnt_yago)\n            score_yago += score_yago_r\n            cnt_yago += 1\n    if args.debug:\n        print('Done with text 2')\n    if cnt_wordnet != 0:\n        score_wordnet /= cnt_wordnet\n    if cnt_yago != 0:\n        score_yago /= cnt_yago\n"]]}
{"hexsha": "0f1378a2b467cd7d379914c635842d2e9e749e55", "ext": "py", "lang": "Python", "content": "@pytest.fixture\ndef domain_from_token(domain):\n    client = dnsimple.Client(domain_token = domain.token, sandbox = True)\n    return client.domain(domain.name)", "fn_id": 0, "class_fn": false, "repo": "vigetlabs/dnsimple", "file": "tests/integration/test_records.py", "last_update_at": "2020-01-03T10:33:59+00:00", "question_id": "0f1378a2b467cd7d379914c635842d2e9e749e55_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.fixture\ndef domain_from_token(domain):\n    client = dnsimple.Client(domain_token=domain.token, sandbox=True)\n"]]}
{"hexsha": "1fadb2ec53e4657eabefd42f3153a608a9fd92d1", "ext": "py", "lang": "Python", "content": "def _put(d, keys, value):\n    \"\"\"\n    helper function to put a value into a nested dict with dotted string notation.\n    Sould not be used from the api.\n    :param d:\n    :param keys:\n    :param value:\n    :return:\n    \"\"\"\n    if not isinstance(d, dict):\n        return\n    if \".\" in keys:\n        key, rest = keys.split(\".\", 1)\n        _put(d[key], rest, value)\n    else:\n        d[keys] = value", "fn_id": 11, "class_fn": false, "repo": "peng-data-minimization/minimizer", "file": "data_minimization_tools/__init__.py", "last_update_at": "2020-08-03T11:39:16+00:00", "question_id": "1fadb2ec53e4657eabefd42f3153a608a9fd92d1_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _put(d, keys, value):\n    \"\"\"\n    helper function to put a value into a nested dict with dotted string notation.\n    Sould not be used from the api.\n    :param d:\n    :param keys:\n    :param value:\n    :return:\n    \"\"\"\n    if not isinstance(d, dict):\n        return\n    if '.' in keys:\n        key, rest = keys.split('.', 1)\n        _put(d[key], rest, value)\n    else:\n"]]}
{"hexsha": "0aa09b2ae31833ea0394320b6b1c3a156b07dad0", "ext": "py", "lang": "Python", "content": "def __getParam__(line: str):\n    key = aigpy.string.getSub(line, \"--\", \":\")\n    value = aigpy.string.getSub(line, \":\", \";\")\n    return key, value", "fn_id": 0, "class_fn": false, "repo": "slimshizn/Tidal-Media-Downloader", "file": "TIDALDL-PY/tidal_gui/theme.py", "last_update_at": "2020-10-22T05:17:04+00:00", "question_id": "0aa09b2ae31833ea0394320b6b1c3a156b07dad0_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def __getParam__(line: str):\n    key = aigpy.string.getSub(line, '--', ':')\n    value = aigpy.string.getSub(line, ':', ';')\n"]]}
{"hexsha": "8ad1d8c425550d3c7509bab19f4ff8bd9d9df6d8", "ext": "py", "lang": "Python", "content": "def flush_memcache(app_name):\n    \"\"\"Flushes the memcache.\n\n    Args:\n        app_name: str. The name of the app to deploy.\n    \"\"\"\n    memcache_url = (\n        'https://console.cloud.google.com/appengine/memcache?'\n        'src=ac&project=%s') % app_name\n    common.open_new_tab_in_browser_if_possible(memcache_url)\n    common.ask_user_to_confirm('Please flush the memcache.')\n\n    admin_misc_tab_url = None\n    if app_name == APP_NAME_OPPIASERVER:\n        admin_misc_tab_url = 'https://www.oppia.org/admin#/misc'\n    else:\n        admin_misc_tab_url = 'https://%s.appspot.com/admin#/misc' % app_name\n\n    if admin_misc_tab_url:\n        common.open_new_tab_in_browser_if_possible(admin_misc_tab_url)\n        common.ask_user_to_confirm('Please flush the cache on Oppia website.')", "fn_id": 5, "class_fn": false, "repo": "kaylahardie/oppia", "file": "scripts/release_scripts/deploy.py", "last_update_at": "2020-03-03T07:09:51+00:00", "question_id": "8ad1d8c425550d3c7509bab19f4ff8bd9d9df6d8_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def flush_memcache(app_name):\n    \"\"\"Flushes the memcache.\n\n    Args:\n        app_name: str. The name of the app to deploy.\n    \"\"\"\n    memcache_url = 'https://console.cloud.google.com/appengine/memcache?src=ac&project=%s' % app_name\n    common.open_new_tab_in_browser_if_possible(memcache_url)\n    common.ask_user_to_confirm('Please flush the memcache.')\n    admin_misc_tab_url = None\n    if app_name == APP_NAME_OPPIASERVER:\n        admin_misc_tab_url = 'https://www.oppia.org/admin#/misc'\n    else:\n        admin_misc_tab_url = 'https://%s.appspot.com/admin#/misc' % app_name\n    if admin_misc_tab_url:\n        common.open_new_tab_in_browser_if_possible(admin_misc_tab_url)\n"]]}
{"hexsha": "d045ca13be848bf190eb3eb9ed327a8ca7bb52a5", "ext": "py", "lang": "Python", "content": "def find_triangles_v004(G):\n    \"\"\"\n    Attempt to use parallelism - is much much slower, \n    because I can't find a good variable scope for adj.\n    \"\"\"\n    nodes = [u for u in G]\n    node2deg = {u: len(G[u]) for u in G}\n    node2deg = {u: (deg, i) for i, (u, deg) in enumerate(sorted(node2deg.items(), key=lambda x: x[1]))}\n    adj = {} # adj[u] neighbours of u with higher degree\n    for u in node2deg:\n        d = node2deg[u]\n        adj[u] = {v for v in G[u] if node2deg[v] > d}\n\n    adjs = [(adj[u], adj) for u in G]\n\n    with Pool(4) as p:\n        return sum(p.map(u2ntriangles, adjs))", "fn_id": 3, "class_fn": false, "repo": "tpi12jwe/garageofcode", "file": "garageofcode/networkx/networkx_cluster.py", "last_update_at": "2020-02-11T17:00:47+00:00", "question_id": "d045ca13be848bf190eb3eb9ed327a8ca7bb52a5_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def find_triangles_v004(G):\n    \"\"\"\n    Attempt to use parallelism - is much much slower, \n    because I can't find a good variable scope for adj.\n    \"\"\"\n    nodes = [u for u in G]\n    node2deg = {u: len(G[u]) for u in G}\n    node2deg = {u: (deg, i) for i, (u, deg) in enumerate(sorted(node2deg.items(), key=lambda x: x[1]))}\n    adj = {}\n    for u in node2deg:\n        d = node2deg[u]\n        adj[u] = {v for v in G[u] if node2deg[v] > d}\n    adjs = [(adj[u], adj) for u in G]\n    with Pool(4) as p:\n"]]}
{"hexsha": "a3fd7eb98d5465d81ba621722bcd4628ee14d300", "ext": "py", "lang": "Python", "content": "def DecodeToPoint3d(item):\n    if item is None:\n        return None\n    if isinstance(item, list):\n        return [DecodeToPoint3d(x) for x in item]\n    return rhino3dm.Point3d(item['X'], item['Y'], item['Z'])", "fn_id": 3, "class_fn": false, "repo": "mcneel/compute.rhino3d.clients", "file": "dist/python/compute_rhino3d/Util.py", "last_update_at": "2020-12-15T09:18:02+00:00", "question_id": "a3fd7eb98d5465d81ba621722bcd4628ee14d300_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def DecodeToPoint3d(item):\n    if item is None:\n        return None\n    if isinstance(item, list):\n        return [DecodeToPoint3d(x) for x in item]\n"]]}
{"hexsha": "a1f97b80e5bb1a4780c222371d34da0ad9e5298d", "ext": "py", "lang": "Python", "content": "def load_data():\n    train_dataset = h5py.File('../Datasets/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('../Datasets/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n\n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes", "fn_id": 0, "class_fn": false, "repo": "theroyakash/deep-neural-network-with-numpy", "file": "neuralnetwork.py", "last_update_at": "2020-08-26T12:10:30+00:00", "question_id": "a1f97b80e5bb1a4780c222371d34da0ad9e5298d_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_data():\n    train_dataset = h5py.File('../Datasets/train_catvnoncat.h5', 'r')\n    train_set_x_orig = np.array(train_dataset['train_set_x'][:])\n    train_set_y_orig = np.array(train_dataset['train_set_y'][:])\n    test_dataset = h5py.File('../Datasets/test_catvnoncat.h5', 'r')\n    test_set_x_orig = np.array(test_dataset['test_set_x'][:])\n    test_set_y_orig = np.array(test_dataset['test_set_y'][:])\n    classes = np.array(test_dataset['list_classes'][:])\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n"]]}
{"hexsha": "ebcf8d7d946847179d73bc39dd12acec145d722b", "ext": "py", "lang": "Python", "content": "def create_cog(cog_name: str, *, allow_erasing: bool = False):\n    if not cog_name.isidentifier():\n        print(\"{!r} is not a valid identifier, not creating.\".format(cog_name))\n        return\n    elif keyword.iskeyword(cog_name.lower()) or cog_name.lower() in [\"async\", \"await\"]:\n        print(\"{!r} is a reserved Python keyword, not creating.\".format(cog_name))\n        return\n\n    cog_dir = root / cog_name.lower()\n\n    if cog_dir.exists():\n        if not allow_erasing and (\n            not input(\n                \"Please confirm that you wish to overwrite the following \"\n                \"directory (y/N): '{}'\\nThis will irreversibly remove this directory, and all \"\n                \"files and/or directories contained within.\"\n                \"\\n\".format(cog_dir)\n            )\n            .lower()\n            .startswith(\"y\")\n        ):\n            print(\"Not overwriting.\")\n            return\n\n        print(\"Removing directory '{}' and all it's contents\".format(cog_dir))\n        shutil.rmtree(str(cog_dir))\n\n    cog_dir = mkdir(cog_name.lower())\n    mkdir(\"locales\", root_dir=cog_dir)\n\n    info = deepcopy(template_info)\n    info[\"tags\"] = args.tag or []\n    info[\"short\"] = args.description if args.short is ... else args.short\n    info[\"description\"] = args.description\n    info = json.dumps(info, indent=2)\n\n    write_file(\n        cog_dir, \"__init__.py\", template_init.format(cog_lower=cog_name.lower(), cog=cog_name)\n    )\n    write_file(\n        cog_dir,\n        \"{}.py\".format(cog_name.lower()),\n        template_cog.format(cog=cog_name, randint=random.randint(1000, 10000000)),\n    )\n    write_file(cog_dir, \"info.json\", info)", "fn_id": 2, "class_fn": false, "repo": "notodinair/RedV3-Cogs", "file": "swift_libs/scripts/gen_cog.py", "last_update_at": "2020-06-08T13:39:30+00:00", "question_id": "ebcf8d7d946847179d73bc39dd12acec145d722b_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_cog(cog_name: str, *, allow_erasing: bool=False):\n    if not cog_name.isidentifier():\n        print('{!r} is not a valid identifier, not creating.'.format(cog_name))\n        return\n    elif keyword.iskeyword(cog_name.lower()) or cog_name.lower() in ['async', 'await']:\n        print('{!r} is a reserved Python keyword, not creating.'.format(cog_name))\n        return\n    cog_dir = root / cog_name.lower()\n    if cog_dir.exists():\n        if not allow_erasing and (not input(\"Please confirm that you wish to overwrite the following directory (y/N): '{}'\\nThis will irreversibly remove this directory, and all files and/or directories contained within.\\n\".format(cog_dir)).lower().startswith('y')):\n            print('Not overwriting.')\n            return\n        print(\"Removing directory '{}' and all it's contents\".format(cog_dir))\n        shutil.rmtree(str(cog_dir))\n    cog_dir = mkdir(cog_name.lower())\n    mkdir('locales', root_dir=cog_dir)\n    info = deepcopy(template_info)\n    info['tags'] = args.tag or []\n    info['short'] = args.description if args.short is ... else args.short\n    info['description'] = args.description\n    info = json.dumps(info, indent=2)\n    write_file(cog_dir, '__init__.py', template_init.format(cog_lower=cog_name.lower(), cog=cog_name))\n    write_file(cog_dir, '{}.py'.format(cog_name.lower()), template_cog.format(cog=cog_name, randint=random.randint(1000, 10000000)))\n"]]}
{"hexsha": "f9dfde5564b274243e48b37f60850b05efef1d65", "ext": "py", "lang": "Python", "content": "def test__SpecialOperatorsDict__set_value():\n    key = 'test'\n    val = 'value'\n\n    special_operators = _SpecialOperatorsDict()\n    assert key not in special_operators\n\n    special_operators._set_value(key, val)\n    assert key in special_operators\n    assert special_operators[key] == val\n\n    with pytest.raises(ValueError, match='Special operator \"test\" already exists'):\n        special_operators._set_value(key, val)", "fn_id": 0, "class_fn": false, "repo": "MatiasRepetto/astropy", "file": "astropy/modeling/tests/test_utils.py", "last_update_at": "2020-02-18T04:10:00+00:00", "question_id": "f9dfde5564b274243e48b37f60850b05efef1d65_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test__SpecialOperatorsDict__set_value():\n    key = 'test'\n    val = 'value'\n    special_operators = _SpecialOperatorsDict()\n    assert key not in special_operators\n    special_operators._set_value(key, val)\n    assert key in special_operators\n    assert special_operators[key] == val\n    with pytest.raises(ValueError, match='Special operator \"test\" already exists'):\n"]]}
{"hexsha": "106de18ec64eb0f46fc6e70a0d2b51ac9eccea14", "ext": "py", "lang": "Python", "content": "def cfg_host_fdl(args):\n    name = args.name if args.name else os.path.basename(args.path)\n\n    global pc\n    pc.add_host_fdl(args.address, args.size, args.path, name)\n    return 0", "fn_id": 7, "class_fn": false, "repo": "siwiembedded/siwiduino", "file": "cores/logicrom/logicromsdk/tools/rda8910/pacgen.py", "last_update_at": "2020-06-21T18:29:02+00:00", "question_id": "106de18ec64eb0f46fc6e70a0d2b51ac9eccea14_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def cfg_host_fdl(args):\n    name = args.name if args.name else os.path.basename(args.path)\n    global pc\n    pc.add_host_fdl(args.address, args.size, args.path, name)\n"]]}
{"hexsha": "9ed2bf6abc747e5f98feaefd6e430539156bc77d", "ext": "py", "lang": "Python", "content": "def map_sv_to_columns_in_dual_coef_matrix(sv_ind_by_class):\n    from collections import defaultdict\n    sv_ind_mapping = defaultdict(lambda: -1)\n    p = 0\n    for indices_per_class in sv_ind_by_class:\n        indices_per_class.sort()\n        for sv_index in indices_per_class:\n            if sv_ind_mapping[sv_index] == -1:\n                sv_ind_mapping[sv_index] = p\n                p += 1\n    return sv_ind_mapping", "fn_id": 2, "class_fn": false, "repo": "PivovarA/scikit-learn_bench", "file": "daal4py/svm.py", "last_update_at": "2020-08-07T16:22:12+00:00", "question_id": "9ed2bf6abc747e5f98feaefd6e430539156bc77d_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def map_sv_to_columns_in_dual_coef_matrix(sv_ind_by_class):\n    from collections import defaultdict\n    sv_ind_mapping = defaultdict(lambda: -1)\n    p = 0\n    for indices_per_class in sv_ind_by_class:\n        indices_per_class.sort()\n        for sv_index in indices_per_class:\n            if sv_ind_mapping[sv_index] == -1:\n                sv_ind_mapping[sv_index] = p\n                p += 1\n"]]}
{"hexsha": "2dbffca75bd9e6751adc440df79bb9eb9723f603", "ext": "py", "lang": "Python", "content": "def swap(l, idx1, idx2):\n    temp = l[idx1]\n    l[idx1] = l[idx2]\n    l[idx2] = temp", "fn_id": 0, "class_fn": false, "repo": "idreesshaikh/python_nlp_2020_fall", "file": "lab_solutions/lab01_qsort.py", "last_update_at": "2020-12-08T09:05:36+00:00", "question_id": "2dbffca75bd9e6751adc440df79bb9eb9723f603_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def swap(l, idx1, idx2):\n    temp = l[idx1]\n    l[idx1] = l[idx2]\n"]]}
{"hexsha": "7fb76cf201d9a53c275ed7ab989adb6c7fc32445", "ext": "py", "lang": "Python", "content": "def _create_hvplot():\n    # Generate some data\n    cl1 = np.random.normal(loc=2, scale=0.2, size=(200, 200))\n    cl2x = np.random.normal(loc=-2, scale=0.6, size=200)\n    cl2y = np.random.normal(loc=-2, scale=0.1, size=200)\n    cl3 = np.random.normal(loc=0, scale=1.5, size=(400, 400))\n    # Create an overlay of points and ellipses\n    clusters = (\n        hv.Points(cl1).opts(color=\"blue\")\n        * hv.Points((cl2x, cl2y)).opts(color=\"green\")\n        * hv.Points(cl3).opts(color=\"#FDDC22\")\n    )\n    plot = clusters * hv.Ellipse(2, 2, 2) * hv.Ellipse(-2, -2, (4, 2))\n    return plot", "fn_id": 1, "class_fn": false, "repo": "Jhsmit/awesome-panel-extensions", "file": "tests/frameworks/fast/test_fast_grid_template.py", "last_update_at": "2020-07-17T12:53:56+00:00", "question_id": "7fb76cf201d9a53c275ed7ab989adb6c7fc32445_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _create_hvplot():\n    cl1 = np.random.normal(loc=2, scale=0.2, size=(200, 200))\n    cl2x = np.random.normal(loc=-2, scale=0.6, size=200)\n    cl2y = np.random.normal(loc=-2, scale=0.1, size=200)\n    cl3 = np.random.normal(loc=0, scale=1.5, size=(400, 400))\n    clusters = hv.Points(cl1).opts(color='blue') * hv.Points((cl2x, cl2y)).opts(color='green') * hv.Points(cl3).opts(color='#FDDC22')\n    plot = clusters * hv.Ellipse(2, 2, 2) * hv.Ellipse(-2, -2, (4, 2))\n"]]}
{"hexsha": "204fe53dee63a8492d4a3b958826dede8b213446", "ext": "py", "lang": "Python", "content": "def read_data():\n    rules = dict()\n    your_ticket = None\n    nearby_tickets = []\n    state = 0\n    with open('in') as f:\n        for line in map(lambda x: x.strip(), f.readlines()):\n            if line == '':\n                state += 1\n                continue\n            if line == 'your ticket:':\n                continue\n            if line == 'nearby tickets:':\n                continue\n\n            if state == 0:\n                parts = line.split(':')\n                ranges = parts[1].split('or')\n                rules[parts[0]] = []\n                for r in ranges:\n                    nums = r.split('-')\n                    rules[parts[0]].append((int(nums[0]), int(nums[1])))\n            if state == 1:\n                your_ticket = list(map(int, line.split(',')))\n\n            if state == 2:\n                nearby_tickets.append(list(map(int, line.split(','))))\n\n    return rules, your_ticket, nearby_tickets", "fn_id": 0, "class_fn": false, "repo": "viddrobnic/adventofcode", "file": "2020/day_16/day_16.py", "last_update_at": "2020-12-01T16:49:12+00:00", "question_id": "204fe53dee63a8492d4a3b958826dede8b213446_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def read_data():\n    rules = dict()\n    your_ticket = None\n    nearby_tickets = []\n    state = 0\n    with open('in') as f:\n        for line in map(lambda x: x.strip(), f.readlines()):\n            if line == '':\n                state += 1\n                continue\n            if line == 'your ticket:':\n                continue\n            if line == 'nearby tickets:':\n                continue\n            if state == 0:\n                parts = line.split(':')\n                ranges = parts[1].split('or')\n                rules[parts[0]] = []\n                for r in ranges:\n                    nums = r.split('-')\n                    rules[parts[0]].append((int(nums[0]), int(nums[1])))\n            if state == 1:\n                your_ticket = list(map(int, line.split(',')))\n            if state == 2:\n                nearby_tickets.append(list(map(int, line.split(','))))\n"]]}
{"hexsha": "bb765b1b232f5f8a00ee5350e917ce8b9edd2da1", "ext": "py", "lang": "Python", "content": "def rectangularVolumeMask(coords,radius,height=None,rcom=None):\n    if rcom is None: rcom = np.zeros(3)\n    if height is None: height = radius  \n\n    r2s = (coords-rcom)**2\n    x_indices = r2s[:,0]<=radius**2\n    y_indices = r2s[:,1]<=radius**2\n\n    z_indices = r2s[:,2]<=height**2\n    return np.logical_and(np.logical_and(x_indices,y_indices),z_indices)", "fn_id": 0, "class_fn": false, "repo": "agurvich/abg_python", "file": "src/abg_python/selection_utils.py", "last_update_at": "2020-09-10T16:36:49+00:00", "question_id": "bb765b1b232f5f8a00ee5350e917ce8b9edd2da1_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def rectangularVolumeMask(coords, radius, height=None, rcom=None):\n    if rcom is None:\n        rcom = np.zeros(3)\n    if height is None:\n        height = radius\n    r2s = (coords - rcom) ** 2\n    x_indices = r2s[:, 0] <= radius ** 2\n    y_indices = r2s[:, 1] <= radius ** 2\n    z_indices = r2s[:, 2] <= height ** 2\n"]]}
{"hexsha": "d3c42b61b2cc3897bab7a120cbae7757dba57eeb", "ext": "py", "lang": "Python", "content": "def reverse_replay_path(path: _PathType) -> Iterator[AccessAssignment]:\n\twith open(path, mode='rb') as file:\n\t\tfor assgnm in reverse_replay(file):\n\t\t\tyield assgnm", "fn_id": 1, "class_fn": false, "repo": "pskopnik/htc-cache-simulator", "file": "src/simulator/recorder.py", "last_update_at": "2020-12-15T16:09:31+00:00", "question_id": "d3c42b61b2cc3897bab7a120cbae7757dba57eeb_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def reverse_replay_path(path: _PathType) -> Iterator[AccessAssignment]:\n    with open(path, mode='rb') as file:\n        for assgnm in reverse_replay(file):\n"]]}
{"hexsha": "0ef4e7618c506b0a37e009f27534dcaa27e57657", "ext": "py", "lang": "Python", "content": "def create_mail(subject, body='', html='false'):\n    \"\"\" Create mail file.\n    \"\"\"\n    if os.path.isfile(body):\n        with open(body, 'r') as fobj:\n            body = fobj.read()\n\n    if subject or body:\n        subject = re.sub(r'\\s+', ' ', subject)\n        if len(subject) > 200:\n            subject = subject[:200] + '...'\n\n        if is_non_ascii(subject):\n            subject = Header(subject, 'utf8').encode()\n\n        if len(body) > 10 * 1000 * 1000:\n            body = body[:10 * 1000 * 1000]\n\n        html = html.lower() == 'true'\n        path = os.path.join(MAILS_PATH,\n                            '{}_{}'.format(int(time.time()), uuid.uuid4()))\n        with open(path, 'w') as fobj:\n            json.dump({'subject': subject, 'body': body, 'html': html}, fobj)", "fn_id": 1, "class_fn": false, "repo": "seastan/lotr-lcg-set-generator", "file": "create_mail.py", "last_update_at": "2020-11-11T17:06:52+00:00", "question_id": "0ef4e7618c506b0a37e009f27534dcaa27e57657_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create_mail(subject, body='', html='false'):\n    \"\"\" Create mail file.\n    \"\"\"\n    if os.path.isfile(body):\n        with open(body, 'r') as fobj:\n            body = fobj.read()\n    if subject or body:\n        subject = re.sub('\\\\s+', ' ', subject)\n        if len(subject) > 200:\n            subject = subject[:200] + '...'\n        if is_non_ascii(subject):\n            subject = Header(subject, 'utf8').encode()\n        if len(body) > 10 * 1000 * 1000:\n            body = body[:10 * 1000 * 1000]\n        html = html.lower() == 'true'\n        path = os.path.join(MAILS_PATH, '{}_{}'.format(int(time.time()), uuid.uuid4()))\n        with open(path, 'w') as fobj:\n"]]}
{"hexsha": "60a75d6f65b97e7dc8786828b1a6a4ebc9dfd802", "ext": "py", "lang": "Python", "content": "def test_xy_displacement_subpixel(data):\n    fft_corr, window_a = data\n    window_b = np.load('data/shift05.npy')\n    #window_a = np.copy(window_b)\n    #window_b = np.roll(window_b, shift=1, axis=0)\n    #window_b = np.roll(window_b, shift=1, axis=1)\n    dx, dy =  fft_corr.get_displacement(window_a, window_b)\n    delta = .5\n    assert abs(dy - delta)  < 0.01\n    assert abs(dx - delta)  < 0.01", "fn_id": 7, "class_fn": false, "repo": "jr7/pypiv", "file": "tests/test_fft_correlator.py", "last_update_at": "2020-06-11T19:42:11+00:00", "question_id": "60a75d6f65b97e7dc8786828b1a6a4ebc9dfd802_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_xy_displacement_subpixel(data):\n    fft_corr, window_a = data\n    window_b = np.load('data/shift05.npy')\n    dx, dy = fft_corr.get_displacement(window_a, window_b)\n    delta = 0.5\n    assert abs(dy - delta) < 0.01\n"]]}
{"hexsha": "adf773952d0a6f5b264d01b0fe831d4833487a9d", "ext": "py", "lang": "Python", "content": "def load_data():\n    # Remove EEG subjects that don't have behavior data\n    behavior_df = pu.load_behavior_data()\n    conn_df = pu.load_connectivity_data()\n    filt_df = conn_df.filter(items=behavior_df.index, axis=0)  # Remove EEG subjects with missing rowvals in behavior_df\n    return behavior_df, filt_df", "fn_id": 0, "class_fn": false, "repo": "boredStats/eeg-machine-learning", "file": "project/eeg_regression.py", "last_update_at": "2020-07-31T11:38:53+00:00", "question_id": "adf773952d0a6f5b264d01b0fe831d4833487a9d_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_data():\n    behavior_df = pu.load_behavior_data()\n    conn_df = pu.load_connectivity_data()\n    filt_df = conn_df.filter(items=behavior_df.index, axis=0)\n"]]}
{"hexsha": "1615ead0da3ff4bda664e18109bd8f5284b1df46", "ext": "py", "lang": "Python", "content": "def visualize_agents(agents, agent_names):\n    \"\"\"Helper to generate agents in a row visualization\"\"\"\n    \n    n = len(agents)\n    src_img = Image.new('RGB', (200*n, 300), 'black')\n    \n    for i, agent in enumerate(agents):\n        src_img.paste(visualize_array(agent.arr), (200*i + 45, 50))\n\n    draw = ImageDraw.Draw(src_img)\n    for i, name in enumerate(agent_names):\n        draw.text(\n            (200*i + 55, 260),\n            name,\n            font=ImageFont.truetype(\"arial\")\n        )\n\n    return src_img", "fn_id": 1, "class_fn": false, "repo": "jasmaa/sorting-ml", "file": "utils.py", "last_update_at": "2020-02-11T18:27:23+00:00", "question_id": "1615ead0da3ff4bda664e18109bd8f5284b1df46_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def visualize_agents(agents, agent_names):\n    \"\"\"Helper to generate agents in a row visualization\"\"\"\n    n = len(agents)\n    src_img = Image.new('RGB', (200 * n, 300), 'black')\n    for i, agent in enumerate(agents):\n        src_img.paste(visualize_array(agent.arr), (200 * i + 45, 50))\n    draw = ImageDraw.Draw(src_img)\n    for i, name in enumerate(agent_names):\n        draw.text((200 * i + 55, 260), name, font=ImageFont.truetype('arial'))\n"]]}
{"hexsha": "81ed0c1ade77b867a5874ae9058d2ef67df9da45", "ext": "py", "lang": "Python", "content": "async def update_channels_stats(DatabaseConn: VTBiliDatabase, dataset_set: list):\n    vtlog.info(\"Collecting channel UUIDs\")\n    channels_uids = []\n    for chan in dataset_set:\n        vtlog.debug(f\"Opening: {chan}\")\n        async with aiofiles.open(chan, \"r\", encoding=\"utf-8\") as fp:\n            dds = ujson.loads(await fp.read())\n        vtlog.debug(f\"Total data: {len(dds)}\")\n        for nn, dd in enumerate(dds):\n            channels_uids.append({\"id\": dd[\"id\"], \"uid\": dd[\"uid\"], \"num\": nn})\n\n    vtlog.info(\"Processing...\")\n    final_data = await main_process_loop(channels_uids)\n    vtlog.info(\"Updating DB data for Hololive...\")\n    try:\n        await asyncio.wait_for(\n            DatabaseConn.update_data(\"hololive_data\", {\"channels\": final_data[\"hololive\"]}), 15.0\n        )\n    except asyncio.TimeoutError:\n        await DatabaseConn.release()\n        DatabaseConn.raise_error()\n        vtlog.error(\"Failed to update Hololive channels data, timeout by 15s...\")\n    vtlog.info(\"Updating DB data for Nijisanji...\")\n    try:\n        await asyncio.wait_for(\n            DatabaseConn.update_data(\"nijisanji_data\", {\"channels\": final_data[\"nijisanji\"]}), 15.0\n        )\n    except asyncio.TimeoutError:\n        await DatabaseConn.release()\n        DatabaseConn.raise_error()\n        vtlog.error(\"Failed to update Nijisanji channels data, timeout by 15s...\")\n    vtlog.info(\"Updating DB data for Others...\")\n    try:\n        await asyncio.wait_for(\n            DatabaseConn.update_data(\"otherbili_data\", {\"channels\": final_data[\"other\"]}), 15.0\n        )\n    except asyncio.TimeoutError:\n        await DatabaseConn.release()\n        DatabaseConn.raise_error()\n        vtlog.error(\"Failed to update Others channels data, timeout by 15s...\")", "fn_id": 3, "class_fn": false, "repo": "noaione/vt-schedule", "file": "server/jobs/channels_bili.py", "last_update_at": "2020-10-08T23:08:17+00:00", "question_id": "81ed0c1ade77b867a5874ae9058d2ef67df9da45_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def update_channels_stats(DatabaseConn: VTBiliDatabase, dataset_set: list):\n    vtlog.info('Collecting channel UUIDs')\n    channels_uids = []\n    for chan in dataset_set:\n        vtlog.debug(f'Opening: {chan}')\n        async with aiofiles.open(chan, 'r', encoding='utf-8') as fp:\n            dds = ujson.loads(await fp.read())\n        vtlog.debug(f'Total data: {len(dds)}')\n        for nn, dd in enumerate(dds):\n            channels_uids.append({'id': dd['id'], 'uid': dd['uid'], 'num': nn})\n    vtlog.info('Processing...')\n    final_data = await main_process_loop(channels_uids)\n    vtlog.info('Updating DB data for Hololive...')\n    try:\n        await asyncio.wait_for(DatabaseConn.update_data('hololive_data', {'channels': final_data['hololive']}), 15.0)\n    except asyncio.TimeoutError:\n        await DatabaseConn.release()\n        DatabaseConn.raise_error()\n        vtlog.error('Failed to update Hololive channels data, timeout by 15s...')\n    vtlog.info('Updating DB data for Nijisanji...')\n    try:\n        await asyncio.wait_for(DatabaseConn.update_data('nijisanji_data', {'channels': final_data['nijisanji']}), 15.0)\n    except asyncio.TimeoutError:\n        await DatabaseConn.release()\n        DatabaseConn.raise_error()\n        vtlog.error('Failed to update Nijisanji channels data, timeout by 15s...')\n    vtlog.info('Updating DB data for Others...')\n    try:\n        await asyncio.wait_for(DatabaseConn.update_data('otherbili_data', {'channels': final_data['other']}), 15.0)\n    except asyncio.TimeoutError:\n        await DatabaseConn.release()\n        DatabaseConn.raise_error()\n"]]}
{"hexsha": "db3e020cf0be41f2bc94999eecbb7411fe3a784b", "ext": "py", "lang": "Python", "content": "@bot.command()\nasync def bestGirl(ctx, arx: str, arx2: str):\n    \"\"\"Tells who best girl is\"\"\"\n    random.seed()\n    if random.randint(1, 100) < 50:\n        await ctx.send(arx)\n    else:\n        await ctx.send(arx2)", "fn_id": 1, "class_fn": false, "repo": "LastAeon77/LibraryOfRuinaBot", "file": "TrailingBot.py", "last_update_at": "2020-08-26T12:16:22+00:00", "question_id": "db3e020cf0be41f2bc94999eecbb7411fe3a784b_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@bot.command()\nasync def bestGirl(ctx, arx: str, arx2: str):\n    \"\"\"Tells who best girl is\"\"\"\n    random.seed()\n    if random.randint(1, 100) < 50:\n        await ctx.send(arx)\n    else:\n"]]}
{"hexsha": "eface47b94c24eac8d6299c4583950766dda3374", "ext": "py", "lang": "Python", "content": "def pkcs7_pad(plaintext: bytes, block_size: int=0x10) -> bytes:\n    \"\"\"\n    Pad a message using the byte padding algorithm described in PKCS#7\n    This padding scheme appends n bytes with value n, with n the amount of padding bytes.\n    \n    The specification describing the padding algorithm can be found here:\n    https://tools.ietf.org/html/rfc2315#section-10.3\n    \"\"\"\n\n    assert 0 < block_size < 0x100\n    \n    # If the plaintext is an exact multiple of block_size, \n    # we need to append a whole block.\n    remainder = block_size - (len(plaintext) % block_size)\n\n    return plaintext + bytes([remainder] * remainder)", "fn_id": 0, "class_fn": false, "repo": "pqlx/pws-crypto", "file": "pws/symmetric/aes/padding.py", "last_update_at": "2020-12-10T01:14:29+00:00", "question_id": "eface47b94c24eac8d6299c4583950766dda3374_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def pkcs7_pad(plaintext: bytes, block_size: int=16) -> bytes:\n    \"\"\"\n    Pad a message using the byte padding algorithm described in PKCS#7\n    This padding scheme appends n bytes with value n, with n the amount of padding bytes.\n    \n    The specification describing the padding algorithm can be found here:\n    https://tools.ietf.org/html/rfc2315#section-10.3\n    \"\"\"\n    assert 0 < block_size < 256\n    remainder = block_size - len(plaintext) % block_size\n"]]}
{"hexsha": "2be0fb16e9aa1df2dc2f258d1f03b85088b475eb", "ext": "py", "lang": "Python", "content": "def verify_test_message(cls, reader):\n    cmd = reader.read_int()\n    for j in range(cmd % 10):\n        s = j % 5\n        if s == 0:\n            cls.assertEqual(1/cmd, reader.read_double())\n        elif s == 1:\n            expected_size = cmd % (1024*10)\n            cls.assertEqual(len(reader.read_string()), expected_size)\n        elif s == 2:\n            cls.assertEqual((cmd % 256), reader.read_byte())\n        elif s == 3:\n            cls.assertEqual(cmd, (reader.read_long() >> 32))\n        elif s == 4:\n            cls.assertEqual('\ud1a9', reader.read_char())\n        else:\n            pass", "fn_id": 0, "class_fn": false, "repo": "jontuk/pychro", "file": "pychro/test/test_pychro.py", "last_update_at": "2020-04-16T15:48:14+00:00", "question_id": "2be0fb16e9aa1df2dc2f258d1f03b85088b475eb_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def verify_test_message(cls, reader):\n    cmd = reader.read_int()\n    for j in range(cmd % 10):\n        s = j % 5\n        if s == 0:\n            cls.assertEqual(1 / cmd, reader.read_double())\n        elif s == 1:\n            expected_size = cmd % (1024 * 10)\n            cls.assertEqual(len(reader.read_string()), expected_size)\n        elif s == 2:\n            cls.assertEqual(cmd % 256, reader.read_byte())\n        elif s == 3:\n            cls.assertEqual(cmd, reader.read_long() >> 32)\n        elif s == 4:\n            cls.assertEqual('\ud1a9', reader.read_char())\n        else:\n"]]}
{"hexsha": "317e5e31316e3a1b047cdee48b3a29e36a75588a", "ext": "py", "lang": "Python", "content": "def check_collision(players, balls):\n    \"\"\"\n    checks if any of the player have collided with any of the balls\n\n    :param players: a dictonary of players\n    :param balls: a list of balls\n    :return: None\n    \"\"\"\n    to_delete = []\n    for player in players:\n        p = players[player]\n        x = p[\"x\"]\n        y = p[\"y\"]\n        for ball in balls:\n            bx = ball[0]\n            by = ball[1]\n\n            dis = math.sqrt((x - bx) ** 2 + (y - by) ** 2)\n            if dis <= START_RADIUS + p[\"score\"]:\n                p[\"score\"] = p[\"score\"] + 0.5\n                balls.remove(ball)", "fn_id": 1, "class_fn": false, "repo": "PabloEckardt/multiplayer-agario-clone", "file": "server.py", "last_update_at": "2020-09-08T09:00:27+00:00", "question_id": "317e5e31316e3a1b047cdee48b3a29e36a75588a_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def check_collision(players, balls):\n    \"\"\"\n    checks if any of the player have collided with any of the balls\n\n    :param players: a dictonary of players\n    :param balls: a list of balls\n    :return: None\n    \"\"\"\n    to_delete = []\n    for player in players:\n        p = players[player]\n        x = p['x']\n        y = p['y']\n        for ball in balls:\n            bx = ball[0]\n            by = ball[1]\n            dis = math.sqrt((x - bx) ** 2 + (y - by) ** 2)\n            if dis <= START_RADIUS + p['score']:\n                p['score'] = p['score'] + 0.5\n"]]}
{"hexsha": "be6bebe00069e88ec10e9e6c433d533d1f7a88bc", "ext": "py", "lang": "Python", "content": "@login_required\ndef editTask(request, id):\n    task = get_object_or_404(Task, pk=id)\n    form = TaskFormEdit(instance=task)\n\n    if (request.method == 'POST'):\n        form = TaskFormEdit(request.POST, instance=task)\n        if form.is_valid():\n            task.save()\n            return redirect('/')\n        else:\n            return render(request, 'tasks/edittask.html', {'task':task, 'form':form})\n\n    else:\n        return render(request, 'tasks/edittask.html', {'task':task, 'form':form})", "fn_id": 3, "class_fn": false, "repo": "hildebrando001/Tarefa", "file": "tasks/views.py", "last_update_at": "2020-09-06T00:28:27+00:00", "question_id": "be6bebe00069e88ec10e9e6c433d533d1f7a88bc_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@login_required\ndef editTask(request, id):\n    task = get_object_or_404(Task, pk=id)\n    form = TaskFormEdit(instance=task)\n    if request.method == 'POST':\n        form = TaskFormEdit(request.POST, instance=task)\n        if form.is_valid():\n            task.save()\n            return redirect('/')\n        else:\n            return render(request, 'tasks/edittask.html', {'task': task, 'form': form})\n    else:\n"]]}
{"hexsha": "1a33654f19204ceeb8cfb59ff9ad6cdec7391d7d", "ext": "py", "lang": "Python", "content": "def rename_feature(feature, feature_type):\n    try:\n        if feature[feature_type] in non_uniq[feature_type]:\n            feature[feature_type] = \"_\".join([\n                feature[feature_type], feature['sequence'],\n                feature['start'], feature['end'], feature['strand']\n            ])\n    except KeyError:\n        pass\n    return feature", "fn_id": 0, "class_fn": false, "repo": "Asplund-Samuelsson/dubdub", "file": "source/genbank_to_gff.py", "last_update_at": "2020-08-18T15:39:54+00:00", "question_id": "1a33654f19204ceeb8cfb59ff9ad6cdec7391d7d_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def rename_feature(feature, feature_type):\n    try:\n        if feature[feature_type] in non_uniq[feature_type]:\n            feature[feature_type] = '_'.join([feature[feature_type], feature['sequence'], feature['start'], feature['end'], feature['strand']])\n    except KeyError:\n        pass\n"]]}
{"hexsha": "1c663fe675187ce878cfcf385a633a6b7fa0cfec", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize(\"length\", [0, 1, 32, 33, 64, 65, 1024])\ndef test_len_string(get_contract, length):\n    source = \"\"\"\n@public\ndef foo(a: string[1024]) -> uint256:\n    return len(a)\n    \"\"\"\n    contract = get_contract(source)\n\n    value = \"a\" * length\n\n    vyper_ast = vy_ast.parse_to_ast(f\"len('{value}')\")\n    old_node = vyper_ast.body[0].value\n    new_node = vy_fn.Len().evaluate(old_node)\n\n    assert contract.foo(value) == new_node.value", "fn_id": 0, "class_fn": false, "repo": "erdnaag/vyper", "file": "tests/functions/folding/test_len.py", "last_update_at": "2020-07-04T01:47:26+00:00", "question_id": "1c663fe675187ce878cfcf385a633a6b7fa0cfec_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('length', [0, 1, 32, 33, 64, 65, 1024])\ndef test_len_string(get_contract, length):\n    source = '\\n@public\\ndef foo(a: string[1024]) -> uint256:\\n    return len(a)\\n    '\n    contract = get_contract(source)\n    value = 'a' * length\n    vyper_ast = vy_ast.parse_to_ast(f\"len('{value}')\")\n    old_node = vyper_ast.body[0].value\n    new_node = vy_fn.Len().evaluate(old_node)\n"]]}
{"hexsha": "b3ad4a325b4fd5bbe6f377742f9f8762fe907760", "ext": "py", "lang": "Python", "content": "def _create_embedded_row(value, out_row, out_col_ndx, config, group_values):\n    \"\"\"\n    Copies the given output row and assigns the given pattern match\n    group values to the corresponding output row columns defined by\n    the configuration.\n\n    :return: the new output row\n    \"\"\"\n    # Make a new output row.\n    row = out_row.copy()\n    # Set the embedded column.\n    row[out_col_ndx] = value\n    # Assign the match group values to the associated output column.\n    for grp_col, grp_value in group_values.items():\n        # Map the output columns to output values.\n        value_dict = config.values_dict.get(grp_col)\n        value = value_dict.get(grp_value, grp_value) if value_dict else grp_value\n        col_ndx = config.out_col_ndx_map[grp_col]\n        row[col_ndx] = value\n\n    return row", "fn_id": 20, "class_fn": false, "repo": "biodev/imppload", "file": "immpload/munger.py", "last_update_at": "2020-03-27T03:23:06+00:00", "question_id": "b3ad4a325b4fd5bbe6f377742f9f8762fe907760_20", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _create_embedded_row(value, out_row, out_col_ndx, config, group_values):\n    \"\"\"\n    Copies the given output row and assigns the given pattern match\n    group values to the corresponding output row columns defined by\n    the configuration.\n\n    :return: the new output row\n    \"\"\"\n    row = out_row.copy()\n    row[out_col_ndx] = value\n    for grp_col, grp_value in group_values.items():\n        value_dict = config.values_dict.get(grp_col)\n        value = value_dict.get(grp_value, grp_value) if value_dict else grp_value\n        col_ndx = config.out_col_ndx_map[grp_col]\n        row[col_ndx] = value\n"]]}
{"hexsha": "33c7add7e4d859c7c7182c4d4fd999748c1ef12c", "ext": "py", "lang": "Python", "content": "def clearWebhooks():\n  \"\"\"Will clear all webhooks associated with the current token.\"\"\"\n\n  api = WebexTeamsAPI(access_token=teams_token)\n  webhooks = api.webhooks.list()\n\n  # no need to iterate through all the webhooks if the list is empty\n  if len(list(webhooks)) == 0:\n    print('no webhooks registered')\n    return\n\n  print('clearing webhooks...')\n\n  # iterate through all webhooks and delete them\n  for webhook in webhooks:\n    print('deleting webhook \"%(webhookName)s\"...' % {'webhookName' : webhook.name})\n    api.webhooks.delete(webhook.id)\n\n  print('done')", "fn_id": 0, "class_fn": false, "repo": "cgeyer/devnet-express-webex-vienna", "file": "task-02/webhooks.py", "last_update_at": "2020-01-17T10:51:11+00:00", "question_id": "33c7add7e4d859c7c7182c4d4fd999748c1ef12c_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def clearWebhooks():\n    \"\"\"Will clear all webhooks associated with the current token.\"\"\"\n    api = WebexTeamsAPI(access_token=teams_token)\n    webhooks = api.webhooks.list()\n    if len(list(webhooks)) == 0:\n        print('no webhooks registered')\n        return\n    print('clearing webhooks...')\n    for webhook in webhooks:\n        print('deleting webhook \"%(webhookName)s\"...' % {'webhookName': webhook.name})\n        api.webhooks.delete(webhook.id)\n"]]}
{"hexsha": "143202c13cf0a8902db0774b74811a68d447cd4b", "ext": "py", "lang": "Python", "content": "def GTF(gtf):\n\td_name = {}\n\td_id = {}\n\n\tsList = []\n\twith open(gtf) as f:\n\t\tfor line in f:\n\t\t\tif not line.startswith('#'):\n\t\t\t\t(chrid, source, genetype, start, end, score, strand, phase, attribute_string) = line.rstrip().split('\\t')\n\t\t\t\tattributes = {}\n\t\n\t\t\t\tif genetype=='gene':\n\t\t\t\t\tkey_value_pair_set = attribute_string.split('; ')\n\t\t\t\t\tfor key_value_pair in key_value_pair_set[: -1]:\n\t\t\t\t\t    key, value = key_value_pair.split(' ')\n\t\t\t\t\t    attributes[key] = value[1: -1] ### remove the first str and last str\n\t\t\t\t\t### specially for last field\n\t\t\t\t\tkey, value = key_value_pair_set[-1][: -1].split(' ')\n\t\t\t\t\tattributes[key] = value[1: -1]\n\t\t\t\t\t\n\t\t\t\t\tgene_id = attributes.get('gene_id')\n\t\t\t\t\tgene_name = attributes.get('gene_name')\n\t\t\t\t\tgene_type = attributes.get('gene_biotype')\n\t\n\t\t\t\t\t#print(gene_id, gene_name, gene_type)\n\t\t\t\t\td_name[gene_name] = gene_type\n\t\t\t\t\td_id[gene_id] = gene_type\n\n\t\t\t\t\tsList.append([gene_id, gene_name, gene_type])\n\t'''\n\tdf_name =pd.Series(d_name)\n\tdf_name_csv = \"gene_name.type.txt\"\n\tdf_name.to_csv(df_name_csv, header=False, sep='\\t')\n\n\tdf_id=pd.Series(d_id)\n\tdf_id_csv = \"gene_id.type.txt\"\n\tdf_id.to_csv(df_id_csv, header=False, sep='\\t')\n\t'''\n\tdf = pd.DataFrame(sList)\n\tdf_csv = \"gene_type.txt\"\n\tdf.to_csv(df_csv, header=False, index=True, sep='\\t')", "fn_id": 1, "class_fn": false, "repo": "liuwell/rapvis", "file": "rapvis/rapvis_build.py", "last_update_at": "2020-10-25T10:23:45+00:00", "question_id": "143202c13cf0a8902db0774b74811a68d447cd4b_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def GTF(gtf):\n    d_name = {}\n    d_id = {}\n    sList = []\n    with open(gtf) as f:\n        for line in f:\n            if not line.startswith('#'):\n                chrid, source, genetype, start, end, score, strand, phase, attribute_string = line.rstrip().split('\\t')\n                attributes = {}\n                if genetype == 'gene':\n                    key_value_pair_set = attribute_string.split('; ')\n                    for key_value_pair in key_value_pair_set[:-1]:\n                        key, value = key_value_pair.split(' ')\n                        attributes[key] = value[1:-1]\n                    key, value = key_value_pair_set[-1][:-1].split(' ')\n                    attributes[key] = value[1:-1]\n                    gene_id = attributes.get('gene_id')\n                    gene_name = attributes.get('gene_name')\n                    gene_type = attributes.get('gene_biotype')\n                    d_name[gene_name] = gene_type\n                    d_id[gene_id] = gene_type\n                    sList.append([gene_id, gene_name, gene_type])\n    '\\n\\tdf_name =pd.Series(d_name)\\n\\tdf_name_csv = \"gene_name.type.txt\"\\n\\tdf_name.to_csv(df_name_csv, header=False, sep=\\'\\t\\')\\n\\n\\tdf_id=pd.Series(d_id)\\n\\tdf_id_csv = \"gene_id.type.txt\"\\n\\tdf_id.to_csv(df_id_csv, header=False, sep=\\'\\t\\')\\n\\t'\n    df = pd.DataFrame(sList)\n    df_csv = 'gene_type.txt'\n"]]}
{"hexsha": "d3f2ff996811b092cea80924609eb5edc78d47dd", "ext": "py", "lang": "Python", "content": "def check_interactive_message_manager(payload):\n    user_id = payload[\"original_message\"][\"attachments\"][0][\"callback_id\"]\n    user_info = get_user_info(user_id)\n    user_profile = user_info[\"profile\"]\n    tool = payload[\"actions\"][0][\"value\"]\n    \n    if payload[\"actions\"][0][\"name\"] == \"approve\":\n        #tool_access = check_record(user_profile[\"email\"], \"jira\", \"tool_status\")\n        #tool_access = check_record(\"test2@gmail.com\", tool, \"tool_status\")\n        tool_access = \"no\"\n        \n        if tool_access == \"no\":\n            request_admin(payload)\n            message = \"Request forwarded to <@\" + \"U015QP5QHN0>\" + \" for `\" + tool + \"` access request by <@\" + user_id + \">\"\n        else:\n            message = \"`\" + tool + \"` access already given to <@\" + user_id + \">\"\n            \n        return response_message(message)\n    else:\n        disapproval_message(payload[\"callback_id\"], \"Manager\", tool)\n        message = \"`\" + tool + \"` access request by <@\" + user_id + \"> disapproved\"\n        return response_message(message)", "fn_id": 2, "class_fn": false, "repo": "sbansiwal/automated-acces-control", "file": "check_message.py", "last_update_at": "2020-07-17T12:09:34+00:00", "question_id": "d3f2ff996811b092cea80924609eb5edc78d47dd_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def check_interactive_message_manager(payload):\n    user_id = payload['original_message']['attachments'][0]['callback_id']\n    user_info = get_user_info(user_id)\n    user_profile = user_info['profile']\n    tool = payload['actions'][0]['value']\n    if payload['actions'][0]['name'] == 'approve':\n        tool_access = 'no'\n        if tool_access == 'no':\n            request_admin(payload)\n            message = 'Request forwarded to <@' + 'U015QP5QHN0>' + ' for `' + tool + '` access request by <@' + user_id + '>'\n        else:\n            message = '`' + tool + '` access already given to <@' + user_id + '>'\n        return response_message(message)\n    else:\n        disapproval_message(payload['callback_id'], 'Manager', tool)\n        message = '`' + tool + '` access request by <@' + user_id + '> disapproved'\n"]]}
{"hexsha": "4d2b7ad55e028493d7488e8c5e88fee0b41d58f3", "ext": "py", "lang": "Python", "content": "def print_arr(arr):\n    \"\"\"\n    Function to get the string version of an array in one line.\n    \"\"\"\n    return \"\\n\".join(arr)", "fn_id": 11, "class_fn": false, "repo": "NUS-Class-Bot/student01S", "file": "main.py", "last_update_at": "2020-05-05T00:44:00+00:00", "question_id": "4d2b7ad55e028493d7488e8c5e88fee0b41d58f3_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def print_arr(arr):\n    \"\"\"\n    Function to get the string version of an array in one line.\n    \"\"\"\n"]]}
{"hexsha": "b0ae4cac8bede0601e22c62d2c062444e520731d", "ext": "py", "lang": "Python", "content": "def colony_extractor():\n    result=re.findall(r'\\w+',rep)\n    a=[]\n    for i in range(len(result)):\n        a.append(result[i])\n    for i in range(len(result)):\n        b = result[i]\n        for j in range(len(result)):\n            if i+j+1<len(result):\n                b=b+' '+result[i+j+1]\n                a.append(b)\n    for x in range(len(a)):\n        if a[x].lower() in colonies:\n            return a[x]", "fn_id": 1, "class_fn": false, "repo": "eshandinesh/gis_based_crime_mapping", "file": "mapping/gis/PARSER/PARSER.py", "last_update_at": "2020-11-16T17:12:33+00:00", "question_id": "b0ae4cac8bede0601e22c62d2c062444e520731d_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def colony_extractor():\n    result = re.findall('\\\\w+', rep)\n    a = []\n    for i in range(len(result)):\n        a.append(result[i])\n    for i in range(len(result)):\n        b = result[i]\n        for j in range(len(result)):\n            if i + j + 1 < len(result):\n                b = b + ' ' + result[i + j + 1]\n                a.append(b)\n    for x in range(len(a)):\n        if a[x].lower() in colonies:\n"]]}
{"hexsha": "e0907fabe3ceff995bc4eb0c5c4499e4c71d9301", "ext": "py", "lang": "Python", "content": "def isdigit(c: str) -> bool:\n    try:\n        c = int(c)\n    except:\n        return False\n    return True", "fn_id": 1, "class_fn": false, "repo": "Pzzzzz5142/animal-forest-QQ-group-bot", "file": "utils.py", "last_update_at": "2020-09-30T12:14:46+00:00", "question_id": "e0907fabe3ceff995bc4eb0c5c4499e4c71d9301_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def isdigit(c: str) -> bool:\n    try:\n        c = int(c)\n    except:\n        return False\n"]]}
{"hexsha": "cb9ecbe044f947ca511e578919296e63703a76e7", "ext": "py", "lang": "Python", "content": "def concat(table1, table2):\n    table1 = database[table1]\n    table2 = database[table2]\n    start_time = time.time()\n    header1 = [i for i in table1[0]]\n    header2 = [i for i in table2[0]]\n    if set(header1) != set(header2):\n        print(\"Error: two tables don't share the same schema!\")\n        return \n    end_time = time.time()\n    print(\"Time elapsed for concatenation is \" + str(end_time - start_time) + \" sec.\")\n    print(\"lines: \" + str(len(table1 + table2[1:])))\n    return table1 + table2[1:]", "fn_id": 16, "class_fn": false, "repo": "nyutian/Miniature-Database-System", "file": "util.py", "last_update_at": "2020-12-07T10:51:21+00:00", "question_id": "cb9ecbe044f947ca511e578919296e63703a76e7_16", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def concat(table1, table2):\n    table1 = database[table1]\n    table2 = database[table2]\n    start_time = time.time()\n    header1 = [i for i in table1[0]]\n    header2 = [i for i in table2[0]]\n    if set(header1) != set(header2):\n        print(\"Error: two tables don't share the same schema!\")\n        return\n    end_time = time.time()\n    print('Time elapsed for concatenation is ' + str(end_time - start_time) + ' sec.')\n    print('lines: ' + str(len(table1 + table2[1:])))\n"]]}
{"hexsha": "6a30ab056fd3fb09f9cb647a13e0d54d034c13fc", "ext": "py", "lang": "Python", "content": "def parse_args():\n    parser = optparse.OptionParser()\n    parser.disable_interspersed_args()\n    parser.add_option('--sources-list')\n    parser.add_option('--verbose', default=False, action='store_true')\n    parser.add_option('--remove-notes', default=False, action='store_true')\n    parser.add_option('--ignore-errors', default=False, action='store_true')\n    return parser.parse_args()", "fn_id": 0, "class_fn": false, "repo": "r1nadeg/04_catboost", "file": "build/scripts/run_javac.py", "last_update_at": "2020-05-21T18:03:08+00:00", "question_id": "6a30ab056fd3fb09f9cb647a13e0d54d034c13fc_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def parse_args():\n    parser = optparse.OptionParser()\n    parser.disable_interspersed_args()\n    parser.add_option('--sources-list')\n    parser.add_option('--verbose', default=False, action='store_true')\n    parser.add_option('--remove-notes', default=False, action='store_true')\n    parser.add_option('--ignore-errors', default=False, action='store_true')\n"]]}
{"hexsha": "7382993073aecd88775af1e61bde0dac69bec96f", "ext": "py", "lang": "Python", "content": "def assert_event(event_obj, expected_event_name, expected_event_args=None, expected_contract_address=None):\n    contract_address, event = event_obj\n\n    if expected_event_args is None:\n        expected_event_args = {}\n\n    if expected_contract_address:\n        assert contract_address == expected_contract_address\n\n    assert event['event'] == expected_event_name\n    assert expected_event_args.items() <= event['args'].items()", "fn_id": 1, "class_fn": false, "repo": "EgoInc/plasma-contracts", "file": "plasma_framework/python_tests/tests/tests_utils/assertions.py", "last_update_at": "2020-05-24T09:19:35+00:00", "question_id": "7382993073aecd88775af1e61bde0dac69bec96f_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def assert_event(event_obj, expected_event_name, expected_event_args=None, expected_contract_address=None):\n    contract_address, event = event_obj\n    if expected_event_args is None:\n        expected_event_args = {}\n    if expected_contract_address:\n        assert contract_address == expected_contract_address\n    assert event['event'] == expected_event_name\n"]]}
{"hexsha": "b889baff60b57f45907cc68676d595dcd8e35eed", "ext": "py", "lang": "Python", "content": "def create(file_name, file_path='./'):\n    \"\"\"\n    \u65b0\u5efa\u6587\u4ef6\n    :param file_name: \u65b0\u6587\u4ef6\u540d\u79f0(\u542b\u540e\u7f00)\n    :param file_path: \u65b0\u6587\u4ef6\u8def\u5f84\n    :return: None\n    \"\"\"\n    f = open(os.path.join(file_path, file_name), 'w+')\n    f.close()", "fn_id": 0, "class_fn": false, "repo": "Super-Breatook/conffey", "file": "build/lib/conffey/Document.py", "last_update_at": "2020-08-09T03:33:14+00:00", "question_id": "b889baff60b57f45907cc68676d595dcd8e35eed_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def create(file_name, file_path='./'):\n    \"\"\"\n    \u65b0\u5efa\u6587\u4ef6\n    :param file_name: \u65b0\u6587\u4ef6\u540d\u79f0(\u542b\u540e\u7f00)\n    :param file_path: \u65b0\u6587\u4ef6\u8def\u5f84\n    :return: None\n    \"\"\"\n    f = open(os.path.join(file_path, file_name), 'w+')\n"]]}
{"hexsha": "15ae329b1bc41d195af092a2b11437a91c641074", "ext": "py", "lang": "Python", "content": "def isString(hou_attrib):\n    \"\"\"Convenience filter function for string attributes.\n    \"\"\"\n    assert type(hou_attrib) is hou.Attrib, \"invalid argument\"\n    return \\\n        hou_attrib.dataType()==hou.attribData.String and \\\n        hou_attrib.size()==1", "fn_id": 5, "class_fn": false, "repo": "JosephSilvermanArt/qLib", "file": "scripts/python/qlibattribmenu.py", "last_update_at": "2020-11-06T06:15:07+00:00", "question_id": "15ae329b1bc41d195af092a2b11437a91c641074_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def isString(hou_attrib):\n    \"\"\"Convenience filter function for string attributes.\n    \"\"\"\n    assert type(hou_attrib) is hou.Attrib, 'invalid argument'\n"]]}
{"hexsha": "bdf457b9398ef057d5ab837ea9ea0040c5e528d6", "ext": "py", "lang": "Python", "content": "def get_value(character, value: str) -> int:\n    try:\n        return int(value)\n    except ValueError:\n        pass\n\n    try:\n        return getattr(character, value)()\n    except Exception:\n        logger.warning(f'Could not parse value {value}')\n        return 0", "fn_id": 1, "class_fn": false, "repo": "Tenebrar/codebase", "file": "pathfinder/charsheet/bonuses.py", "last_update_at": "2020-04-21T11:39:25+00:00", "question_id": "bdf457b9398ef057d5ab837ea9ea0040c5e528d6_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_value(character, value: str) -> int:\n    try:\n        return int(value)\n    except ValueError:\n        pass\n    try:\n        return getattr(character, value)()\n    except Exception:\n        logger.warning(f'Could not parse value {value}')\n"]]}
{"hexsha": "e3cea6ee2cccf733622fb86e322c3efc46d63f5c", "ext": "py", "lang": "Python", "content": "def check_helm_install(kube_config, kube_context):\n    cmd_helm_installed = [\"helm\", \"--kubeconfig\", kube_config, \"--debug\"]\n    if kube_context:\n        cmd_helm_installed.extend([\"--kube-context\", kube_context])\n    try:\n        response_helm_installed = Popen(cmd_helm_installed, stdout=PIPE, stderr=PIPE)\n        _, error_helm_installed = response_helm_installed.communicate()\n        if response_helm_installed.returncode != 0:\n            if \"unknown flag\" in error_helm_installed.decode(\"ascii\"):\n                telemetry.set_user_fault()\n                telemetry.set_exception(exception='Helm 3 not found', fault_type=Helm_Version_Fault_Type,\n                                        summary='Helm3 not found on the machine')\n                raise CLIError(\"Please install the latest version of Helm. \" +\n                               \"Learn more at https://aka.ms/arc/k8s/onboarding-helm-install\")\n            telemetry.set_user_fault()\n            telemetry.set_exception(exception=error_helm_installed.decode(\"ascii\"), fault_type=Helm_Installation_Fault_Type,\n                                    summary='Helm3 not installed on the machine')\n            raise CLIError(error_helm_installed.decode(\"ascii\"))\n    except FileNotFoundError as e:\n        telemetry.set_exception(exception=e, fault_type=Check_HelmInstallation_Fault_Type,\n                                summary='Unable to verify helm installation')\n        raise CLIError(\"Helm is not installed or requires elevated permissions. \" +\n                       \"Ensure that you have the latest version of Helm installed on your machine. \" +\n                       \"Learn more at https://aka.ms/arc/k8s/onboarding-helm-install\")\n    except subprocess.CalledProcessError as e2:\n        e2.output = e2.output.decode(\"ascii\")\n        print(e2.output)", "fn_id": 5, "class_fn": false, "repo": "tilnl/azure-cli-extensions", "file": "src/connectedk8s/azext_connectedk8s/custom.py", "last_update_at": "2020-07-28T18:01:53+00:00", "question_id": "e3cea6ee2cccf733622fb86e322c3efc46d63f5c_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def check_helm_install(kube_config, kube_context):\n    cmd_helm_installed = ['helm', '--kubeconfig', kube_config, '--debug']\n    if kube_context:\n        cmd_helm_installed.extend(['--kube-context', kube_context])\n    try:\n        response_helm_installed = Popen(cmd_helm_installed, stdout=PIPE, stderr=PIPE)\n        _, error_helm_installed = response_helm_installed.communicate()\n        if response_helm_installed.returncode != 0:\n            if 'unknown flag' in error_helm_installed.decode('ascii'):\n                telemetry.set_user_fault()\n                telemetry.set_exception(exception='Helm 3 not found', fault_type=Helm_Version_Fault_Type, summary='Helm3 not found on the machine')\n                raise CLIError('Please install the latest version of Helm. ' + 'Learn more at https://aka.ms/arc/k8s/onboarding-helm-install')\n            telemetry.set_user_fault()\n            telemetry.set_exception(exception=error_helm_installed.decode('ascii'), fault_type=Helm_Installation_Fault_Type, summary='Helm3 not installed on the machine')\n            raise CLIError(error_helm_installed.decode('ascii'))\n    except FileNotFoundError as e:\n        telemetry.set_exception(exception=e, fault_type=Check_HelmInstallation_Fault_Type, summary='Unable to verify helm installation')\n        raise CLIError('Helm is not installed or requires elevated permissions. ' + 'Ensure that you have the latest version of Helm installed on your machine. ' + 'Learn more at https://aka.ms/arc/k8s/onboarding-helm-install')\n    except subprocess.CalledProcessError as e2:\n        e2.output = e2.output.decode('ascii')\n"]]}
{"hexsha": "967124f087e34dac7da54aae5381bd300b7fee07", "ext": "py", "lang": "Python", "content": "def bowling_score(frames):\n    score=frames.split(\" \")\n    total=0\n    for i in range(len(score)-1):\n        if calculate(score[i])==10:\n            if \"X\" in score[i]:\n                if len(score[i+1])==1:\n                    total+=10+calculate(score[i+1][0])+calculate(score[i+2][0])\n                else:\n                    total+=10+calculate(score[i+1][:2])\n            else:\n                total+=10+calculate(score[i+1][0])\n        else:\n            total+=calculate(score[i])\n    if len(score[-1])==2:\n        return total+calculate(score[-1])\n    return total+calculate(score[-1][:2])+calculate(score[-1][2:])", "fn_id": 0, "class_fn": false, "repo": "mwk0408/codewars_solutions", "file": "4 kyu/TenPin Bowling.py", "last_update_at": "2020-12-07T04:10:01+00:00", "question_id": "967124f087e34dac7da54aae5381bd300b7fee07_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def bowling_score(frames):\n    score = frames.split(' ')\n    total = 0\n    for i in range(len(score) - 1):\n        if calculate(score[i]) == 10:\n            if 'X' in score[i]:\n                if len(score[i + 1]) == 1:\n                    total += 10 + calculate(score[i + 1][0]) + calculate(score[i + 2][0])\n                else:\n                    total += 10 + calculate(score[i + 1][:2])\n            else:\n                total += 10 + calculate(score[i + 1][0])\n        else:\n            total += calculate(score[i])\n    if len(score[-1]) == 2:\n        return total + calculate(score[-1])\n"]]}
{"hexsha": "113d12a47d2ffbce2673b6b05a7a1596f6ef4aaa", "ext": "py", "lang": "Python", "content": "def print_list_to_user():\n        with open('countries-json/country-by-abbreviation.json') as json_file:\n            number = 0\n            string = \"\"\n            for line in yaml.safe_load(json_file):\n                string += \"{}. {}:{}\".format(number, line['COUNTRY'],\\\n                              line['ABBREVIATION'] + '\\n')\n                number += 1\n        number = 0\n        pager(string)", "fn_id": 5, "class_fn": false, "repo": "emanuel2718/Covid-19-Scrapper", "file": "covid19.py", "last_update_at": "2020-03-25T03:20:37+00:00", "question_id": "113d12a47d2ffbce2673b6b05a7a1596f6ef4aaa_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def print_list_to_user():\n    with open('countries-json/country-by-abbreviation.json') as json_file:\n        number = 0\n        string = ''\n        for line in yaml.safe_load(json_file):\n            string += '{}. {}:{}'.format(number, line['COUNTRY'], line['ABBREVIATION'] + '\\n')\n            number += 1\n    number = 0\n"]]}
{"hexsha": "6fcf9f8703a9d44037b79b8e07343692d017ecf6", "ext": "py", "lang": "Python", "content": "def mult_dict_service(parameters):\n\n##########  Description  #######\n    '''\n    '''\n#############  BODY ############\n\n    mult = []\n    # May be some logic based on par1, par2, ... value\n    mult.append({})\n    mult[0]['eq_addr'] = 'example_device1'\n    mult[0]['eq_parameter'] = 'some_parameter'\n    mult[0]['cmd'] = {}\n    mult[0]['cmd']['ad'] = []\n    mult[0]['cmd']['rm'] = []\n    mult[0]['cmd']['ad'].append('extemplates_1.create_service')\n    mult[0]['cmd']['rm'].append('extemplates_1.delete_service')\n    mult.append({})\n    mult[1]['eq_addr'] = 'example_device2'\n    mult[1]['eq_parameter'] = 'some_parameter'\n    mult[1]['cmd'] = {}\n    mult[1]['cmd']['ad'] = []\n    mult[1]['cmd']['rm'] = []\n    mult[1]['cmd']['ad'].append('extemplates_2.create_service')\n    mult[1]['cmd']['rm'].append('extemplates_2.delete_service')\n\n    return (mult)", "fn_id": 2, "class_fn": false, "repo": "nihole/PSEFABRIC", "file": "PROJECTS/p000/PSEF_SCRIPTS/psef_logic.py", "last_update_at": "2020-03-12T07:19:11+00:00", "question_id": "6fcf9f8703a9d44037b79b8e07343692d017ecf6_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def mult_dict_service(parameters):\n    \"\"\"\n    \"\"\"\n    mult = []\n    mult.append({})\n    mult[0]['eq_addr'] = 'example_device1'\n    mult[0]['eq_parameter'] = 'some_parameter'\n    mult[0]['cmd'] = {}\n    mult[0]['cmd']['ad'] = []\n    mult[0]['cmd']['rm'] = []\n    mult[0]['cmd']['ad'].append('extemplates_1.create_service')\n    mult[0]['cmd']['rm'].append('extemplates_1.delete_service')\n    mult.append({})\n    mult[1]['eq_addr'] = 'example_device2'\n    mult[1]['eq_parameter'] = 'some_parameter'\n    mult[1]['cmd'] = {}\n    mult[1]['cmd']['ad'] = []\n    mult[1]['cmd']['rm'] = []\n    mult[1]['cmd']['ad'].append('extemplates_2.create_service')\n    mult[1]['cmd']['rm'].append('extemplates_2.delete_service')\n"]]}
{"hexsha": "b268a992fd9a0747d77e55dac4560f07ec08c871", "ext": "py", "lang": "Python", "content": "def test_rank(candidates):\n    model_name = \"LR_15_components\"\n    result = rank(candidates, model_name)\n    assert isinstance(result, list)", "fn_id": 0, "class_fn": false, "repo": "pombredanne/vulnerability-assessment-kb", "file": "prospector/ranking/rank_test.py", "last_update_at": "2020-06-04T13:39:52+00:00", "question_id": "b268a992fd9a0747d77e55dac4560f07ec08c871_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_rank(candidates):\n    model_name = 'LR_15_components'\n    result = rank(candidates, model_name)\n"]]}
{"hexsha": "42e5ffc678a9c27afc50d25cb70baaefa7c72dde", "ext": "py", "lang": "Python", "content": "def plot_median_scale_diagram(results_dict, res_median, algo):\n    fig_name = algo + '-time-median.png'\n          \n    D = collections.OrderedDict(sorted((res_median.items())))\n    log_input = [20, 21, 22, 23, 24, 25]\n   \n    plt.xticks(range(len(D)), log_input)\n    plt.xlabel(\"log[input size]\", fontsize=15)\n    plt.ylabel(\"Execution of time (s)\", fontsize=15)\n    plt.title(\"Weak Scaling PLP\", fontsize=15)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.bar(range(len(D)), list(D.values()), color='darkred', zorder=2)   \n    y_hor = range(10, 50, 10)\n    for y in y_hor:\n        plt.axhline(y=y, linewidth=0.8, color='black', linestyle='--', zorder=1)\n    plt.savefig(fig_name)", "fn_id": 2, "class_fn": false, "repo": "dlekkas/comm_detect", "file": "results/plot_weak_scaling.py", "last_update_at": "2020-11-30T06:24:43+00:00", "question_id": "42e5ffc678a9c27afc50d25cb70baaefa7c72dde_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def plot_median_scale_diagram(results_dict, res_median, algo):\n    fig_name = algo + '-time-median.png'\n    D = collections.OrderedDict(sorted(res_median.items()))\n    log_input = [20, 21, 22, 23, 24, 25]\n    plt.xticks(range(len(D)), log_input)\n    plt.xlabel('log[input size]', fontsize=15)\n    plt.ylabel('Execution of time (s)', fontsize=15)\n    plt.title('Weak Scaling PLP', fontsize=15)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.bar(range(len(D)), list(D.values()), color='darkred', zorder=2)\n    y_hor = range(10, 50, 10)\n    for y in y_hor:\n        plt.axhline(y=y, linewidth=0.8, color='black', linestyle='--', zorder=1)\n"]]}
{"hexsha": "8a6eb3b68b5a246a778357aed199eaeef8602436", "ext": "py", "lang": "Python", "content": "def downloadAllVideos(data_dir, video_dict):\n    \"\"\"\n    download the video\n    :param video_dict: all video ids saved in the dict\n    :return:\n    \"\"\"\n\n    # use multiprocessing pool\n    pool = multiprocessing.Pool(4)\n    for key in video_dict:\n        # Extract the words consisting of video_id, start_time, end_time, list of video_tags\n        for v_id in video_dict[key]:\n            pool.apply_async(download_vid, (data_dir, key, v_id))\n    pool.close()\n    pool.join()", "fn_id": 1, "class_fn": false, "repo": "Lihit/Sound-of-Pixels", "file": "data/download_video_audio.py", "last_update_at": "2020-07-04T10:37:04+00:00", "question_id": "8a6eb3b68b5a246a778357aed199eaeef8602436_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def downloadAllVideos(data_dir, video_dict):\n    \"\"\"\n    download the video\n    :param video_dict: all video ids saved in the dict\n    :return:\n    \"\"\"\n    pool = multiprocessing.Pool(4)\n    for key in video_dict:\n        for v_id in video_dict[key]:\n            pool.apply_async(download_vid, (data_dir, key, v_id))\n    pool.close()\n"]]}
{"hexsha": "5f3c10937424ed8391eebd000884277a2595ae8c", "ext": "py", "lang": "Python", "content": "def download(url, filename):\n\n    if not os.path.exists(mnist_path):\n        os.makedirs(mnist_path)\n    out_file = os.path.join(mnist_path, filename)\n    if not os.path.isfile(out_file):\n        urlretrieve(url, out_file)", "fn_id": 0, "class_fn": false, "repo": "cassianobecker/dnn", "file": "dataset/mnist/data.py", "last_update_at": "2020-09-29T15:20:00+00:00", "question_id": "5f3c10937424ed8391eebd000884277a2595ae8c_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def download(url, filename):\n    if not os.path.exists(mnist_path):\n        os.makedirs(mnist_path)\n    out_file = os.path.join(mnist_path, filename)\n    if not os.path.isfile(out_file):\n"]]}
{"hexsha": "c8939b6523ddc3e7a9fd8625c26176a2ef5dd22a", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize(\n    [\"inlets\", \"outlets\"],\n    [\n        pytest.param(\n            # Airflow 1.10.x uses a dictionary structure for inlets and outlets.\n            # We want the lineage backend to support this structure for backwards\n            # compatability reasons, so this test is not conditional.\n            {\"datasets\": [Dataset(\"snowflake\", \"mydb.schema.tableConsumed\")]},\n            {\"datasets\": [Dataset(\"snowflake\", \"mydb.schema.tableProduced\")]},\n            id=\"airflow-1-10-lineage-syntax\",\n        ),\n        pytest.param(\n            # Airflow 2.x also supports a flattened list for inlets and outlets.\n            # We want to test this capability.\n            [Dataset(\"snowflake\", \"mydb.schema.tableConsumed\")],\n            [Dataset(\"snowflake\", \"mydb.schema.tableProduced\")],\n            marks=pytest.mark.skipif(\n                AIRFLOW_VERSION < packaging.version.parse(\"2.0.0\"),\n                reason=\"list-style lineage is only supported in Airflow 2.x\",\n            ),\n            id=\"airflow-2-lineage-syntax\",\n        ),\n    ],\n)\n@mock.patch(\"datahub_provider.hooks.datahub.DatahubRestHook.make_emitter\")\ndef test_lineage_backend(mock_emit, inlets, outlets):\n    DEFAULT_DATE = days_ago(2)\n    mock_emitter = Mock()\n    mock_emit.return_value = mock_emitter\n    # Using autospec on xcom_pull and xcom_push methods fails on Python 3.6.\n    with mock.patch.dict(\n        os.environ,\n        {\n            \"AIRFLOW__LINEAGE__BACKEND\": \"datahub_provider.lineage.datahub.DatahubLineageBackend\",\n            \"AIRFLOW__LINEAGE__DATAHUB_CONN_ID\": datahub_rest_connection_config.conn_id,\n            \"AIRFLOW__LINEAGE__DATAHUB_KWARGS\": json.dumps(\n                {\"graceful_exceptions\": False, \"capture_executions\": False}\n            ),\n        },\n    ), mock.patch(\"airflow.models.BaseOperator.xcom_pull\"), mock.patch(\n        \"airflow.models.BaseOperator.xcom_push\"\n    ), patch_airflow_connection(\n        datahub_rest_connection_config\n    ):\n        func = mock.Mock()\n        func.__name__ = \"foo\"\n\n        dag = DAG(dag_id=\"test_lineage_is_sent_to_backend\", start_date=DEFAULT_DATE)\n\n        with dag:\n            op1 = DummyOperator(\n                task_id=\"task1_upstream\",\n                inlets=inlets,\n                outlets=outlets,\n            )\n            op2 = DummyOperator(\n                task_id=\"task2\",\n                inlets=inlets,\n                outlets=outlets,\n            )\n            op1 >> op2\n\n        # Airflow < 2.2 requires the execution_date parameter. Newer Airflow\n        # versions do not require it, but will attempt to find the associated\n        # run_id in the database if execution_date is provided. As such, we\n        # must fake the run_id parameter for newer Airflow versions.\n        if AIRFLOW_VERSION < packaging.version.parse(\"2.2.0\"):\n            ti = TaskInstance(task=op2, execution_date=DEFAULT_DATE)\n        else:\n            ti = TaskInstance(task=op2, run_id=f\"test_airflow-{DEFAULT_DATE}\")\n        ctx1 = {\n            \"dag\": dag,\n            \"task\": op2,\n            \"ti\": ti,\n            \"task_instance\": ti,\n            \"execution_date\": DEFAULT_DATE,\n            \"ts\": \"2021-04-08T00:54:25.771575+00:00\",\n        }\n\n        prep = prepare_lineage(func)\n        prep(op2, ctx1)\n        post = apply_lineage(func)\n        post(op2, ctx1)\n\n        # Verify that the inlets and outlets are registered and recognized by Airflow correctly,\n        # or that our lineage backend forces it to.\n        assert len(op2.inlets) == 1\n        assert len(op2.outlets) == 1\n        assert all(map(lambda let: isinstance(let, Dataset), op2.inlets))\n        assert all(map(lambda let: isinstance(let, Dataset), op2.outlets))\n\n        # Check that the right things were emitted.\n        assert mock_emitter.emit.call_count == 9\n        # Running further checks based on python version because args only exists in python 3.7+\n        if sys.version_info[:3] > (3, 7):\n            assert mock_emitter.method_calls[0].args[0].aspectName == \"dataFlowInfo\"\n            assert (\n                mock_emitter.method_calls[0].args[0].entityUrn\n                == \"urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod)\"\n            )\n\n            assert mock_emitter.method_calls[1].args[0].aspectName == \"ownership\"\n            assert (\n                mock_emitter.method_calls[1].args[0].entityUrn\n                == \"urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod)\"\n            )\n\n            assert mock_emitter.method_calls[2].args[0].aspectName == \"globalTags\"\n            assert (\n                mock_emitter.method_calls[2].args[0].entityUrn\n                == \"urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod)\"\n            )\n\n            assert mock_emitter.method_calls[3].args[0].aspectName == \"dataJobInfo\"\n            assert (\n                mock_emitter.method_calls[3].args[0].entityUrn\n                == \"urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task2)\"\n            )\n\n            assert (\n                mock_emitter.method_calls[4].args[0].aspectName == \"dataJobInputOutput\"\n            )\n            assert (\n                mock_emitter.method_calls[4].args[0].entityUrn\n                == \"urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task2)\"\n            )\n            assert (\n                mock_emitter.method_calls[4].args[0].aspect.inputDatajobs[0]\n                == \"urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task1_upstream)\"\n            )\n            assert (\n                mock_emitter.method_calls[4].args[0].aspect.inputDatasets[0]\n                == \"urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableConsumed,PROD)\"\n            )\n            assert (\n                mock_emitter.method_calls[4].args[0].aspect.outputDatasets[0]\n                == \"urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableProduced,PROD)\"\n            )\n\n            assert mock_emitter.method_calls[5].args[0].aspectName == \"status\"\n            assert (\n                mock_emitter.method_calls[5].args[0].entityUrn\n                == \"urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableConsumed,PROD)\"\n            )\n\n            assert mock_emitter.method_calls[6].args[0].aspectName == \"status\"\n            assert (\n                mock_emitter.method_calls[6].args[0].entityUrn\n                == \"urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableProduced,PROD)\"\n            )\n\n            assert mock_emitter.method_calls[7].args[0].aspectName == \"ownership\"\n            assert (\n                mock_emitter.method_calls[7].args[0].entityUrn\n                == \"urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task2)\"\n            )\n\n            assert mock_emitter.method_calls[8].args[0].aspectName == \"globalTags\"\n            assert (\n                mock_emitter.method_calls[8].args[0].entityUrn\n                == \"urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task2)\"\n            )", "fn_id": 7, "class_fn": false, "repo": "cuong-pham/datahub", "file": "metadata-ingestion/tests/unit/test_airflow.py", "last_update_at": "2020-01-22T22:12:02+00:00", "question_id": "c8939b6523ddc3e7a9fd8625c26176a2ef5dd22a_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize(['inlets', 'outlets'], [pytest.param({'datasets': [Dataset('snowflake', 'mydb.schema.tableConsumed')]}, {'datasets': [Dataset('snowflake', 'mydb.schema.tableProduced')]}, id='airflow-1-10-lineage-syntax'), pytest.param([Dataset('snowflake', 'mydb.schema.tableConsumed')], [Dataset('snowflake', 'mydb.schema.tableProduced')], marks=pytest.mark.skipif(AIRFLOW_VERSION < packaging.version.parse('2.0.0'), reason='list-style lineage is only supported in Airflow 2.x'), id='airflow-2-lineage-syntax')])\n@mock.patch('datahub_provider.hooks.datahub.DatahubRestHook.make_emitter')\ndef test_lineage_backend(mock_emit, inlets, outlets):\n    DEFAULT_DATE = days_ago(2)\n    mock_emitter = Mock()\n    mock_emit.return_value = mock_emitter\n    with mock.patch.dict(os.environ, {'AIRFLOW__LINEAGE__BACKEND': 'datahub_provider.lineage.datahub.DatahubLineageBackend', 'AIRFLOW__LINEAGE__DATAHUB_CONN_ID': datahub_rest_connection_config.conn_id, 'AIRFLOW__LINEAGE__DATAHUB_KWARGS': json.dumps({'graceful_exceptions': False, 'capture_executions': False})}), mock.patch('airflow.models.BaseOperator.xcom_pull'), mock.patch('airflow.models.BaseOperator.xcom_push'), patch_airflow_connection(datahub_rest_connection_config):\n        func = mock.Mock()\n        func.__name__ = 'foo'\n        dag = DAG(dag_id='test_lineage_is_sent_to_backend', start_date=DEFAULT_DATE)\n        with dag:\n            op1 = DummyOperator(task_id='task1_upstream', inlets=inlets, outlets=outlets)\n            op2 = DummyOperator(task_id='task2', inlets=inlets, outlets=outlets)\n            op1 >> op2\n        if AIRFLOW_VERSION < packaging.version.parse('2.2.0'):\n            ti = TaskInstance(task=op2, execution_date=DEFAULT_DATE)\n        else:\n            ti = TaskInstance(task=op2, run_id=f'test_airflow-{DEFAULT_DATE}')\n        ctx1 = {'dag': dag, 'task': op2, 'ti': ti, 'task_instance': ti, 'execution_date': DEFAULT_DATE, 'ts': '2021-04-08T00:54:25.771575+00:00'}\n        prep = prepare_lineage(func)\n        prep(op2, ctx1)\n        post = apply_lineage(func)\n        post(op2, ctx1)\n        assert len(op2.inlets) == 1\n        assert len(op2.outlets) == 1\n        assert all(map(lambda let: isinstance(let, Dataset), op2.inlets))\n        assert all(map(lambda let: isinstance(let, Dataset), op2.outlets))\n        assert mock_emitter.emit.call_count == 9\n        if sys.version_info[:3] > (3, 7):\n            assert mock_emitter.method_calls[0].args[0].aspectName == 'dataFlowInfo'\n            assert mock_emitter.method_calls[0].args[0].entityUrn == 'urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod)'\n            assert mock_emitter.method_calls[1].args[0].aspectName == 'ownership'\n            assert mock_emitter.method_calls[1].args[0].entityUrn == 'urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod)'\n            assert mock_emitter.method_calls[2].args[0].aspectName == 'globalTags'\n            assert mock_emitter.method_calls[2].args[0].entityUrn == 'urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod)'\n            assert mock_emitter.method_calls[3].args[0].aspectName == 'dataJobInfo'\n            assert mock_emitter.method_calls[3].args[0].entityUrn == 'urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task2)'\n            assert mock_emitter.method_calls[4].args[0].aspectName == 'dataJobInputOutput'\n            assert mock_emitter.method_calls[4].args[0].entityUrn == 'urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task2)'\n            assert mock_emitter.method_calls[4].args[0].aspect.inputDatajobs[0] == 'urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task1_upstream)'\n            assert mock_emitter.method_calls[4].args[0].aspect.inputDatasets[0] == 'urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableConsumed,PROD)'\n            assert mock_emitter.method_calls[4].args[0].aspect.outputDatasets[0] == 'urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableProduced,PROD)'\n            assert mock_emitter.method_calls[5].args[0].aspectName == 'status'\n            assert mock_emitter.method_calls[5].args[0].entityUrn == 'urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableConsumed,PROD)'\n            assert mock_emitter.method_calls[6].args[0].aspectName == 'status'\n            assert mock_emitter.method_calls[6].args[0].entityUrn == 'urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableProduced,PROD)'\n            assert mock_emitter.method_calls[7].args[0].aspectName == 'ownership'\n            assert mock_emitter.method_calls[7].args[0].entityUrn == 'urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task2)'\n            assert mock_emitter.method_calls[8].args[0].aspectName == 'globalTags'\n"]]}
{"hexsha": "8af82a24e17cdc02aeef759da18fe3c8404ca60f", "ext": "py", "lang": "Python", "content": "def importInput(input):\n    input_file = open(input, \"r\")\n    lines = input_file.read().split('\\n')\n    instructionsList = []\n    for line in lines:\n        instruction = {}\n        instruction[line[:3]] = line[4:]\n        instructionsList.append(instruction)\n    return instructionsList", "fn_id": 0, "class_fn": false, "repo": "Adilius/adventofcode", "file": "2020/Day_08/part2.py", "last_update_at": "2020-12-03T17:08:43+00:00", "question_id": "8af82a24e17cdc02aeef759da18fe3c8404ca60f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def importInput(input):\n    input_file = open(input, 'r')\n    lines = input_file.read().split('\\n')\n    instructionsList = []\n    for line in lines:\n        instruction = {}\n        instruction[line[:3]] = line[4:]\n        instructionsList.append(instruction)\n"]]}
{"hexsha": "01720fb0d1d1409387c115b371906cdeb3b4343a", "ext": "py", "lang": "Python", "content": "@app.route(\"/create\", methods=[\"POST\"])\ndef create_container():\n    if request.method == \"POST\":\n        req = request.get_json()\n        image = req.get(\"image\")\n        password = req.get(\"password\")\n        job_id = random_string(string_length=5)\n\n        if image is not None:\n            return json.dumps(\n                controller.create_job(job_id, {\"PASSWD\": password}, image=image)\n            )\n        else:\n            return json.dumps(controller.create_job(job_id, {\"PASSWD\": password}))", "fn_id": 0, "class_fn": false, "repo": "tunl/tunl", "file": "pipeflow/api/api.py", "last_update_at": "2020-02-14T19:53:40+00:00", "question_id": "01720fb0d1d1409387c115b371906cdeb3b4343a_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.route('/create', methods=['POST'])\ndef create_container():\n    if request.method == 'POST':\n        req = request.get_json()\n        image = req.get('image')\n        password = req.get('password')\n        job_id = random_string(string_length=5)\n        if image is not None:\n            return json.dumps(controller.create_job(job_id, {'PASSWD': password}, image=image))\n        else:\n"]]}
{"hexsha": "53d4f65919872c85a5bbd4228e58cef5098a6ad3", "ext": "py", "lang": "Python", "content": "def _second_argument_type(str_arg):\n    second = int(str_arg)\n    if second >= 0:\n        return second\n    else:\n        msg = \"%r is not a valid second, data range is [0, +inf)\" % str_arg\n        raise argparse.ArgumentTypeError(msg)", "fn_id": 1, "class_fn": false, "repo": "zhiyanliu/swing", "file": "config/option.py", "last_update_at": "2020-05-11T10:01:24+00:00", "question_id": "53d4f65919872c85a5bbd4228e58cef5098a6ad3_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _second_argument_type(str_arg):\n    second = int(str_arg)\n    if second >= 0:\n        return second\n    else:\n        msg = '%r is not a valid second, data range is [0, +inf)' % str_arg\n"]]}
{"hexsha": "a71d65d89e1dae351ffd4427733e54cfe675a681", "ext": "py", "lang": "Python", "content": "def upload_file(request):\n    \"\"\"\n    Upload the ReID video and the ReID query picture\n    :param request: HTTP request\n    :return: HTTP Web Page and ReID query result url\n    \"\"\"\n\n    # the url and url_name for query failed\n    url = ['/static/fail.jpg']\n    url_name = ['fail']\n\n    if request.method == 'POST':\n\n        reid_video_file = request.FILES.getlist('reid_video')[0]\n        reid_pic_file = request.FILES.getlist('reid_pic')[0]\n\n        # Handle the uploading action\n        handle_uploaded_file(reid_video_file)\n        handle_uploaded_file(reid_pic_file)\n\n        # ReID service\n        reid_result = PersonReID(reid_video=os.path.join('reid_query', str(reid_video_file)),\n                                 reid_pic=os.path.join('reid_query', str(reid_pic_file)))\n        url, url_name = reid_result.reid_result()\n\n    context = {'reid_url': zip(url, url_name)}  # API dictionary for showing ReID query result\n\n    return render(request, 'result.html', context)", "fn_id": 0, "class_fn": false, "repo": "MikeCun/PersonReID", "file": "upload/views.py", "last_update_at": "2020-08-28T05:54:39+00:00", "question_id": "a71d65d89e1dae351ffd4427733e54cfe675a681_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def upload_file(request):\n    \"\"\"\n    Upload the ReID video and the ReID query picture\n    :param request: HTTP request\n    :return: HTTP Web Page and ReID query result url\n    \"\"\"\n    url = ['/static/fail.jpg']\n    url_name = ['fail']\n    if request.method == 'POST':\n        reid_video_file = request.FILES.getlist('reid_video')[0]\n        reid_pic_file = request.FILES.getlist('reid_pic')[0]\n        handle_uploaded_file(reid_video_file)\n        handle_uploaded_file(reid_pic_file)\n        reid_result = PersonReID(reid_video=os.path.join('reid_query', str(reid_video_file)), reid_pic=os.path.join('reid_query', str(reid_pic_file)))\n        url, url_name = reid_result.reid_result()\n    context = {'reid_url': zip(url, url_name)}\n"]]}
{"hexsha": "9d969833ce839224792d6fec999354f6bce66c85", "ext": "py", "lang": "Python", "content": "def find_packages(where, exclude=None):\n    if not exclude:\n        exclude = ()\n    if isinstance(where, str):\n        where = (where, )\n\n    ret_list = []\n    for name in chain.from_iterable(map(lambda w: (\n      n for _, n, ispkg in w if ispkg), (walk_packages(p) for p in where))):\n        if not any(wc_match(name, p) for p in exclude):\n            ret_list.append(name)\n\n    return tuple(ret_list)", "fn_id": 0, "class_fn": false, "repo": "NPalopoli/biopandas", "file": "setup.py", "last_update_at": "2020-04-14T16:46:24+00:00", "question_id": "9d969833ce839224792d6fec999354f6bce66c85_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def find_packages(where, exclude=None):\n    if not exclude:\n        exclude = ()\n    if isinstance(where, str):\n        where = (where,)\n    ret_list = []\n    for name in chain.from_iterable(map(lambda w: (n for _, n, ispkg in w if ispkg), (walk_packages(p) for p in where))):\n        if not any((wc_match(name, p) for p in exclude)):\n            ret_list.append(name)\n"]]}
{"hexsha": "9185f2ff121be31130e9d0e6dfee4cde19524eb1", "ext": "py", "lang": "Python", "content": "def binary_correlate(series_x, series_y):\n    \"\"\"Helper function to Correlate binary Data\n\n    Both the series should have same indices\n\n    For binary time series data:\n\n    .. math::\n\n        \\\\alpha_{corr} = \\\\frac{N_{agree} - N_{disagree}}{N}\n\n    :param series_x: First time Series data\n    :type series_x: :mod:`pandas.Series`\n\n    :param series_y: Second time Series data\n    :type series_y: :mod:`pandas.Series`\n    \"\"\"\n\n    if len(series_x) != len(series_y):\n        raise ValueError(\"Cannot compute binary correlation for \\\n                          unequal vectors\")\n\n    agree = len(series_x[series_x == series_y])\n    disagree = len(series_x[series_x != series_y])\n\n    return (agree - disagree) / len(series_x)", "fn_id": 10, "class_fn": false, "repo": "JaimeVHArm/lisa", "file": "external/bart/bart/sched/functions.py", "last_update_at": "2020-11-30T16:14:02+00:00", "question_id": "9185f2ff121be31130e9d0e6dfee4cde19524eb1_10", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def binary_correlate(series_x, series_y):\n    \"\"\"Helper function to Correlate binary Data\n\n    Both the series should have same indices\n\n    For binary time series data:\n\n    .. math::\n\n        \\\\alpha_{corr} = \\\\frac{N_{agree} - N_{disagree}}{N}\n\n    :param series_x: First time Series data\n    :type series_x: :mod:`pandas.Series`\n\n    :param series_y: Second time Series data\n    :type series_y: :mod:`pandas.Series`\n    \"\"\"\n    if len(series_x) != len(series_y):\n        raise ValueError('Cannot compute binary correlation for                           unequal vectors')\n    agree = len(series_x[series_x == series_y])\n    disagree = len(series_x[series_x != series_y])\n"]]}
{"hexsha": "d3a81116896a687811e9c64e62dfec686e730487", "ext": "py", "lang": "Python", "content": "def set_window_to_foreground(title):\n    import win32gui\n    import win32con\n    import win32com\n    try:\n        handle = win32gui.FindWindow(None, title)\n        if not handle:\n            raise Exception('Could not find a window with title \"{}\"'.format(title))\n\n        win32gui.ShowWindow(handle, win32con.SW_SHOWMAXIMIZED)\n        shell = win32com.client.Dispatch(\"WScript.Shell\")\n        shell.SendKeys('%')\n        win32gui.SetForegroundWindow(handle)\n    except Exception as ex:\n        raise ex", "fn_id": 1, "class_fn": false, "repo": "rocketbot-cl/Windows", "file": "__init__.py", "last_update_at": "2020-10-27T14:44:08+00:00", "question_id": "d3a81116896a687811e9c64e62dfec686e730487_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def set_window_to_foreground(title):\n    import win32gui\n    import win32con\n    import win32com\n    try:\n        handle = win32gui.FindWindow(None, title)\n        if not handle:\n            raise Exception('Could not find a window with title \"{}\"'.format(title))\n        win32gui.ShowWindow(handle, win32con.SW_SHOWMAXIMIZED)\n        shell = win32com.client.Dispatch('WScript.Shell')\n        shell.SendKeys('%')\n        win32gui.SetForegroundWindow(handle)\n    except Exception as ex:\n"]]}
{"hexsha": "ad285f43cf253d1f2ca927b49f3a5a07545a593e", "ext": "py", "lang": "Python", "content": "@with_altair_and_later\n@with_presets([MAINNET], reason=\"to create duplicate committee\")\n@spec_state_test\ndef test_sync_committee_rewards_duplicate_committee_full_participation(spec, state):\n    committee_indices = get_committee_indices(spec, state, duplicates=True)\n    committee_size = len(committee_indices)\n    committee_bits = [True] * committee_size\n    active_validator_count = len(spec.get_active_validator_indices(state, spec.get_current_epoch(state)))\n\n    # Preconditions of this test case\n    assert active_validator_count < spec.SYNC_COMMITTEE_SIZE\n    assert committee_size > len(set(committee_indices))\n\n    yield from run_successful_sync_committee_test(spec, state, committee_indices, committee_bits)", "fn_id": 10, "class_fn": false, "repo": "sthagen/eth2.0-specs", "file": "tests/core/pyspec/eth2spec/test/altair/block_processing/test_process_sync_aggregate.py", "last_update_at": "2020-02-02T11:20:55+00:00", "question_id": "ad285f43cf253d1f2ca927b49f3a5a07545a593e_10", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@with_altair_and_later\n@with_presets([MAINNET], reason='to create duplicate committee')\n@spec_state_test\ndef test_sync_committee_rewards_duplicate_committee_full_participation(spec, state):\n    committee_indices = get_committee_indices(spec, state, duplicates=True)\n    committee_size = len(committee_indices)\n    committee_bits = [True] * committee_size\n    active_validator_count = len(spec.get_active_validator_indices(state, spec.get_current_epoch(state)))\n    assert active_validator_count < spec.SYNC_COMMITTEE_SIZE\n    assert committee_size > len(set(committee_indices))\n"]]}
{"hexsha": "67f3e66d38336b1034c4e6bd59a34e57ffdc444a", "ext": "py", "lang": "Python", "content": "def normalize_tokens(word_list, extra_stop=STOP_WORDS):\n    #We can use a generator here as we just need to iterate over it\n    normalized = []\n\n    if type(word_list) == list and len(word_list) == 1:\n        word_list = word_list[0]\n\n    elif type(word_list) == list:\n        word_list = ' '.join([str(elem) for elem in word_list])\n\n    doc = NLP(word_list.lower())\n\n    # add the property of stop word to words considered as stop words\n    if len(extra_stop) > 0:\n        for stopword in extra_stop:\n            lexeme = NLP.vocab[stopword]\n            lexeme.is_stop = True\n\n    for w in doc:\n        # if it's not a stop word or punctuation mark, add it to our list\n        include = (w.text != '\\n' and not w.is_stop \\\n           and not w.is_punct and not w.like_num \\\n           and len(w.text.strip()) > 1 and w.is_alpha \\\n           and not w.lemma_ in extra_stop)\n\n        if include:\n            # we add the lematized version of the word\n            normalized.append(str(w.lemma_))\n\n    return normalized", "fn_id": 3, "class_fn": false, "repo": "ccsuehara/Peruvian-president-s-speeches", "file": "scripts/data_cleaning.py", "last_update_at": "2020-03-12T01:21:07+00:00", "question_id": "67f3e66d38336b1034c4e6bd59a34e57ffdc444a_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def normalize_tokens(word_list, extra_stop=STOP_WORDS):\n    normalized = []\n    if type(word_list) == list and len(word_list) == 1:\n        word_list = word_list[0]\n    elif type(word_list) == list:\n        word_list = ' '.join([str(elem) for elem in word_list])\n    doc = NLP(word_list.lower())\n    if len(extra_stop) > 0:\n        for stopword in extra_stop:\n            lexeme = NLP.vocab[stopword]\n            lexeme.is_stop = True\n    for w in doc:\n        include = w.text != '\\n' and (not w.is_stop) and (not w.is_punct) and (not w.like_num) and (len(w.text.strip()) > 1) and w.is_alpha and (not w.lemma_ in extra_stop)\n        if include:\n            normalized.append(str(w.lemma_))\n"]]}
{"hexsha": "252e0da195253d46eb41c35c484c0fab335ef58b", "ext": "py", "lang": "Python", "content": "def main():\n    input_filepath = 'etcpasswd'\n    output_filepath = './output.tsv'\n    passwd_to_csv(input_filepath, output_filepath)", "fn_id": 2, "class_fn": false, "repo": "honchardev/Fun", "file": "Python/pyworkout/files/ex22.py", "last_update_at": "2020-04-15T19:40:41+00:00", "question_id": "252e0da195253d46eb41c35c484c0fab335ef58b_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def main():\n    input_filepath = 'etcpasswd'\n    output_filepath = './output.tsv'\n"]]}
{"hexsha": "e439767795ece2a963bfe472a4321d12b64cec29", "ext": "py", "lang": "Python", "content": "def reduce_puzzle(values):\n    \"\"\"\n    Iterate eliminate() and only_choice(). If at some point, there is a box with no available values, return False.\n    If the sudoku is solved, return the sudoku.\n    If after an iteration of both functions, the sudoku remains the same, return the sudoku.\n    Input: A sudoku in dictionary form.\n    Output: The resulting sudoku in dictionary form.\n    \"\"\"\n    stalled = False\n    while not stalled:\n        # Check how many boxes have a determined value\n        solved_values_before = len(\n            [box for box in values.keys() if len(values[box]) == 1]\n        )\n\n        # Eliminate Strategy\n        values = eliminate(values)\n        # Only Choice Strategy\n        values = only_choice(values)\n\n        # Check how many boxes have a determined value, to compare\n        solved_values_after = len(\n            [box for box in values.keys() if len(values[box]) == 1]\n        )\n\n        # If no new values were added, stop the loop.\n        stalled = solved_values_before == solved_values_after\n\n        # Sanity check, return False if there is a box with zero available values.\n        if len([box for box in values.keys() if len(values[box]) == 0]):\n            return False\n    return values", "fn_id": 5, "class_fn": false, "repo": "alejandrosocorro/pytudes", "file": "sudoku/sudoku.py", "last_update_at": "2020-02-23T13:11:41+00:00", "question_id": "e439767795ece2a963bfe472a4321d12b64cec29_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def reduce_puzzle(values):\n    \"\"\"\n    Iterate eliminate() and only_choice(). If at some point, there is a box with no available values, return False.\n    If the sudoku is solved, return the sudoku.\n    If after an iteration of both functions, the sudoku remains the same, return the sudoku.\n    Input: A sudoku in dictionary form.\n    Output: The resulting sudoku in dictionary form.\n    \"\"\"\n    stalled = False\n    while not stalled:\n        solved_values_before = len([box for box in values.keys() if len(values[box]) == 1])\n        values = eliminate(values)\n        values = only_choice(values)\n        solved_values_after = len([box for box in values.keys() if len(values[box]) == 1])\n        stalled = solved_values_before == solved_values_after\n        if len([box for box in values.keys() if len(values[box]) == 0]):\n            return False\n"]]}
{"hexsha": "fb7e7f906e0c564022344bbeb3b0342b50ea10b7", "ext": "py", "lang": "Python", "content": "def voice():\n  gsp = gspeech2.Gspeech()\n  while True:\n      stt = gsp.getText()\n      if stt is None:\n         break\n      print(stt)\n      time.sleep(0.01)\n      if ('\uc88c' in stt):\n        return 'a'\n        break\n      if ('\uc6b0' in stt):\n        return 'd'\n        break\n      if ('\ud558' in stt):\n        return 's'\n        break\n      if ('\ub05d' in stt):\n        return ' '\n        break\n      if ('\ube68\uac15' in stt):\n        return 'r'\n        break\n      if ('\ub178\ub791' in stt):\n        return 'o'\n        break\n      if ('\ucd08\ub85d' in stt):\n        return 'g'\n        break\n      if ('\uc9c0\uc6cc' in stt):\n        return 'e'\n        break\n      if ('\uc5f4\uae30' in stt):\n        return 'o'\n        break\n      if ('\ud78c\ud2b8' in stt):\n        return 'h'\n        break", "fn_id": 0, "class_fn": false, "repo": "tjddus0403/osscap2020", "file": "memory_stt/fuc.py", "last_update_at": "2020-10-07T12:49:33+00:00", "question_id": "fb7e7f906e0c564022344bbeb3b0342b50ea10b7_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def voice():\n    gsp = gspeech2.Gspeech()\n    while True:\n        stt = gsp.getText()\n        if stt is None:\n            break\n        print(stt)\n        time.sleep(0.01)\n        if '\uc88c' in stt:\n            return 'a'\n            break\n        if '\uc6b0' in stt:\n            return 'd'\n            break\n        if '\ud558' in stt:\n            return 's'\n            break\n        if '\ub05d' in stt:\n            return ' '\n            break\n        if '\ube68\uac15' in stt:\n            return 'r'\n            break\n        if '\ub178\ub791' in stt:\n            return 'o'\n            break\n        if '\ucd08\ub85d' in stt:\n            return 'g'\n            break\n        if '\uc9c0\uc6cc' in stt:\n            return 'e'\n            break\n        if '\uc5f4\uae30' in stt:\n            return 'o'\n            break\n        if '\ud78c\ud2b8' in stt:\n            return 'h'\n"]]}
{"hexsha": "7346174d177d9a0833ca426ec09adc4a8542b9b5", "ext": "py", "lang": "Python", "content": "def test2():\n    with Switch() as (switch, case, default):\n        try:switch('hola')\n        except case(1):\n            print(1)\n        except case('holaS'):\n            print('holaS')\n        except case('hola'):\n            print('hola')\n        except default():\n            print('default..')", "fn_id": 2, "class_fn": false, "repo": "JohannesBuchner/pystrict3", "file": "tests/data23/recipe-521914.py", "last_update_at": "2020-06-05T08:53:26+00:00", "question_id": "7346174d177d9a0833ca426ec09adc4a8542b9b5_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test2():\n    with Switch() as (switch, case, default):\n        try:\n            switch('hola')\n        except case(1):\n            print(1)\n        except case('holaS'):\n            print('holaS')\n        except case('hola'):\n            print('hola')\n        except default():\n"]]}
{"hexsha": "c03ef307f1e8703eb8538ab36dc9b3b3af98024c", "ext": "py", "lang": "Python", "content": "def get_tag2index():\n    if TASK == 0: return {\"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-LOC\": 3, \"I-LOC\": 4, \"B-ORG\": 5, \"I-ORG\": 6}         # NER \u6807\u6ce8\u7684\u6807\u7b7e\n    #else: return {\"O\": 0, \"B-R\": 1, \"I-R\": 2, \"B-M\": 3, \"I-M\": 4, \"B-S\": 5, \"I-S\": 6, \"B-W\": 7, \"I-W\": 8}         # query \u7ea0\u9519\u6807\u6ce8\u7684\u6807\u7b7e1\n    else: return {\"O\": 0, \"B-SP\": 1, \"I-SP\": 2, \"B-SS\": 3, \"I-SS\": 4}  # query \u7ea0\u9519\u6807\u6ce8\u7684\u6807\u7b7e", "fn_id": 1, "class_fn": false, "repo": "zouning68/ner-demo", "file": "DataProcess/vocab.py", "last_update_at": "2020-12-27T06:17:33+00:00", "question_id": "c03ef307f1e8703eb8538ab36dc9b3b3af98024c_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_tag2index():\n    if TASK == 0:\n        return {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-LOC': 3, 'I-LOC': 4, 'B-ORG': 5, 'I-ORG': 6}\n    else:\n"]]}
{"hexsha": "bad8a37752f433d0f64e35629b026451a9b7f31b", "ext": "py", "lang": "Python", "content": "def read_info(file_name):\n    info = []\n    with open(file_name, \"rb\") as file:\n        if file.read(4) != b'\\x00\\xA0\\x00\\x00':\n            print(\"Incorrect magic!\")\n            return\n        else:\n            file.seek(0)\n\n        read_node(info, file)            \n\n    return info", "fn_id": 4, "class_fn": false, "repo": "aspadm/EIrepack", "file": "Formats/mob.py", "last_update_at": "2020-08-29T19:15:19+00:00", "question_id": "bad8a37752f433d0f64e35629b026451a9b7f31b_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def read_info(file_name):\n    info = []\n    with open(file_name, 'rb') as file:\n        if file.read(4) != b'\\x00\\xa0\\x00\\x00':\n            print('Incorrect magic!')\n            return\n        else:\n            file.seek(0)\n        read_node(info, file)\n"]]}
{"hexsha": "a438cfea753c52adfbf8794a2e34672479d5b2b2", "ext": "py", "lang": "Python", "content": "def GetHealthChecks(args, resource_parser):\n  \"\"\"Returns health check URIs from arguments.\"\"\"\n  health_check_refs = []\n\n  if args.http_health_checks:\n    health_check_refs.extend(resource_parser.CreateGlobalReferences(\n        args.http_health_checks, resource_type='httpHealthChecks'))\n\n  if getattr(args, 'https_health_checks', None):\n    health_check_refs.extend(resource_parser.CreateGlobalReferences(\n        args.https_health_checks, resource_type='httpsHealthChecks'))\n\n  if getattr(args, 'health_checks', None):\n    if health_check_refs:\n      raise exceptions.ToolException(\n          'Mixing --health-checks with --http-health-checks or with '\n          '--https-health-checks is not supported.')\n    else:\n      health_check_refs.extend(resource_parser.CreateGlobalReferences(\n          args.health_checks, resource_type='healthChecks'))\n\n  return [health_check_ref.SelfLink() for health_check_ref in health_check_refs]", "fn_id": 0, "class_fn": false, "repo": "bopopescu/SDK", "file": "lib/googlecloudsdk/api_lib/compute/backend_services_utils.py", "last_update_at": "2020-07-25T12:23:41+00:00", "question_id": "a438cfea753c52adfbf8794a2e34672479d5b2b2_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def GetHealthChecks(args, resource_parser):\n    \"\"\"Returns health check URIs from arguments.\"\"\"\n    health_check_refs = []\n    if args.http_health_checks:\n        health_check_refs.extend(resource_parser.CreateGlobalReferences(args.http_health_checks, resource_type='httpHealthChecks'))\n    if getattr(args, 'https_health_checks', None):\n        health_check_refs.extend(resource_parser.CreateGlobalReferences(args.https_health_checks, resource_type='httpsHealthChecks'))\n    if getattr(args, 'health_checks', None):\n        if health_check_refs:\n            raise exceptions.ToolException('Mixing --health-checks with --http-health-checks or with --https-health-checks is not supported.')\n        else:\n            health_check_refs.extend(resource_parser.CreateGlobalReferences(args.health_checks, resource_type='healthChecks'))\n"]]}
{"hexsha": "b9fb27243f961eaa90ac630fd530777809ae9a39", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize(\"input_shape\", input_shapes)\ndef test_concat_attributes_saved_during_graph_building(input_shape):\n    model = ModelForTestWithReshapeFlattenAndConcat()\n    input_info = ModelInputInfo(input_shape)\n    graph_builder = GraphBuilder(create_dummy_forward_fn([input_info, ], with_input_tracing=True,\n                                                         with_output_tracing=True))\n    graph = graph_builder.build_graph(model)\n    cat_nodes_with_attributes = {\n        'ModelForTestWithReshapeFlattenAndConcat/cat_0': {'axis': 1},\n        'ModelForTestWithReshapeFlattenAndConcat/cat_1': {'axis': 6},\n        'ModelForTestWithReshapeFlattenAndConcat/cat_2': {'axis': 1},\n        'ModelForTestWithReshapeFlattenAndConcat/stack_0': None,\n        'ModelForTestWithReshapeFlattenAndConcat/stack_1': None\n    }\n\n    for node in graph.get_all_nodes():\n        if node.metatype is PTCatMetatype:\n            assert node.node_name in cat_nodes_with_attributes\n            if isinstance(node.layer_attributes, MultipleInputLayerAttributes):\n                assert node.layer_attributes.axis == cat_nodes_with_attributes[node.node_name]['axis']\n            else:\n                assert node.layer_attributes is None\n                assert cat_nodes_with_attributes[node.node_name] is None", "fn_id": 4, "class_fn": false, "repo": "MaximProshin/nncf", "file": "tests/torch/test_graph_building.py", "last_update_at": "2020-10-28T06:10:50+00:00", "question_id": "b9fb27243f961eaa90ac630fd530777809ae9a39_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@pytest.mark.parametrize('input_shape', input_shapes)\ndef test_concat_attributes_saved_during_graph_building(input_shape):\n    model = ModelForTestWithReshapeFlattenAndConcat()\n    input_info = ModelInputInfo(input_shape)\n    graph_builder = GraphBuilder(create_dummy_forward_fn([input_info], with_input_tracing=True, with_output_tracing=True))\n    graph = graph_builder.build_graph(model)\n    cat_nodes_with_attributes = {'ModelForTestWithReshapeFlattenAndConcat/cat_0': {'axis': 1}, 'ModelForTestWithReshapeFlattenAndConcat/cat_1': {'axis': 6}, 'ModelForTestWithReshapeFlattenAndConcat/cat_2': {'axis': 1}, 'ModelForTestWithReshapeFlattenAndConcat/stack_0': None, 'ModelForTestWithReshapeFlattenAndConcat/stack_1': None}\n    for node in graph.get_all_nodes():\n        if node.metatype is PTCatMetatype:\n            assert node.node_name in cat_nodes_with_attributes\n            if isinstance(node.layer_attributes, MultipleInputLayerAttributes):\n                assert node.layer_attributes.axis == cat_nodes_with_attributes[node.node_name]['axis']\n            else:\n                assert node.layer_attributes is None\n"]]}
{"hexsha": "fbf3bd1d40d29fc5addd1a8f53a9041763e47088", "ext": "py", "lang": "Python", "content": "def merge_array_type(type1, type2):\n    assert is_array(type1) or is_array(type2)\n    if is_java_lang_object(type2):\n        return type2\n    elif is_java_lang_object(type1):\n        return type1\n    if is_array(type1):\n        if is_array(type2):\n            new_type = merge_type(type1[1:], type2[1:])\n            if new_type:\n                return '[' + new_type\n            else:\n                return None\n        else:\n            return 'Ljava/lang/Object;'\n    else:\n        return merge_array_type(type2, type1)", "fn_id": 16, "class_fn": false, "repo": "scnucrypto/dcc", "file": "dex2c/util.py", "last_update_at": "2020-11-08T10:31:06+00:00", "question_id": "fbf3bd1d40d29fc5addd1a8f53a9041763e47088_16", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def merge_array_type(type1, type2):\n    assert is_array(type1) or is_array(type2)\n    if is_java_lang_object(type2):\n        return type2\n    elif is_java_lang_object(type1):\n        return type1\n    if is_array(type1):\n        if is_array(type2):\n            new_type = merge_type(type1[1:], type2[1:])\n            if new_type:\n                return '[' + new_type\n            else:\n                return None\n        else:\n            return 'Ljava/lang/Object;'\n    else:\n"]]}
{"hexsha": "59181b8e3372489fe073e8503ed1828d4bead9ca", "ext": "py", "lang": "Python", "content": "def url_get_parent(url):\n    \"\"\"\n    http://one/two/three  =>  http://one/two\n    http://one            =>  http://one\n    \"\"\"\n    index = url.rfind(\"/\")\n    if index > 8:           # avoid https://\n        return url[0:index]\n    else:\n        return url", "fn_id": 8, "class_fn": false, "repo": "luca-vercelli/AppImageRepository", "file": "appimage-crawler.py", "last_update_at": "2020-05-22T21:47:51+00:00", "question_id": "59181b8e3372489fe073e8503ed1828d4bead9ca_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def url_get_parent(url):\n    \"\"\"\n    http://one/two/three  =>  http://one/two\n    http://one            =>  http://one\n    \"\"\"\n    index = url.rfind('/')\n    if index > 8:\n        return url[0:index]\n    else:\n"]]}
{"hexsha": "d677a88d7854c4b095ce2ed9a3917c1b9276b15f", "ext": "py", "lang": "Python", "content": "def _get_help_record(opt):\n    \"\"\"Re-implementation of click.Opt.get_help_record.\n    The variant of 'get_help_record' found in Click makes uses of slashes to\n    separate multiple opts, and formats option arguments using upper case. This\n    is not compatible with Sphinx's 'option' directive, which expects\n    comma-separated opts and option arguments surrounded by angle brackets [1].\n    [1] http://www.sphinx-doc.org/en/stable/domains.html#directive-option\n    \"\"\"\n\n    def _write_opts(opts):\n        rv, _ = click.formatting.join_options(opts)\n        if not opt.is_flag and not opt.count:\n            rv += ' <{}>'.format(opt.name)\n        return rv\n\n    rv = [_write_opts(opt.opts)]\n    if opt.secondary_opts:\n        rv.append(_write_opts(opt.secondary_opts))\n\n    help = opt.help or ''\n    extra = []\n    if opt.default is not None and opt.show_default:\n        extra.append(\n            'default: %s' % (', '.join('%s' % d for d in opt.default)\n                             if isinstance(opt.default,\n                                           (list, tuple)) else opt.default, ))\n    if opt.required:\n        extra.append('required')\n    if extra:\n        help = '%s[%s]' % (help and help + '  ' or '', '; '.join(extra))\n\n    return ', '.join(rv), help", "fn_id": 2, "class_fn": false, "repo": "mimakaev/cooler", "file": "docs/make_cli_rst.py", "last_update_at": "2020-09-24T12:15:13+00:00", "question_id": "d677a88d7854c4b095ce2ed9a3917c1b9276b15f_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _get_help_record(opt):\n    \"\"\"Re-implementation of click.Opt.get_help_record.\n    The variant of 'get_help_record' found in Click makes uses of slashes to\n    separate multiple opts, and formats option arguments using upper case. This\n    is not compatible with Sphinx's 'option' directive, which expects\n    comma-separated opts and option arguments surrounded by angle brackets [1].\n    [1] http://www.sphinx-doc.org/en/stable/domains.html#directive-option\n    \"\"\"\n\n    def _write_opts(opts):\n        rv, _ = click.formatting.join_options(opts)\n        if not opt.is_flag and (not opt.count):\n            rv += ' <{}>'.format(opt.name)\n        return rv\n    rv = [_write_opts(opt.opts)]\n    if opt.secondary_opts:\n        rv.append(_write_opts(opt.secondary_opts))\n    help = opt.help or ''\n    extra = []\n    if opt.default is not None and opt.show_default:\n        extra.append('default: %s' % (', '.join(('%s' % d for d in opt.default)) if isinstance(opt.default, (list, tuple)) else opt.default,))\n    if opt.required:\n        extra.append('required')\n    if extra:\n        help = '%s[%s]' % (help and help + '  ' or '', '; '.join(extra))\n"]]}
{"hexsha": "4b26ec02cc4c451cfd66418e36858cfb578b80da", "ext": "py", "lang": "Python", "content": "def get_mimetype_from_filename(filename):\n    mimetype = 'application/octet-stream'\n    if filename:\n        detected_mimetype = mimetypes.guess_type(filename)\n        if detected_mimetype[0]:\n            mimetype = detected_mimetype[0]\n    return mimetype", "fn_id": 0, "class_fn": false, "repo": "aless80/devilry-django", "file": "devilry/devilry_import_v2database/modelimporters/modelimporter_utils.py", "last_update_at": "2020-11-10T21:28:27+00:00", "question_id": "4b26ec02cc4c451cfd66418e36858cfb578b80da_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_mimetype_from_filename(filename):\n    mimetype = 'application/octet-stream'\n    if filename:\n        detected_mimetype = mimetypes.guess_type(filename)\n        if detected_mimetype[0]:\n            mimetype = detected_mimetype[0]\n"]]}
{"hexsha": "fa7225e5d07b9d807a5cf5efa399a86caa12943a", "ext": "py", "lang": "Python", "content": "async def vk(user_id: str) -> ParsedData:\n    page_link = _get_page_link(user_id)\n\n    page = BeautifulSoup(await get_page(page_link), features=\"html.parser\")\n\n    if page.find(\"img\", {\"src\": \"/images/pics/spamfight.gif\"}) or (\n            page.find(\"div\", {\"class\": \"message_page_title\"}) and page.title == \"Information\"):\n        logger.debug(f\"Bad user {page_link}\")\n        raise BadID\n\n    image_element = page.find(\"img\", {\"class\": \"page_avatar_img\"})\n\n    if image_element is None or \"alt\" not in image_element.attrs:\n        logger.debug(\"\u0413\u0440\u0443\u043f\u043f\u0430 \u0438\u043b\u0438 \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u043d\u0430\u044f \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0430\")\n        raise BadID\n\n    user_name = image_element[\"alt\"]\n    image: bytes\n\n    try:\n        image = await _get_quality_photo(page)\n    except Exception as e:\n        logger.debug(f\"Failed to fetch hires image for {page_link}\")\n        image = await get_file(image_element[\"src\"])\n\n    return ParsedData(face=image, traits={\"name\": [user_name], \"vk_url\": [page_link]})", "fn_id": 2, "class_fn": false, "repo": "keddad/hseproject", "file": "PROJECT/socialcomp/parsers/vk.py", "last_update_at": "2020-02-14T11:44:14+00:00", "question_id": "fa7225e5d07b9d807a5cf5efa399a86caa12943a_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def vk(user_id: str) -> ParsedData:\n    page_link = _get_page_link(user_id)\n    page = BeautifulSoup(await get_page(page_link), features='html.parser')\n    if page.find('img', {'src': '/images/pics/spamfight.gif'}) or (page.find('div', {'class': 'message_page_title'}) and page.title == 'Information'):\n        logger.debug(f'Bad user {page_link}')\n        raise BadID\n    image_element = page.find('img', {'class': 'page_avatar_img'})\n    if image_element is None or 'alt' not in image_element.attrs:\n        logger.debug('\u0413\u0440\u0443\u043f\u043f\u0430 \u0438\u043b\u0438 \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u043d\u0430\u044f \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0430')\n        raise BadID\n    user_name = image_element['alt']\n    image: bytes\n    try:\n        image = await _get_quality_photo(page)\n    except Exception as e:\n        logger.debug(f'Failed to fetch hires image for {page_link}')\n        image = await get_file(image_element['src'])\n"]]}
{"hexsha": "3b5b9dbc619f35ba55f85d5a86864732ec73d976", "ext": "py", "lang": "Python", "content": "def get_or_create(cls, **kwargs):\n    \"\"\"Get or create a ``cls`` instance using the ``kwargs`` provided.\"\"\"\n    instance = cls.query.filter_by(**kwargs).first()\n    if not instance:\n        instance = cls(**kwargs)\n    return instance", "fn_id": 2, "class_fn": false, "repo": "thruflo/pyramid_basemodel", "file": "src/pyramid_basemodel/util.py", "last_update_at": "2020-07-20T11:59:49+00:00", "question_id": "3b5b9dbc619f35ba55f85d5a86864732ec73d976_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_or_create(cls, **kwargs):\n    \"\"\"Get or create a ``cls`` instance using the ``kwargs`` provided.\"\"\"\n    instance = cls.query.filter_by(**kwargs).first()\n    if not instance:\n        instance = cls(**kwargs)\n"]]}
{"hexsha": "7395848f17a2b2be80fce364d79347b0bafb3b67", "ext": "py", "lang": "Python", "content": "def highestship(f):\n    highest = \"freighters\"\n    for ship in v.shipindices:\n        if f.__dict__[ship] > 0:\n            highest = ship\n    return highest", "fn_id": 7, "class_fn": false, "repo": "heidi666/WorldsAtWar", "file": "wawmembers/display.py", "last_update_at": "2020-08-29T22:30:26+00:00", "question_id": "7395848f17a2b2be80fce364d79347b0bafb3b67_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def highestship(f):\n    highest = 'freighters'\n    for ship in v.shipindices:\n        if f.__dict__[ship] > 0:\n            highest = ship\n"]]}
{"hexsha": "1cf8634ccc9b149becd266d49f008d61337f0553", "ext": "py", "lang": "Python", "content": "def export_table(data, path):\n    \"\"\"\n    Export final table as LaTeX table.\n    \"\"\"\n    # Keep columns we care about\n    data['time_base'] = data['time_base'].map('{:.3f}'.format) + ' ' + data['stdev_base'].map('({:.4f})'.format)\n    data['time_ebph'] = data['time_ebph'].map('{:.3f}'.format) + ' ' + data['stdev_ebph'].map('({:.4f})'.format)\n    data['overhead'] = data['overhead'].map('{:.3f}'.format) + ' ' + data['stdev_overhead'].map('({:.4f})'.format)\n    data = data[['syscall', 'time_base', 'time_ebph', 'overhead']]\n    # Export table, after renaming columns\n    if args.out:\n        data[:]['syscall'] =  data[:]['syscall'].str.replace('_', r'\\_')\n        data = data.rename(columns={\n            'syscall':r'\\multicolumn{1}{l}{System Call}',\n            'time_base':r'$T_{\\text{base}}$ ($\\mu$s)',\n            'time_ebph':r'$T_{\\text{ebpH}}$ ($\\mu$s)',\n            'overhead':r'\\% Overhead'})\n        data.to_latex(index=0, escape=0, buf=path, column_format=r'>{\\ttfamily}lrrrr')\n    else:\n        print(data)", "fn_id": 9, "class_fn": false, "repo": "HousedHorse/COMP4906", "file": "data/analyze-bpfbench.py", "last_update_at": "2020-05-18T20:45:16+00:00", "question_id": "1cf8634ccc9b149becd266d49f008d61337f0553_9", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def export_table(data, path):\n    \"\"\"\n    Export final table as LaTeX table.\n    \"\"\"\n    data['time_base'] = data['time_base'].map('{:.3f}'.format) + ' ' + data['stdev_base'].map('({:.4f})'.format)\n    data['time_ebph'] = data['time_ebph'].map('{:.3f}'.format) + ' ' + data['stdev_ebph'].map('({:.4f})'.format)\n    data['overhead'] = data['overhead'].map('{:.3f}'.format) + ' ' + data['stdev_overhead'].map('({:.4f})'.format)\n    data = data[['syscall', 'time_base', 'time_ebph', 'overhead']]\n    if args.out:\n        data[:]['syscall'] = data[:]['syscall'].str.replace('_', '\\\\_')\n        data = data.rename(columns={'syscall': '\\\\multicolumn{1}{l}{System Call}', 'time_base': '$T_{\\\\text{base}}$ ($\\\\mu$s)', 'time_ebph': '$T_{\\\\text{ebpH}}$ ($\\\\mu$s)', 'overhead': '\\\\% Overhead'})\n        data.to_latex(index=0, escape=0, buf=path, column_format='>{\\\\ttfamily}lrrrr')\n    else:\n"]]}
{"hexsha": "51eac0c750ecb1ecd08b99a51d74d7a88d097d37", "ext": "py", "lang": "Python", "content": "def breadth_first_search(tree, target):\n    \"\"\"\n    Exercise 2: Implement your depth first search algorithm here\n    \"\"\"\n    nodes = [tree]\n    while nodes:\n        node = nodes.pop(0)\n        print(\"BFS: Checking node with value '{}'\".format(node.value))\n        if node.value == target:\n            return node\n        if node.left:\n            nodes.append(node.left)\n        if node.right:\n            nodes.append(node.right)", "fn_id": 1, "class_fn": false, "repo": "mgadagin/PythonClass", "file": "course/algorithms/tree-search-algorithms/solutions.py", "last_update_at": "2020-12-08T10:07:19+00:00", "question_id": "51eac0c750ecb1ecd08b99a51d74d7a88d097d37_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def breadth_first_search(tree, target):\n    \"\"\"\n    Exercise 2: Implement your depth first search algorithm here\n    \"\"\"\n    nodes = [tree]\n    while nodes:\n        node = nodes.pop(0)\n        print(\"BFS: Checking node with value '{}'\".format(node.value))\n        if node.value == target:\n            return node\n        if node.left:\n            nodes.append(node.left)\n        if node.right:\n"]]}
{"hexsha": "5a3b2f1a78ce0acfc63139c48cf2862a4a7bfbad", "ext": "py", "lang": "Python", "content": "def normalize_angle(x):\n    x = x % (2 * np.pi)  # force in range [0, 2 pi)\n    if x > np.pi:  # move to [-pi, pi)\n        x -= 2 * np.pi\n    return x", "fn_id": 1, "class_fn": false, "repo": "clydemcqueen/orca2", "file": "orca_base/scripts/nees_fp.py", "last_update_at": "2020-11-23T16:57:41+00:00", "question_id": "5a3b2f1a78ce0acfc63139c48cf2862a4a7bfbad_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def normalize_angle(x):\n    x = x % (2 * np.pi)\n    if x > np.pi:\n        x -= 2 * np.pi\n"]]}
{"hexsha": "e01891d97318d84fe167c82674fbdb538fd95021", "ext": "py", "lang": "Python", "content": "def webform_download(request,slug):\n    dump_dir = '/protwis/construct_dump'\n    # dump_dir = '/web/sites/files/construct_data' #for sites\n    file = dump_dir+\"/\"+str(slug)+\".json\"\n    out_stream = open(file,\"rb\").read()\n    response = HttpResponse(content_type=\"application/json\")\n    response['Content-Disposition'] = 'attachment; filename=\"{}\"'.format(file)\n    response.write(out_stream)\n    return response", "fn_id": 10, "class_fn": false, "repo": "GPCRmd/GPCRmd", "file": "structure/views.py", "last_update_at": "2020-08-05T15:31:29+00:00", "question_id": "e01891d97318d84fe167c82674fbdb538fd95021_10", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def webform_download(request, slug):\n    dump_dir = '/protwis/construct_dump'\n    file = dump_dir + '/' + str(slug) + '.json'\n    out_stream = open(file, 'rb').read()\n    response = HttpResponse(content_type='application/json')\n    response['Content-Disposition'] = 'attachment; filename=\"{}\"'.format(file)\n    response.write(out_stream)\n"]]}
{"hexsha": "4afdd419dc71244616cb7fa1fa0d29c8e678751a", "ext": "py", "lang": "Python", "content": "def process_format(variant, my_sample_idx, format_ids_values, write_block):\n\n    format_ids_in_variant = str(variant).split('\\t')[8].split(':')\n    sample_data_in_variant = str(variant).rstrip('\\n').split('\\t')[9::]\n\n\n    for nth in my_sample_idx:\n        nth_sample = sample_data_in_variant[nth].split(':')\n\n        # update the values for each keys in the dictionary\n        format_ids_vals = list(zip(format_ids_in_variant, nth_sample))\n\n        for ks, vs in format_ids_vals:\n            if ks in format_ids_values.keys():\n                format_ids_values[ks] = vs    # condition: if ks in format_ids_values.keys()\n\n\n        # Now, write the values to each FORMAT field, for each sample\n        for vs in format_ids_values.values():\n            write_block.write('\\t' + str(vs))", "fn_id": 3, "class_fn": false, "repo": "harish0201/VCF-Simplify", "file": "OldVersions/v1.0/vcf_simplify-v1.py", "last_update_at": "2020-12-30T18:03:35+00:00", "question_id": "4afdd419dc71244616cb7fa1fa0d29c8e678751a_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def process_format(variant, my_sample_idx, format_ids_values, write_block):\n    format_ids_in_variant = str(variant).split('\\t')[8].split(':')\n    sample_data_in_variant = str(variant).rstrip('\\n').split('\\t')[9:]\n    for nth in my_sample_idx:\n        nth_sample = sample_data_in_variant[nth].split(':')\n        format_ids_vals = list(zip(format_ids_in_variant, nth_sample))\n        for ks, vs in format_ids_vals:\n            if ks in format_ids_values.keys():\n                format_ids_values[ks] = vs\n        for vs in format_ids_values.values():\n"]]}
{"hexsha": "dc12eb19ca637e291bc5f601e2db96e7671e7103", "ext": "py", "lang": "Python", "content": "@app.route('/My_Cart')\ndef my_cart():\n    username = request.args.get('username')\n    products, cnt, cart_cost = my_items_in_cart(username) # gets all the products in cart of this current customer\n    return render_template('mycart.html', products=products, cnt=cnt, username=username, cart_cost=cart_cost)", "fn_id": 16, "class_fn": false, "repo": "ASHIK11ab/ON2YOU-E-commerce-website-using-Flask_and_pickle_module", "file": "app.py", "last_update_at": "2020-07-01T15:51:37+00:00", "question_id": "dc12eb19ca637e291bc5f601e2db96e7671e7103_16", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.route('/My_Cart')\ndef my_cart():\n    username = request.args.get('username')\n    products, cnt, cart_cost = my_items_in_cart(username)\n"]]}
{"hexsha": "56aee7432e0a8335d435e9bcae875984619fd76a", "ext": "py", "lang": "Python", "content": "def train_epoch(\n    epoch,\n    model, \n    data_loader, \n    loss_fn, \n    optimizer, \n    device, \n    scheduler,\n    print_freq = 50\n    ):\n    \"\"\" Train the model of a single epoch, and log progress with wandb.\n    Parameters: \n    epoch (int): Which epoch number is being trained (for logging).\n    model (PyTorch model): Provide a trainable model.\n    data_loader (PyTorch Dataloader): Provide a data loader\n    loss_fn (function): Loss function\n    optimizer (PyTorch optimizer): Which optimizer to use\n    scheduler (PyTorch scheduler): Update optimizer parameters at each iteration\n    print_freq (int): Log frequency for wandb\n    \"\"\" \n    \n    model = model.train()\n    train_loss = 0\n    total = 0\n    correct_predictions = 0\n    start_time = time.time()\n    total_batches = len(data_loader)\n    for batch_idx, d  in enumerate(data_loader): \n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        total += targets.size(0)\n        correct_predictions += torch.sum(preds == targets)\n        train_loss += loss.item()\n        if (1+ batch_idx) % print_freq == 0:\n            acc = 100.*float(correct_predictions)/float(total)\n            print('Batch: {:3.0f}/{:3.0f}. Train Loss: {:0.3f}  Acc: {:2.0f} '.format(\n                  batch_idx+1,total_batches,train_loss/(batch_idx+1),acc))\n            lr = list(optimizer.param_groups)[0]['lr']\n            wandb.log({'epoch': epoch, 'lr':lr , 'train_loss':train_loss/(batch_idx+1), 'train_acc':acc})\n            \n    print('Time/epoch: {:3.0f}'.format(time.time()-start_time))\n    acc = 100.*float(correct_predictions)/float(total)\n    lr = list(optimizer.param_groups)[0]['lr']\n    wandb.log({'epoch': epoch, 'lr':lr , 'train_loss':train_loss/(batch_idx+1), 'train_acc':acc})\n    print('-----------')\n    return acc, train_loss / (1+batch_idx)", "fn_id": 1, "class_fn": false, "repo": "tbachlechner/Heuristik", "file": "src/training.py", "last_update_at": "2020-07-10T19:12:39+00:00", "question_id": "56aee7432e0a8335d435e9bcae875984619fd76a_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def train_epoch(epoch, model, data_loader, loss_fn, optimizer, device, scheduler, print_freq=50):\n    \"\"\" Train the model of a single epoch, and log progress with wandb.\n    Parameters: \n    epoch (int): Which epoch number is being trained (for logging).\n    model (PyTorch model): Provide a trainable model.\n    data_loader (PyTorch Dataloader): Provide a data loader\n    loss_fn (function): Loss function\n    optimizer (PyTorch optimizer): Which optimizer to use\n    scheduler (PyTorch scheduler): Update optimizer parameters at each iteration\n    print_freq (int): Log frequency for wandb\n    \"\"\"\n    model = model.train()\n    train_loss = 0\n    total = 0\n    correct_predictions = 0\n    start_time = time.time()\n    total_batches = len(data_loader)\n    for batch_idx, d in enumerate(data_loader):\n        input_ids = d['input_ids'].to(device)\n        attention_mask = d['attention_mask'].to(device)\n        targets = d['targets'].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        total += targets.size(0)\n        correct_predictions += torch.sum(preds == targets)\n        train_loss += loss.item()\n        if (1 + batch_idx) % print_freq == 0:\n            acc = 100.0 * float(correct_predictions) / float(total)\n            print('Batch: {:3.0f}/{:3.0f}. Train Loss: {:0.3f}  Acc: {:2.0f} '.format(batch_idx + 1, total_batches, train_loss / (batch_idx + 1), acc))\n            lr = list(optimizer.param_groups)[0]['lr']\n            wandb.log({'epoch': epoch, 'lr': lr, 'train_loss': train_loss / (batch_idx + 1), 'train_acc': acc})\n    print('Time/epoch: {:3.0f}'.format(time.time() - start_time))\n    acc = 100.0 * float(correct_predictions) / float(total)\n    lr = list(optimizer.param_groups)[0]['lr']\n    wandb.log({'epoch': epoch, 'lr': lr, 'train_loss': train_loss / (batch_idx + 1), 'train_acc': acc})\n    print('-----------')\n"]]}
{"hexsha": "4040d3ac60b4e9bdd741f3ffe95d2a3d6fb1afba", "ext": "py", "lang": "Python", "content": "def xtest_umls(rosetta):\n    node = KNode(\"UMLS:C0015625\",label=\"Fanconi Anemia\", type=node_types.DISEASE)\n    rosetta.synonymizer.synonymize(node)\n    print(node.synonyms)\n    assert node.id == 'MONDO:0019339'", "fn_id": 11, "class_fn": false, "repo": "TranslatorIIPrototypes/robo-commons", "file": "greent/test/test_cache.py", "last_update_at": "2020-06-16T13:23:13+00:00", "question_id": "4040d3ac60b4e9bdd741f3ffe95d2a3d6fb1afba_11", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def xtest_umls(rosetta):\n    node = KNode('UMLS:C0015625', label='Fanconi Anemia', type=node_types.DISEASE)\n    rosetta.synonymizer.synonymize(node)\n    print(node.synonyms)\n"]]}
{"hexsha": "552e3e4a969408d74f6713246cdf1d8c919a173f", "ext": "py", "lang": "Python", "content": "def gen_dummy_meta(num_spk, num_utt_per_spk):\n  ''' Generate a dummy data. '''\n  meta = kaldi_dir.KaldiMetaData()\n  for spk_idx in range(num_spk):\n    for utt_idx in range(num_utt_per_spk):\n      spk = str(spk_idx)\n      utt = '%s_%d' % (spk, utt_idx)\n      utt_meta = kaldi_dir.Utt()\n      utt_meta.feat = 'foo/bar/feat/%s' % (utt)\n      utt_meta.vad = 'foo/bar/vad/%s' % (utt)\n      utt_meta.spk = spk\n      meta.utts[utt] = utt_meta\n  meta.collect_spks_from_utts()\n  return meta", "fn_id": 0, "class_fn": false, "repo": "didichuxing/delta", "file": "delta/utils/kaldi/kaldi_dir_utils.py", "last_update_at": "2020-11-15T09:52:09+00:00", "question_id": "552e3e4a969408d74f6713246cdf1d8c919a173f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def gen_dummy_meta(num_spk, num_utt_per_spk):\n    \"\"\" Generate a dummy data. \"\"\"\n    meta = kaldi_dir.KaldiMetaData()\n    for spk_idx in range(num_spk):\n        for utt_idx in range(num_utt_per_spk):\n            spk = str(spk_idx)\n            utt = '%s_%d' % (spk, utt_idx)\n            utt_meta = kaldi_dir.Utt()\n            utt_meta.feat = 'foo/bar/feat/%s' % utt\n            utt_meta.vad = 'foo/bar/vad/%s' % utt\n            utt_meta.spk = spk\n            meta.utts[utt] = utt_meta\n    meta.collect_spks_from_utts()\n"]]}
{"hexsha": "0220ca6c530af678b249eb7ed0726942eed3cd3b", "ext": "py", "lang": "Python", "content": "def palindrom_bul():\n    global islem_sayisi\n    palindrom_sayisi = 0\n\n    # her sat\u0131rda kontrol edilecek.\n    for satir in range(len(list)):\n\n        '''10 ve daha fazla kelime palindrom mu kontrol\u00fc.\n        her s\u00fctuna bak\u0131lacak'''\n        for s\u0131n\u0131r in range(10, len(list[0]) + 1):\n            baslangic = 0\n\n            # sat\u0131r sonuna geldi mi kontrol\u00fc.\n            while (s\u0131n\u0131r != len(list[0]) + 1):\n\n                # polindrom kontrol\u00fc\n                word = \"\".join(list[satir][baslangic:s\u0131n\u0131r])\n                palindrom = str(word) == str(word)[::-1]\n\n                islem_sayisi += 1\n                # palindrom bulundu ise yazd\u0131r.\n                if (palindrom == True):\n                    palindrom_sayisi += 1\n                    print(palindrom, word)\n\n                baslangic += 1\n                s\u0131n\u0131r += 1\n\n    print(\"palindrom say\u0131s\u0131 ->\", palindrom_sayisi)", "fn_id": 2, "class_fn": false, "repo": "bayramcicek/algoritma-analizi", "file": "final/final-1-matris.py", "last_update_at": "2020-03-08T22:22:13+00:00", "question_id": "0220ca6c530af678b249eb7ed0726942eed3cd3b_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def palindrom_bul():\n    global islem_sayisi\n    palindrom_sayisi = 0\n    for satir in range(len(list)):\n        '10 ve daha fazla kelime palindrom mu kontrol\u00fc.\\n        her s\u00fctuna bak\u0131lacak'\n        for s\u0131n\u0131r in range(10, len(list[0]) + 1):\n            baslangic = 0\n            while s\u0131n\u0131r != len(list[0]) + 1:\n                word = ''.join(list[satir][baslangic:s\u0131n\u0131r])\n                palindrom = str(word) == str(word)[::-1]\n                islem_sayisi += 1\n                if palindrom == True:\n                    palindrom_sayisi += 1\n                    print(palindrom, word)\n                baslangic += 1\n                s\u0131n\u0131r += 1\n"]]}
{"hexsha": "6aa4e08ed331f4b42d4c62d5c6a6e50b5168594d", "ext": "py", "lang": "Python", "content": "def test_stabilized_vs_sinkhorn_multidim():\n    # test if stable version matches sinkhorn\n    # for multidimensional inputs\n    n = 100\n\n    # Gaussian distributions\n    a = ot.datasets.make_1D_gauss(n, m=20, s=5)  # m= mean, s= std\n    b1 = ot.datasets.make_1D_gauss(n, m=60, s=8)\n    b2 = ot.datasets.make_1D_gauss(n, m=30, s=4)\n\n    # creating matrix A containing all distributions\n    b = np.vstack((b1, b2)).T\n\n    M = ot.utils.dist0(n)\n    M /= np.median(M)\n    epsilon = 0.1\n    G, log = ot.bregman.sinkhorn(a, b, M, reg=epsilon,\n                                 method=\"sinkhorn_stabilized\",\n                                 log=True)\n    G2, log2 = ot.bregman.sinkhorn(a, b, M, epsilon,\n                                   method=\"sinkhorn\", log=True)\n\n    np.testing.assert_allclose(G, G2)", "fn_id": 10, "class_fn": false, "repo": "panpan2/POT", "file": "test/test_bregman.py", "last_update_at": "2020-05-25T12:34:43+00:00", "question_id": "6aa4e08ed331f4b42d4c62d5c6a6e50b5168594d_10", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_stabilized_vs_sinkhorn_multidim():\n    n = 100\n    a = ot.datasets.make_1D_gauss(n, m=20, s=5)\n    b1 = ot.datasets.make_1D_gauss(n, m=60, s=8)\n    b2 = ot.datasets.make_1D_gauss(n, m=30, s=4)\n    b = np.vstack((b1, b2)).T\n    M = ot.utils.dist0(n)\n    M /= np.median(M)\n    epsilon = 0.1\n    G, log = ot.bregman.sinkhorn(a, b, M, reg=epsilon, method='sinkhorn_stabilized', log=True)\n    G2, log2 = ot.bregman.sinkhorn(a, b, M, epsilon, method='sinkhorn', log=True)\n"]]}
{"hexsha": "0a52b0cdbe643eadbbae2f651d39c93cc2739a49", "ext": "py", "lang": "Python", "content": "def cal_psnr_img(img1, img2):\n    '''\n    Calculate PSNR from two image\n    :param img1: numpy array\n    :param img2: numpy array\n    '''\n    # img1 and img2 have range [0, 255]\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    mse = np.mean((img1 - img2) ** 2)\n        \n    if mse == 0:\n        return float('inf')\n        \n    return 20 * math.log10(255.0 / math.sqrt(mse))", "fn_id": 2, "class_fn": false, "repo": "tnquang1416/frame_interpolation_GAN", "file": "utils/utils.py", "last_update_at": "2020-11-19T08:22:11+00:00", "question_id": "0a52b0cdbe643eadbbae2f651d39c93cc2739a49_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def cal_psnr_img(img1, img2):\n    \"\"\"\n    Calculate PSNR from two image\n    :param img1: numpy array\n    :param img2: numpy array\n    \"\"\"\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    mse = np.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return float('inf')\n"]]}
{"hexsha": "de10d9e95e585f0825d05fe6aa56c200dd42c475", "ext": "py", "lang": "Python", "content": "def _lemma(doc, remove_stop):\n    if remove_stop:\n        lemmatized = [\n            process_token(\n                str(token), token._.iwnlp_lemmas, token.pos_, token.whitespace_\n            )\n            for token in doc\n            if not token.is_stop\n        ]\n    else:\n        lemmatized = [\n            process_token(\n                str(token), token._.iwnlp_lemmas, token.pos_, token.whitespace_\n            )\n            for token in doc\n        ]\n    return \"\".join(lemmatized)", "fn_id": 2, "class_fn": false, "repo": "jfilter/german-lemmatizer-docker", "file": "lemmatize.py", "last_update_at": "2020-10-17T20:53:53+00:00", "question_id": "de10d9e95e585f0825d05fe6aa56c200dd42c475_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _lemma(doc, remove_stop):\n    if remove_stop:\n        lemmatized = [process_token(str(token), token._.iwnlp_lemmas, token.pos_, token.whitespace_) for token in doc if not token.is_stop]\n    else:\n        lemmatized = [process_token(str(token), token._.iwnlp_lemmas, token.pos_, token.whitespace_) for token in doc]\n"]]}
{"hexsha": "22d283140ec3f2cb090cce2dd66da4c79d19bb97", "ext": "py", "lang": "Python", "content": "def grabCaptureRectPerHash(hwnd, tLeft, tTop, tRight, tBottom, needShow=False):\n    setForegroundWindow(hwnd)\n\n    xLeft = getPosX(hwnd, tLeft)\n    yLeft = getPosY(hwnd, tTop)\n    xRight = getPosX(hwnd, tRight)\n    yRight = getPosY(hwnd, tBottom)\n\n    img = ImageGrab.grab(bbox=(xLeft, yLeft, xRight, yRight))\n    phash = imgHash(img, hashSize, highfreq_factor)\n    screenPath = path.getProjectPath()+\"screen\\\\rect_per\\\\\"+phash+\"_\"+str(tLeft) + \\\n        \"_\"+str(tTop) + \"_\" + str(tRight)+\"_\"+str(tBottom) + \".png\"\n    if not os.path.exists(path.getProjectPath()+\"screen\\\\rect_per\"):\n        os.makedirs(path.getProjectPath()+\"screen\\\\rect_per\")\n    img.save(screenPath)\n    if needShow == True:\n        img.show()\n    img.close()", "fn_id": 8, "class_fn": false, "repo": "LuniChang/azur_lane_tools", "file": "core/common/screen.py", "last_update_at": "2020-08-04T06:18:49+00:00", "question_id": "22d283140ec3f2cb090cce2dd66da4c79d19bb97_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def grabCaptureRectPerHash(hwnd, tLeft, tTop, tRight, tBottom, needShow=False):\n    setForegroundWindow(hwnd)\n    xLeft = getPosX(hwnd, tLeft)\n    yLeft = getPosY(hwnd, tTop)\n    xRight = getPosX(hwnd, tRight)\n    yRight = getPosY(hwnd, tBottom)\n    img = ImageGrab.grab(bbox=(xLeft, yLeft, xRight, yRight))\n    phash = imgHash(img, hashSize, highfreq_factor)\n    screenPath = path.getProjectPath() + 'screen\\\\rect_per\\\\' + phash + '_' + str(tLeft) + '_' + str(tTop) + '_' + str(tRight) + '_' + str(tBottom) + '.png'\n    if not os.path.exists(path.getProjectPath() + 'screen\\\\rect_per'):\n        os.makedirs(path.getProjectPath() + 'screen\\\\rect_per')\n    img.save(screenPath)\n    if needShow == True:\n        img.show()\n"]]}
{"hexsha": "17e01b873e44a92bf8df48fb793eb8c6946bfd05", "ext": "py", "lang": "Python", "content": "def start_cli():\n    topo = create_topo()\n    net = Mininet(topo=topo)\n    net.start()\n    dumpNodeConnections(net.hosts)\n    #s1_2, s1_3, s1_4, s1_5, s1_6, s2, s3, s4, s5, s6, h1, h2, h3, h4, h5, h6 = net.get('s1_2', 's1_3', 's1_4', 's1_5', 's1_6', 's2', 's3', 's4', 's5', 's6', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6')\n    net.pingAll()\n    #net.configLinkStatus('s1_2', 's2', 'down')\n    #net.configLinkStatus('s1_3', 's3', 'down')\n    #net.configLinkStatus('s1_4', 's4', 'down')\n    #net.configLinkStatus('s1_5', 's5', 'down')\n    #net.configLinkStatus('s1_6', 's6', 'down')\n    #net.pingAll()\n    h2.cmd('python3 modbus_server.py &')\n    h3.cmd('python3 modbus_server.py &')\n    h4.cmd('python3 modbus_server.py &')\n    h5.cmd('python3 modbus_server.py &')\n    h6.cmd('python3 modbus_server.py &')\n    h1.cmd('tcpdump -i h1-eth0 -w h1_multi.pcap &')\n    h2.cmd('tcpdump -i h2-eth0 -w h2_multi.pcap &')\n    h3.cmd('tcpdump -i h3-eth0 -w h3_multi.pcap &')\n    h4.cmd('tcpdump -i h4-eth0 -w h4_multi.pcap &')\n    h5.cmd('tcpdump -i h5-eth0 -w h5_multi.pcap &')\n    h6.cmd('tcpdump -i h6-eth0 -w h6_multi.pcap &')\n    #h1.cmd('python3 modbus_client.py h2')\n    #h1.cmd('python3 modbus_client.py h3')\n    #h1.cmd('python3 modbus_client.py h4')\n    #h1.cmd('python3 modbus_client.py h5')\n    #h1.cmd('python3 modbus_client.py h6')\n\n    CLI(net)\n    net.stop()", "fn_id": 0, "class_fn": false, "repo": "brytul/IoT_Sec_Gateway", "file": "IoTAutoPatch/sim/topo_multi.py", "last_update_at": "2020-09-24T15:53:32+00:00", "question_id": "17e01b873e44a92bf8df48fb793eb8c6946bfd05_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def start_cli():\n    topo = create_topo()\n    net = Mininet(topo=topo)\n    net.start()\n    dumpNodeConnections(net.hosts)\n    net.pingAll()\n    h2.cmd('python3 modbus_server.py &')\n    h3.cmd('python3 modbus_server.py &')\n    h4.cmd('python3 modbus_server.py &')\n    h5.cmd('python3 modbus_server.py &')\n    h6.cmd('python3 modbus_server.py &')\n    h1.cmd('tcpdump -i h1-eth0 -w h1_multi.pcap &')\n    h2.cmd('tcpdump -i h2-eth0 -w h2_multi.pcap &')\n    h3.cmd('tcpdump -i h3-eth0 -w h3_multi.pcap &')\n    h4.cmd('tcpdump -i h4-eth0 -w h4_multi.pcap &')\n    h5.cmd('tcpdump -i h5-eth0 -w h5_multi.pcap &')\n    h6.cmd('tcpdump -i h6-eth0 -w h6_multi.pcap &')\n    CLI(net)\n"]]}
{"hexsha": "a3caf21b9bd90bd1f45542f3e58da9656c86d898", "ext": "py", "lang": "Python", "content": "def namespace_details(request, namespace_id):\n    if not request.user.is_superuser or not request.user.is_staff:\n        raise PermissionDenied\n\n    namespace = StatisticNamespace.get(namespace_id)\n\n    return render_to_response('generic_list.html', {\n        'object': namespace,\n        'namespace': namespace,\n        'object_list': namespace.statistics,\n        'hide_link': True,\n        'title': _(u'namespace details for: %s') % namespace,\n        'object_name': _(u'namespace'),\n    }, context_instance=RequestContext(request))", "fn_id": 1, "class_fn": false, "repo": "Dave360-crypto/mayan-edms", "file": "mayan/apps/statistics/views.py", "last_update_at": "2020-10-20T03:52:21+00:00", "question_id": "a3caf21b9bd90bd1f45542f3e58da9656c86d898_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def namespace_details(request, namespace_id):\n    if not request.user.is_superuser or not request.user.is_staff:\n        raise PermissionDenied\n    namespace = StatisticNamespace.get(namespace_id)\n"]]}
{"hexsha": "d542abf61ad1a206d0eafc9e06cad3b3673edccc", "ext": "py", "lang": "Python", "content": "def plot_related_features(spn, featureId_x, featureId_y, detail=100, dictionary=None, evidence=None, fname=None):\n    \"\"\"\n    Plots a 2d representation of the joint marginal probability of these two\n    features.\n\n    :param spn: the root node of the spn\n    :param featureId_x: featureid of the first feature\n    :param featureId_y: featureid of the second feature\n    :param detail: granularity of the plotting grid\n    :param dictionary: the data dictionary to extract meta information\n    :param evidence: evidence to condition the plot on\n    :param fname: file name to save the resulting plot\n    :return: a plotly dictionary containing data and context\n    \"\"\"\n\n    # construct the grid\n    num_features = len(spn.scope)\n    context = dictionary['context']\n    categoricals = get_categoricals(spn, context)\n    domain_x = context.get_domains_by_scope([featureId_x])[0]\n    domain_y = context.get_domains_by_scope([featureId_y])[0]\n    feature_names = context.feature_names\n    x_range = (domain_x[0],\n               domain_x[-1])\n    y_range = (domain_y[0],\n               domain_y[-1])\n    x_detail = detail\n    y_detail = detail\n    x_cat = False\n    y_cat = False\n    if featureId_x in categoricals:\n        x_cat = True\n        x_detail = len(domain_x)\n        enc = dictionary['features'][featureId_x]['encoder'].inverse_transform\n        x_range = domain_x\n        x_names = enc([int(x) for x in x_range])\n    if featureId_y in categoricals:\n        y_cat = True\n        y_detail = len(domain_y)\n        enc = dictionary['features'][featureId_y]['encoder'].inverse_transform\n        y_range = domain_y\n        y_names = enc([int(y) for y in y_range])\n    grid = np.mgrid[x_range[0]:x_range[-1]:x_detail*1j, y_range[0]:y_range[-1]:y_detail*1j]\n    grid = grid.reshape(2,-1).T\n\n    # construct query\n    query = np.zeros((1,num_features))\n    query[:] = np.nan\n    query = np.repeat(query, grid.shape[0], axis=0)\n    query[:, featureId_x] = grid[:, 0]\n    query[:, featureId_y] = grid[:, 1]\n\n    # calculate the probability and shape the array\n    result = likelihood(spn, query)\n\n    result.shape = (x_detail, y_detail)\n    \n    # plot\n    data = [Heatmap(z=result,\n            x=np.linspace(domain_y[0], domain_y[-1], y_detail) if not y_cat else y_names,\n            y=np.linspace(domain_x[0], domain_x[-1], x_detail) if not x_cat else x_names,\n            colorbar=ColorBar(\n                title='Colorbar'\n            ),\n            colorscale='Hot')]\n    layout = dict(width=450, \n                  height=450,\n                  xaxis=dict(title=feature_names[featureId_y], autotick=True),\n                  yaxis=dict(title=feature_names[featureId_x], autotick=True)\n                 )\n\n    if fname is None:\n        return {'data': data, 'layout': layout}\n    else:\n        raise NotImplementedError", "fn_id": 1, "class_fn": false, "repo": "Iliricon/DeepNotebooks", "file": "src/dn_plot.py", "last_update_at": "2020-04-27T02:39:50+00:00", "question_id": "d542abf61ad1a206d0eafc9e06cad3b3673edccc_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def plot_related_features(spn, featureId_x, featureId_y, detail=100, dictionary=None, evidence=None, fname=None):\n    \"\"\"\n    Plots a 2d representation of the joint marginal probability of these two\n    features.\n\n    :param spn: the root node of the spn\n    :param featureId_x: featureid of the first feature\n    :param featureId_y: featureid of the second feature\n    :param detail: granularity of the plotting grid\n    :param dictionary: the data dictionary to extract meta information\n    :param evidence: evidence to condition the plot on\n    :param fname: file name to save the resulting plot\n    :return: a plotly dictionary containing data and context\n    \"\"\"\n    num_features = len(spn.scope)\n    context = dictionary['context']\n    categoricals = get_categoricals(spn, context)\n    domain_x = context.get_domains_by_scope([featureId_x])[0]\n    domain_y = context.get_domains_by_scope([featureId_y])[0]\n    feature_names = context.feature_names\n    x_range = (domain_x[0], domain_x[-1])\n    y_range = (domain_y[0], domain_y[-1])\n    x_detail = detail\n    y_detail = detail\n    x_cat = False\n    y_cat = False\n    if featureId_x in categoricals:\n        x_cat = True\n        x_detail = len(domain_x)\n        enc = dictionary['features'][featureId_x]['encoder'].inverse_transform\n        x_range = domain_x\n        x_names = enc([int(x) for x in x_range])\n    if featureId_y in categoricals:\n        y_cat = True\n        y_detail = len(domain_y)\n        enc = dictionary['features'][featureId_y]['encoder'].inverse_transform\n        y_range = domain_y\n        y_names = enc([int(y) for y in y_range])\n    grid = np.mgrid[x_range[0]:x_range[-1]:x_detail * 1j, y_range[0]:y_range[-1]:y_detail * 1j]\n    grid = grid.reshape(2, -1).T\n    query = np.zeros((1, num_features))\n    query[:] = np.nan\n    query = np.repeat(query, grid.shape[0], axis=0)\n    query[:, featureId_x] = grid[:, 0]\n    query[:, featureId_y] = grid[:, 1]\n    result = likelihood(spn, query)\n    result.shape = (x_detail, y_detail)\n    data = [Heatmap(z=result, x=np.linspace(domain_y[0], domain_y[-1], y_detail) if not y_cat else y_names, y=np.linspace(domain_x[0], domain_x[-1], x_detail) if not x_cat else x_names, colorbar=ColorBar(title='Colorbar'), colorscale='Hot')]\n    layout = dict(width=450, height=450, xaxis=dict(title=feature_names[featureId_y], autotick=True), yaxis=dict(title=feature_names[featureId_x], autotick=True))\n    if fname is None:\n        return {'data': data, 'layout': layout}\n    else:\n"]]}
{"hexsha": "210f4184067b1376ef51ab6ca1f142db872e3747", "ext": "py", "lang": "Python", "content": "def get_schemas():\n    schemas = list()\n    schemas.append(hello_tablelike.generate_schema())\n    schemas.append(hello_tablelike_sub.generate_schema())\n    return schemas", "fn_id": 0, "class_fn": false, "repo": "KnowEnG/platform", "file": "nest_py/hello_world/data_types/hw_schemas.py", "last_update_at": "2020-07-31T03:19:51+00:00", "question_id": "210f4184067b1376ef51ab6ca1f142db872e3747_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_schemas():\n    schemas = list()\n    schemas.append(hello_tablelike.generate_schema())\n    schemas.append(hello_tablelike_sub.generate_schema())\n"]]}
{"hexsha": "eed2efc4148dbb9731bc5c658e05c97127ecf2a3", "ext": "py", "lang": "Python", "content": "def distlineline(line1, line2, extend_line1=False, extend_line2=False):\n    from .line import asline\n    line1 = asline(line1)\n    line2 = asline(line2)\n    x1a = line1.pt1[0]\n    y1a = line1.pt1[1]\n    z1a = line1.pt1[2]\n    x1b = line1.pt2[0]\n    y1b = line1.pt2[1]\n    z1b = line1.pt2[2]\n    x2a = line2.pt1[0]\n    y2a = line2.pt1[1]\n    z2a = line2.pt1[2]\n    x2b = line2.pt2[0]\n    y2b = line2.pt2[1]\n    z2b = line2.pt2[2]\n    C1 = ((x2b - x2a) * (x1b - x1a) +\n          (y2b - y2a) * (y1b - y1a) +\n          (z2b - z2a) * (z1b - z1a))\n    C2 = (x1b - x1a)**2 + (y1b - y1a)**2 + (z1b - z1a)**2\n    C3 = ((x1a * (-x1a + x2a + x1b) - x1b * x2a) +\n          (y1a * (-y1a + y2a + y1b) - y1b * y2a) +\n          (z1a * (-z1a + z2a + z1b) - z1b * z2a))\n    C4 = (x2b - x2a)**2 + (y2b - y2a)**2 + (z2b - z2a)**2\n    C5 = ((x2a * (x2a - x1a - x2b) + x1a * x2b) +\n          (y2a * (y2a - y1a - y2b) + y1a * y2b) +\n          (z2a * (z2a - z1a - z2b) + z1a * z2b))\n    if angle2lines(line1, line2) < 0.01:\n        dist1 = distlinept(line1, line2.pt1, extend_line1)\n        dist2 = distlinept(line2, line1.pt1, extend_line2)\n        return min(dist1, dist2)\n    else:\n        t = (C1 * C5 - C4 * C3) / (C2 * C4 - C1**2)\n        u = (C2 * C5 - C1 * C3) / (C2 * C4 - C1**2)\n        if not extend_line1:\n            t = min(t, 1)\n            t = max(t, 0)\n        if not extend_line2:\n            u = min(u, 1)\n            u = max(u, 0)\n        return distptpt(line1.pt(t), line2.pt(u))", "fn_id": 5, "class_fn": false, "repo": "saullocastro/alg3dpy", "file": "alg3dpy/distances.py", "last_update_at": "2020-03-15T21:37:33+00:00", "question_id": "eed2efc4148dbb9731bc5c658e05c97127ecf2a3_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def distlineline(line1, line2, extend_line1=False, extend_line2=False):\n    from .line import asline\n    line1 = asline(line1)\n    line2 = asline(line2)\n    x1a = line1.pt1[0]\n    y1a = line1.pt1[1]\n    z1a = line1.pt1[2]\n    x1b = line1.pt2[0]\n    y1b = line1.pt2[1]\n    z1b = line1.pt2[2]\n    x2a = line2.pt1[0]\n    y2a = line2.pt1[1]\n    z2a = line2.pt1[2]\n    x2b = line2.pt2[0]\n    y2b = line2.pt2[1]\n    z2b = line2.pt2[2]\n    C1 = (x2b - x2a) * (x1b - x1a) + (y2b - y2a) * (y1b - y1a) + (z2b - z2a) * (z1b - z1a)\n    C2 = (x1b - x1a) ** 2 + (y1b - y1a) ** 2 + (z1b - z1a) ** 2\n    C3 = x1a * (-x1a + x2a + x1b) - x1b * x2a + (y1a * (-y1a + y2a + y1b) - y1b * y2a) + (z1a * (-z1a + z2a + z1b) - z1b * z2a)\n    C4 = (x2b - x2a) ** 2 + (y2b - y2a) ** 2 + (z2b - z2a) ** 2\n    C5 = x2a * (x2a - x1a - x2b) + x1a * x2b + (y2a * (y2a - y1a - y2b) + y1a * y2b) + (z2a * (z2a - z1a - z2b) + z1a * z2b)\n    if angle2lines(line1, line2) < 0.01:\n        dist1 = distlinept(line1, line2.pt1, extend_line1)\n        dist2 = distlinept(line2, line1.pt1, extend_line2)\n        return min(dist1, dist2)\n    else:\n        t = (C1 * C5 - C4 * C3) / (C2 * C4 - C1 ** 2)\n        u = (C2 * C5 - C1 * C3) / (C2 * C4 - C1 ** 2)\n        if not extend_line1:\n            t = min(t, 1)\n            t = max(t, 0)\n        if not extend_line2:\n            u = min(u, 1)\n            u = max(u, 0)\n"]]}
{"hexsha": "7b51f5033b7fb5ec575deb4c087d1c4e4eb32f93", "ext": "py", "lang": "Python", "content": "def construct_payload(pipeline_name: str, pipeline_state: str, pipeline_stage: str):\n    payload = {\n        \"state\": \"error\",\n        \"context\": \"default\"\n    }\n    codepipeline_url = \"https://{region}.console.aws.amazon.com/codepipeline/home?region={region}#/view/{pipeline_name}\"\n    payload[\"context\"] = pipeline_stage\n    payload[\"target_url\"] = codepipeline_url.format(region=\"us-east-1\",pipeline_name=pipeline_name)\n    if pipeline_state == \"STARTED\":\n        payload[\"state\"] = \"pending\"\n        payload[\"description\"] = \"Running \" + pipeline_stage\n    if pipeline_state == \"FAILED\":\n        payload[\"state\"] = \"failure\"\n        payload[\"description\"] = \"Failed to run \" + pipeline_stage\n    if pipeline_state == \"SUCCEEDED\":\n        payload[\"state\"] = \"success\"\n        payload[\"description\"] = \"Successfully ran \" + pipeline_stage\n    return payload", "fn_id": 1, "class_fn": false, "repo": "rearc/terraform-aws-codepipeline-status", "file": "codepipeline-status-reporter/lambda_function.py", "last_update_at": "2020-05-22T12:04:16+00:00", "question_id": "7b51f5033b7fb5ec575deb4c087d1c4e4eb32f93_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def construct_payload(pipeline_name: str, pipeline_state: str, pipeline_stage: str):\n    payload = {'state': 'error', 'context': 'default'}\n    codepipeline_url = 'https://{region}.console.aws.amazon.com/codepipeline/home?region={region}#/view/{pipeline_name}'\n    payload['context'] = pipeline_stage\n    payload['target_url'] = codepipeline_url.format(region='us-east-1', pipeline_name=pipeline_name)\n    if pipeline_state == 'STARTED':\n        payload['state'] = 'pending'\n        payload['description'] = 'Running ' + pipeline_stage\n    if pipeline_state == 'FAILED':\n        payload['state'] = 'failure'\n        payload['description'] = 'Failed to run ' + pipeline_stage\n    if pipeline_state == 'SUCCEEDED':\n        payload['state'] = 'success'\n        payload['description'] = 'Successfully ran ' + pipeline_stage\n"]]}
{"hexsha": "04d4a0d0403cc682e167109d526d0965af180363", "ext": "py", "lang": "Python", "content": "@eel.expose\ndef set_known_folder(folder):\n    if folder:\n        try:\n            global sr\n            sr = FaceRecogniser(folder)\n            if sr:\n                return \"Success initializing\"\n        except Exception as ex:\n            return f\"Error initializing -- {ex}\"\n\n    else:\n        return \"Error initializing -- No folder\"", "fn_id": 0, "class_fn": false, "repo": "StormInside/Simple-Face-Recognition-Program", "file": "Start_interface.py", "last_update_at": "2020-10-06T09:02:24+00:00", "question_id": "04d4a0d0403cc682e167109d526d0965af180363_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@eel.expose\ndef set_known_folder(folder):\n    if folder:\n        try:\n            global sr\n            sr = FaceRecogniser(folder)\n            if sr:\n                return 'Success initializing'\n        except Exception as ex:\n            return f'Error initializing -- {ex}'\n    else:\n"]]}
{"hexsha": "775e151e6316a16f75b1eb8a5a671b8d74cdea9f", "ext": "py", "lang": "Python", "content": "def profile_information(profile):\n    \"\"\"\n    Returns the profile information about a given connection\n    \n    =====================================================================     ====================================================================\n    **Parameter**                                                             **Description**\n    ---------------------------------------------------------------------     --------------------------------------------------------------------           \n    profile                                                                   Required String. The name of the profile to get the information about.\n    =====================================================================     ====================================================================\n    \n    :returns: Dict\n    \n    \"\"\"\n    home_dir = str(Path.home())\n    profile_file = os.path.join(home_dir, \".arcgisprofile\")\n    if os.path.isfile(profile_file):\n        config = configparser.ConfigParser()\n        config.read(profile_file)\n        keys = config.options(profile)\n        values = {}\n        for key in keys:\n            try:\n                if key == 'date_modified':\n                    import datetime as _datetime\n                    values[key] = _datetime.datetime.strptime(\n                        config.get(\n                            profile, \n                            key), \n                        \"%Y-%m-%d %H:%M:%S.%f\")\n                else:\n                    values[key] = config.get(profile, key)\n            except:\n                values[key] = None\n        return values\n    return None  ", "fn_id": 1, "class_fn": false, "repo": "achapkowski/profile_tools", "file": "arcgisprofile.py", "last_update_at": "2020-04-22T11:39:05+00:00", "question_id": "775e151e6316a16f75b1eb8a5a671b8d74cdea9f_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def profile_information(profile):\n    \"\"\"\n    Returns the profile information about a given connection\n    \n    =====================================================================     ====================================================================\n    **Parameter**                                                             **Description**\n    ---------------------------------------------------------------------     --------------------------------------------------------------------           \n    profile                                                                   Required String. The name of the profile to get the information about.\n    =====================================================================     ====================================================================\n    \n    :returns: Dict\n    \n    \"\"\"\n    home_dir = str(Path.home())\n    profile_file = os.path.join(home_dir, '.arcgisprofile')\n    if os.path.isfile(profile_file):\n        config = configparser.ConfigParser()\n        config.read(profile_file)\n        keys = config.options(profile)\n        values = {}\n        for key in keys:\n            try:\n                if key == 'date_modified':\n                    import datetime as _datetime\n                    values[key] = _datetime.datetime.strptime(config.get(profile, key), '%Y-%m-%d %H:%M:%S.%f')\n                else:\n                    values[key] = config.get(profile, key)\n            except:\n                values[key] = None\n        return values\n"]]}
{"hexsha": "e9e1490a0f09f2b18bb81b050e808323b4018b62", "ext": "py", "lang": "Python", "content": "def parse_industry_code_dict(industry):\n    data_container = {}\n    for order_book_id, industry_dict in industry.items():\n        industry_container = {}\n        for criterion, industry_code_dict in industry_dict.items():\n            industry_container[criterion] = industry_code_dict[\"industry_code\"]\n        data_container[order_book_id] = industry_container\n    data_df = pd.DataFrame(data_container).transpose()\n    data_df.index.name = \"order_book_id\"\n    return data_df", "fn_id": 0, "class_fn": false, "repo": "StateOfTheArt-quant/xqdata", "file": "xqdata/base_data_source/default_data_source.py", "last_update_at": "2020-06-30T07:57:43+00:00", "question_id": "e9e1490a0f09f2b18bb81b050e808323b4018b62_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def parse_industry_code_dict(industry):\n    data_container = {}\n    for order_book_id, industry_dict in industry.items():\n        industry_container = {}\n        for criterion, industry_code_dict in industry_dict.items():\n            industry_container[criterion] = industry_code_dict['industry_code']\n        data_container[order_book_id] = industry_container\n    data_df = pd.DataFrame(data_container).transpose()\n    data_df.index.name = 'order_book_id'\n"]]}
{"hexsha": "92f69a35a83d10885396fe2decc7334caa06e8d4", "ext": "py", "lang": "Python", "content": "def valid_or_test(mode, perf_dict=None):\n    if perf_dict is None:\n        perf_dict = {'accuracy': [], 'f1': {'naf': [], 'af': []}}\n    model.eval()\n    val_loss = 0\n    correct = 0\n    pred_list = []\n    label_list = []\n    with torch.no_grad():\n        for batch_idx, (data, target, feature, _, _, feature_rep) in enumerate(val_loader):\n            data, target, feature, feature_rep = data.to(device), target.to(device), feature.to(device), feature_rep.to(device)\n            logits, _, gap = model(data, feature_rep)\n\n            hsic_loss = independence_criterion.calc_loss(gap, feature)\n\n            loss = classification_criterion(logits, target) + hsic_loss\n\n            val_loss += loss.item()\n\n            _, predicted = torch.max(logits.data, 1)\n\n            if torch.cuda.is_available():\n                correct += (predicted.detach().cpu().numpy() == target.detach().cpu().numpy()).sum()\n            else:\n                correct += (predicted == target).sum()\n\n            pred_list.append(predicted.detach().cpu().numpy())\n            label_list.append(target.detach().cpu().numpy())\n\n    preds = np.concatenate(pred_list, axis=0)\n    labels = np.concatenate(label_list, axis=0)\n\n    f1_total = f1_score(labels, preds, labels=[0, 1], average=None)\n\n    val_loss /= len(val_loader.dataset)\n    epoch_accuracy = 100 * float(correct) / val_loader.dataset.__len__()\n\n    perf_dict['accuracy'].append(epoch_accuracy)\n    perf_dict['f1']['naf'].append(f1_total[0])\n    perf_dict['f1']['af'].append(f1_total[1])\n\n    if mode == 'valid' and epoch_accuracy >= np.max(perf_dict['accuracy']):\n        torch.save(model.state_dict(), os.path.join(file_dir, f'{exp_name}_params.pkl'))\n        print(['Saved @  ' + str(epoch_accuracy) + '%'])\n\n    print(f'====> {mode} set loss: {val_loss:.5f}')\n    print(f'{mode} accuracy: {epoch_accuracy:.4f}')\n    print(f'{mode} F1: {f1_total[0]}, {f1_total[1]}')\n\n    if mode == 'test':\n        with open(os.path.join(file_dir, f'{exp_name}_test_perf.pkl'), 'wb') as handle:\n            pkl.dump(perf_dict, handle, protocol=pkl.HIGHEST_PROTOCOL)\n\n    return perf_dict", "fn_id": 1, "class_fn": false, "repo": "vishalbelsare/deep-scientific-discovery", "file": "ECG/train_main_task.py", "last_update_at": "2020-11-18T18:13:03+00:00", "question_id": "92f69a35a83d10885396fe2decc7334caa06e8d4_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def valid_or_test(mode, perf_dict=None):\n    if perf_dict is None:\n        perf_dict = {'accuracy': [], 'f1': {'naf': [], 'af': []}}\n    model.eval()\n    val_loss = 0\n    correct = 0\n    pred_list = []\n    label_list = []\n    with torch.no_grad():\n        for batch_idx, (data, target, feature, _, _, feature_rep) in enumerate(val_loader):\n            data, target, feature, feature_rep = (data.to(device), target.to(device), feature.to(device), feature_rep.to(device))\n            logits, _, gap = model(data, feature_rep)\n            hsic_loss = independence_criterion.calc_loss(gap, feature)\n            loss = classification_criterion(logits, target) + hsic_loss\n            val_loss += loss.item()\n            _, predicted = torch.max(logits.data, 1)\n            if torch.cuda.is_available():\n                correct += (predicted.detach().cpu().numpy() == target.detach().cpu().numpy()).sum()\n            else:\n                correct += (predicted == target).sum()\n            pred_list.append(predicted.detach().cpu().numpy())\n            label_list.append(target.detach().cpu().numpy())\n    preds = np.concatenate(pred_list, axis=0)\n    labels = np.concatenate(label_list, axis=0)\n    f1_total = f1_score(labels, preds, labels=[0, 1], average=None)\n    val_loss /= len(val_loader.dataset)\n    epoch_accuracy = 100 * float(correct) / val_loader.dataset.__len__()\n    perf_dict['accuracy'].append(epoch_accuracy)\n    perf_dict['f1']['naf'].append(f1_total[0])\n    perf_dict['f1']['af'].append(f1_total[1])\n    if mode == 'valid' and epoch_accuracy >= np.max(perf_dict['accuracy']):\n        torch.save(model.state_dict(), os.path.join(file_dir, f'{exp_name}_params.pkl'))\n        print(['Saved @  ' + str(epoch_accuracy) + '%'])\n    print(f'====> {mode} set loss: {val_loss:.5f}')\n    print(f'{mode} accuracy: {epoch_accuracy:.4f}')\n    print(f'{mode} F1: {f1_total[0]}, {f1_total[1]}')\n    if mode == 'test':\n        with open(os.path.join(file_dir, f'{exp_name}_test_perf.pkl'), 'wb') as handle:\n            pkl.dump(perf_dict, handle, protocol=pkl.HIGHEST_PROTOCOL)\n"]]}
{"hexsha": "be9e2dd0898164c46a7279e606ef50771da3efeb", "ext": "py", "lang": "Python", "content": "def load_yaml(path):\n    # Fix yaml numbers https://stackoverflow.com/a/30462009/11037553\n    loader = yaml.SafeLoader\n    loader.add_implicit_resolver(\n        u'tag:yaml.org,2002:float',\n        re.compile(u'''^(?:\n         [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)?\n        |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)\n        |\\\\.[0-9_]+(?:[eE][-+][0-9]+)?\n        |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\\\.[0-9_]*\n        |[-+]?\\\\.(?:inf|Inf|INF)\n        |\\\\.(?:nan|NaN|NAN))$''', re.X),\n        list(u'-+0123456789.'))\n    with open(preprocess_paths(path), \"r\", encoding=\"utf-8\") as file:\n        return yaml.load(file, Loader=loader)", "fn_id": 0, "class_fn": false, "repo": "Honghe/TensorFlowASR", "file": "tensorflow_asr/configs/user_config.py", "last_update_at": "2020-10-20T11:42:08+00:00", "question_id": "be9e2dd0898164c46a7279e606ef50771da3efeb_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def load_yaml(path):\n    loader = yaml.SafeLoader\n    loader.add_implicit_resolver(u'tag:yaml.org,2002:float', re.compile(u'^(?:\\n         [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)?\\n        |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)\\n        |\\\\.[0-9_]+(?:[eE][-+][0-9]+)?\\n        |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\\\.[0-9_]*\\n        |[-+]?\\\\.(?:inf|Inf|INF)\\n        |\\\\.(?:nan|NaN|NAN))$', re.X), list(u'-+0123456789.'))\n    with open(preprocess_paths(path), 'r', encoding='utf-8') as file:\n"]]}
{"hexsha": "6c15f275758549ce70287b55bb9e3ca32cb95422", "ext": "py", "lang": "Python", "content": "@classmethod\ndef Set(cls, instance, overwrite=False):\n  assert cls._instance is None or overwrite is True\n  assert isinstance(instance, cls)\n  cls._instance = instance", "fn_id": 0, "class_fn": false, "repo": "igushev/fase_lib", "file": "base_util/singleton_util.py", "last_update_at": "2020-01-10T05:30:48+00:00", "question_id": "6c15f275758549ce70287b55bb9e3ca32cb95422_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@classmethod\ndef Set(cls, instance, overwrite=False):\n    assert cls._instance is None or overwrite is True\n    assert isinstance(instance, cls)\n"]]}
{"hexsha": "f1aca00692c3fb5a0596a1389db9f9bc57ccfc63", "ext": "py", "lang": "Python", "content": "def adjust_params_names_group(params = ['v', 'a', 'w'], \n                              params_bounds = [], \n                              params_bounds_epsilon= [],\n                              param_varies = [0, 0, 1],\n                              n_subjects = 3):\n    params_adj = []\n    params_bounds_adj = []\n    params_bounds_epsilon_adj = []\n    cnt = 0\n    for p in params:\n        if param_varies[cnt]:\n            for i in range(n_subjects):\n                params_adj.append(p + '_' + str(i))\n                params_bounds_adj.append(params_bounds[cnt])\n                params_bounds_epsilon_adj.append(params_bounds_epsilon[cnt])\n        else:\n            params_adj.append(p)\n            params_bounds_adj.append(params_bounds[cnt])\n            params_bounds_epsilon_adj.append(params_bounds_epsilon[cnt])\n        cnt += 1\n    return params_adj, params_bounds_adj, params_bounds_epsilon_adj", "fn_id": 1, "class_fn": false, "repo": "prashantramnani/nn_likelihoods", "file": "deprecated/kde_mle_parallel.py", "last_update_at": "2020-03-13T12:47:23+00:00", "question_id": "f1aca00692c3fb5a0596a1389db9f9bc57ccfc63_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def adjust_params_names_group(params=['v', 'a', 'w'], params_bounds=[], params_bounds_epsilon=[], param_varies=[0, 0, 1], n_subjects=3):\n    params_adj = []\n    params_bounds_adj = []\n    params_bounds_epsilon_adj = []\n    cnt = 0\n    for p in params:\n        if param_varies[cnt]:\n            for i in range(n_subjects):\n                params_adj.append(p + '_' + str(i))\n                params_bounds_adj.append(params_bounds[cnt])\n                params_bounds_epsilon_adj.append(params_bounds_epsilon[cnt])\n        else:\n            params_adj.append(p)\n            params_bounds_adj.append(params_bounds[cnt])\n            params_bounds_epsilon_adj.append(params_bounds_epsilon[cnt])\n        cnt += 1\n"]]}
{"hexsha": "728beb48a1989afc90b2c0545ddb4a22fe2ec79b", "ext": "py", "lang": "Python", "content": "def PDec2DMS (dec):\n    \"\"\" Convert a declination in degrees to degrees, min, seconds\n\n    dec  = Declination in deg.\n    \"\"\"\n    ################################################################\n    p = math.fabs(dec)\n    if dec>0.0:\n        sgn = \" \"\n    else:\n        sgn = \"-\"\n    d = int (p)\n    p = (p - d) * 60.0\n    m = int(p)\n    s = (p - m) * 60.0\n    out = \"%s%2.2d %2d %7.4f \" % (sgn, d,m,s)\n    return out", "fn_id": 10, "class_fn": false, "repo": "sarrvesh/Obit", "file": "ObitSystem/Obit/python/UVDesc.py", "last_update_at": "2020-10-20T01:08:59+00:00", "question_id": "728beb48a1989afc90b2c0545ddb4a22fe2ec79b_10", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def PDec2DMS(dec):\n    \"\"\" Convert a declination in degrees to degrees, min, seconds\n\n    dec  = Declination in deg.\n    \"\"\"\n    p = math.fabs(dec)\n    if dec > 0.0:\n        sgn = ' '\n    else:\n        sgn = '-'\n    d = int(p)\n    p = (p - d) * 60.0\n    m = int(p)\n    s = (p - m) * 60.0\n    out = '%s%2.2d %2d %7.4f ' % (sgn, d, m, s)\n"]]}
{"hexsha": "6a53ef4afee44ed94a9ea479580eb834f4e6045d", "ext": "py", "lang": "Python", "content": "def find_scope(current_phil, scope_name):\n  i = 0\n  while (i < len(current_phil.objects)):\n    full_path = current_phil.objects[i].full_path()\n    if (full_path == scope_name):\n      return current_phil.objects[i]\n    elif (scope_name.startswith(full_path + \".\")):\n      return find_scope(current_phil.objects[i], scope_name)\n    i += 1", "fn_id": 23, "class_fn": false, "repo": "ErwinP/cctbx_project", "file": "libtbx/phil/__init__.py", "last_update_at": "2020-02-04T15:39:06+00:00", "question_id": "6a53ef4afee44ed94a9ea479580eb834f4e6045d_23", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def find_scope(current_phil, scope_name):\n    i = 0\n    while i < len(current_phil.objects):\n        full_path = current_phil.objects[i].full_path()\n        if full_path == scope_name:\n            return current_phil.objects[i]\n        elif scope_name.startswith(full_path + '.'):\n            return find_scope(current_phil.objects[i], scope_name)\n"]]}
{"hexsha": "9f82107efc808c49ec52025faefacdfb9a2bee12", "ext": "py", "lang": "Python", "content": "def form_another_2d_array(df_list):\n    new_list = np.zeros(len(df_list)+1)\n    final_table = np.array([trisect_ages(x_df) for x_df in df_list])\n\n    \n    return final_table", "fn_id": 3, "class_fn": false, "repo": "karenkathryn/opportunity_youth", "file": "src/utilities/utilities.py", "last_update_at": "2020-03-07T22:47:24+00:00", "question_id": "9f82107efc808c49ec52025faefacdfb9a2bee12_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def form_another_2d_array(df_list):\n    new_list = np.zeros(len(df_list) + 1)\n    final_table = np.array([trisect_ages(x_df) for x_df in df_list])\n"]]}
{"hexsha": "68dae762b1051faf827e121f6af8f11505a356e5", "ext": "py", "lang": "Python", "content": "def _MakeEagerLogicalBlob(op_attribute, obn, blob_register):\n    lbi = op_attribute.arg_signature.bn_in_op2lbi[obn]\n    blob_object = blob_register.GetObject4BlobName(\n        \"%s/%s\" % (lbi.op_name, lbi.blob_name)\n    )\n    mirrored_sig_map = op_attribute.mirrored_signature.bn_in_op2opt_mirrored_parallel\n    if mirrored_sig_map[obn].HasField(\"mirrored_parallel\"):\n        return remote_blob_util.EagerMirroredBlob(lbi, blob_object)\n    else:\n        return remote_blob_util.EagerConsistentBlob(lbi, blob_object)", "fn_id": 6, "class_fn": false, "repo": "Sodu-Qinming/Oneflow", "file": "oneflow/python/eager/op_executor.py", "last_update_at": "2020-10-22T09:42:15+00:00", "question_id": "68dae762b1051faf827e121f6af8f11505a356e5_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _MakeEagerLogicalBlob(op_attribute, obn, blob_register):\n    lbi = op_attribute.arg_signature.bn_in_op2lbi[obn]\n    blob_object = blob_register.GetObject4BlobName('%s/%s' % (lbi.op_name, lbi.blob_name))\n    mirrored_sig_map = op_attribute.mirrored_signature.bn_in_op2opt_mirrored_parallel\n    if mirrored_sig_map[obn].HasField('mirrored_parallel'):\n        return remote_blob_util.EagerMirroredBlob(lbi, blob_object)\n    else:\n"]]}
{"hexsha": "ce04b0b29c2afca534c792a20617760365209ee1", "ext": "py", "lang": "Python", "content": "@timeit('Part 1')\ndef part_one(x):\n    '''Solves part one'''\n    im = format_image(25, 6, x)\n\n    min_zeros = (10000, -1) # N, layer\n    print(im.shape)\n    for i in range(im.shape[0]):\n        n = sum(im[i,:,:].flatten() == 0)\n        # print(n)\n        if n < min_zeros[0]:\n            min_zeros = (n, i)\n\n    n_ones = sum(im[min_zeros[1],:,:].flatten() == 1)\n    n_twos = sum(im[min_zeros[1],:,:].flatten() == 2)\n\n    return n_ones*n_twos", "fn_id": 2, "class_fn": false, "repo": "orrinjelo/AdventOfCode2019", "file": "day_08/main.py", "last_update_at": "2020-12-14T21:04:29+00:00", "question_id": "ce04b0b29c2afca534c792a20617760365209ee1_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@timeit('Part 1')\ndef part_one(x):\n    \"\"\"Solves part one\"\"\"\n    im = format_image(25, 6, x)\n    min_zeros = (10000, -1)\n    print(im.shape)\n    for i in range(im.shape[0]):\n        n = sum(im[i, :, :].flatten() == 0)\n        if n < min_zeros[0]:\n            min_zeros = (n, i)\n    n_ones = sum(im[min_zeros[1], :, :].flatten() == 1)\n    n_twos = sum(im[min_zeros[1], :, :].flatten() == 2)\n"]]}
{"hexsha": "fca508875ec0524ea60b427ecfc075489e13efd6", "ext": "py", "lang": "Python", "content": "def JoinTypeAndIdentifier(typeName, identifier):\n\t# Add a space to separate type from identifier unless type is pointer\n\tif typeName.endswith(\"*\"):\n\t\treturn typeName + identifier\n\treturn typeName + \" \" + identifier", "fn_id": 2, "class_fn": false, "repo": "Novodes/notepad2", "file": "scintilla/scripts/ScintillaAPIFacer.py", "last_update_at": "2020-01-30T11:46:30+00:00", "question_id": "fca508875ec0524ea60b427ecfc075489e13efd6_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def JoinTypeAndIdentifier(typeName, identifier):\n    if typeName.endswith('*'):\n        return typeName + identifier\n"]]}
{"hexsha": "2363166cde2c637b4f881512de76cfa3c7c32673", "ext": "py", "lang": "Python", "content": "def read_data(file):\n    with open(file, 'rb') as f:\n        # The protocol version used is detected automatically, so we do not\n        # have to specify it.\n        data = pickle.load(f)\n        return data", "fn_id": 1, "class_fn": false, "repo": "samlet/stack", "file": "sagas/util/serializer.py", "last_update_at": "2020-08-25T22:34:15+00:00", "question_id": "2363166cde2c637b4f881512de76cfa3c7c32673_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def read_data(file):\n    with open(file, 'rb') as f:\n        data = pickle.load(f)\n"]]}
{"hexsha": "30dbf385fbbd53298b0f80ccdc03bc0e7b9d6313", "ext": "py", "lang": "Python", "content": "@contextmanager\ndef set_signal(sig, func):\n    \"\"\"Temporarily change a signal's action to the given func.\"\"\"\n    old_action = signal.signal(sig, func)\n    try:\n        yield\n    finally:\n        if old_action is not None:\n            signal.signal(sig, old_action)", "fn_id": 1, "class_fn": false, "repo": "songy23/opencensus-experiments", "file": "interoptest/src/pythonservice/util.py", "last_update_at": "2020-02-04T07:48:24+00:00", "question_id": "30dbf385fbbd53298b0f80ccdc03bc0e7b9d6313_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@contextmanager\ndef set_signal(sig, func):\n    \"\"\"Temporarily change a signal's action to the given func.\"\"\"\n    old_action = signal.signal(sig, func)\n    try:\n        yield\n    finally:\n        if old_action is not None:\n"]]}
{"hexsha": "57d9b09605aa2dbd63885c26adbd8139e92f01f7", "ext": "py", "lang": "Python", "content": "def calculate_kld(pe, qe, vb=True):\n    \"\"\"\n    Calculates the Kullback-Leibler Divergence between two PDFs.\n\n    Parameters\n    ----------\n    pe: numpy.ndarray, float\n        probability distribution evaluated on a grid whose distance from `q`\n        will be calculated.\n    qe: numpy.ndarray, float\n        probability distribution evaluated on a grid whose distance to `p` will\n        be calculated.\n    vb: boolean\n        report on progress to stdout?\n\n    Returns\n    -------\n    Dpq: float\n        the value of the Kullback-Leibler Divergence from `q` to `p`\n    \"\"\"\n    # Normalize the evaluations, so that the integrals can be done\n    # (very approximately!) by simple summation:\n    pn = pe / np.sum(pe)\n    qn = qe / np.sum(qe)\n    # Compute the log of the normalized PDFs\n    logp = u.safe_log(pn)\n    logq = u.safe_log(qn)\n    # Calculate the KLD from q to p\n    Dpq = np.sum(pn * (logp - logq))\n    return Dpq", "fn_id": 2, "class_fn": false, "repo": "eacharles/chippr", "file": "chippr/stat_utils.py", "last_update_at": "2020-07-27T18:57:58+00:00", "question_id": "57d9b09605aa2dbd63885c26adbd8139e92f01f7_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def calculate_kld(pe, qe, vb=True):\n    \"\"\"\n    Calculates the Kullback-Leibler Divergence between two PDFs.\n\n    Parameters\n    ----------\n    pe: numpy.ndarray, float\n        probability distribution evaluated on a grid whose distance from `q`\n        will be calculated.\n    qe: numpy.ndarray, float\n        probability distribution evaluated on a grid whose distance to `p` will\n        be calculated.\n    vb: boolean\n        report on progress to stdout?\n\n    Returns\n    -------\n    Dpq: float\n        the value of the Kullback-Leibler Divergence from `q` to `p`\n    \"\"\"\n    pn = pe / np.sum(pe)\n    qn = qe / np.sum(qe)\n    logp = u.safe_log(pn)\n    logq = u.safe_log(qn)\n    Dpq = np.sum(pn * (logp - logq))\n"]]}
{"hexsha": "ce6d28800f6264762b35182834863c5269a488ae", "ext": "py", "lang": "Python", "content": "async def join_game(request):\n    logging.info(f\"Request: {request.url}\")\n    username = request.query['username'] if 'username' in request.query else None\n    round_id = request.match_info['round_id']\n    return add_player_to_round(request, round_id, username)", "fn_id": 2, "class_fn": false, "repo": "bdvllrs/any-board-game", "file": "any_board_game/server/join_round.py", "last_update_at": "2020-06-05T09:06:50+00:00", "question_id": "ce6d28800f6264762b35182834863c5269a488ae_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["async def join_game(request):\n    logging.info(f'Request: {request.url}')\n    username = request.query['username'] if 'username' in request.query else None\n    round_id = request.match_info['round_id']\n"]]}
{"hexsha": "006d4a00afb998127c6a887ca3169a1a94ec90c4", "ext": "py", "lang": "Python", "content": "def _delete_edge(edges_from, n, to):\n    \"\"\"\n    Removes an edge from the graph.\n\n    @param      edges_from      structure which contains the edges (will be modified)\n    @param      n               first vertex\n    @param      to              second vertex\n    @return                     the edge\n    \"\"\"\n    le = edges_from[to]\n    f = None\n    for i, e in enumerate(le):\n        if (e[1] == to and e[2] == n) or (e[2] == to and e[1] == n):\n            f = i\n            break\n\n    assert f is not None\n    del le[f]\n    if len(le) == 0:\n        del edges_from[to]\n\n    le = edges_from[n]\n    f = None\n    for i, e in enumerate(le):\n        if (e[1] == to and e[2] == n) or (e[2] == to and e[1] == n):\n            f = i\n            break\n\n    assert f is not None\n    keep = le[f]\n    del le[f]\n    if len(le) == 0:\n        del edges_from[n]\n\n    return keep", "fn_id": 5, "class_fn": false, "repo": "sdpython/ensae_projects", "file": "src/ensae_projects/challenge/city_tour.py", "last_update_at": "2020-11-22T10:24:54+00:00", "question_id": "006d4a00afb998127c6a887ca3169a1a94ec90c4_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def _delete_edge(edges_from, n, to):\n    \"\"\"\n    Removes an edge from the graph.\n\n    @param      edges_from      structure which contains the edges (will be modified)\n    @param      n               first vertex\n    @param      to              second vertex\n    @return                     the edge\n    \"\"\"\n    le = edges_from[to]\n    f = None\n    for i, e in enumerate(le):\n        if e[1] == to and e[2] == n or (e[2] == to and e[1] == n):\n            f = i\n            break\n    assert f is not None\n    del le[f]\n    if len(le) == 0:\n        del edges_from[to]\n    le = edges_from[n]\n    f = None\n    for i, e in enumerate(le):\n        if e[1] == to and e[2] == n or (e[2] == to and e[1] == n):\n            f = i\n            break\n    assert f is not None\n    keep = le[f]\n    del le[f]\n    if len(le) == 0:\n        del edges_from[n]\n"]]}
{"hexsha": "5f3909cc586b83372539840e1eb7519068d06e8b", "ext": "py", "lang": "Python", "content": "def test_inheritance(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        from easy_addoption import AddOption\n\n        class BarAddOption(AddOption):\n            bar: str\n        \n        class FooAddOption(BarAddOption):\n            foo: str\n        \n        def pytest_addoption(parser):\n            FooAddOption.register(parser)\n        \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        from conftest import FooAddOption\n\n        def test_manual_register(request):\n            options = FooAddOption(request.config)\n\n            assert options.foo == 'foo'\n            assert options.bar == 'bar'\n        \"\"\"\n    )\n\n    result = testdir.runpytest_inprocess(\"--foo=foo\", \"--bar=bar\")\n    result.assert_outcomes(passed=1)", "fn_id": 0, "class_fn": false, "repo": "uriyyo/pytest-easy-addoption", "file": "tests/acceptance/test_inheritance.py", "last_update_at": "2020-01-22T22:05:24+00:00", "question_id": "5f3909cc586b83372539840e1eb7519068d06e8b_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_inheritance(testdir):\n    testdir.makeconftest('\\n        from easy_addoption import AddOption\\n\\n        class BarAddOption(AddOption):\\n            bar: str\\n        \\n        class FooAddOption(BarAddOption):\\n            foo: str\\n        \\n        def pytest_addoption(parser):\\n            FooAddOption.register(parser)\\n        ')\n    testdir.makepyfile(\"\\n        from conftest import FooAddOption\\n\\n        def test_manual_register(request):\\n            options = FooAddOption(request.config)\\n\\n            assert options.foo == 'foo'\\n            assert options.bar == 'bar'\\n        \")\n    result = testdir.runpytest_inprocess('--foo=foo', '--bar=bar')\n"]]}
{"hexsha": "53196e29960a327c8c1ad1da91ce1840a4fd68bd", "ext": "py", "lang": "Python", "content": "def leave_comment(api, urn, pr, body):\n    path = \"/repos/{urn}/issues/{pr}/comments\".format(urn=urn, pr=pr)\n    data = {\"body\": body}\n    resp = api(\"post\", path, json=data)\n    return resp", "fn_id": 3, "class_fn": false, "repo": "yet-another-account/chaos", "file": "github_api/comments.py", "last_update_at": "2020-11-02T23:38:51+00:00", "question_id": "53196e29960a327c8c1ad1da91ce1840a4fd68bd_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def leave_comment(api, urn, pr, body):\n    path = '/repos/{urn}/issues/{pr}/comments'.format(urn=urn, pr=pr)\n    data = {'body': body}\n    resp = api('post', path, json=data)\n"]]}
{"hexsha": "b922ff8e9939b3fae6971a68b806f7cde1387785", "ext": "py", "lang": "Python", "content": "def portage_group_warning():\n\twarn_prefix = colorize(\"BAD\", \"*** WARNING ***  \")\n\tmylines = [\n\t\t\"For security reasons, only system administrators should be\",\n\t\t\"allowed in the portage group.  Untrusted users or processes\",\n\t\t\"can potentially exploit the portage group for attacks such as\",\n\t\t\"local privilege escalation.\"\n\t]\n\tfor x in mylines:\n\t\twritemsg(warn_prefix, noiselevel=-1)\n\t\twritemsg(x, noiselevel=-1)\n\t\twritemsg(\"\\n\", noiselevel=-1)\n\twritemsg(\"\\n\", noiselevel=-1)", "fn_id": 0, "class_fn": false, "repo": "maxim5/code-inspector", "file": "data/python/45f539f2f525639a1c88426284d9ea6f_data.py", "last_update_at": "2020-07-30T13:15:29+00:00", "question_id": "b922ff8e9939b3fae6971a68b806f7cde1387785_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def portage_group_warning():\n    warn_prefix = colorize('BAD', '*** WARNING ***  ')\n    mylines = ['For security reasons, only system administrators should be', 'allowed in the portage group.  Untrusted users or processes', 'can potentially exploit the portage group for attacks such as', 'local privilege escalation.']\n    for x in mylines:\n        writemsg(warn_prefix, noiselevel=-1)\n        writemsg(x, noiselevel=-1)\n        writemsg('\\n', noiselevel=-1)\n"]]}
{"hexsha": "0781b1dde191a2671a44781c37c127bcb64fb318", "ext": "py", "lang": "Python", "content": "def test_cli_datasorce_new(caplog, empty_data_context, filesystem_csv_2):\n    project_root_dir = empty_data_context.root_directory\n    context = DataContext(project_root_dir)\n    assert context.list_datasources() == []\n\n    runner = CliRunner(mix_stderr=False)\n    result = runner.invoke(\n        cli,\n        [\"datasource\", \"new\", \"-d\", project_root_dir],\n        input=\"1\\n1\\n%s\\nmynewsource\\n\" % str(filesystem_csv_2),\n        catch_exceptions=False,\n    )\n    stdout = result.stdout\n\n    assert \"What data would you like Great Expectations to connect to?\" in stdout\n    assert \"What are you processing your files with?\" in stdout\n    assert \"Give your new data source a short name.\" in stdout\n    assert \"A new datasource 'mynewsource' was added to your project.\" in stdout\n\n    assert result.exit_code == 0\n\n    config_path = os.path.join(project_root_dir, DataContext.GE_YML)\n    config = yaml.load(open(config_path, \"r\"))\n    datasources = config[\"datasources\"]\n    assert \"mynewsource\" in datasources.keys()\n    data_source_class = datasources[\"mynewsource\"][\"data_asset_type\"][\"class_name\"]\n    assert data_source_class == \"PandasDataset\"\n    assert_no_logging_messages_or_tracebacks(caplog, result)", "fn_id": 1, "class_fn": false, "repo": "kyleaton/great_expectations", "file": "tests/cli/test_datasource_pandas.py", "last_update_at": "2020-05-07T18:16:21+00:00", "question_id": "0781b1dde191a2671a44781c37c127bcb64fb318_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_cli_datasorce_new(caplog, empty_data_context, filesystem_csv_2):\n    project_root_dir = empty_data_context.root_directory\n    context = DataContext(project_root_dir)\n    assert context.list_datasources() == []\n    runner = CliRunner(mix_stderr=False)\n    result = runner.invoke(cli, ['datasource', 'new', '-d', project_root_dir], input='1\\n1\\n%s\\nmynewsource\\n' % str(filesystem_csv_2), catch_exceptions=False)\n    stdout = result.stdout\n    assert 'What data would you like Great Expectations to connect to?' in stdout\n    assert 'What are you processing your files with?' in stdout\n    assert 'Give your new data source a short name.' in stdout\n    assert \"A new datasource 'mynewsource' was added to your project.\" in stdout\n    assert result.exit_code == 0\n    config_path = os.path.join(project_root_dir, DataContext.GE_YML)\n    config = yaml.load(open(config_path, 'r'))\n    datasources = config['datasources']\n    assert 'mynewsource' in datasources.keys()\n    data_source_class = datasources['mynewsource']['data_asset_type']['class_name']\n    assert data_source_class == 'PandasDataset'\n"]]}
{"hexsha": "665fb9b308ef46f86b3755679d32ccd7e787916a", "ext": "py", "lang": "Python", "content": "def get_action_form(request, obj, action):\n    action_function = action['function']\n    action_verbose_name = action['verbose_name']\n    initial = action['initial']\n    action_input = action['input']\n    action_choices = action['choices']\n    action_display = action['display']\n    app_label = get_metadata(type(obj), 'app_label')\n    func = getattr(obj, action_function.__name__, action_function)\n\n    if initial and hasattr(obj, initial):\n        initial = getattr(obj, initial)()\n    else:\n        initial = {}\n    if action_choices and hasattr(obj, action_choices):\n        action_choices = getattr(obj, action_choices)()\n    else:\n        action_choices = {}\n\n    if action_input:\n        # it is a form name\n        if type(action_input) in [str, str] and '.' not in action_input:\n            full_app_name = settings.APP_MAPPING.get(app_label, app_label)\n            fromlist = app_label\n            module = __import__('{}.forms'.format(full_app_name), fromlist=list(map(str, [app_label])))\n            form_cls = getattr(module, action_input)\n\n        # it is a model or model name\n        else:\n            if type(action_input) in [str, str]:\n                app_name, class_name = action_input.split('.')\n                action_input = apps.get_model(app_name, class_name)\n\n            class Form(forms.ModelForm):\n                class Meta:\n                    model = action_input\n                    fields = get_parameters_names(func)\n                    title = action_verbose_name\n                    submit_label = action_verbose_name\n\n            form_cls = Form\n    else:\n        class Form(forms.ModelForm):\n            class Meta:\n                model = func.__self__.__class__\n                fields = get_parameters_names(func)\n                title = action_verbose_name\n                submit_label = action_verbose_name\n\n        form_cls = Form\n\n    if issubclass(form_cls, forms.ModelForm):\n        for key in list(initial.keys()):\n            if hasattr(obj, key) and obj.pk and getattr(obj, key):\n                del (initial[key])\n        form = form_cls(request, instance=obj, initial=initial)\n    else:\n        form = form_cls(request, initial=initial)\n\n    if action_display:\n        for lookup in action_display:\n            label = get_fiendly_name(func.__self__.__class__, lookup)\n            value = getattr2(obj, lookup)\n            form.fields[lookup] = forms.CharField(\n                label=label, initial=value, required=False, widget=forms.widgets.DisplayInput(value)\n            )\n\n    if action_choices:\n        for field_name in action_choices:\n            form.fields[field_name].queryset = action_choices[field_name]\n\n    if not obj.pk:\n        verbose_name = get_metadata(obj.__class__, 'verbose_name')\n        form.fields['instance'] = forms.ModelChoiceField(type(obj).objects.all(), label=verbose_name)\n        if form.fieldsets:\n            form.fieldsets = ((verbose_name, {'fields': ('instance',)}),) + form.fieldsets\n\n    return form", "fn_id": 8, "class_fn": false, "repo": "adelsonllima/djangoplus", "file": "ui/components/forms/factory.py", "last_update_at": "2020-01-16T20:02:08+00:00", "question_id": "665fb9b308ef46f86b3755679d32ccd7e787916a_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_action_form(request, obj, action):\n    action_function = action['function']\n    action_verbose_name = action['verbose_name']\n    initial = action['initial']\n    action_input = action['input']\n    action_choices = action['choices']\n    action_display = action['display']\n    app_label = get_metadata(type(obj), 'app_label')\n    func = getattr(obj, action_function.__name__, action_function)\n    if initial and hasattr(obj, initial):\n        initial = getattr(obj, initial)()\n    else:\n        initial = {}\n    if action_choices and hasattr(obj, action_choices):\n        action_choices = getattr(obj, action_choices)()\n    else:\n        action_choices = {}\n    if action_input:\n        if type(action_input) in [str, str] and '.' not in action_input:\n            full_app_name = settings.APP_MAPPING.get(app_label, app_label)\n            fromlist = app_label\n            module = __import__('{}.forms'.format(full_app_name), fromlist=list(map(str, [app_label])))\n            form_cls = getattr(module, action_input)\n        else:\n            if type(action_input) in [str, str]:\n                app_name, class_name = action_input.split('.')\n                action_input = apps.get_model(app_name, class_name)\n\n            class Form(forms.ModelForm):\n\n                class Meta:\n                    model = action_input\n                    fields = get_parameters_names(func)\n                    title = action_verbose_name\n                    submit_label = action_verbose_name\n            form_cls = Form\n    else:\n\n        class Form(forms.ModelForm):\n\n            class Meta:\n                model = func.__self__.__class__\n                fields = get_parameters_names(func)\n                title = action_verbose_name\n                submit_label = action_verbose_name\n        form_cls = Form\n    if issubclass(form_cls, forms.ModelForm):\n        for key in list(initial.keys()):\n            if hasattr(obj, key) and obj.pk and getattr(obj, key):\n                del initial[key]\n        form = form_cls(request, instance=obj, initial=initial)\n    else:\n        form = form_cls(request, initial=initial)\n    if action_display:\n        for lookup in action_display:\n            label = get_fiendly_name(func.__self__.__class__, lookup)\n            value = getattr2(obj, lookup)\n            form.fields[lookup] = forms.CharField(label=label, initial=value, required=False, widget=forms.widgets.DisplayInput(value))\n    if action_choices:\n        for field_name in action_choices:\n            form.fields[field_name].queryset = action_choices[field_name]\n    if not obj.pk:\n        verbose_name = get_metadata(obj.__class__, 'verbose_name')\n        form.fields['instance'] = forms.ModelChoiceField(type(obj).objects.all(), label=verbose_name)\n        if form.fieldsets:\n            form.fieldsets = ((verbose_name, {'fields': ('instance',)}),) + form.fieldsets\n"]]}
{"hexsha": "2323f08443b78e0cd0e6f50fb5ed203b47fefb0c", "ext": "py", "lang": "Python", "content": "def lab2rgb(lab):\n    xyz = lab2xyz(lab)\n    rgb = xyz2rgb(xyz)\n    return [round(val) for val in rgb]", "fn_id": 3, "class_fn": false, "repo": "cypreess/chroma-py", "file": "chroma/conversions.py", "last_update_at": "2020-10-06T11:28:55+00:00", "question_id": "2323f08443b78e0cd0e6f50fb5ed203b47fefb0c_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def lab2rgb(lab):\n    xyz = lab2xyz(lab)\n    rgb = xyz2rgb(xyz)\n"]]}
{"hexsha": "e6748189acda1eff223bb2a0be2522fd5a5e5ad3", "ext": "py", "lang": "Python", "content": "def test_get_lift():\n    \"\"\"Check that the lift force is calculated correctly\n    \"\"\"\n    # given\n    h = 0.4\n    rho = 1.3\n    wsp = 4\n    air_df = pd.DataFrame([[1, 0], [0.5, h/2],\n                           [0, 0], [0.5, -h/2]],\n                          columns=['x', 'y'])\n    exp_lift_force = 0.5 * 0.2 * rho * wsp**2 * h  # h is area here\n    # when\n    calc_lift_force = get_lift(air_df, wsp, rho=rho)\n    # then\n    np.testing.assert_equal(exp_lift_force, calc_lift_force)", "fn_id": 0, "class_fn": false, "repo": "alfredocarella/code-for-the-world", "file": "tests/test_calcs.py", "last_update_at": "2020-11-18T14:45:38+00:00", "question_id": "e6748189acda1eff223bb2a0be2522fd5a5e5ad3_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_get_lift():\n    \"\"\"Check that the lift force is calculated correctly\n    \"\"\"\n    h = 0.4\n    rho = 1.3\n    wsp = 4\n    air_df = pd.DataFrame([[1, 0], [0.5, h / 2], [0, 0], [0.5, -h / 2]], columns=['x', 'y'])\n    exp_lift_force = 0.5 * 0.2 * rho * wsp ** 2 * h\n    calc_lift_force = get_lift(air_df, wsp, rho=rho)\n"]]}
{"hexsha": "ddfc9694e766a47f9627d1539085cb001f48b7fb", "ext": "py", "lang": "Python", "content": "def displayAssetDetails():\n\tprint(\"Input the asset name\")\n\tassetName = input()\n\tif assets.hasAsset(assetName):\n\t\tasset = assets.getAsset(assetName)\n\t\tprint(f'\\n{assetName}')\n\t\tprint('Name:', asset.getName())\n\t\tprint('Market Cap:', asset.getMarketCap())\n\t\tprint('Price:', asset.getPrice())\n\t\tprint('Circulating Supply:', asset.getCirculatingSupply())\n\t\tprint('Volume:', asset.getVolume())\n\t\tprint('One Hour Percent:'+asset.getOneHourPercent()+'%')\n\t\tprint('Twenty Four Hour Percent:'+asset.getTwentyFourHourPercent()+'%')\n\t\tprint('Seven Day Percent:'+asset.getSevenDayPercent()+'%')\n\telse:\n\t\tprint(\"\\nAsset doesn't exist\\n\")", "fn_id": 4, "class_fn": false, "repo": "jobymathew/Cryptocurrency_Trade_Details_and_Overview", "file": "cryptoGraph.py", "last_update_at": "2020-11-08T17:48:43+00:00", "question_id": "ddfc9694e766a47f9627d1539085cb001f48b7fb_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def displayAssetDetails():\n    print('Input the asset name')\n    assetName = input()\n    if assets.hasAsset(assetName):\n        asset = assets.getAsset(assetName)\n        print(f'\\n{assetName}')\n        print('Name:', asset.getName())\n        print('Market Cap:', asset.getMarketCap())\n        print('Price:', asset.getPrice())\n        print('Circulating Supply:', asset.getCirculatingSupply())\n        print('Volume:', asset.getVolume())\n        print('One Hour Percent:' + asset.getOneHourPercent() + '%')\n        print('Twenty Four Hour Percent:' + asset.getTwentyFourHourPercent() + '%')\n        print('Seven Day Percent:' + asset.getSevenDayPercent() + '%')\n    else:\n"]]}
{"hexsha": "f911717dd5e15cb38cd861059742205ea07a3ae6", "ext": "py", "lang": "Python", "content": "@app.route(\"/fake_news_detector\", methods=[\"POST\"])\ndef fake_news_detector_response():\n    text = request.form['text']\n    response = predict(news_model, news_vectorizer, text)\n    return render_template(\"fake_news_detector_response.html\", value = response)", "fn_id": 2, "class_fn": false, "repo": "dhruvmk/ml-web-app", "file": "main.py", "last_update_at": "2020-11-01T18:30:55+00:00", "question_id": "f911717dd5e15cb38cd861059742205ea07a3ae6_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@app.route('/fake_news_detector', methods=['POST'])\ndef fake_news_detector_response():\n    text = request.form['text']\n    response = predict(news_model, news_vectorizer, text)\n"]]}
{"hexsha": "7624926cb8ac6ce963b6ce4d55df92e231663099", "ext": "py", "lang": "Python", "content": "def getPos(s):\n    r = s[0] + 1\n    c = 'ABCDEFGH'[s[1]]\n    return c + str(r)", "fn_id": 1, "class_fn": false, "repo": "honux77/algorithm", "file": "baekjoon/implementation/1063-chess/1063-king.py", "last_update_at": "2020-11-19T12:23:52+00:00", "question_id": "7624926cb8ac6ce963b6ce4d55df92e231663099_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def getPos(s):\n    r = s[0] + 1\n    c = 'ABCDEFGH'[s[1]]\n"]]}
{"hexsha": "85a38ac12be3dc9bf96e5a913454de2eff28cd7e", "ext": "py", "lang": "Python", "content": "def register_Ns3MacLow_methods(root_module, cls):\n    ## mac-low.h (module 'wifi'): ns3::MacLow::MacLow(ns3::MacLow const & arg0) [copy constructor]\n    cls.add_constructor([param('ns3::MacLow const &', 'arg0')])\n    ## mac-low.h (module 'wifi'): ns3::MacLow::MacLow() [constructor]\n    cls.add_constructor([])\n    ## mac-low.h (module 'wifi'): ns3::Ptr<ns3::Packet> ns3::MacLow::AggregateToAmpdu(ns3::Ptr<ns3::Packet const> packet, ns3::WifiMacHeader const hdr) [member function]\n    cls.add_method('AggregateToAmpdu', \n                   'ns3::Ptr< ns3::Packet >', \n                   [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const', 'hdr')])\n    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::CalculateTransmissionTime(ns3::Ptr<ns3::Packet const> packet, ns3::WifiMacHeader const * hdr, ns3::MacLowTransmissionParameters const & parameters) const [member function]\n    cls.add_method('CalculateTransmissionTime', \n                   'ns3::Time', \n                   [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const *', 'hdr'), param('ns3::MacLowTransmissionParameters const &', 'parameters')], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::CreateBlockAckAgreement(ns3::MgtAddBaResponseHeader const * respHdr, ns3::Mac48Address originator, uint16_t startingSeq) [member function]\n    cls.add_method('CreateBlockAckAgreement', \n                   'void', \n                   [param('ns3::MgtAddBaResponseHeader const *', 'respHdr'), param('ns3::Mac48Address', 'originator'), param('uint16_t', 'startingSeq')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::DeaggregateAmpduAndReceive(ns3::Ptr<ns3::Packet> aggregatedPacket, double rxSnr, ns3::WifiMode txMode, ns3::WifiPreamble preamble) [member function]\n    cls.add_method('DeaggregateAmpduAndReceive', \n                   'void', \n                   [param('ns3::Ptr< ns3::Packet >', 'aggregatedPacket'), param('double', 'rxSnr'), param('ns3::WifiMode', 'txMode'), param('ns3::WifiPreamble', 'preamble')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::DestroyBlockAckAgreement(ns3::Mac48Address originator, uint8_t tid) [member function]\n    cls.add_method('DestroyBlockAckAgreement', \n                   'void', \n                   [param('ns3::Mac48Address', 'originator'), param('uint8_t', 'tid')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::FlushAggregateQueue() [member function]\n    cls.add_method('FlushAggregateQueue', \n                   'void', \n                   [])\n    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetAckTimeout() const [member function]\n    cls.add_method('GetAckTimeout', \n                   'ns3::Time', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): ns3::Mac48Address ns3::MacLow::GetAddress() const [member function]\n    cls.add_method('GetAddress', \n                   'ns3::Mac48Address', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetBasicBlockAckTimeout() const [member function]\n    cls.add_method('GetBasicBlockAckTimeout', \n                   'ns3::Time', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): ns3::Mac48Address ns3::MacLow::GetBssid() const [member function]\n    cls.add_method('GetBssid', \n                   'ns3::Mac48Address', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetCompressedBlockAckTimeout() const [member function]\n    cls.add_method('GetCompressedBlockAckTimeout', \n                   'ns3::Time', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetCtsTimeout() const [member function]\n    cls.add_method('GetCtsTimeout', \n                   'ns3::Time', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): bool ns3::MacLow::GetCtsToSelfSupported() const [member function]\n    cls.add_method('GetCtsToSelfSupported', \n                   'bool', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): ns3::Ptr<ns3::WifiPhy> ns3::MacLow::GetPhy() const [member function]\n    cls.add_method('GetPhy', \n                   'ns3::Ptr< ns3::WifiPhy >', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetPifs() const [member function]\n    cls.add_method('GetPifs', \n                   'ns3::Time', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetRifs() const [member function]\n    cls.add_method('GetRifs', \n                   'ns3::Time', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetSifs() const [member function]\n    cls.add_method('GetSifs', \n                   'ns3::Time', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetSlotTime() const [member function]\n    cls.add_method('GetSlotTime', \n                   'ns3::Time', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): bool ns3::MacLow::IsPromisc() const [member function]\n    cls.add_method('IsPromisc', \n                   'bool', \n                   [], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::NotifySleepNow() [member function]\n    cls.add_method('NotifySleepNow', \n                   'void', \n                   [])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::NotifySwitchingStartNow(ns3::Time duration) [member function]\n    cls.add_method('NotifySwitchingStartNow', \n                   'void', \n                   [param('ns3::Time', 'duration')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::ReceiveError(ns3::Ptr<ns3::Packet const> packet, double rxSnr) [member function]\n    cls.add_method('ReceiveError', \n                   'void', \n                   [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('double', 'rxSnr')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::ReceiveOk(ns3::Ptr<ns3::Packet> packet, double rxSnr, ns3::WifiMode txMode, ns3::WifiPreamble preamble, bool ampduSubframe) [member function]\n    cls.add_method('ReceiveOk', \n                   'void', \n                   [param('ns3::Ptr< ns3::Packet >', 'packet'), param('double', 'rxSnr'), param('ns3::WifiMode', 'txMode'), param('ns3::WifiPreamble', 'preamble'), param('bool', 'ampduSubframe')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::RegisterBlockAckListenerForAc(ns3::AcIndex ac, ns3::MacLowBlockAckEventListener * listener) [member function]\n    cls.add_method('RegisterBlockAckListenerForAc', \n                   'void', \n                   [param('ns3::AcIndex', 'ac'), param('ns3::MacLowBlockAckEventListener *', 'listener')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::RegisterDcfListener(ns3::MacLowDcfListener * listener) [member function]\n    cls.add_method('RegisterDcfListener', \n                   'void', \n                   [param('ns3::MacLowDcfListener *', 'listener')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::ResetPhy() [member function]\n    cls.add_method('ResetPhy', \n                   'void', \n                   [])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetAckTimeout(ns3::Time ackTimeout) [member function]\n    cls.add_method('SetAckTimeout', \n                   'void', \n                   [param('ns3::Time', 'ackTimeout')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetAddress(ns3::Mac48Address ad) [member function]\n    cls.add_method('SetAddress', \n                   'void', \n                   [param('ns3::Mac48Address', 'ad')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetBasicBlockAckTimeout(ns3::Time blockAckTimeout) [member function]\n    cls.add_method('SetBasicBlockAckTimeout', \n                   'void', \n                   [param('ns3::Time', 'blockAckTimeout')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetBssid(ns3::Mac48Address ad) [member function]\n    cls.add_method('SetBssid', \n                   'void', \n                   [param('ns3::Mac48Address', 'ad')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetCompressedBlockAckTimeout(ns3::Time blockAckTimeout) [member function]\n    cls.add_method('SetCompressedBlockAckTimeout', \n                   'void', \n                   [param('ns3::Time', 'blockAckTimeout')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetCtsTimeout(ns3::Time ctsTimeout) [member function]\n    cls.add_method('SetCtsTimeout', \n                   'void', \n                   [param('ns3::Time', 'ctsTimeout')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetCtsToSelfSupported(bool enable) [member function]\n    cls.add_method('SetCtsToSelfSupported', \n                   'void', \n                   [param('bool', 'enable')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetMpduAggregator(ns3::Ptr<ns3::MpduAggregator> aggregator) [member function]\n    cls.add_method('SetMpduAggregator', \n                   'void', \n                   [param('ns3::Ptr< ns3::MpduAggregator >', 'aggregator')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetPhy(ns3::Ptr<ns3::WifiPhy> phy) [member function]\n    cls.add_method('SetPhy', \n                   'void', \n                   [param('ns3::Ptr< ns3::WifiPhy >', 'phy')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetPifs(ns3::Time pifs) [member function]\n    cls.add_method('SetPifs', \n                   'void', \n                   [param('ns3::Time', 'pifs')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetPromisc() [member function]\n    cls.add_method('SetPromisc', \n                   'void', \n                   [])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetRifs(ns3::Time rifs) [member function]\n    cls.add_method('SetRifs', \n                   'void', \n                   [param('ns3::Time', 'rifs')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetRxCallback(ns3::Callback<void, ns3::Ptr<ns3::Packet>, ns3::WifiMacHeader const*, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty> callback) [member function]\n    cls.add_method('SetRxCallback', \n                   'void', \n                   [param('ns3::Callback< void, ns3::Ptr< ns3::Packet >, ns3::WifiMacHeader const *, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty >', 'callback')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetSifs(ns3::Time sifs) [member function]\n    cls.add_method('SetSifs', \n                   'void', \n                   [param('ns3::Time', 'sifs')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetSlotTime(ns3::Time slotTime) [member function]\n    cls.add_method('SetSlotTime', \n                   'void', \n                   [param('ns3::Time', 'slotTime')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetWifiRemoteStationManager(ns3::Ptr<ns3::WifiRemoteStationManager> manager) [member function]\n    cls.add_method('SetWifiRemoteStationManager', \n                   'void', \n                   [param('ns3::Ptr< ns3::WifiRemoteStationManager >', 'manager')])\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::StartTransmission(ns3::Ptr<ns3::Packet const> packet, ns3::WifiMacHeader const * hdr, ns3::MacLowTransmissionParameters parameters, ns3::MacLowTransmissionListener * listener) [member function]\n    cls.add_method('StartTransmission', \n                   'void', \n                   [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const *', 'hdr'), param('ns3::MacLowTransmissionParameters', 'parameters'), param('ns3::MacLowTransmissionListener *', 'listener')], \n                   is_virtual=True)\n    ## mac-low.h (module 'wifi'): bool ns3::MacLow::StopAggregation(ns3::Ptr<ns3::Packet const> peekedPacket, ns3::WifiMacHeader peekedHdr, ns3::Ptr<ns3::Packet> aggregatedPacket, uint16_t size) const [member function]\n    cls.add_method('StopAggregation', \n                   'bool', \n                   [param('ns3::Ptr< ns3::Packet const >', 'peekedPacket'), param('ns3::WifiMacHeader', 'peekedHdr'), param('ns3::Ptr< ns3::Packet >', 'aggregatedPacket'), param('uint16_t', 'size')], \n                   is_const=True)\n    ## mac-low.h (module 'wifi'): ns3::WifiTxVector ns3::MacLow::GetDataTxVector(ns3::Ptr<ns3::Packet const> packet, ns3::WifiMacHeader const * hdr) const [member function]\n    cls.add_method('GetDataTxVector', \n                   'ns3::WifiTxVector', \n                   [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const *', 'hdr')], \n                   is_const=True, visibility='protected', is_virtual=True)\n    ## mac-low.h (module 'wifi'): void ns3::MacLow::DoDispose() [member function]\n    cls.add_method('DoDispose', \n                   'void', \n                   [], \n                   visibility='private', is_virtual=True)\n    return", "fn_id": 206, "class_fn": false, "repo": "gustavo978/helpful", "file": "ns-allinone-3.22/ns-3.22/src/wave/bindings/modulegen__gcc_LP64.py", "last_update_at": "2020-04-07T17:20:55+00:00", "question_id": "85a38ac12be3dc9bf96e5a913454de2eff28cd7e_206", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def register_Ns3MacLow_methods(root_module, cls):\n    cls.add_constructor([param('ns3::MacLow const &', 'arg0')])\n    cls.add_constructor([])\n    cls.add_method('AggregateToAmpdu', 'ns3::Ptr< ns3::Packet >', [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const', 'hdr')])\n    cls.add_method('CalculateTransmissionTime', 'ns3::Time', [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const *', 'hdr'), param('ns3::MacLowTransmissionParameters const &', 'parameters')], is_const=True)\n    cls.add_method('CreateBlockAckAgreement', 'void', [param('ns3::MgtAddBaResponseHeader const *', 'respHdr'), param('ns3::Mac48Address', 'originator'), param('uint16_t', 'startingSeq')])\n    cls.add_method('DeaggregateAmpduAndReceive', 'void', [param('ns3::Ptr< ns3::Packet >', 'aggregatedPacket'), param('double', 'rxSnr'), param('ns3::WifiMode', 'txMode'), param('ns3::WifiPreamble', 'preamble')])\n    cls.add_method('DestroyBlockAckAgreement', 'void', [param('ns3::Mac48Address', 'originator'), param('uint8_t', 'tid')])\n    cls.add_method('FlushAggregateQueue', 'void', [])\n    cls.add_method('GetAckTimeout', 'ns3::Time', [], is_const=True)\n    cls.add_method('GetAddress', 'ns3::Mac48Address', [], is_const=True)\n    cls.add_method('GetBasicBlockAckTimeout', 'ns3::Time', [], is_const=True)\n    cls.add_method('GetBssid', 'ns3::Mac48Address', [], is_const=True)\n    cls.add_method('GetCompressedBlockAckTimeout', 'ns3::Time', [], is_const=True)\n    cls.add_method('GetCtsTimeout', 'ns3::Time', [], is_const=True)\n    cls.add_method('GetCtsToSelfSupported', 'bool', [], is_const=True)\n    cls.add_method('GetPhy', 'ns3::Ptr< ns3::WifiPhy >', [], is_const=True)\n    cls.add_method('GetPifs', 'ns3::Time', [], is_const=True)\n    cls.add_method('GetRifs', 'ns3::Time', [], is_const=True)\n    cls.add_method('GetSifs', 'ns3::Time', [], is_const=True)\n    cls.add_method('GetSlotTime', 'ns3::Time', [], is_const=True)\n    cls.add_method('IsPromisc', 'bool', [], is_const=True)\n    cls.add_method('NotifySleepNow', 'void', [])\n    cls.add_method('NotifySwitchingStartNow', 'void', [param('ns3::Time', 'duration')])\n    cls.add_method('ReceiveError', 'void', [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('double', 'rxSnr')])\n    cls.add_method('ReceiveOk', 'void', [param('ns3::Ptr< ns3::Packet >', 'packet'), param('double', 'rxSnr'), param('ns3::WifiMode', 'txMode'), param('ns3::WifiPreamble', 'preamble'), param('bool', 'ampduSubframe')])\n    cls.add_method('RegisterBlockAckListenerForAc', 'void', [param('ns3::AcIndex', 'ac'), param('ns3::MacLowBlockAckEventListener *', 'listener')])\n    cls.add_method('RegisterDcfListener', 'void', [param('ns3::MacLowDcfListener *', 'listener')])\n    cls.add_method('ResetPhy', 'void', [])\n    cls.add_method('SetAckTimeout', 'void', [param('ns3::Time', 'ackTimeout')])\n    cls.add_method('SetAddress', 'void', [param('ns3::Mac48Address', 'ad')])\n    cls.add_method('SetBasicBlockAckTimeout', 'void', [param('ns3::Time', 'blockAckTimeout')])\n    cls.add_method('SetBssid', 'void', [param('ns3::Mac48Address', 'ad')])\n    cls.add_method('SetCompressedBlockAckTimeout', 'void', [param('ns3::Time', 'blockAckTimeout')])\n    cls.add_method('SetCtsTimeout', 'void', [param('ns3::Time', 'ctsTimeout')])\n    cls.add_method('SetCtsToSelfSupported', 'void', [param('bool', 'enable')])\n    cls.add_method('SetMpduAggregator', 'void', [param('ns3::Ptr< ns3::MpduAggregator >', 'aggregator')])\n    cls.add_method('SetPhy', 'void', [param('ns3::Ptr< ns3::WifiPhy >', 'phy')])\n    cls.add_method('SetPifs', 'void', [param('ns3::Time', 'pifs')])\n    cls.add_method('SetPromisc', 'void', [])\n    cls.add_method('SetRifs', 'void', [param('ns3::Time', 'rifs')])\n    cls.add_method('SetRxCallback', 'void', [param('ns3::Callback< void, ns3::Ptr< ns3::Packet >, ns3::WifiMacHeader const *, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty >', 'callback')])\n    cls.add_method('SetSifs', 'void', [param('ns3::Time', 'sifs')])\n    cls.add_method('SetSlotTime', 'void', [param('ns3::Time', 'slotTime')])\n    cls.add_method('SetWifiRemoteStationManager', 'void', [param('ns3::Ptr< ns3::WifiRemoteStationManager >', 'manager')])\n    cls.add_method('StartTransmission', 'void', [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const *', 'hdr'), param('ns3::MacLowTransmissionParameters', 'parameters'), param('ns3::MacLowTransmissionListener *', 'listener')], is_virtual=True)\n    cls.add_method('StopAggregation', 'bool', [param('ns3::Ptr< ns3::Packet const >', 'peekedPacket'), param('ns3::WifiMacHeader', 'peekedHdr'), param('ns3::Ptr< ns3::Packet >', 'aggregatedPacket'), param('uint16_t', 'size')], is_const=True)\n    cls.add_method('GetDataTxVector', 'ns3::WifiTxVector', [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const *', 'hdr')], is_const=True, visibility='protected', is_virtual=True)\n    cls.add_method('DoDispose', 'void', [], visibility='private', is_virtual=True)\n"]]}
{"hexsha": "8a57b60f368b3402ae548707659863eb15e60f59", "ext": "py", "lang": "Python", "content": "def myMain(baseDir):\n    \"\"\"Main function. Run the tests. \"\"\"\n    \n    print(\"Test the error propagation. \")\n\n    from instru import * \n    \n    fac = Factory(\"DemoRootFactory\")\n    print(\"Using DemoRootFactory. \")\n    \n    print(\"Create module from failer factory\")\n    failer = fac.select(\"branch\").select(\"failer\").create(\"failer\")\n\n    runModule(failer)\n\n    try:\n        waitAll()\n    except RuntimeError:\n        print(\"ok, waitAll raised exception\")\n    else:\n        raise RuntimeError(\"an error should have been raised\")\n    \n    print(\"End of script failTest.py\")", "fn_id": 0, "class_fn": false, "repo": "Opticalp/instrumentall", "file": "testsuite/python/failTest.py", "last_update_at": "2020-05-19T02:06:55+00:00", "question_id": "8a57b60f368b3402ae548707659863eb15e60f59_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def myMain(baseDir):\n    \"\"\"Main function. Run the tests. \"\"\"\n    print('Test the error propagation. ')\n    from instru import *\n    fac = Factory('DemoRootFactory')\n    print('Using DemoRootFactory. ')\n    print('Create module from failer factory')\n    failer = fac.select('branch').select('failer').create('failer')\n    runModule(failer)\n    try:\n        waitAll()\n    except RuntimeError:\n        print('ok, waitAll raised exception')\n    else:\n        raise RuntimeError('an error should have been raised')\n"]]}
{"hexsha": "b3d7f177513de669b8b68077fe962eb68f228b7f", "ext": "py", "lang": "Python", "content": "def align_session(session, audiopath, outpath, chans=None):\n    \"\"\"Align all channels within a given session.\"\"\"\n    chime_data = tu.chime_data()\n\n    # The first binaural recorder is taken as the reference\n    ref_chan = chime_data[session]['pids'][0]\n\n    # If chans not specified then use all channels available\n    if chans is None:  \n        pids = chime_data[session]['pids']\n        kinects = chime_data[session]['kinects']\n        chans = pids[1:] + kinects\n\n    all_results = dict()  # Empty dictionary for storing results\n\n    for target_chan in chans:\n        print(target_chan)\n\n        # For dealing with channels with big missing audio segments\n        missing = None\n        if (('missing' in chime_data[session] and\n             target_chan in chime_data[session]['missing'])):\n            missing = chime_data[session]['missing'][target_chan]\n\n        # Parameters for alignment depend on whether target is\n        # a binaural mic ('P') or a kinect mic\n        if target_chan[0] == 'P':\n            search_duration = BINAURAL_SEARCH_DURATION\n            template_duration = BINAURAL_TEMPLATE_DURATION\n            alignment_resolution = BINAURAL_RESOLUTION\n            target_chan_name = target_chan\n        else:\n            search_duration = KINECT_SEARCH_DURATION\n            template_duration = KINECT_TEMPLATE_DURATION\n            alignment_resolution = KINECT_RESOLUTION\n            target_chan_name = target_chan + '.CH1'\n\n        # Place it try-except block so that can continue\n        # if a channel fails. This shouldn't happen unless\n        # there is some problem reading the audio data.\n        try:\n            offset = 0\n            if missing is not None:\n                _, offset = missing\n\n            ref_fn = f'{audiopath}/{session}_{ref_chan}.wav'\n            target_fn = f'{audiopath}/{session}_{target_chan_name}.wav'\n\n            # Will analyse the alignment offset at regular intervals\n            session_duration = int(min(wavfile_duration(ref_fn) - offset,\n                                   wavfile_duration(target_fn))\n                                   - template_duration - search_duration)\n            analysis_times = range(alignment_resolution, session_duration, alignment_resolution)\n\n            # Run the alignment code and store results in dictionary\n            all_results[target_chan] = \\\n                align_channels(ref_fn, \n                               target_fn,\n                               analysis_times, \n                               search_duration,\n                               template_duration, \n                               missing=missing)\n        except:\n            traceback.print_exc()\n\n    pickle.dump(all_results, open(f'{outpath}/align.{session}.p', \"wb\"))", "fn_id": 8, "class_fn": false, "repo": "boeddeker/chime5-synchronisation", "file": "estimate_alignment.py", "last_update_at": "2020-12-10T12:35:41+00:00", "question_id": "b3d7f177513de669b8b68077fe962eb68f228b7f_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def align_session(session, audiopath, outpath, chans=None):\n    \"\"\"Align all channels within a given session.\"\"\"\n    chime_data = tu.chime_data()\n    ref_chan = chime_data[session]['pids'][0]\n    if chans is None:\n        pids = chime_data[session]['pids']\n        kinects = chime_data[session]['kinects']\n        chans = pids[1:] + kinects\n    all_results = dict()\n    for target_chan in chans:\n        print(target_chan)\n        missing = None\n        if 'missing' in chime_data[session] and target_chan in chime_data[session]['missing']:\n            missing = chime_data[session]['missing'][target_chan]\n        if target_chan[0] == 'P':\n            search_duration = BINAURAL_SEARCH_DURATION\n            template_duration = BINAURAL_TEMPLATE_DURATION\n            alignment_resolution = BINAURAL_RESOLUTION\n            target_chan_name = target_chan\n        else:\n            search_duration = KINECT_SEARCH_DURATION\n            template_duration = KINECT_TEMPLATE_DURATION\n            alignment_resolution = KINECT_RESOLUTION\n            target_chan_name = target_chan + '.CH1'\n        try:\n            offset = 0\n            if missing is not None:\n                _, offset = missing\n            ref_fn = f'{audiopath}/{session}_{ref_chan}.wav'\n            target_fn = f'{audiopath}/{session}_{target_chan_name}.wav'\n            session_duration = int(min(wavfile_duration(ref_fn) - offset, wavfile_duration(target_fn)) - template_duration - search_duration)\n            analysis_times = range(alignment_resolution, session_duration, alignment_resolution)\n            all_results[target_chan] = align_channels(ref_fn, target_fn, analysis_times, search_duration, template_duration, missing=missing)\n        except:\n            traceback.print_exc()\n"]]}
{"hexsha": "64eba55d4037f01fdf216a06c8967f0c4c24b860", "ext": "py", "lang": "Python", "content": "@frappe.whitelist()\ndef update_transferred_qty(self, status):\n\tif print_debug: frappe.logger().debug(\"---radplusplus.manufacturing_controllers.update_transferred_qty---\")\n\t\"\"\" Called to refresh transferred_qty based on stock_entry\"\"\"\n\tself = frappe.get_doc(\"Work Order\", self)\n\tstatus = update_status(self,status)\n\tself.update_planned_qty()\n\tfrappe.msgprint(_(\"Work Order status is {0}\").format(status))\n\tself.notify_update()", "fn_id": 7, "class_fn": false, "repo": "radplusplus/radplusplus", "file": "radplusplus/radplusplus/controllers/manufacturing_controllers.py", "last_update_at": "2020-12-20T15:38:32+00:00", "question_id": "64eba55d4037f01fdf216a06c8967f0c4c24b860_7", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@frappe.whitelist()\ndef update_transferred_qty(self, status):\n    if print_debug:\n        frappe.logger().debug('---radplusplus.manufacturing_controllers.update_transferred_qty---')\n    ' Called to refresh transferred_qty based on stock_entry'\n    self = frappe.get_doc('Work Order', self)\n    status = update_status(self, status)\n    self.update_planned_qty()\n    frappe.msgprint(_('Work Order status is {0}').format(status))\n"]]}
{"hexsha": "6b376db563fe190172a2bcd0f2956ff8d8bf3a81", "ext": "py", "lang": "Python", "content": "def get_task_name(args):\n    task_name = 'BC'\n    task_name += '.{}'.format(args.env_id.split(\"-\")[0])\n    task_name += '.traj_limitation_{}'.format(args.traj_limitation)\n    task_name += \".seed_{}\".format(args.seed)\n    return task_name", "fn_id": 2, "class_fn": false, "repo": "jtchilders/deephyper", "file": "deephyper/search/nas/baselines/gail/behavior_clone.py", "last_update_at": "2020-08-26T09:19:13+00:00", "question_id": "6b376db563fe190172a2bcd0f2956ff8d8bf3a81_2", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def get_task_name(args):\n    task_name = 'BC'\n    task_name += '.{}'.format(args.env_id.split('-')[0])\n    task_name += '.traj_limitation_{}'.format(args.traj_limitation)\n    task_name += '.seed_{}'.format(args.seed)\n"]]}
{"hexsha": "360c8ad7a45cf87f3d372aba1e9e3c70c3244161", "ext": "py", "lang": "Python", "content": "def print_help():\n    ''' show usage help '''\n    print(\"usage: ns_ad_converter.py [type] [year] [month] [date]\")\n    print(\"\")\n    print(\"valid type [type]: to_ns | to_ad\")\n    print((\"valid range [year] [month] [date]: FROM 880 10 20 to ... (to_ns),\"\n           \" 1 1 1 to ... (to_ad)\"))\n    print(\"\")", "fn_id": 4, "class_fn": false, "repo": "brihat-rb/brihat-rb.github.io", "file": "solarnsconverter/ns_ad_converter.py", "last_update_at": "2020-04-23T08:26:54+00:00", "question_id": "360c8ad7a45cf87f3d372aba1e9e3c70c3244161_4", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def print_help():\n    \"\"\" show usage help \"\"\"\n    print('usage: ns_ad_converter.py [type] [year] [month] [date]')\n    print('')\n    print('valid type [type]: to_ns | to_ad')\n    print('valid range [year] [month] [date]: FROM 880 10 20 to ... (to_ns), 1 1 1 to ... (to_ad)')\n"]]}
{"hexsha": "5abc01d8c1be04b67c4bdf9afcd108c605937d33", "ext": "py", "lang": "Python", "content": "def test_ha_should_mark_one_user_on_email_mark_page(client):\n    helpers.create_health_authority(client)\n    helpers.login_authority(client)\n    helpers.create_user(client)\n\n    user = get_user_by_id(1)\n    res = client.post(\n        \"/marks/new\",\n        data={\"identifier\": user[\"email\"], \"duration\": 15},\n        follow_redirects=False,\n    )\n\n    user = get_user_by_id(1)\n\n    assert user[\"marked\"]\n    assert res.status_code == 302", "fn_id": 8, "class_fn": false, "repo": "Squad002/gooutsafe-service", "file": "tests/test_component/test_authority_mark.py", "last_update_at": "2020-12-12T19:29:51+00:00", "question_id": "5abc01d8c1be04b67c4bdf9afcd108c605937d33_8", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_ha_should_mark_one_user_on_email_mark_page(client):\n    helpers.create_health_authority(client)\n    helpers.login_authority(client)\n    helpers.create_user(client)\n    user = get_user_by_id(1)\n    res = client.post('/marks/new', data={'identifier': user['email'], 'duration': 15}, follow_redirects=False)\n    user = get_user_by_id(1)\n    assert user['marked']\n"]]}
{"hexsha": "b6e3968aa5cb7b8b2efe0878f96be6a53e3b6ef6", "ext": "py", "lang": "Python", "content": "def getIndex() :\n    # get the ortholog accoding to protein sequence id, that means Alloascoidea_hylecoeti@Seq_1 as the key, 0_0 as the value\n    with open(\"/Users/leyu/Documents/coding/evolution_code/orthomcl_output/orthomcl_SeqIDs_index.txt\", \"r\") as indexFile :\n        indexs = indexFile.readlines()\n\n    indexSeqId = dict()\n    for index in indexs :\n        index_Seq = index.strip().split(\": \")\n        indexSeqId[index_Seq[1]] = index_Seq[0]\n\n    return indexSeqId ", "fn_id": 0, "class_fn": false, "repo": "SysBioChalmers/Multi_scale_evolution", "file": "evolution_analysis/code/HGT_analysis/HGT_from_non_fungi/complementaryScripts/ortholog_HGT.py", "last_update_at": "2020-05-27T13:43:55+00:00", "question_id": "b6e3968aa5cb7b8b2efe0878f96be6a53e3b6ef6_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def getIndex():\n    with open('/Users/leyu/Documents/coding/evolution_code/orthomcl_output/orthomcl_SeqIDs_index.txt', 'r') as indexFile:\n        indexs = indexFile.readlines()\n    indexSeqId = dict()\n    for index in indexs:\n        index_Seq = index.strip().split(': ')\n        indexSeqId[index_Seq[1]] = index_Seq[0]\n"]]}
{"hexsha": "7cb5b926f98e1cea539ac04b351eeffb1251b000", "ext": "py", "lang": "Python", "content": "def gh_store_quad_mesh_edge_polyedge(quad_mesh, u, v):\n\tpolyedge = quad_mesh.collect_polyedge(u, v)\n\tpolyline = rs.AddPolyline([quad_mesh.vertex_coordinates(vkey) for vkey in polyedge])\n\treturn polyedge, polyline", "fn_id": 10, "class_fn": false, "repo": "BlockResearchGroup/singular", "file": "src/compas_singular/rhino/grasshopper.py", "last_update_at": "2020-04-15T10:28:49+00:00", "question_id": "7cb5b926f98e1cea539ac04b351eeffb1251b000_10", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def gh_store_quad_mesh_edge_polyedge(quad_mesh, u, v):\n    polyedge = quad_mesh.collect_polyedge(u, v)\n    polyline = rs.AddPolyline([quad_mesh.vertex_coordinates(vkey) for vkey in polyedge])\n"]]}
{"hexsha": "3d532ad5ce5fde114405a21932f02c31ad88688f", "ext": "py", "lang": "Python", "content": "def like(m):\n    #print(\"m:\", m)\n    #print(\"m.items():\", [(k,v) for k,v in m.items()])\n    a = m[\"model1::x\"]\n    ScannerBit.print(\"my_param\", 0.5) # can print custom parameters \n\n    return -a*a/2.0", "fn_id": 0, "class_fn": false, "repo": "bjfar/pyscannerbit", "file": "tests/interface_test.py", "last_update_at": "2020-10-02T21:10:07+00:00", "question_id": "3d532ad5ce5fde114405a21932f02c31ad88688f_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def like(m):\n    a = m['model1::x']\n    ScannerBit.print('my_param', 0.5)\n"]]}
{"hexsha": "9dee20267c3c857c4a320728f256f9dcabc4acdb", "ext": "py", "lang": "Python", "content": "def deconv2d(input_, output_shape,\n             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n             name=\"deconv2d\", with_w=False):\n    with tf.variable_scope(name):\n        # filter : [height, width, output_channels, in_channels]\n        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n                            initializer=tf.random_normal_initializer(stddev=stddev))\n\n        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n                                        strides=[1, d_h, d_w, 1])\n\n        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n\n        if with_w:\n            return deconv, w, biases\n        else:\n            return deconv", "fn_id": 3, "class_fn": false, "repo": "cberner/deepstep", "file": "libs/dcgan.py", "last_update_at": "2020-04-16T21:32:28+00:00", "question_id": "9dee20267c3c857c4a320728f256f9dcabc4acdb_3", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def deconv2d(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name='deconv2d', with_w=False):\n    with tf.variable_scope(name):\n        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]], initializer=tf.random_normal_initializer(stddev=stddev))\n        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n        if with_w:\n            return (deconv, w, biases)\n        else:\n"]]}
{"hexsha": "fb45c17955be14892f31d632e1f120bae095d667", "ext": "py", "lang": "Python", "content": "def Evaluate(context, string):\n    \"\"\"\n    The dyn:evaluate function evaluates a string as an XPath expression and\n    returns the resulting value, which might be a boolean, number, string,\n    node set, result tree fragment or external object. The sole argument is\n    the string to be evaluated. If the string is an invalid XPath expression,\n    an empty node-set is returned.\n\n    http://www.exslt.org/dyn/functions/evaluate/index.html\n    \"\"\"\n    string = Conversions.StringValue(string)\n    p = parser.new()\n    try:\n        result = p.parse(string).evaluate(context)\n    except SyntaxError:\n        tb = handle_traceback()\n        msg = 'Syntax error in XPath \"%s\", masked by empty node set return:\\n%s' % (string, tb.getvalue())\n        context.processor.warning(msg)\n        result = []\n    except:\n        import traceback\n        traceback.print_exc()\n        result = []\n    return result", "fn_id": 1, "class_fn": false, "repo": "aleasims/Peach", "file": "dependencies/src/4Suite-XML-1.0.2/Ft/Xml/Xslt/Exslt/Dynamic.py", "last_update_at": "2020-07-26T03:57:45+00:00", "question_id": "fb45c17955be14892f31d632e1f120bae095d667_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def Evaluate(context, string):\n    \"\"\"\n    The dyn:evaluate function evaluates a string as an XPath expression and\n    returns the resulting value, which might be a boolean, number, string,\n    node set, result tree fragment or external object. The sole argument is\n    the string to be evaluated. If the string is an invalid XPath expression,\n    an empty node-set is returned.\n\n    http://www.exslt.org/dyn/functions/evaluate/index.html\n    \"\"\"\n    string = Conversions.StringValue(string)\n    p = parser.new()\n    try:\n        result = p.parse(string).evaluate(context)\n    except SyntaxError:\n        tb = handle_traceback()\n        msg = 'Syntax error in XPath \"%s\", masked by empty node set return:\\n%s' % (string, tb.getvalue())\n        context.processor.warning(msg)\n        result = []\n    except:\n        import traceback\n        traceback.print_exc()\n        result = []\n"]]}
{"hexsha": "ad11f1bee346b20475f233ce5a92683d6b0b38bf", "ext": "py", "lang": "Python", "content": "def test_plots_modify_should_not_change_lockfile(\n    tmp_dir, dvc, run_copy_metrics, custom_template\n):\n    _write_json(tmp_dir, [{\"a\": 1, \"b\": 2}], \"metric_t.json\")\n    run_copy_metrics(\n        \"metric_t.json\",\n        \"metric.json\",\n        plots_no_cache=[\"metric.json\"],\n        name=\"copy-metrics\",\n        single_stage=False,\n    )\n\n    (tmp_dir / PIPELINE_LOCK).unlink()\n    dvc.plots.modify(\n        \"metric.json\", props={\"template\": relpath(custom_template)}\n    )\n    assert not (tmp_dir / PIPELINE_LOCK).exists()", "fn_id": 1, "class_fn": false, "repo": "indhupriya/dvc", "file": "tests/func/plots/test_modify.py", "last_update_at": "2020-08-01T08:31:18+00:00", "question_id": "ad11f1bee346b20475f233ce5a92683d6b0b38bf_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_plots_modify_should_not_change_lockfile(tmp_dir, dvc, run_copy_metrics, custom_template):\n    _write_json(tmp_dir, [{'a': 1, 'b': 2}], 'metric_t.json')\n    run_copy_metrics('metric_t.json', 'metric.json', plots_no_cache=['metric.json'], name='copy-metrics', single_stage=False)\n    (tmp_dir / PIPELINE_LOCK).unlink()\n    dvc.plots.modify('metric.json', props={'template': relpath(custom_template)})\n"]]}
{"hexsha": "8d600c9ed6b041bdeb712b54fef9597f1b181b1f", "ext": "py", "lang": "Python", "content": "def test_init_existing_project(init_project_fixture, zeff_configuration):\n    \"\"\"Run init on existing project and verify files and contents.\"\"\"\n    env = {}\n    args = [\"init\", \"generic\"]\n    options = zeff.cli.parse_commandline(args, config=zeff_configuration)\n    with patch(\"builtins.input\", new=mock_user_input) as mock_input:\n        with patch(\"zeff.cloud.dataset.Dataset.create_dataset\") as create:\n            dataset = MagicMock(spec=Dataset)\n            dataset.dataset_id = \"mock.dataset_id\"\n            create.return_value = dataset\n            init_project(options)\n\n    def change_user_input(prompt=None):\n        mo = re.match(r\"(?P<pstr>.+?) \\[(?P<dstr>.*?)\\]\\?\", prompt)\n        assert mo is not None, f\"Invalid prompt: `{prompt}`\"\n        response = {\n            \"Configuration generator init argument\": str(\n                Path.cwd() / \"generator_arg_mock\"\n            ),\n            \"Record builder init argument\": str(Path.cwd() / \"builder_arg_mock\"),\n        }\n        return response.get(mo.groupdict()[\"pstr\"], \"\")\n\n    with patch(\"builtins.input\", new=change_user_input) as mock_input:\n        with patch(\"zeff.cloud.dataset.Dataset.create_dataset\") as create:\n            dataset = MagicMock(spec=Dataset)\n            dataset.dataset_id = \"mock.dataset_id\"\n            create.return_value = dataset\n            init_project(options)\n            assert_zeff_conf()", "fn_id": 6, "class_fn": false, "repo": "ziff/ZeffClient", "file": "tests/zeffcliTestSuite/test_init.py", "last_update_at": "2020-04-30T16:33:45+00:00", "question_id": "8d600c9ed6b041bdeb712b54fef9597f1b181b1f_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def test_init_existing_project(init_project_fixture, zeff_configuration):\n    \"\"\"Run init on existing project and verify files and contents.\"\"\"\n    env = {}\n    args = ['init', 'generic']\n    options = zeff.cli.parse_commandline(args, config=zeff_configuration)\n    with patch('builtins.input', new=mock_user_input) as mock_input:\n        with patch('zeff.cloud.dataset.Dataset.create_dataset') as create:\n            dataset = MagicMock(spec=Dataset)\n            dataset.dataset_id = 'mock.dataset_id'\n            create.return_value = dataset\n            init_project(options)\n\n    def change_user_input(prompt=None):\n        mo = re.match('(?P<pstr>.+?) \\\\[(?P<dstr>.*?)\\\\]\\\\?', prompt)\n        assert mo is not None, f'Invalid prompt: `{prompt}`'\n        response = {'Configuration generator init argument': str(Path.cwd() / 'generator_arg_mock'), 'Record builder init argument': str(Path.cwd() / 'builder_arg_mock')}\n        return response.get(mo.groupdict()['pstr'], '')\n    with patch('builtins.input', new=change_user_input) as mock_input:\n        with patch('zeff.cloud.dataset.Dataset.create_dataset') as create:\n            dataset = MagicMock(spec=Dataset)\n            dataset.dataset_id = 'mock.dataset_id'\n            create.return_value = dataset\n            init_project(options)\n"]]}
{"hexsha": "03043fa778e8dd29dbef454ad82f1a06933c489a", "ext": "py", "lang": "Python", "content": "def detect_patterns(input_path: str, output_path: Optional[str] = None,\n                    patterns: Optional[str] = None) -> int:\n    enabled_matchers = (matchers[p] for p in patterns) if patterns else matchers.values()\n    patterns = AppFactory(input_path).create_pattern_finder(enabled_matchers).patterns()\n\n    if output_path:\n        json.encode_patterns(patterns, output_path)\n    else:\n        for pattern in sorted(patterns):\n            print(pattern)\n\n    return 0", "fn_id": 0, "class_fn": false, "repo": "IvanoBilenchi/umlens", "file": "app/controller.py", "last_update_at": "2020-12-01T10:49:03+00:00", "question_id": "03043fa778e8dd29dbef454ad82f1a06933c489a_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def detect_patterns(input_path: str, output_path: Optional[str]=None, patterns: Optional[str]=None) -> int:\n    enabled_matchers = (matchers[p] for p in patterns) if patterns else matchers.values()\n    patterns = AppFactory(input_path).create_pattern_finder(enabled_matchers).patterns()\n    if output_path:\n        json.encode_patterns(patterns, output_path)\n    else:\n        for pattern in sorted(patterns):\n            print(pattern)\n"]]}
{"hexsha": "cb8f32025446e825956a5aee487699a63e09fbb9", "ext": "py", "lang": "Python", "content": "def PrintSolution():\n    if sa_obj is None:\n        print(\"Instance not solved correctly.\")\n        return\n    print(\"Best TSP tour length: \", GetBestDist())\n    sa_obj.PrintConvergence()\n    \n    print(\"Best TSP tour: \", GetBestTour())\n    PrintBestTour()", "fn_id": 5, "class_fn": false, "repo": "hcluo/satsp", "file": "satsp/solver.py", "last_update_at": "2020-05-26T21:34:51+00:00", "question_id": "cb8f32025446e825956a5aee487699a63e09fbb9_5", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def PrintSolution():\n    if sa_obj is None:\n        print('Instance not solved correctly.')\n        return\n    print('Best TSP tour length: ', GetBestDist())\n    sa_obj.PrintConvergence()\n    print('Best TSP tour: ', GetBestTour())\n"]]}
{"hexsha": "225edfcc9f8e7320473f2236c21115016fff4784", "ext": "py", "lang": "Python", "content": "@Resolver.register\n@isLoggedIn\ndef play(plugin, channel_id, showtime=None, srno=None):\n    with open(EXTRA_CHANNELS, \"r\") as f:\n        extra = json.load(f)\n    if showtime is None and extra.get(str(channel_id)):\n        return PLAY_EX_URL + extra.get(str(channel_id)).get(\"data\")\n\n    rjson = {\n        \"channel_id\": int(channel_id),\n        \"stream_type\": \"Seek\"\n    }\n    if showtime and srno:\n        rjson[\"showtime\"] = showtime\n        rjson[\"srno\"] = srno\n        rjson[\"stream_type\"] = \"Catchup\"\n\n    resp = urlquick.post(GET_CHANNEL_URL, json=rjson).json()\n    return Listitem().from_dict(**{\n        \"label\": plugin._title,\n        \"callback\": resp.get(\"result\", \"\") + \"?\" + urlencode(getTokenParams()),\n        \"properties\": {\n            \"IsPlayable\": True,\n            \"inputstreamaddon\": \"inputstream.adaptive\",\n            \"inputstream.adaptive.stream_headers\": \"User-Agent=KAIOS\",\n            \"inputstream.adaptive.manifest_type\": \"hls\",\n            \"inputstream.adaptive.license_key\": urlencode(getTokenParams()) + \"|\" + urlencode(getHeaders()) + \"|R{SSM}|\",\n        }\n    })", "fn_id": 6, "class_fn": false, "repo": "rrosajp/repository.botallen", "file": "plugin.video.jiotv/resources/lib/main.py", "last_update_at": "2020-04-01T14:43:04+00:00", "question_id": "225edfcc9f8e7320473f2236c21115016fff4784_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@Resolver.register\n@isLoggedIn\ndef play(plugin, channel_id, showtime=None, srno=None):\n    with open(EXTRA_CHANNELS, 'r') as f:\n        extra = json.load(f)\n    if showtime is None and extra.get(str(channel_id)):\n        return PLAY_EX_URL + extra.get(str(channel_id)).get('data')\n    rjson = {'channel_id': int(channel_id), 'stream_type': 'Seek'}\n    if showtime and srno:\n        rjson['showtime'] = showtime\n        rjson['srno'] = srno\n        rjson['stream_type'] = 'Catchup'\n    resp = urlquick.post(GET_CHANNEL_URL, json=rjson).json()\n"]]}
{"hexsha": "c7b317f22e40f5de16f8d702d737dfbbfe164a5d", "ext": "py", "lang": "Python", "content": "@sched.scheduled_job('cron', hour=2, minute=00, day_of_week='sat')\ndef dividend_run():\n    loop = 8\n    while loop >= 0:\n        try:\n            DividendFinancingRecorder().run()\n            RightsIssueDetailRecorder().run()\n            SPODetailRecorder().run()\n            DividendDetailRecorder().run()\n\n            break\n        except Exception as e:\n            loop -= 1\n            logger.exception('eastmoney dividend_run runner error:{}'.format(e))\n            time.sleep(60*(10-loop))\n", "fn_id": 0, "class_fn": false, "repo": "jsrenywei/zvt", "file": "script/eastmoney_run_recorder.py", "last_update_at": "2020-01-03T06:24:30+00:00", "question_id": "c7b317f22e40f5de16f8d702d737dfbbfe164a5d_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@sched.scheduled_job('cron', hour=2, minute=0, day_of_week='sat')\ndef dividend_run():\n    loop = 8\n    while loop >= 0:\n        try:\n            DividendFinancingRecorder().run()\n            RightsIssueDetailRecorder().run()\n            SPODetailRecorder().run()\n            DividendDetailRecorder().run()\n            break\n        except Exception as e:\n            loop -= 1\n            logger.exception('eastmoney dividend_run runner error:{}'.format(e))\n"]]}
{"hexsha": "d03f776dd9f6581f454d5320aed6625b9c6770c0", "ext": "py", "lang": "Python", "content": "@argh.arg(\"--join-table\", \"-j\", type=str, required=True)\n@argh.arg(\"--keys\", \"-k\", nargs=\"+\", type=str, required=True)\ndef main(join_table=None, keys=None):\n    with open(join_table, \"r\") as f:\n        reader = csv.reader(f)\n        join_columns = reader.next()\n        join_selector = make_selector(keys, join_columns)\n\n        join_records = dict()\n\n        for record in reader:\n            key = join_selector(record)\n            join_records.setdefault(key, tuple())\n            join_records[key] += (record,)\n\n    reader = csv.reader(sys.stdin)\n    input_columns = reader.next()\n    input_selector = make_selector(keys, input_columns)\n\n    columns = (input_columns\n               + [column for column in join_columns\n                         if column not in keys])\n\n    writer = csv.writer(sys.stdout)\n    writer.writerow(columns)\n\n    for record in reader:\n        key = input_selector(record)\n\n        if key not in join_records:\n            writer.writerow(\n                record\n                + [None] * (len(columns) - len(input_columns)))\n        else:\n            for join_record in join_records.get(key):\n                writer.writerow(\n                    record\n                    + [join_record[i] for i, column in enumerate(join_columns)\n                                      if column not in keys])", "fn_id": 1, "class_fn": false, "repo": "gn0/sacsv", "file": "sacsv/csvleftjoin.py", "last_update_at": "2020-10-12T20:10:45+00:00", "question_id": "d03f776dd9f6581f454d5320aed6625b9c6770c0_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@argh.arg('--join-table', '-j', type=str, required=True)\n@argh.arg('--keys', '-k', nargs='+', type=str, required=True)\ndef main(join_table=None, keys=None):\n    with open(join_table, 'r') as f:\n        reader = csv.reader(f)\n        join_columns = reader.next()\n        join_selector = make_selector(keys, join_columns)\n        join_records = dict()\n        for record in reader:\n            key = join_selector(record)\n            join_records.setdefault(key, tuple())\n            join_records[key] += (record,)\n    reader = csv.reader(sys.stdin)\n    input_columns = reader.next()\n    input_selector = make_selector(keys, input_columns)\n    columns = input_columns + [column for column in join_columns if column not in keys]\n    writer = csv.writer(sys.stdout)\n    writer.writerow(columns)\n    for record in reader:\n        key = input_selector(record)\n        if key not in join_records:\n            writer.writerow(record + [None] * (len(columns) - len(input_columns)))\n        else:\n            for join_record in join_records.get(key):\n"]]}
{"hexsha": "ea0c4d79e0cb53326aa689828b7f860f052970e3", "ext": "py", "lang": "Python", "content": "@db_api.context_manager.reader\ndef _vnf_package_get_by_id(context, package_uuid, columns_to_join=None):\n\n    query = api.model_query(context, models.VnfPackage,\n                            read_deleted=\"no\", project_only=True). \\\n        filter_by(id=package_uuid).options(joinedload('_metadata'))\n\n    if columns_to_join:\n        for column in columns_to_join:\n            query = query.options(joinedload(column))\n\n    result = query.first()\n\n    if not result:\n        raise exceptions.VnfPackageNotFound(id=package_uuid)\n\n    return result", "fn_id": 1, "class_fn": false, "repo": "hotephen/tacker_r", "file": "tacker/objects/vnf_package.py", "last_update_at": "2020-06-18T07:24:59+00:00", "question_id": "ea0c4d79e0cb53326aa689828b7f860f052970e3_1", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@db_api.context_manager.reader\ndef _vnf_package_get_by_id(context, package_uuid, columns_to_join=None):\n    query = api.model_query(context, models.VnfPackage, read_deleted='no', project_only=True).filter_by(id=package_uuid).options(joinedload('_metadata'))\n    if columns_to_join:\n        for column in columns_to_join:\n            query = query.options(joinedload(column))\n    result = query.first()\n    if not result:\n        raise exceptions.VnfPackageNotFound(id=package_uuid)\n"]]}
{"hexsha": "14acda7eaaf974f831b8ad931b14001810580688", "ext": "py", "lang": "Python", "content": "def make_difficulty_bar(df, sheet):\n    plt.figure()\n    series = df.loc[sheet]\n    labels, vals = series.index, series.values\n    plt.bar(labels, vals)\n    plt.title(f\"Perceived difficulty of sheet {sheet}\")", "fn_id": 6, "class_fn": false, "repo": "DormBrand/infomark-scripts", "file": "questionnaire-plot.py", "last_update_at": "2020-06-18T10:14:57+00:00", "question_id": "14acda7eaaf974f831b8ad931b14001810580688_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def make_difficulty_bar(df, sheet):\n    plt.figure()\n    series = df.loc[sheet]\n    labels, vals = (series.index, series.values)\n    plt.bar(labels, vals)\n"]]}
{"hexsha": "6314a6d5aa14dd4c4d4b0adfdddcca55b001979b", "ext": "py", "lang": "Python", "content": "def remove_nth_from_end_1(head, n):\n    helper = ListNode(0)\n    helper.next = head\n\n    first = helper\n    second = helper\n\n    # count to N with the first pointer\n    for i in range(n + 1):\n        first = first.next\n\n    # go (Length - N) elements with first pointer\n    # and in that way the second pointer will be Nth from the end\n    while first != None:\n        first = first.next\n        second = second.next\n\n    # remove the element (change the next pointer from the previous element)\n    second.next = second.next.next\n\n    return helper.next", "fn_id": 0, "class_fn": false, "repo": "accbest/coding-problems", "file": "Linked Lists/remove_nth_ll.py", "last_update_at": "2020-05-17T12:05:13+00:00", "question_id": "6314a6d5aa14dd4c4d4b0adfdddcca55b001979b_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def remove_nth_from_end_1(head, n):\n    helper = ListNode(0)\n    helper.next = head\n    first = helper\n    second = helper\n    for i in range(n + 1):\n        first = first.next\n    while first != None:\n        first = first.next\n        second = second.next\n    second.next = second.next.next\n"]]}
{"hexsha": "1e33d8d011c6d736a3545d4965716342cd10d0cd", "ext": "py", "lang": "Python", "content": "def pressionProspectionEpci(connection, nom_epci_simple):\n    if config.GROS_JEU_DONNEES:\n        sql = \"\"\"SELECT\n                obs.id_maille,\n                obs.geojson_maille,\n                a.nom_organisme AS orgaobs, \n                count(obs.id_observation) as nbobs,\n                max(extract(year from dateobs)) as annee\n            FROM atlas.vm_observations_mailles obs\n            JOIN atlas.vm_observations o ON o.id_observation = obs.id_observation\n            JOIN atlas.vm_taxons t ON t.cd_ref=o.cd_ref\n            JOIN atlas.l_communes_epci ec ON ec.insee = o.insee\n            JOIN atlas.vm_epci e ON ec.id = e.id\n            JOIN atlas.vm_organismes a ON a.id_organisme = o.id_organisme \n            WHERE e.nom_epci_simple = :thisNomEpciSimple\n            GROUP BY\n                obs.id_maille,\n                obs.geojson_maille,\n                a.nom_organisme\n            ORDER BY obs.id_maille\"\"\"    \n    else:\n        sql = \"\"\"SELECT\n                obs.id_maille,\n                obs.geojson_maille,\n                a.nom_organisme AS orgaobs, \n                o.dateobs,\n                extract(YEAR FROM o.dateobs) as annee\n            FROM atlas.vm_observations_mailles obs\n            JOIN atlas.vm_observations o ON o.id_observation = obs.id_observation\n            JOIN atlas.vm_taxons t ON t.cd_ref=o.cd_ref\n            JOIN atlas.l_communes_epci ec ON ec.insee = o.insee\n            JOIN atlas.vm_epci e ON ec.id = e.id\n            JOIN atlas.vm_organismes a ON a.id_organisme = o.id_organisme \n           WHERE e.nom_epci_simple = :thisNomEpciSimple\n            ORDER BY id_maille\"\"\"\n\n\n    observations = connection.execute(text(sql), thisNomEpciSimple=nom_epci_simple)\n    tabObs = list()\n\n    if config.GROS_JEU_DONNEES:\n        for o in observations:\n            temp = {\n                'id_maille': o.id_maille,\n                'nb_observations': o.nbobs,\n                'annee': o.annee,\n                'dateobs': None,\n                'orga_obs': o.orgaobs,\n                'geojson_maille': ast.literal_eval(o.geojson_maille)\n            }\n            tabObs.append(temp)\n    else:\n        for o in observations:\n            temp = {\n                'id_maille': o.id_maille,\n                'nb_observations': 1,\n                'annee': o.annee,\n                'dateobs': str(o.dateobs),\n                'orga_obs': o.orgaobs,\n                'geojson_maille': ast.literal_eval(o.geojson_maille)\n            }\n            tabObs.append(temp)\n    return tabObs", "fn_id": 6, "class_fn": false, "repo": "Splendens/atlas_biodiv_pdl", "file": "main/modeles/repositories/vmObservationsMaillesRepository.py", "last_update_at": "2020-11-21T06:43:18+00:00", "question_id": "1e33d8d011c6d736a3545d4965716342cd10d0cd_6", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def pressionProspectionEpci(connection, nom_epci_simple):\n    if config.GROS_JEU_DONNEES:\n        sql = 'SELECT\\n                obs.id_maille,\\n                obs.geojson_maille,\\n                a.nom_organisme AS orgaobs, \\n                count(obs.id_observation) as nbobs,\\n                max(extract(year from dateobs)) as annee\\n            FROM atlas.vm_observations_mailles obs\\n            JOIN atlas.vm_observations o ON o.id_observation = obs.id_observation\\n            JOIN atlas.vm_taxons t ON t.cd_ref=o.cd_ref\\n            JOIN atlas.l_communes_epci ec ON ec.insee = o.insee\\n            JOIN atlas.vm_epci e ON ec.id = e.id\\n            JOIN atlas.vm_organismes a ON a.id_organisme = o.id_organisme \\n            WHERE e.nom_epci_simple = :thisNomEpciSimple\\n            GROUP BY\\n                obs.id_maille,\\n                obs.geojson_maille,\\n                a.nom_organisme\\n            ORDER BY obs.id_maille'\n    else:\n        sql = 'SELECT\\n                obs.id_maille,\\n                obs.geojson_maille,\\n                a.nom_organisme AS orgaobs, \\n                o.dateobs,\\n                extract(YEAR FROM o.dateobs) as annee\\n            FROM atlas.vm_observations_mailles obs\\n            JOIN atlas.vm_observations o ON o.id_observation = obs.id_observation\\n            JOIN atlas.vm_taxons t ON t.cd_ref=o.cd_ref\\n            JOIN atlas.l_communes_epci ec ON ec.insee = o.insee\\n            JOIN atlas.vm_epci e ON ec.id = e.id\\n            JOIN atlas.vm_organismes a ON a.id_organisme = o.id_organisme \\n           WHERE e.nom_epci_simple = :thisNomEpciSimple\\n            ORDER BY id_maille'\n    observations = connection.execute(text(sql), thisNomEpciSimple=nom_epci_simple)\n    tabObs = list()\n    if config.GROS_JEU_DONNEES:\n        for o in observations:\n            temp = {'id_maille': o.id_maille, 'nb_observations': o.nbobs, 'annee': o.annee, 'dateobs': None, 'orga_obs': o.orgaobs, 'geojson_maille': ast.literal_eval(o.geojson_maille)}\n            tabObs.append(temp)\n    else:\n        for o in observations:\n            temp = {'id_maille': o.id_maille, 'nb_observations': 1, 'annee': o.annee, 'dateobs': str(o.dateobs), 'orga_obs': o.orgaobs, 'geojson_maille': ast.literal_eval(o.geojson_maille)}\n            tabObs.append(temp)\n"]]}
{"hexsha": "53c2e307ae080cda4fd9bcc1d3a071a0262d3734", "ext": "py", "lang": "Python", "content": "@mock.patch('multiprocessing.pool.ThreadPool', test_utils.MockPool)\n@mock.patch('common.new_process.execute')\n@mock.patch('common.filesystem.directories_have_same_files')\n@pytest.mark.skip(reason=\"See crbug.com/1012329\")\ndef test_measure_all_trials_no_more(mocked_directories_have_same_files,\n                                    mocked_execute):\n    \"\"\"Test measure_all_trials does what is intended when the experiment is\n    done.\"\"\"\n    mocked_directories_have_same_files.return_value = True\n    mocked_execute.return_value = new_process.ProcessResult(0, '', False)\n    mock_pool = test_utils.MockPool()\n    assert not measurer.measure_all_trials(\n        experiment_utils.get_experiment_name(), MAX_TOTAL_TIME, mock_pool,\n        queue.Queue())", "fn_id": 10, "class_fn": false, "repo": "zchcai/fuzzbench", "file": "experiment/test_measurer.py", "last_update_at": "2020-07-29T18:13:14+00:00", "question_id": "53c2e307ae080cda4fd9bcc1d3a071a0262d3734_10", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["@mock.patch('multiprocessing.pool.ThreadPool', test_utils.MockPool)\n@mock.patch('common.new_process.execute')\n@mock.patch('common.filesystem.directories_have_same_files')\n@pytest.mark.skip(reason='See crbug.com/1012329')\ndef test_measure_all_trials_no_more(mocked_directories_have_same_files, mocked_execute):\n    \"\"\"Test measure_all_trials does what is intended when the experiment is\n    done.\"\"\"\n    mocked_directories_have_same_files.return_value = True\n    mocked_execute.return_value = new_process.ProcessResult(0, '', False)\n    mock_pool = test_utils.MockPool()\n"]]}
{"hexsha": "0b69bc56064c9d36d3700fefed89530bd5049f63", "ext": "py", "lang": "Python", "content": "def find_host_name(indice):\n    search_first = es_client.search(\n        index=indice,\n        body={\n            \"sort\": [\n                {\"@timestamp\": \"desc\"}\n            ],\n            \"size\": 1,\n        }\n    )\n    host_name = search_first['hits']['hits'][0]['_source']['host']['name']\n    return host_name", "fn_id": 0, "class_fn": false, "repo": "leesk212/Sysmon-EL-Python_PyQt", "file": "Code/SeokMin.py", "last_update_at": "2020-08-25T02:50:33+00:00", "question_id": "0b69bc56064c9d36d3700fefed89530bd5049f63_0", "category": "coding", "instruct": "Please complete the following code and only return the code.", "turns": [["def find_host_name(indice):\n    search_first = es_client.search(index=indice, body={'sort': [{'@timestamp': 'desc'}], 'size': 1})\n    host_name = search_first['hits']['hits'][0]['_source']['host']['name']\n"]]}
