--- 0 --
Question ID: ddfc9694e766a47f9627d1539085cb001f48b7fb_4
Original Code:
```
def displayAssetDetails():
	print("Input the asset name")
	assetName = input()
	if assets.hasAsset(assetName):
		asset = assets.getAsset(assetName)
		print(f'\n{assetName}')
		print('Name:', asset.getName())
		print('Market Cap:', asset.getMarketCap())
		print('Price:', asset.getPrice())
		print('Circulating Supply:', asset.getCirculatingSupply())
		print('Volume:', asset.getVolume())
		print('One Hour Percent:'+asset.getOneHourPercent()+'%')
		print('Twenty Four Hour Percent:'+asset.getTwentyFourHourPercent()+'%')
		print('Seven Day Percent:'+asset.getSevenDayPercent()+'%')
	else:
		print("\nAsset doesn't exist\n")
```


Overlapping Code:
```
 displayAssetDetails():
print("Input the asset name")
assetName = input()
if assets.hasAsset(assetName):
asset = assets.getAsset(assetName)
print(f'\n{assetName}')
print('Name:', asset.getName())
print('Market Cap:', asset.getMarketCap())
print('Price:', asset.getPrice())
print('Circulating Supply:', asset.getCirculatingSupply())
print('Volume:', asset.getVolume())
print('One Hour Percent:'+asset.getOneHourPercent()+'%')
print('Twenty Four Hour Percent:'+asset.getTwentyFourHourPercent()+'%')
print('Seven Day Percent:'+asset.getSevenDayPercent()
```
<Overlap Ratio: 0.9212730318257957>

---

--- 1 --
Question ID: e5aabded9df3715d8d66b20c171cae6935a88db0_2
Original Code:
```
def update_hex_infowidget(infowidget):
    observed_hex = infowidget.game_state.grid.observed_hex
    font = pygame.font.SysFont("Calibri", 20)
    infowidget.image.fill(pr.WHITE_RGB)
    if observed_hex is not None:
        h, w = infowidget.height, infowidget.width
        triplets = [(observed_hex, h // 4, str(observed_hex.hex_type))]
        if observed_hex.object is not None:
            triplets.append((observed_hex.object, 3 * h // 4, str(observed_hex.object.character_type)))
        for obj, oh, tl in triplets:
            texture = obj.texture.copy()
            if obj.height > h // 2:
                texture = pygame.transform.scale(texture,
                                                 (obj.width, h // 2))
            texture_rect = texture.get_rect(size=texture.get_size(),
                                            center=(1 * w // 4, oh))
            infowidget.image.blit(texture, texture_rect)
            text = font.render(tl, True, pr.BLACK_RGB)
            if text.get_width() > w // 2:
                text = pygame.transform.scale(text, (w // 2, text.get_height()))
            text_rect = text.get_rect(size=text.get_size(),
                                      center=(3 * w // 4, oh))
            infowidget.image.blit(text, text_rect)
    else:
        text = font.render("Nothing to observe", True, pr.BLACK_RGB)
        text_rect = text.get_rect(size=text.get_size(),
                                  center=(infowidget.width // 2, infowidget.height // 2))
        infowidget.image.blit(text, text_rect)
```


Overlapping Code:
```
nfowidget(infowidget):
observed_hex = infowidget.game_state.grid.observed_hex
font = pygame.font.SysFont("Calibri", 20)
infowidget.image.fill(pr.WHITE_RGB)
if observed_hex is not None:
h, w = infowidget.height, infowidget.width
triplets = [(observed_hex, h // 4, str(observed_hex.hex_type))]
if observed_hex.object is not None:
triplets.append((observed_hex.object, 3 * h // 4, str(observed_hex.object.character_type)))
for obj, oh, tl in triplets:
texture = obj.texture.copy()
if obj.height > h // 2:
texture = pygame.transform.scale(texture,
(obj.width, h // 2))
texture_rect = texture.get_rect(size=texture.get_size(),
center=(1 * w // 4, oh))
infowidget.image.blit(texture, texture_rect)
text = font.render(tl, True, pr.BLACK_RGB)
if text.get_width() > w // 2:
text = pygame.transform.scale(text, (w // 2, text.get_height()))
text_rect = text.get_rect(size=text.get_size(),
center=(3 * w // 4, oh))
infowidget.image.blit(text, text_rect)
else:
text = font.render("Nothing to observe", True, pr.BLACK_RGB)
text_rect = text.get_rect(size=text.get_size(),
center=(infowidget.width // 2, infowidget.height // 2))
infowidget.image.blit(text, text_rect
```
<Overlap Ratio: 0.9854327335047129>

---

--- 2 --
Question ID: 6b468d698e3030ea2a8e970e69a4fccd67db2cb6_1
Original Code:
```
def train(model_config, loader_config):    
    '''
        This function trains the model that is passed in the first argument,
        using the arguments used afterwards.
    '''
    best_acc = 0.0
    for epoch in range(0, model_config.epochs):
        train_acc = parse_epoch(loader_config.trainloader, model_config.model, model_config.optimizer, model_config.criterion, model_config.device)
        torch.cuda.empty_cache()
        model_config.scheduler.step()
        accuracy = parse_epoch(loader_config.testloader, model_config.model, model_config.optimizer, model_config.criterion, model_config.device, train=False)
        
        # if accuracy > best_acc:
        model_config.save_model()
        best_acc = accuracy  

        if train_acc > 0.9:
            break
```


Overlapping Code:
```
g, loader_config): 
'''
This function trains the model that is passed in the first argument,
using the arguments used afterwards.
'''
best_acc = 0.0
for epoch in range(0, model_config.epochs):
train_acc = parse_epoch(loader_config.trainloader, model_config.model, model_config.optimizer, model_config.criterion, model_config.device)
torch.cuda.empty_cache()
model_config.scheduler.step()
accuracy = parse_epoch(loader_config.testloader, model_config.model, model_config.optimizer, model_config.criterion, model_config.device, train=False)

# if accuracy > best_acc:
model_config.save_model()
best_acc
```
<Overlap Ratio: 0.9104704097116844>

---

--- 3 --
Question ID: 7624926cb8ac6ce963b6ce4d55df92e231663099_1
Original Code:
```
def getPos(s):
    r = s[0] + 1
    c = 'ABCDEFGH'[s[1]]
    return c + str(r)
```


Overlapping Code:
```

```
<Overlap Ratio: 0.0>

---

--- 4 --
Question ID: c4e262116e7f8784cfb7bfd6854ce533f7849843_7
Original Code:
```
def appendToProjects():
    global projects
    state = State()

    sort = usfm_verses.verseCounts[state.ID]["sort"]
    testament = 'nt'
    if sort < 40:
        testament = 'ot'
    project = { "title": state.toc2, "id": state.ID.lower(), "sort": sort, \
                "path": "./" + makeUsfmFilename(state.ID), \
                "categories": "[ 'bible-" + testament + "' ]" }
    projects.append(project)
```


Overlapping Code:
```
ects():
global projects
state = State()
sort = usfm_verses.verseCounts[state.ID]["sort"]
testament = 'nt'
if sort < 40:
testament = 'ot'
project = { "title": state.toc2, "id": state.ID.lower(), "sort": sort, \
"path": "./" + makeUsfmFilename(state.ID), \
"categories": "[ 'bible-" + testament + "' ]" }
projects
```
<Overlap Ratio: 0.9067055393586005>

---

--- 5 --
Question ID: b922ff8e9939b3fae6971a68b806f7cde1387785_0
Original Code:
```
def portage_group_warning():
	warn_prefix = colorize("BAD", "*** WARNING ***  ")
	mylines = [
		"For security reasons, only system administrators should be",
		"allowed in the portage group.  Untrusted users or processes",
		"can potentially exploit the portage group for attacks such as",
		"local privilege escalation."
	]
	for x in mylines:
		writemsg(warn_prefix, noiselevel=-1)
		writemsg(x, noiselevel=-1)
		writemsg("\n", noiselevel=-1)
	writemsg("\n", noiselevel=-1)
```


Overlapping Code:
```
n_prefix = colorize("BAD", "*** WARNING *** ")
mylines = [
"For security reasons, only system administrators should be",
"allowed in the portage group. Untrusted users or processes",
"can potentially exploit the portage group for attacks such as",
"local privilege escalation."
]
for x in mylines:
writemsg(warn_prefix, noiselevel=-1)
writemsg(x, noiselevel=-1)
writemsg("\n", noiselevel=-1)
writemsg
```
<Overlap Ratio: 0.8830022075055187>

---

--- 6 --
Question ID: 6f3265a2804fcedfdd4236756f8a2aed5c157ffd_21
Original Code:
```
def write_spectrum_out(R):
    names = ['wavelength', 'F_lambda', 'e_F_lambda', 'Sky_lambda']
    hdu = fits.PrimaryHDU(np.array([R.rect_wave, R.flux, R.fluxerr, R.skyflux],
                                   dtype='float32'))
    hdu.header['DWAVE'] = R.rect_wave[1] - R.rect_wave[0]
    hdu.header['WAVE0'] = R.rect_wave[0]
    hdu.header['WAVESOL'] = 'WAVE0 + DWAVE * linspace(0, NAXIS1)'
    hdu.header['WAVEUNIT'] = 'A'
    hdu.header['FLUXUNIT'] = 'ergs/s/cm2/A'
    for i, name in enumerate(names):
        hdu.header['ROW%i' % (i+1)] = name
    for key in R.header.keys():
        if key in hdu.header:
            continue
        hdu.header[key] = R.header[key]
    hdu.writeto(op.join(R.path, 'spectrum_%s.fits' % R.side_name),
                overwrite=True)
```


Overlapping Code:
```
out(R):
names = ['wavelength', 'F_lambda', 'e_F_lambda', 'Sky_lambda']
hdu = fits.PrimaryHDU(np.array([R.rect_wave, R.flux, R.fluxerr, R.skyflux],
dtype='float32'))
hdu.header['DWAVE'] = R.rect_wave[1] - R.rect_wave[0]
hdu.header['WAVE0'] = R.rect_wave[0]
hdu.header['WAVESOL'] = 'WAVE0 + DWAVE * linspace(0, NAXIS1)'
hdu.header['WAVEUNIT'] = 'A'
hdu.header['FLUXUNIT'] = 'ergs/s/cm2/A'
for i, name in enumerate(names):
hdu.header['ROW%i' % (i+1)] = name
for key in R.header.keys():
if key in hdu.header:
continue
hdu.header[key] = R.header[key]
hdu.writeto(op.join(R.path, 'spectrum_%s.fits' % R.sid
```
<Overlap Ratio: 0.9331259720062208>

---

--- 7 --
Question ID: 87fd0a978d1cf015b580c9ad87216b134879573c_3
Original Code:
```
def storagepool_register(args):
    obj = StoragePool(args.ip, args.port)

    try:
        res = obj.storagepool_register_main(args.serialnumber,
                                            args.type, args.name)

    except SOSError as e:
        if(e.err_code == SOSError.NOT_FOUND_ERR):
            raise SOSError(SOSError.NOT_FOUND_ERR,
                           "Storagepool register failed: "
                           + e.err_text)
        else:
            raise e
```


Overlapping Code:
```
ister(args):
obj = StoragePool(args.ip, args.port)
try:
res = obj.storagepool_register_main(args.serialnumber,
args.type, args.name)
except SOSError as e:
if(e.err_code == SOSError.NOT_FOUND_ERR):
raise SOSError(SOSError.NOT_FOUND_ERR,
"Storagepool register failed: "
+ e.err_text
```
<Overlap Ratio: 0.89171974522293>

---

--- 8 --
Question ID: e8262f6673f183045cb8601be3e6fddd719417be_9
Original Code:
```
def wait_amd_device_plugin_ready(total_time=3600):
    while pod_is_ready_or_not("name", "amdgpu-dp-ds", "AMD-Device-Plugin") != True:
        logger.info("AMD-Device-Plugin is not ready yet. Please wait for a moment!")
        time.sleep(10)
        total_time = total_time - 10
        if total_time < 0:
            logger.error("An issue occure when starting up AMD-Device-Plugin")
            sys.exit(1)
```


Overlapping Code:
```
ef wait_amd_device_plugin_ready(total_time=3600):
while pod_is_ready_or_not("name", "amdgpu-dp-ds", "AMD-Device-Plugin") != True:
logger.info("AMD-Device-Plugin is not ready yet. Please wait for a moment!")
time.sleep(10)
total_time = total_time - 10
if total_time < 0:
logger.error("An issue occure when starting up AMD-Device-Plugi
```
<Overlap Ratio: 0.9541547277936963>

---

--- 9 --
Question ID: f911717dd5e15cb38cd861059742205ea07a3ae6_2
Original Code:
```
@app.route("/fake_news_detector", methods=["POST"])
def fake_news_detector_response():
    text = request.form['text']
    response = predict(news_model, news_vectorizer, text)
    return render_template("fake_news_detector_response.html", value = response)
```


Overlapping Code:
```
 methods=["POST"])
def fake_news_detector_response():
text = request.form['text']
response = predict(news_model, news_vectorizer, text)
return render_template("fake_news_detector_response.html", value
```
<Overlap Ratio: 0.8163265306122449>

---

--- 10 --
Question ID: 6a53ef4afee44ed94a9ea479580eb834f4e6045d_23
Original Code:
```
def find_scope(current_phil, scope_name):
  i = 0
  while (i < len(current_phil.objects)):
    full_path = current_phil.objects[i].full_path()
    if (full_path == scope_name):
      return current_phil.objects[i]
    elif (scope_name.startswith(full_path + ".")):
      return find_scope(current_phil.objects[i], scope_name)
    i += 1
```


Overlapping Code:
```
pe_name):
i = 0
while (i < len(current_phil.objects)):
full_path = current_phil.objects[i].full_path()
if (full_path == scope_name):
return current_phil.objects[i]
elif (scope_name.startswith(full_path + ".")):
return find_scope(current_phil.objects[
```
<Overlap Ratio: 0.8223684210526315>

---

--- 11 --
Question ID: e9257c65339acddd1b1ca0c6a52f786edb4edf4a_3
Original Code:
```
def get_playlist_tracks(playlist_id):
    """songname, id"""
    playlist_tracks = sp.get_playlist_tracks(playlist_id)

    tracks = []
    offset = 0
    for item in playlist_tracks['items']:
        id = item['track']['id']
        name = item['track']['name']

        tracks.append(Track(name, id))

        # if there are multiple pages
        if playlist_tracks['next'] is not None:
            playlist_tracks = sp.get_playlist_tracks(playlist_id, offset=offset)
    return tracks
```


Overlapping Code:
```
ngname, id"""
playlist_tracks = sp.get_playlist_tracks(playlist_id)
tracks = []
offset = 0
for item in playlist_tracks['items']:
id = item['track']['id']
name = item['track']['name']
tracks.append(Track(name, id))
# if there are multiple pages
if playlist_tracks['next'] is not None:
playlist_tracks = sp.get_playlist_tracks(playlist_id, offset=offse
```
<Overlap Ratio: 0.8557457212713936>

---

--- 12 --
Question ID: b6e3968aa5cb7b8b2efe0878f96be6a53e3b6ef6_0
Original Code:
```
def getIndex() :
    # get the ortholog accoding to protein sequence id, that means Alloascoidea_hylecoeti@Seq_1 as the key, 0_0 as the value
    with open("/Users/leyu/Documents/coding/evolution_code/orthomcl_output/orthomcl_SeqIDs_index.txt", "r") as indexFile :
        indexs = indexFile.readlines()

    indexSeqId = dict()
    for index in indexs :
        index_Seq = index.strip().split(": ")
        indexSeqId[index_Seq[1]] = index_Seq[0]

    return indexSeqId 
```


Overlapping Code:
```
ef getIndex() :
# get the ortholog accoding to protein sequence id, that means Alloascoidea_hylecoeti@Seq_1 as the key, 0_0 as the value
with open("/Users/leyu/Documents/coding/evolution_code/orthomcl_output/orthomcl_SeqIDs_index.txt", "r") as indexFile :
indexs = indexFile.readlines()
indexSeqId = dict()
for index in indexs :
index_Seq = index.strip().split(": ")
indexSeqId[index_Seq[1]] = index_Seq[0]
return indexSeqI
```
<Overlap Ratio: 0.9952941176470588>

---

--- 13 --
Question ID: c7e217a46f347647a01730ba6c2f44adea4ea3c1_10
Original Code:
```
@pytest.mark.parametrize(
    "config_name, expect",
    [
        ("JAVA_GATEWAY_ADDRESS", "127.0.0.1"),
        ("JAVA_GATEWAY_PORT", 25333),
        ("JAVA_GATEWAY_AUTO_CONVERT", True),
        ("USER_NAME", "userPythonGateway"),
        ("USER_PASSWORD", "userPythonGateway"),
        ("USER_EMAIL", "userPythonGateway@dolphinscheduler.com"),
        ("USER_PHONE", "11111111111"),
        ("USER_STATE", 1),
        ("WORKFLOW_PROJECT", "project-pydolphin"),
        ("WORKFLOW_TENANT", "tenant_pydolphin"),
        ("WORKFLOW_USER", "userPythonGateway"),
        ("WORKFLOW_QUEUE", "queuePythonGateway"),
        ("WORKFLOW_WORKER_GROUP", "default"),
        ("WORKFLOW_TIME_ZONE", "Asia/Shanghai"),
        ("WORKFLOW_WARNING_TYPE", "NONE"),
    ],
)
def test_get_configuration(config_name: str, expect: Any):
    """Test get exists attribute in :mod:`configuration`."""
    assert expect == getattr(configuration, config_name)
```


Overlapping Code:
```
name, expect",
[
("JAVA_GATEWAY_ADDRESS", "127.0.0.1"),
("JAVA_GATEWAY_PORT", 25333),
("JAVA_GATEWAY_AUTO_CONVERT", True),
("USER_NAME", "userPythonGa("USER_STATE", 1),
("WORKFLOW_PROJECT", "project-pydolphin"),
("WORKFLOW_TENANT", "tenant_pydolphin"),
("WORKFLOW_USER", "userPythonGateway"),
("WORKFLOW_QUEUE", "queuePythonGateway"),
("WORKFLOW_WORKER_GROUP", "default"),
("WORKFLOW_TIME_ZONE", "Asia/Shanghai"),
("WORKFLOW_WARNING_TYPE", "NONE"),
],
)
def test_get_configuration(config_name: str, expect: Any):
"""Test get exists attribute in :mod:`configuration`."""
assert expect == getattr(confi
```
<Overlap Ratio: 0.7556675062972292>

---

--- 14 --
Question ID: 7b5770c3c7844ccbf260cf913a5169f92fc3d014_0
Original Code:
```
def remove_ponto(cnpj):
    cnpj_limpo = ''
    for x in cnpj:
        if x.isnumeric():
            cnpj_limpo += x
    return cnpj_limpo
```


Overlapping Code:
```
pj):
cnpj_limpo = ''
for x in cnpj:
if x.isnumeric
```
<Overlap Ratio: 0.4716981132075472>

---

--- 15 --
Question ID: 153d99f089c9ae1ca9c52bc4b10adfc64105fd5b_2
Original Code:
```
def generate_initial_random_policy(env):
    policy = dict()

    for i in range(GRID_HEIGHT):
        for j in range(GRID_WIDTH):
            if (i, j) not in env.TERMINAL_STATES:
                actions = []
                prob = []
                for action in env.ACTIONS:
                    actions.append(action)
                    prob.append(0.25)

                policy[(i, j)] = (actions, prob)

    return policy
```


Overlapping Code:
```
enerate_initial_random_policy(env):
policy = dict()
for i in range(GRID_HEIGHT):
for j in range(GRID_WIDTH):
if (i, j) not in env.TERMINAL_STATES:
actions = []
prob = []
for action in env.ACTIONS:
actions.append(action)
prob.append(0.25)
policy[(i, j)] = (actions, prob)
return policy
```
<Overlap Ratio: 0.9826989619377162>

---

--- 16 --
Question ID: adbc181767aff9fc1b04daf91833d5790e1e9dca_0
Original Code:
```
def extract_summaries(logdir: str):
    """ extracts and pickles only relevant scalars

    :param logdir: Path to the directory having event logs
    """
    # Collect data : we recognize all files which have tfevents
    scalars_info = defaultdict(dict)

    for root, dirs, files in os.walk(logdir):
        game_name = root.split('-v')[0].split('/')[-1]
        event_files = [x for x in files if 'tfevents' in x]
        if len(event_files) > 0:
            assert len(event_files) == 1, 'only one tf file allowed per experiment.'

            if game_name not in scalars_info:
                scalars_info[game_name] = {'fixed': {}, 'variable': {}}

            event_path = os.path.join(root, event_files[0])
            acc = ea.EventAccumulator(event_path)
            acc.Reload()  # load data

            if 'fixed' in root:
                repeat_mode = 'fixed'
                repeat_count = root.split('action_repeat_')[1].split('/')[0]
                if repeat_count not in scalars_info[game_name]['fixed']:
                    scalars_info[game_name]['fixed'][repeat_count] = {'test': {'seed': {}},
                                                                      'avg_action_repeat': {'seed': {}}}
                dest = scalars_info[game_name]['fixed'][repeat_count]
            elif 'variable' in root:
                repeat_mode = 'variable'
                repeat_count = ''
                if 'test' not in scalars_info[game_name]['variable']:
                    scalars_info[game_name]['variable'] = {'test': {'seed': {}},
                                                           'avg_action_repeat': {'seed': {}}}
                dest = scalars_info[game_name]['variable']
            else:
                raise NotImplementedError

            seed = root.split('seed_')[1].split('/')[0]
            x = [s.step for s in acc.Scalars(TEST_TAG['ref'])]
            y = [s.value for s in acc.Scalars(TEST_TAG['ref'])]
            dest['test']['seed'][seed] = {'x': x, 'y': y}

            x = [s.step for s in acc.Scalars(AVG_ACTION_REPEAT_TAG['ref'])]
            y = [s.value for s in acc.Scalars(AVG_ACTION_REPEAT_TAG['ref'])]
            dest['avg_action_repeat']['seed'][seed] = {'x': x, 'y': y}

            print('Processed {}, seed:{} , mode:{} , repeat: {}'.format(game_name, seed, repeat_mode, repeat_count))

    return scalars_info
```


Overlapping Code:
```
ries(logdir: str):
""" extracts and pickles only relevant scalars
:param logdir: Path to the directory having event logs
"""
# Collect data : we recognize all files which have tfevents
scalars_info = defaultdict(dict)
for root, dirs, files in os.walk(logdir):
game_name = root.split('-v')[0].split('/')[-1]
event_files = [x for x in files if 'tfevents' in x]
if len(event_files) > 0:
assert len(event_files) == 1, 'only one tf file allowed per experiment.'
if game_name not in scalars_info:
scalars_info[game_name] = {'fixed': {}, 'variable': {}}
event_path = os.path.join(root, event_files[0])
acc = ea.EventAccumulator(event_path)
acc.Reload() # load data
if 'fixed' in root:
repeat_mode = 'fixed'
repeat_count = root.split('action_repeat_')[1].split('/')[0]
if repeat_count not in scalars_info[game_name]['fixed']:
scalars_info[game_name]['fixed'][repeat_count] = {'test': {'seed': {}},
'avg_action_repeat': {'seed': {}}}
dest = scalars_info[game_name]['fixed'][repeat_count]
elif 'variable' in root:
repeat_mode = 'variable'
repeat_count = ''
if 'test' not in scalars_info[game_name]['variable']:
scalars_info[game_name]['variable'] = {'test': {'seed': {}},
'avg_action_repeat': {'seed': {}}}
dest = scalars_info[game_name]['variable']
else:
raise NotImplementedError
seed = root.split('seed_')[1].split('/')[0]
x = [s.step for s in acc.Scalars(TEST_TAG['ref'])]
y = [s.value for s in acc.Scalars(TEST_TAG['ref'])]
dest['test']['seed'][seed] = {'x': x, 'y': y}
x = [s.step for s in acc.Scalars(AVG_ACTION_REPEAT_TAG['ref'])]
y = [s.value for s in acc.Scalars(AVG_ACTION_REPEAT_TAG['ref'])]
dest['avg_action_repeat']['seed'][seed] = {'x': x, 'y': y}
print('Processed {}, seed:{} , mode:{} , repeat: {}'.format(game_name, seed, repeat_mode, repeat_
```
<Overlap Ratio: 0.9754738015607581>

---

--- 17 --
Question ID: d9c423c56af9c8224d38bf22c0671b06cd27bee9_1
Original Code:
```
def ParseNolintSuppressions(filename, raw_line, linenum, error):
  """Updates the global list of line error-suppressions.

  Parses any NOLINT comments on the current line, updating the global
  error_suppressions store.  Reports an error if the NOLINT comment
  was malformed.

  Args:
    filename: str, the name of the input file.
    raw_line: str, the line of input text, with comments.
    linenum: int, the number of the current line.
    error: function, an error handler.
  """
  matched = Search(r'\bNOLINT(NEXTLINE)?\b(\([^)]+\))?', raw_line)
  if matched:
    if matched.group(1):
      suppressed_line = linenum + 1
    else:
      suppressed_line = linenum
    category = matched.group(2)
    if category in (None, '(*)'):  # => "suppress all"
      _error_suppressions.setdefault(None, set()).add(suppressed_line)
    else:
      if category.startswith('(') and category.endswith(')'):
        category = category[1:-1]
        if category in _ERROR_CATEGORIES:
          _error_suppressions.setdefault(category, set()).add(suppressed_line)
        elif category not in _LEGACY_ERROR_CATEGORIES:
          error(filename, linenum, 'readability/nolint', 5,
                'Unknown NOLINT error category: %s' % category)
```


Overlapping Code:
```
def ParseNolintSuppressions(filename, raw_line, linenum, error):
"""Updates the global list of line error-suppressions.
Parses any NOLINT comments on the current line, updating the global
error_suppressions store. Reports an error if the NOLINT comment
was malformed.
Args:
filename: str, the name of the input file.
raw_line: str, the line of input text, with comments.
linenum: int, the number of the current line.
error: function, an error handler.
"""
matched = Search(r'\bNOLINT(NEXTLINE)?\b(\([^)]+\))?', raw_line)
if matched:
if matched.group(1):
suppressed_line = linenum + 1
else:
suppressed_line = linenum
category = matched.group(2)
if category in (None, '(*)'): # => "suppress all"
_error_suppressions.setdefault(None, set()).add(suppressed_line)
else:
if category.startswith('(') and category.endswith(')'):
category = category[1:-1]
if category in _ERROR_CATEGORIES:
_error_suppressions.setdefault(category, set()).add(suppressed_line)
elif category not in _LEGACY_ERROR_CATEGORIES:
error(filename, linenum, 'readability/nolint', 5,
'Unknown NOLINT error category: %s' % categor
```
<Overlap Ratio: 0.9981718464351006>

---

--- 18 --
Question ID: 2de6b204572c28fb0c1a6c077e8c9cb0a8f35349_21
Original Code:
```
def perc_thresh_n(connections):

    n = len(connections)
    previous = {}

    for ind in [n-2, n-1]:
        previous[ind] = {ind}

    if connections[n-1, n-2]:
        previous[n-1].add(n-2)

    no_points = 2

    
    while (n-2) not in previous[n-1] and no_points<n:
        
        no_points += 1
        
        upto = n-no_points
        previous[upto] = {upto}

        for node_to_current in np.nonzero(connections[n-no_points, n-no_points:])[0]:
            previous[upto].update(previous[upto + node_to_current])

        nodes_from = set(upto + np.nonzero(connections[upto:, upto])[0])

        for other_node in range(upto, n):
            if previous[other_node].intersection(nodes_from):
                previous[other_node].update(previous[upto])

    return no_points
```


Overlapping Code:
```
hresh_n(connections):
n = len(connections)
previous = {}
for ind in [n-2, n-1]:
previous[ind] = {ind}
if connections[n-1, n-2]:
previous[n-1].add(n-2)
no_points = 2

while (n-2) not in previous[n-1] and no_points<n:

no_points += 1

upto = n-no_points
previous[upto] = {upto}
for node_to_current in np.nonzero(connections[n-no_points, n-no_points:])[0]:
previous[upto].update(previous[upto + node_to_current])
nodes_from = set(upto + np.nonzero(connections[upto:, upto])[0])
for other_node in range(upto, n):
if previous[other_node].intersection(nodes_from):
previous[other_node].update(previous[upto
```
<Overlap Ratio: 0.9538950715421304>

---

--- 19 --
Question ID: eface47b94c24eac8d6299c4583950766dda3374_0
Original Code:
```
def pkcs7_pad(plaintext: bytes, block_size: int=0x10) -> bytes:
    """
    Pad a message using the byte padding algorithm described in PKCS#7
    This padding scheme appends n bytes with value n, with n the amount of padding bytes.
    
    The specification describing the padding algorithm can be found here:
    https://tools.ietf.org/html/rfc2315#section-10.3
    """

    assert 0 < block_size < 0x100
    
    # If the plaintext is an exact multiple of block_size, 
    # we need to append a whole block.
    remainder = block_size - (len(plaintext) % block_size)

    return plaintext + bytes([remainder] * remainder)
```


Overlapping Code:
```
ize: int=0x10) -> bytes:
"""
Pad a message using the byte padding algorithm described in PKCS#7
This padding scheme appends n bytes with value n, with n the amount of padding bytes.

The specification describing the padding algorithm can be found here:
https://tools.ietf.org/html/rfc2315#section-10.3
"""
assert 0 < block_size < 0x100

# If the plaintext is an exact multiple of block_size, 
# we need to append a whole block.
remainder = block_size - (len(plaintext) % block_size)
return plaintext + b
```
<Overlap Ratio: 0.8809106830122592>

---

--- 20 --
Question ID: 03043fa778e8dd29dbef454ad82f1a06933c489a_0
Original Code:
```
def detect_patterns(input_path: str, output_path: Optional[str] = None,
                    patterns: Optional[str] = None) -> int:
    enabled_matchers = (matchers[p] for p in patterns) if patterns else matchers.values()
    patterns = AppFactory(input_path).create_pattern_finder(enabled_matchers).patterns()

    if output_path:
        json.encode_patterns(patterns, output_path)
    else:
        for pattern in sorted(patterns):
            print(pattern)

    return 0
```


Overlapping Code:
```
patterns(input_path: str, output_path: Optional[str] = None,
patterns: Optional[str] = None) -> int:
enabled_matchers = (matchers[p] for p in patterns) if patterns else matchers.values()
patterns = AppFactory(input_path).create_pattern_finder(enabled_matchers).patterns()
if output_path:
json.encode_patterns(patterns, output_path)
else:
for pattern 
```
<Overlap Ratio: 0.8641975308641975>

---

--- 21 --
Question ID: 6f167e5d880e5593a503c1848335d997ce05c074_1
Original Code:
```
def start():
    register_tasks()

    connection = pika.BlockingConnection(get_config())
    channel = connection.channel()

    rpc_server = RPCServer(channel, rpc_methods)
    rpc_server.rpc_register()

    job_server = JobServer(channel, job_methods)
    job_server.job_register()

    channel.start_consuming()
```


Overlapping Code:
```
nnection = pika.BlockingConnection(get_config())
channel = connection.channel()
rpc_server = RPCServer(channel, rpc_methods)
rpc_server.rpc_register()
job_server = JobServer(channel, job_methods)
job_
```
<Overlap Ratio: 0.7168458781362007>

---

--- 22 --
Question ID: c4b59ea674aa8a31f87633b437e5863be80f3ef3_11
Original Code:
```
def test_cmp():
    p1 = PseudoMotor(5)
    p2 = PseudoMotor(10)
    assert AngledJoint(p1,p2) == AngledJoint(p1, p2)
```


Overlapping Code:
```
 test_cmp():
p1 = PseudoMotor(5)
p2 = PseudoMotor(10)
assert AngledJoint(p1,p2) == AngledJoint(p1, p
```
<Overlap Ratio: 0.9523809523809523>

---

--- 23 --
Question ID: e7bbbbcf44242be8de62aa96ec4e01dbf0130092_1
Original Code:
```
def ping(input, ping_time, ping_tries):
    ping_string = "ping -c {} -w {} {} > /dev/null 2>&1"\
        .format(ping_tries, ping_time, input)
    hookenv.log('Ping command: {}'.format(ping_string), 'DEBUG')
    response = os.system(ping_string)
    if response == 0:
        return 0
    else:
        return 1
```


Overlapping Code:
```
ut, ping_time, ping_tries):
ping_string = "ping -c {} -w {} {} > /dev/null 2>&1"\
.format(ping_tries, ping_time, input)
hookenv.log('Ping command: {}'.format(ping_string), 'DEBUG')
response = os.system(ping_string)
if response == 0:
return 0
else:
re
```
<Overlap Ratio: 0.9328358208955224>

---

--- 24 --
Question ID: cc87b34f04ef1079888a151f9b233741b7d3bace_1
Original Code:
```
@app.route('/response', methods=['GET'])
def response():
    response_form = ResponseForm()
    response_form.body.data = Model.get_response()
    return render_template('response.html', form=response_form)
```


Overlapping Code:
```
ute('/response', methods=['GET'])
def response():
response_form = ResponseForm()
response_form.body.data = Model.get_response()
return render_template('response.html', form=resp
```
<Overlap Ratio: 0.9123711340206185>

---

--- 25 --
Question ID: 346ae55e96b95c36974792f3f6d58b71b72d1202_3
Original Code:
```
def test_db_update(data):
    db = data._fields.db_update
    assert isinstance(db, list)
    assert len(db) == 2
    assert set(fld.name for fld in db) == set(('name', 'age'))
```


Overlapping Code:
```
data._fields.db_update
assert isinstance(db, list)
assert len(db) == 2
assert set(fld.name for fld i
```
<Overlap Ratio: 0.625>

---

--- 26 --
Question ID: 1bcf9d1d14f51f6cb18ecf75f8c60cb4cb91e48e_2
Original Code:
```
def get_dict_descendants(p, level=0):
    """Returns a list of dictionaries, one for each family where p is
    father or mother."""

    if level < 0:
        return 1, level, None
    if p is None:
        return 0, 100, {}  # do not decrease level

    data = [p_dict_descendants(p, '1')]
    fams = p.get_children()
    height = 0
    new_level = level

    def add_family(data, family):
        height = 0
        partner, children, _family = family
        data[-1].update(p_dict_descendants(partner, '2'))
        new_level = level

        if level > 0:
            data[-1]['parents'] = []
            for ch in children:
                h, nl, d = get_dict_descendants(ch, level-1)
                height += h
                new_level = min(new_level, nl)
                data[-1]['parents'].extend(d)
            else:
                height += 1
        return height, new_level

    if fams:
        ht, new_level = add_family(data, fams[0])
        height += ht
    for family in fams[1:]:
        # Now handle other families
        # Here, replace name of p by '...'
        data.append(p_dict_descendants('...', '1'))
        ht, nl = add_family(data, family)
        height += ht
        new_level = min(new_level, nl)

    return height+2, new_level, data
```


Overlapping Code:
```
turns a list of dictionaries, one for each family where p is
father or mother."""
if level < 0:
return 1, level, None
if p is None:
return 0, 100, {} # do not decrease level
data = [p_dict_descendants(p, '1')]
fams = p.get_children()
height = 0
new_level = level
def add_family(data, family):
height = 0
partner, children, _family = family
data[-1].update(p_dict_descendants(partner, '2'))
new_level = level
if level > 0:
data[-1]['parents'] = []
for ch in children:
h, nl, d = get_dict_descendants(ch, level-1)
height += h
new_level = min(new_level, nl)
data[-1]['parents'].extend(d)
else:
height += 1
return height, new_level
if fams:
ht, new_level = add_family(data, fams[0])
height += ht
for family in fams[1:]:
# Now handle other families
# Here, replace name of p by '...'
data.append(p_dict_descendants('...', '1'))
ht, nl = add_family(data, family)
height += ht
new_level = min(new_level, nl)
```
<Overlap Ratio: 0.9221311475409836>

---

--- 27 --
Question ID: e6748189acda1eff223bb2a0be2522fd5a5e5ad3_0
Original Code:
```
def test_get_lift():
    """Check that the lift force is calculated correctly
    """
    # given
    h = 0.4
    rho = 1.3
    wsp = 4
    air_df = pd.DataFrame([[1, 0], [0.5, h/2],
                           [0, 0], [0.5, -h/2]],
                          columns=['x', 'y'])
    exp_lift_force = 0.5 * 0.2 * rho * wsp**2 * h  # h is area here
    # when
    calc_lift_force = get_lift(air_df, wsp, rho=rho)
    # then
    np.testing.assert_equal(exp_lift_force, calc_lift_force)
```


Overlapping Code:
```
est_get_lift():
"""Check that the lift force is calculated correctly
"""
# given
h = 0.4
rho = 1.3
wsp = 4
air_df = pd.DataFrame([[1, 0], [0.5, h/2],
[0, 0], [0.5, -h/2]],
columns=['x', 'y'])
exp_lift_force = 0.5 * 0.2 * rho * wsp**2 * h # h is area here
# when
calc_lift_force = get_lift(air_df, wsp, rho=rho)
# then
np.testing.assert_equal(exp_lift
```
<Overlap Ratio: 0.9234828496042217>

---

--- 28 --
Question ID: 680b55d43b4cf5b4b13423a4802a180b441ec7a4_6
Original Code:
```
def test_no_sample():
    """Expect element to pass empty sample to next element."""
    config = _fall_detect_config()
    result = 'Something'

    def sample_callback(image=None, inference_result=None, **kwargs):
        nonlocal result
        result = image is None and inference_result is None
    fall_detector = FallDetector(**config)
    output = _OutPipeElement(sample_callback=sample_callback)
    fall_detector.connect_to_next_element(output)
    fall_detector.receive_next_sample()
    assert result is True
```


Overlapping Code:
```
 test_no_sample():
"""Expect element to pass empty sample to next element."""
config = _fall_detect_config()
result = 'Something'
def sample_callback(image=None, inference_result=None, **kwargs):
nonlocal result
result = image is None and inference_result is None
fall_detector = FallDetector(**config)
output = _OutPipeElement(sample_callback=sample_callback)
fall_detector.connect_to_next_element(output)
fall_detector.receive_next_sample()
assert result
```
<Overlap Ratio: 0.9764453961456103>

---

--- 29 --
Question ID: 4a6d1498b256f25d29bcfb889fa36e24e388e3b6_4
Original Code:
```
@projects.command("delete")
@click.argument("project_identifier")
@click.option("--identifier_type", type=click.Choice(["name", "id"]), default="name")
def cli_project_delete(project_identifier, identifier_type):
    """Delete project with provided name. Returns total deleted projects"""
    project_identifier_arg = project_identifier
    logger.info("about to delete project by {0} = {1}".format(identifier_type, project_identifier))
    if identifier_type == "id":
        project_identifier_arg = int(project_identifier)
    project_delete(project_identifier_arg)
    print_success("Successfuly deleted project name: {0}".format(project_identifier))
```


Overlapping Code:
```
project_identifier")
@click.option("--identifier_type", type=click.Choice(["name", "id"]), default="name")
def cli_project_delete(project_identifier, identifier_type):
"""Delete project with provided name. Returns total deleted projects"""
project_identifier_arg = project_identifier
logger.info("about to delete project by {0} = {1}".format(identifier_type, project_identifier))
if identifier_type == "id":
project_identifier_arg = int(project_identifier)
project_delete(project_identifier_arg)
print_success("Successfuly deleted project name: {0}".form
```
<Overlap Ratio: 0.8906752411575563>

---

--- 30 --
Question ID: 1615ead0da3ff4bda664e18109bd8f5284b1df46_1
Original Code:
```
def visualize_agents(agents, agent_names):
    """Helper to generate agents in a row visualization"""
    
    n = len(agents)
    src_img = Image.new('RGB', (200*n, 300), 'black')
    
    for i, agent in enumerate(agents):
        src_img.paste(visualize_array(agent.arr), (200*i + 45, 50))

    draw = ImageDraw.Draw(src_img)
    for i, name in enumerate(agent_names):
        draw.text(
            (200*i + 55, 260),
            name,
            font=ImageFont.truetype("arial")
        )

    return src_img
```


Overlapping Code:
```
names):
"""Helper to generate agents in a row visualization"""

n = len(agents)
src_img = Image.new('RGB', (200*n, 300), 'black')

for i, agent in enumerate(agents):
src_img.paste(visualize_array(agent.arr), (200*i + 45, 50))
draw = ImageDraw.Draw(src_img)
for i, name in enumerate(agent_names):
draw.text(
(200*i + 55, 260),
name,
font=ImageFont.tru
```
<Overlap Ratio: 0.8413461538461539>

---

--- 31 --
Question ID: d829f5ac7f3264f41e3c6bc6f67d722b5f56d2d1_2
Original Code:
```
def exp_firstrun(tas):
    def func(group):
        deb = rl.first_run(group.where(group.time.dt.month < 7) > thresh, window, 'time')
        fin = rl.first_run(group.where(group.time.dt.month >= 7) < thresh, window, 'time')
        return fin - deb

    return tas.resample(time='YS').apply(func)
```


Overlapping Code:
```
def func(group):
deb = rl.first_run(group.where(group.time.dt.month < 7) > thresh, window, 'time')
fin = rl.first_run(group.where(group.time.dt.month >= 7) < thresh, window, 'time')
return fin - deb
return tas
```
<Overlap Ratio: 0.7916666666666666>

---

--- 32 --
Question ID: a3bff75ac5e1b06534652cc7d1866fca3be20f91_3
Original Code:
```
def deploy(
        functions: ListOfCloudFunctionTypes,
        project_id: str,
        region: str,
        main_py_filename: str = 'main.py',
        labels: dict = None,
        execute=False
):
    """
    - Generates `main.py` containing all the functions
    - Generates and runs `gcloud functions deploy` commands for all the functions from the list
    https://cloud.google.com/sdk/gcloud/reference/functions/deploy

    :param functions: List of `CloudFunction` types
    :param project_id: GCP Project ID
    :param region: GCP Cloud Functions Region
    :param main_py_filename: full name with path to `main.py` e.g. ```../main.py```
    :param labels: dictionary with labels
    :param execute: True if you also want to execute generated commands, False by default

    :return: list of commands to execute
    """
    main_py_generator.generate(functions, main_py_filename)
    commands = generate_commands(project_id, functions, region, labels)
    for command in commands:
        print('Executing: {}'.format(command))
        if execute:
            os.system(command)
    return commands
```


Overlapping Code:
```
 deploy(
functions: ListOfCloudFunctionTypes,
project_id: str,
region: str,
main_py_filename: str = 'main.py',
labels: dict = None,
execute=False
):
"""
- Generates `main.py` containing all the functions
- Generates and runs `gcloud functions deploy` commands for all the functions from the list
https://cloud.google.com/sdk/gcloud/reference/functions/deploy
:param functions: List of `CloudFunction` types
:param project_id: GCP Project ID
:param region: GCP Cloud Functions Region
:param main_py_filename: full name with path to `main.py` e.g. ```../main.py```
:param labels: dictionary with labels
:param execute: True if you also want to execute generated commands, False by default
:return: list of commands to execute
"""
main_py_generator.generate(functions, main_py_filename)
commands = generate_commands(project_id, functions, region, labels)
for command in commands:
print('Executing: {}'.format(command))
if execute:
os.system(command)
ret
```
<Overlap Ratio: 0.9844559585492227>

---

--- 33 --
Question ID: 222a892a1147c783fa0085707592bb7f4944af76_1
Original Code:
```
def test_asset_key_not_in_list(setup: str) -> None:
    target = setup
    collection = Collection("fake_title")
    collection.survey = "survey_id"
    collection.description = "fake_description"
    collection.license = "fake_license"
    item = Item("item_id")
    item.datetime = datetime.now()
    item.linz_geospatial_type = "black and white image"
    collection.add_item(item)
    item.collection = collection

    test_asset = Asset("./test_data/tiffs/SURVEY_1/CONTROL.tiff")
    test_asset.target = "fake_title/fake_target.tiff"
    test_asset.key_name = None
    item.add_asset(test_asset)

    with pytest.raises(Exception, match=r"No asset key set for asset ./item_id.tiff"):
        transfer_collection(item.collection, target, DataType("imagery.historic"))
```


Overlapping Code:
```
p: str) -> None:
target = setup
collection = Collection("fake_title")
collection.survey = "survey_id"
collection.description = "fake_description"
collection.license = "fake_license"
item = Item("item_id")
item.datetime = datetime.now()
item.linz_geospatial_type = "black and white image"
collection.add_item(item)
item.collection = collection
test_asset = Asset("./test_data/tiffs/SURVEY_1/CONTROL.tiff")
test_asset.target = "fake_title/fake_target.tiff"
test_asset.key_name = None
item.add_asset(test_asset)
with pytest.raises(Exception, match=r"No asset key set for asset ./item_id.tiff"):
transfer_collection(item.collection, target, DataType("imagery.his
```
<Overlap Ratio: 0.9386590584878745>

---

--- 34 --
Question ID: 842aa11162b5ae87e312df6f26e737f7209a554f_2
Original Code:
```
async def notify_leave(room, name):
    """Notify users in [room] that someone [name] left.
    [room]: list of websockets
    [name]: name of the person leaving
    """
    # if the user never identified themselves, don't mention
    # anything to anyone
    if name:
        msg = json.dumps({"content": f"{name} left the room."})
        if room:
            await asyncio.wait([user.send(msg) for user in room])
```


Overlapping Code:
```
c def notify_leave(room, name):
"""Notify users in [room] that someone [name] left.
[room]: list of websockets
[name]: name of the person leaving
"""
# if the user never identified themselves, don't mention
# anything to anyone
if name:
msg = json.dumps({"content": f"{name} left the room."})
if room:
await asyncio.wait([user.send(msg) for user in r
```
<Overlap Ratio: 0.9749303621169917>

---

--- 35 --
Question ID: 0ef4e7618c506b0a37e009f27534dcaa27e57657_1
Original Code:
```
def create_mail(subject, body='', html='false'):
    """ Create mail file.
    """
    if os.path.isfile(body):
        with open(body, 'r') as fobj:
            body = fobj.read()

    if subject or body:
        subject = re.sub(r'\s+', ' ', subject)
        if len(subject) > 200:
            subject = subject[:200] + '...'

        if is_non_ascii(subject):
            subject = Header(subject, 'utf8').encode()

        if len(body) > 10 * 1000 * 1000:
            body = body[:10 * 1000 * 1000]

        html = html.lower() == 'true'
        path = os.path.join(MAILS_PATH,
                            '{}_{}'.format(int(time.time()), uuid.uuid4()))
        with open(path, 'w') as fobj:
            json.dump({'subject': subject, 'body': body, 'html': html}, fobj)
```


Overlapping Code:
```
te_mail(subject, body='', html='false'):
""" Create mail file.
"""
if os.path.isfile(body):
with open(body, 'r') as fobj:
body = fobj.read()
if subject or body:
subject = re.sub(r'\s+', ' ', subject)
if len(subject) > 200:
subject = subject[:200] + '...'
if is_non_ascii(subject):
subject = Header(subject, 'utf8').encode()
if len(body) > 10 * 1000 * 1000:
body = body[:10 * 1000 * 1000]
html = html.lower() == 'true'
path = os.path.join(MAILS_PATH,
'{}_{}'.format(int(time.time()), uuid.uuid4()))
with open(path, 'w') as fobj:
json.dump({'subject': subject, 'body': body, 'html'
```
<Overlap Ratio: 0.9633943427620633>

---

--- 36 --
Question ID: 8c553947537e9aee480fe515a38baf2b5377262a_2
Original Code:
```
def publish(temp,hum):
	API_KEY = 'D9EBWUZRNK5VQRPS' 
	HOST = 'api.thingspeak.com'
	data = b"api_key="+ API_KEY + "&field1=" + str(temp) + "&field2=" + str(hum)
	s=_socket.socket()
	ai = _socket.getaddrinfo(HOST, 443)
	addr = ai[0][-1]
	s.connect(addr)
	s = ssl.wrap_socket(s)  
	s.write("POST /update HTTP/1.0\r\n")
	s.write("Host: " + HOST + "\r\n")
	s.write("Content-Length: " + str(len(data)) + "\r\n\r\n")
	s.write(data)
	s.close()
```


Overlapping Code:
```
om'
data = b"api_key="+ API_KEY + "&field1=" + str(temp) + "&field2=" + str(hum)
s=_socket.socket()
ai = _socket.getaddrinfo(HOST, 443)
addr = ai[0][-1]
s.connect(addr)
s = ssl.wrap_socket(s) 
s.write("POST /update HTTP/1.0\r\n")
s.write("Host: " + HOST + "\r\n")
s.write("Content-Length: " + str(len(data)) + "\r\n\
```
<Overlap Ratio: 0.7488151658767772>

---

--- 37 --
Question ID: 1049046d27bf69fb8f572aeb210b4d9a4a45cca2_1
Original Code:
```
def has_body(name):
    try:
        body_from_name(name)
    except ValueError:
        return False
    return True
```


Overlapping Code:
```
dy(name):
try:
body_from_name(name)
except ValueError:
return False
return 
```
<Overlap Ratio: 0.8426966292134831>

---

--- 38 --
Question ID: 3607b51f0527a413dfd012afbddd58e748fb41bb_2
Original Code:
```
def chunks2bio(chunks, sent_len):
    bio_tags = ['O'] * sent_len
    for (start, end, label) in chunks:
        bio_tags[start] = 'B-'+label
        for j in range(start+1, end):
            bio_tags[j] = 'I-'+label
    return bio_tags
```


Overlapping Code:
```
_len):
bio_tags = ['O'] * sent_len
for (start, end, label) in chunks:
bio_tags[start] = 'B-'+label
for j in range(start+1, end):
bio_tags[j] = 'I-'+label
return bio_ta
```
<Overlap Ratio: 0.8520408163265306>

---

--- 39 --
Question ID: 9ed2bf6abc747e5f98feaefd6e430539156bc77d_2
Original Code:
```
def map_sv_to_columns_in_dual_coef_matrix(sv_ind_by_class):
    from collections import defaultdict
    sv_ind_mapping = defaultdict(lambda: -1)
    p = 0
    for indices_per_class in sv_ind_by_class:
        indices_per_class.sort()
        for sv_index in indices_per_class:
            if sv_ind_mapping[sv_index] == -1:
                sv_ind_mapping[sv_index] = p
                p += 1
    return sv_ind_mapping
```


Overlapping Code:
```
mns_in_dual_coef_matrix(sv_ind_by_class):
from collections import defaultdict
sv_ind_mapping = defaultdict(lambda: -1)
p = 0
for indices_per_class in sv_ind_by_class:
indices_per_class.sort()
for sv_index in indices_per_class:
if sv_ind_mapping[sv_index] == -1:
sv_ind_mapping[sv_index] = p
p += 1
return sv_ind_mapping
```
<Overlap Ratio: 0.9465875370919882>

---

--- 40 --
Question ID: 2c97255ca89c177a0d423b27c58da90c9f3f60b9_4
Original Code:
```
def testISSNAppending(filename):
    '''
    Test function that just uses a few random ISSNs to check
    whether appending rows to the exisiting csv is working correctly
    '''
    issnRaws = ["0002-2667", "1096-1216",  "1000-9361"]
    for issn in issnRaws:
        infoList = getInfoFromISSN(issn)
        addEntryToISSNLibrary(filename, infoList)
```


Overlapping Code:
```
tISSNAppending(filename):
'''
Test function that just uses a few random ISSNs to check
whether appending rows to the exisiting csv is working correctly
'''
issnRaws = ["0002-2667", "1096-1216", "1000-9361"]
for issn in issnRaws:
infoList = getInfoFromISSN(issn)
addEntryToISSNLibrary(filename, infoLi
```
<Overlap Ratio: 0.967741935483871>

---

--- 41 --
Question ID: 92f69a35a83d10885396fe2decc7334caa06e8d4_1
Original Code:
```
def valid_or_test(mode, perf_dict=None):
    if perf_dict is None:
        perf_dict = {'accuracy': [], 'f1': {'naf': [], 'af': []}}
    model.eval()
    val_loss = 0
    correct = 0
    pred_list = []
    label_list = []
    with torch.no_grad():
        for batch_idx, (data, target, feature, _, _, feature_rep) in enumerate(val_loader):
            data, target, feature, feature_rep = data.to(device), target.to(device), feature.to(device), feature_rep.to(device)
            logits, _, gap = model(data, feature_rep)

            hsic_loss = independence_criterion.calc_loss(gap, feature)

            loss = classification_criterion(logits, target) + hsic_loss

            val_loss += loss.item()

            _, predicted = torch.max(logits.data, 1)

            if torch.cuda.is_available():
                correct += (predicted.detach().cpu().numpy() == target.detach().cpu().numpy()).sum()
            else:
                correct += (predicted == target).sum()

            pred_list.append(predicted.detach().cpu().numpy())
            label_list.append(target.detach().cpu().numpy())

    preds = np.concatenate(pred_list, axis=0)
    labels = np.concatenate(label_list, axis=0)

    f1_total = f1_score(labels, preds, labels=[0, 1], average=None)

    val_loss /= len(val_loader.dataset)
    epoch_accuracy = 100 * float(correct) / val_loader.dataset.__len__()

    perf_dict['accuracy'].append(epoch_accuracy)
    perf_dict['f1']['naf'].append(f1_total[0])
    perf_dict['f1']['af'].append(f1_total[1])

    if mode == 'valid' and epoch_accuracy >= np.max(perf_dict['accuracy']):
        torch.save(model.state_dict(), os.path.join(file_dir, f'{exp_name}_params.pkl'))
        print(['Saved @  ' + str(epoch_accuracy) + '%'])

    print(f'====> {mode} set loss: {val_loss:.5f}')
    print(f'{mode} accuracy: {epoch_accuracy:.4f}')
    print(f'{mode} F1: {f1_total[0]}, {f1_total[1]}')

    if mode == 'test':
        with open(os.path.join(file_dir, f'{exp_name}_test_perf.pkl'), 'wb') as handle:
            pkl.dump(perf_dict, handle, protocol=pkl.HIGHEST_PROTOCOL)

    return perf_dict
```


Overlapping Code:
```
erf_dict is None:
perf_dict = {'accuracy': [], 'f1': {'naf': [], 'af': []}}
model.eval()
val_loss = 0
correct = 0
pred_list = []
label_list = []
with torch.no_grad():
for batch_idx, (data, target, feature, _, _, feature_rep) in enumerate(val_loader):
data, target, feature, feature_rep = data.to(device), target.to(device), feature.to(device), feature_rep.to(device)
logits, _, gap = model(data, feature_rep)
hsic_loss = independence_criterion.calc_loss(gap, feature)
loss = classification_criterion(logits, target) + hsic_loss
val_loss += loss.item()
_, predicted = torch.max(logits.data, 1)
if torch.cuda.is_available():
correct += (predicted.detach().cpu().numpy() == target.detach().cpu().numpy()).sum()
else:
correct += (predicted == target).sum()
pred_list.append(predicted.detach().cpu().numpy())
label_list.append(target.detach().cpu().numpy())
preds = np.concatenate(pred_list, axis=0)
labels = np.concatenate(label_list, axis=0)
f1_total = f1_score(labels, preds, labels=[0, 1], average=None)
val_loss /= len(val_loader.dataset)
epoch_accuracy = 100 * float(correct) / val_loader.dataset.__len__()
perf_dict['accuracy'].append(epoch_accuracy)
perf_dict['f1']['naf'].append(f1_total[0])
perf_dict['f1']['af'].append(f1_total[1])
if mode == 'valid' and epoch_accuracy >= np.max(perf_dict['accuracy']):
torch.save(model.state_dict(), os.path.join(file_dir, f'{exp_name}_params.pkl'))
print(['Saved @ ' + str(epoch_accuracy) + '%'])
print(f'====> {mode} set loss: {val_loss:.5f}')
print(f'{mode} accuracy: {epoch_accuracy:.4f}')
print(f'{mode} F1: {f1_total[0]}, {f1_total[1]}')
if mode == 'test':
with open(os.path.join(file_dir, f'{exp_name}_test_perf.pkl'), 'wb') as handle:
pkl.dump(perf_dict, handle, protocol=pkl.HIGHEST_PROTOCOL)
return 
```
<Overlap Ratio: 0.9700665188470067>

---

--- 42 --
Question ID: 335382bd5fe33266872070ff2a061a4b3d32104c_9
Original Code:
```
def _info_from_first_line(line):
    """
    Gets the info from the first line of landmarks' txt. The format of the file
    is hardcoded for now, e.g. the expected numbers and fields.
    It returns a dictionary, which enables future extensions of what is returned.

    Along with the functions _from_line_to_vec, from_txt_to_numpy_points, and
    access_ln_frame they are the functions to access the single txt (with sparse
    landmarks) per video.
    Unless you know how to call the function, please avoid calling it directly, it is used
    internally by the from_txt_to_numpy_points().
    :param line:
    :return: Dict with meta-data.
    """
    info = {}
    # # get the framename of the first (assumed int!)
    info['init_framename'] = int(line[line.find(':') + 1: line.find('\t')])
    # # get the number of frames.
    line1 = line[line.find('n_frames:'):]
    info['n_frames'] = int(line1[9: line1.find('\t')])
    # # get the number of landmarks.
    line2 = line[line.find('n_landmarks:'):]
    info['n_landm'] = int(line2[12: line2.find('\n')])
    return info
```


Overlapping Code:
```
_info_from_first_line(line):
"""
Gets the info from the first line of landmarks' txt. The format of the file
is hardcoded for now, e.g. the expected numbers and fields.
It returns a dictionary, which enables future extensions of what is returned.
Along with the functions _from_line_to_vec, from_txt_to_numpy_points, and
access_ln_frame they are the functions to access the single txt (with sparse
landmarks) per video.
Unless you know how to call the function, please avoid calling it directly, it is used
internally by the from_txt_to_numpy_points().
:param line:
:return: Dict with meta-data.
"""
info = {}
# # get the framename of the first (assumed int!)
info['init_framename'] = int(line[line.find(':') + 1: line.find('\t')])
# # get the number of frames.
line1 = line[line.find('n_frames:'):]
info['n_frames'] = int(line1[9: line1.find('\t')])
# # get the number of landmarks.
line2 = line[line.find('n_landmarks:'):]
info['n_landm'] = int(lin
```
<Overlap Ratio: 0.958627648839556>

---

--- 43 --
Question ID: 56aee7432e0a8335d435e9bcae875984619fd76a_1
Original Code:
```
def train_epoch(
    epoch,
    model, 
    data_loader, 
    loss_fn, 
    optimizer, 
    device, 
    scheduler,
    print_freq = 50
    ):
    """ Train the model of a single epoch, and log progress with wandb.
    Parameters: 
    epoch (int): Which epoch number is being trained (for logging).
    model (PyTorch model): Provide a trainable model.
    data_loader (PyTorch Dataloader): Provide a data loader
    loss_fn (function): Loss function
    optimizer (PyTorch optimizer): Which optimizer to use
    scheduler (PyTorch scheduler): Update optimizer parameters at each iteration
    print_freq (int): Log frequency for wandb
    """ 
    
    model = model.train()
    train_loss = 0
    total = 0
    correct_predictions = 0
    start_time = time.time()
    total_batches = len(data_loader)
    for batch_idx, d  in enumerate(data_loader): 
        input_ids = d["input_ids"].to(device)
        attention_mask = d["attention_mask"].to(device)
        targets = d["targets"].to(device)
        outputs = model(input_ids=input_ids,attention_mask=attention_mask)
        _, preds = torch.max(outputs, dim=1)
        loss = loss_fn(outputs, targets)
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
        total += targets.size(0)
        correct_predictions += torch.sum(preds == targets)
        train_loss += loss.item()
        if (1+ batch_idx) % print_freq == 0:
            acc = 100.*float(correct_predictions)/float(total)
            print('Batch: {:3.0f}/{:3.0f}. Train Loss: {:0.3f}  Acc: {:2.0f} '.format(
                  batch_idx+1,total_batches,train_loss/(batch_idx+1),acc))
            lr = list(optimizer.param_groups)[0]['lr']
            wandb.log({'epoch': epoch, 'lr':lr , 'train_loss':train_loss/(batch_idx+1), 'train_acc':acc})
            
    print('Time/epoch: {:3.0f}'.format(time.time()-start_time))
    acc = 100.*float(correct_predictions)/float(total)
    lr = list(optimizer.param_groups)[0]['lr']
    wandb.log({'epoch': epoch, 'lr':lr , 'train_loss':train_loss/(batch_idx+1), 'train_acc':acc})
    print('-----------')
    return acc, train_loss / (1+batch_idx)
```


Overlapping Code:
```
_epoch(
epoch,
model, 
data_loader, 
loss_fn, 
optimizer, 
device, 
scheduler,
print_freq = 50
):
""" Train the model of a single epoch, and log progress with wandb.
Parameters: 
epoch (int): Which epoch number is being trained (for logging).
model (PyTorch model): Provide a trainable model.
data_loader (PyTorch Dataloader): Provide a data loader
loss_fn (function): Loss function
optimizer (PyTorch optimizer): Which optimizer to use
scheduler (PyTorch scheduler): Update optimizer parameters at each iteration
print_freq (int): Log frequency for wandb
""" 

model = model.train()
train_loss = 0
total = 0
correct_predictions = 0
start_time = time.time()
total_batches = len(data_loader)
for batch_idx, d in enumerate(data_loader): 
input_ids = d["input_ids"].to(device)
attention_mask = d["attention_mask"].to(device)
targets = d["targets"].to(device)
outputs = model(input_ids=input_ids,attention_mask=attention_mask)
_, preds = torch.max(outputs, dim=1)
loss = loss_fn(outputs, targets)
loss.backward()
nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
scheduler.step()
optimizer.zero_grad()
total += targets.size(0)
correct_predictions += torch.sum(preds == targets)
train_loss += loss.item()
if (1+ batch_idx) % print_freq == 0:
acc = 100.*float(correct_predictions)/float(total)
print('Batch: {:3.0f}/{:3.0f}. Train Loss: {:0.3f} Acc: {:2.0f} '.format(
batch_idx+1,total_batches,train_loss/(batch_idx+1),acc))
lr = list(optimizer.param_groups)[0]['lr']
wandb.log({'epoch': epoch, 'lr':lr , 'train_loss':train_loss/(batch_idx+1), 'train_acc':acc})

print('Time/epoch: {:3.0f}'.format(time.time()-start_time))
acc = 100.*float(correct_predictions)/float(total)
lr = list(optimizer.param_groups)[0]['lr']
wandb.log({'epoch': epoch, 'lr':lr , 'train_loss':train_loss/(batch_idx+1), 'train_acc':acc})
print('-----------'
```
<Overlap Ratio: 0.9741969457609269>

---

--- 44 --
Question ID: 6b376db563fe190172a2bcd0f2956ff8d8bf3a81_2
Original Code:
```
def get_task_name(args):
    task_name = 'BC'
    task_name += '.{}'.format(args.env_id.split("-")[0])
    task_name += '.traj_limitation_{}'.format(args.traj_limitation)
    task_name += ".seed_{}".format(args.seed)
    return task_name
```


Overlapping Code:
```
get_task_name(args):
task_name = 'BC'
task_name += '.{}'.format(args.env_id.split("-")[0])
task_name += '.traj_limitation_{}'.format(args.traj_limitation)
task_name += ".seed_{}".format(args.seed)
ret
```
<Overlap Ratio: 0.9216589861751152>

---

--- 45 --
Question ID: 006d4a00afb998127c6a887ca3169a1a94ec90c4_5
Original Code:
```
def _delete_edge(edges_from, n, to):
    """
    Removes an edge from the graph.

    @param      edges_from      structure which contains the edges (will be modified)
    @param      n               first vertex
    @param      to              second vertex
    @return                     the edge
    """
    le = edges_from[to]
    f = None
    for i, e in enumerate(le):
        if (e[1] == to and e[2] == n) or (e[2] == to and e[1] == n):
            f = i
            break

    assert f is not None
    del le[f]
    if len(le) == 0:
        del edges_from[to]

    le = edges_from[n]
    f = None
    for i, e in enumerate(le):
        if (e[1] == to and e[2] == n) or (e[2] == to and e[1] == n):
            f = i
            break

    assert f is not None
    keep = le[f]
    del le[f]
    if len(le) == 0:
        del edges_from[n]

    return keep
```


Overlapping Code:
```
ef _delete_edge(edges_from, n, to):
"""
Removes an edge from the graph.
@param edges_from structure which contains the edges (will be modified)
@param n first vertex
@param to second vertex
@return the edge
"""
le = edges_from[to]
f = None
for i, e in enumerate(le):
if (e[1] == to and e[2] == n) or (e[2] == to and e[1] == n):
f = i
break
assert f is not None
del le[f]
if len(le) == 0:
del edges_from[to]
le = edges_from[n]
f = None
for i, e in enumerate(le):
if (e[1] == to and e[2] == n) or (e[2] == to and e[1] == n):
f = i
break
assert f is not None
keep = le[f]
del le[f]
if len(le) == 0:
del edges_from[
```
<Overlap Ratio: 0.9760383386581469>

---

--- 46 --
Question ID: d7eda8a5af5b265f999a02095a8ce528f9ca6d94_0
Original Code:
```
def chord_parse(rn):
    str_chord = rn.figure
    ret = ''
    for char in str_chord:
        if (char == 'i') or (char == 'v') or (char == 'I') or (char == 'V'):
            ret += char
    return ret
```


Overlapping Code:
```
):
str_chord = rn.figure
ret = ''
for char in str_chord:
if (char == 'i') or (char == 'v') or (char 
```
<Overlap Ratio: 0.6024096385542169>

---

--- 47 --
Question ID: adf773952d0a6f5b264d01b0fe831d4833487a9d_0
Original Code:
```
def load_data():
    # Remove EEG subjects that don't have behavior data
    behavior_df = pu.load_behavior_data()
    conn_df = pu.load_connectivity_data()
    filt_df = conn_df.filter(items=behavior_df.index, axis=0)  # Remove EEG subjects with missing rowvals in behavior_df
    return behavior_df, filt_df
```


Overlapping Code:
```
't have behavior data
behavior_df = pu.load_behavior_data()
conn_df = pu.load_connectivity_data()
filt_df = conn_df.filter(items=behavior_df.index, axis=0) # Remove EEG subjects with missing rowvals in behavior
```
<Overlap Ratio: 0.7291666666666666>

---

--- 48 --
Question ID: 09d742f8da13770052a2bbd09a2bb6e30f034e1a_3
Original Code:
```
def __make_daybyday_interactive_timeline(
    df: pd.DataFrame,
    *,
    geo_df: geopandas.GeoDataFrame,
    value_col: str,
    transform_df_func: Callable[[pd.DataFrame], pd.DataFrame] = None,
    stage: Union[DiseaseStage, Literal[Select.ALL]] = Select.ALL,
    count: Union[Counting, Literal[Select.ALL]] = Select.ALL,
    out_file_basename: str,
    subplot_title_prefix: str,
    plot_aspect_ratio: float = None,
    cmap=None,
    n_cbar_buckets: int = None,
    n_buckets_btwn_major_ticks: int = None,
    n_minor_ticks_btwn_major_ticks: int = None,
    per_capita_denominator: int = None,
    x_range: Tuple[float, float],
    y_range: Tuple[float, float],
    min_visible_y_range: float,
    should_make_video: bool,
) -> InfoForAutoload:
    """Create the bokeh interactive timeline plot(s)

    This function takes the given DataFrame, which must contain COVID data for locations
    on different dates, and a GeoDataFrame, which contains the long/lat coords for those
    locations, and creates an interactive choropleth of the COVID data over time.

    :param df: The COVID data DataFrame
    :type df: pd.DataFrame
    :param geo_df: The geometry GeoDataFrame for the locations in `df`
    :type geo_df: geopandas.GeoDataFrame
    :param value_col: The column of `df` containing the values to plot in the
    choropleth; should be something like "Case_Counts" or "Case_Diff_From_Prev_Day"
    :type value_col: str
    :param stage: The DiseaseStage to plot, defaults to Select.ALL. If ALL, then all
    stages are plotted and are stacked vertically.
    :type stage: Union[DiseaseStage, Literal[Select.ALL]], optional
    :param count: The Counting to plot, defaults to Select.ALL. If ALL, then all
    count types are plotted and are stacked horizontally.
    :type count: Union[Counting, Literal[Select.ALL]], optional
    :param out_file_basename: The basename of the file to save the interactive plots to
    (there are two components, the JS script and the HTML <div>)
    :type out_file_basename: str
    :param subplot_title_prefix: What the first part of the subplot title should be;
    probably a function of `value_col` (if value_col is "Case_Counts" then this param
    might be "Cases" or "# of Cases")
    :type subplot_title_prefix: str
    :param x_range: The range of the x-axis as (min, max)
    :type x_range: Tuple[float, float]
    :param y_range: The range of the y-axis as (min, max)
    :type y_range: Tuple[float, float]
    :param min_visible_y_range: The minimum height (in axis units) of the y-axis; it
    will not be possible to zoom in farther than this on the choropleth.
    :type min_visible_y_range: float
    :param should_make_video: Optionally run through the timeline day by day, capture
    a screenshot for each day, and then stitch the screenshots into a video. The video
    shows the same info as the interactive plots, but not interactively. This easily
    takes 20x as long as just making the graphs themselves, so use with caution.
    :type should_make_video: bool
    :param transform_df_func: This function expects data in a certain format, and does
    a bunch of preprocessing (expected to be common) before plotting. This gives you a
    chance to do any customization on the postprocessed df before it's plotted. Defaults
    to None, in which case no additional transformation is performed.
    :type transform_df_func: Callable[[pd.DataFrame], pd.DataFrame], optional
    :param plot_aspect_ratio: The aspect ratio of the plot as width/height; if set, the
    aspect ratio will be fixed to this. Defaults to None, in which case the aspect ratio
    is determined from the x_range and y_range arguments
    :type plot_aspect_ratio: float, optional
    :param cmap: The colormap to use as either a matplotlib-compatible colormap or a
    list of hex strings (e.g., ["#ae8f1c", ...]). Defaults to None in which case a
    reasonable default is used.
    :type cmap: Matplotlib-compatible colormap or List[str], optional
    :param n_cbar_buckets: How many colorbar buckets to use. Has little effect if the
    colormap is continuous, but always works in conjunction with
    n_buckets_btwn_major_ticks to determine the number of major ticks. Defaults to 6.
    :type n_cbar_buckets: int, optional
    :param n_buckets_btwn_major_ticks: How many buckets are to lie between colorbar
    major ticks, determining how many major ticks are drawn. Defaults to 1.
    :type n_buckets_btwn_major_ticks: int, optional
    :param n_minor_ticks_btwn_major_ticks: How many minor ticks to draw between colorbar
    major ticks. Defaults to 8 (which means each pair of major ticks has 10 ticks
    total).
    :type n_minor_ticks_btwn_major_ticks: int, optional
    :param per_capita_denominator: When describing per-capita numbers, what to use as
    the denominator (e.g., cases per 100,000 people). If None, it is automatically
    computed per plot to be appropriately scaled for the data.
    :type per_capita_denominator: int, optional
    :raises ValueError: [description]
    :return: The two pieces of info required to make a Bokeh autoloading HTML+JS plot:
    the HTML div to be inserted somewhere in the HTML body, and the JS file that will
    load the plot into that div.
    :rtype: InfoForAutoload
    """

    Counting.verify(count, allow_select=True)
    DiseaseStage.verify(stage, allow_select=True)

    # The date as a string, so that bokeh can use it as a column name
    STRING_DATE_COL = "String_Date_"
    # A column whose sole purpose is to be a (the same) date associated with each
    # location
    FAKE_DATE_COL = "Fake_Date_"
    # The column we'll actually use for the colors; it's computed from value_col
    COLOR_COL = "Color_"

    # Under no circumstances may you change this date format
    # It's not just a pretty date representation; it actually has to match up with the
    # date strings computed in JS
    DATE_FMT = r"%Y-%m-%d"

    ID_COLS = [
        REGION_NAME_COL,
        Columns.DATE,
        Columns.STAGE,
        Columns.COUNT_TYPE,
    ]

    if cmap is None:
        cmap = cmocean.cm.matter

    if n_cbar_buckets is None:
        n_cbar_buckets = 6

    if n_buckets_btwn_major_ticks is None:
        n_buckets_btwn_major_ticks = 1

    if n_minor_ticks_btwn_major_ticks is None:
        n_minor_ticks_btwn_major_ticks = 8

    n_cbar_major_ticks = n_cbar_buckets // n_buckets_btwn_major_ticks + 1

    try:
        color_list = [
            # Convert matplotlib colormap to bokeh (list of hex strings)
            # https://stackoverflow.com/a/49934218
            RGB(*rgb).to_hex()
            for i, rgb in enumerate((255 * cmap(range(256))).astype("int"))
        ]
    except TypeError:
        color_list = cmap

    color_list: List[BokehColor]

    if stage is Select.ALL:
        stage_list = list(DiseaseStage)
    else:
        stage_list = [stage]

    if count is Select.ALL:
        count_list = list(Counting)
    else:
        count_list = [count]

    stage_list: List[DiseaseStage]
    count_list: List[Counting]

    stage_count_list: List[Tuple[DiseaseStage, Counting]] = list(
        itertools.product(stage_list, count_list)
    )

    df = df.copy()

    # Unadjust dates (see SaveFormats._adjust_dates)
    normalized_dates = df[Columns.DATE].dt.normalize()
    is_at_midnight = df[Columns.DATE] == normalized_dates
    df.loc[is_at_midnight, Columns.DATE] -= pd.Timedelta(days=1)
    df.loc[~is_at_midnight, Columns.DATE] = normalized_dates[~is_at_midnight]

    min_date, max_date = df[Columns.DATE].agg(["min", "max"])
    dates: List[pd.Timestamp] = pd.date_range(start=min_date, end=max_date, freq="D")
    max_date_str = max_date.strftime(DATE_FMT)

    # Get day-by-day case diffs per location, date, stage, count-type

    # Make sure data exists for every date for every state so that the entire country is
    # plotted each day; fill missing data with 0 (missing really *is* as good as 0)
    # enums will be replaced by their name (kind of important)
    id_cols_product: pd.MultiIndex = pd.MultiIndex.from_product(
        [
            df[REGION_NAME_COL].unique(),
            dates,
            [s.name for s in DiseaseStage],
            [c.name for c in Counting],
        ],
        names=ID_COLS,
    )

    df = (
        id_cols_product.to_frame(index=False)
        .merge(df, how="left", on=ID_COLS,)
        .sort_values(ID_COLS)
    )

    df[STRING_DATE_COL] = df[Columns.DATE].dt.strftime(DATE_FMT)
    df[Columns.CASE_COUNT] = df[Columns.CASE_COUNT].fillna(0)

    if transform_df_func is not None:
        df = transform_df_func(df)

    df = geo_df.merge(df, how="inner", on=REGION_NAME_COL)[
        [
            REGION_NAME_COL,
            Columns.DATE,
            STRING_DATE_COL,
            Columns.STAGE,
            Columns.COUNT_TYPE,
            value_col,
        ]
    ]

    dates: List[pd.Timestamp] = [pd.Timestamp(d) for d in df[Columns.DATE].unique()]

    values_mins_maxs = (
        df[df[value_col] > 0]
        .groupby([Columns.STAGE, Columns.COUNT_TYPE])[value_col]
        .agg(["min", "max"])
    )

    vmins: pd.Series = values_mins_maxs["min"]
    vmaxs: pd.Series = values_mins_maxs["max"]

    pow10s_series: pd.Series = vmaxs.map(lambda x: int(10 ** (-np.floor(np.log10(x)))))

    # _pow_10s_series_dict = {}
    # for stage in DiseaseStage:
    #     _pow_10s_series_dict.update(
    #         {
    #             (stage.name, Counting.TOTAL_CASES.name): 100000,
    #             (stage.name, Counting.PER_CAPITA.name): 10000,
    #         }
    #     )

    # pow10s_series = pd.Series(_pow_10s_series_dict)

    vmins: dict = vmins.to_dict()
    vmaxs: dict = vmaxs.to_dict()

    for stage in DiseaseStage:
        _value_key = (stage.name, Counting.PER_CAPITA.name)
        if per_capita_denominator is None:
            _max_pow10 = pow10s_series.loc[
                (slice(None), Counting.PER_CAPITA.name)
            ].max()
        else:
            _max_pow10 = per_capita_denominator

        vmins[_value_key] *= _max_pow10
        vmaxs[_value_key] *= _max_pow10
        pow10s_series[_value_key] = _max_pow10

    percap_pow10s: pd.Series = df.apply(
        lambda row: pow10s_series[(row[Columns.STAGE], row[Columns.COUNT_TYPE])],
        axis=1,
    )

    _per_cap_rows = df[Columns.COUNT_TYPE] == Counting.PER_CAPITA.name
    df.loc[_per_cap_rows, value_col] *= percap_pow10s.loc[_per_cap_rows]

    # Ideally we wouldn't have to pivot, and we could do a JIT join of state longs/lats
    # after filtering the data. Unfortunately this is not possible, and a long data
    # format leads to duplication of the very large long/lat lists; pivoting is how we
    # avoid that. (This seems to be one downside of bokeh when compared to plotly)
    df = (
        df.pivot_table(
            index=[REGION_NAME_COL, Columns.STAGE, Columns.COUNT_TYPE],
            columns=STRING_DATE_COL,
            values=value_col,
            aggfunc="first",
        )
        .reset_index()
        .merge(
            geo_df[[REGION_NAME_COL, LONG_COL, LAT_COL]],
            how="inner",
            on=REGION_NAME_COL,
        )
    )

    # All three oclumns are just initial values; they'll change with the date slider
    df[value_col] = df[max_date_str]
    df[FAKE_DATE_COL] = max_date_str
    df[COLOR_COL] = np.where(df[value_col] > 0, df[value_col], "NaN")

    # Technically takes a df but we don't need the index
    bokeh_data_source = ColumnDataSource(
        {k: v.tolist() for k, v in df.to_dict(orient="series").items()}
    )

    filters = [
        [
            GroupFilter(column_name=Columns.STAGE, group=stage.name),
            GroupFilter(column_name=Columns.COUNT_TYPE, group=count.name),
        ]
        for stage, count in stage_count_list
    ]

    figures = []

    for subplot_index, (stage, count) in enumerate(stage_count_list):
        # fig = bplotting.figure()
        # ax: plt.Axes = fig.add_subplot(
        #     len(stage_list), len(count_list), subplot_index
        # )

        # # Add timestamp to top right axis
        # if subplot_index == 2:
        #     ax.text(
        #         1.25,  # Coords are arbitrary magic numbers
        #         1.23,
        #         f"Last updated {NOW_STR}",
        #         horizontalalignment="right",
        #         fontsize="small",
        #         transform=ax.transAxes,
        #     )

        view = CDSView(source=bokeh_data_source, filters=filters[subplot_index])

        vmin = vmins[(stage.name, count.name)]
        vmax = vmaxs[(stage.name, count.name)]

        # Compute and set axes titles
        if stage is DiseaseStage.CONFIRMED:
            fig_stage_name = "Cases"
        elif stage is DiseaseStage.DEATH:
            fig_stage_name = "Deaths"
        else:
            raise ValueError

        fig_title_components: List[str] = []
        if subplot_title_prefix is not None:
            fig_title_components.append(subplot_title_prefix)

        fig_title_components.append(fig_stage_name)

        if count is Counting.PER_CAPITA:
            _per_cap_denom = pow10s_series[(stage.name, count.name)]
            fig_title_components.append(f"Per {_per_cap_denom:,d} people")
            formatter = PrintfTickFormatter(format=r"%2.3f")
            label_standoff = 12
            tooltip_fmt = "{0.000}"
        else:
            formatter = NumeralTickFormatter(format="0.0a")
            label_standoff = 10
            tooltip_fmt = "{0}"

        color_mapper = LogColorMapper(
            color_list, low=vmin, high=vmax, nan_color="#f2f2f2",
        )

        fig_title = " ".join(fig_title_components)

        if plot_aspect_ratio is None:
            if x_range is None or y_range is None:
                raise ValueError(
                    "Must provide both `x_range` and `y_range`"
                    + " when `plot_aspect_ratio` is None"
                )
            plot_aspect_ratio = (x_range[1] - x_range[0]) / (y_range[1] - y_range[0])

        # Create figure object
        p = bplotting.figure(
            title=fig_title,
            title_location="above",
            tools=[
                HoverTool(
                    tooltips=[
                        ("Date", f"@{{{FAKE_DATE_COL}}}"),
                        ("State", f"@{{{REGION_NAME_COL}}}"),
                        ("Count", f"@{{{value_col}}}{tooltip_fmt}"),
                    ],
                    toggleable=False,
                ),
                PanTool(),
                BoxZoomTool(match_aspect=True),
                ZoomInTool(),
                ZoomOutTool(),
                ResetTool(),
            ],
            active_drag=None,
            aspect_ratio=plot_aspect_ratio,
            output_backend="webgl",
            lod_factor=4,
            lod_interval=400,
            lod_threshold=1000,
            lod_timeout=300,
        )

        p.xgrid.grid_line_color = None
        p.ygrid.grid_line_color = None
        # Finally, add the actual choropleth data we care about
        p.patches(
            LONG_COL,
            LAT_COL,
            source=bokeh_data_source,
            view=view,
            fill_color={"field": COLOR_COL, "transform": color_mapper},
            line_color="black",
            line_width=0.25,
            fill_alpha=1,
        )

        # Add evenly spaced ticks and their labels to the colorbar
        # First major, then minor
        # Adapted from https://stackoverflow.com/a/50314773
        bucket_size = (vmax / vmin) ** (1 / n_cbar_buckets)
        tick_dist = bucket_size ** n_buckets_btwn_major_ticks

        # Simple log scale math
        major_tick_locs = (
            vmin
            * (tick_dist ** np.arange(0, n_cbar_major_ticks))
            # * (bucket_size ** 0.5) # Use this if centering ticks on buckets
        )
        # Get minor locs by linearly interpolating between major ticks
        minor_tick_locs = []
        for major_tick_index, this_major_tick in enumerate(major_tick_locs[:-1]):
            next_major_tick = major_tick_locs[major_tick_index + 1]

            # Get minor ticks as numbers in range [this_major_tick, next_major_tick]
            # and exclude the major ticks themselves (once we've used them to
            # compute the minor tick locs)
            minor_tick_locs.extend(
                np.linspace(
                    this_major_tick,
                    next_major_tick,
                    n_minor_ticks_btwn_major_ticks + 2,
                )[1:-1]
            )

        color_bar = ColorBar(
            color_mapper=color_mapper,
            ticker=FixedTicker(ticks=major_tick_locs, minor_ticks=minor_tick_locs),
            formatter=formatter,
            label_standoff=label_standoff,
            major_tick_out=0,
            major_tick_in=13,
            major_tick_line_color="white",
            major_tick_line_width=1,
            minor_tick_out=0,
            minor_tick_in=5,
            minor_tick_line_color="white",
            minor_tick_line_width=1,
            location=(0, 0),
            border_line_color=None,
            bar_line_color=None,
            orientation="vertical",
        )

        p.add_layout(color_bar, "right")
        p.hover.point_policy = "follow_mouse"

        # Bokeh axes (and most other things) are splattable
        p.axis.visible = False

        figures.append(p)

    # Make all figs pan and zoom together by setting their axes equal to each other
    # Also fix the plots' aspect ratios
    figs_iter = iter(np.ravel(figures))
    anchor_fig = next(figs_iter)

    if x_range is not None and y_range is not None:
        data_aspect_ratio = (x_range[1] - x_range[0]) / (y_range[1] - y_range[0])
    else:
        data_aspect_ratio = plot_aspect_ratio

    if x_range is not None:
        anchor_fig.x_range = Range1d(
            *x_range,
            bounds="auto",
            min_interval=min_visible_y_range * data_aspect_ratio,
        )

    if y_range is not None:
        anchor_fig.y_range = Range1d(
            *y_range, bounds="auto", min_interval=min_visible_y_range
        )

    for fig in figs_iter:
        fig.x_range = anchor_fig.x_range
        fig.y_range = anchor_fig.y_range

    # 2x2 grid (for now)
    gp = gridplot(
        figures,
        ncols=len(count_list),
        sizing_mode="scale_both",
        toolbar_location="above",
    )
    plot_layout = [gp]

    # Ok, pause
    # Now we're going into a whole other thing: we're doing all the JS logic behind a
    # date slider that changes which date is shown on the graphs. The structure of the
    # data is one column per date, one row per location, and a few extra columns to
    # store the data the graph will use. When we adjust the date of the slider, we copy
    # the relevant column of the df into the columns the graphs are looking at.
    # That's the easy part; the hard part is handling the "play button" functionality,
    # whereby the user can click one button and the date slider will periodically
    # advance itself. That requires a fair bit of logic to schedule and cancel the
    # timers and make it all feel right.

    # Create unique ID for the JS playback info object for this plot (since it'll be on
    # the webpage with other plots, and their playback info isn't shared)
    _THIS_PLOT_ID = uuid.uuid4().hex

    __TIMER = "'timer'"
    __IS_ACTIVE = "'isActive'"
    __SELECTED_INDEX = "'selectedIndex'"
    __BASE_INTERVAL_MS = "'BASE_INTERVAL'"  # Time (in MS) btwn frames when speed==1
    __TIMER_START_DATE = "'startDate'"
    __TIMER_ELAPSED_TIME_MS = "'elapsedTimeMS'"
    __TIMER_ELAPSED_TIME_PROPORTION = "'elapsedTimeProportion'"
    __SPEEDS_KEY = "'SPEEDS'"
    __PLAYBACK_INFO = f"window._playbackInfo_{_THIS_PLOT_ID}"

    _PBI_TIMER = f"{__PLAYBACK_INFO}[{__TIMER}]"
    _PBI_IS_ACTIVE = f"{__PLAYBACK_INFO}[{__IS_ACTIVE}]"
    _PBI_SELECTED_INDEX = f"{__PLAYBACK_INFO}[{__SELECTED_INDEX}]"
    _PBI_TIMER_START_DATE = f"{__PLAYBACK_INFO}[{__TIMER_START_DATE}]"
    _PBI_TIMER_ELAPSED_TIME_MS = f"{__PLAYBACK_INFO}[{__TIMER_ELAPSED_TIME_MS}]"
    _PBI_TIMER_ELAPSED_TIME_PROPORTION = (
        f"{__PLAYBACK_INFO}[{__TIMER_ELAPSED_TIME_PROPORTION}]"
    )
    _PBI_BASE_INTERVAL = f"{__PLAYBACK_INFO}[{__BASE_INTERVAL_MS}]"
    _PBI_SPEEDS = f"{__PLAYBACK_INFO}[{__SPEEDS_KEY}]"
    _PBI_CURR_INTERVAL_MS = (
        f"{_PBI_BASE_INTERVAL} / {_PBI_SPEEDS}[{_PBI_SELECTED_INDEX}]"
    )

    _SPEED_OPTIONS = [0.25, 0.5, 1.0, 2.0]
    _DEFAULT_SPEED = 1.0
    _DEFAULT_SELECTED_INDEX = _SPEED_OPTIONS.index(_DEFAULT_SPEED)

    _SETUP_WINDOW_PLAYBACK_INFO = f"""
        if (typeof({__PLAYBACK_INFO}) === 'undefined') {{
            {__PLAYBACK_INFO} = {{
                {__TIMER}: null,
                {__IS_ACTIVE}: false,
                {__SELECTED_INDEX}: {_DEFAULT_SELECTED_INDEX},
                {__TIMER_START_DATE}: null,
                {__TIMER_ELAPSED_TIME_MS}: 0,
                {__TIMER_ELAPSED_TIME_PROPORTION}: 0,
                {__BASE_INTERVAL_MS}: 1000,
                {__SPEEDS_KEY}: {_SPEED_OPTIONS}
            }};
        }}

    """

    _DEFFUN_INCR_DATE = f"""
        // See this link for why this works (it's an undocumented feature?)
        // https://discourse.bokeh.org/t/5254
        // Tl;dr we need this to automatically update the hover as the play button plays
        // Without this, the hover tooltip only updates when we jiggle the mouse
        // slightly

        let prev_val = null;
        source.inspect.connect(v => prev_val = v);

        function updateDate() {{
            {_PBI_TIMER_START_DATE} = new Date();
            {_PBI_TIMER_ELAPSED_TIME_MS} = 0
            if (dateSlider.value < maxDate) {{
                dateSlider.value += 86400000;
            }}

            if (dateSlider.value >= maxDate) {{
                console.log(dateSlider.value, maxDate)
                console.log('reached end')
                clearInterval({_PBI_TIMER});
                {_PBI_IS_ACTIVE} = false;
                playPauseButton.active = false;
                playPauseButton.change.emit();
                playPauseButton.label = 'Restart';
            }}

            dateSlider.change.emit();

            // This is pt. 2 of the prev_val/inspect stuff above
            if (prev_val !== null) {{
                source.inspect.emit(prev_val);
            }}
        }}
    """

    _DO_START_TIMER = f"""
        function startLoopTimer() {{
            updateDate();
            if ({_PBI_IS_ACTIVE}) {{
                {_PBI_TIMER} = setInterval(updateDate, {_PBI_CURR_INTERVAL_MS})
            }}

        }}

        {_PBI_TIMER_START_DATE} = new Date();

        // Should never be <0 or >1 but I am being very defensive here
        const proportionRemaining = 1 - (
            {_PBI_TIMER_ELAPSED_TIME_PROPORTION} <= 0
            ? 0
            : {_PBI_TIMER_ELAPSED_TIME_PROPORTION} >= 1
            ? 1
            : {_PBI_TIMER_ELAPSED_TIME_PROPORTION}
        );
        const remainingTimeMS = (
            {_PBI_CURR_INTERVAL_MS} * proportionRemaining
        );
        const initialInterval = (
            {_PBI_TIMER_ELAPSED_TIME_MS} === 0
            ? 0
            : remainingTimeMS
        );

        {_PBI_TIMER} = setTimeout(
            startLoopTimer,
            initialInterval
        );
    """

    _DO_STOP_TIMER = f"""
        const now = new Date();
        {_PBI_TIMER_ELAPSED_TIME_MS} += (
            now.getTime() - {_PBI_TIMER_START_DATE}.getTime()
        );
        {_PBI_TIMER_ELAPSED_TIME_PROPORTION} = (
            {_PBI_TIMER_ELAPSED_TIME_MS} / {_PBI_CURR_INTERVAL_MS}
        );
        clearInterval({_PBI_TIMER});
    """

    update_on_date_change_callback = CustomJS(
        args={"source": bokeh_data_source},
        code=f"""

        {_SETUP_WINDOW_PLAYBACK_INFO}

        const sliderValue = cb_obj.value;
        const sliderDate = new Date(sliderValue)
        // Ugh, actually requiring the date to be YYYY-MM-DD (matching DATE_FMT)
        const dateStr = sliderDate.toISOString().split('T')[0]

        const data = source.data;

        {_PBI_TIMER_ELAPSED_TIME_MS} = 0

        if (typeof(data[dateStr]) !== 'undefined') {{
            data['{value_col}'] = data[dateStr]

            const valueCol = data['{value_col}'];
            const colorCol = data['{COLOR_COL}'];
            const fakeDateCol = data['{FAKE_DATE_COL}']

            for (var i = 0; i < data['{value_col}'].length; i++) {{
                const value = valueCol[i]
                if (value == 0) {{
                    colorCol[i] = 'NaN';
                }} else {{
                    colorCol[i] = value;
                }}

                fakeDateCol[i] = dateStr;
            }}

            source.change.emit();

        }}

        """,
    )

    # Taking day-over-day diffs means the min slider day is one more than the min data
    # date (might be off by 1 if not using day over diffs but in practice not an issue)
    min_slider_date = min_date + pd.Timedelta(days=1)
    date_slider = DateSlider(
        start=min_slider_date,
        end=max_date,
        value=max_date,
        step=1,
        sizing_mode="stretch_width",
        width_policy="fit",
    )
    date_slider.js_on_change("value", update_on_date_change_callback)

    play_pause_button = Toggle(
        label="Start playing",
        button_type="success",
        active=False,
        sizing_mode="stretch_width",
    )

    animate_playback_callback = CustomJS(
        args={
            "source": bokeh_data_source,
            "dateSlider": date_slider,
            "playPauseButton": play_pause_button,
            "maxDate": max_date,
            "minDate": min_slider_date,
        },
        code=f"""

        {_SETUP_WINDOW_PLAYBACK_INFO}
        {_DEFFUN_INCR_DATE}

        if (dateSlider.value >= maxDate) {{
            if (playPauseButton.active) {{
                dateSlider.value = minDate;
                dateSlider.change.emit();

                // Hack to get timer to wait after date slider wraps; any positive
                // number works but the smaller the better
                {_PBI_TIMER_ELAPSED_TIME_MS} = 1;
            }}
        }}

        const active = cb_obj.active;
        {_PBI_IS_ACTIVE} = active;

        if (active) {{
            playPauseButton.label = 'Playing – Click/tap to pause'
            {_DO_START_TIMER}
        }} else {{
            playPauseButton.label = 'Paused – Click/tap to play'
            {_DO_STOP_TIMER}
        }}

        """,
    )

    play_pause_button.js_on_click(animate_playback_callback)

    change_playback_speed_callback = CustomJS(
        args={
            "source": bokeh_data_source,
            "dateSlider": date_slider,
            "playPauseButton": play_pause_button,
            "maxDate": max_date,
        },
        code=f"""

        {_SETUP_WINDOW_PLAYBACK_INFO}
        {_DEFFUN_INCR_DATE}

        // Must stop timer before handling changing the speed, as stopping the timer
        // saves values based on the current (unchaged) speed selection
        if ({_PBI_TIMER} !== null) {{
            {_DO_STOP_TIMER}
        }}

        const selectedIndex = cb_obj.active;
        {_PBI_SELECTED_INDEX} = selectedIndex;

        if ({_PBI_IS_ACTIVE}) {{
            {_DO_START_TIMER}
        }} else {{
            {_PBI_TIMER_ELAPSED_TIME_MS} = 0
        }}

        console.log({__PLAYBACK_INFO})

    """,
    )

    playback_speed_radio = RadioButtonGroup(
        labels=[f"{speed:.2g}x speed" for speed in _SPEED_OPTIONS],
        active=_DEFAULT_SELECTED_INDEX,
        sizing_mode="stretch_width",
    )
    playback_speed_radio.js_on_click(change_playback_speed_callback)

    plot_layout.append(
        layout_column(
            [
                date_slider,
                layout_row(
                    [play_pause_button, playback_speed_radio], height_policy="min",
                ),
            ],
            width_policy="fit",
            height_policy="min",
        )
    )
    plot_layout = layout_column(plot_layout, sizing_mode="scale_both")

    # grid = gridplot(figures, ncols=len(count_list), sizing_mode="stretch_both")

    # Create the autoloading bokeh plot info (HTML + JS)
    js_path = str(Path(out_file_basename + "_autoload").with_suffix(".js"))
    tag_html_path = str(Path(out_file_basename + "_div_tag").with_suffix(".html"))

    js_code, tag_code = autoload_static(plot_layout, CDN, js_path)
    tag_uuid = re.search(r'id="([^"]+)"', tag_code).group(1)
    tag_code = re.sub(r'src="([^"]+)"', f'src="\\1?uuid={tag_uuid}"', tag_code)

    with open(Paths.DOCS / js_path, "w") as f_js, open(
        Paths.DOCS / tag_html_path, "w"
    ) as f_html:
        f_js.write(js_code)
        f_html.write(tag_code)

    # Create the video by creating stills of the graphs for each date and then stitching
    # the images into a video
    if should_make_video:
        save_dir: Path = PNG_SAVE_ROOT_DIR / out_file_basename
        save_dir.mkdir(parents=True, exist_ok=True)

        STILL_WIDTH = 1500
        STILL_HEIGHT = int(
            np.ceil(STILL_WIDTH / plot_aspect_ratio) * 1.05
        )  # Unclear why *1.05 is necessary
        gp.height = STILL_HEIGHT
        gp.width = STILL_WIDTH
        gp.sizing_mode = "fixed"
        orig_title = anchor_fig.title.text

        for date in dates:
            date_str = date.strftime(DATE_FMT)
            anchor_fig.title = Title(text=f"{orig_title} {date_str}")

            for p in figures:
                p.title = Title(text=p.title.text, text_font_size="20px")

            # Just a reimplementation of the JS code in the date slider's callback
            data = bokeh_data_source.data
            data[value_col] = data[date_str]

            for i, value in enumerate(data[value_col]):
                if value == 0:
                    data[COLOR_COL][i] = "NaN"
                else:
                    data[COLOR_COL][i] = value

                data[FAKE_DATE_COL][i] = date_str

            save_path: Path = (save_dir / date_str).with_suffix(".png")
            export_png(gp, filename=save_path)
            resize_to_even_dims(save_path, pad_bottom=0.08)

            if date == max(dates):
                poster_path: Path = (
                    PNG_SAVE_ROOT_DIR / (out_file_basename + "_poster")
                ).with_suffix(".png")
                poster_path.write_bytes(save_path.read_bytes())

        make_video(save_dir, out_file_basename, 0.9)

    print(f"Did interactive {out_file_basename}")

    return (js_code, tag_code)
```


Overlapping Code:
```
df: pd.DataFrame,
*,
geo_df: geopandas.GeoDataFrame,
value_col: str,
transform_df_func: Callable[[pd.DataFrame], pd.DataFrame] = None,
stage: Union[DiseaseStage, Literal[Select.ALL]] = Select.ALL,
count: Union[Counting, Literal[Select.ALL]] = Select.ALL,
out_file_basename: str,
subplot_title_prefix: str,
plot_aspect_ratio: float = None,
cmap=None,
n_cbar_buckets: int = None,
n_buckets_btwn_major_ticks: int = None,
n_minor_ticks_btwn_major_ticks: int = None,
per_capita_denominator: int = None,
x_range: Tuple[float, float],
y_range: Tuple[float, float],
min_visible_y_range: float,
should_make_video: bool,
) -> InfoForAutoload:
"""Create the bokeh interactive timeline plot(s)
This function takes the given DataFrame, which must contain COVID data for locations
on different dates, and a GeoDataFrame, which contains the long/lat coords for those
locations, and creates an interactive choropleth of the COVID data over time.
:param df: The COVID data DataFrame
:type df: pd.DataFrame
:param geo_df: The geometry GeoDataFrame for the locations in `df`
:type geo_df: geopandas.GeoDataFrame
:param value_col: The column of `df` containing the values to plot in the
choropleth; should be something like "Case_Counts" or "Case_Diff_From_Prev_Day"
:type value_col: str
:param stage: The DiseaseStage to plot, defaults to Select.ALL. If ALL, then all
stages are plotted and are stacked vertically.
:type stage: Union[DiseaseStage, Literal[Select.ALL]], optional
:param count: The Counting to plot, defaults to Select.ALL. If ALL, then all
count types are plotted and are stacked horizontally.
:type count: Union[Counting, Literal[Select.ALL]], optional
:param out_file_basename: The basename of the file to save the interactive plots to
(there are two components, the JS script and the HTML <div>)
:type out_file_basename: str
:param subplot_title_prefix: What the first part of the subplot title should be;
probably a function of `value_col` (if value_col is "Case_Counts" then this param
might be "Cases" or "# of Cases")
:type subplot_title_prefix: str
:param x_range: The range of the x-axis as (min, max)
:type x_range: Tuple[float, float]
:param y_range: The range of the y-axis as (min, max)
:type y_range: Tuple[float, float]
:param min_visible
```
<Overlap Ratio: 0.9761388286334056>

---

--- 49 --
Question ID: 0781b1dde191a2671a44781c37c127bcb64fb318_1
Original Code:
```
def test_cli_datasorce_new(caplog, empty_data_context, filesystem_csv_2):
    project_root_dir = empty_data_context.root_directory
    context = DataContext(project_root_dir)
    assert context.list_datasources() == []

    runner = CliRunner(mix_stderr=False)
    result = runner.invoke(
        cli,
        ["datasource", "new", "-d", project_root_dir],
        input="1\n1\n%s\nmynewsource\n" % str(filesystem_csv_2),
        catch_exceptions=False,
    )
    stdout = result.stdout

    assert "What data would you like Great Expectations to connect to?" in stdout
    assert "What are you processing your files with?" in stdout
    assert "Give your new data source a short name." in stdout
    assert "A new datasource 'mynewsource' was added to your project." in stdout

    assert result.exit_code == 0

    config_path = os.path.join(project_root_dir, DataContext.GE_YML)
    config = yaml.load(open(config_path, "r"))
    datasources = config["datasources"]
    assert "mynewsource" in datasources.keys()
    data_source_class = datasources["mynewsource"]["data_asset_type"]["class_name"]
    assert data_source_class == "PandasDataset"
    assert_no_logging_messages_or_tracebacks(caplog, result)
```


Overlapping Code:
```
st_cli_datasorce_new(caplog, empty_data_context, filesystem_csv_2):
project_root_dir = empty_data_context.root_directory
context = DataContext(project_root_dir)
assert context.list_datasources() == []
runner = CliRunner(mix_stderr=False)
result = runner.invoke(
cli,
["datasource", "new", "-d", project_root_dir],
input="1\n1\n%s\nmynewsource\n" % str(filesystem_csv_2),
catch_exceptions=False,
)
stdout = result.stdout
assert "What data would you like Great Expectations to connect to?" in stdout
assert "What are you processing your files with?" in stdout
assert "Give your new data source a short name." in stdout
assert "A new datasource 'mynewsource' was added to your project." in stdout
assert result.exit_code == 0
config_path = os.path.join(project_root_dir, DataContext.GE_YML)
config = yaml.load(open(config_path, "r"))
datasources = config["datasources"]
assert "mynewsource" in datasources.keys()
data_source_class = datasources["mynewsource"]["data_asset_type"]["class_name"]
assert data_source_class == "PandasDataset"
assert_no_logging_messages_or_tracebacks(caplog, result)
```
<Overlap Ratio: 0.9945255474452555>

---

--- 50 --
Question ID: fbf3bd1d40d29fc5addd1a8f53a9041763e47088_16
Original Code:
```
def merge_array_type(type1, type2):
    assert is_array(type1) or is_array(type2)
    if is_java_lang_object(type2):
        return type2
    elif is_java_lang_object(type1):
        return type1
    if is_array(type1):
        if is_array(type2):
            new_type = merge_type(type1[1:], type2[1:])
            if new_type:
                return '[' + new_type
            else:
                return None
        else:
            return 'Ljava/lang/Object;'
    else:
        return merge_array_type(type2, type1)
```


Overlapping Code:
```
ray(type1) or is_array(type2)
if is_java_lang_object(type2):
return type2
elif is_java_lang_object(type1):
return type1
if is_array(type1):
if is_array(type2):
new_type = merge_type(type1[1:], type2[1:])
if new_type:
return '[' + new_type
else:
return None
else:
return 'Ljava/lang/Object;'
else:
ret
```
<Overlap Ratio: 0.7853403141361257>

---

--- 51 --
Question ID: 1a94529672b7505165acc92ab8eef6c2a078d80f_4
Original Code:
```
def rebatch_metadata_by_experiment(metadata):
    normal, normal_rest = prioritize_normals(metadata)
    batch = metadata[0]["participant"]
    tumor_batch = [tz.assoc(x, "batch", batch) for x in metadata
                   if x["sample_type"] in PRIORITIZED_TUMOR_CODES.keys()]
    normal = [tz.assoc(normal, "batch", batch)] if normal else []
    # run each non priority normal as its own tumor sample with no control
    normal_rest = [tz.assoc(x, "batch", batch + "-" + x["sample_type"]) for x
                   in normal_rest]
    normal_rest = [tz.assoc(x, "phenotype", "tumor") for x in normal_rest]
    all_batches = normal + normal_rest + tumor_batch
    return all_batches
```


Overlapping Code:
```
experiment(metadata):
normal, normal_rest = prioritize_normals(metadata)
batch = metadata[0]["participant"]
tumor_batch = [tz.assoc(x, "batch", batch) for x in metadata
if x["sample_type"] in PRIORITIZED_TUMOR_CODES.keys()]
normal = [tz.assoc(normal, "batch", batch)] if normal else []
# run each non priority normal as its own tumor sample with no control
normal_rest = [tz.assoc(x, "batch", batch + "-" + x["sample_type"]) for x
in normal_rest]
normal_rest = [tz.assoc(x, "phenotype", "tumor") for x in normal_rest]
all_batches = normal + normal_rest + tumo
```
<Overlap Ratio: 0.9178981937602627>

---

--- 52 --
Question ID: d74cf4ec18d91885e973a541620f636e9ba6ebd5_7
Original Code:
```
def checkGapPhase(ID, **kwargs):
    """
    check ID gap, phase
    return True if success, otherwise False
    """
    gapMin, gapMax, gapStep, gapTol = kwargs.get("gap",
                                                 _params[ID.name]["gap"])
    phaseMin, phaseMax, phaseStep, phaseTol = \
        kwargs.get("phase",  _params[ID.name].get("phase", (None, None, None, None)))
    timeout = kwargs.get("timeout", 150)
    throw   = kwargs.get("throw", True)
    unitsys = kwargs.get("unitsys", _params[ID.name]["unitsys"])
    verbose = kwargs.get("verbose", 0)
    gapStep = kwargs.get("gapStep", gapStep)
    phaseStep = kwargs.get("phaseStep", phaseStep)

    flds = ID.fields()
    if 'gap' in flds:
        for gap in np.linspace(gapMin, gapMax, gapStep):
            gapList = [['gap',gap, gapTol]]
            gapStatus = putPar(ID,gapList,timeout=timeout,
                               throw=throw,unitsys=unitsys,verbose=verbose)
            if not gapStatus:
                return False
    if 'phase' in flds:
        for phase in np.linspace(phaseMin,phaseMax,phaseStep):
            phaseList = [['phase',phase,phaseTol]]
            phaseStatus = putPar(ID,phaseList,timeout=timeout,
                               throw=throw,unitsys=unitsys,verbose=verbose)
            if not phaseStatus:
                return False
    return True
```


Overlapping Code:
```
ase(ID, **kwargs):
"""
check ID gap, phase
return True if success, otherwise False
"""
gapMin, gapMax, gapStep, gapTol = kwargs.get("gap",
_params[ID.name]["gap"])
phaseMin, phaseMax, phaseStep, phaseTol = \
kwargs.get("phase", _params[ID.name].get("phase", (None, None, None, None)))
timeout = kwargs.get("timeout", 150)
throw = kwargs.get("throw", True)
unitsys = kwargs.get("unitsys", _params[ID.name]["unitsys"])
verbose = kwargs.get("verbose", 0)
gapStep = kwargs.get("gapStep", gapStep)
phaseStep = kwargs.get("phaseStep", phaseStep)
flds = ID.fields()
if 'gap' in flds:
for gap in np.linspace(gapMin, gapMax, gapStep):
gapList = [['gap',gap, gapTol]]
gapStatus = putPar(ID,gapList,timeout=timeout,
throw=throw,unitsys=unitsys,verbose=verbose)
if not gapStatus:
return False
if 'phase' in flds:
for phase in np.linspace(phaseMin,phaseMax,phaseStep):
phaseList = [['phase',phase,phaseTol]]
phaseStatus = putPar(ID,phaseList,timeout=timeout,
throw=throw,unitsys=unitsys,verbose=verbose)
if not phas
```
<Overlap Ratio: 0.9551954242135366>

---

--- 53 --
Question ID: 73de06f4ec2d77ed4ca06cc5612ef4cb7bdec7ac_2
Original Code:
```
@tf.function
def wavegan_loss(gen, disc, x, z):
    G_z = gen(z)
    D_x = disc(x)
    D_G_z = disc(G_z)
    gen_loss_one = -tf.reduce_mean(D_G_z) #Expected value
    gen_loss = tf.reduce_mean(D_x) - tf.reduce_mean(D_G_z)
    disc_loss = -gen_loss

    # Gradient penalty
    epsilon = tf.random.uniform([hyperparams['batch_size'], 1, 1], minval=0., maxval=1.)
    x_hat = epsilon * x + (1 - epsilon) * G_z #batch x 16384 x 1
    with tf.GradientTape() as tape:
        tape.watch(x_hat)
        y = disc(x_hat) # 64 x 1
    dy_d_x_hat = tape.gradient(y, x_hat) #batch x 16384 x 1
    slopes = tf.sqrt(tf.compat.v1.reduce_sum(tf.square(dy_d_x_hat), reduction_indices=[1, 2]))
    gp = hyperparams['wgan_gp_lambda'] * tf.reduce_mean((slopes - 1.) ** 2)
    return gen_loss, disc_loss + gp, gen_loss_one
```


Overlapping Code:
```
tf.function
def wavegan_loss(gen, disc, x, z):
G_z = gen(z)
D_x = disc(x)
D_G_z = disc(G_z)
gen_loss_one = -tf.reduce_mean(D_G_z) #Expected value
gen_loss = tf.reduce_mean(D_x) - tf.reduce_mean(D_G_z)
disc_loss = -gen_loss
# Gradient penalty
epsilon = tf.random.uniform([hyperparams['batch_size'], 1, 1], minval=0., maxval=1.)
x_hat = epsilon * x + (1 - epsilon) * G_z #batch x 16384 x 1
with tf.GradientTape() as tape:
tape.watch(x_hat)
y = disc(x_hat) # 64 x 1
dy_d_x_hat = tape.gradient(y, x_hat) #batch x 16384 x 1
slopes = tf.sqrt(tf.compat.v1.reduce_sum(tf.square(dy_d_x_hat), reduction_indices=[1, 2]))
gp = hyperparams['wgan_gp_lambda'] * tf.reduce_mean((slopes - 1.) ** 2)
return gen_loss, d
```
<Overlap Ratio: 0.9615384615384616>

---

--- 54 --
Question ID: 67db3baa9c7e7a9495b74e51cbbdea983ae6d547_0
Original Code:
```
def get_agent(x, reuse=False):
    """
    Generate the CNN agent
    :param x: tensor, Input frames concatenated along axis 3
    :param reuse: bool, True -> Reuse weight variables
                        False -> Create new ones
    :return: Tensor, logits for each valid action
    """
    if reuse:
        tf.get_variable_scope().reuse_variables()

    x = tf.divide(x, 255.0, name='Normalize')
    conv_1 = tf.nn.relu(ops.cnn_2d(x, weight_shape=mc.conv_1, strides=mc.stride_1, name='conv_1'))
    conv_2 = tf.nn.relu(ops.cnn_2d(conv_1, weight_shape=mc.conv_2, strides=mc.stride_2, name='conv_2'))
    conv_3 = tf.nn.relu(ops.cnn_2d(conv_2, weight_shape=mc.conv_3, strides=mc.stride_3, name='conv_3'))
    conv_3_r = tf.reshape(conv_3, [-1, 7 * 7 * 64], name='reshape')
    dense_1 = tf.nn.relu(ops.dense(conv_3_r, 7 * 7 * 64, mc.dense_1, name='dense_1'))
    output = ops.dense(dense_1, mc.dense_1, mc.dense_2, name='dense_2')
    return output
```


Overlapping Code:
```
et_agent(x, reuse=False):
"""
Generate the CNN agent
:param x: tensor, Input frames concatenated along axis 3
:param reuse: bool, True -> Reuse weight variables
False -> Create new ones
:return: Tensor, logits for each valid action
"""
if reuse:
tf.get_variable_scope().reuse_variables()
x = tf.divide(x, 255.0, name='Normalize')
conv_1 = tf.nn.relu(ops.cnn_2d(x, weight_shape=mc.conv_1, strides=mc.stride_1, name='conv_1'))
conv_2 = tf.nn.relu(ops.cnn_2d(conv_1, weight_shape=mc.conv_2, strides=mc.stride_2, name='conv_2'))
conv_3 = tf.nn.relu(ops.cnn_2d(conv_2, weight_shape=mc.conv_3, strides=mc.stride_3, name='conv_3'))
conv_3_r = tf.reshape(conv_3, [-1, 7 * 7 * 64], name='reshape')
dense_1 = tf.nn.relu(ops.dense(conv_3_r, 7 * 7 * 64, mc.dense_1, name='dense_1'))
output = ops.dense(dense_1, mc.dense_1, mc.dense_2, name='dense
```
<Overlap Ratio: 0.9731621936989499>

---

--- 55 --
Question ID: 69b5d11bd9427c7424a01c1cf4dd0916e3330729_0
Original Code:
```
def lr_decay(step):
	lr = 0.1
	step = step/20000
	step = tf.math.floor(step)
	step = tf.math.pow(0.1, step)
	lr = lr * step 
	return lr 
```


Overlapping Code:
```
def lr_decay(step):
lr = 0.1
step = step/20000
step = tf.math.floor(step)
step = tf.math.pow(0.1, step)
lr = lr * step 
return 
```
<Overlap Ratio: 0.9844961240310077>

---

--- 56 --
Question ID: 45ada3a47f33c093e6660ca1de89f92fa8c1c7ca_76
Original Code:
```
def _switch(db, dbid, row, keys, **kwargs):
    j = _entity(db, dbid, row, entity_keys)
    j.update(_copy_kv(row, keys))
    return j, None
```


Overlapping Code:
```
(db, dbid, row, keys, **kwargs):
j = _entity(db, dbid, row, entity_keys)
j.update(_copy_kv(row, keys))
return j,
```
<Overlap Ratio: 0.875>

---

--- 57 --
Question ID: 7336f50c7ae17c7ac9743931d9def55fd899e436_0
Original Code:
```
def image_resize_aspectratio(arImage, nMinDim = 256):
    """
    Resize aspect ratio of image.
    Rescale height to 256.

    Keyword arguments:
    arImage -- np.array
    nMinDim -- Minimum Dimension (default 256)

    returns np.array of floats
    """
    if (len(arImage.shape) < 2 ): raise ValueError("Image doesnot exist")
    nHeigth, nWidth, _ = arImage.shape

    if nWidth >= nHeigth:
        # wider than high => map heigth to 224
        fRatio = nMinDim / nHeigth
    else: 
        fRatio = nMinDim / nWidth

    if fRatio != 1.0:
        arImage = cv2.resize(arImage, dsize = (0,0), fx = fRatio, fy = fRatio, interpolation=cv2.INTER_LINEAR)

    return arImage
```


Overlapping Code:
```
_aspectratio(arImage, nMinDim = 256):
"""
Resize aspect ratio of image.
Rescale height to 256.
Keyword arguments:
arImage -- np.array
nMinDim -- Minimum Dimension (default 256)
returns np.array of floats
"""
if (len(arImage.shape) < 2 ): raise ValueError("Image doesnot exist")
nHeigth, nWidth, _ = arImage.shape
if nWidth >= nHeigth:
# wider than high => map heigth to 224
fRatio = nMinDim / nHeigth
else: 
fRatio = nMinDim / nWidth
if fRatio != 1.0:
arImage = cv2.resize(arImage, dsize = (0,0), fx = fRatio, fy = fRatio, interpolation=cv2.INTER_LIN
```
<Overlap Ratio: 0.9401709401709402>

---

--- 58 --
Question ID: 81d0a0210737aab52b87876253b5e3ba1dc52858_2
Original Code:
```
def _get_unused_letters(used_letters):
    doubles = [first + second for second in string.ascii_lowercase
               for first in string.ascii_lowercase]
    all_letters = set(list(string.ascii_lowercase) + doubles)
    letters = list(all_letters - used_letters)
    # NOTE(vish): prepend ` so all shorter sequences sort first
    letters.sort(key=lambda x: x.rjust(2, '`'))
    return letters[0]
```


Overlapping Code:
```
etters(used_letters):
doubles = [first + second for second in string.ascii_lowercase
for first in string.ascii_lowercase]
all_letters = set(list(string.ascii_lowercase) + doubles)
letters = list(all_letters - used_letters)
# NOTE(vish): prepend ` so all shorter sequences sort first
letters.sort(key=lambda x: x.rjust(2, '`'))
return letters[0
```
<Overlap Ratio: 0.9501385041551247>

---

--- 59 --
Question ID: 01720fb0d1d1409387c115b371906cdeb3b4343a_0
Original Code:
```
@app.route("/create", methods=["POST"])
def create_container():
    if request.method == "POST":
        req = request.get_json()
        image = req.get("image")
        password = req.get("password")
        job_id = random_string(string_length=5)

        if image is not None:
            return json.dumps(
                controller.create_job(job_id, {"PASSWD": password}, image=image)
            )
        else:
            return json.dumps(controller.create_job(job_id, {"PASSWD": password}))
```


Overlapping Code:
```
te", methods=["POST"])
def create_container():
if request.method == "POST":
req = request.get_json()assword")
job_id = random_string(string_length=5)
if image is not None:
return json.dumps(
controller.create_job(job_id, {"PASSWD": password}, image=image)
)
else:
return json.dumps(controller.create_
```
<Overlap Ratio: 0.7537688442211056>

---

--- 60 --
Question ID: 8d78cb3a80d5c2001dda0cac594eb5af4a4f142b_0
Original Code:
```
def methane_molecule():
    symbols = ['C','H','H','H','H']
    coordinates = np.array([
    [1, 1, 1],
    [2.4, 1, 1],
    [-0.4, 1, 1],
    [1, 1, 2.4],
    [1, 1, 0.4],
    ])
    return symbols, coordinates
```


Overlapping Code:
```
','H']
coordinates = np.array([
[1, 1, 1],
[2.4, 1, 1],
[-0.4, 1, 1],
[1, 1, 2.4],
[1, 1, 0.4],
])
r
```
<Overlap Ratio: 0.5714285714285714>

---

--- 61 --
Question ID: 8a00b7a1095424bbea3b3ede093816e290d1e3ff_2
Original Code:
```
def get_target_extended_spec(s, target, logger):
    """Return target's spec extended with linked elements.

    That is extend spec.endpoints with the selector of the
    linkedVirtualService, and set spec.cluster.spec to spec.listener
    of the linkedVirtualService.

    """
    spec = deepcopy(target['spec'])
    # 1. Find the linked VirtualService
    try:
        vsvc_name = spec['linkedVirtualService']
    except KeyError:
        return spec
    del spec['linkedVirtualService']
    vsvc = s['virtualservices'].get(vsvc_name)
    if not vsvc:
        for v in s['virtualservices'].values():
            if v['metadata']['name'] == vsvc_name:
                vsvc = v
                break
    if not vsvc:
        return {}

    # 2.
    spec['cluster'] = spec.get('cluster', {})
    eps = spec['cluster'].get('endpoints', [])
    new_ep = vsvc['spec'].get('selector', [])
    if new_ep:
        eps.append({'selector': new_ep})
        spec['cluster'].update({'endpoints': eps})

    # 3.
    spec['cluster']['spec'] = vsvc['spec']['listener']

    # 4.
    return spec
```


Overlapping Code:
```
ended_spec(s, target, logger):
"""Return target's spec extended with linked elements.
That is extend spec.endpoints with the selector of the
linkedVirtualService, and set spec.cluster.spec to spec.listener
of the linkedVirtualService.
"""
spec = deepcopy(target['spec'])
# 1. Find the linked VirtualService
try:
vsvc_name = spec['linkedVirtualService']
except KeyError:
return spec
del spec['linkedVirtualService']
vsvc = s['virtualservices'].get(vsvc_name)
if not vsvc:
for v in s['virtualservices'].values():
if v['metadata']['name'] == vsvc_name:
vsvc = v
break
if not vsvc:
return {}
# 2.
spec['cluster'] = spec.get('cluster', {})
eps = spec['cluster'].get('endpoints', [])
new_ep = vsvc['spec'].get('selector', [])
if new_ep:
eps.append({'selector': new_ep})
spec['cluster'].update({'endpoints': eps})
# 3.
spec['cluster']['spec'] = vsvc['spec']
```
<Overlap Ratio: 0.947603121516165>

---

--- 62 --
Question ID: 65fb11ef823908155f63a299cd3969cb1a761a0a_1
Original Code:
```
def add_test_methods_to_database():
    for content_type, method_dict in registry.items():
        for method_name, defaults in method_dict.items():
            TestMethod.objects.update_or_create(
                content_type=content_type,
                method_name=method_name,
                defaults=defaults
            )
```


Overlapping Code:
```
ef add_test_methods_to_database():
for content_type, method_dict in registry.items():
for method_name, defaults in method_dict.items():
TestMethod.objects.update_or_create(
content_type=content_type,

```
<Overlap Ratio: 0.8163265306122449>

---

--- 63 --
Question ID: 22d283140ec3f2cb090cce2dd66da4c79d19bb97_8
Original Code:
```
def grabCaptureRectPerHash(hwnd, tLeft, tTop, tRight, tBottom, needShow=False):
    setForegroundWindow(hwnd)

    xLeft = getPosX(hwnd, tLeft)
    yLeft = getPosY(hwnd, tTop)
    xRight = getPosX(hwnd, tRight)
    yRight = getPosY(hwnd, tBottom)

    img = ImageGrab.grab(bbox=(xLeft, yLeft, xRight, yRight))
    phash = imgHash(img, hashSize, highfreq_factor)
    screenPath = path.getProjectPath()+"screen\\rect_per\\"+phash+"_"+str(tLeft) + \
        "_"+str(tTop) + "_" + str(tRight)+"_"+str(tBottom) + ".png"
    if not os.path.exists(path.getProjectPath()+"screen\\rect_per"):
        os.makedirs(path.getProjectPath()+"screen\\rect_per")
    img.save(screenPath)
    if needShow == True:
        img.show()
    img.close()
```


Overlapping Code:
```
tLeft, tTop, tRight, tBottom, needShow=False):
setForegroundWindow(hwnd)
xLeft = getPosX(hwnd, tLeft)
yLeft = getPosY(hwnd, tTop)
xRight = getPosX(hwnd, tRight)
yRight = getPosY(hwnd, tBottom)
img = ImageGrab.grab(bbox=(xLeft, yLeft, xRight, yRight))
phash = imgHash(img, hashSize, highfreq_factor)
screenPath = path.getProjectPath()+"screen\\rect_per\\"+phash+"_"+str(tLeft) + \
"_"+str(tTop) + "_" + str(tRight)+"_"+str(tBottom) + ".png"
if not os.path.exists(path.getProjectPath()+"screen\\rect_per"):
os.makedirs(path.getProjectPath()+"screen\\rect_per")
img.save(screenPath)
if needShow == True:
img.sh
```
<Overlap Ratio: 0.9253048780487805>

---

--- 64 --
Question ID: 8af82a24e17cdc02aeef759da18fe3c8404ca60f_0
Original Code:
```
def importInput(input):
    input_file = open(input, "r")
    lines = input_file.read().split('\n')
    instructionsList = []
    for line in lines:
        instruction = {}
        instruction[line[:3]] = line[4:]
        instructionsList.append(instruction)
    return instructionsList
```


Overlapping Code:
```
t(input):
input_file = open(input, "r")
lines = input_file.read().split('\n')
instructionsList = []
for line in lines:
instruction = {}
instruction[line[:3]] = line[4:]
instructionsList.append(instruc
```
<Overlap Ratio: 0.823045267489712>

---

--- 65 --
Question ID: d3a81116896a687811e9c64e62dfec686e730487_1
Original Code:
```
def set_window_to_foreground(title):
    import win32gui
    import win32con
    import win32com
    try:
        handle = win32gui.FindWindow(None, title)
        if not handle:
            raise Exception('Could not find a window with title "{}"'.format(title))

        win32gui.ShowWindow(handle, win32con.SW_SHOWMAXIMIZED)
        shell = win32com.client.Dispatch("WScript.Shell")
        shell.SendKeys('%')
        win32gui.SetForegroundWindow(handle)
    except Exception as ex:
        raise ex
```


Overlapping Code:
```
_foreground(title):
import win32gui
import win32con
import win32com
try:
handle = win32gui.FindWindow(None, title)
if not handle:
raise Exception('Could not find a window with title "{}"'.format(title))
win32gui.ShowWindow(handle, win32con.SW_SHOWMAXIMIZED)
shell = win32com.client.Dispatch("WScript.Shell")
shell.SendKeys('%')
win32gui.SetForegroundWindow(handle)
except Exception as ex:
```
<Overlap Ratio: 0.9371980676328503>

---

--- 66 --
Question ID: 106de18ec64eb0f46fc6e70a0d2b51ac9eccea14_7
Original Code:
```
def cfg_host_fdl(args):
    name = args.name if args.name else os.path.basename(args.path)

    global pc
    pc.add_host_fdl(args.address, args.size, args.path, name)
    return 0
```


Overlapping Code:
```
args):
name = args.name if args.name else os.path.basename(args.path)
global pc
pc.add_host_fdl(args.address, args.
```
<Overlap Ratio: 0.7055214723926381>

---

--- 67 --
Question ID: 188e15d17c57132182d32d6babcbf40c8d59f90a_7
Original Code:
```
def get_version(lang, default):
    """
    function get_version -- gets language version from config

    This function reads user-specified version of a language from config file.
    If the version was not found, it fall backs to the default specified.

    params:
    lang: str -- the language to look up in the config file
    default: str -- the value to return if version was not found
    """
    if 'Version' in config[lang]:
        return config[lang]['Version']
    return default
```


Overlapping Code:
```
(lang, default):
"""
function get_version -- gets language version from config
This function reads user-specified version of a language from config file.
If the version was not found, it fall backs to the default specified.
params:
lang: str -- the language to look up in the config file
default: str -- the value to return if version was not found
"""
if 'Version' in config[lang]:
return config[lan
```
<Overlap Ratio: 0.9029345372460497>

---

--- 68 --
Question ID: 102f25fe83def843136195641c40352461bf2b68_0
Original Code:
```
def main(use_stream=True):
    # Ask for data
    d.measure()
    data = b"{'t': " + str(d.temperature()) + ", 'h': " + str(d.humidity()) + "}"

    s = socket.socket()

    ai = socket.getaddrinfo(HOST, 443)
    addr = ai[0][-1]
    s.connect(addr)

    # SSL wrap
    s = ussl.wrap_socket(s)
    
    # Send POST request to Azure IoT Hub
    s.write("POST /devices/" + DEVICE + "/messages/events?api-version=2016-02-03 HTTP/1.0\r\n")
    # HTTP Headers
    s.write("Host: " + HOST + "\r\n")
    s.write("Authorization: " + SAS + "\r\n")
    s.write("Content-Type: application/json\r\n")
    s.write("Connection: close\r\n")
    s.write("Content-Length: " + str(len(data)) + "\r\n\r\n")
    # Data
    s.write(data)
    
    # Print 128 bytes of response
    print(s.read(128))

    s.close()
```


Overlapping Code:
```
in(use_stream=True):
# Ask for data
d.measure()
data = b"{'t': " + str(d.temperature()) + ", 'h': " + str(d.humidity()) + "}"
s = socket.socket()
ai = socket.getaddrinfo(HOST, 443)
addr = ai[0][-1]
s.connect(addr)
# SSL wrap
s = ussl.wrap_socket(s)

# Send POST request to Azure IoT Hub
s.write("POST /devices/" + DEVICE + "/messages/events?api-version=2016-02-03 HTTP/1.0\r\n")
# HTTP Headers
s.write("Host: " + HOST + "\r\n")
s.write("Authorization: " + SAS + "\r\n")
s.write("Content-Type: application/json\r\n")
s.write("Connection: close\r\n")
s.write("Content-Length: " + str(len(data)) + "\r\n\r\n")
# Data
s.write(data)

# Print 128 bytes of response
print(s.read(1
```
<Overlap Ratio: 0.9711399711399712>

---

--- 69 --
Question ID: eed2efc4148dbb9731bc5c658e05c97127ecf2a3_5
Original Code:
```
def distlineline(line1, line2, extend_line1=False, extend_line2=False):
    from .line import asline
    line1 = asline(line1)
    line2 = asline(line2)
    x1a = line1.pt1[0]
    y1a = line1.pt1[1]
    z1a = line1.pt1[2]
    x1b = line1.pt2[0]
    y1b = line1.pt2[1]
    z1b = line1.pt2[2]
    x2a = line2.pt1[0]
    y2a = line2.pt1[1]
    z2a = line2.pt1[2]
    x2b = line2.pt2[0]
    y2b = line2.pt2[1]
    z2b = line2.pt2[2]
    C1 = ((x2b - x2a) * (x1b - x1a) +
          (y2b - y2a) * (y1b - y1a) +
          (z2b - z2a) * (z1b - z1a))
    C2 = (x1b - x1a)**2 + (y1b - y1a)**2 + (z1b - z1a)**2
    C3 = ((x1a * (-x1a + x2a + x1b) - x1b * x2a) +
          (y1a * (-y1a + y2a + y1b) - y1b * y2a) +
          (z1a * (-z1a + z2a + z1b) - z1b * z2a))
    C4 = (x2b - x2a)**2 + (y2b - y2a)**2 + (z2b - z2a)**2
    C5 = ((x2a * (x2a - x1a - x2b) + x1a * x2b) +
          (y2a * (y2a - y1a - y2b) + y1a * y2b) +
          (z2a * (z2a - z1a - z2b) + z1a * z2b))
    if angle2lines(line1, line2) < 0.01:
        dist1 = distlinept(line1, line2.pt1, extend_line1)
        dist2 = distlinept(line2, line1.pt1, extend_line2)
        return min(dist1, dist2)
    else:
        t = (C1 * C5 - C4 * C3) / (C2 * C4 - C1**2)
        u = (C2 * C5 - C1 * C3) / (C2 * C4 - C1**2)
        if not extend_line1:
            t = min(t, 1)
            t = max(t, 0)
        if not extend_line2:
            u = min(u, 1)
            u = max(u, 0)
        return distptpt(line1.pt(t), line2.pt(u))
```


Overlapping Code:
```
e2, extend_line1=False, extend_line2=False):
from .line import asline
line1 = asline(line1)
line2 = asline(line2)
x1a = line1.pt1[0]
y1a = line1.pt1[1]
z1a = line1.pt1[2]
x1b = line1.pt2[0]
y1b = line1.pt2[1]
z1b = line1.pt2[2]
x2a = line2.pt1[0]
y2a = line2.pt1[1]
z2a = line2.pt1[2]
x2b = line2.pt2[0]
y2b = line2.pt2[1]
z2b = line2.pt2[2]
C1 = ((x2b - x2a) * (x1b - x1a) +
(y2b - y2a) * (y1b - y1a) +
(z2b - z2a) * (z1b - z1a))
C2 = (x1b - x1a)**2 + (y1b - y1a)**2 + (z1b - z1a)**2
C3 = ((x1a * (-x1a + x2a + x1b) - x1b * x2a) +
(y1a * (-y1a + y2a + y1b) - y1b * y2a) +
(z1a * (-z1a + z2a + z1b) - z1b * z2a))
C4 = (x2b - x2a)**2 + (y2b - y2a)**2 + (z2b - z2a)**2
C5 = ((x2a * (x2a - x1a - x2b) + x1a * x2b) +
(y2a * (y2a - y1a - y2b) + y1a * y2b) +
(z2a * (z2a - z1a - z2b) + z1a * z2b))
if angle2lines(line1, line2) < 0.01:
dist1 = distlinept(line1, line2.pt1, extend_line1)
dist2 = distlinept(line2, line1.pt1, extend_line2)
return min(dist1, dist2)
else:
t = (C1 * C5 - C4 * C3) / (C2 * C4 - C1**2)
u = (C2 * C5 - C1 * C3) / (C2 * C4 - C1**2)
if not extend_line1:
t = min(t, 1)
t = max(t, 0)
if not extend_line2:
u = min(u, 1)
u = max(u, 0)
re
```
<Overlap Ratio: 0.9457236842105263>

---

--- 70 --
Question ID: 8a647be5c6a9ddba7a6bce39fd7e5b29c396fd4a_3
Original Code:
```
@pytest.mark.asyncio
async def test_enable_proxy_unable_to_get_process_number(
        mocker, mailchimp_list):
    """Tests the enable_proxy function when current_process().index
    raises an AttributeError."""
    mocked_os = mocker.patch('app.lists.os')
    mocked_os.environ.get.side_effect = [None, 'foo']
    mocked_current_proccess = mocker.patch('app.lists.current_process')
    del mocked_current_proccess.return_value.index
    mocked_requests = mocker.patch('app.lists.requests')
    mocked_requests.get.return_value.text = 'foo:bar:baz'
    mocker.patch('app.lists.asyncio.sleep', new=CoroutineMock())
    await mailchimp_list.enable_proxy()
    mocked_requests.get.assert_called_with(
        'http://us-proxies.com/api.php',
        params=(
            ('api', ''),
            ('uid', '9557'),
            ('pwd', 'foo'),
            ('cmd', 'rotate'),
            ('process', '1'),
        ),
    )
```


Overlapping Code:
```
@pytest.mark.asyncio
async def test_enable_proxy_unable_to_get_process_number(
mocker, mailchimp_list):
"""Tests the enable_proxy function when current_process().index
raises an AttributeError."""
mocked_os = mocker.patch('app.lists.os')
mocked_os.environ.get.side_effect = [None, 'foo']
mocked_current_proccess = mocker.patch('app.lists.current_process')
del mocked_current_proccess.return_value.index
mocked_requests = mocker.patch('app.lists.requests')
mocked_requests.get.return_value.text = 'foo:bar:baz'
mocker.patch('app.lists.asyncio.sleep', new=CoroutineMock())
await mailchimp_list.enable_proxy()
mocked_requests.get.assert_called_with(
'http://us-proxies.com/api.php',
params=(
('api', ''),
('uid', '95
```
<Overlap Ratio: 0.9188144329896907>

---

--- 71 --
Question ID: 6fcf9f8703a9d44037b79b8e07343692d017ecf6_2
Original Code:
```
def mult_dict_service(parameters):

##########  Description  #######
    '''
    '''
#############  BODY ############

    mult = []
    # May be some logic based on par1, par2, ... value
    mult.append({})
    mult[0]['eq_addr'] = 'example_device1'
    mult[0]['eq_parameter'] = 'some_parameter'
    mult[0]['cmd'] = {}
    mult[0]['cmd']['ad'] = []
    mult[0]['cmd']['rm'] = []
    mult[0]['cmd']['ad'].append('extemplates_1.create_service')
    mult[0]['cmd']['rm'].append('extemplates_1.delete_service')
    mult.append({})
    mult[1]['eq_addr'] = 'example_device2'
    mult[1]['eq_parameter'] = 'some_parameter'
    mult[1]['cmd'] = {}
    mult[1]['cmd']['ad'] = []
    mult[1]['cmd']['rm'] = []
    mult[1]['cmd']['ad'].append('extemplates_2.create_service')
    mult[1]['cmd']['rm'].append('extemplates_2.delete_service')

    return (mult)
```


Overlapping Code:
```
dict_service(parameters):
########## Description #######
'''
'''
############# BODY ############
mult = []
# May be some logic based on par1, par2, ... value
mult.append({})
mult[0]['eq_addr'] = 'example_device1'
mult[0]['eq_parameter'] = 'some_parameter'
mult[0]['cmd'] = {}
mult[0]['cmd']['ad'] = []
mult[0]['cmd']['rm'] = []
mult[0]['cmd']['ad'].append('extemplates_1.create_service')
mult[0]['cmd']['rm'].append('extemplates_1.delete_service')
mult.append({})
mult[1]['eq_addr'] = 'example_device2'
mult[1]['eq_parameter'] = 'some_parameter'
mult[1]['cmd'] = {}
mult[1]['cmd']['ad'] = []
mult[1]['cmd']['rm'] = []
mult[1]['cmd']['ad'].append('extemplates_2.create_service')
mult[1]['cmd']['rm'].append('extemplates_2.delete_service')
return (mult
```
<Overlap Ratio: 0.9868421052631579>

---

--- 72 --
Question ID: 1fadb2ec53e4657eabefd42f3153a608a9fd92d1_11
Original Code:
```
def _put(d, keys, value):
    """
    helper function to put a value into a nested dict with dotted string notation.
    Sould not be used from the api.
    :param d:
    :param keys:
    :param value:
    :return:
    """
    if not isinstance(d, dict):
        return
    if "." in keys:
        key, rest = keys.split(".", 1)
        _put(d[key], rest, value)
    else:
        d[keys] = value
```


Overlapping Code:
```
tion to put a value into a nested dict with dotted string notation.
Sould not be used from the api.
:param d:
:param keys:
:param value:
:return:
"""
if not isinstance(d, dict):
return
if "." in keys:
key, rest = keys.split(".", 1)
_put(d[key], rest,
```
<Overlap Ratio: 0.78125>

---

--- 73 --
Question ID: 18c51a1886214e619b1ff8291453dddf2f1a0263_2
Original Code:
```
@borg.on(admin_cmd("speed ?(.*)"))
async def _(event):
    if event.fwd_from:
        return
    input_str = event.pattern_match.group(1)
    as_text = True
    as_document = False
    if input_str == "image":
        as_document = False
    elif input_str == "file":
        as_document = True
    elif input_str == "text":
        as_text = True
    await event.edit("`Calculating my internet speed. Please wait!`")
    start = datetime.now()
    s = speedtest.Speedtest()
    s.get_best_server()
    s.download()
    s.upload()
    end = datetime.now()
    ms = (end - start).microseconds / 1000
    response = s.results.dict()
    download_speed = response.get("download")
    upload_speed = response.get("upload")
    ping_time = response.get("ping")
    client_infos = response.get("client")
    i_s_p = client_infos.get("isp")
    i_s_p_rating = client_infos.get("isprating")
    reply_msg_id = event.message.id
    if event.reply_to_msg_id:
        reply_msg_id = event.reply_to_msg_id
    try:
        response = s.results.share()
        speedtest_image = response
        if as_text:
            await event.edit("""`SpeedTest completed in {} seconds`

`Download: {}`
`Upload: {}`
`Ping: {}`
`Internet Service Provider: {}`
`ISP Rating: {}`""".format(ms, convert_from_bytes(download_speed), convert_from_bytes(upload_speed), ping_time, i_s_p, i_s_p_rating))
        else:
            await borg.send_file(
                event.chat_id,
                speedtest_image,
                caption="**SpeedTest** completed in {} seconds".format(ms),
                force_document=as_document,
                reply_to=reply_msg_id,
                allow_cache=False
            )
            await event.delete()
    except Exception as exc:
        await event.edit("""**SpeedTest** completed in {} seconds
Download: {}
Upload: {}
Ping: {}

__With the Following ERRORs__
{}""".format(ms, convert_from_bytes(download_speed), convert_from_bytes(upload_speed), ping_time, str(exc)))
```


Overlapping Code:
```
_cmd("speed ?(.*)"))
async def _(event):
if event.fwd_from:
return
input_str = event.pattern_match.group(1)
as_text = True
as_document = False
if input_str == "image":
as_document = False
elif input_str == "file":
as_document = True
elif input_str == "text":
as_text = True
await event.edit("`Calculating my internet speed. Please wait!`")
start = datetime.now()
s = speedtest.Speedtest()
s.get_best_server()
s.download()
s.upload()
end = datetime.now()
ms = (end - start).microseconds / 1000
response = s.results.dict()
download_speed = response.get("download")
upload_speed = response.get("upload")
ping_time = response.get("ping")
client_infos = response.get("client")
i_s_p = client_infos.get("isp")
i_s_p_rating = client_infos.get("isprating")
reply_msg_id = event.message.id
if event.reply_to_msg_id:
reply_msg_id = event.reply_to_msg_id
try:
response = s.results.share()
speedtest_image = response
if as_text:
await event.edit("""`SpeedTest completed in {} seconds`
`Download: {}`
`Upload: {}`
`Ping: {}`
`Internet Service Provider: {}`
`ISP Rating: {}`""".format(ms, convert_from_bytes(download_speed), convert_from_bytes(upload_speed), ping_time, i_s_p, i_s_p_rating))
else:
await borg.send_file(
event.chat_id,
speedtest_image,
caption="**SpeedTest** completed in {} seconds".format(ms),
force_document=as_document,
reply_to=reply_msg_id,
allow_cache=False
)
await event.delete()
except Exception as exc:
await event.edit("""**SpeedTest** completed in {} seconds
Download: {}
Upload: {}
Ping: {}
__With the Following ERRORs__
{}""".format(ms, convert_from_bytes(download_speed), convert_from_bytes(upload_speed), ping_time, str(exc))
```
<Overlap Ratio: 0.9909529553679132>

---

--- 74 --
Question ID: 084ffc6f53d15e2d7e6c129e64073616a4cc7ba3_0
Original Code:
```
def show_warning(ser):
    warning_bytes = []
    for x in "5757a4a4".split():
        warning_bytes.append(binascii.a2b_hex(x))
        ser.writelines(warning_bytes)
```


Overlapping Code:
```
n "5757a4a4".split():
warning_bytes.append(binasci
```
<Overlap Ratio: 0.352112676056338>

---

--- 75 --
Question ID: 85a38ac12be3dc9bf96e5a913454de2eff28cd7e_206
Original Code:
```
def register_Ns3MacLow_methods(root_module, cls):
    ## mac-low.h (module 'wifi'): ns3::MacLow::MacLow(ns3::MacLow const & arg0) [copy constructor]
    cls.add_constructor([param('ns3::MacLow const &', 'arg0')])
    ## mac-low.h (module 'wifi'): ns3::MacLow::MacLow() [constructor]
    cls.add_constructor([])
    ## mac-low.h (module 'wifi'): ns3::Ptr<ns3::Packet> ns3::MacLow::AggregateToAmpdu(ns3::Ptr<ns3::Packet const> packet, ns3::WifiMacHeader const hdr) [member function]
    cls.add_method('AggregateToAmpdu', 
                   'ns3::Ptr< ns3::Packet >', 
                   [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const', 'hdr')])
    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::CalculateTransmissionTime(ns3::Ptr<ns3::Packet const> packet, ns3::WifiMacHeader const * hdr, ns3::MacLowTransmissionParameters const & parameters) const [member function]
    cls.add_method('CalculateTransmissionTime', 
                   'ns3::Time', 
                   [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const *', 'hdr'), param('ns3::MacLowTransmissionParameters const &', 'parameters')], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): void ns3::MacLow::CreateBlockAckAgreement(ns3::MgtAddBaResponseHeader const * respHdr, ns3::Mac48Address originator, uint16_t startingSeq) [member function]
    cls.add_method('CreateBlockAckAgreement', 
                   'void', 
                   [param('ns3::MgtAddBaResponseHeader const *', 'respHdr'), param('ns3::Mac48Address', 'originator'), param('uint16_t', 'startingSeq')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::DeaggregateAmpduAndReceive(ns3::Ptr<ns3::Packet> aggregatedPacket, double rxSnr, ns3::WifiMode txMode, ns3::WifiPreamble preamble) [member function]
    cls.add_method('DeaggregateAmpduAndReceive', 
                   'void', 
                   [param('ns3::Ptr< ns3::Packet >', 'aggregatedPacket'), param('double', 'rxSnr'), param('ns3::WifiMode', 'txMode'), param('ns3::WifiPreamble', 'preamble')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::DestroyBlockAckAgreement(ns3::Mac48Address originator, uint8_t tid) [member function]
    cls.add_method('DestroyBlockAckAgreement', 
                   'void', 
                   [param('ns3::Mac48Address', 'originator'), param('uint8_t', 'tid')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::FlushAggregateQueue() [member function]
    cls.add_method('FlushAggregateQueue', 
                   'void', 
                   [])
    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetAckTimeout() const [member function]
    cls.add_method('GetAckTimeout', 
                   'ns3::Time', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): ns3::Mac48Address ns3::MacLow::GetAddress() const [member function]
    cls.add_method('GetAddress', 
                   'ns3::Mac48Address', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetBasicBlockAckTimeout() const [member function]
    cls.add_method('GetBasicBlockAckTimeout', 
                   'ns3::Time', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): ns3::Mac48Address ns3::MacLow::GetBssid() const [member function]
    cls.add_method('GetBssid', 
                   'ns3::Mac48Address', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetCompressedBlockAckTimeout() const [member function]
    cls.add_method('GetCompressedBlockAckTimeout', 
                   'ns3::Time', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetCtsTimeout() const [member function]
    cls.add_method('GetCtsTimeout', 
                   'ns3::Time', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): bool ns3::MacLow::GetCtsToSelfSupported() const [member function]
    cls.add_method('GetCtsToSelfSupported', 
                   'bool', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): ns3::Ptr<ns3::WifiPhy> ns3::MacLow::GetPhy() const [member function]
    cls.add_method('GetPhy', 
                   'ns3::Ptr< ns3::WifiPhy >', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetPifs() const [member function]
    cls.add_method('GetPifs', 
                   'ns3::Time', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetRifs() const [member function]
    cls.add_method('GetRifs', 
                   'ns3::Time', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetSifs() const [member function]
    cls.add_method('GetSifs', 
                   'ns3::Time', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): ns3::Time ns3::MacLow::GetSlotTime() const [member function]
    cls.add_method('GetSlotTime', 
                   'ns3::Time', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): bool ns3::MacLow::IsPromisc() const [member function]
    cls.add_method('IsPromisc', 
                   'bool', 
                   [], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): void ns3::MacLow::NotifySleepNow() [member function]
    cls.add_method('NotifySleepNow', 
                   'void', 
                   [])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::NotifySwitchingStartNow(ns3::Time duration) [member function]
    cls.add_method('NotifySwitchingStartNow', 
                   'void', 
                   [param('ns3::Time', 'duration')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::ReceiveError(ns3::Ptr<ns3::Packet const> packet, double rxSnr) [member function]
    cls.add_method('ReceiveError', 
                   'void', 
                   [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('double', 'rxSnr')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::ReceiveOk(ns3::Ptr<ns3::Packet> packet, double rxSnr, ns3::WifiMode txMode, ns3::WifiPreamble preamble, bool ampduSubframe) [member function]
    cls.add_method('ReceiveOk', 
                   'void', 
                   [param('ns3::Ptr< ns3::Packet >', 'packet'), param('double', 'rxSnr'), param('ns3::WifiMode', 'txMode'), param('ns3::WifiPreamble', 'preamble'), param('bool', 'ampduSubframe')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::RegisterBlockAckListenerForAc(ns3::AcIndex ac, ns3::MacLowBlockAckEventListener * listener) [member function]
    cls.add_method('RegisterBlockAckListenerForAc', 
                   'void', 
                   [param('ns3::AcIndex', 'ac'), param('ns3::MacLowBlockAckEventListener *', 'listener')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::RegisterDcfListener(ns3::MacLowDcfListener * listener) [member function]
    cls.add_method('RegisterDcfListener', 
                   'void', 
                   [param('ns3::MacLowDcfListener *', 'listener')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::ResetPhy() [member function]
    cls.add_method('ResetPhy', 
                   'void', 
                   [])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetAckTimeout(ns3::Time ackTimeout) [member function]
    cls.add_method('SetAckTimeout', 
                   'void', 
                   [param('ns3::Time', 'ackTimeout')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetAddress(ns3::Mac48Address ad) [member function]
    cls.add_method('SetAddress', 
                   'void', 
                   [param('ns3::Mac48Address', 'ad')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetBasicBlockAckTimeout(ns3::Time blockAckTimeout) [member function]
    cls.add_method('SetBasicBlockAckTimeout', 
                   'void', 
                   [param('ns3::Time', 'blockAckTimeout')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetBssid(ns3::Mac48Address ad) [member function]
    cls.add_method('SetBssid', 
                   'void', 
                   [param('ns3::Mac48Address', 'ad')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetCompressedBlockAckTimeout(ns3::Time blockAckTimeout) [member function]
    cls.add_method('SetCompressedBlockAckTimeout', 
                   'void', 
                   [param('ns3::Time', 'blockAckTimeout')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetCtsTimeout(ns3::Time ctsTimeout) [member function]
    cls.add_method('SetCtsTimeout', 
                   'void', 
                   [param('ns3::Time', 'ctsTimeout')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetCtsToSelfSupported(bool enable) [member function]
    cls.add_method('SetCtsToSelfSupported', 
                   'void', 
                   [param('bool', 'enable')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetMpduAggregator(ns3::Ptr<ns3::MpduAggregator> aggregator) [member function]
    cls.add_method('SetMpduAggregator', 
                   'void', 
                   [param('ns3::Ptr< ns3::MpduAggregator >', 'aggregator')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetPhy(ns3::Ptr<ns3::WifiPhy> phy) [member function]
    cls.add_method('SetPhy', 
                   'void', 
                   [param('ns3::Ptr< ns3::WifiPhy >', 'phy')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetPifs(ns3::Time pifs) [member function]
    cls.add_method('SetPifs', 
                   'void', 
                   [param('ns3::Time', 'pifs')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetPromisc() [member function]
    cls.add_method('SetPromisc', 
                   'void', 
                   [])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetRifs(ns3::Time rifs) [member function]
    cls.add_method('SetRifs', 
                   'void', 
                   [param('ns3::Time', 'rifs')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetRxCallback(ns3::Callback<void, ns3::Ptr<ns3::Packet>, ns3::WifiMacHeader const*, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty> callback) [member function]
    cls.add_method('SetRxCallback', 
                   'void', 
                   [param('ns3::Callback< void, ns3::Ptr< ns3::Packet >, ns3::WifiMacHeader const *, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty >', 'callback')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetSifs(ns3::Time sifs) [member function]
    cls.add_method('SetSifs', 
                   'void', 
                   [param('ns3::Time', 'sifs')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetSlotTime(ns3::Time slotTime) [member function]
    cls.add_method('SetSlotTime', 
                   'void', 
                   [param('ns3::Time', 'slotTime')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::SetWifiRemoteStationManager(ns3::Ptr<ns3::WifiRemoteStationManager> manager) [member function]
    cls.add_method('SetWifiRemoteStationManager', 
                   'void', 
                   [param('ns3::Ptr< ns3::WifiRemoteStationManager >', 'manager')])
    ## mac-low.h (module 'wifi'): void ns3::MacLow::StartTransmission(ns3::Ptr<ns3::Packet const> packet, ns3::WifiMacHeader const * hdr, ns3::MacLowTransmissionParameters parameters, ns3::MacLowTransmissionListener * listener) [member function]
    cls.add_method('StartTransmission', 
                   'void', 
                   [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const *', 'hdr'), param('ns3::MacLowTransmissionParameters', 'parameters'), param('ns3::MacLowTransmissionListener *', 'listener')], 
                   is_virtual=True)
    ## mac-low.h (module 'wifi'): bool ns3::MacLow::StopAggregation(ns3::Ptr<ns3::Packet const> peekedPacket, ns3::WifiMacHeader peekedHdr, ns3::Ptr<ns3::Packet> aggregatedPacket, uint16_t size) const [member function]
    cls.add_method('StopAggregation', 
                   'bool', 
                   [param('ns3::Ptr< ns3::Packet const >', 'peekedPacket'), param('ns3::WifiMacHeader', 'peekedHdr'), param('ns3::Ptr< ns3::Packet >', 'aggregatedPacket'), param('uint16_t', 'size')], 
                   is_const=True)
    ## mac-low.h (module 'wifi'): ns3::WifiTxVector ns3::MacLow::GetDataTxVector(ns3::Ptr<ns3::Packet const> packet, ns3::WifiMacHeader const * hdr) const [member function]
    cls.add_method('GetDataTxVector', 
                   'ns3::WifiTxVector', 
                   [param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns3::WifiMacHeader const *', 'hdr')], 
                   is_const=True, visibility='protected', is_virtual=True)
    ## mac-low.h (module 'wifi'): void ns3::MacLow::DoDispose() [member function]
    cls.add_method('DoDispose', 
                   'void', 
                   [], 
                   visibility='private', is_virtual=True)
    return
```


Overlapping Code:
```
 const & arg0) [copy constructor]
cls.add_constructor([param('ns3::', 
[param('ns3::Ptr< ns3::Packet const >', 'packet'), param('ns', 
[param('ns3::Ptr< ns3::Packet const >', 'packet'), param('nse, ns3::WifiPreamble preamble) [member function]
cl::DestroyBlockAckAgreement(ns3::Mac48Address origi
```
<Overlap Ratio: 0.132973944294699>

---

--- 76 --
Question ID: 55e32f3c41a24b89029d05342314cf2f54db12f5_0
Original Code:
```
def get_data(req):

    # humens = Humen.objects.filter(money__gt=10050)
    # avg_age = humens.aggregate(Avg('age'))
    # return  HttpResponse(avg_age.get('age__avg'))

    humens = Humen.objects.filter(money__gt=10050)
    avg_age = humens.aggregate(Sum('age'))
    return  HttpResponse(avg_age.get('age__sum'))
```


Overlapping Code:
```
s = Humen.objects.filter(money__gt=10050)
# avg_age = humens.aggregate(Avg('age'))
# return HttpResponse(avg_age.get('age__avg'))
humens = Humen.objects.filter(money__gt=10050)
avg_age = humens.aggregate(Sum('age'))
return HttpResponse(avg_age.get('a
```
<Overlap Ratio: 0.8741258741258742>

---

--- 77 --
Question ID: e01891d97318d84fe167c82674fbdb538fd95021_10
Original Code:
```
def webform_download(request,slug):
    dump_dir = '/protwis/construct_dump'
    # dump_dir = '/web/sites/files/construct_data' #for sites
    file = dump_dir+"/"+str(slug)+".json"
    out_stream = open(file,"rb").read()
    response = HttpResponse(content_type="application/json")
    response['Content-Disposition'] = 'attachment; filename="{}"'.format(file)
    response.write(out_stream)
    return response
```


Overlapping Code:
```
ebform_download(request,slug):
dump_dir = '/protwis/construct_dump'
# dump_dir = '/web/sites/files/construct_data' #for sites
file = dump_dir+"/"+str(slug)+".json"
out_stream = open(file,"rb").read()
response = HttpResponse(content_type="application/json")
response['Content-Disposition'] = 'attachment; filename="{}"'.format(file)
response.write(out_strea
```
<Overlap Ratio: 0.9393139841688655>

---

--- 78 --
Question ID: fa894e365b78fea6e59360041eb5a7771f2f7045_23
Original Code:
```
@given(frame())
def test_grayscale(frame: Frame):
    with patch("facelift.transform.cv2.cvtColor") as mocked_cv2_cvtColor:
        transform.grayscale(frame)

    mocked_cv2_cvtColor.assert_called_once_with(src=frame, code=cv2.COLOR_BGR2GRAY)
```


Overlapping Code:
```
scale(frame: Frame):
with patch("facelift.transform.cv2.cvtColor") as mocked_cv2_cvtColor:
transform.grayscale(frame)
mocked_cv2_cvtColor.assert_called_once_with(src=frame, code=cv2.COL
```
<Overlap Ratio: 0.8185840707964602>

---

--- 79 --
Question ID: 9d969833ce839224792d6fec999354f6bce66c85_0
Original Code:
```
def find_packages(where, exclude=None):
    if not exclude:
        exclude = ()
    if isinstance(where, str):
        where = (where, )

    ret_list = []
    for name in chain.from_iterable(map(lambda w: (
      n for _, n, ispkg in w if ispkg), (walk_packages(p) for p in where))):
        if not any(wc_match(name, p) for p in exclude):
            ret_list.append(name)

    return tuple(ret_list)
```


Overlapping Code:
```
 not exclude:
exclude = ()
if isinstance(where, str):
where = (where, )
ret_list = []
for name in chain.from_iterable(map(lambda w: (
n for _, n, ispkg in w if ispkg), (walk_packages(p) for p in where))):
if not any(wc_match(name, p) for p in exclude
```
<Overlap Ratio: 0.7374631268436578>

---

--- 80 --
Question ID: 5ad24b1552a54c6d7d1651eab5ec89d070518b50_0
Original Code:
```
def upload(filepath):
    payload = {}
    files = [
        ('file', open(filepath, 'rb'))
    ]
    response = requests.request("POST", url, data=payload, files=files)
    if response.ok:
        return 1
        print(response.json)
    else:
        print(123)
```


Overlapping Code:
```
:
payload = {}
files = [
('file', open(filepath, 'rb'))
]
response = requests.request("POST", url, data=payload, files=files)
if response.ok:
return 1
```
<Overlap Ratio: 0.7211538461538461>

---

--- 81 --
Question ID: 768927f522f0df61a37ddb309601dac8d35246dd_21
Original Code:
```
@test
def plano_command():
    if PYTHON2: # pragma: nocover
        raise PlanoTestSkipped("The plano command is not supported on Python 2")

    with working_dir():
        PlanoCommand().main([])

    with working_dir():
        write("Planofile", "garbage")

        with expect_system_exit():
            PlanoCommand().main([])

    with expect_system_exit():
        PlanoCommand("no-such-file").main([])

    with expect_system_exit():
        PlanoCommand().main(["-f", "no-such-file"])

    def run_command(*args):
        PlanoCommand().main(["-f", test_project_dir] + list(args))

    with test_project():
        run_command()
        run_command("--help")
        run_command("--quiet")
        run_command("--init-only")

        run_command("build")
        run_command("install")
        run_command("clean")

        with expect_system_exit():
            run_command("build", "--help")

        with expect_system_exit():
            run_command("no-such-command")

        with expect_system_exit():
            run_command("no-such-command", "--help")

        with expect_system_exit():
            run_command("--help", "no-such-command")

        run_command("extended-command", "a", "b", "--omega", "z")

        with expect_system_exit():
            run_command("echo")

        with expect_exception(contains="Trouble"):
            run_command("echo", "Hello", "--trouble")

        run_command("echo", "Hello", "--count", "5")

        with expect_system_exit():
            run_command("echo", "Hello", "--count", "not-an-int")

        run_command("haberdash", "ballcap", "fedora", "hardhat", "--last", "turban")
        result = read_json("haberdash.json")
        assert result == ["ballcap", "fedora", "hardhat", "turban"], result

        run_command("haberdash", "ballcap", "--last", "turban")
        result = read_json("haberdash.json")
        assert result == ["ballcap", "turban"], result

        run_command("haberdash", "ballcap")
        result = read_json("haberdash.json")
        assert result == ["ballcap", "bowler"], result

        run_command("balderdash", "bunk", "poppycock")
        result = read_json("balderdash.json")
        assert result == ["bunk", "poppycock", "rubbish"], result

        run_command("balderdash", "bunk")
        result = read_json("balderdash.json")
        assert result == ["bunk", "malarkey", "rubbish"], result

        run_command("balderdash", "bunk", "--other", "bollocks")
        result = read_json("balderdash.json")
        assert result == ["bunk", "malarkey", "bollocks"], result
```


Overlapping Code:
```
test
def plano_command():
if PYTHON2: # pragma: nocover
raise PlanoTestSkipped("The plano command is not supported on Python 2")
with working_dir():
PlanoCommand().main([])
with working_dir():
write("Planofile", "garbage")
with expect_system_exit():
PlanoCommand().main([])
with expect_system_exit():
PlanoCommand("no-such-file").main([])
with expect_system_exit():
PlanoCommand().main(["-f", "no-such-file"])
def run_command(*args):
PlanoCommand().main(["-f", test_project_dir] + list(args))
with test_project():
run_command()
run_command("--help")
run_command("--quiet")
run_command("--init-only")
run_command("build")
run_command("install")
run_command("clean")
with expect_system_exit():
run_command("build", "--help")
with expect_system_exit():
run_command("no-such-command")
with expect_system_exit():
run_command("no-such-command", "--help")
with expect_system_exit():
run_command("--help", "no-such-command")
run_command("extended-command", "a", "b", "--omega", "z")
with expect_system_exit():
run_command("echo")
with expect_exception(contains="Trouble"):
run_command("echo", "Hello", "--trouble")
run_command("echo", "Hello", "--count", "5")
with expect_system_exit():
run_command("echo", "Hello", "--count", "not-an-int")
run_command("haberdash", "ballcap", "fedora", "hardhat", "--last", "turban")
result = read_json("haberdash.json")
assert result == ["ballcap", "fedora", "hardhat", "turban"], result
run_command("haberdash", "ballcap", "--last", "turban")
result = read_json("haberdash.json")
assert result == ["ballcap", "turban"], result
run_command("haberdash", "ballcap")
result = read_json("haberdash.json")
assert result == ["ballcap", "bowler"], result
run_command("balderdash", "bunk", "poppycock")
result = read_json("balderdash.json")
assert result == ["bunk", "poppycock", "rubbish"], result
run_command("balderdash", "bunk")
result = read_json("balderdash.json")
assert result == ["bunk", "malarkey", "rubbish"], result
run_command("balderdash", "bunk", "--other", "bollock
```
<Overlap Ratio: 0.984251968503937>

---

--- 82 --
Question ID: 9efbc8fc3a21394fe8714b4e0acd8bb2d358a0cd_0
Original Code:
```
def run_tests(test_config: config.TestConfig) -> None:
    """Run tests with specification given in config"""

    print("Testing config:")
    print(test_config)
    print()

    if not os.path.isdir(test_config.test_dir):
        cli.print_error(f"No directory `{test_config.test_dir}`")
        return

    if test_config.groups is None:
        groups = get_groups_in_dir(test_config.test_dir)
    else:
        groups = set()
        for pattern in test_config.groups:
            for group in get_groups_in_dir_matching(test_config.test_dir,
                                                    pattern):
                groups.add(group)

    for group in groups:
        run_test_group(group, test_config)
```


Overlapping Code:
```
un_tests(test_config: config.TestConfig) -> None:
"""Run tests with specification given in config"""
print("Testing config:")
print(test_config)
print()
if not os.path.isdir(test_config.test_dir):
cli.print_error(f"No directory `{test_config.test_dir}`")
return
if test_config.groups is None:
groups = get_groups_in_dir(test_config.test_dir)
else:
groups = set()
for pattern in test_config.groups:
for group in get_groups_in_dir_matching(test_config.test_dir,
pattern):
groups.add(group)
for group in
```
<Overlap Ratio: 0.9124087591240876>

---

--- 83 --
Question ID: 4679dc674345cea16066de909faaff14eeb152bc_2
Original Code:
```
def test_edge_end_node_not_in_graph():

    graph = Graph()

    end = Vertex('end')

    start = graph.add_node('start')

    with pytest.raises(KeyError):
        graph.add_edge(start, end)
```


Overlapping Code:
```
test_edge_end_node_not_in_graph():
graph = Graph()
end = Vertex('end')
start = graph.add_node('start')
with pytest.raises(KeyError):
graph.add_edge(start, e
```
<Overlap Ratio: 0.9570552147239264>

---

--- 84 --
Question ID: 2ff8007a69a858e48ee7fb5000d8bff0bc1af91a_0
Original Code:
```
def parse_args(args):
    parser = argparse.ArgumentParser(description='Process optional arguments.')
    parser.add_argument('package', help='Specify the package name and version')
    parser.add_argument('--json', dest="is_json", action='store_true',
                        help='Format the result as JSON (default: false)')

    return parser.parse_args(args)
```


Overlapping Code:
```
def parse_args(args):
parser = argparse.ArgumentParser(description='Process optional arguments.')
parser.add_argument('package', help='Specify the package name and version')
parser.add_argument('--json', dest="is_json", action='store_true',
help='Format the result as JSON (defau
```
<Overlap Ratio: 0.8664596273291926>

---

--- 85 --
Question ID: 66f57937dfe7f1fd6761886efdb63338d19ce62a_5
Original Code:
```
def create_user_pack_roles(mycursor):
    '''create table for admin roles for packs
        there can be only one role per user-pack pair'''
    mycursor.execute(
        '''
        create table if not exists user_pack_roles (
            user_id int,
            pack_id int,
            role varchar(255),
            user_granted_id int,
            granted_dttm datetime
            );
        '''
    )
```


Overlapping Code:
```
te table for admin roles for packs
there can be only one role per user-pack pair'''
mycursor.execute(
'''
create table if not exists user_pack_roles (
user_id int,
pack_id int,
role varchar(255),
user
```
<Overlap Ratio: 0.684931506849315>

---

--- 86 --
Question ID: 22b29b34b674dc37e0c08ea058b6d7d9ad9291d6_0
Original Code:
```
def test_decode_text_unicode():
    value = u'\uffff'
    decoded = decode_text(value)
    assert decoded == value
```


Overlapping Code:
```
st_decode_text_unicode():
value = u'\uffff'
decode
```
<Overlap Ratio: 0.49019607843137253>

---

--- 87 --
Question ID: 1f04cf2476169e88ee631d20646ec0fa03dac648_0
Original Code:
```
@hooks.before_each
def auth_before_each_hook(transaction):
    auth = generate_token()
    if 'request' in transaction:
        if 'headers' in transaction['request'] and 'Authorization' in transaction['request']['headers']:
            transaction['request']['headers']['Authorization'] = auth
```


Overlapping Code:
```
@hooks.before_each
def auth_before_each_hook(transaction):
auth = generate_token()
if 'request' in transaction:
if 'headers' in transaction['request'] and 'Authorization' in transaction['request']['headers']:
transaction['request']['headers']['Authorization'] = a
```
<Overlap Ratio: 0.9887218045112782>

---

--- 88 --
Question ID: 67f3e66d38336b1034c4e6bd59a34e57ffdc444a_3
Original Code:
```
def normalize_tokens(word_list, extra_stop=STOP_WORDS):
    #We can use a generator here as we just need to iterate over it
    normalized = []

    if type(word_list) == list and len(word_list) == 1:
        word_list = word_list[0]

    elif type(word_list) == list:
        word_list = ' '.join([str(elem) for elem in word_list])

    doc = NLP(word_list.lower())

    # add the property of stop word to words considered as stop words
    if len(extra_stop) > 0:
        for stopword in extra_stop:
            lexeme = NLP.vocab[stopword]
            lexeme.is_stop = True

    for w in doc:
        # if it's not a stop word or punctuation mark, add it to our list
        include = (w.text != '\n' and not w.is_stop \
           and not w.is_punct and not w.like_num \
           and len(w.text.strip()) > 1 and w.is_alpha \
           and not w.lemma_ in extra_stop)

        if include:
            # we add the lematized version of the word
            normalized.append(str(w.lemma_))

    return normalized
```


Overlapping Code:
```
STOP_WORDS):
#We can use a generator here as we just need to iterate over it
normalized = []
if type(word_list) == list and len(word_list) == 1:
word_list = word_list[0]
elif type(word_list) == list:
word_list = ' '.join([str(elem) for elem in word_list])
doc = NLP(word_list.lower())
# add the property of stop word to words considered as stop words
if len(extra_stop) > 0:
for stopword in extra_stop:
lexeme = NLP.vocab[stopword]
lexeme.is_stop = True
for w in doc:
# if it's not a stop word or punctuation mark, add it to our list
include = (w.text != '\n' and not w.is_stop \
and not w.is_punct and not w.like_num \
and len(w.text.strip()) > 1 and w.is_alpha \
and not w.lemma_ in extra_stop)
if include:
# we add the lematized version of the word
normalized.append(str(w.lemma_))
return normaliz
```
<Overlap Ratio: 0.9467455621301775>

---

--- 89 --
Question ID: b889baff60b57f45907cc68676d595dcd8e35eed_0
Original Code:
```
def create(file_name, file_path='./'):
    """
    新建文件
    :param file_name: 新文件名称(含后缀)
    :param file_path: 新文件路径
    :return: None
    """
    f = open(os.path.join(file_path, file_name), 'w+')
    f.close()
```


Overlapping Code:
```
文件
:param file_name: 新文件名称(含后缀)
:param file_path: 新文件路径
:return: None
"""
f = open(os.path.join(file
```
<Overlap Ratio: 0.5586592178770949>

---

--- 90 --
Question ID: 42e5ffc678a9c27afc50d25cb70baaefa7c72dde_2
Original Code:
```
def plot_median_scale_diagram(results_dict, res_median, algo):
    fig_name = algo + '-time-median.png'
          
    D = collections.OrderedDict(sorted((res_median.items())))
    log_input = [20, 21, 22, 23, 24, 25]
   
    plt.xticks(range(len(D)), log_input)
    plt.xlabel("log[input size]", fontsize=15)
    plt.ylabel("Execution of time (s)", fontsize=15)
    plt.title("Weak Scaling PLP", fontsize=15)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.bar(range(len(D)), list(D.values()), color='darkred', zorder=2)   
    y_hor = range(10, 50, 10)
    for y in y_hor:
        plt.axhline(y=y, linewidth=0.8, color='black', linestyle='--', zorder=1)
    plt.savefig(fig_name)
```


Overlapping Code:
```
gram(results_dict, res_median, algo):
fig_name = algo + '-time-median.png'

D = collections.OrderedDict(sorted((res_median.items())))
log_input = [20, 21, 22, 23, 24, 25]

plt.xticks(range(len(D)), log_input)
plt.xlabel("log[input size]", fontsize=15)
plt.ylabel("Execution of time (s)", fontsize=15)
plt.title("Weak Scaling PLP", fontsize=15)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.bar(range(len(D)), list(D.values()), color='darkred', zorder=2) 
y_hor = range(10, 50, 10)
for y in y_hor:
plt.axhline(y=y, linewidth=0.8, color='black', 
```
<Overlap Ratio: 0.8842443729903537>

---

--- 91 --
Question ID: d9aae88f68e5810fa9375636cd67128ca2d2b315_1
Original Code:
```
def parseStrictToken(token):
    """given a string in the the format '<sign><number><unit>' return a number of seconds"""
    # get rid of the equals sign if it exists
    # print token
    if token[0] == '=':
        token = token[1:]
    # get the unit
    unit = token[-1]
    # chop the unit off the token
    token = token[:-1]

    # use python's eval to turn the possibly signed number string into an integer
    number = float(token)

    # get the unit multiplier and if necessary raise an invalid unit error
    if not unit in units:
        raise MalformedTimeString('Invalid Unit')

    unitMult = units[unit]

    return int(unitMult * number)
```


Overlapping Code:
```
n(token):
"""given a string in the the format '<sign><number><unit>' return a number of seconds"""
# get rid of the equals sign if it exists
# print token
if token[0] == '=':
token = token[1:]
# get the unit
unit = token[-1]
# chop the unit off the token
token = token[:-1]
# use python's eval to turn the possibly signed number string into an integer
number = float(token)
# get the unit multiplier and if necessary raise an invalid unit error
if not unit in units:
raise MalformedTimeString('Invalid Unit')
unitMult = units[unit]
return int(unitMul
```
<Overlap Ratio: 0.9482758620689655>

---

--- 92 --
Question ID: 6dff4785e5f5d988d4b88df1f9376d4bfac1a614_0
Original Code:
```
def _date_range(calendar_span, focal_date=None):
    # returns (start_date, end_date)
    # When calendar_span == week
    # start_date is the latest monday that is less than or equal to today,
    # end_date is start_date + seven days
    # When calendar_span == month 
    # start_date is the first day of the new month
    # end_date is start_date + length of the current month
    # both values are then padded by 24 hours to allow for TZ differences
    # throws ValueError if focal_date is not in ISO-8601 format

    if focal_date:
        initial_date = datetime.strptime(focal_date, ISO_8601_DATE_FORMAT)
    else:
        initial_date = datetime.now()

    initial_date = utc.localize(initial_date)
    if calendar_span == "month":
        reducer = timedelta(int(initial_date.strftime("%d")))
        span = timedelta(31)
    else:
        # This calculation depends on the fact that monday == 0 in python
        reducer = timedelta(initial_date.weekday())
        span = ONE_WEEK

    start_date = initial_date - reducer
    end_date = start_date + span + ONE_DAY
    adjusted_start_date = start_date - ONE_DAY
    return adjusted_start_date, end_date
```


Overlapping Code:
```
nge(calendar_span, focal_date=None):
# returns (start_date, end_date)
# When calendar_span == week
# start_date is the latest monday that is less than or equal to today,
# end_date is start_date + seven days
# When calendar_span == month 
# start_date is the first day of the new month
# end_date is start_date + length of the current month
# both values are then padded by 24 hours to allow for TZ differences
# throws ValueError if focal_date is not in ISO-8601 format
if focal_date:
initial_date = datetime.strptime(focal_date, ISO_8601_DATE_FORMAT)
else:
initial_date = datetime.now()
initial_date = utc.localize(initial_date)
if calendar_span == "month":
reducer = timedelta(int(initial_date.strftime("%d")))
span = timedelta(31)
else:
# This calculation depends on the fact that monday == 0 in python
reducer = timedelta(initial_date.weekday())
span = ONE_WEEK
start_date = initial_date - reducer
end_date = start_date + span + ONE_DAY
adjusted_start_date = start_date - ONE_DAY
return adjusted_start_date, end_da
```
<Overlap Ratio: 0.9864472410454985>

---

--- 93 --
Question ID: bdf457b9398ef057d5ab837ea9ea0040c5e528d6_1
Original Code:
```
def get_value(character, value: str) -> int:
    try:
        return int(value)
    except ValueError:
        pass

    try:
        return getattr(character, value)()
    except Exception:
        logger.warning(f'Could not parse value {value}')
        return 0
```


Overlapping Code:
```
 str) -> int:
try:
return int(value)
except ValueError:
pass
try:
return getattr(character, value)()
except Exception:
logger.warning(f'Could not parse value {value
```
<Overlap Ratio: 0.7922705314009661>

---

--- 94 --
Question ID: 4d2b7ad55e028493d7488e8c5e88fee0b41d58f3_11
Original Code:
```
def print_arr(arr):
    """
    Function to get the string version of an array in one line.
    """
    return "\n".join(arr)
```


Overlapping Code:
```
print_arr(arr):
"""
Function to get the string version of an array in one line.
"""
return "\n".join
```
<Overlap Ratio: 0.9174311926605505>

---

--- 95 --
Question ID: 76e22b95c378d69dee27d9c9b5652c34f4c41680_4
Original Code:
```
def supports_c_sizeof(conf, mandatory=True):
  '''
     Check for F2008 c_sizeof support.
  '''
  fcenv = conf.env.derive()
  fcenv.detach()

  conf.check_fc( fragment = '''
program check_c_sizeof
  use, intrinsic :: iso_c_binding, only: c_sizeof, c_float
  implicit none
  real(kind=c_float) :: a_real
  integer :: realsize
  realsize = c_sizeof(a_real)
  write(*,*) realsize
end program check_c_sizeof
''',
                 msg = 'Checking for F2008 c_sizeof support',
                 mandatory = mandatory, define_name='c_sizeof')
  fcenv['fortsupp_c_sizeof'] = conf.is_defined('c_sizeof')

  conf.env = fcenv
```


Overlapping Code:
```
pports_c_sizeof(conf, mandatory=True):
'''
Check for F2008 c_sizeof support.
'''
fcenv = conf.env.derive()
fcenv.detach()
conf.check_fc( fragment = '''
program check_c_sizeof
use, intrinsic :: iso_c_binding, only: c_sizeof, c_float
implicit none
real(kind=c_float) :: a_real
integer :: realsize
realsize = c_sizeof(a_real)
write(*,*) realsize
end program check_c_sizeof
''',
msg = 'Checking for F2008 c_sizeof support',
mandatory = mandatory, define_name='c_sizeof')
fcenv['fortsupp_c_sizeof'] = conf.is_defined('c_sizeof'
```
<Overlap Ratio: 0.9560439560439561>

---

--- 96 --
Question ID: 48c9411b3fa428be33edb97fb59b28fd37cf9156_51
Original Code:
```
def test_parse_rsa_algorithm_ps512():
    (hash, padding) = parse_rsa_algorithm('PS512')
    assert hash
    assert hash.name == 'sha512'
    assert padding.name == 'EMSA-PSS'
```


Overlapping Code:
```

```
<Overlap Ratio: 0.0>

---

--- 97 --
Question ID: 6ca24807df6446ce04faf8cc3dba08f6450d2fe3_0
Original Code:
```
def push_notification(request):
    notification_type = request.get('type')
    user_source = request.get('source')
    user_target = request.get('target')
    post_id = request.get('post_id')
    return {
        NotificationType.FOLLOW : lambda: send_follow_notification(user_target, user_source),
        NotificationType.UNFOLLOW : lambda: remove_follow_notification(user_target, user_source),
        NotificationType.LIKE : lambda: send_like_notification(user_target, user_source, post_id),
        NotificationType.DISLIKE : lambda: remove_like_notification(user_target, user_source, post_id),
        NotificationType.COMMENT : lambda: send_comment_notification(user_target, user_source, post_id)
    }[notification_type]()
```


Overlapping Code:
```
notification_type = request.get('type')
user_source = request.get('source')
user_target = request.get('target')
post_id = request.get('post_id')
return {
NotificationType.FOLLOW : lambda: send_follow_notification(user_target, user_source),
NotificationType.UNFOLLOW : lambda: remove_follow_notification(user_target, user_source),
NotificationType.LIKE : lambda: send_like_notification(user_target, user_source, post_id),
NotificationType.DISLIKE : lambda: remove_like_notification(user_target, user_source, post_id),
NotificationType.COMMENT : lambda: send_comment_notification(user_target, user_sour
```
<Overlap Ratio: 0.8995502248875562>

---

--- 98 --
Question ID: 5a008166fc049653195ca50a0b6d46f3b8d5e955_14
Original Code:
```
@main.group(cls=DYMGroup)
def run():
    """Run actions in an app"""
    pass
```


Overlapping Code:
```

```
<Overlap Ratio: 0.0>

---

--- 99 --
Question ID: 489067cb013ca441b8482a649f2bc350bc5740d6_0
Original Code:
```
def printBanner():
    print(BLUE + BOLD + '                    ____     ____')
    print("                  /'    |   |    \\")
    print('                /    /  |   | \   \\')
    print('              /    / |  |   |  \   \\')
    print('             (   /   |  """"   |\   \ ' + END + BOLD + '   Ahhh HA!!!')
    print(BLUE + BOLD + '             | /   / /^\    /^\  \  _| ' + END + BOLD + '       I love big big carrot!!!')
    print(BLUE + BOLD + '              ~   | |   |  |   | | ~')
    print('                  | |__' + END + BOLD + 'O' + BLUE + BOLD + '|__|' + END + BOLD + 'O' + BLUE + BOLD + '__| |')
    print('                /~~      \/     ~~\\')
    print('               /   (      |      )  \\')
    print("         _--_  /,   \____/^\___/'   \  _--_")
    print('       /~    ~\ / -____-|_|_|-____-\ /~    ~\\')
    print('     /________|___/~~~~\___/~~~~\ __|________\\')
    print('--~~~          ^ |     |   |     |  -     :  ~~~~~:~-_     ___-----~~~~~~~~|')
    print("   /             `^-^-^'   `^-^-^'                  :  ~\ /'   ____/--------|")
    print(' ;                                    :              :    |----------/--------|')
    print(':          ,   ' + VIOLET + BOLD + 'ChubbyListener:' + RED + BOLD + ' v1.0.0' + BLUE + BOLD + '             ;    .  |---\\--------------|')
    print(' :     -      ' + VIOLET + BOLD + 'Writen by:' + RED + BOLD + ' 0BL1V10N V01D' + BLUE + BOLD + ' .              : : |______________-__|')
    print("  :              ,                 ,                :   /'~----___________|")
    print('__  \\\        ^                          ,, ;; ;; ;._-~')
    print('  ~~~-----____________________________________----~~~')
```


Overlapping Code:
```
 ____ ____')
print(" /' | | \\")
print(' / / | | \ \\')
print(' / / | | | \ \\')
print(' ( / | """" |\ \ ' + END + BOLD + ' Ahhh HA!!!')
print(BLUE + BOLD + ' | / / /^\ /^\ \ _| ' + END + BOLD + ' I love big big carrot!!!')
print(BLUE + BOLD + ' ~ | | | | | | ~')
print(' | |__' + END + BOLD + 'O' + BLUE + BOLD + '|__|' + END + BOLD + 'O' + BLUE + BOLD + '__| |')
print(' /~~ \/ ~~\\')
print(' / ( | ) \\')
print(" _--_ /, \____/^\___/' \ _--_")
print(' /~ ~\ / -____-|_|_|-____-\ /~ ~\\')
print(' /________|___/~~~~\___/~~~~\ __|________\\')
print('--~~~ ^ | | | | - : ~~~~~:~-_ ___-----~~~~~~~~|')
print(" / `^-^-^' `^-^-^' : ~\ /' ____/--------|")
print(' ; : : |----------/--------|')
print(': , ' + VIOLET + BOLD + 'ChubbyListener:' + RED + BOLD + ' v1.0.0' + BLUE + BOLD + ' ; . |---\\--------------|')
print(' : - ' + VIOLET + BOLD + 'Writen by:' + RED + BOLD + ' 0BL1V10N V01D' + BLUE + BOLD + ' . : : |______________-__|')
print(" : , , : /'~----___________|")
print('__ \\\ ^ ,, ;; ;; ;._-~')
print(' ~~~-----______________________________
```
<Overlap Ratio: 0.9502262443438914>

---

--- 100 --
Question ID: 8a6eb3b68b5a246a778357aed199eaeef8602436_1
Original Code:
```
def downloadAllVideos(data_dir, video_dict):
    """
    download the video
    :param video_dict: all video ids saved in the dict
    :return:
    """

    # use multiprocessing pool
    pool = multiprocessing.Pool(4)
    for key in video_dict:
        # Extract the words consisting of video_id, start_time, end_time, list of video_tags
        for v_id in video_dict[key]:
            pool.apply_async(download_vid, (data_dir, key, v_id))
    pool.close()
    pool.join()
```


Overlapping Code:
```
llVideos(data_dir, video_dict):
"""
download the video
:param video_dict: all video ids saved in the dict
:return:
"""
# use multiprocessing pool
pool = multiprocessing.Pool(4)
for key in video_dict:
# Extract the words consisting of video_id, start_time, end_time, list of video_tags
for v_id in video_dict[key]:
pool.apply_async(download_vid, (data
```
<Overlap Ratio: 0.8641975308641975>

---

--- 101 --
Question ID: d81a96d9bba7bb461e51e7310f24c9106ee37c39_19
Original Code:
```
def build_rowid_blocks(Zvr):
    A = np.asarray(Zvr).T
    U = map(tuple, A)
    return {u:np.where(np.all(A==u, axis=1))[0] for u in U}
```


Overlapping Code:
```
ray(Zvr).T
U = map(tuple, A)
return {u:np.where(np
```
<Overlap Ratio: 0.4032258064516129>

---

--- 102 --
Question ID: ccf044473bec01f2984b4d2acf41dc342e0e7bd6_5
Original Code:
```
def parse_quote_page(xml: minidom.Document, start_tag: str, cats: list, title_tag=TITLE_TAG) -> list:
    """
    Reads through the page with quotes and parses them out
    @param xml: the xml node list to parse
    @param start_tag: xml tag to start reading elements from
    @param cats: category list for the quotes on this page
    @param title_tag: tag to parse the page title from
    @return: list of quote objects
    """

    quote_area = xml.getElementsByTagName(start_tag)
    title_elem = minidom.NodeList(xml.getElementsByTagName(title_tag))
    author = title_elem.item(0).getAttribute('title')
    page_data = quote_area[0].firstChild.data.split('\n')

    i = 0
    quotes = []

    for line in page_data:
        # remove the denotation chars for a quote  {\{citat\ |
        matches = re.match('\* ([\S ]+)', line) \
                  or re.match('# ([^\']+)', line) \
                  or re.match('\*([^*]+)', line) \
                  or re.match('\{\{citat(?:ion)?\|([\S ]+)', line)

        # crap to remove in other pages/languages
        # or re.match('# ([^\']+)', line) or re.match('\*([^*]+)', line)

        if matches:
            quotes.append(format_quote(quote_line=matches.group(1), quote_id=i, author=author, cats=cats))
            i += 1
        else:
            matches = re.match('\*\*([^*]+)', line)

            if matches:
                last_quote = quotes.pop()
                last_quote.ref = matches.group(1).strip()\
                    .replace('[', '').replace(']', '')  # .replace('"', '""')
                quotes.append(last_quote)

        if re.match('\{\{misattributed|\{\{disputed', line, re.IGNORECASE):
            break  # don't care about getting anything in misattributed and below

    return quotes
```


Overlapping Code:
```
nt, start_tag: str, cats: list, title_tag=TITLE_TAG) -> list:
"""
Reads through the page with quotes and parses them out
@param xml: the xml node list to parse
@param start_tag: xml tag to start reading elements from
@param cats: category list for the quotes on this page
@param title_tag: tag to parse the page title from
@return: list of quote objects
"""
quote_area = xml.getElementsByTagName(start_tag)
title_elem = minidom.NodeList(xml.getElementsByTagName(title_tag))
author = title_elem.item(0).getAttribute('title')
page_data = quote_area[0].firstChild.data.split('\n')
i = 0
quotes = []
for line in page_data:
# remove the denotation chars for a quote {\{citat\ |
matches = re.match('\* ([\S ]+)', line) \
or re.match('# ([^\']+)', line) \
or re.match('\*([^*]+)', line) \
or re.match('\{\{citat(?:ion)?\|([\S ]+)', line)
# crap to remove in other pages/languages
# or re.match('# ([^\']+)', line) or re.match('\*([^*]+)', line)
if matches:
quotes.append(format_quote(quote_line=matches.group(1), quote_id=i, author=author, cats=cats))
i += 1
else:
matches = re.match('\*\*([^*]+)', line)
if matches:
last_quote = quotes.pop()
last_quote.ref = matches.group(1).strip()\
.replace('[', '').replace(']', '') # .replace('"', '""')
quotes.append(last_quote)
if re.match('\{\{misattributed|\{\{disputed', line, re.IGNORECASE):
break # don't care about getting anything in misattributed and below
r
```
<Overlap Ratio: 0.9641873278236914>

---

--- 103 --
Question ID: 0d35655989af14d192d0ac1f0d96cb5914890556_4
Original Code:
```
def truncate_treat_sample(y, t, time, policy, rng, final=False, stack=False):
    y_rx = np.array(y)
    t_rx = np.array(t)
    treated = np.zeros(len(t), dtype=bool)
    rx = np.zeros(len(t))

    for i, t0 in enumerate(t):
        if t0 > time: # only treat before `time`
            break

        rx[i] = policy.sample_treatment(y_rx[:(i+1)], t_rx[:(i+1)], rng)

        if rx[i] == 1:
            y_rx, t_rx, treated = policy.treat(y_rx, t_rx, treated, t0, stack)

    # Add a final treatment at `time`.
    if final:
        rx[t_rx == time] = 1.0
        y_rx, t_rx, treated = policy.treat(y_rx, t_rx, treated, time, stack)

    return y_rx, (t_rx, rx)
```


Overlapping Code:
```
ample(y, t, time, policy, rng, final=False, stack=False):
y_rx = np.array(y)
t_rx = np.array(t)
treated = np.zeros(len(t), dtype=bool)
rx = np.zeros(len(t))
for i, t0 in enumerate(t):
if t0 > time: # only treat before `time`
break
rx[i] = policy.sample_treatment(y_rx[:(i+1)], t_rx[:(i+1)], rng)
if rx[i] == 1:
y_rx, t_rx, treated = policy.treat(y_rx, t_rx, treated, t0, stack)
# Add a final treatment at `time`.
if final:
rx[t_rx == time] = 1.0
y_rx, t_rx, treated = policy.treat(y_rx, t_rx, treated
```
<Overlap Ratio: 0.8960573476702509>

---

--- 104 --
Question ID: a11c2ef4cf43b1ec2e0dd41218ccb654f28f840e_1
Original Code:
```
def registration(request):

    user_registered = False
    error_flag = ''

    if request.method == 'POST':
        user_form = UserProfileForm(data=request.POST)
        participants_form = UserParticipantsForm(data=request.POST)

        #profile validation
        if user_form.is_valid():

            user = user_form.save()
            user.set_password(user.password)
            user.save()

            participants = participants_form.save(commit = False)
            participants.user = user

            participants.save()

            user_registered = True

        else:
            error_flag = 'Error occured during validation!'

    else:
        user_form = UserProfileForm()
        participants_form = UserParticipantsForm()

    return render(request,'user/registration.html',{'user_registered':user_registered,
                                                    'error_flag':error_flag,
                                                    'user_form':user_form,
                                                    'participants_form':participants_form,})
```


Overlapping Code:
```
request):
user_registered = False
error_flag = ''
if request.method == 'POST':
user_form = UserProfileForm(data=request.POST)
participants_form = UserParticipantsForm(data=request.POST)
#profile validation
if user_form.is_valid():
user = user_form.save()
user.set_password(user.password)
user.save()ticipants = participants_form.save(commit = False)
participants.user = user
participants.save()
user_registered = True
else:
error_flag = 'Error occured during validation!'
else:
user_form = UserProfileForm()
participants_form = UserParticipantsForm()
return render(request,'user/registration.html',{'user_registered':user_registered,
'error_flag':error_flag,
'user_form':user_form,
'participants_for
```
<Overlap Ratio: 0.9407806191117093>

---

--- 105 --
Question ID: 967124f087e34dac7da54aae5381bd300b7fee07_0
Original Code:
```
def bowling_score(frames):
    score=frames.split(" ")
    total=0
    for i in range(len(score)-1):
        if calculate(score[i])==10:
            if "X" in score[i]:
                if len(score[i+1])==1:
                    total+=10+calculate(score[i+1][0])+calculate(score[i+2][0])
                else:
                    total+=10+calculate(score[i+1][:2])
            else:
                total+=10+calculate(score[i+1][0])
        else:
            total+=calculate(score[i])
    if len(score[-1])==2:
        return total+calculate(score[-1])
    return total+calculate(score[-1][:2])+calculate(score[-1][2:])
```


Overlapping Code:
```
def bowling_score(frames):
score=frames.split(" ")
total=0
for i in range(len(score)-1):
if calculate(score[i])==10:
if "X" in score[i]:
if len(score[i+1])==1:
total+=10+calculate(score[i+1][0])+calculate(score[i+2][0])
else:
total+=10+calculate(score[i+1][:2])
else:
total+=10+calculate(score[i+1][0])
else:
total+=calculate(score[i])
if len(score[-1])==2:
return total+calculate(score[-1])
return total+calculate(score[-1][:2])+calculate(score[-1][
```
<Overlap Ratio: 0.9911894273127754>

---

--- 106 --
Question ID: 79680950fe809ef8c16b2d6dbc028b141c8f2aa8_6
Original Code:
```
@pandas_udf("boolean", PandasUDFType.SCALAR)
def ST_Equals(left, right):
    arr_left = pa.array(left, type='string')
    arr_right = pa.array(right, type='string')
    from arctern import ST_Equals
    rs = ST_Equals(arr_left, arr_right)
    return rs.to_pandas()
```


Overlapping Code:
```
@pandas_udf("boolean", PandasUDFType.SCALAR)
def ST_Equals(left, right):
arr_left = pa.array(left, type='string')
arr_right = pa.array(right, type='string')
from arctern import ST_Equals
rs = ST_Equals(arr_left, arr_right)
return rs.to_pandas(
```
<Overlap Ratio: 0.9959016393442623>

---

--- 107 --
Question ID: a0aeaa987fec12bfa0ace9c55e26b264dab3bd52_1
Original Code:
```
def register_font_path(font_path):
    # Caching seems to cause problems. Disable for now
    ff = fontfinder.FontFinder(useCache=False)
    ff.addDirectory(font_path, recur=True)

    try:
        ff.search()
    except KeyError as ke:
        logging.warning("Problem parsing font: {}".format(ke))
    except Exception as e:
        logging.warning(e)

    for family_name in ff.getFamilyNames():
        fonts_in_family = ff.getFontsInFamily(family_name)
        for font in fonts_in_family:
            if len(fonts_in_family) == 1:
                try:
                    ttfont = TTFont(family_name.decode("utf-8"), font.fileName)
                    pdfmetrics.registerFont(ttfont)
                    pdfmetrics.registerFontFamily(family_name)
                except TTFError as e:
                    logging.warning("Could not register {}, {}".format(family_name, e))
                    continue
            elif len(fonts_in_family) > 1:
                '''If font family has multiple weights/styles'''
                font_name = family_name + "-".encode() + font.styleName
                font_name = font_name.decode("utf-8")
                try:
                    ttfont = TTFont(font_name, font.fileName)
                    pdfmetrics.registerFont(ttfont)
                    addMapping(font.familyName, font.isBold, font.isItalic, font_name)
                except TTFError as e:
                    logging.warning("Could not register {}, {}".format(family_name, e))
                    continue
```


Overlapping Code:
```
 register_font_path(font_path):
# Caching seems to cause problems. Disable for now
ff = fontfinder.FontFinder(useCache=False)
ff.addDirectory(font_path, recur=True)
try:
ff.search()
except KeyError as ke:
logging.warning("Problem parsing font: {}".format(ke))
except Exception as e:
logging.warning(e)
for family_name in ff.getFamilyNames():
fonts_in_family = ff.getFontsInFamily(family_name)
for font in fonts_in_family:
if len(fonts_in_family) == 1:
try:
ttfont = TTFont(family_name.decode("utf-8"), font.fileName)
pdfmetrics.registerFont(ttfont)
pdfmetrics.registerFontFamily(family_name)
except TTFError as e:
logging.warning("Could not register {}, {}".format(family_name, e))
continue
elif len(fonts_in_family) > 1:
'''If font family has multiple weights/styles'''
font_name = family_name + "-".encode() + font.styleName
font_name = font_name.decode("utf-8")
try:
ttfont = TTFont(font_name, font.fileName)
pdfmetrics.registerFont(ttfont)
addMapping(font.familyName, font.isBold, font.isItalic, font_name)
except TTFError as e:
logging.warning("Could not register {}, {}".format(family_name, e))
continu
```
<Overlap Ratio: 0.9964028776978417>

---

--- 108 --
Question ID: 30dbf385fbbd53298b0f80ccdc03bc0e7b9d6313_1
Original Code:
```
@contextmanager
def set_signal(sig, func):
    """Temporarily change a signal's action to the given func."""
    old_action = signal.signal(sig, func)
    try:
        yield
    finally:
        if old_action is not None:
            signal.signal(sig, old_action)
```


Overlapping Code:
```
xtmanager
def set_signal(sig, func):
"""Temporarily change a signal's action to the given func."""
old_action = signal.signal(sig, func)
try:
yield
finally:
if old_action is not None:
signal.signal(si
```
<Overlap Ratio: 0.9090909090909091>

---

--- 109 --
Question ID: 1c663fe675187ce878cfcf385a633a6b7fa0cfec_0
Original Code:
```
@pytest.mark.parametrize("length", [0, 1, 32, 33, 64, 65, 1024])
def test_len_string(get_contract, length):
    source = """
@public
def foo(a: string[1024]) -> uint256:
    return len(a)
    """
    contract = get_contract(source)

    value = "a" * length

    vyper_ast = vy_ast.parse_to_ast(f"len('{value}')")
    old_node = vyper_ast.body[0].value
    new_node = vy_fn.Len().evaluate(old_node)

    assert contract.foo(value) == new_node.value
```


Overlapping Code:
```
.mark.parametrize("length", [0, 1, 32, 33, 64, 65, 1024])
def test_len_string(get_contract, length):
source = """
@public
def foo(a: string[1024]) -> uint256:
return len(a)
"""
contract = get_contract(source)
value = "a" * length
vyper_ast = vy_ast.parse_to_ast(f"len('{value}')")
old_node = vyper_ast.body[0].value
new_node = vy_fn.Len().evaluate(old_node)
assert contract.foo(value) == n
```
<Overlap Ratio: 0.9511002444987775>

---

--- 110 --
Question ID: 6314a6d5aa14dd4c4d4b0adfdddcca55b001979b_0
Original Code:
```
def remove_nth_from_end_1(head, n):
    helper = ListNode(0)
    helper.next = head

    first = helper
    second = helper

    # count to N with the first pointer
    for i in range(n + 1):
        first = first.next

    # go (Length - N) elements with first pointer
    # and in that way the second pointer will be Nth from the end
    while first != None:
        first = first.next
        second = second.next

    # remove the element (change the next pointer from the previous element)
    second.next = second.next.next

    return helper.next
```


Overlapping Code:
```
nth_from_end_1(head, n):
helper = ListNode(0)
helper.next = head
first = helper
second = helper
# count to N with the first pointer
for i in range(n + 1):
first = first.next
# go (Length - N) elements with first pointer
# and in that way the second pointer will be Nth from the end
while first != None:
first = first.next
second = second.next
# remove the element (change the next pointer from the previous element)
second.next = second.next.next
ret
```
<Overlap Ratio: 0.9453781512605042>

---

--- 111 --
Question ID: 552e3e4a969408d74f6713246cdf1d8c919a173f_0
Original Code:
```
def gen_dummy_meta(num_spk, num_utt_per_spk):
  ''' Generate a dummy data. '''
  meta = kaldi_dir.KaldiMetaData()
  for spk_idx in range(num_spk):
    for utt_idx in range(num_utt_per_spk):
      spk = str(spk_idx)
      utt = '%s_%d' % (spk, utt_idx)
      utt_meta = kaldi_dir.Utt()
      utt_meta.feat = 'foo/bar/feat/%s' % (utt)
      utt_meta.vad = 'foo/bar/vad/%s' % (utt)
      utt_meta.spk = spk
      meta.utts[utt] = utt_meta
  meta.collect_spks_from_utts()
  return meta
```


Overlapping Code:
```
''' Generate a dummy data. '''
meta = kaldi_dir.KaldiMetaData()
for spk_idx in range(num_spk):
for utt_idx in range(num_utt_per_spk):
spk = str(spk_idx)
utt = '%s_%d' % (spk, utt_idx)
utt_meta = kaldi_dir.Utt()
utt_meta.feat = 'foo/bar/feat/%s' % (utt)
utt_meta.vad = 'foo/bar/vad/%s' % (utt)
utt_meta.spk = spk
meta.utts[utt] = utt_meta
meta.collect_spks_from_utts()
```
<Overlap Ratio: 0.8635294117647059>

---

--- 112 --
Question ID: 7382993073aecd88775af1e61bde0dac69bec96f_1
Original Code:
```
def assert_event(event_obj, expected_event_name, expected_event_args=None, expected_contract_address=None):
    contract_address, event = event_obj

    if expected_event_args is None:
        expected_event_args = {}

    if expected_contract_address:
        assert contract_address == expected_contract_address

    assert event['event'] == expected_event_name
    assert expected_event_args.items() <= event['args'].items()
```


Overlapping Code:
```
f assert_event(event_obj, expected_event_name, expected_event_args=None, expected_contract_address=None):
contract_address, event = event_obj
if expected_event_args is None:
expected_event_args = {}
if expected_contract_address:
assert contract_address == expected_contract_address
assert event['event'] == expected_event_name
assert expected_event_a
```
<Overlap Ratio: 0.9020618556701031>

---

--- 113 --
Question ID: 5f3909cc586b83372539840e1eb7519068d06e8b_0
Original Code:
```
def test_inheritance(testdir):
    testdir.makeconftest(
        """
        from easy_addoption import AddOption

        class BarAddOption(AddOption):
            bar: str
        
        class FooAddOption(BarAddOption):
            foo: str
        
        def pytest_addoption(parser):
            FooAddOption.register(parser)
        """
    )

    testdir.makepyfile(
        """
        from conftest import FooAddOption

        def test_manual_register(request):
            options = FooAddOption(request.config)

            assert options.foo == 'foo'
            assert options.bar == 'bar'
        """
    )

    result = testdir.runpytest_inprocess("--foo=foo", "--bar=bar")
    result.assert_outcomes(passed=1)
```


Overlapping Code:
```
heritance(testdir):
testdir.makeconftest(
"""
from easy_addoption import AddOption
class BarAddOption(AddOption):
bar: str

class FooAddOption(BarAddOption):
foo: str

def pytest_addoption(parser):
FooAddOption.register(parser)
"""
)
testdir.makepyfile(
"""
from conftest import FooAddOption
def test_manual_register(request):
options = FooAddOption(request.config)
assert options.foo == 'foo'
assert options.bar == 'bar'
"""
)
result = testdir.runpytest_inprocess("--foo=foo", "--bar=bar")
result.as
```
<Overlap Ratio: 0.9363295880149812>

---

--- 114 --
Question ID: 1ef274404dd1388e716e84806cab52c4e315fe83_3
Original Code:
```
def status_robo(pytradebot_id):
    """
    Metódo para retornar o estado do robô e atualizar o saldo da carteira
    :param pytradebot_id: Parâmetro de id do robô com o id do usuário
    :return: Retorna um dicionário contendo o estado e a ação
    """
    resultado = {
        "estado": "Offline",
        "acao": "iniciar"
    }
    # Chama o método para executar o comando do robô para verificar se está online
    process = comando_robo(pytradebot_id=pytradebot_id, comando='online')
    # Se o robô estiver sendo executado o processo retorna código 0
    if process['code'] == 0:
        # Remove os caracteres desnecessários
        convert_output = str(process['out']).lstrip('(b"[')
        convert_output2 = convert_output.rstrip(f']\\n"')
        # Converte para uma tuple
        json_tuple = ast.literal_eval(convert_output2)
        # Verificar o estado do robô
        if json_tuple['status'] == 'stopped':
            resultado['estado'] = 'Parado'
            resultado['acao'] = 'retomar'
        elif json_tuple['status'] == 'running':
            resultado['estado'] = 'Operando'
            resultado['acao'] = 'parar'
        elif json_tuple['status'] == 'running-no-buy':
            resultado['estado'] = 'Operando (Sem Comprar)'
            resultado['acao'] = 'parar'

    user_model = get_user_model()
    usuario = user_model.objects.get(pk=pytradebot_id)
    # Se o usuário tiver carteira cadastrada
    if CarteiraCriptomoeda.objects.filter(usuario=usuario).exists():
        # Verifica se o robô está operando
        if resultado['estado'] == 'Operando (Sem Comprar)' or resultado['estado'] == 'Operando':
            carteira = CarteiraCriptomoeda.objects.get(usuario=usuario)
            # Executa um subprocesso com o comando para retornar o saldo da carteira
            # com base do saldo na Exchange
            process = comando_robo(pytradebot_id=pytradebot_id, comando='balance')
            # Se o processo retornar código 0 (sucesso)
            if process['code'] == 0:
                # Remove os caracteres desnecessários
                convert_output = str(process['out']).lstrip('(b"[')
                convert_output2 = convert_output.rstrip(f']\\n"')
                # Converte para uma tuple
                json_tuple = ast.literal_eval(convert_output2)
                # Converte a tuple para String, trocando aspas simples para aspas duplas
                # e depois converte a String para JSON
                json_string = json.loads(str(json_tuple).replace("\'", "\""))
                # Pega o saldo total equivalente em Bitcoin e
                # converte para float com 2 casas decimais
                temp_btc = float(json_string['total'])
                saldo_btc = format(temp_btc, '.8f')
                carteira.saldo = saldo_btc
                carteira.save()

    return resultado
```


Overlapping Code:
```
radebot_id):
"""
Metódo para retornar o estado do robô e atualizar o saldo da carteira
:param pytradebot_id: Parâmetro de id do robô com o id do usuário
:return: Retorna um dicionário contendo o estado e a ação
"""
resultado = {
"estado": "Offline",
"acao": "iniciar"
}
# Chama o método para executar o comando do robô para verificar se está online
process = comando_robo(pytradebot_id=pytradebot_id, comando='online')
# Se o robô estiver sendo executado o processo retorna código 0
if process['code'] == 0:
# Remove os caracteres desnecessários
convert_output = str(process['out']).lstrip('(b"[')
convert_output2 = convert_output.rstrip(f']\\n"')
# Converte para uma tuple
json_tuple = ast.literal_eval(convert_output2)
# Verificar o estado do robô
if json_tuple['status'] == 'stopped':
resultado['estado'] = 'Parado'
resultado['acao'] = 'retomar'
elif json_tuple['status'] == 'running':
resultado['estado'] = 'Operando'
resultado['acao'] = 'parar'
elif json_tuple['status'] == 'running-no-buy':
resultado['estado'] = 'Operando (Sem Comprar)'
resultado['acao'] = 'parar'
user_model = get_user_model()
usuario = user_model.objects.get(pk=pytradebot_id)
# Se o usuário tiver carteira cadastrada
if CarteiraCriptomoeda.objects.filter(usuario=usuario).exists():
# Verifica se o robô está operando
if resultado['estado'] == 'Operando (Sem Comprar)' or resultado['estado'] == 'Operando':
carteira = CarteiraCriptomoeda.objects.get(usuario=usuario)
# Executa um subprocesso com o comando para retornar o saldo da carteira
# com base do saldo na Exchange
process = comando_robo(pytradebot_id=pytradebot_id, comando='balance')
# Se o processo retornar código 0 (sucesso)
if process['code'] == 0:
# Remove os caracteres desnecessários
convert_output = str(process['out']).lstrip('(b"[')
convert_output2 = convert_output.rstrip(f']\\n"')
# Converte para uma tuple
json_tuple = ast.literal_eval(convert_output2)
# Converte a tuple para String, trocando aspas simples para aspas duplas
# e depois converte a Stri
```
<Overlap Ratio: 0.9699321047526673>

---

--- 115 --
Question ID: 8da77b89f2d73f87d82c9499b1d3cd7508e9c4c5_0
Original Code:
```
def read(*paths, lines=False):
    """
    Build a file path from *paths and return the contents.

    Parameters:
        lines - if True, return a list of lines. Defaults to
            False (send back raw text).
    """
    with open(os.path.join(*paths), 'r') as src:
        return src.readlines() if lines else src.read()
```


Overlapping Code:
```
read(*paths, lines=False):
"""
Build a file path from *paths and return the contents.
Parameters:
lines - if True, return a list of lines. Defaults to
False (send back raw text).
"""
with open(os.path.join(*paths), 'r') as src:
return src.readlines()
```
<Overlap Ratio: 0.8960573476702509>

---

--- 116 --
Question ID: a3fd7eb98d5465d81ba621722bcd4628ee14d300_3
Original Code:
```
def DecodeToPoint3d(item):
    if item is None:
        return None
    if isinstance(item, list):
        return [DecodeToPoint3d(x) for x in item]
    return rhino3dm.Point3d(item['X'], item['Y'], item['Z'])
```


Overlapping Code:
```
):
if item is None:
return None
if isinstance(item, list):
return [DecodeToPoint3d(x) for x in item]
return rhino3dm.Point3d(item['X'], item['Y'], item['
```
<Overlap Ratio: 0.8453038674033149>

---

--- 117 --
Question ID: ea0c4d79e0cb53326aa689828b7f860f052970e3_1
Original Code:
```
@db_api.context_manager.reader
def _vnf_package_get_by_id(context, package_uuid, columns_to_join=None):

    query = api.model_query(context, models.VnfPackage,
                            read_deleted="no", project_only=True). \
        filter_by(id=package_uuid).options(joinedload('_metadata'))

    if columns_to_join:
        for column in columns_to_join:
            query = query.options(joinedload(column))

    result = query.first()

    if not result:
        raise exceptions.VnfPackageNotFound(id=package_uuid)

    return result
```


Overlapping Code:
```
pi.context_manager.reader
def _vnf_package_get_by_id(context, package_uuid, columns_to_join=None):
query = api.model_query(context, models.VnfPackage,
read_deleted="no", project_only=True). \
filter_by(id=package_uuid).options(joinedload('_metadata'))
if columns_to_join:
for column in columns_to_join:
query = query.options(joinedload(column))
result = query.first()
if not result:
raise exceptions.Vn
```
<Overlap Ratio: 0.8854625550660793>

---

--- 118 --
Question ID: d03f776dd9f6581f454d5320aed6625b9c6770c0_1
Original Code:
```
@argh.arg("--join-table", "-j", type=str, required=True)
@argh.arg("--keys", "-k", nargs="+", type=str, required=True)
def main(join_table=None, keys=None):
    with open(join_table, "r") as f:
        reader = csv.reader(f)
        join_columns = reader.next()
        join_selector = make_selector(keys, join_columns)

        join_records = dict()

        for record in reader:
            key = join_selector(record)
            join_records.setdefault(key, tuple())
            join_records[key] += (record,)

    reader = csv.reader(sys.stdin)
    input_columns = reader.next()
    input_selector = make_selector(keys, input_columns)

    columns = (input_columns
               + [column for column in join_columns
                         if column not in keys])

    writer = csv.writer(sys.stdout)
    writer.writerow(columns)

    for record in reader:
        key = input_selector(record)

        if key not in join_records:
            writer.writerow(
                record
                + [None] * (len(columns) - len(input_columns)))
        else:
            for join_record in join_records.get(key):
                writer.writerow(
                    record
                    + [join_record[i] for i, column in enumerate(join_columns)
                                      if column not in keys])
```


Overlapping Code:
```
str, required=True)
@argh.arg("--keys", "-k", nargs="+", type=str, required=True)
def main(join_table=None, keys=None):
with open(join_table, "r") as f:
reader = csv.reader(f)
join_columns = reader.next()
join_selector = make_selector(keys, join_columns)
join_records = dict()
for record in reader:
key = join_selector(record)
join_records.setdefault(key, tuple())
join_records[key] += (record,)
reader = csv.reader(sys.stdin)
input_columns = reader.next()
input_selector = make_selector(keys, input_columns)
columns = (input_columns
+ [column for column in join_columns
if column not in keys])
writer = csv.writer(sys.stdout)
writer.writerow(columns)
for record in reader:
key = input_selector(record)
if key not in join_records:
writer.writerow(
record
+ [None] * (len(columns) - len(input_columns)))
else:
for join_record in join_records.get(key):
writer.writerow(
record
+ [join_record[i] for i, column in enumerate(join_columns)
if column not in
```
<Overlap Ratio: 0.9557344064386318>

---

--- 119 --
Question ID: 5f3c10937424ed8391eebd000884277a2595ae8c_0
Original Code:
```
def download(url, filename):

    if not os.path.exists(mnist_path):
        os.makedirs(mnist_path)
    out_file = os.path.join(mnist_path, filename)
    if not os.path.isfile(out_file):
        urlretrieve(url, out_file)
```


Overlapping Code:
```
ef download(url, filename):
if not os.path.exists(mnist_path):
os.makedirs(mnist_path)
out_file = os.path.join(mnist_path, filename)
if not os.path.isfile(out_file):
urlretrieve(url, o
```
<Overlap Ratio: 0.9533678756476683>

---

--- 120 --
Question ID: 8a2a6300d8cbd4857fddcb78bbb2c8e343a8dc20_0
Original Code:
```
def _check_tensorflow_computation(label, comp):
  py_typecheck.check_type(comp, computation_base.Computation, label)
  comp_proto = computation_impl.ComputationImpl.get_proto(comp)
  which_comp = comp_proto.WhichOneof('computation')
  if which_comp != 'tensorflow':
    raise TypeError('Expected all computations supplied as arguments to '
                    'be plain TensorFlow, found {}.'.format(which_comp))
```


Overlapping Code:
```
_tensorflow_computation(label, comp):
py_typecheck.check_type(comp, computation_base.Computation, label)
comp_proto = computation_impl.ComputationImpl.get_proto(comp)
which_comp = comp_proto.WhichOneof('computation')
if which_comp != 'tensorflow':
raise TypeError('Expected all computations supplied as arguments to '
'be plain TensorFlow, found {}.'.format(whic
```
<Overlap Ratio: 0.9526315789473684>

---

--- 121 --
Question ID: afba808c2a3b5e03497dcb564c6ed1b603da8646_0
Original Code:
```
def main():
    args = parser.parse_args()

    # Fetch the image glob
    image_filenames = sorted(glob.glob(args.image_glob))

    # Parse original info from the experiment root and add new ones.
    args_file = os.path.join(args.experiment_root, 'args.json')
    if not os.path.isfile(args_file):
        raise IOError('`args.json` not found in {}'.format(args_file))
    print('Loading args from {}.'.format(args_file))
    with open(args_file, 'r') as f:
        args_resumed = json.load(f)
    for key, value in args_resumed.items():
        if key not in args.__dict__:
            args.__dict__[key] = value

    # In case no dataset config was specified, we need to fix the argument here
    # since there will be a list of configs.
    if args.dataset_config is None:
        args.dataset_config = args_resumed['dataset_config'][0]

    # Load the config for the dataset.
    with open(args.dataset_config, 'r') as f:
        dataset_config = json.load(f)

    # Compute the label to color map
    id_to_rgb = np.asarray(
        dataset_config['rgb_colors'] + [(0, 0, 0)], dtype=np.uint8)[:,::-1]

    # Setup the input
    image_file_tensor = tf.data.Dataset.from_tensor_slices(image_filenames)

    dataset = image_file_tensor.map(
        lambda fn: tf.image.decode_png(tf.read_file(fn), channels=3))

    dataset = dataset.map(lambda x: ((tf.to_float(x) - 128.0) / 128.0))

    dataset = tf.data.Dataset.zip((dataset, image_file_tensor))

    dataset = dataset.batch(1)

    dataset = dataset.prefetch(1)

    image_input, image_filename = dataset.make_one_shot_iterator().get_next()

    if args.rescale_h is not None or args.rescale_w is not None:
        if args.rescale_h is not None and args.rescale_w is not None:
            image_input_resized = tf.image.resize_images(
                image_input, (args.rescale_h, args.rescale_w))
        else:
            raise ValueError('Either both rescale_h and rescale_w should be '
                             'left undefined or both should be set. Got {} and '
                             '{}'.format(args.rescale_h, args.rescale_w))
    else:
        image_input_resized = image_input

    # Determine the checkpoint location.
    if args.checkpoint_iteration == -1:
        # The default TF way to do this fails when moving folders.
        checkpoint = os.path.join(
            args.experiment_root,
            'checkpoint-{}'.format(args.train_iterations))
    else:
        checkpoint = os.path.join(
            args.experiment_root,
            'checkpoint-{}'.format(args.checkpoint_iteration))
    iteration = int(checkpoint.split('-')[-1])
    print('Restoring from checkpoint: {}'.format(checkpoint))

    # Check if the checkpoint contains a specifically named output_conv. This is
    # needed for models trained with the older single dataset code.
    reader = tf.train.NewCheckpointReader(checkpoint)
    var_to_shape_map = reader.get_variable_to_shape_map()
    output_conv_name = 'output_conv_{}'.format(
        dataset_config.get('dataset_name'))
    output_conv_name_found = False

    for k in var_to_shape_map.keys():
        output_conv_name_found = output_conv_name in k
        if output_conv_name_found:
            break

    if not output_conv_name_found:
        print('Warning: An output for the specific dataset could not be found '
              'in the checkpoint. This likely means it\'s an old checkpoint. '
              'Revertig to the old default output name. This could cause '
              'issues if the dataset class count and output classes of the '
              'network do not match.')
        output_conv_name = 'output_conv'

    # Setup the network for simple forward passing.
    model = import_module('networks.' + args.model_type)
    with tf.name_scope('model'):
        net = model.network(image_input_resized, is_training=False,
            **args.model_params)
        logits = slim.conv2d(net, len(dataset_config['class_names']),
            [3,3], scope=output_conv_name, activation_fn=None,
            weights_initializer=slim.variance_scaling_initializer(),
            biases_initializer=tf.zeros_initializer())
        predictions = tf.nn.softmax(logits)

        predictions_full = tf.image.resize_images(
            predictions, tf.shape(image_input)[1:3])

    with tf.Session() as sess:
        checkpoint_loader = tf.train.Saver()
        checkpoint_loader.restore(sess, checkpoint)

        # Loop over all images
        timings = []
        print()
        while True:
            try:
                start = time.time()
                preds, fn = sess.run([predictions_full, image_filename])
                timings.append(time.time() - start)
                pred_class = np.argmax(preds[0], -1)
                pred_out = id_to_rgb[pred_class]
                if len(timings) > 1:
                    print('Time for loading, resizing and forwarding per frame:'
                          ' {:7.4f}s±{:7.4f}s'.format(
                                np.mean(timings[-100:]),
                                np.std(timings[-100:])),
                          end='\r')
                in_filename = fn[0].decode("utf-8")
                base_dir = os.path.dirname(in_filename)
                out_filename = in_filename.replace(
                    base_dir, args.result_directory)
                extension = os.path.splitext(out_filename)[1]
                out_filename = out_filename.replace(extension, '.png')
                if not os.path.isdir(args.result_directory):
                    os.makedirs(args.result_directory)
                cv2.imwrite(out_filename, pred_out)

            except tf.errors.OutOfRangeError:
                # Done!
                break

    # For the timings we skip the first frame since this is where Tensorflow
    # hides the compilation time.
    if len(timings) > 1:
        timings = timings[-100:]
        print('Time for loading, resizing and forwarding per frame: '
              '{:7.4f}s±{:7.4f}s'.format(np.mean(timings), np.std(timings)))
    else:
        print('Loading and forwarding took {:7.4f}s. '
              'This includes compilation'.format(timings[0]))
```


Overlapping Code:
```
the image glob
image_filenames = sorted(glob.glob(args.image_glob))
# Parse original info from the experiment root and add new ones.
args_file = os.path.join(args.experiment_root, 'args.json')
if not os.path.isfile(args_file):
raise IOError('`args.json` not found in {}'.format(args_file))
print('Loading args from {}.'.format(args_file))
with open(args_file, 'r') as f:
args_resumed = json.load(f)
for key, value in args_resumed.items():
if key not in args.__dict__:
args.__dict__[key] = value
# In case no dataset config was specified, we need to fix the argument here
# since there will be a list of configs.
if args.dataset_config is None:
args.dataset_config = args_resumed['dataset_config'][0]
# Load the config for the dataset.
with open(args.dataset_config, 'r') as f:
dataset_config = json.load(f)
# Compute the label to color map
id_to_rgb = np.asarray(
dataset_config['rgb_colors'] + [(0, 0, 0)], dtype=np.uint8)[:,::-1]
# Setup the input
image_file_tensor = tf.data.Dataset.from_tensor_slices(image_filenames)
dataset = image_file_tensor.map(
lambda fn: tf.image.decode_png(tf.read_file(fn), channels=3))
dataset = dataset.map(lambda x: ((tf.to_float(x) - 128.0) / 128.0))
dataset = tf.data.Dataset.zip((dataset, image_file_tensor))
dataset = dataset.batch(1)
dataset = dataset.prefetch(1)
image_input, image_filename = dataset.make_one_shot_iterator().get_next()
if args.rescale_h is not None or args.rescale_w is not None:
if args.rescale_h is not None and args.rescale_w is not None:
image_input_resized = tf.image.resize_images(
image_input, (args.rescale_h, args.rescale_w))
else:
raise ValueError('Either both rescale_h and rescale_w should be '
'left undefined or both should be set. Got {} and '
'{}'.format(args.rescale_h, args.rescale_w))
else:
image_input_resized = image_input
# Determine the checkpoint location.
if args.checkpoint_iteration == -1:
# The default TF way to do this fails when moving folders.
checkpoint = os.path.join(
args.experiment_root,
'checkpoint-{}'.format(args.train_iterations))
else:
checkpoint = os.pat
```
<Overlap Ratio: 0.9693251533742331>

---

--- 122 --
Question ID: 1161b1c262be58ef58ec2227e5c5e31125e1c5cc_0
Original Code:
```
@vcr.use_cassette
def test_instruments_get_candles_with_instrument_only(cli_oandapy):
    instrument = 'EUR_USD'
    result = cli_oandapy.instrument.get_candles(instrument).as_dict()
    assert 'candles' in result
    assert result['instrument'] == instrument
```


Overlapping Code:
```
vcr.use_cassette
def test_instruments_get_candles_with_instrument_only(cli_oandapy):
instrument = 'EUR_USD'
result = cli_oandapy.instrument.get_candles(instrument).as_dict()
assert 'candles' in result
assert result['instrument'] == instrum
```
<Overlap Ratio: 0.9835390946502057>

---

--- 123 --
Question ID: f75e85b6be66063f5f251a3924187e4cef4564ab_11
Original Code:
```
def createVirtualEnv():
    print(f"{Text.HEADER}*** CREATING VIRTUAL ENVIRONMENT ***{Text.ENDC}")
    os.chdir('PiirBlaster')
    cmdResult = execCommand(Commands.CREATE_VRITUAL_ENV)
    if cmdResult != 0:
        print(f"{Text.FAIL}CREATING VIRTUAL ENVIRONEMENT FAILED!!!{Text.ENDC}")
        return False
    print(f"{Text.SUCCESS}CREATING VIRTUAL ENVIRONMENT DONE{Text.ENDC}")
    return True
```


Overlapping Code:
```
t.HEADER}*** CREATING VIRTUAL ENVIRONMENT ***{Text.ENDC}")
os.chdir('PiirBlaster')
cmdResult = execCommand(Commands.CREATE_VRITUAL_ENV)
if cmdResult != 0:
print(f"{Text.FAIL}CREATING VIRTUAL ENVIRONEMENT FAILED!!!{Text.ENDC}")
return False
print(f"{Text.SUCCESS}CREATING VIRTUAL ENVIRONMENT DONE{Text
```
<Overlap Ratio: 0.8426966292134831>

---

--- 124 --
Question ID: b3ad4a325b4fd5bbe6f377742f9f8762fe907760_20
Original Code:
```
def _create_embedded_row(value, out_row, out_col_ndx, config, group_values):
    """
    Copies the given output row and assigns the given pattern match
    group values to the corresponding output row columns defined by
    the configuration.

    :return: the new output row
    """
    # Make a new output row.
    row = out_row.copy()
    # Set the embedded column.
    row[out_col_ndx] = value
    # Assign the match group values to the associated output column.
    for grp_col, grp_value in group_values.items():
        # Map the output columns to output values.
        value_dict = config.values_dict.get(grp_col)
        value = value_dict.get(grp_value, grp_value) if value_dict else grp_value
        col_ndx = config.out_col_ndx_map[grp_col]
        row[col_ndx] = value

    return row
```


Overlapping Code:
```
col_ndx, config, group_values):
"""
Copies the given output row and assigns the given pattern match
group values to the corresponding output row columns defined by
the configuration.
:return: the new output row
"""
# Make a new output row.
row = out_row.copy()
# Set the embedded column.
row[out_col_ndx] = value
# Assign the match group values to the associated output column.
for grp_col, grp_value in group_values.items():
# Map the output columns to output values.
value_dict = config.values_dict.get(grp_col)
value = value_dict.get(grp_value, grp_value) if value_dict else grp_value
col_ndx = config.out_col_ndx_map[grp_col]
row[col_ndx] = value
```
<Overlap Ratio: 0.9206798866855525>

---

--- 125 --
Question ID: 9dee20267c3c857c4a320728f256f9dcabc4acdb_3
Original Code:
```
def deconv2d(input_, output_shape,
             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,
             name="deconv2d", with_w=False):
    with tf.variable_scope(name):
        # filter : [height, width, output_channels, in_channels]
        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],
                            initializer=tf.random_normal_initializer(stddev=stddev))

        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,
                                        strides=[1, d_h, d_w, 1])

        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))
        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())

        if with_w:
            return deconv, w, biases
        else:
            return deconv
```


Overlapping Code:
```
def deconv2d(input_, output_shape,
k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,
name="deconv2d", with_w=False):
with tf.variable_scope(name):
# filter : [height, width, output_channels, in_channels]
w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],
initializer=tf.random_normal_initializer(stddev=stddev))
deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,
strides=[1, d_h, d_w, 1])
biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))
deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())
if with_w:
return deconv, w, biases
else:
return deconv
```
<Overlap Ratio: 1.0>

---

--- 126 --
Question ID: d826704b14ad9179eee070a894057980bda56405_0
Original Code:
```
def main(argv):
    # buffer hack
    sys.stdout = Unbuffered(sys.stdout)

    path = 'local/config.yml'

    if len(argv) >= 1:
        path = argv[0]

    try:
        cc.loadConfig(path)
    except IOError as ioe:
        print("{0}".format(ioe))

    for experiment in cc.cfg['experiments']:
        # load configuration into global var and run it
        cc.loadExperiment(experiment)


        if cc.cfg['model'] == 'rnn':

            if cc.cfg['grid']:

                # REMOVEME: hardcoded grid for ratio
                grid = [
                    [1,1,1],
                    # [1,1,2],
                    # [1,2,1],
                    # [2,1,1],
                    # [1,2,2],
                    # [2,1,2],
                    [2,2,1]
                ]
                cc.exp['grid'] = {}

                import rnn.rnn
                reload(rnn.rnn)

                comment = '[GRID][RATIO={}][A549][TDGRUGRU] performing grid search for ratio'
                for ratios in grid:
                    cc.exp['params']['rnn']['comment'] = comment.format(':'.join([str(x) for x in ratios]))
                    cc.exp['grid']['ratios'] = ratios

                    print(cc.cfg,cc.exp)
                    rnn.rnn.run()
            else:
                import rnn.rnn
                reload(rnn.rnn)

                print(cc.cfg,cc.exp)
                rnn.rnn.run()

        elif cc.cfg['model'] == 'dnn':
            import rnn.dnn
            reload(rnn.dnn)

            print(cc.cfg,cc.exp)
            rnn.dnn.run()
        else:
            raise Exception('Run: unknown model')
```


Overlapping Code:
```
ck
sys.stdout = Unbuffered(sys.stdout)
path = 'local/config.yml'
if len(argv) >= 1:
path = argv[0]
try:
cc.loadConfig(path)
except IOError as ioe:
print("{0}".format(ioe))
for experiment in cc.cfg['experiments']:
# load configuration into global var and run it
cc.loadExperiment(experiment)
if cc.cfg['model'] == 'rnn':
if cc.cfg['grid']:
# REMOVEME: hardcoded grid for ratio
grid = [
[1,1,1],
# [1,1,2],
# [1,2,1],
# [2,1,1],
# [1,2,2],
# [2,1,2],
[2,2,1]
]
cc.exp['grid'] = {}
import rnn.rnn
reload(rnn.rnn)
comment = '[GRID][RATIO={}][A549][TDGRUGRU] performing grid search for ratio'
for ratios in grid:
cc.exp['params']['rnn']['comment'] = comment.format(':'.join([str(x) for x in ratios]))
cc.exp['grid']['ratios'] = ratios
print(cc.cfg,cc.exp)
rnn.rnn.run()
else:
import rnn.rnn
reload(rnn.rnn)
print(cc.cfg,cc.exp)
rnn.rnn.run()
elif cc.cfg['model'] == 'dnn':
import rnn.dnn
reload(rnn.dnn)
print(cc.cfg,cc.exp)
rnn.dnn.run()
else:
raise Exception('Run: unknown mo
```
<Overlap Ratio: 0.9681274900398407>

---

--- 127 --
Question ID: 6c9e8429a08fe7cae9883a160beddb9681ecf991_0
Original Code:
```
def get_specs(logger, conf, directory, specification_kinds):
    """
    Get specification kinds descriptions and parse all JSON files separating them on the base of markets in
    specification kinds.

    :param logger:
    :param conf:
    :param directory:
    :param specification_kinds:
    :return:
    """
    logger.info('Search for various EMG generators specifications in {}'.format(directory))
    # Find all json files
    file_candidates = set()
    for root, dirs, files in os.walk(directory):
        # Check only full paths to files
        json_files = glob.glob('{}/*.json'.format(root))
        file_candidates.update(json_files)

    # Filter specifications
    for file in file_candidates:
        with open(file, encoding="utf8") as fh:
            try:
                content = ujson.loads(fh.read())
            except ValueError:
                raise ValueError("Cannot parse EMG specification file {!r}".format(os.path.abspath(file)))

        if isinstance(content, dict):
            __check_file(logger, file, content, specification_kinds)

    # Merge specifications
    for kind in specification_kinds:
        spec = __merge_spec_versions(specification_kinds[kind]['specification'],
                                     get_necessary_conf_property(conf, 'specifications set'))
        specification_kinds[kind]['specification'] = spec
        __save_collection(logger, spec, '{} spec.json'.format(kind))
    return specification_kinds
```


Overlapping Code:
```
onf, directory, specification_kinds):
"""
Get specification kinds descriptions and parse all JSON files separating them on the base of markets in
specification kinds.
:param logger:
:param conf:
:param directory:
:param specification_kinds:
:return:
"""
logger.info('Search for various EMG generators specifications in {}'.format(directory))
# Find all json files
file_candidates = set()
for root, dirs, files in os.walk(directory):
# Check only full paths to files
json_files = glob.glob('{}/*.json'.format(root))
file_candidates.update(json_files)
# Filter specifications
for file in file_candidates:
with open(file, encoding="utf8") as fh:
try:
content = ujson.loads(fh.read())
except ValueError:
raise ValueError("Cannot parse EMG specification file {!r}".format(os.path.abspath(file)))
if isinstance(content, dict):
__check_file(logger, file, content, specification_kinds)
# Merge specifications
for kind in specification_kinds:
spec = __merge_spec_versions(specification_kinds[kind]['specification'],
get_necessary_conf_property(conf, 'specifications set'))
specification_kinds[kind]['specification'] = spec
__save_collection(logger, spec, '{} spec.json'.format(kind))
return specification_kind
```
<Overlap Ratio: 0.9803921568627451>

---

--- 128 --
Question ID: d677a88d7854c4b095ce2ed9a3917c1b9276b15f_2
Original Code:
```
def _get_help_record(opt):
    """Re-implementation of click.Opt.get_help_record.
    The variant of 'get_help_record' found in Click makes uses of slashes to
    separate multiple opts, and formats option arguments using upper case. This
    is not compatible with Sphinx's 'option' directive, which expects
    comma-separated opts and option arguments surrounded by angle brackets [1].
    [1] http://www.sphinx-doc.org/en/stable/domains.html#directive-option
    """

    def _write_opts(opts):
        rv, _ = click.formatting.join_options(opts)
        if not opt.is_flag and not opt.count:
            rv += ' <{}>'.format(opt.name)
        return rv

    rv = [_write_opts(opt.opts)]
    if opt.secondary_opts:
        rv.append(_write_opts(opt.secondary_opts))

    help = opt.help or ''
    extra = []
    if opt.default is not None and opt.show_default:
        extra.append(
            'default: %s' % (', '.join('%s' % d for d in opt.default)
                             if isinstance(opt.default,
                                           (list, tuple)) else opt.default, ))
    if opt.required:
        extra.append('required')
    if extra:
        help = '%s[%s]' % (help and help + '  ' or '', '; '.join(extra))

    return ', '.join(rv), help
```


Overlapping Code:
```
d(opt):
"""Re-implementation of click.Opt.get_help_record.
The variant of 'get_help_record' found in Click makes uses of slashes to
separate multiple opts, and formats option arguments using upper case. This
is not compatible with Sphinx's 'option' directive, which expects
comma-separated opts and option arguments surrounded by angle brackets [1].
[1] http://www.sphinx-doc.org/en/stable/domains.html#directive-option
"""
def _write_opts(opts):
rv, _ = click.formatting.join_options(opts)
if not opt.is_flag and not opt.count:
rv += ' <{}>'.format(opt.name)
return rv
rv = [_write_opts(opt.opts)]
if opt.secondary_opts:
rv.append(_write_opts(opt.secondary_opts))
help = opt.help or ''
extra = []
if opt.default is not None and opt.show_default:
extra.append(
'default: %s' % (', '.join('%s' % d for d in opt.default)
if isinstance(opt.default,
(list, tuple)) else opt.default, ))
if opt.required:
extra.append('required')
if extra:
help = '%s[%s]' % (help and help + ' ' or '', '; '.join(extra))
ret
```
<Overlap Ratio: 0.959731543624161>

---

--- 129 --
Question ID: 7156199fb832e7d8f6893c42b29e477a2c978f37_34
Original Code:
```
@app.route('/adapters/<service_platform>/instantiations', methods=['POST'])
def serviceInstantiation(service_platform):
    content = request.get_json()
    LOG.debug("service_uuid : "+content['service_uuid'])
    service_uuid = content['service_uuid']
    instantiate_str = "{\"service_uuid\": \"" + service_uuid + "\"}" 
    ad = adapter.Adapter(service_platform)      
    return ad.instantiation(instantiate_str)
```


Overlapping Code:
```
@app.route('/adapters/<service_platform>/instantiations', methods=['POST'])
def serviceInstantiation(service_platform):
content = request.get_json()
LOG.debug("service_uuid : "+content['service_uuid'])
service_uuid = content['service_uuid']
instantiate_str = "{\"service_uuid\": \"" + service_uuid + "\"}" 
ad = adapter.Adapter(service_platform
```
<Overlap Ratio: 0.8888888888888888>

---

--- 130 --
Question ID: 6a30ab056fd3fb09f9cb647a13e0d54d034c13fc_0
Original Code:
```
def parse_args():
    parser = optparse.OptionParser()
    parser.disable_interspersed_args()
    parser.add_option('--sources-list')
    parser.add_option('--verbose', default=False, action='store_true')
    parser.add_option('--remove-notes', default=False, action='store_true')
    parser.add_option('--ignore-errors', default=False, action='store_true')
    return parser.parse_args()
```


Overlapping Code:
```
def parse_args():
parser = optparse.OptionParser()
parser.disable_interspersed_args()
parser.add_option('--sources-list')
parser.add_option('--verbose', default=False, action='store_true')
parser.add_option('--remove-notes', default=False, action='store_true')
parser.add_option('--ignore-errors', default=False, action='store_true')
return parser.parse_args()
```
<Overlap Ratio: 1.0>

---

--- 131 --
Question ID: 4264c8ca3040630f755480554f4cbc48f89bf3c4_5
Original Code:
```
def make_train_loader(cifar_img_dim, shuffle=10000, batch_size=FLAGS.batch_size):
    num_dataset_instances = xm.xrt_world_size() * FLAGS.num_workers
    epoch_size = trainsize // num_dataset_instances

    image_transform = transforms.Compose(
        [
            transforms.RandomCrop(cifar_img_dim, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ]
    )
    
    dataset = (
        wds.WebDataset(FLAGS.wds_traindir,
                       splitter=my_worker_splitter,
                       nodesplitter=my_node_splitter,
                       shardshuffle=True, length=epoch_size)
        .shuffle(shuffle)
        .decode("pil")
        .to_tuple("ppm;jpg;jpeg;png", "cls")
        .map_tuple(image_transform, identity)
        .batched(batch_size, partial=True)
        )

    loader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, drop_last=False, num_workers=FLAGS.num_workers) # , worker_init_fn=worker_init_fn
    return loader
```


Overlapping Code:
```
0, batch_size=FLAGS.batch_size):
num_dataset_instances = xm.xrt_world_size() * FLAGS.num_workers
epoch_size = trainsize // num_dataset_instances
image_transform = transforms.Compose(
[
transforms.RandomCrop(cifar_img_dim, padding=4),
transforms.RandomHorizontalFlip(),
transforms.ToTensor(),
normalize,
]
)

dataset = (
wds.WebDataset(FLAGS.wds_traindir,
splitter=my_worker_splitter,
nodesplitter=my_node_splitter,
shardshuffle=True, length=epoch_size)
.shuffle(shuffle)
.decode("pil")
.to_tuple("ppm;jpg;jpeg;png", "cls")
.map_tuple(image_transform, identity)
.batched(batch_size, partial=True)
)
loader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, drop_last=False, num_workers=FLAGS.num_workers) # , worker_init_fn=worker_
```
<Overlap Ratio: 0.9146341463414634>

---

--- 132 --
Question ID: 02ded602e60b99c611e797ccb5c9cae95556d9be_0
Original Code:
```
def get_coord(place):
    """
    Returns the coordinates from the POI json
    """
    coord = place.get_coord()
    if coord:
        lon = coord.get("lon")
        lat = coord.get("lat")
        return (lat, lon)
    return None
```


Overlapping Code:
```
d(place):
"""
Returns the coordinates from the POI json
"""
coord = place.get_coord()
if coord:
lon = coord.get("lon")
lat = coord.get("lat")
return (
```
<Overlap Ratio: 0.819672131147541>

---

--- 133 --
Question ID: 79347a15010591124840a4abc3e1eface25a91c5_1
Original Code:
```
def make_command(list, module_index, routine_count=-1, make_file=False):
    if routine_count > 0:  # routine 의 경우
        commands = []
        for item in list:
            data = item.split(", ")
            command_type = data[0]
            if module_index == 0:  # Zigbee HA
                if command_type == "connect":
                    print("connect")
                elif command_type == "on/off":
                    value = data[1]
                    if value == "on" or value == "0x01":  # on
                        commands.append("resource\\command\\Zigbee\\on.json")
                    elif value == "off" or value == "0x00":  # off
                        commands.append("resource\\command\\Zigbee\\off.json")
                    elif value == "toggle":
                        commands.append("resource\\command\\Zigbee\\toggle.json")
                    elif value == "regular random":
                        commands.append("resource\\command\\Zigbee\\onoff_random.json")
                    elif value == "irregular random":
                        commands.append("resource\\command\\Zigbee\\onoff_random.json")
                    else:
                        commands.append("resource\\command\\Zigbee\\onoff_random.json")
                elif command_type == "color":
                    value = data[1]
                    if value == "regular random":
                        commands.append("resource\\command\\Zigbee\\color_random.json")
                    elif value == "irregular random":
                        commands.append(
                            "resource\\command\\Zigbee\\color_random.json")
                    elif value == "random":
                        commands.append("resource\\command\\Zigbee\\color_random.json")
                    elif value == "cw":
                        commands.append("resource\\command\\Zigbee\\color_cw.json")
                    elif value == "dl":
                        commands.append("resource\\command\\Zigbee\\color_dl.json")
                    elif value == "nw":
                        commands.append("resource\\command\\Zigbee\\color_nw.json")
                    elif value == "sw":
                        commands.append("resource\\command\\Zigbee\\color_sw.json")
                    elif value == "ww":
                        commands.append("resource\\command\\Zigbee\\color_ww.json")
                elif command_type == "level":
                    value = data[1]
                    if value == "regular random":
                        commands.append("resource\\command\\Zigbee\\level_random.json")
                    elif value == "irregular random":
                        commands.append(
                            "resource\\command\\Zigbee\\level_random.json")
                    elif value == "random":
                        commands.append("resource\\command\\Zigbee\\level_random.json")
                    elif value == "10":
                        commands.append("resource\\command\\Zigbee\\level_10.json")
                    elif value == "50":
                        commands.append("resource\\command\\Zigbee\\level_50.json")
                    elif value == "100":
                        commands.append("resource\\command\\Zigbee\\level_100.json")
                elif command_type == "disconnect":
                    print("disconnect")
        file_data = OrderedDict()
        file_data["task_list"] = commands
        file_data["iteration"] = routine_count
        if make_file:
            with open('command_routine.json', 'w', encoding='utf-8') as make_file:
                json.dump(file_data, make_file, ensure_ascii=False, indent="\t")
        return file_data
    else:  # single command 와 read attribute의 경우
        commands = []
        for item in list:
            data = item.split(", ")
            command_type = data[0]
            if module_index == 0:  # Zigbee HA
                if command_type == "connect":
                    print("connect")
                elif command_type == "on/off":
                    value = data[1]
                    cluster = ON_OFF_CLUSTER
                    payloads = None
                    duration = 0.51
                    if value == "on" or value == "0x01" or value == "1":  # on
                        commands.append(get_zigbee_command(cluster, ON_OFF_ON_CMD, payloads, duration, task_kind = COMMAND_TASK))
                    elif value == "off" or value == "0x00" or value == "0":  # off
                        commands.append(get_zigbee_command(cluster, ON_OFF_OFF_CMD, payloads, duration, task_kind = COMMAND_TASK))
                    elif value == "toggle":  # toggle
                        commands.append(get_zigbee_command(cluster, ON_OFF_TOGGLE_CMD, payloads, duration, task_kind = COMMAND_TASK))
                    elif value == "regular random":
                        command = 0x00
                        payloads = 'random'
                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))
                    elif value == "irregular random":
                        command = 0x00
                        payloads = 'random'
                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))
                    else:
                        command = 0x00
                        payloads = 'random'
                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))
                elif command_type == "color":
                    value = data[1]
                    cluster = COLOR_CTRL_CLUSTER
                    command = 0x0a
                    duration = 0.51
                    if value == "regular random":
                        payloads = "random"
                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))
                    elif value == "irregular random":
                        payloads = "random"
                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))
                    elif value == "random":
                        payloads = "random"
                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))
                    else:
                        payloads = [[int(value), 0x21], [0, 0x21]]
                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))
                elif command_type == "level":
                    value = data[1]
                    cluster = LVL_CTRL_CLUSTER
                    command = 0x04
                    duration = 0.51
                    if value == "regular random":
                        payloads = "random"
                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))
                    elif value == "irregular random":
                        payloads = "random"
                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))
                    elif value == "random":
                        payloads = "random"
                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))
                    else:
                        payloads = [[int(value), 0x20], [0, 0x21]]
                        commands.append(get_zigbee_command(cluster, command, payloads, duration, task_kind = COMMAND_TASK))
                elif command_type == "disconnect":
                    print("disconnect")
                elif command_type == "read attribute":
                    task_kind = 1
                    attribute = data[1]
                    duration = 0.51
                    if "ONOFF" in attribute:
                        cluster = ON_OFF_CLUSTER
                        attr_id = zigbee_str_to_attr[cluster][attribute]
                        command = get_zigbee_command(cluster, attr_id=attr_id, duration=duration, task_kind=task_kind)
                        commands.append(command)
                    elif "COLOR" in attribute:
                        cluster = COLOR_CTRL_CLUSTER
                        attr_id = zigbee_str_to_attr[cluster][attribute]
                        command = get_zigbee_command(cluster, attr_id=attr_id, duration=duration, task_kind=task_kind)
                        commands.append(command)
                    elif "LVL" in attribute:
                        cluster = LVL_CTRL_CLUSTER
                        attr_id = zigbee_str_to_attr[cluster][attribute]
                        command = get_zigbee_command(cluster, attr_id=attr_id, duration=duration, task_kind=task_kind)
                        commands.append(command)
        file_data = OrderedDict()
        file_data["tasks"] = commands
        if make_file:
            with open('sample_command.json', 'w', encoding='utf-8') as make_file:
                json.dump(file_data, make_file, ensure_ascii=False, indent="\t")
        return file_data
```


Overlapping Code:
```
e_command(list, module_index, routine_count=-1, make_file=False):
if routine_count > 0: # routine 의 경우
commands = []
for item in list:
data = item.split(", ")
command_type = data[0]
if module_index == 0: # Zigbee HA
if command_type == "connect":
print("connect")
elif command_type == "on/off":
value = data[1]
if value == "on" or value == "0x01": # on
commands.append("resource\\command\\Zigbee\\on.json")
elif value == "off" or value == "0x00": # off
commands.append("resource\\command\\Zigbee\\off.json")
elif value == "toggle":
commands.append("resource\\command\\Zigbee\\toggle.json")
elif value == "regular random":
commands.append("resource\\command\\Zigbee\\onoff_random.json")
elif value == "irregular random":
commands.append("resource\\command\\Zigbee\\onoff_random.json")
else:
commands.append("resource\\command\\Zigbee\\onoff_random.json")
elif command_type == "color":
value = data[1]
if value == "regular random":
commands.append("resource\\command\\Zigbee\\color_random.json")
elif value == "irregular random":
commands.append(
"resource\\command\\Zigbee\\color_random.json")
elif value == "random":
commands.append("resource\\command\\Zigbee\\color_random.json")
elif value == "cw":
commands.append("resource\\command\\Zigbee\\color_cw.json")
elif value == "dl":
commands.append("resource\\command\\Zigbee\\color_dl.json")
elif value == "nw":
commands.append("resource\\command\\Zigbee\\color_nw.json")
elif value == "sw":
commands.append("resource\\command\\Zigbee\\color_sw.json")
elif value == "ww":
commands.append("resource\\command\\Zigbee\\color_ww.json")
elif command_type ==
```
<Overlap Ratio: 0.9803921568627451>

---

--- 134 --
Question ID: fca508875ec0524ea60b427ecfc075489e13efd6_2
Original Code:
```
def JoinTypeAndIdentifier(typeName, identifier):
	# Add a space to separate type from identifier unless type is pointer
	if typeName.endswith("*"):
		return typeName + identifier
	return typeName + " " + identifier
```


Overlapping Code:
```
er(typeName, identifier):
# Add a space to separate type from identifier unless type is pointer
if typeName.endswith("*"):
return typeName + identifie
```
<Overlap Ratio: 0.7177033492822966>

---

--- 135 --
Question ID: a0c54ce4eadc711ce3405e6d511725ad1980c2c1_2
Original Code:
```
def create_replicated_pool(remote, name, pgnum, cluster_name="ceph", application=None):
    remote.run(args=[
        'sudo', 'ceph', 'osd', 'pool', 'create', name, str(pgnum), str(pgnum), '--cluster', cluster_name
        ])
    if application:
        remote.run(args=[
            'sudo', 'ceph', 'osd', 'pool', 'application', 'enable', name, application, '--cluster', cluster_name
        ], check_status=False)
```


Overlapping Code:
```
ote, name, pgnum, cluster_name="ceph", application=None):
remote.run(args=[
'sudo', 'ceph', 'osd', 'pool', 'create', name, str(pgnum), str(pgnum), '--cluster', cluster_name
])
if application:
remote.run(args=[
'sudo', 'ceph', 'osd', 'pool', 'application', 'enable', name, application, '--cluster', cluster_name
], ch
```
<Overlap Ratio: 0.8705234159779615>

---

--- 136 --
Question ID: 252e0da195253d46eb41c35c484c0fab335ef58b_2
Original Code:
```
def main():
    input_filepath = 'etcpasswd'
    output_filepath = './output.tsv'
    passwd_to_csv(input_filepath, output_filepath)
```


Overlapping Code:
```
:
input_filepath = 'etcpasswd'
output_filepath = './output.tsv'
passwd_to_csv(input_filepath, output
```
<Overlap Ratio: 0.8333333333333334>

---

--- 137 --
Question ID: c196b83429923b591360fe6626e240c60f7a76ac_0
Original Code:
```
def scaleCallback(newValue):
	global previousValue
	print('scaleCallback: The type is '+ str(type(newValue)))
	# setting of the inital value
	if previousValue == 0.0 and newValue > MIN_WEIGHT_THRESHOLD:
		previousValue = newValue
		print('Set new value ' + str(previousValue))
		uploadValue(newValue)
	# handle case when user presses on the item on the scale (pump bottle)
	elif (newValue - previousValue) > previousValue * 2 and (newValue - previousValue) >= 150:
		print('User most likely consumed by pressing on the scale skipping')
		return
	# should hit when a single usage is made
	elif (previousValue - newValue) >= VAL_THRESHOLD:
		diff = previousValue - newValue
		previousValue = newValue
		print('Value changed to ' + str(previousValue) + ' with diff ' + str(diff))
		uploadValue(newValue)
	else:
		print('Previous value is ' + str(previousValue) + ' new value is ' + str(newValue) + ' diff is too small, skipping')
```


Overlapping Code:
```
global previousValue
print('scaleCallback: The type is '+ str(type(newValue)))
# setting of the inital value
if previousValue == 0.0 and newValue > MIN_WEIGHT_THRESHOLD:
previousValue = newValue
print('Set new value ' + str(previousValue))
uploadValue(newValue)
# handle case when user presses on the item on the scale (pump bottle)
elif (newValue - previousValue) > previousValue * 2 and (newValue - previousValue) >= 150:
print('User most likely consumed by pressing on the scale skipping')
return
# should hit when a single usage is made
elif (previousValue - newValue) >= VAL_THRESHOLD:
diff = previousValue - newValue
previousValue = newValue
print('Value changed to ' + str(previousValue) + ' with diff ' + str(diff))
uploadValue(newValue)
else:
print('Previous value is ' + str(previousValue) + ' new value is ' + str(newValue) + ' diff is too
```
<Overlap Ratio: 0.947603121516165>

---

--- 138 --
Question ID: e1fcc40719c6de647b5c2d78a3453d599b57a0c4_2
Original Code:
```
def df_to_dataset(df, predictor,  batch_size=32):
    df = df.copy()
    labels = df.pop(predictor)
    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))
    ds = ds.shuffle(buffer_size=len(df))
    ds = ds.batch(batch_size)
    return ds
```


Overlapping Code:
```
def df_to_dataset(df, predictor, batch_size=32):
df = df.copy()
labels = df.pop(predictor)
ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))
ds = ds.shuffle(buffer_size=len(df))
ds = ds.batch(batch_size)
return d
```
<Overlap Ratio: 0.9955156950672646>

---

--- 139 --
Question ID: 438d12f3e6ab73dd71324fca9771be402155918f_2
Original Code:
```
def is_image(resp):
    """Returns true if html request response was an image"""
    try:
        return resp.headers['content-type'].split('/')[0] == 'image'
    except:
        return False
```


Overlapping Code:
```
 if html request response was an image"""
try:
return resp.headers['content-type'].split('/')[0] == 
```
<Overlap Ratio: 0.6134969325153374>

---

--- 140 --
Question ID: 2363166cde2c637b4f881512de76cfa3c7c32673_1
Original Code:
```
def read_data(file):
    with open(file, 'rb') as f:
        # The protocol version used is detected automatically, so we do not
        # have to specify it.
        data = pickle.load(f)
        return data
```


Overlapping Code:
```
e, 'rb') as f:
# The protocol version used is detected automatically, so we do not
# have to specify it.
data = pickle.load(f)
retur
```
<Overlap Ratio: 0.7674418604651163>

---

--- 141 --
Question ID: 53c2e307ae080cda4fd9bcc1d3a071a0262d3734_10
Original Code:
```
@mock.patch('multiprocessing.pool.ThreadPool', test_utils.MockPool)
@mock.patch('common.new_process.execute')
@mock.patch('common.filesystem.directories_have_same_files')
@pytest.mark.skip(reason="See crbug.com/1012329")
def test_measure_all_trials_no_more(mocked_directories_have_same_files,
                                    mocked_execute):
    """Test measure_all_trials does what is intended when the experiment is
    done."""
    mocked_directories_have_same_files.return_value = True
    mocked_execute.return_value = new_process.ProcessResult(0, '', False)
    mock_pool = test_utils.MockPool()
    assert not measurer.measure_all_trials(
        experiment_utils.get_experiment_name(), MAX_TOTAL_TIME, mock_pool,
        queue.Queue())
```


Overlapping Code:
```
ultiprocessing.pool.ThreadPool', test_utils.MockPool)
@mock.patch('common.new_process.execute')
@mock.patch('common.filesystem.directories_have_same_files')
@pytest.mark.skip(reason="See crbug.com/1012329")
def test_measure_all_trials_no_more(mocked_directories_have_same_files,
mocked_execute):
"""Test measure_all_trials does what is intended when the experiment is
done."""
mocked_directories_have_same_files.return_value = True
mocked_execute.return_value = new_process.ProcessResult(0, '', False)
mock_pool = test_utils.MockPool()
assert not measurer.measure_all_trials(
experiment_utils.get_experiment_name(), MAX_TOTAL_TIME, mock_pool,
queue.Queue(
```
<Overlap Ratio: 0.9761549925484352>

---

--- 142 --
Question ID: 8ac7b233e97406cd0036cf8e9bc77b7e813edc5e_0
Original Code:
```
def create_ioclass_config(
        add_default_rule: bool = True, ioclass_config_path: str = default_config_file_path
):
    TestRun.LOGGER.info(f"Creating config file {ioclass_config_path}")
    output = TestRun.executor.run(
        f'echo {IO_CLASS_CONFIG_HEADER} > {ioclass_config_path}'
    )
    if output.exit_code != 0:
        raise Exception(
            "Failed to create ioclass config file. "
            + f"stdout: {output.stdout} \n stderr :{output.stderr}"
        )
    if add_default_rule:
        output = TestRun.executor.run(
            f'echo "0,unclassified,22,1" >> {ioclass_config_path}'
        )
        if output.exit_code != 0:
            raise Exception(
                "Failed to create ioclass config file. "
                + f"stdout: {output.stdout} \n stderr :{output.stderr}"
            )
```


Overlapping Code:
```
ig(
add_default_rule: bool = True, ioclass_config_path: str = default_config_file_path
):
TestRun.LOGGER.info(f"Creating config file {ioclass_config_path}")
output = TestRun.executor.run(
f'echo {IO_CLASS_CONFIG_HEADER} > {ioclass_config_path}'
)
if output.exit_code != 0:
raise Exception(
"Failed to create ioclass config file. "
+ f"stdout: {output.stdout} \n stderr :{output.stderr}"
)
if add_default_rule:
output = TestRun.executor.run(
f'echo "0,unclassified,22,1" >> {ioclass_config_path}'
)
if output.exit_code != 0:
raise Exception(
"Failed to create ioclass config file. "
+ f"stdout: {output.stdout} \n stderr :{output.stderr}"
)
```
<Overlap Ratio: 0.9652567975830816>

---

--- 143 --
Question ID: be9e2dd0898164c46a7279e606ef50771da3efeb_0
Original Code:
```
def load_yaml(path):
    # Fix yaml numbers https://stackoverflow.com/a/30462009/11037553
    loader = yaml.SafeLoader
    loader.add_implicit_resolver(
        u'tag:yaml.org,2002:float',
        re.compile(u'''^(?:
         [-+]?(?:[0-9][0-9_]*)\\.[0-9_]*(?:[eE][-+]?[0-9]+)?
        |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)
        |\\.[0-9_]+(?:[eE][-+][0-9]+)?
        |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\.[0-9_]*
        |[-+]?\\.(?:inf|Inf|INF)
        |\\.(?:nan|NaN|NAN))$''', re.X),
        list(u'-+0123456789.'))
    with open(preprocess_paths(path), "r", encoding="utf-8") as file:
        return yaml.load(file, Loader=loader)
```


Overlapping Code:
```
def load_yaml(path):
# Fix yaml numbers https://stackoverflow.com/a/30462009/11037553
loader = yaml.SafeLoader
loader.add_implicit_resolver(
u'tag:yaml.org,2002:float',
re.compile(u'''^(?:
[-+]?(?:[0-9][0-9_]*)\\.[0-9_]*(?:[eE][-+]?[0-9]+)?
|[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)
|\\.[0-9_]+(?:[eE][-+][0-9]+)?
|[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\.[0-9_]*
|[-+]?\\.(?:inf|Inf|INF)
|\\.(?:nan|NaN|NAN))$''', re.X),
list(u'-+0123456789.'))
with open(preprocess_paths(path), "r", encoding="utf-8") as file:
return yaml.load(file, 
```
<Overlap Ratio: 0.9743119266055046>

---

--- 144 --
Question ID: 29c2d4519c1ca7534ac3b8ab14387d713f625c82_10
Original Code:
```
@pytest.mark.install
@pytest.mark.sanity
def test_vol_create():
    # Verify px status before calling create px volume
    px_status = px_utils.check_px_status()
    assert px_status == 2, "PORTWORX: Avoiding volume create, status returned: {}".format(px_status)
    sleep(120) # Observed failures while volume creation, so introducing sleep of 90 seconds here. 

    pod_count, pod_list = px_utils.get_px_pod_list()
    if pod_count <= 0:
        log.info("PORTWORX: Can't proceed with volume creation, Pod count is: {}".format(pod_count))
        raise
    pod_name = pod_list[0]

    px_utils.px_create_volume(pod_name, "px_dcos_vol_1", 5)
    sleep(30) # Wait before immediatly calling vol size, it is observed that dcos needs few seconds to refresh
    vol_size = px_utils.px_get_vol_size_in_gb("px_dcos_vol_1")
    if 5 != vol_size:
        log.info("PORTWORX: Size of created volume if incorrect, provided: 5, obtained: {}".format(vol_size))
        raise
```


Overlapping Code:
```

def test_vol_create():
# Verify px status before calling create px volume
px_status = px_utils.check_px_status()
assert px_status == 2, "PORTWORX: Avoiding volume create, status returned: {}".format(px_status)
sleep(120) # Observed failures while volume creation, so introducing sleep of 90 seconds here. 
pod_count, pod_list = px_utils.get_px_pod_list()
if pod_count <= 0:
log.info("PORTWORX: Can't proceed with volume creation, Pod count is: {}".format(pod_count))
raise
pod_name = pod_list[0]
px_utils.px_create_volume(pod_name, "px_dcos_vol_1", 5)
sleep(30) # Wait before immediatly calling vol size, it is observed that dcos needs few seconds to refresh
vol_size = px_utils.px_get_vol_size_in_gb("px_dcos_vol_1")
if 5 != vol_size:
log.info("PORTWORX: Size of created volume if incorrect, provid
```
<Overlap Ratio: 0.9049773755656109>

---

--- 145 --
Question ID: 9f0297ebd0788f0dccf07a7bd85a4c5f8e814f45_1
Original Code:
```
def plot_angle_hist(vals, labels, dp):
    nom_v  = vals[np.where(labels==0)[0]]
    anom_v = vals[np.where(labels==1)[0]]
    bins = np.arange(start=np.min(vals), stop=np.max(vals), step=(np.max(vals)-np.min(vals))/50)
    font = {'xtick.labelsize': 16,
            'ytick.labelsize': 16}
    mpl.rc(font)
    pl = dp.get_next_plot()
    plt.xlabel(r"angles from ${\bf w}_{unif}$ (degrees)", fontsize=20)
    plt.ylabel("fraction of instances (%)", fontsize=20)
    logger.debug("\n%s" % str(list(nom_v)))
    n1, bins1 = np.histogram(nom_v, bins=bins, normed=True)
    n2, bins2 = np.histogram(anom_v, bins=bins, normed=True)
    width = 0.7 * (bins[1] - bins[0])
    center = (bins[:-1] + bins[1:]) / 2
    plt.bar(center, n1, align='center', width=width, facecolor='green', alpha=0.50)
    plt.bar(center, n2, align='center', width=width, facecolor='red', alpha=0.50)
```


Overlapping Code:
```
angle_hist(vals, labels, dp):
nom_v = vals[np.where(labels==0)[0]]
anom_v = vals[np.where(labels==1)[0]]
bins = np.arange(start=np.min(vals), stop=np.max(vals), step=(np.max(vals)-np.min(vals))/50)
font = {'xtick.labelsize': 16,
'ytick.labelsize': 16}
mpl.rc(font)
pl = dp.get_next_plot()
plt.xlabel(r"angles from ${\bf w}_{unif}$ (degrees)", fontsize=20)
plt.ylabel("fraction of instances (%)", fontsize=20)
logger.debug("\n%s" % str(list(nom_v)))
n1, bins1 = np.histogram(nom_v, bins=bins, normed=True)
n2, bins2 = np.histogram(anom_v, bins=bins, normed=True)
width = 0.7 * (bins[1] - bins[0])
center = (bins[:-1] + bins[1:]) / 2
plt.bar(center, n1, align='center', width=width, facecolor='green', alpha=0.50)
plt.bar(center, n2, align='center', width=width, facecolor='red', alpha=0
```
<Overlap Ratio: 0.9837092731829574>

---

--- 146 --
Question ID: 6757e77dbd602d533e36ec095f38398dacb4e6e3_0
Original Code:
```
def _LogFeedback(try_job_id, email):
  formatter = quick_logger.Formatter()
  logger = quick_logger.QuickLogger('bad_bisect', 'report', formatter)
  message = '%s marked try job %d.' % (email, try_job_id)
  logger.Log(message)
  logger.Save()
```


Overlapping Code:
```
ogFeedback(try_job_id, email):
formatter = quick_logger.Formatter()
logger = quick_logger.QuickLogger('bad_bisect', 'report', formatter)
message = '%s marked try job %d.' % (email, try_job_id)
logger.Log(messag
```
<Overlap Ratio: 0.9051724137931034>

---

--- 147 --
Question ID: c5af7336463d4fdf0b0c0ab9e39d2fca2711f16c_0
Original Code:
```
def isint(s):
  try: 
    int(s)
    return True
  except ValueError:
    return False
```


Overlapping Code:
```
sint(s):
try: 
int(s)
return True
except ValueError:
return False
```
<Overlap Ratio: 0.9285714285714286>

---

--- 148 --
Question ID: 0a4ee7ec3cf7e8ba6da8726fac8e19bee3434dcf_2
Original Code:
```
def decompose_tokens(tokens, shuffle):
    decomposed = list()
    for i, token in enumerate(tokens):
        decomposed.append(tokens[:i+1])
    if shuffle:
        random.shuffle(decomposed)
    return decomposed
```


Overlapping Code:
```
sed = list()
for i, token in enumerate(tokens):
decomposed.append(tokens[:i+1])
if shuffle:
random.s
```
<Overlap Ratio: 0.5494505494505495>

---

--- 149 --
Question ID: fef0c05fbc3097742bed72246ffd798b20bd0fc8_3
Original Code:
```
@contextlib.contextmanager
def lock_repository_write(repo_path):
    with acquire_repo_lock("write", repo_path):
        repo = _WritableLocalRepository(repo_path)
        try:
            yield repo
        finally:
            repo.invalidate()
```


Overlapping Code:
```
ory_write(repo_path):
with acquire_repo_lock("write", repo_path):
repo = _WritableLocalRepository(repo_path)
try:
yield repo
finally:
repo.invalidate(
```
<Overlap Ratio: 0.7731958762886598>

---

--- 150 --
Question ID: 10185d3f9e286470c2b937976eaee587dbf057bc_2
Original Code:
```
def add_default_location_to_dataset(dataset_folder, location):
    metadata = read_dataset_metadata(dataset_folder)
    metadata["defaultLocation"] = location
    validate_with_schema(metadata, "dataset")
    write_dataset_metadata(dataset_folder, metadata)
```


Overlapping Code:
```
 add_default_location_to_dataset(dataset_folder, location):
metadata = read_dataset_metadata(dataset_folder)
metadata["defaultLocation"] = location
validate_with_schema(metadata, "dataset")
write_data
```
<Overlap Ratio: 0.8298755186721992>

---

--- 151 --
Question ID: 3861303222e76fe1f05361773256708f65fd81c8_0
Original Code:
```
def elems_in_dir(path):
    if not os.path.exists(path):
        return 0
    return len(os.listdir(path))
```


Overlapping Code:
```
path):
if not os.path.exists(path):
return 0
return len(os.l
```
<Overlap Ratio: 0.6666666666666666>

---

--- 152 --
Question ID: 7346174d177d9a0833ca426ec09adc4a8542b9b5_2
Original Code:
```
def test2():
    with Switch() as (switch, case, default):
        try:switch('hola')
        except case(1):
            print(1)
        except case('holaS'):
            print('holaS')
        except case('hola'):
            print('hola')
        except default():
            print('default..')
```


Overlapping Code:
```
as (switch, case, default):
try:switch('hola')
except case(1):
print(1)
except case('holaS'):
print('holaS')
except case('hola'):
print('hola')
except
```
<Overlap Ratio: 0.7246376811594203>

---

--- 153 --
Question ID: d9da01dae503bd93e661ea5caa90fa1d369938c4_0
Original Code:
```
def get_namespace_to_patch(namespace_target: NamespaceTarget) -> object:
    if namespace_target == NamespaceTarget.TORCH_NN_FUNCTIONAL:
        return torch.nn.functional
    if namespace_target == NamespaceTarget.TORCH_TENSOR:
        return TracedTensor
    if namespace_target == NamespaceTarget.TORCH:
        return torch
    raise RuntimeError("{} namespace wasn't found in {}".format(namespace_target, NamespaceTarget))
```


Overlapping Code:
```
f get_namespace_to_patch(namespace_target: NamespaceTarget) -> object:
if namespace_target == NamespaceTarget.TORCH_NN_FUNCTIONAL:
return torch.nn.functional
if namespace_target == NamespaceTarget.TORCH_TENSOR:
return TracedTensor
if namespace_target == NamespaceTarget.TORCH:
return torch
raise RuntimeError("{} namespace wasn't found in {}".format(
```
<Overlap Ratio: 0.9043927648578811>

---

--- 154 --
Question ID: 7a8e28da299122c36d383ba559ef8dae560f1c9b_2
Original Code:
```
async def run_all(cmds, timeout=None, retry=0, loop=None, output=None, **kwargs):
    output = _get_output(output)
    ps = []
    for cmd in cmds: ps.append(await Process.create(cmd, timeout, retry, loop, **kwargs))
    mp = MutiProcesses(ps)
    await mp.wait()
    results = []
    for p in mp.proccesses:
        if p.returncode != 0:
            results.append(CmdRunError(p.cmd, p.returncode, await _readstr(p.stderr), await _readstr(p.stdout)))
        else:
            results.append(await output(p.stdout))
    return results
```


Overlapping Code:
```
imeout=None, retry=0, loop=None, output=None, **kwargs):
output = _get_output(output)
ps = []
for cmd in cmds: ps.append(await Process.create(cmd, timeout, retry, loop, **kwargs))
mp = MutiProcesses(ps)
await mp.wait()
results = []
for p in mp.proccesses:
if p.returncode != 0:
results.append(CmdRunError(p.cmd, p.returncode, await _readstr(p.stderr), await _readstr(p.stdout)))
else:
results.append(await output
```
<Overlap Ratio: 0.8898488120950324>

---

--- 155 --
Question ID: 3d532ad5ce5fde114405a21932f02c31ad88688f_0
Original Code:
```
def like(m):
    #print("m:", m)
    #print("m.items():", [(k,v) for k,v in m.items()])
    a = m["model1::x"]
    ScannerBit.print("my_param", 0.5) # can print custom parameters 

    return -a*a/2.0
```


Overlapping Code:
```
int("m.items():", [(k,v) for k,v in m.items()])
a = m["model1::x"]
ScannerBit.print("my_param", 0.5) # can print custom parameters
```
<Overlap Ratio: 0.7262569832402235>

---

--- 156 --
Question ID: 8865cc07f43910d1429edc8b327be6a6081c800a_9
Original Code:
```
def f1_internet2(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())


    return f1_val
```


Overlapping Code:
```
ef f1_internet2(y_true, y_pred):
true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
precision = true_positives / (predicted_positives + K.epsilon())
recall = true_positives / (possible_positives + K.epsilon())
f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())
return f1_va
```
<Overlap Ratio: 0.9952830188679245>

---

--- 157 --
Question ID: 60a75d6f65b97e7dc8786828b1a6a4ebc9dfd802_7
Original Code:
```
def test_xy_displacement_subpixel(data):
    fft_corr, window_a = data
    window_b = np.load('data/shift05.npy')
    #window_a = np.copy(window_b)
    #window_b = np.roll(window_b, shift=1, axis=0)
    #window_b = np.roll(window_b, shift=1, axis=1)
    dx, dy =  fft_corr.get_displacement(window_a, window_b)
    delta = .5
    assert abs(dy - delta)  < 0.01
    assert abs(dx - delta)  < 0.01
```


Overlapping Code:
```
pixel(data):
fft_corr, window_a = data
window_b = np.load('data/shift05.npy')
#window_a = np.copy(window_b)
#window_b = np.roll(window_b, shift=1, axis=0)
#window_b = np.roll(window_b, shift=1, axis=1)
dx, dy = fft_corr.get_displacement(window_a, window_b)
delta = .5
assert abs(dy - delta) < 0.01
assert abs(dx - de
```
<Overlap Ratio: 0.8901408450704226>

---

--- 158 --
Question ID: 817fda110457850c41c112c92508235db682bda2_5
Original Code:
```
def test_beam_with_open_issues_should_not_be_vacuumed(
    eager_celery, db_session, create_beam, expired_beam_date, issue
):
    beam = create_beam(start=expired_beam_date, completed=True)
    beam.issues.append(issue)
    db_session.commit()
    vacuum.delay()
    assert not is_vacuumed(db_session, beam)
```


Overlapping Code:
```
_be_vacuumed(
eager_celery, db_session, create_beam, expired_beam_date, issue
):
beam = create_beam(start=expired_beam_date, completed=True)
beam.issues.append(issue)
db_session.commit()
vacuum.delay()
assert not is_vacuumed(db_session, bea
```
<Overlap Ratio: 0.8480565371024735>

---

--- 159 --
Question ID: 1e33d8d011c6d736a3545d4965716342cd10d0cd_6
Original Code:
```
def pressionProspectionEpci(connection, nom_epci_simple):
    if config.GROS_JEU_DONNEES:
        sql = """SELECT
                obs.id_maille,
                obs.geojson_maille,
                a.nom_organisme AS orgaobs, 
                count(obs.id_observation) as nbobs,
                max(extract(year from dateobs)) as annee
            FROM atlas.vm_observations_mailles obs
            JOIN atlas.vm_observations o ON o.id_observation = obs.id_observation
            JOIN atlas.vm_taxons t ON t.cd_ref=o.cd_ref
            JOIN atlas.l_communes_epci ec ON ec.insee = o.insee
            JOIN atlas.vm_epci e ON ec.id = e.id
            JOIN atlas.vm_organismes a ON a.id_organisme = o.id_organisme 
            WHERE e.nom_epci_simple = :thisNomEpciSimple
            GROUP BY
                obs.id_maille,
                obs.geojson_maille,
                a.nom_organisme
            ORDER BY obs.id_maille"""    
    else:
        sql = """SELECT
                obs.id_maille,
                obs.geojson_maille,
                a.nom_organisme AS orgaobs, 
                o.dateobs,
                extract(YEAR FROM o.dateobs) as annee
            FROM atlas.vm_observations_mailles obs
            JOIN atlas.vm_observations o ON o.id_observation = obs.id_observation
            JOIN atlas.vm_taxons t ON t.cd_ref=o.cd_ref
            JOIN atlas.l_communes_epci ec ON ec.insee = o.insee
            JOIN atlas.vm_epci e ON ec.id = e.id
            JOIN atlas.vm_organismes a ON a.id_organisme = o.id_organisme 
           WHERE e.nom_epci_simple = :thisNomEpciSimple
            ORDER BY id_maille"""


    observations = connection.execute(text(sql), thisNomEpciSimple=nom_epci_simple)
    tabObs = list()

    if config.GROS_JEU_DONNEES:
        for o in observations:
            temp = {
                'id_maille': o.id_maille,
                'nb_observations': o.nbobs,
                'annee': o.annee,
                'dateobs': None,
                'orga_obs': o.orgaobs,
                'geojson_maille': ast.literal_eval(o.geojson_maille)
            }
            tabObs.append(temp)
    else:
        for o in observations:
            temp = {
                'id_maille': o.id_maille,
                'nb_observations': 1,
                'annee': o.annee,
                'dateobs': str(o.dateobs),
                'orga_obs': o.orgaobs,
                'geojson_maille': ast.literal_eval(o.geojson_maille)
            }
            tabObs.append(temp)
    return tabObs
```


Overlapping Code:
```
tionEpci(connection, nom_epci_simple):
if config.GROS_JEU_DONNEES:
sql = """SELECT
obs.id_maille,
obs.geojson_maille,
a.nom_organisme AS orgaobs, 
count(obs.id_observation) as nbobs,
max(extract(year from dateobs)) as annee
FROM atlas.vm_observations_mailles obs
JOIN atlas.vm_observations o ON o.id_observation = obs.id_observation
JOIN atlas.vm_taxons t ON t.cd_ref=o.cd_ref
JOIN atlas.l_communes_epci ec ON ec.insee = o.insee
JOIN atlas.vm_epci e ON ec.id = e.id
JOIN atlas.vm_organismes a ON a.id_organisme = o.id_organisme 
WHERE e.nom_epci_simple = :thisNomEpciSimple
GROUP BY
obs.id_maille,
obs.geojson_maille,
a.nom_organisme
ORDER BY obs.id_maille""" 
else:
sql = """SELECT
obs.id_maille,
obs.geojson_maille,
a.nom_organisme AS orgaobs, 
o.dateobs,
extract(YEAR FROM o.dateobs) as annee
FROM atlas.vm_observations_mailles obs
JOIN atlas.vm_observations o ON o.id_observation = obs.id_observation
JOIN atlas.vm_taxons t ON t.cd_ref=o.cd_ref
JOIN atlas.l_communes_epci ec ON ec.insee = o.insee
JOIN atlas.vm_epci e ON ec.id = e.id
JOIN atlas.vm_organismes a ON a.id_organisme = o.id_organisme 
WHERE e.nom_epci_simple = :thisNomEpciSimple
ORDER BY id_maille"""
observations = connection.execute(text(sql), thisNomEpciSimple=nom_epci_simple)
tabObs = list()
if config.GROS_JEU_DONNEES:
for o in observations:
temp = {
'id_maille': o.id_maille,
'nb_observations': o.nbobs,
'annee': o.annee,
'dateobs': None,
'orga_obs': o.orgaobs,
'geojson_maille': ast.literal_eval(o.geojson_maille)
}
tabObs.append(temp)
else:
for o in observations:
temp = {
'id_maille': o.id_maille,
'nb_observations': 1,
'annee': o.annee,
'dateobs': str(o.dateobs),
'orga_obs': o.orgaobs,
'geojson_maille': ast.literal_eval(o.geojson_maille)
}
tabObs.append(t
```
<Overlap Ratio: 0.9869169510807736>

---

--- 160 --
Question ID: ff3e6273138416f8f635747b52c2637170ad6f94_2
Original Code:
```
def test_user_title():

    test_data = [2, "test"]
    title = data.UserTitle(test_data)

    assert title.id == 2
    assert title.title == "test"
    assert title.to_row() == test_data
    assert title.table_name() == "user_titles"
```


Overlapping Code:
```
t_user_title():
test_data = [2, "test"]
title = data.UserTitle(test_data)
assert title.id == 2
assert title.title == "test"
assert title.to_row() == test_data
assert title.table_name() == "user_titles
```
<Overlap Ratio: 0.9615384615384616>

---

--- 161 --
Question ID: 5bc57ff79acfe3428f9ddf5565f8a646f4c3ccbb_0
Original Code:
```
def move_left():
    x = player.xcor()
    #y = player.ycor()
    x = x - playerspeed  # x = x - playerspeed ( Changing value of x each time )
    if x < -280:  # Blocking the player from crossing
        x = - 280
    player.setx(x)  # setting the player's location to new x
```


Overlapping Code:
```
)
#y = player.ycor()
x = x - playerspeed # x = x - playerspeed ( Changing value of x each time )
if x < -280: # Blocking the player from crossing
x = - 280
player.setx(x) # setting the player's locati
```
<Overlap Ratio: 0.819672131147541>

---

--- 162 --
Question ID: 03cbde7afaaf370dff1a69ad6713b70c2e5595e3_0
Original Code:
```
def main():
    # Prompt for a colour - TODO: add validation
    colour = input("Please pick a Colour: ")
    print()
    # Print out the colour a letter per row, slowly
    for c in colour:
        print (str.upper(c) + "...")
        time.sleep(0.5)

    # Prompt for a number - TODO: add validation
    n = get_positive_int()
    print()
    # Count out slowly
    i = 0
    for i in range(1, n+1):
        print (i, end="")
        time.sleep(0.2)
        print(".", end="")
        time.sleep(0.2)
        print(".", end="")
        time.sleep(0.2)
        print(".", end="\n")
        i =+ 1

    # Generate a number at random
    final = random.randrange(1,8,1)
    #print("\nThe random number is: " + str(final))
    time.sleep(0.5)
    # Build suspense
    print ("\nI will now open the flap...", end="\n\n")
    time.sleep(2.0)
    # Return the instruction
    if (final == 1) or (final == 5):
        print(str.center("You must turn left.", 30))
    elif (final == 2) or (final == 6):
        print(str.center("You must turn right.", 30))
    elif (final == 3) or (final == 8):
        print(str.center("You must turn back.", 30))
    else:
        print(str.center("You must continue ahead.", 30))
    print()
```


Overlapping Code:
```
 validation
colour = input("Please pick a Colour: ")
print()
# Print out the colour a letter per row, slowly
for c in colour:
print (str.upper(c) + "...")
time.sleep(0.5)
# Prompt for a number - TODO: add validation
n = get_positive_int()
print()
# Count out slowly
i = 0
for i in range(1, n+1):
print (i, end="")
time.sleep(0.2)
print(".", end="")
time.sleep(0.2)
print(".", end="")
time.sleep(0.2)
print(".", end="\n")
i =+ 1
# Generate a number at random
final = random.randrange(1,8,1)
#print("\nThe random number is: " + str(final))
time.sleep(0.5)
# Build suspense
print ("\nI will now open the flap...", end="\n\n")
time.sleep(2.0)
# Return the instruction
if (final == 1) or (final == 5):
print(str.center("You must turn left.", 30))
elif (final == 2) or (final == 6):
print(str.center("You must turn right.", 30))
elif (final == 3) or (final == 8):
print(str.center("You must turn back.", 30))
else:
print(str.center("You must continue ahead
```
<Overlap Ratio: 0.9396636993076162>

---

--- 163 --
Question ID: 095ac223d811183eda37c0c4ac06bce2dd225277_0
Original Code:
```
def find_2x2_matching(matrix):
    matches = 0
    for i in range(len(matrix) - 1):
        for j in range(len(matrix[i]) - 1):
            if matrix[i][j] == matrix[i][j + 1] == matrix[i + 1][j] == matrix[i + 1][j + 1]:
                matches += 1
    print(matches)
```


Overlapping Code:
```
 i in range(len(matrix) - 1):
for j in range(len(matrix[i]) - 1):
if matrix[i][j] == matrix[i][j + 1] == matrix[i + 1][j] == matrix[i + 1][j + 1]:
matc
```
<Overlap Ratio: 0.6863636363636364>

---

--- 164 --
Question ID: a438cfea753c52adfbf8794a2e34672479d5b2b2_0
Original Code:
```
def GetHealthChecks(args, resource_parser):
  """Returns health check URIs from arguments."""
  health_check_refs = []

  if args.http_health_checks:
    health_check_refs.extend(resource_parser.CreateGlobalReferences(
        args.http_health_checks, resource_type='httpHealthChecks'))

  if getattr(args, 'https_health_checks', None):
    health_check_refs.extend(resource_parser.CreateGlobalReferences(
        args.https_health_checks, resource_type='httpsHealthChecks'))

  if getattr(args, 'health_checks', None):
    if health_check_refs:
      raise exceptions.ToolException(
          'Mixing --health-checks with --http-health-checks or with '
          '--https-health-checks is not supported.')
    else:
      health_check_refs.extend(resource_parser.CreateGlobalReferences(
          args.health_checks, resource_type='healthChecks'))

  return [health_check_ref.SelfLink() for health_check_ref in health_check_refs]
```


Overlapping Code:
```
ef GetHealthChecks(args, resource_parser):
"""Returns health check URIs from arguments."""
health_check_refs = []
if args.http_health_checks:
health_check_refs.extend(resource_parser.CreateGlobalReferences(
args.http_health_checks, resource_type='httpHealthChecks'))
if getattr(args, 'https_health_checks', None):
health_check_refs.extend(resource_parser.CreateGlobalReferences(
args.https_health_checks, resource_type='httpsHealthChecks'))
if getattr(args, 'health_checks', None):
if health_check_refs:
raise exceptions.ToolException(
'Mixing --health-checks with --http-health-checks or with '
'--https-health-checks is not supported.')
else:
health_check_refs.extend(resource_parser.CreateGlobalReferences(
args.health_checks, resource_type='healthChecks'))
return [health_check_ref.SelfLink() for health_check_ref in health_chec
```
<Overlap Ratio: 0.9904761904761905>

---

--- 165 --
Question ID: 4aa4c0e26f8b9c3e062654d27b8693d160af92d1_4
Original Code:
```
def _render_option(option, create_fun):
    if option.render_context is None:
        create_fun(option.source)
    rendered = option.value
    return rendered
```


Overlapping Code:
```
ption.render_context is None:
create_fun(option.so
```
<Overlap Ratio: 0.3597122302158273>

---

--- 166 --
Question ID: 3357f1134ab6d6b6880995f4a2cd67cfb3a2b35a_1
Original Code:
```
def convert_image_by_inkscape(inputimg, outputdir, output_imgname,
                              outputname='', outputformat='png', overwrite=True,
                              dpi=150):
    outputimg = detect_output_file_exist(outputdir, output_imgname,
                                         outputformat,
                                         overwrite)
    if not outputimg:
        return None

    if inputimg == outputimg:
        raise FileExistsError

    if outputformat == 'png':
        outflag = 'e'
    elif outputformat == 'pdf':
        outflag = 'A'
    elif outputformat == 'ps':
        outflag = 'P'
    elif outputformat == 'eps':
        outflag = 'E'

    if shutil.which('inkscape'):
        process_cmd = ['inkscape', '-zC',
                       '-f', inputimg, '-{0}'.format(outflag),
                       outputimg, '-d', str(dpi)]
        logger.debug(f'start call cmd {process_cmd}')
        subprocess.check_call(process_cmd)
        return outputimg  # only retcode is zero
    else:
        raise InkscapeProcessError("inkscape commond not found.")
```


Overlapping Code:
```
t_image_by_inkscape(inputimg, outputdir, output_imgname,
outputname='', outputformat='png', overwrite=True,
dpi=150):
outputimg = detect_output_file_exist(outputdir, output_imgname,
outputformat,
overwrite)
if not outputimg:
return None
if inputimg == outputimg:
raise FileExistsError
if outputformat == 'png':
outflag = 'e'
elif outputformat == 'pdf':
outflag = 'A'
elif outputformat == 'ps':
outflag = 'P'
elif outputformat == 'eps':
outflag = 'E'
if shutil.which('inkscape'):
process_cmd = ['inkscape', '-zC',
'-f', inputimg, '-{0}'.format(outflag),
outputimg, '-d', str(dpi)]
logger.debug(f'start call cmd {process_cmd}')
subprocess.check_call(process_cmd)
return outputimg # only retcode is zero
else:
raise InkscapeProcessError("inkscape common
```
<Overlap Ratio: 0.9689922480620154>

---

--- 167 --
Question ID: 360c8ad7a45cf87f3d372aba1e9e3c70c3244161_4
Original Code:
```
def print_help():
    ''' show usage help '''
    print("usage: ns_ad_converter.py [type] [year] [month] [date]")
    print("")
    print("valid type [type]: to_ns | to_ad")
    print(("valid range [year] [month] [date]: FROM 880 10 20 to ... (to_ns),"
           " 1 1 1 to ... (to_ad)"))
    print("")
```


Overlapping Code:
```
 usage help '''
print("usage: ns_ad_converter.py [type] [year] [month] [date]")
print("")
print("valid type [type]: to_ns | to_ad")
print(("valid range [year] [month] [date]: FROM 880 10 20 to ... (to
```
<Overlap Ratio: 0.746268656716418>

---

--- 168 --
Question ID: 882bbc1addb5a30394d5fa8f5a1cffe9fb2931e4_0
Original Code:
```
def read_csv(fn):
    '''reads a csv file and returns a pandas dataframe'''
    temp = pd.read_csv(fn)
    return pd.DataFrame(temp)
```


Overlapping Code:
```
urns a pandas dataframe'''
temp = pd.read_csv(fn)

```
<Overlap Ratio: 0.4166666666666667>

---

--- 169 --
Question ID: 16d3163ada87d80c9109401edea455d226b76fb9_1
Original Code:
```
def ball_test():
    cnf = Canvas3DFrame(None)
    vts, fs, ns, cs = geoutil.build_ball((100,100,100),50, (1,0,0))
    cnf.add_surf('ball', vts, fs, ns, cs)
    cnf.Show()
```


Overlapping Code:
```
cnf = Canvas3DFrame(None)
vts, fs, ns, cs = geoutil.build_ball((100,100,100),50, (1,0,0))
cnf.add_surf('ball', vts, fs, ns
```
<Overlap Ratio: 0.7870967741935484>

---

--- 170 --
Question ID: be6bebe00069e88ec10e9e6c433d533d1f7a88bc_3
Original Code:
```
@login_required
def editTask(request, id):
    task = get_object_or_404(Task, pk=id)
    form = TaskFormEdit(instance=task)

    if (request.method == 'POST'):
        form = TaskFormEdit(request.POST, instance=task)
        if form.is_valid():
            task.save()
            return redirect('/')
        else:
            return render(request, 'tasks/edittask.html', {'task':task, 'form':form})

    else:
        return render(request, 'tasks/edittask.html', {'task':task, 'form':form})
```


Overlapping Code:
```
login_required
def editTask(request, id):
task = get_object_or_404(Task, pk=id)
form = TaskFormEdit(instance=task)
if (request.method == 'POST'):
form = TaskFormEdit(request.POST, instance=task)
if form.is_valid():
task.save()
return redirect('/')
else:
return render(request, 'tasks/edittask.html', {'task':task, 'form':form})
else:
return render(request, 'tasks/edittask.html', {'task':task, 'form':form
```
<Overlap Ratio: 0.9926470588235294>

---

--- 171 --
Question ID: 64eba55d4037f01fdf216a06c8967f0c4c24b860_7
Original Code:
```
@frappe.whitelist()
def update_transferred_qty(self, status):
	if print_debug: frappe.logger().debug("---radplusplus.manufacturing_controllers.update_transferred_qty---")
	""" Called to refresh transferred_qty based on stock_entry"""
	self = frappe.get_doc("Work Order", self)
	status = update_status(self,status)
	self.update_planned_qty()
	frappe.msgprint(_("Work Order status is {0}").format(status))
	self.notify_update()
```


Overlapping Code:
```
ty(self, status):
if print_debug: frappe.logger().debug("---radplusplus.manufacturing_controllers.update_transferred_qty---")
""" Called to refresh transferred_qty based on stock_entry"""
self = frappe.get_doc("Work Order", self)
status = update_status(self,status)
self.update_planned_qty()
frappe.msgprint(_("Work Order status is {0}").format(statu
```
<Overlap Ratio: 0.8373205741626795>

---

--- 172 --
Question ID: 9c92e3cfe0b484e62f56af56c09981bd06d65ca1_26
Original Code:
```
def fetch_servers(conn, userId):
    sql = "SELECT server_id,server_name,server_icon FROM Servers WHERE owner_id=?"
    cur = conn.cursor()
    cur.execute(sql, (userId,))
    return cur.fetchall()
```


Overlapping Code:
```
 = "SELECT server_id,server_name,server_icon FROM Servers WHERE owner_id=?"
cur = conn.cursor()
cur.execute(sql, (userId,
```
<Overlap Ratio: 0.6685082872928176>

---

--- 173 --
Question ID: 5c2ee2d33e6d6e2c2be0782ba6f20e9865d4008f_0
Original Code:
```
def edit_distance(str1, str2, m, n):

    # If either string is empty, the answer is length of the other string
    # because we would have to insert that many
    if m == 0:
        return n
    if n == 0:
        return m

  
    # If last two characters are same, just compare for the rest of the string
    if str1[n-1] == str2[m-1]:
        return edit_distance(str1, str2, m-1, n-1)
  
    # If characters aren't the same, just consider all the options
    # these are relative to string 2
    return 1 + min(edit_distance(str1, str2, m, n-1),       # insert
                   edit_distance(str1, str2, m-1, n),       # remove
                   edit_distance(str1, str2, m-1, n-1))     # replace
```


Overlapping Code:
```
str1, str2, m, n):
# If either string is empty, the answer is length of the other string
# because we would have to insert that many
if m == 0:
return n
if n == 0:
return m

# If last two characters are same, just compare for the rest of the string
if str1[n-1] == str2[m-1]:
return edit_distance(str1, str2, m-1, n-1)

# If characters aren't the same, just consider all the options
# these are relative to string 2
return 1 + min(edit_distance(str1, str2, m, n-1), # insert
edit_distance(str1, str2, m-1, n), # remove
edit_distance(str1, str2, m-1, 
```
<Overlap Ratio: 0.9433962264150944>

---

--- 174 --
Question ID: e900714ee7c1622a99ce0b8aa996f354ebce7e08_5
Original Code:
```
def inception_C_block(name, input):

    '''

    :param name:
    :param input:
    :return:
    '''

    with tf.name_scope(name) as scope:
        pool_1b1 = layers.pool_layer(scope+'pool_1b1', input, ksize=[1,1], kstep=[1,1], padding='SAME', pool_fun=tf.nn.avg_pool)
        conv_1b1 = layers.conv_layer(scope+'conv_1b1', pool_1b1, ksize=[1,1], depth=256, kstep=[1,1],padding='SAME')

        conv_2b2 = layers.conv_layer(scope+'conv_2b2', input, ksize=[1,1], depth=256, kstep=[1,1], padding='SAME')

        conv_3b3 = layers.conv_layer(scope+'conv_3b3', input, ksize=[1,1], depth=384, kstep=[1,1], padding='SAME')
        conv_4b3_1 = layers.conv_layer(scope+'conv_4b3_1',conv_3b3, ksize=[1,3], depth=256, kstep=[1,1], padding='SAME')
        conv_5b3_2 = layers.conv_layer(scope+'conv_5b3_2',conv_3b3, ksize=[3,1], depth=256, kstep=[1,1], padding='SAME')

        conv_6b4 = layers.conv_layer(scope+'conv_6b4', input, ksize=[1,1], depth=384, kstep=[1,1], padding='SAME')
        conv_7b4 = layers.conv_layer(scope+'conv_7b4', conv_6b4, ksize=[1,3],depth=448, kstep=[1,1], padding='SAME')
        conv_8b4 = layers.conv_layer(scope+'conv_8b4', conv_7b4, ksize=[3,1], depth=512, kstep=[1,1], padding='SAME')
        conv_9b4_1 = layers.conv_layer(scope+'conv_9b4_1', conv_8b4, ksize=[3,1], depth=256, kstep=[1,1], padding='SAME')
        conv_10b4_2 = layers.conv_layer(scope+'conv_10b4_2',conv_8b4,ksize=[1,3], depth=256, kstep=[1,1],padding='SAME')

        concat_0 = tf.concat(3, [conv_1b1,conv_2b2,conv_4b3_1,conv_5b3_2,conv_9b4_1,conv_10b4_2],name=scope+'concat_0')
        return concat_0
```


Overlapping Code:
```
_block(name, input):
'''
:param name:
:param input:
:return:
'''
with tf.name_scope(name) as scope:
pool_1b1 = layers.pool_layer(scope+'pool_1b1', input, ksize=[1,1], kstep=[1,1], padding='SAME', pool_fun=tf.nn.avg_pool)
conv_1b1 = layers.conv_layer(scope+'conv_1b1', pool_1b1, ksize=[1,1], depth=256, kstep=[1,1],padding='SAME')
conv_2b2 = layers.conv_layer(scope+'conv_2b2', input, ksize=[1,1], depth=256, kstep=[1,1], padding='SAME')
conv_3b3 = layers.conv_layer(scope+'conv_3b3', input, ksize=[1,1], depth=384, kstep=[1,1], padding='SAME')
conv_4b3_1 = layers.conv_layer(scope+'conv_4b3_1',conv_3b3, ksize=[1,3], depth=256, kstep=[1,1], padding='SAME')
conv_5b3_2 = layers.conv_layer(scope+'conv_5b3_2',conv_3b3, ksize=[3,1], depth=256, kstep=[1,1], padding='SAME')
conv_6b4 = layers.conv_layer(scope+'conv_6b4', input, ksize=[1,1], depth=384, kstep=[1,1], padding='SAME')
conv_7b4 = layers.conv_layer(scope+'conv_7b4', conv_6b4, ksize=[1,3],depth=448, kstep=[1,1], padding='SAME')
conv_8b4 = layers.conv_layer(scope+'conv_8b4', conv_7b4, ksize=[3,1], depth=512, kstep=[1,1], padding='SAME')
conv_9b4_1 = layers.conv_layer(scope+'conv_9b4_1', conv_8b4, ksize=[3,1], depth=256, kstep=[1,1], padding='SAME')
conv_10b4_2 = layers.conv_layer(scope+'conv_10b4_2',conv_8b4,ksize=[1,3], depth=256, kstep=[1,1],padding='SAME')
concat_0 = tf.concat(3, [conv_1b1,conv_2b2,conv_4b3_1,conv_5b3_2,conv_9b4_1,conv_10b4_2],
```
<Overlap Ratio: 0.9638225255972697>

---

--- 175 --
Question ID: 4040d3ac60b4e9bdd741f3ffe95d2a3d6fb1afba_11
Original Code:
```
def xtest_umls(rosetta):
    node = KNode("UMLS:C0015625",label="Fanconi Anemia", type=node_types.DISEASE)
    rosetta.synonymizer.synonymize(node)
    print(node.synonyms)
    assert node.id == 'MONDO:0019339'
```


Overlapping Code:
```
(rosetta):
node = KNode("UMLS:C0015625",label="Fanconi Anemia", type=node_types.DISEASE)
rosetta.synonymizer.synonymize(node)
print(node.synonyms)
ass
```
<Overlap Ratio: 0.7731958762886598>

---

--- 176 --
Question ID: a3caf21b9bd90bd1f45542f3e58da9656c86d898_1
Original Code:
```
def namespace_details(request, namespace_id):
    if not request.user.is_superuser or not request.user.is_staff:
        raise PermissionDenied

    namespace = StatisticNamespace.get(namespace_id)

    return render_to_response('generic_list.html', {
        'object': namespace,
        'namespace': namespace,
        'object_list': namespace.statistics,
        'hide_link': True,
        'title': _(u'namespace details for: %s') % namespace,
        'object_name': _(u'namespace'),
    }, context_instance=RequestContext(request))
```


Overlapping Code:
```
space_details(request, namespace_id):
if not request.user.is_superuser or not request.user.is_staff:
raise PermissionDenied
namespace = StatisticNamespace.get(namespace_id)
return render_to_response('generic_list.html', {
'object': namespace,
'namespace': namespace,
'object_list': namespace.statistics,
'hide_link': True,
'title': _(u'namespace details for: %s') % namespace,
'object_name': _(u'namespace'),
}, context_instance=RequestContext(reques
```
<Overlap Ratio: 0.9761388286334056>

---

--- 177 --
Question ID: e70a25a5bdee482509843e096b266c23d77d2875_1
Original Code:
```
def load_data(train_data_location):
    train_data = {};
    lines = []

    test_file = open(train_data_location, "r")
    for line in test_file:
        lines.append(unicode(line, "utf-8"))
    test_file.close()
    random.shuffle(lines)
    lines = lines[-INTENT_LIMIT:]
    train_data["intent_data"] = get_intent_training_data(lines);
    train_data["entity_data"] = get_entity_training_data(lines);
            
    return train_data
```


Overlapping Code:
```
 = {};
lines = []
test_file = open(train_data_location, "r")
for line in test_file:
lines.append(unicode(line, "utf-8"))
test_file.close()
random.shuffle(lines)
lines = lines[-INTENT_LIMIT:]
train_data["intent_data"] = get_intent_training_data(lines);
train_data["entity_data"] = get_entity_training_
```
<Overlap Ratio: 0.7957559681697612>

---

--- 178 --
Question ID: 0f1378a2b467cd7d379914c635842d2e9e749e55_0
Original Code:
```
@pytest.fixture
def domain_from_token(domain):
    client = dnsimple.Client(domain_token = domain.token, sandbox = True)
    return client.domain(domain.name)
```


Overlapping Code:
```
pytest.fixture
def domain_from_token(domain):
client = dnsimple.Client(domain_token = domain.token, 
```
<Overlap Ratio: 0.6666666666666666>

---

--- 179 --
Question ID: 3a47b724c63d217caa739ccda16a1fc5ef189af3_8
Original Code:
```
@cocotb.test(skip = False)
def write_char_test(dut):
    """
    Description:

    Test ID: 7

    Expected Results:
        **
    """
    dut.rst <= 1
    dut.test_id <= 7
    axim = AXI4LiteMaster(dut, "AXIML", dut.clk)
    video_in = AXI4StreamSlave(dut, "AXISS", dut.clk, width=24)

    setup_dut(dut)
    yield Timer(CLK_PERIOD * 10)
    dut.rst <= 0
    yield Timer(CLK_PERIOD * 10)
    dut.log.info("Ready")
    yield Timer(CLK_PERIOD * 300)

    control = 0x00
    control |= 1 << BIT_CTRL_EN
    yield axim.write(REG_CONTROL, control)
    yield Timer(CLK_PERIOD * 10)

    #Write a characer down
    char_val = 0x0101
    yield axim.write(REG_CONSOLE_CHAR, char_val)
    yield Timer(CLK_PERIOD * 10)
```


Overlapping Code:
```
.test(skip = False)
def write_char_test(dut):
"""
Description:
Test ID: 7
Expected Results:
**
"""
dut.rst <= 1
dut.test_id <= 7
axim = AXI4LiteMaster(dut, "AXIML", dut.clk)
video_in = AXI4StreamSlave(dut, "AXISS", dut.clk, width=24)
setup_dut(dut)
yield Timer(CLK_PERIOD * 10)
dut.rst <= 0
yield Timer(CLK_PERIOD * 10)
dut.log.info("Ready")
yield Timer(CLK_PERIOD * 300)
control = 0x00
control |= 1 << BIT_CTRL_EN
yield axim.write(REG_CONTROL, control)
yield Timer(CLK_PERIOD * 10)
#Write a characer down
char_val = 0x0101
yield axim.write(REG_CONSOLE_CHAR, char_val)
yield Timer(CLK_PERIOD * 
```
<Overlap Ratio: 0.9834437086092715>

---

--- 180 --
Question ID: 32bca387c950131122c8810c44812e28258f86fb_0
Original Code:
```
def test_timer():
    with util.time.timer() as t:
        time.sleep(1e-6)
    assert t['seconds'] > 0
```


Overlapping Code:
```
h util.time.timer() as t:
time.sleep(1e-6)
assert 
```
<Overlap Ratio: 0.5747126436781609>

---

--- 181 --
Question ID: 14acda7eaaf974f831b8ad931b14001810580688_6
Original Code:
```
def make_difficulty_bar(df, sheet):
    plt.figure()
    series = df.loc[sheet]
    labels, vals = series.index, series.values
    plt.bar(labels, vals)
    plt.title(f"Perceived difficulty of sheet {sheet}")
```


Overlapping Code:
```
ty_bar(df, sheet):
plt.figure()
series = df.loc[sheet]
labels, vals = series.index, series.values
plt.bar(labels, vals)
plt.title(f"Perceived difficul
```
<Overlap Ratio: 0.7978723404255319>

---

--- 182 --
Question ID: 2323f08443b78e0cd0e6f50fb5ed203b47fefb0c_3
Original Code:
```
def lab2rgb(lab):
    xyz = lab2xyz(lab)
    rgb = xyz2rgb(xyz)
    return [round(val) for val in rgb]
```


Overlapping Code:
```

```
<Overlap Ratio: 0.0>

---

--- 183 --
Question ID: ebbf2e42ee03772b9cfb6f2e07d80134666fd0de_0
Original Code:
```
def save_model_to_hdf5_group(grp: h5py.Group, net: torch.nn.Module):
    # this will work regardless whether
    for x, y in net.named_parameters():
        if x not in grp:
            # currently, it's not big. So I don't do compression at all, for speed.
            grp.create_dataset(x, data=y.data.cpu().numpy())
            grp.file.flush()
```


Overlapping Code:
```
def save_model_to_hdf5_group(grp: h5py.Group, net: torch.nn.Module):
# this will work regardless whether
for x, y in net.named_parameters():
if x not in grp:
# currently, it's not big. So I don't do compression at all, for speed.
grp.create_dataset(x
```
<Overlap Ratio: 0.847457627118644>

---

--- 184 --
Question ID: 12ce798abf33b127100da403940af825afc78b5b_8
Original Code:
```
def test_document_where():
    with patch(f"{__name__}.Document1.collection_ref") as mock_collection_ref:
        stream = Mock()
        stream.stream.return_value = snapshot_generator()
        ref = Mock()
        ref.where.return_value = stream
        mock_collection_ref.return_value = ref
        docs = list(Document1.where("str_attr", "==", "foo", None).stream())
        assert len(docs) == 1
        doc = docs.pop()
        assert doc.dict() == mock_data(with_id=True)
```


Overlapping Code:
```
h patch(f"{__name__}.Document1.collection_ref") as mock_collection_ref:
stream = Mock()
stream.stream.return_value = snapshot_generator()
ref = Mock()
ref.where.return_value = stream
mock_collection_ref.return_value = ref
docs = list(Document1.where("str_attr", "==", "foo", None).stream())
assert len(docs) == 1
doc = docs.pop()
assert doc.dict() ==
```
<Overlap Ratio: 0.8663366336633663>

---

--- 185 --
Question ID: abc8b8aad9073f9b6658bee489bd7146e5d7c3ad_1
Original Code:
```
def get_earliest_sct(xcert):
    """Calculate the earliest time this certificate was logged to a CT log.

    If it was not logged by the CA, then the not_before time is returned.

    Arguments:
    xcert -- an x509 certificate object

    Returns (datetime, bool):
        datetime: the earliest calculated date.
        bool: True if an SCT was used, False otherwise

    """
    try:
        earliest = datetime.max
        scts = xcert.extensions.get_extension_for_class(
            x509.PrecertificateSignedCertificateTimestamps
        ).value
        for sct in scts:
            earliest = min(earliest, sct.timestamp)
        return earliest, True
    except x509.extensions.ExtensionNotFound:
        return xcert.not_valid_before, False
```


Overlapping Code:
```
rliest time this certificate was logged to a CT log.
If it was not logged by the CA, then the not_before time is returned.
Arguments:
xcert -- an x509 certificate object
Returns (datetime, bool):
datetime: the earliest calculated date.
bool: True if an SCT was used, False otherwise
"""
try:
earliest = datetime.max
scts = xcert.extensions.get_extension_for_class(
x509.PrecertificateSignedCertificateTimestamps
).value
for sct in scts:
earliest = min(earliest, sct.timestamp)
return earliest, True
except x509.extensions.ExtensionNotFound:
return xc
```
<Overlap Ratio: 0.88>

---

--- 186 --
Question ID: e7fe69ac6c25e8c5c536abbb50fb17f89cffa8f7_0
Original Code:
```
def _interpret_nth(interpreter: Interpreter, actual_params: tp.List[ast.AST], idx: int) -> d.Data:
    param_value = interpreter.visit(actual_params[0])
    param_type = type(param_value)

    if not issubclass(param_type, d.List) or len(param_value) <= idx:
        raise err.ArgumentTypeError(
            expected=d.List,
            given=param_value,
            min_length=idx+1,
            max_length=None
        )

    result = param_value[idx]
    return result
```


Overlapping Code:
```
(interpreter: Interpreter, actual_params: tp.List[ast.AST], idx: int) -> d.Data:
param_value = interpreter.visit(actual_params[0])
param_type = type(param_value)
if not issubclass(param_type, d.List) or len(param_value) <= idx:
raise err.ArgumentTypeError(
expected=d.List,
given=param_value,
min_length=idx+1,
max_length=None
)
result = param_value[idx
```
<Overlap Ratio: 0.9145077720207254>

---

--- 187 --
Question ID: 5f9b67a19d8463a079a3e7e4abc256e61e5d35c6_22
Original Code:
```
def groupdel(cmd, grp, loc_namenode):
    """Allow to execute groupdel command.
    
    Parameters
    ----------
    cmd --> str, the command
    grp --> list, the list of groups to which the user belongs
    loc_namenode --> str, the master namenode in the moment in which the command has been invoked
    
    Returns
    -------
    None
    """
    f, required_by, group = cmd.split()
    #call the groupdel command with a rpc
    with xmlrpc.client.ServerProxy(loc_namenode) as proxy:
        try:
            res = proxy.groupdel(required_by, group) #no print
        except xmlrpc.client.Fault as err:
            #the user is not allowed to delete the group
            if 'RootNecessaryException' in err.faultString:
                logging.warning(err.faultString)
            #the group the user want to delete does not exist
            if 'GroupNotFoundException' in err.faultString:
                logging.warning(err.faultString)
            #the group the user want to delete is the main group of a user (must delete the user first)
            if 'MainUserGroupException' in err.faultString:
                logging.warning(err.faultString)
    return
```


Overlapping Code:
```
amenode):
"""Allow to execute groupdel command.

Parameters
----------
cmd --> str, the command
grp --> list, the list of groups to which the user belongs
loc_namenode --> str, the master namenode in the moment in which the command has been invoked

Returns
-------
None
"""
f, required_by, group = cmd.split()
#call the groupdel command with a rpc
with xmlrpc.client.ServerProxy(loc_namenode) as proxy:
try:
res = proxy.groupdel(required_by, group) #no print
except xmlrpc.client.Fault as err:
#the user is not allowed to delete the group
if 'RootNecessaryException' in err.faultString:
logging.warning(err.faultString)
#the group the user want to delete does not exist
if 'GroupNotFoundException' in err.faultString:
logging.warning(err.faultString)
#the group the user want to delete is the main group of a user (must delete the user first)
if 'MainUserGroupException' in err.faultString:
logging.warning(err.faultString)
return
```
<Overlap Ratio: 0.9708029197080292>

---

--- 188 --
Question ID: b26d22aa84a8cfc772d57eb1f17690ed81271c6e_7
Original Code:
```
def nested_if_pg_test():
    s = '''
    proctype p(){
        bit x;
        if
        :: if
           :: true
           :: false
           fi
        :: x
        fi
    }
    '''
    tree = parser.parse(s)
    g = tree[0].to_pg()
    dump(g)
    h = nx.MultiDiGraph()
    h.add_edges_from([(0, 1), (0, 1), (0, 1)])
    assert nx.is_isomorphic(g, h)
```


Overlapping Code:
```
pg_test():
s = '''
proctype p(){
bit x;
if
:: if
:: true
:: false
fi
:: x
fi
}
'''
tree = parser.parse(s)
g = tree[0].to_pg()
dump(g)
h = nx.MultiDiGraph()
h.add_edges_from([(0, 1), (0, 1), (0, 1)])
a
```
<Overlap Ratio: 0.8264462809917356>

---

--- 189 --
Question ID: df8ffbba8b5e866c8e52242d4e473423a24171bf_0
Original Code:
```
def add_follower(page_name=None):
    """

    @param page_name:
    @return: @rtype:
    """
    error = list()

    # get the id of the user you want to follow
    if request.method == 'GET':
        # via ajax
        message = request.args.get('follow-user-id', '', type=str)
    else:
        # via no javascript post
        message = request.form['follow-user-id']

    if 'user' in session:
        dbh = MessageHelper(StrictRedis())
        message_object = Message(session['user']['id'], message)
        dbh.post_message(message_object)

    else:
        return redirect(url_for('login'))

    if request.method == 'GET':
        return jsonify(post_time=message_object.posted_time,
                       format_time=message_object.formatted_time,
                       msg_id=message_object.id)

    return redirect(url_for('dash'))
```


Overlapping Code:
```
age_name=None):
"""
@param page_name:
@return: @rtype:
"""
error = list()
# get the id of the user you want to follow
if request.method == 'GET':
# via ajax
message = request.args.get('follow-user-id', '', type=str)
else:
# via no javascript post
message = request.form['follow-user-id']
if 'user' in session:
dbh = MessageHelper(StrictRedis())
message_object = Message(session['user']['id'], message)
dbh.post_message(message_object)
else:
return redirect(url_for('login'))
if request.method == 'GET':
return jsonify(post_time=message_object.posted_time,
format_time=message_object.formatted_time,
msg_id=message_object.id)
return 
```
<Overlap Ratio: 0.9362962962962963>

---

--- 190 --
Question ID: f4e51510ce220d2328c476c7c1ebe7f2984840cd_3
Original Code:
```
def get_kw(string):
    """Return all the keywords in a string."""
    try:
        keys = keywords(string)
        keys = keys['keywords']
        key_list = []
        for i in range(len(keys)):
            key_list.append(keys[i]['keyword'])
        return key_list

    except:
        print("Error in ", string)
```


Overlapping Code:
```
Return all the keywords in a string."""
try:
keys = keywords(string)
keys = keys['keywords']
key_list = []
for i in range(len(keys)):
key_list.append(keys[i]['keyword'])
return key_list
except:
print(
```
<Overlap Ratio: 0.823045267489712>

---

--- 191 --
Question ID: 4b26ec02cc4c451cfd66418e36858cfb578b80da_0
Original Code:
```
def get_mimetype_from_filename(filename):
    mimetype = 'application/octet-stream'
    if filename:
        detected_mimetype = mimetypes.guess_type(filename)
        if detected_mimetype[0]:
            mimetype = detected_mimetype[0]
    return mimetype
```


Overlapping Code:
```
ename):
mimetype = 'application/octet-stream'
if filename:
detected_mimetype = mimetypes.guess_type(filename)
if detected_mimetype[0]:
mimetype = dete
```
<Overlap Ratio: 0.6944444444444444>

---

--- 192 --
Question ID: 05ec34444bf0054b5764c6af03d7745638ec37c6_0
Original Code:
```
def main():
  """main entry point for module execution
  """

  argument_spec = dict(gather_subset=dict(default=['!config'], type='list'))

  argument_spec.update(fujitsu_srs_argument_spec)

  module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True)

  gather_subset = module.params['gather_subset']

  runable_subsets = set()
  exclude_subsets = set()

  for subset in gather_subset:
    if subset == 'all':
      runable_subsets.update(VALID_SUBSETS)
      continue

    if subset.startswith('!'):
      subset = subset[1:]
      if subset == 'all':
        exclude_subsets.update(VALID_SUBSETS)
        continue
      exclude = True
    else:
      exclude = False

    if subset not in VALID_SUBSETS:
      module.fail_json(msg='Bad subset')

    if exclude:
      exclude_subsets.add(subset)
    else:
      runable_subsets.add(subset)

  if not runable_subsets:
    runable_subsets.update(VALID_SUBSETS)

  runable_subsets.difference_update(exclude_subsets)
  runable_subsets.add('default')

  facts = dict()
  facts['gather_subset'] = list(runable_subsets)

  instances = list()
  for key in runable_subsets:
    instances.append(FACT_SUBSETS[key](module))

  for inst in instances:
    inst.populate()
    facts.update(inst.facts)

  # prepend 'ansible_net_' to the facts key
  ansible_facts = dict()
  for key, value in iteritems(facts):
    key = 'ansible_net_%s' % key
    ansible_facts[key] = value

  warnings = list()
  check_args(module, warnings)

  module.exit_json(ansible_facts=ansible_facts, warnings=warnings)
```


Overlapping Code:
```
def main():
"""main entry point for module execution
"""
argument_spec = dict(gather_subset=dict(default=['!config'], type='list'))
argument_spec.update(fujitsu_srs_argument_spec)
module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True)
gather_subset = module.params['gather_subset']
runable_subsets = set()
exclude_subsets = set()
for subset in gather_subset:
if subset == 'all':
runable_subsets.update(VALID_SUBSETS)
continue
if subset.startswith('!'):
subset = subset[1:]
if subset == 'all':
exclude_subsets.update(VALID_SUBSETS)
continue
exclude = True
else:
exclude = False
if subset not in VALID_SUBSETS:
module.fail_json(msg='Bad subset')
if exclude:
exclude_subsets.add(subset)
else:
runable_subsets.add(subset)
if not runable_subsets:
runable_subsets.update(VALID_SUBSETS)
runable_subsets.difference_update(exclude_subsets)
runable_subsets.add('default')
facts = dict()
facts['gather_subset'] = list(runable_subsets)
instances = list()
for key in runable_subsets:
instances.append(FACT_SUBSETS[key](module))
for inst in instances:
inst.populate()
facts.update(inst.facts)
# prepend 'ansible_net_' to the facts key
ansible_facts = dict()
for key, value in iteritems(facts):
key = 'ansible_net_%s' % key
ansible_facts[key] = value
warnings = list()
check_args(module, warnings)
module.exit_json(ansible_facts=ansible_facts, warnings=warni
```
<Overlap Ratio: 0.997080291970803>

---

--- 193 --
Question ID: a27cc4ec14928624b14756f50405a6e45c0b9530_0
Original Code:
```
def ask(message, strict=False, quiet=False):
    if quiet:
        return True
    yesno = 'YES/NO' if strict else 'y/N'
    negatives = ('NO', 'N', 'n', 'no', '')
    affirmatives = ('YES',) if strict else ('y', 'Y', 'yes')
    acceptable_options = affirmatives + negatives

    response = ask_option(message, yesno.split('/'), acceptable_options)
    return response in affirmatives
```


Overlapping Code:
```
f quiet:
return True
yesno = 'YES/NO' if strict else 'y/N'
negatives = ('NO', 'N', 'n', 'no', '')
affirmatives = ('YES',) if strict else ('y', 'Y', 'yes')
acceptable_options = affirmatives + negatives
response = ask_option(message, yesno.split('/'), acceptable_options)
return response in affirmative
```
<Overlap Ratio: 0.8645533141210374>

---

--- 194 --
Question ID: b0ae4cac8bede0601e22c62d2c062444e520731d_1
Original Code:
```
def colony_extractor():
    result=re.findall(r'\w+',rep)
    a=[]
    for i in range(len(result)):
        a.append(result[i])
    for i in range(len(result)):
        b = result[i]
        for j in range(len(result)):
            if i+j+1<len(result):
                b=b+' '+result[i+j+1]
                a.append(b)
    for x in range(len(a)):
        if a[x].lower() in colonies:
            return a[x]
```


Overlapping Code:
```

```
<Overlap Ratio: 0.0>

---

--- 195 --
Question ID: 81ed0c1ade77b867a5874ae9058d2ef67df9da45_3
Original Code:
```
async def update_channels_stats(DatabaseConn: VTBiliDatabase, dataset_set: list):
    vtlog.info("Collecting channel UUIDs")
    channels_uids = []
    for chan in dataset_set:
        vtlog.debug(f"Opening: {chan}")
        async with aiofiles.open(chan, "r", encoding="utf-8") as fp:
            dds = ujson.loads(await fp.read())
        vtlog.debug(f"Total data: {len(dds)}")
        for nn, dd in enumerate(dds):
            channels_uids.append({"id": dd["id"], "uid": dd["uid"], "num": nn})

    vtlog.info("Processing...")
    final_data = await main_process_loop(channels_uids)
    vtlog.info("Updating DB data for Hololive...")
    try:
        await asyncio.wait_for(
            DatabaseConn.update_data("hololive_data", {"channels": final_data["hololive"]}), 15.0
        )
    except asyncio.TimeoutError:
        await DatabaseConn.release()
        DatabaseConn.raise_error()
        vtlog.error("Failed to update Hololive channels data, timeout by 15s...")
    vtlog.info("Updating DB data for Nijisanji...")
    try:
        await asyncio.wait_for(
            DatabaseConn.update_data("nijisanji_data", {"channels": final_data["nijisanji"]}), 15.0
        )
    except asyncio.TimeoutError:
        await DatabaseConn.release()
        DatabaseConn.raise_error()
        vtlog.error("Failed to update Nijisanji channels data, timeout by 15s...")
    vtlog.info("Updating DB data for Others...")
    try:
        await asyncio.wait_for(
            DatabaseConn.update_data("otherbili_data", {"channels": final_data["other"]}), 15.0
        )
    except asyncio.TimeoutError:
        await DatabaseConn.release()
        DatabaseConn.raise_error()
        vtlog.error("Failed to update Others channels data, timeout by 15s...")
```


Overlapping Code:
```
els_stats(DatabaseConn: VTBiliDatabase, dataset_set: list):
vtlog.info("Collecting channel UUIDs")
channels_uids = []
for chan in dataset_set:
vtlog.debug(f"Opening: {chan}")
async with aiofiles.open(chan, "r", encoding="utf-8") as fp:
dds = ujson.loads(await fp.read())
vtlog.debug(f"Total data: {len(dds)}")
for nn, dd in enumerate(dds):
channels_uids.append({"id": dd["id"], "uid": dd["uid"], "num": nn})
vtlog.info("Processing...")
final_data = await main_process_loop(channels_uids)
vtlog.info("Updating DB data for Hololive...")
try:
await asyncio.wait_for(
DatabaseConn.update_data("hololive_data", {"channels": final_data["hololive"]}), 15.0
)
except asyncio.TimeoutError:
await DatabaseConn.release()
DatabaseConn.raise_error()
vtlog.error("Failed to update Hololive channels data, timeout by 15s...")
vtlog.info("Updating DB data for Nijisanji...")
try:
await asyncio.wait_for(
DatabaseConn.update_data("nijisanji_data", {"channels": final_data["nijisanji"]}), 15.0
)
except asyncio.TimeoutError:
await DatabaseConn.release()
DatabaseConn.raise_error()
vtlog.error("Failed to update Nijisanji channels data, timeout by 15s...")
vtlog.info("Updating DB data for Others...")
try:
await asyncio.wait_for(
DatabaseConn.update_data("otherbili_data", {"channels": final_data["other"]}), 15.0
)
except asyncio.TimeoutError:
await DatabaseConn.release()
DatabaseConn.raise_error()
vtlog.error("Failed to update Others channels data, timeout by 15s.
```
<Overlap Ratio: 0.9823848238482384>

---

--- 196 --
Question ID: ac8b701ffc5ac7b2621a9dfad531c11bb42c6c0f_4
Original Code:
```
def text_similarity(text_1, text_2):
    #tokenize, get unique words and clean data.
    text_1 = clean_text(text_1)
    text_2 = clean_text(text_2)

    #word-to-word translation
    if args.translate:
        print("Translating data")
        text_1 = translate(text_1)
        text_2 = translate(text_2)
    else:
        print("Text already in english")
        text_1 = [[word] for word in text_1] #convert in an list of list
        text_2 = [[word] for word in text_2] #convert in an list of list

    score_wordnet = 0
    score_yago = 0 
    cnt_wordnet = 0
    cnt_yago = 0
    if args.debug:
        print(len(text_1),len(text_2))
    for xx, t in enumerate(text_1):
        if args.debug:
            print('-->', xx, t)
        score_wordnet_r, score_yago_r, ok_wordnet, ok_yago = word_similarity(t, text_2)
        if ok_wordnet:
            if args.debug:
                print("++", cnt_wordnet)
            score_wordnet += score_wordnet_r
            cnt_wordnet += 1
        if ok_yago:
            if args.debug:
                print("--", cnt_yago)
            score_yago += score_yago_r
            cnt_yago += 1
    if args.debug:
        print("Done with text 1")

    for xx, t in enumerate(text_2):
        if args.debug:
            print('-->', xx, t)
        score_wordnet_r, score_yago_r, ok_wordnet, ok_yago = word_similarity(t, text_1)
        if ok_wordnet:
            if args.debug:
                print("++", cnt_wordnet)
            score_wordnet += score_wordnet_r
            cnt_wordnet += 1
        if ok_yago:
            if args.debug:
                print("--", cnt_yago)
            score_yago += score_yago_r
            cnt_yago += 1
    if args.debug:
        print("Done with text 2")

    if cnt_wordnet != 0:
        score_wordnet /= cnt_wordnet
    if cnt_yago != 0:
        score_yago /= cnt_yago
    return score_wordnet, score_yago
```


Overlapping Code:
```
ize, get unique words and clean data.
text_1 = clean_text(text_1)
text_2 = clean_text(text_2)
#word-to-word translation
if args.translate:
print("Translating data")
text_1 = translate(text_1)
text_2 = translate(text_2)
else:
print("Text already in english")
text_1 = [[word] for word in text_1] #convert in an list of list
text_2 = [[word] for word in text_2] #convert in an list of list
score_wordnet = 0
score_yago = 0 
cnt_wordnet = 0
cnt_yago = 0
if args.debug:
print(len(text_1),len(text_2))
for xx, t in enumerate(text_1):
if args.debug:
print('-->', xx, t)
score_wordnet_r, score_yago_r, ok_wordnet, ok_yago = word_similarity(t, text_2)
if ok_wordnet:
if args.debug:
print("++", cnt_wordnet)
score_wordnet += score_wordnet_r
cnt_wordnet += 1
if ok_yago:
if args.debug:
print("--", cnt_yago)
score_yago += score_yago_r
cnt_yago += 1
if args.debug:
print("Done with text 1")
for xx, t in enumerate(text_2):
if args.debug:
print('-->', xx, t)
score_wordnet_r, score_yago_r, ok_wordnet, ok_yago = word_similarity(t, text_1)
if ok_wordnet:
if args.debug:
print("++", cnt_wordnet)
score_wordnet += score_wordnet_r
cnt_wordnet += 1
if ok_yago:
if args.debug:
print("--", cnt_yago)
score_yago += score_yago_r
cnt_yago += 1
if args.debug:
print("Done with text 2")
if cnt_wordnet != 0:
score_wordnet /= cnt_wordnet
if cnt_yago != 0:
score_yago /= cnt_y
```
<Overlap Ratio: 0.9447165850244926>

---

--- 197 --
Question ID: 5fde36e6a35655aa406e77611971ec0c4c646953_11
Original Code:
```
def vectorize_array(f, array, *args):
    """ Helper function to vectorize across the rows of a 2D numpy array

    :params f: function to vectorize
    :params array: 2darray to iterate over.
    :params args: list of arguments to pass to the function f
    :returns: ndarray of the results of applying the function to each row.
    """
    return np.array([f(row, *args) for row in array])
```


Overlapping Code:
```
rize_array(f, array, *args):
""" Helper function to vectorize across the rows of a 2D numpy array
:params f: function to vectorize
:params array: 2darray to iterate over.
:params args: list of arguments to pass to the function f
:returns: ndarray of the results of applying the function to each row.
"""
return np.array([f(row, *args) for row in arra
```
<Overlap Ratio: 0.9668508287292817>

---

--- 198 --
Question ID: de10d9e95e585f0825d05fe6aa56c200dd42c475_2
Original Code:
```
def _lemma(doc, remove_stop):
    if remove_stop:
        lemmatized = [
            process_token(
                str(token), token._.iwnlp_lemmas, token.pos_, token.whitespace_
            )
            for token in doc
            if not token.is_stop
        ]
    else:
        lemmatized = [
            process_token(
                str(token), token._.iwnlp_lemmas, token.pos_, token.whitespace_
            )
            for token in doc
        ]
    return "".join(lemmatized)
```


Overlapping Code:
```
a(doc, remove_stop):
if remove_stop:
lemmatized = [
process_token(
str(token), token._.iwnlp_lemmas, token.pos_, token.whitespace_
)
for token in doc
if not token.is_stop
]
else:
lemmatized = [
process_token(
str(token), token._.iwnlp_lemmas, token.pos_, token.whitespace_
)
for token in doc
]
return "".join(lemmatize
```
<Overlap Ratio: 0.9665653495440729>

---

--- 199 --
Question ID: 43ae8701be6db11e210380b41a09e72c18c9dfe8_1306
Original Code:
```
@commandWrap
def CreateEmptyUVSetOptions(*args, **kwargs):
    u""":rtype: list|str|DagNode|AttrObject|ArrayAttrObject|Components1Base"""
    return cmds.CreateEmptyUVSetOptions(*args, **kwargs)
```


Overlapping Code:
```
gs):
u""":rtype: list|str|DagNode|AttrObject|Array
```
<Overlap Ratio: 0.26881720430107525>

---

--- 200 --
Question ID: 44e92408042831c3a6a471f99d878ae32e8d1431_0
Original Code:
```
async def ses_webhook(request):
    if not compare_digest(request.app['settings'].ses_url_token, request.match_info['token']):
        raise JsonErrors.HTTPForbidden('invalid url')

    # content type is plain text for SNS, so we have to decode json manually
    try:
        data = json.loads(await request.text())
    except ValueError:
        raise JsonErrors.HTTPBadRequest('invalid json')

    # avoid keeping multiple copies of the request data in memory
    request._read_byte = None

    await verify_sns(request.app, data)

    sns_type = data['Type']
    if sns_type == 'SubscriptionConfirmation':
        logger.info('confirming aws Subscription')
        # TODO check we actually want this subscription
        async with request.app['http_client'].head(data['SubscribeURL'], raise_for_status=True):
            pass
    else:
        assert sns_type == 'Notification', sns_type
        raw_msg = data['Message']
        # TODO any other messages to ignore?
        if raw_msg != 'Successfully validated SNS topic for Amazon SES event publishing.':
            message = json.loads(raw_msg)
            del data
            if message.get('notificationType') == 'Received':
                await asyncio.shield(_record_email_message(request, message))
            else:
                await asyncio.shield(_record_email_event(request, message))
    return Response(status=204)
```


Overlapping Code:
```
st):
if not compare_digest(request.app['settings'].ses_url_token, request.match_info['token']):
raise JsonErrors.HTTPForbidden('invalid url')
# content type is plain text for SNS, so we have to decode json manually
try:
data = json.loads(await request.text())
except ValueError:
raise JsonErrors.HTTPBadRequest('invalid json')
# avoid keeping multiple copies of the request data in memory
request._read_byte = None
await verify_sns(request.app, data)
sns_type = data['Type']
if sns_type == 'SubscriptionConfirmation':
logger.info('confirming aws Subscription')
# TODO check we actually want this subscription
async with request.app['http_client'].head(data['SubscribeURL'], raise_for_status=True):
pass
else:
assert sns_type == 'Notification', sns_type
raw_msg = data['Message']
# TODO any other messages to ignore?
if raw_msg != 'Successfully validated SNS topic for Amazon SES event publishing.':
message = json.loads(raw_msg)
del data
if message.get('notificationType') == 'Received':
await asyncio.shield(_record_email_message(request, message))
else:
await asyncio.shield(_record_email_event(req
```
<Overlap Ratio: 0.9401709401709402>

---

--- 201 --
Question ID: 13a7e5434cc83bfa96ea6615181e8a978e0e4ee0_3
Original Code:
```
@commands.command("run-local")
@cli_args.MODEL_PATH
@cli_args.RUN_ID
@click.option("--port", "-p", default=5000, help="Server port. [default: 5000]")
@click.option("--image", "-i", default=IMAGE, help="Docker image name")
@click.option("--flavor", "-f", default=None,
              help=("The name of the flavor to use for local serving. Must be one of the following:"
                    " {supported_flavors}. If unspecified, a flavor will be automatically selected"
                    " from the model's available flavors.".format(
                        supported_flavors=mlflow.sagemaker.SUPPORTED_DEPLOYMENT_FLAVORS)))
def run_local(model_path, run_id, port, image, flavor):
    """
    Serve model locally running in a Sagemaker-compatible Docker container.
    """
    mlflow.sagemaker.run_local(
        model_path=model_path, run_id=run_id, port=port, image=image, flavor=flavor)
```


Overlapping Code:
```
EL_PATH
@cli_args.RUN_ID
@click.option("--port", "-p", default=5000, help="Server port. [default: 5000]")
@click.option("--image", "-i", default=IMAGE, help="Docker image name")
@click.option("--flavor", "-f", default=None,
help=("The name of the flavor to use for local serving. Must be one of the following:"
" {supported_flavors}. If unspecified, a flavor will be automatically selected"
" from the model's available flavors.".format(
supported_flavors=mlflow.sagemaker.SUPPORTED_DEPLOYMENT_FLAVORS)))
def run_local(model_path, run_id, port, image, flavor):
"""
Serve model locally running in a Sagemaker-compatible Docker container.
"""
mlflow.sagemaker.run_local(
model_path=model_path, run_id=run
```
<Overlap Ratio: 0.8897338403041825>

---

--- 202 --
Question ID: 5abc01d8c1be04b67c4bdf9afcd108c605937d33_8
Original Code:
```
def test_ha_should_mark_one_user_on_email_mark_page(client):
    helpers.create_health_authority(client)
    helpers.login_authority(client)
    helpers.create_user(client)

    user = get_user_by_id(1)
    res = client.post(
        "/marks/new",
        data={"identifier": user["email"], "duration": 15},
        follow_redirects=False,
    )

    user = get_user_by_id(1)

    assert user["marked"]
    assert res.status_code == 302
```


Overlapping Code:
```
should_mark_one_user_on_email_mark_page(client):
helpers.create_health_authority(client)
helpers.login_authority(client)
helpers.create_user(client)
user = get_user_by_id(1)
res = client.post(
"/marks/new",
data={"identifier": user["email"], "duration": 15},
follow_redirects=False,
)
user = get_user_by_id(1)
assert user["marked"]
assert res.status_
```
<Overlap Ratio: 0.938337801608579>

---

--- 203 --
Question ID: a95af293f0db3ab6a81d94fa0a9fc9af58b9e376_4
Original Code:
```
@pytest.fixture
def setup_and_finalize(request, CTX):
    # Clear City collection before test
    _delete_all(collection_name="City", CTX=CTX)

    # Clear City collection after test
    def fin():
        _delete_all(collection_name="City", CTX=CTX)

    request.addfinalizer(fin)
```


Overlapping Code:
```
ixture
def setup_and_finalize(request, CTX):
# Clear City collection before test
_delete_all(collection_name="City", CTX=CTX)
# Clear City collection after test
def fin():
_delete_all(collection_name="City", CTX=CTX)
request.addf
```
<Overlap Ratio: 0.9123505976095617>

---

--- 204 --
Question ID: b9fb27243f961eaa90ac630fd530777809ae9a39_4
Original Code:
```
@pytest.mark.parametrize("input_shape", input_shapes)
def test_concat_attributes_saved_during_graph_building(input_shape):
    model = ModelForTestWithReshapeFlattenAndConcat()
    input_info = ModelInputInfo(input_shape)
    graph_builder = GraphBuilder(create_dummy_forward_fn([input_info, ], with_input_tracing=True,
                                                         with_output_tracing=True))
    graph = graph_builder.build_graph(model)
    cat_nodes_with_attributes = {
        'ModelForTestWithReshapeFlattenAndConcat/cat_0': {'axis': 1},
        'ModelForTestWithReshapeFlattenAndConcat/cat_1': {'axis': 6},
        'ModelForTestWithReshapeFlattenAndConcat/cat_2': {'axis': 1},
        'ModelForTestWithReshapeFlattenAndConcat/stack_0': None,
        'ModelForTestWithReshapeFlattenAndConcat/stack_1': None
    }

    for node in graph.get_all_nodes():
        if node.metatype is PTCatMetatype:
            assert node.node_name in cat_nodes_with_attributes
            if isinstance(node.layer_attributes, MultipleInputLayerAttributes):
                assert node.layer_attributes.axis == cat_nodes_with_attributes[node.node_name]['axis']
            else:
                assert node.layer_attributes is None
                assert cat_nodes_with_attributes[node.node_name] is None
```


Overlapping Code:
```
pytest.mark.parametrize("input_shape", input_shapes)
def test_concat_attributes_saved_during_graph_building(input_shape):
model = ModelForTestWithReshapeFlattenAndConcat()
input_info = ModelInputInfo(input_shape)
graph_builder = GraphBuilder(create_dummy_forward_fn([input_info, ], with_input_tracing=True,
with_output_tracing=True))
graph = graph_builder.build_graph(model)
cat_nodes_with_attributes = {
'ModelForTestWithReshapeFlattenAndConcat/cat_0': {'axis': 1},
'ModelForTestWithReshapeFlattenAndConcat/cat_1': {'axis': 6},
'ModelForTestWithReshapeFlattenAndConcat/cat_2': {'axis': 1},
'ModelForTestWithReshapeFlattenAndConcat/stack_0': None,
'ModelForTestWithReshapeFlattenAndConcat/stack_1': None
}
for node in graph.get_all_nodes():
if node.metatype is PTCatMetatype:
assert node.node_name in cat_nodes_with_attributes
if isinstance(node.layer_attributes, MultipleInputLayerAttributes):
assert node.layer_attributes.axis == cat_nodes_with_attributes[node.node_name]['axis']
else:
assert node.layer_attributes is None
assert cat_nodes_with_attributes[node.node_name] is Non
```
<Overlap Ratio: 0.9981515711645101>

---

--- 205 --
Question ID: 8d600c9ed6b041bdeb712b54fef9597f1b181b1f_6
Original Code:
```
def test_init_existing_project(init_project_fixture, zeff_configuration):
    """Run init on existing project and verify files and contents."""
    env = {}
    args = ["init", "generic"]
    options = zeff.cli.parse_commandline(args, config=zeff_configuration)
    with patch("builtins.input", new=mock_user_input) as mock_input:
        with patch("zeff.cloud.dataset.Dataset.create_dataset") as create:
            dataset = MagicMock(spec=Dataset)
            dataset.dataset_id = "mock.dataset_id"
            create.return_value = dataset
            init_project(options)

    def change_user_input(prompt=None):
        mo = re.match(r"(?P<pstr>.+?) \[(?P<dstr>.*?)\]\?", prompt)
        assert mo is not None, f"Invalid prompt: `{prompt}`"
        response = {
            "Configuration generator init argument": str(
                Path.cwd() / "generator_arg_mock"
            ),
            "Record builder init argument": str(Path.cwd() / "builder_arg_mock"),
        }
        return response.get(mo.groupdict()["pstr"], "")

    with patch("builtins.input", new=change_user_input) as mock_input:
        with patch("zeff.cloud.dataset.Dataset.create_dataset") as create:
            dataset = MagicMock(spec=Dataset)
            dataset.dataset_id = "mock.dataset_id"
            create.return_value = dataset
            init_project(options)
            assert_zeff_conf()
```


Overlapping Code:
```
_project_fixture, zeff_configuration):
"""Run init on existing project and verify files and contents."""
env = {}
args = ["init", "generic"]
options = zeff.cli.parse_commandline(args, config=zeff_configuration)
with patch("builtins.input", new=mock_user_input) as mock_input:
with patch("zeff.cloud.dataset.Dataset.create_dataset") as create:
dataset = MagicMock(spec=Dataset)
dataset.dataset_id = "mock.dataset_id"
create.return_value = dataset
init_project(options)
def change_user_input(prompt=None):
mo = re.match(r"(?P<pstr>.+?) \[(?P<dstr>.*?)\]\?", prompt)
assert mo is not None, f"Invalid prompt: `{prompt}`"
response = {
"Configuration generator init argument": str(
Path.cwd() / "generator_arg_mock"
),
"Record builder init argument": str(Path.cwd() / "builder_arg_mock"),
}
return response.get(mo.groupdict()["pstr"], "")
with patch("builtins.input", new=change_user_input) as mock_input:
with patch("zeff.cloud.dataset.Dataset.create_dataset") as create:
dataset = MagicMock(spec=Dataset)
dataset.dataset_id = "mock.dataset_id"
create.return_value = dataset
init_project(options)
assert_z
```
<Overlap Ratio: 0.9606986899563319>

---

--- 206 --
Question ID: 2a556969e43c4a0ef11b142419ac0b2f9e28455d_5
Original Code:
```
def fold_lines(lines, cols, linespace, word_warp):
    filtered = [line for line in lines if linespace == False or len(lines) > 0]
    out = []
    init = True
    for line in filtered:
        if init:
            init = False
        elif linespace:
            out.append(to_attributed_line(''))
        for l in fold_line(line, cols, word_warp):
            out.append(to_attributed_line(l))
    return out
```


Overlapping Code:
```
(lines, cols, linespace, word_warp):
filtered = [line for line in lines if linespace == False or len(lines) > 0]
out = []
init = True
for line in filtered:
if init:
init = False
elif linespace:
out.append(to_attributed_line(''))
for l in fold_line(line, cols, word_warp):
out.append(to_attributed_lin
```
<Overlap Ratio: 0.9090909090909091>

---

--- 207 --
Question ID: 4654a5cbc3f0ec0f604f2153bb14165d5526a3b9_28
Original Code:
```
def _regenerate(
    file_h,
    h,
    pabot_args,
    outs_dir,
    datasources,
    options,
    lines): # type: (Optional[Hashes], Hashes, Dict[str, str], str, List[str], Dict[str, str], List[ExecutionItem]) -> List[ExecutionItem]
    assert(all(isinstance(s, ExecutionItem) for s in lines))
    if (file_h is None or file_h.suitesfrom != h.suitesfrom) \
        and 'suitesfrom' in pabot_args \
        and os.path.isfile(pabot_args['suitesfrom']):
        suites = _suites_from_outputxml(pabot_args['suitesfrom'])
        if file_h is None or file_h.dirs != h.dirs:
            all_suites = generate_suite_names_with_builder(outs_dir, datasources, options)
        else:
            all_suites = [suite for suite in lines if suite]
        suites = _preserve_order(all_suites, suites)
    else:
        suites = generate_suite_names_with_builder(outs_dir, datasources, options)
        if pabot_args.get('testlevelsplit'):
            tests = [] # type: List[TestItem]
            for s in suites:
                tests.extend(s.tests)
            suites = tests
        suites = _preserve_order(suites, [suite for suite in lines if suite])
    if suites:
        store_suite_names(h, suites)
    assert(all(isinstance(s, ExecutionItem) for s in suites))
    return suites
```


Overlapping Code:
```
h,
h,
pabot_args,
outs_dir,
datasources,
options,
lines): # type: (Optional[Hashes], Hashes, Dict[str, str], str, List[str], Dict[str, str], List[ExecutionItem]) -> List[ExecutionItem]
assert(all(isinstance(s, ExecutionItem) for s in lines))
if (file_h is None or file_h.suitesfrom != h.suitesfrom) \
and 'suitesfrom' in pabot_args \
and os.path.isfile(pabot_args['suitesfrom']):
suites = _suites_from_outputxml(pabot_args['suitesfrom'])
if file_h is None or file_h.dirs != h.dirs:
all_suites = generate_suite_names_with_builder(outs_dir, datasources, options)
else:
all_suites = [suite for suite in lines if suite]
suites = _preserve_order(all_suites, suites)
else:
suites = generate_suite_names_with_builder(outs_dir, datasources, options)
if pabot_args.get('testlevelsplit'):
tests = [] # type: List[TestItem]
for s in suites:
tests.extend(s.tests)
suites = tests
suites = _preserve_order(suites, [suite for suite in lines if suite])
if suites:
store_suite_names(h, suites)
assert(all(isinstance(s, ExecutionItem) for s in suites))
re
```
<Overlap Ratio: 0.9691588785046729>

---

--- 208 --
Question ID: 5953f08bea7d8f1e7dd8178cf204d267bdabb64a_2
Original Code:
```
def getJobList(url, jobStatus):
    V100_task_list = []
    P4_task_list = []
    response = requests.get(url).json()['news']
    for t in response:
        #if t['jobname'] != 'PADDLE_DOCKER_BUILD': #需不需要把构建镜像去掉？
        task = {}
        task['CIName'] = t['name']
        task[jobStatus] = t[jobStatus] if t[jobStatus] != None else 0
        task['PR'] = str(t['prid'])
        task['commitId'] = t['commit']
        task['targetId'] = t['bid']
        if jobStatus == 'running':
            task['jobname'] = t['jobname']
        if t['name'].startswith('PR-CI-Py35') or t['name'].startswith(
                'PR-CI-Coverage'):
            V100_task_list.append(task)
        elif t['name'].startswith('PR-CI-CPU-Py2') or t['name'].startswith(
                'PR-CI-Inference'):
            P4_task_list.append(task)
    return V100_task_list, P4_task_list
```


Overlapping Code:
```
ist = []
P4_task_list = []
response = requests.get(url).json()['news']
for t in response:
#if t['jobname'] != 'PADDLE_DOCKER_BUILD': #需不需要把构建镜像去掉？
task = {}
task['CIName'] = t['name']
task[jobStatus] = t[jobStatus] if t[jobStatus] != None else 0
task['PR'] = str(t['prid'])
task['commitId'] = t['commit']
task['targetId'] = t['bid']
if jobStatus == 'running':
task['jobname'] = t['jobname']
if t['name'].startswith('PR-CI-Py35') or t['name'].startswith(
'PR-CI-Coverage'):
V100_task_list.append(task)
elif t['name'].startswith('PR-CI-CPU-Py2') or t['name'].startswith(
'PR-CI-Inference'):
P4_task_list.append(task)
return V100_task_list, P4_task_list
```
<Overlap Ratio: 0.937950937950938>

---

--- 209 --
Question ID: f9dfde5564b274243e48b37f60850b05efef1d65_0
Original Code:
```
def test__SpecialOperatorsDict__set_value():
    key = 'test'
    val = 'value'

    special_operators = _SpecialOperatorsDict()
    assert key not in special_operators

    special_operators._set_value(key, val)
    assert key in special_operators
    assert special_operators[key] == val

    with pytest.raises(ValueError, match='Special operator "test" already exists'):
        special_operators._set_value(key, val)
```


Overlapping Code:
```
t_value():
key = 'test'
val = 'value'
special_operators = _SpecialOperatorsDict()
assert key not in special_operators
special_operators._set_value(key, val)
assert key in special_operators
assert special_operators[key] == val
with pytest.raises(ValueError, match='Special operator "test" already exists'):
```
<Overlap Ratio: 0.8068783068783069>

---

--- 210 --
Question ID: c046cc3578bbb9a00d04c7eb1c123082d3a6fd77_1
Original Code:
```
def GetCheckeredBitmap(blocksize=8,ntiles=4,rgb0=b'\xFF', rgb1=b'\xCC'):
    """Creates a square RGB checkered bitmap using the two specified colors.

    Inputs:
    - blocksize:  the number of pixels in each solid color square
    - ntiles:  the number of tiles along width and height.  Each tile is 2x2 blocks.
    - rbg0,rgb1:  the first and second colors, as 3-byte strings.
                  If only 1 byte is provided, it is treated as a grey value.

    The bitmap returned will have width = height = blocksize*ntiles*2
    """
    size = blocksize*ntiles*2

    if len(rgb0)==1:
        rgb0 = rgb0 * 3
    if len(rgb1)==1:
        rgb1 = rgb1 * 3

    strip0 = (rgb0*blocksize + rgb1*blocksize)*(ntiles*blocksize)
    strip1 = (rgb1*blocksize + rgb0*blocksize)*(ntiles*blocksize)
    band = strip0 + strip1
    data = band * ntiles
    return wx.Bitmap(data, size, size)
```


Overlapping Code:
```
(blocksize=8,ntiles=4,rgb0=b'\xFF', rgb1=b'\xCC'):
"""Creates a square RGB checkered bitmap using the two specified colors.
Inputs:
- blocksize: the number of pixels in each solid color square
- ntiles: the number of tiles along width and height. Each tile is 2x2 blocks.
- rbg0,rgb1: the first and second colors, as 3-byte strings.
If only 1 byte is provided, it is treated as a grey value.
The bitmap returned will have width = height = blocksize*ntiles*2
"""
size = blocksize*ntiles*2
if len(rgb0)==1:
rgb0 = rgb0 * 3
if len(rgb1)==1:
rgb1 = rgb1 * 3
strip0 = (rgb0*blocksize + rgb1*blocksize)*(ntiles*blocksize)
strip1 = (rgb1*blocksize + rgb0*blocksize)*(ntiles*blocksize)
band = strip0 + strip1
data = band * ntiles
return wx.Bitmap(data, size,
```
<Overlap Ratio: 0.9640102827763496>

---

--- 211 --
Question ID: e53074476cdd98e3b43185741b05edc305ab6e9b_6
Original Code:
```
def test_build_examples():
    mock_state = np.array([[0, 1], [2, 2]])
    mock_next_state = np.array([[0, 1], [2, 3]])
    mock_reward = 42
    mock_action = 1  # -> right, second item in prediction array
    mock_action_str = "right"
    game_over = False
    q_update = 1.234

    class MockQNetworkGradientStep:
        def predict(self, state):
            assert (state == mock_state.reshape((1, 2, 2, 1))).all()
            # return 2d array, because we are predicting a "batch" of 1 data item
            return [[1.0,
                     2.0,  # we picked this action
                     -1.0,
                     -1.5]]

        def fit(self, state, q_values, verbose):
            assert (state == mock_state).all()
            assert q_values[0][mock_action] == q_update
            assert q_values[0][0] == 1.0
            assert q_values[0][2] == -1.0
            assert q_values[0][3] == -1.5

    mock_network = MockQNetworkGradientStep()
    agent = dql_agent.DQLAgent(mock_config, mock_network)
    agent.get_q_update = lambda *args: q_update

    input, expected_output = agent.build_training_examples(
        [(mock_state, mock_action_str, mock_reward, mock_next_state, game_over)])
    assert input == [mock_state]
    assert expected_output == [[1.0,
                                q_update,  # we picked this action
                                -1.0,
                                -1.5]]
```


Overlapping Code:
```
state = np.array([[0, 1], [2, 2]])
mock_next_state = np.array([[0, 1], [2, 3]])
mock_reward = 42
mock_action = 1 # -> right, second item in prediction array
mock_action_str = "right"
game_over = False
q_update = 1.234
class MockQNetworkGradientStep:
def predict(self, state):
assert (state == mock_state.reshape((1, 2, 2, 1))).all()
# return 2d array, because we are predicting a "batch" of 1 data item
return [[1.0,
2.0, # we picked this action
-1.0,
-1.5]]
def fit(self, state, q_values, verbose):
assert (state == mock_state).all()
assert q_values[0][mock_action] == q_update
assert q_values[0][0] == 1.0
assert q_values[0][2] == -1.0
assert q_values[0][3] == -1.5
mock_network = MockQNetworkGradientStep()
agent = dql_agent.DQLAgent(mock_config, mock_network)
agent.get_q_update = lambda *args: q_update
input, expected_output = agent.build_training_examples(
[(mock_state, mock_action_str, mock_reward, mock_next_state, game_over)])
assert input == [mock_state]
assert expected_output == [[1.0,

```
<Overlap Ratio: 0.9276437847866419>

---

--- 212 --
Question ID: 53d4f65919872c85a5bbd4228e58cef5098a6ad3_1
Original Code:
```
def _second_argument_type(str_arg):
    second = int(str_arg)
    if second >= 0:
        return second
    else:
        msg = "%r is not a valid second, data range is [0, +inf)" % str_arg
        raise argparse.ArgumentTypeError(msg)
```


Overlapping Code:
```
t_type(str_arg):
second = int(str_arg)
if second >= 0:
return second
else:
msg = "%r is not a valid second, data range is [0, +inf)" % str_arg
raise a
```
<Overlap Ratio: 0.7537688442211056>

---

--- 213 --
Question ID: 02fe1b4f5a53f5cdc4d866927100e6f2eac23547_9
Original Code:
```
def test_datetime_to_millis():
    assert datetime_to_millis(DT_0) == 0
    assert datetime_to_millis(DT) == 1542333064162
    datetime_to_millis(DT_NO_TIMEZONE)  # Depends on system timezone
```


Overlapping Code:
```
is():
assert datetime_to_millis(DT_0) == 0
assert datetime_to_millis(DT) == 1542333064162
datetime_to_millis(DT_NO_TIMEZONE) # Depends on system timez
```
<Overlap Ratio: 0.8426966292134831>

---

--- 214 --
Question ID: bc39e97d66c908e8a4ea6cf66ecdff3dc41c0bcf_1
Original Code:
```
@login_required
def eventoCreate(request):
    form = EventoForm()
    if request.method == 'POST':
        form = EventoForm(request.POST)
        if form.is_valid():
            form.save()
            return redirect('dashboard')

    context = {'form': form}
    return render(request, 'model_form.html', context)
```


Overlapping Code:
```
quired
def eventoCreate(request):
form = EventoForm()
if request.method == 'POST':
form = EventoForm(request.POST)
if form.is_valid():
form.save()
return redirect('dashboard')
context = {'form': form}
return render(request, 'model_form.html', context
```
<Overlap Ratio: 0.9615384615384616>

---

--- 215 --
Question ID: 8a57b60f368b3402ae548707659863eb15e60f59_0
Original Code:
```
def myMain(baseDir):
    """Main function. Run the tests. """
    
    print("Test the error propagation. ")

    from instru import * 
    
    fac = Factory("DemoRootFactory")
    print("Using DemoRootFactory. ")
    
    print("Create module from failer factory")
    failer = fac.select("branch").select("failer").create("failer")

    runModule(failer)

    try:
        waitAll()
    except RuntimeError:
        print("ok, waitAll raised exception")
    else:
        raise RuntimeError("an error should have been raised")
    
    print("End of script failTest.py")
```


Overlapping Code:
```
"Main function. Run the tests. """

print("Test the error propagation. ")
from instru import * 

fac = Factory("DemoRootFactory")
print("Using DemoRootFactory. ")

print("Create module from failer factory")
failer = fac.select("branch").select("failer").create("failer")
runModule(failer)
try:
waitAll()
except RuntimeError:
print("ok, waitAll raised exception")
else:
raise RuntimeError("an error should have b
```
<Overlap Ratio: 0.8526970954356846>

---

--- 216 --
Question ID: 811e1b4c581229632441bd29abb0180fd80abbfa_0
Original Code:
```
def load_entrypoint_plugins(entry_points, airflow_plugins):
    """
    Load AirflowPlugin subclasses from the entrypoints
    provided. The entry_point group should be 'airflow.plugins'.

    :param entry_points: A collection of entrypoints to search for plugins
    :type entry_points: Generator[setuptools.EntryPoint, None, None]
    :param airflow_plugins: A collection of existing airflow plugins to
        ensure we don't load duplicates
    :type airflow_plugins: List[AirflowPlugin]
    :return: List[Type[AirflowPlugin]]
    """
    for entry_point in entry_points:
        log.debug('Importing entry_point plugin %s', entry_point.name)
        plugin_obj = entry_point.load()
        if is_valid_plugin(plugin_obj, airflow_plugins):
            if callable(getattr(plugin_obj, 'on_load', None)):
                plugin_obj.on_load()
                airflow_plugins.append(plugin_obj)
    return airflow_plugins
```


Overlapping Code:
```
trypoint_plugins(entry_points, airflow_plugins):
"""
Load AirflowPlugin subclasses from the entrypoints
provided. The entry_point group should be 'airflow.plugins'.
:param entry_points: A collection of entrypoints to search for plugins
:type entry_points: Generator[setuptools.EntryPoint, None, None]
:param airflow_plugins: A collection of existing airflow plugins to
ensure we don't load duplicates
:type airflow_plugins: List[AirflowPlugin]
:return: List[Type[AirflowPlugin]]
"""
for entry_point in entry_points:
log.debug('Importing entry_point plugin %s', entry_point.name)
plugin_obj = entry_point.load()
if is_valid_plugin(plugin_obj, airflow_plugins):
if callable(getattr(plugin_obj, 'on_load', None)):
plugin_obj.on_load()
airflow_plugins.append(plugin_obj)
re
```
<Overlap Ratio: 0.96125>

---

--- 217 --
Question ID: 913c1814ae960b7b49a7fa6851e3371d07acf33f_0
Original Code:
```
def utility_for_consumers(
    quality,
    prev_usage,
    usage_counter,
    privacy_concern,
    firm_privacy_score,
    util_weight_dict,
):
    """
    Input:
    - quality = (firm, category)
    - prev_usage = (consumer, category, firm)
    - usage_counter = (consumer, category, firm)
    - privacy_concern = (consumer)
    - firm_privacy_score = (firm)
    - util_weight_dict has all the weights we need
    returns (Customer, Category, Firm): utility of each product for the customer (whether or not it exists).
    """
    n_consumers, n_categories, n_firms = prev_usage.shape
    usage_company = usage_counter.sum(axis=1)
    w_priv = util_weight_dict["w_priv"]
    w_qual = util_weight_dict["w_qual"]
    w_loyal_firm = util_weight_dict["w_loyal_firm"]
    w_loyal_category = util_weight_dict["w_loyal_category"]
    return (
        w_qual * quality.T[None, :, :]
        + w_loyal_category * usage_counter
        + w_loyal_firm * usage_company[:, None, :]
        - w_priv
        * privacy_concern[:, None, None]
        * (1 - firm_privacy_score)[None, None, :]
    )
```


Overlapping Code:
```
def utility_for_consumers(
quality,
prev_usage,
usage_counter,
privacy_concern,
firm_privacy_score,
util_weight_dict,
):
"""
Input:
- quality = (firm, category)
- prev_usage = (consumer, category, firm)
- usage_counter = (consumer, category, firm)
- privacy_concern = (consumer)
- firm_privacy_score = (firm)
- util_weight_dict has all the weights we need
returns (Customer, Category, Firm): utility of each product for the customer (whether or not it exists).
"""
n_consumers, n_categories, n_firms = prev_usage.shape
usage_company = usage_counter.sum(axis=1)
w_priv = util_weight_dict["w_priv"]
w_qual = util_weight_dict["w_qual"]
w_loyal_firm = util_weight_dict["w_loyal_firm"]
w_loyal_category = util_weight_dict["w_loyal_category"]
return (
w_qual * quality.T[None, :, :]
+ w_loyal_category * usage_counter
+ w_loyal_firm * usage_company[:, None, :]
- w_priv
* privacy_concern[:, None, None]
* (
```
<Overlap Ratio: 0.9574468085106383>

---

--- 218 --
Question ID: 0a52b0cdbe643eadbbae2f651d39c93cc2739a49_2
Original Code:
```
def cal_psnr_img(img1, img2):
    '''
    Calculate PSNR from two image
    :param img1: numpy array
    :param img2: numpy array
    '''
    # img1 and img2 have range [0, 255]
    img1 = img1.astype(np.float64)
    img2 = img2.astype(np.float64)
    mse = np.mean((img1 - img2) ** 2)
        
    if mse == 0:
        return float('inf')
        
    return 20 * math.log10(255.0 / math.sqrt(mse))
```


Overlapping Code:
```
:
'''
Calculate PSNR from two image
:param img1: numpy array
:param img2: numpy array
'''
# img1 and img2 have range [0, 255]
img1 = img1.astype(np.float64)
img2 = img2.astype(np.float64)
mse = np.mean((img1 - img2) ** 2)

if mse == 0:
return float('inf')

return 20 * math.log10(255.0 / math.sqrt(ms
```
<Overlap Ratio: 0.9063444108761329>

---

--- 219 --
Question ID: 2a34e489fa2b16cef9b2f528b6f6e9b0e4d2acb6_2
Original Code:
```
def load_nli_file(data_path, num_par=2):
  """Build a tf.data.Data from a file of NLI examples."""
  tokenizer = tokenization.NltkTokenizer()
  dataset = tf.data.TextLineDataset(data_path)
  dataset = dataset.map(
      functools.partial(_nli_line_to_tensors, tokenizer=tokenizer),
      num_parallel_calls=num_par)
  dataset = dataset.filter(lambda x: tf.greater_equal(x["label"], 0))
  return dataset
```


Overlapping Code:
```
ath, num_par=2):
"""Build a tf.data.Data from a file of NLI examples."""
tokenizer = tokenization.NltkTokenizer()
dataset = tf.data.TextLineDataset(data_path)
dataset = dataset.map(
functools.partial(_nli_line_to_tensors, tokenizer=tokenizer),
num_parallel_calls=num_par)
dataset = dataset.filter(lambda x: tf.greater_equal(x["label"], 0))
return dat
```
<Overlap Ratio: 0.9259259259259259>

---

--- 220 --
Question ID: 4278fefc3d350168b06ea3d81a1c98f054b4d235_1
Original Code:
```
def set_inputfile(hgrid, dico):
    basicinput = """
logfile: Yes
dft:
    ixc: PBE
    ncong: 2
    rmult: [10, 8]
    itermax: 3
    idsx: 0
    gnrm_cv: 1e-8
#Control the diagonalisation scheme
mix:
    iscf: 7
    itrpmax: 200
    rpnrm_cv: 1e-12
    tel: 1e-3
    alphamix: 0.5
    norbsempty: 1000
    alphadiis: 1.0

#perf:
#    accel: OCLGPU
#    ocl_devices: Tesla K40c
#    blas: Yes

"""
    import yaml
    import os
    var = yaml.load(basicinput)
    # Spin parameters
    var["dft"].update(set_spin(dico["name"], dico["nat"]))
    # K point parameters
    var["kpt"] = set_kpoints(dico["nat"])
    var["dft"]["hgrids"] = (hgrid, hgrid, hgrid)
    # Control the diagonalisation scheme
    if dico["name"] in ("Cr", ):
        var["mix"]["iscf"] = 3
        var["mix"]["alphamix"] = 0.9
    if dico["name"] in ("Ba", "Ca"):
        var["mix"]["norbsempty"] = 8
    var["ig_occupation"] = {dico["name"]: {"empty_shells": ("s", "p", "d")}}
    # Atoms
    pspfile = "psppar."+dico["name"]
    if not os.path.isfile(pspfile):
        safe_print("WARNING: Using default PSP for atom", dico["name"])
    else:
        var[pspfile] = open(pspfile, 'r').read()
    # var["posinp"] = {"positions": [{dico["name"]:
    #                  map(float, dico[i + 1].split())}
    #                  for i in range(dico["nat"])],
    #                  "units": "reduced",
    #                  "cell": (dico["a"], dico["b"], dico["c"])}
    var["posinp"] = {"positions": [{dico["name"]: dico[i + 1]} for i in range(
        dico["nat"])], "units": "reduced",
        "cell": (dico["a"], dico["b"], dico["c"])}
    # We round robin the igspins.
    if "mpol" in var["dft"]:
        mpol = 0
        while mpol < var["dft"]["mpol"]:
            for at in var["posinp"]["positions"]:
                if mpol < var["dft"]["mpol"]:
                    if "IGSpin" in at:
                        at["IGSpin"] += 1
                    else:
                        at["IGSpin"] = 1
                    mpol += 1
    elif "nspin" in var["dft"] and var["dft"]["nspin"] == 2:
        for (i, at) in enumerate(var["posinp"]["positions"]):
            at["IGSpin"] = 1 - 2 * (i % 2)
    return var
```


Overlapping Code:
```
e(hgrid, dico):
basicinput = """
logfile: Yes
dft:
ixc: PBE
ncong: 2
rmult: [10, 8]
itermax: 3
idsx: 0
gnrm_cv: 1e-8
#Control the diagonalisation scheme
mix:
iscf: 7
itrpmax: 200
rpnrm_cv: 1e-12
tel: 1e-3
alphamix: 0.5
norbsempty: 1000
alphadiis: 1.0
#perf:
# accel: OCLGPU
# ocl_devices: Tesla K40c
# blas: Yes
"""
import yaml
import os
var = yaml.load(basicinput)
# Spin parameters
var["dft"].update(set_spin(dico["name"], dico["nat"]))
# K point parameters
var["kpt"] = set_kpoints(dico["nat"])
var["dft"]["hgrids"] = (hgrid, hgrid, hgrid)
# Control the diagonalisation scheme
if dico["name"] in ("Cr", ):
var["mix"]["iscf"] = 3
var["mix"]["alphamix"] = 0.9
if dico["name"] in ("Ba", "Ca"):
var["mix"]["norbsempty"] = 8
var["ig_occupation"] = {dico["name"]: {"empty_shells": ("s", "p", "d")}}
# Atoms
pspfile = "psppar."+dico["name"]
if not os.path.isfile(pspfile):
safe_print("WARNING: Using default PSP for atom", dico["name"])
else:
var[pspfile] = open(pspfile, 'r').read()
# var["posinp"] = {"positions": [{dico["name"]:
# map(float, dico[i + 1].split())}
# for i in range(dico["nat"])],
# "units": "reduced",
# "cell": (dico["a"], dico["b"], dico["c"])}
var["posinp"] = {"positions": [{dico["name"]: dico[i + 1]} for i in range(
dico["nat"])], "units": "reduced",
"cell": (dico["a"], dico["b"], dico["c"])}
# We round robin the igspins.
if "mpol" in var["dft"]:
mpol = 0
while mpol < var["dft"]["mpol"]:
for at in var["posinp"]["positions"]:
if mpol < var["dft"]["mpol"]:
if "IGSpin" in at:
at["IGSpin"] += 1
else:
at["IGSpin"] = 1
mpol += 1
elif "nspin" in var["dft"] and var["dft"]["nspin"] == 2:
for (i, at) in enumerate(var["posinp"]["positions"]):
at["IGSpin"] = 1 - 2 * (i % 2)
return v
```
<Overlap Ratio: 0.989522700814901>

---

--- 221 --
Question ID: f1aca00692c3fb5a0596a1389db9f9bc57ccfc63_1
Original Code:
```
def adjust_params_names_group(params = ['v', 'a', 'w'], 
                              params_bounds = [], 
                              params_bounds_epsilon= [],
                              param_varies = [0, 0, 1],
                              n_subjects = 3):
    params_adj = []
    params_bounds_adj = []
    params_bounds_epsilon_adj = []
    cnt = 0
    for p in params:
        if param_varies[cnt]:
            for i in range(n_subjects):
                params_adj.append(p + '_' + str(i))
                params_bounds_adj.append(params_bounds[cnt])
                params_bounds_epsilon_adj.append(params_bounds_epsilon[cnt])
        else:
            params_adj.append(p)
            params_bounds_adj.append(params_bounds[cnt])
            params_bounds_epsilon_adj.append(params_bounds_epsilon[cnt])
        cnt += 1
    return params_adj, params_bounds_adj, params_bounds_epsilon_adj
```


Overlapping Code:
```
['v', 'a', 'w'], 
params_bounds = [], 
params_bounds_epsilon= [],
param_varies = [0, 0, 1],
n_subjects = 3):
params_adj = []
params_bounds_adj = []
params_bounds_epsilon_adj = []
cnt = 0
for p in params:
if param_varies[cnt]:
for i in range(n_subjects):
params_adj.append(p + '_' + str(i))
params_bounds_adj.append(params_bounds[cnt])
params_bounds_epsilon_adj.append(params_bounds_epsilon[cnt])
else:
params_adj.append(p)
params_bounds_adj.append(params_bounds[cnt])
params_bounds_epsilon_adj.append(params_bounds_epsilon[cnt])
cnt += 1
return params_adj, params_bounds_adj, params_bounds_epsilon_ad
```
<Overlap Ratio: 0.9375>

---

--- 222 --
Question ID: 25cf96ae6a5620e334caa61021d1a181071e0d66_0
Original Code:
```
def get_opt_perf(folder, ext):
    folder = Path(folder)
    files = folder.rglob(f"optim_performance*.{ext}")

    data = []
    for i, f in enumerate(files):
        # Divide by max hypervolume to get normalised
        if ext == "csv":
            run = pd.read_csv(f, sep=",")["hypervolume"].values / 1e7
        elif ext == "txt":
            run = pd.read_csv(f, sep="\t")["hypervolume"].values / 1e7
        data.append(run)
    mean = np.array(data).mean(0)
    std = np.array(data).std(0)
    stats = pd.DataFrame({"gen": range(mean.shape[0]), "mean": mean, "std": std})
    stats.to_csv(folder / "optim_hypervolume.csv", index=False, sep=",")
```


Overlapping Code:
```
et_opt_perf(folder, ext):
folder = Path(folder)
files = folder.rglob(f"optim_performance*.{ext}")
data = []
for i, f in enumerate(files):
# Divide by max hypervolume to get normalised
if ext == "csv":
run = pd.read_csv(f, sep=",")["hypervolume"].values / 1e7
elif ext == "txt":
run = pd.read_csv(f, sep="\t")["hypervolume"].values / 1e7
data.append(run)
mean = np.array(data).mean(0)
std = np.array(data).std(0)
stats = pd.DataFrame({"gen": range(mean.shape[0]), "mean": mean, "std": std})
stats.to_csv(folder / "optim_hypervolume.csv", index=False, 
```
<Overlap Ratio: 0.9769094138543517>

---

--- 223 --
Question ID: 350a6c06372099ca7c1a9d8998b9726f2f877b99_3
Original Code:
```
def template_contains_key(template, key):
    if ContextParser.search_deep_keys(key, template, []):
        return True
    return False
```


Overlapping Code:
```
ef template_contains_key(template, key):
if ContextParser.search_deep_keys(key, template, []):
retur
```
<Overlap Ratio: 0.8333333333333334>

---

--- 224 --
Question ID: 17e01b873e44a92bf8df48fb793eb8c6946bfd05_0
Original Code:
```
def start_cli():
    topo = create_topo()
    net = Mininet(topo=topo)
    net.start()
    dumpNodeConnections(net.hosts)
    #s1_2, s1_3, s1_4, s1_5, s1_6, s2, s3, s4, s5, s6, h1, h2, h3, h4, h5, h6 = net.get('s1_2', 's1_3', 's1_4', 's1_5', 's1_6', 's2', 's3', 's4', 's5', 's6', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6')
    net.pingAll()
    #net.configLinkStatus('s1_2', 's2', 'down')
    #net.configLinkStatus('s1_3', 's3', 'down')
    #net.configLinkStatus('s1_4', 's4', 'down')
    #net.configLinkStatus('s1_5', 's5', 'down')
    #net.configLinkStatus('s1_6', 's6', 'down')
    #net.pingAll()
    h2.cmd('python3 modbus_server.py &')
    h3.cmd('python3 modbus_server.py &')
    h4.cmd('python3 modbus_server.py &')
    h5.cmd('python3 modbus_server.py &')
    h6.cmd('python3 modbus_server.py &')
    h1.cmd('tcpdump -i h1-eth0 -w h1_multi.pcap &')
    h2.cmd('tcpdump -i h2-eth0 -w h2_multi.pcap &')
    h3.cmd('tcpdump -i h3-eth0 -w h3_multi.pcap &')
    h4.cmd('tcpdump -i h4-eth0 -w h4_multi.pcap &')
    h5.cmd('tcpdump -i h5-eth0 -w h5_multi.pcap &')
    h6.cmd('tcpdump -i h6-eth0 -w h6_multi.pcap &')
    #h1.cmd('python3 modbus_client.py h2')
    #h1.cmd('python3 modbus_client.py h3')
    #h1.cmd('python3 modbus_client.py h4')
    #h1.cmd('python3 modbus_client.py h5')
    #h1.cmd('python3 modbus_client.py h6')

    CLI(net)
    net.stop()
```


Overlapping Code:
```
_cli():
topo = create_topo()
net = Mininet(topo=topo)
net.start()
dumpNodeConnections(net.hosts)
#s1_2, s1_3, s1_4, s1_5, s1_6, s2, s3, s4, s5, s6, h1, h2, h3, h4, h5, h6 = net.get('s1_2', 's1_3', 's1_4', 's1_5', 's1_6', 's2', 's3', 's4', 's5', 's6', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6')
net.pingAll()
#net.configLinkStatus('s1_2', 's2', 'down')
#net.configLinkStatus('s1_3', 's3', 'down')
#net.configLinkStatus('s1_4', 's4', 'down')
#net.configLinkStatus('s1_5', 's5', 'down')
#net.configLinkStatus('s1_6', 's6', 'down')
#net.pingAll()
h2.cmd('python3 modbus_server.py &')
h3.cmd('python3 modbus_server.py &')
h4.cmd('python3 modbus_server.py &')
h5.cmd('python3 modbus_server.py &')
h6.cmd('python3 modbus_server.py &')
h1.cmd('tcpdump -i h1-eth0 -w h1_multi.pcap &')
h2.cmd('tcpdump -i h2-eth0 -w h2_multi.pcap &')
h3.cmd('tcpdump -i h3-eth0 -w h3_multi.pcap &')
h4.cmd('tcpdump -i h4-eth0 -w h4_multi.pcap &')
h5.cmd('tcpdump -i h5-eth0 -w h5_multi.pcap &')
h6.cmd('tcpdump -i h6-eth0 -w h6_multi.pcap &')
#h1.cmd('python3 modbus_client.py h2')
#h1.cmd('python3 modbus_client.py h3')
#h1.cmd('python3 modbus_client.py h4')
#h1.cmd('python3 modbus_client.py h5')
#h1.cmd('python3 modbus_client.py h6')
```
<Overlap Ratio: 0.976461038961039>

---

--- 225 --
Question ID: 9185f2ff121be31130e9d0e6dfee4cde19524eb1_10
Original Code:
```
def binary_correlate(series_x, series_y):
    """Helper function to Correlate binary Data

    Both the series should have same indices

    For binary time series data:

    .. math::

        \\alpha_{corr} = \\frac{N_{agree} - N_{disagree}}{N}

    :param series_x: First time Series data
    :type series_x: :mod:`pandas.Series`

    :param series_y: Second time Series data
    :type series_y: :mod:`pandas.Series`
    """

    if len(series_x) != len(series_y):
        raise ValueError("Cannot compute binary correlation for \
                          unequal vectors")

    agree = len(series_x[series_x == series_y])
    disagree = len(series_x[series_x != series_y])

    return (agree - disagree) / len(series_x)
```


Overlapping Code:
```
"""Helper function to Correlate binary Data
Both the series should have same indices
For binary time series data:
.. math::
\\alpha_{corr} = \\frac{N_{agree} - N_{disagree}}{N}
:param series_x: First time Series data
:type series_x: :mod:`pandas.Series`
:param series_y: Second time Series data
:type series_y: :mod:`pandas.Series`
"""
if len(series_x) != len(series_y):
raise ValueError("Cannot compute binary correlation for \
unequal vectors")
agree = len(series_x[series_x == series_y])
disagree = len(series_x[series_x != series_y])
return (agree - disagree) 
```
<Overlap Ratio: 0.9082125603864735>

---

--- 226 --
Question ID: cb8f32025446e825956a5aee487699a63e09fbb9_5
Original Code:
```
def PrintSolution():
    if sa_obj is None:
        print("Instance not solved correctly.")
        return
    print("Best TSP tour length: ", GetBestDist())
    sa_obj.PrintConvergence()
    
    print("Best TSP tour: ", GetBestTour())
    PrintBestTour()
```


Overlapping Code:
```
if sa_obj is None:
print("Instance not solved correctly.")
return
print("Best TSP tour length: ", GetBestDist())
sa_obj.PrintConvergence()

print("Best 
```
<Overlap Ratio: 0.7037037037037037>

---

--- 227 --
Question ID: 728beb48a1989afc90b2c0545ddb4a22fe2ec79b_10
Original Code:
```
def PDec2DMS (dec):
    """ Convert a declination in degrees to degrees, min, seconds

    dec  = Declination in deg.
    """
    ################################################################
    p = math.fabs(dec)
    if dec>0.0:
        sgn = " "
    else:
        sgn = "-"
    d = int (p)
    p = (p - d) * 60.0
    m = int(p)
    s = (p - m) * 60.0
    out = "%s%2.2d %2d %7.4f " % (sgn, d,m,s)
    return out
```


Overlapping Code:
```
:
""" Convert a declination in degrees to degrees, min, seconds
dec = Declination in deg.
"""
################################################################
p = math.fabs(dec)
if dec>0.0:
sgn = " "
else:
sgn = "-"
d = int (p)
p = (p - d) * 60.0
m = int(p)
s = (p - m) * 60.0
out = "%s%2.2d %2d %7.4f " % (
```
<Overlap Ratio: 0.8847262247838616>

---

--- 228 --
Question ID: a07476cc01bd32fa4a5458dc36f7f4b9768fd5c4_0
Original Code:
```
def build_ml_decoder_model(backbone, num_classes=80, num_of_groups=-1, decoder_embedding=768, **kwargs):
    """Create a model
    """
    num_features = backbone.num_features
    if num_classes == -1:
        num_classes = backbone.num_classes
    # loading ML decoder model
    if hasattr(backbone, "model"): # timm models
        backbone.model.classifier = None
    else:
        backbone.classifier = None
    model = MLDecoder(backbone, num_classes=num_classes, initial_num_features=num_features, num_of_groups=num_of_groups,
                      decoder_embedding=decoder_embedding, **kwargs)
    return model
```


Overlapping Code:
```
80, num_of_groups=-1, decoder_embedding=768, **kwargs):
"""Create a model
"""
num_features = backbone.num_features
if num_classes == -1:
num_classes = backbone.num_classes
# loading ML decoder model
if hasattr(backbone, "model"): # timm models
backbone.model.classifier = None
else:
backbone.classifier = None
model = MLDecoder(backbone, num_classes=num_classes, initial_num_features=num_features, num_of_groups=num_of_groups,
decoder_embedding=decod
```
<Overlap Ratio: 0.8411214953271028>

---

--- 229 --
Question ID: 29660d157f32a58dfbbbeb98f373bb3e4e42bc15_4
Original Code:
```
@archives.route("/fs", defaults={'req_path': ''})
@archives.route("/fs/<path:req_path>")
def dir_listing(req_path):
    req_path = __resolve_path(req_path)
    #print("archives", divisor)
    #trace('dir_listing')
    abs_path = _get_req_absolute_path(req_path)
    # Check if path is a file and serve
    if os.path.isfile(abs_path):
        return send_file(abs_path)
    (files_info, locations, isRoot, folderEnv) = _dir_listing(req_path)
    return render_template('archives_view.html', files_info=files_info, locations=locations, isRoot=isRoot, folderEnv=folderEnv)
```


Overlapping Code:
```
: ''})
@archives.route("/fs/<path:req_path>")
def dir_listing(req_path):
req_path = __resolve_path(req_path)
#print("archives", divisor)
#trace('dir_listing')
abs_path = _get_req_absolute_path(req_path)
# Check if path is a file and serve
if os.path.isfile(abs_path):
return send_file(abs_path)
(files_info, locations, isRoot, folderEnv) = _dir_listing(req_path)
return render_template('archives_view.html', files_info=files_info, locations=locations, isRoot=isRoot, folderEnv=folderE
```
<Overlap Ratio: 0.9132075471698113>

---

--- 230 --
Question ID: 8ad1d8c425550d3c7509bab19f4ff8bd9d9df6d8_5
Original Code:
```
def flush_memcache(app_name):
    """Flushes the memcache.

    Args:
        app_name: str. The name of the app to deploy.
    """
    memcache_url = (
        'https://console.cloud.google.com/appengine/memcache?'
        'src=ac&project=%s') % app_name
    common.open_new_tab_in_browser_if_possible(memcache_url)
    common.ask_user_to_confirm('Please flush the memcache.')

    admin_misc_tab_url = None
    if app_name == APP_NAME_OPPIASERVER:
        admin_misc_tab_url = 'https://www.oppia.org/admin#/misc'
    else:
        admin_misc_tab_url = 'https://%s.appspot.com/admin#/misc' % app_name

    if admin_misc_tab_url:
        common.open_new_tab_in_browser_if_possible(admin_misc_tab_url)
        common.ask_user_to_confirm('Please flush the cache on Oppia website.')
```


Overlapping Code:
```
 flush_memcache(app_name):
"""Flushes the memcache.
Args:
app_name: str. The name of the app to deploy.
"""
memcache_url = (
'https://console.cloud.google.com/appengine/memcache?'
'src=ac&project=%s') % app_name
common.open_new_tab_in_browser_if_possible(memcache_url)
common.ask_user_to_confirm('Please flush the memcache.')
admin_misc_tab_url = None
if app_name == APP_NAME_OPPIASERVER:
admin_misc_tab_url = 'https://www.oppia.org/admin#/misc'
else:
admin_misc_tab_url = 'https://%s.appspot.com/admin#/misc' % app_name
if admin_misc_tab_url:
common.open_new_tab_in_browser_if_possible(admin_misc_tab_url)
common.ask_user_to_confirm('Please flush th
```
<Overlap Ratio: 0.9558823529411765>

---

--- 231 --
Question ID: e68a49671a938cba7d7700ecdb66d7a7f6d794d6_1
Original Code:
```
def log(message):
	global args
	try:args
	except NameError:pass
	else:
		if args.verbose:
			stdout.write('%s\n'%message)
			if args.debug:
				if message.startswith('[%s]'%args.debug.upper()):
					exit(1)
```


Overlapping Code:
```

except NameError:pass
else:
if args.verbose:
stdout.write('%s\n'%message)
if args.debug:
if message
```
<Overlap Ratio: 0.5405405405405406>

---

--- 232 --
Question ID: a4381e7060c53f60e6c99da88e2a53670cdff3db_8
Original Code:
```
def model_ensemble_classification(X,
                                  y,
                                  feature_trans,
                                  estimator_list,
                                  score_eval,
                                  model_perf_tuning_df):

    stacked_model_eval_dict = {}

    with open('config.yaml') as f:
        config_data = yaml.load(f, Loader=yaml.FullLoader)

    tree_models_short_name_dict = config_data['tree_models_short_name']
    df = model_perf_tuning_df.copy()
    df.set_index('Model', inplace=True)

    ll = estimator_list
    model_combinations_list = []
    for ii in range(2, len(ll)+1):
        if ii == 2:
            res = [list(ele) for ele in list(itertools.permutations(ll, ii))]
            model_combinations_list.extend(res)
        else:
            res = [list(ele) for ele in list(itertools.permutations(ll, ii))]
            final_list = []
            for i in res:
                pop_item = i.pop()
                sorted_list = sorted(i)
                final_list.append(sorted_list+[pop_item])

            final_list.sort()
            complete_list = list(final_list for final_list,_ in itertools.groupby(final_list))
            model_combinations_list.extend(complete_list)

    # model_combinations_tuple = permutations(estimator_list)
    for models_list in model_combinations_list:
        level0 = list()
        model_params_dict = {}
        for i in models_list[:-1]:
            level0.append((tree_models_short_name_dict[i][0],  globals()[i]()))

            for key, val in eval(df.loc[i, 'Best_Params']).items():
                key = key.replace('__', '__'+tree_models_short_name_dict[i][0]+'__')
                model_params_dict[key] = val

        # define meta learner model
        level1 = globals()[models_list[-1]]()
        final_model_short_name = tree_models_short_name_dict[models_list[-1]][0]

        for key, val in eval(df.loc[models_list[-1], 'Best_Params']).items():
            # key = key.replace('__', '__'+tree_models_short_name_dict[models_list[-1]][0]+'__')
            key = key.replace('__', '__final_estimator__')
            model_params_dict[key] = val

        # define the stacking ensemble
        model_stack = StackingClassifier(estimators=level0, final_estimator=level1)

        model_pipeline = Pipeline([('feat_trans', feature_trans),
                                   ('estimator', model_stack)])

        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
        model_pipeline.set_params(**model_params_dict)
        oof_preds = cross_val_predict(estimator=model_pipeline,
                                      X=X,
                                      y=y,
                                      cv=cv,
                                      n_jobs=-1,
                                      method='predict',
                                      # fit_params=clf.best_params_,
                                      verbose=1)

        # oof_roc_auc_score = roc_auc_score(y, oof_preds)
        oof_eval_score = score_eval(y, oof_preds)

        stacking_str = ''
        for j in level0:
            stacking_str = stacking_str+j[0]+'+'
        stacking_str = stacking_str[:-1]
        stacking_str = stacking_str+'-->'+final_model_short_name

        stacked_model_eval_dict[stacking_str] = {'OOF_'+score_eval.__name__: round(oof_eval_score, 5),
                                          'Best_Params': str(model_params_dict)}

    stacked_model_eval_df = pd.DataFrame.from_dict(stacked_model_eval_dict,
                                                  orient='index').reset_index().rename(columns={'index': 'Model'})


    # stacked_model_eval_df = stacked_model_eval_df.sort_values(by='OOF_ROC_AUC_Score', ascending=False)
    #=========================================================================
    # Below is the code for pickle the best stacked model
    stacked_model_eval_df = stacked_model_eval_df.sort_values(by='OOF_'+score_eval.__name__, ascending=False)
    best_stacked_model_best_params = eval(stacked_model_eval_df.iloc[0].to_dict()['Best_Params'])
    # logger.info(f'Best Stacked')
    models_split = stacked_model_eval_df.iloc[0].to_dict()['Model'].split('-->')
    logger.info(f"Best Stacked Tree Model - {stacked_model_eval_df.iloc[0].to_dict()['Model']}")
    logger.info(f"\nBest Stacked Tree Model - {stacked_model_eval_df.iloc[0].to_dict()['Model']} Params:\n{pformat(best_stacked_model_best_params)}")
    final_esti = [key for key, value in tree_models_short_name_dict.items() if value[0] == models_split[-1]]
    level1 = globals()[final_esti[0]]()

    level0 = []
    for i in models_split[0].split('+'):
        level0_esti = [key for key, value in tree_models_short_name_dict.items() if value[0] == i]
        level0.append((i,  globals()[level0_esti[0]]()))

    model_stack = StackingClassifier(estimators=level0, final_estimator=level1)
    model_pipeline = Pipeline([('feat_trans', feature_trans),
                               ('estimator', model_stack)])
    model_pipeline.set_params(**best_stacked_model_best_params)
    with open('best_stacked_tree_model.plk', 'wb') as f:
        pickle.dump(model_pipeline, f)
    #=========================================================================

    # stacked_model_eval_df = stacked_model_eval_df.append(model_perf_tuning_df[['Model', 'OOF_ROC_AUC_Score', 'Best_Params']])
    all_tree_model_eval_df = stacked_model_eval_df.append(model_perf_tuning_df[['Model', 'OOF_'+score_eval.__name__, 'Best_Params']])


    all_tree_model_eval_df['Model'] = all_tree_model_eval_df['Model'].map(lambda x: tree_models_short_name_dict[x][0] if x in tree_models_short_name_dict else x)
    all_tree_model_eval_df = all_tree_model_eval_df.sort_values(by='OOF_'+score_eval.__name__, ascending=False)
    all_tree_model_eval_df.reset_index(drop=True, inplace=True)
    logger.info(f"\nAll Tree Models Evaluation Dataframe\n{all_tree_model_eval_df[['Model', 'OOF_'+score_eval.__name__]].to_string()}")
    best_tree_model = all_tree_model_eval_df.iloc[0].to_dict()['Model']
    best_tree_model_params = eval(all_tree_model_eval_df.iloc[0].to_dict()['Best_Params'])
    logger.info(f"Best Tree Model (Single+Stacked) : {best_tree_model}")
    logger.info(f"Best Tree Model (Single+Stacked) - {best_tree_model} Params\n{pformat(best_tree_model_params)}")
    # breakpoint()
    return all_tree_model_eval_df, best_tree_model
```


Overlapping Code:
```
eature_trans,
estimator_list,
score_eval,
model_perf_tuning_df):
stacked_model_eval_dict = {}
with open('config.yaml') as f:
config_data = yaml.load(f, Loader=yaml.FullLoader)
tree_models_short_name_dict = config_data['tree_models_short_name']
df = model_perf_tuning_df.copy()
df.set_index('Model', inplace=True)
ll = estimator_list
model_combinations_list = []
for ii in range(2, len(ll)+1):
if ii == 2:
res = [list(ele) for ele in list(itertools.permutations(ll, ii))]
model_combinations_list.extend(res)
else:
res = [list(ele) for ele in list(itertools.permutations(ll, ii))]
final_list = []
for i in res:
pop_item = i.pop()
sorted_list = sorted(i)
final_list.append(sorted_list+[pop_item])
final_list.sort()
complete_list = list(final_list for final_list,_ in itertools.groupby(final_list))
model_combinations_list.extend(complete_list)
# model_combinations_tuple = permutations(estimator_list)
for models_list in model_combinations_list:
level0 = list()
model_params_dict = {}
for i in models_list[:-1]:
level0.append((tree_models_short_name_dict[i][0], globals()[i]()))
for key, val in eval(df.loc[i, 'Best_Params']).items():
key = key.replace('__', '__'+tree_models_short_name_dict[i][0]+'__')
model_params_dict[key] = val
# define meta learner model
level1 = globals()[models_list[-1]]()
final_model_short_name = tree_models_short_name_dict[models_list[-1]][0]
for key, val in eval(df.loc[models_list[-1], 'Best_Params']).items():
# key = key.replace('__', '__'+tree_models_short_name_dict[models_list[-1]][0]+'__')
key = key.replace('__', '__final_estimator__')
model_params_dict[key] = val
# define the stacking ensemble
model_stack = StackingClassifier(estimators=level0, final_estimator=level1)
model_pipeline = Pipeline([('feat_trans', feature_trans),
('estimator', model_stack)])
cv = StratifiedKFold(n_splits=10, shuffle=True, random_sta
```
<Overlap Ratio: 0.9783412572636028>

---

--- 233 --
Question ID: ce2056f419905897c0583d459b2a3dedbe84d838_5
Original Code:
```
@banner_item_api.route('/hide/<int:bid>', methods=['PUT'])
@route_meta(auth='隐藏横幅子项', module='横幅子项')
@group_required
def hide(bid):
    BannerItem.hide_model(bid, throw=True)
    return Success(msg='横幅隐藏成功')
```


Overlapping Code:
```
 methods=['PUT'])
@route_meta(auth='隐藏横幅子项', module='横幅子项')
@group_required
def hide(bid):
BannerItem.hide_model(bid, throw=True)
return Success(msg='横幅
```
<Overlap Ratio: 0.7638190954773869>

---

--- 234 --
Question ID: e439767795ece2a963bfe472a4321d12b64cec29_5
Original Code:
```
def reduce_puzzle(values):
    """
    Iterate eliminate() and only_choice(). If at some point, there is a box with no available values, return False.
    If the sudoku is solved, return the sudoku.
    If after an iteration of both functions, the sudoku remains the same, return the sudoku.
    Input: A sudoku in dictionary form.
    Output: The resulting sudoku in dictionary form.
    """
    stalled = False
    while not stalled:
        # Check how many boxes have a determined value
        solved_values_before = len(
            [box for box in values.keys() if len(values[box]) == 1]
        )

        # Eliminate Strategy
        values = eliminate(values)
        # Only Choice Strategy
        values = only_choice(values)

        # Check how many boxes have a determined value, to compare
        solved_values_after = len(
            [box for box in values.keys() if len(values[box]) == 1]
        )

        # If no new values were added, stop the loop.
        stalled = solved_values_before == solved_values_after

        # Sanity check, return False if there is a box with zero available values.
        if len([box for box in values.keys() if len(values[box]) == 0]):
            return False
    return values
```


Overlapping Code:
```
ef reduce_puzzle(values):
"""
Iterate eliminate() and only_choice(). If at some point, there is a box with no available values, return False.
If the sudoku is solved, return the sudoku.
If after an iteration of both functions, the sudoku remains the same, return the sudoku.
Input: A sudoku in dictionary form.
Output: The resulting sudoku in dictionary form.
"""
stalled = False
while not stalled:
# Check how many boxes have a determined value
solved_values_before = len(
[box for box in values.keys() if len(values[box]) == 1]
)
# Eliminate Strategy
values = eliminate(values)
# Only Choice Strategy
values = only_choice(values)
# Check how many boxes have a determined value, to compare
solved_values_after = len(
[box for box in values.keys() if len(values[box]) == 1]
)
# If no new values were added, stop the loop.
stalled = solved_values_before == solved_values_after
# Sanity check, return False if there is a box with zero available values.
if len([box for box in values.keys() if len(values[box]) == 0]):
return False
return values
```
<Overlap Ratio: 0.9990412272291467>

---

--- 235 --
Question ID: b08f9649ffcfb47313fcc0852c8df04ac13b239c_4
Original Code:
```
def scrapePapago(language):
    # Define the URL to be opened
    url = 'https://papago.naver.com/'
    # Define the driver for Selenium to use
    driver = webdriver.Chrome('chromedriver.exe')
    # Open the specfied URL
    driver.get(url)
    # Sleep to avoid errors
    time.sleep(2)
    # Find the Text Field and type the sentences
    textFieldSource = driver.find_element_by_xpath('//*[@id="txtSource"]')
    test = source.loc[0:0].to_string(index = False, columns = None, header = False)
    batch = 1
    sentencesCleanBatches = {}
    sentencesClean = []
    for row in source['origin']:
        textFieldSource.send_keys(row)
        textFieldSource.send_keys(Keys.RETURN)
        try:
            char_usage = driver.find_element_by_xpath('//*[@id="root"]/div/div[1]/section/div/div[1]/div[1]/div/p[1]').text
            char_usage1 = char_usage[0:4]
            char_usage2 = char_usage1.strip()
            char_count = int(char_usage2)
        except Exception:
            char_count = 0
            pass
        if char_count >= 4500:
            print(char_count)
            # Pause to avoid errors
            time.sleep(5)
            # Push the Translate button
            translate_button = driver.find_element_by_xpath('//*[@id="btnTranslate"]').click()
            # Pause to avoid errors
            time.sleep(5)
            # Store the translated text as a string
            translatedText = driver.find_element_by_xpath('//*[@id="txtTarget"]')
            translatedText_content = translatedText.text
            # Clean the output text and print
            sentencesCleanBatches[batch] = translatedText_content.split("\n")
            batch += 1
            # Clear the input field
            textFieldSource.clear()
    # Pause to avoid errors
    time.sleep(5)
    # Push the Translate button
    translate_button = driver.find_element_by_xpath('//*[@id="btnTranslate"]').click()
    # Pause to avoid errors
    time.sleep(5)
    # Store the translated text as a string
    translatedText = driver.find_element_by_xpath('//*[@id="txtTarget"]')
    translatedText_content = translatedText.text
    # Clean the output text and print
    sentencesCleanBatches[batch] = translatedText_content.split("\n")
    # Concatenate all the batches into once list
    for i in range(1, batch + 1):
        sentencesClean = sentencesClean + sentencesCleanBatches[i]
    # Close the browser window
    driver.quit()
    # Return necessary objects
    return sentencesClean
```


Overlapping Code:
```
:
# Define the URL to be opened
url = 'https://papago.naver.com/'
# Define the driver for Selenium to use
driver = webdriver.Chrome('chromedriver.exe')
# Open the specfied URL
driver.get(url)
# Sleep to avoid errors
time.sleep(2)
# Find the Text Field and type the sentences
textFieldSource = driver.find_element_by_xpath('//*[@id="txtSource"]')
test = source.loc[0:0].to_string(index = False, columns = None, header = False)
batch = 1
sentencesCleanBatches = {}
sentencesClean = []
for row in source['origin']:
textFieldSource.send_keys(row)
textFieldSource.send_keys(Keys.RETURN)
try:
char_usage = driver.find_element_by_xpath('//*[@id="root"]/div/div[1]/section/div/div[1]/div[1]/div/p[1]').text
char_usage1 = char_usage[0:4]
char_usage2 = char_usage1.strip()
char_count = int(char_usage2)
except Exception:
char_count = 0
pass
if char_count >= 4500:
print(char_count)
# Pause to avoid errors
time.sleep(5)
# Push the Translate button
translate_button = driver.find_element_by_xpath('//*[@id="btnTranslate"]').click()
# Pause to avoid errors
time.sleep(5)
# Store the translated text as a string
translatedText = driver.find_element_by_xpath('//*[@id="txtTarget"]')
translatedText_content = translatedText.text
# Clean the output text and print
sentencesCleanBatches[batch] = translatedText_content.split("\n")
batch += 1
# Clear the input field
textFieldSource.clear()
# Pause to avoid errors
time.sleep(5)
# Push the Translate button
translate_button = driver.find_element_by_xpath('//*[@id="btnTranslate"]').click()
# Pause to avoid errors
time.sleep(5)
# Store the translated text as a string
translatedText = driver.find_element_by_xpath('//*[@id="txtTarget"]')
translatedText_content = translatedText.text
# Clean the output text and print
sentencesCleanBatches[batch] = translatedText_content.split("\n")
# Concatenate all the batches into once list
for i in range(1, batch + 1):
sentencesClean = sentencesClean + sentencesCleanBatches[i]
# Close the browser window
driver.quit()
# Return necessary objects
return
```
<Overlap Ratio: 0.9801356589147286>

---

--- 236 --
Question ID: dbdec1ad3f0ea63990d793307077e1b18001ef46_2
Original Code:
```
def print_mat(n):
    maxlen = 0
    for row in n:
        for f in row:
            if len(str(f)) > maxlen:
                maxlen = len(str(f))
    for row in n:
        row_str = ""
        for f in row:
            difflen = maxlen - len(str(f))
            sep = " "
            for i in range(difflen):
                sep = sep + " "
            row_str = row_str + sep + str(f)
        print(row_str)
```


Overlapping Code:
```
:
maxlen = 0
for row in n:
for f in row:
if len(str(f)) > maxlen:
maxlen = len(str(f))
for row in n:
row_str = ""
for f in row:
difflen = maxlen - len(str(f))
sep = " "
for i in range(difflen):
sep = sep + " "
row_str = row_str + sep + str(f)
print(r
```
<Overlap Ratio: 0.9157509157509157>

---

--- 237 --
Question ID: ad1bfd456a20de9b88243f631836c545f7b4696f_18
Original Code:
```
def symm2str(r,t=np.zeros([3, 1], dtype=float)):
    xyz = "xyz"
    string_all = []
    for i in range(3):
        string_row = ""
        for j in range(3):
            rval = r[i,j]
            if rval == -1:
                string_row += "-{}".format(xyz[j])
            if rval == 1:
                if string_row != "":
                    string_row += "+"
                string_row += "{}".format(xyz[j])
        tval = t[i,0]
        if tval%1 == 0.5:
            tval = "+1/2"
        elif tval%1 == 0.25:
            tval = "+1/4"
        elif tval%1 == 0.75:
            tval = "+3/4"
        elif tval%1 == 1.0/3.0:
            tval = "+1/3"
        elif tval%1 == 2.0/3.0:
            tval = "+2/3"
        elif tval%1 == 1.0/6.0:
            tval = "+1/6"
        elif tval%1 == 5.0/6.0:
            tval = "+5/6"
        else:
            tval = ""
        string_row += "{}".format(tval)
        if string_row == "":
            string_row = "0"
        string_all.append(string_row)
    return ", ".join(string_all)
```


Overlapping Code:
```
 dtype=float)):
xyz = "xyz"
string_all = []
for i in range(3):
string_row = ""
for j in range(3):
rval = r[i,j]
if rval == -1:
string_row += "-{}".format(xyz[j])
if rval == 1:
if string_row != "":
string_row += "+"
string_row += "{}".format(xyz[j])
tval = t[i,0]
if tval%1 == 0.5:
tval = "+1/2"
elif tval%1 == 0.25:
tval = "+1/4"
elif tval%1 == 0.75:
tval = "+3/4"
elif tval%1 == 1.0/3.0:
tval = "+1/3"
elif tval%1 == 2.0/3.0:
tval = "+2/3"
elif tval%1 == 1.0/6.0:
tval = "+1/6"
elif tval%1 == 5.0/6.0:
tval = "+5/6"
else:
tval = ""
string_row += "{}".format(tval)
if string_row == "":
string_row = "0"
string_all.append(string_row)
return ", ".join(
```
<Overlap Ratio: 0.9365994236311239>

---

--- 238 --
Question ID: 59181b8e3372489fe073e8503ed1828d4bead9ca_8
Original Code:
```
def url_get_parent(url):
    """
    http://one/two/three  =>  http://one/two
    http://one            =>  http://one
    """
    index = url.rfind("/")
    if index > 8:           # avoid https://
        return url[0:index]
    else:
        return url
```


Overlapping Code:
```
rent(url):
"""
http://one/two/three => http://one/two
http://one => http://one
"""
index = url.rfind("/")
if index > 8: # avoid https://
return url[0:
```
<Overlap Ratio: 0.8021390374331551>

---

--- 239 --
Question ID: fc80231c6c6cfbc08f59cba6b6ed371e9b7a2a7f_0
Original Code:
```
def decorator_blank_line_and_sleep(function):
    """ This decorator just waits one second and prints a blank line after executing function"""
    def inner_wrapper(*args, **kwargs):
        input("")
        value = function(*args, **kwargs)
        print()
        return value
    return inner_wrapper
```


Overlapping Code:
```
_blank_line_and_sleep(function):
""" This decorator just waits one second and prints a blank line after executing function"""
def inner_wrapper(*args, **kwargs):
input("")
value = function(*args, **kw
```
<Overlap Ratio: 0.7692307692307693>

---

--- 240 --
Question ID: c7b317f22e40f5de16f8d702d737dfbbfe164a5d_0
Original Code:
```
@sched.scheduled_job('cron', hour=2, minute=00, day_of_week='sat')
def dividend_run():
    loop = 8
    while loop >= 0:
        try:
            DividendFinancingRecorder().run()
            RightsIssueDetailRecorder().run()
            SPODetailRecorder().run()
            DividendDetailRecorder().run()

            break
        except Exception as e:
            loop -= 1
            logger.exception('eastmoney dividend_run runner error:{}'.format(e))
            time.sleep(60*(10-loop))

```


Overlapping Code:
```
hed.scheduled_job('cron', hour=2, minute=00, day_of_week='sat')
def dividend_run():
loop = 8
while loop >= 0:
try:
DividendFinancingRecorder().run()
RightsIssueDetailRecorder().run()
SPODetailRecorder().run()
DividendDetailRecorder().run()
break
except Exception as e:
loop -= 1
logger.exception('eastmoney dividend_run runner error:{}'.format(e))
time.sleep(60*(10-loo
```
<Overlap Ratio: 0.984>

---

--- 241 --
Question ID: 7cb5b926f98e1cea539ac04b351eeffb1251b000_10
Original Code:
```
def gh_store_quad_mesh_edge_polyedge(quad_mesh, u, v):
	polyedge = quad_mesh.collect_polyedge(u, v)
	polyline = rs.AddPolyline([quad_mesh.vertex_coordinates(vkey) for vkey in polyedge])
	return polyedge, polyline
```


Overlapping Code:
```
olyedge(quad_mesh, u, v):
polyedge = quad_mesh.collect_polyedge(u, v)
polyline = rs.AddPolyline([quad_mesh.vertex_coordinates(vkey) for vkey in polyedg
```
<Overlap Ratio: 0.722488038277512>

---

--- 242 --
Question ID: bb765b1b232f5f8a00ee5350e917ce8b9edd2da1_0
Original Code:
```
def rectangularVolumeMask(coords,radius,height=None,rcom=None):
    if rcom is None: rcom = np.zeros(3)
    if height is None: height = radius  

    r2s = (coords-rcom)**2
    x_indices = r2s[:,0]<=radius**2
    y_indices = r2s[:,1]<=radius**2

    z_indices = r2s[:,2]<=height**2
    return np.logical_and(np.logical_and(x_indices,y_indices),z_indices)
```


Overlapping Code:
```
s,radius,height=None,rcom=None):
if rcom is None: rcom = np.zeros(3)
if height is None: height = radius 
r2s = (coords-rcom)**2
x_indices = r2s[:,0]<=radius**2
y_indices = r2s[:,1]<=radius**2
z_indices = r2s[:,2]<=height**2
return np.logical_and(np.l
```
<Overlap Ratio: 0.7739938080495357>

---

--- 243 --
Question ID: ad11f1bee346b20475f233ce5a92683d6b0b38bf_1
Original Code:
```
def test_plots_modify_should_not_change_lockfile(
    tmp_dir, dvc, run_copy_metrics, custom_template
):
    _write_json(tmp_dir, [{"a": 1, "b": 2}], "metric_t.json")
    run_copy_metrics(
        "metric_t.json",
        "metric.json",
        plots_no_cache=["metric.json"],
        name="copy-metrics",
        single_stage=False,
    )

    (tmp_dir / PIPELINE_LOCK).unlink()
    dvc.plots.modify(
        "metric.json", props={"template": relpath(custom_template)}
    )
    assert not (tmp_dir / PIPELINE_LOCK).exists()
```


Overlapping Code:
```
fy_should_not_change_lockfile(
tmp_dir, dvc, run_copy_metrics, custom_template
):
_write_json(tmp_dir, [{"a": 1, "b": 2}], "metric_t.json")
run_copy_metrics(
"metric_t.json",
"metric.json",
plots_no_cache=["metric.json"],
name="copy-metrics",
single_stage=False,
)
(tmp_dir / PIPELINE_LOCK).unlink()
dvc.plots.modify(
"metric.json", props={"template": relpath(custom_template)}
)
assert not (tmp_dir / PIPELINE_LOCK).exis
```
<Overlap Ratio: 0.9481981981981982>

---

--- 244 --
Question ID: 6b6e4879a7f160c3a987091402db4e7bffcd3699_18
Original Code:
```
def test_cd_dataframe():
	df = pd.DataFrame({"Voltage(V)":[1,2,3], "Smoothed_dQ/dV": [4,5,6]})
	proper_V = df['Voltage(V)']
	proper_dqdv = df["Smoothed_dQ/dV"]
	try:
		descriptors.fitters.cd_dataframe(proper_V,proper_dqdv, 'd')
		#this should fail because if dqdv is negative, it is probably discharge data and we want to flip to be positive for peak finding
	except (AssertionError):
		pass
	else:
		raise Exception ("Exception not handled by asserts")

	return
```


Overlapping Code:
```
cd_dataframe():
df = pd.DataFrame({"Voltage(V)":[1,2,3], "Smoothed_dQ/dV": [4,5,6]})
proper_V = df['Voltage(V)']
proper_dqdv = df["Smoothed_dQ/dV"]
try:
descriptors.fitters.cd_dataframe(proper_V,proper_dqdv, 'd')
#this should fail because if dqdv is negative, it is probably discharge data and we want to flip to be positive for peak finding
except (AssertionError):
pass
else:
raise Exception ("Exception not handled by asser
```
<Overlap Ratio: 0.9551569506726457>

---

--- 245 --
Question ID: 8ca7caaa0a55affaa1c1a80379fc6d3e4bbd965a_13
Original Code:
```
async def test_if_fires_on_multiple_user_ids(hass, calls, context_with_user):
    """Test the firing of event when the trigger has multiple user ids."""
    assert await async_setup_component(
        hass,
        automation.DOMAIN,
        {
            automation.DOMAIN: {
                "trigger": {
                    "platform": "event",
                    "event_type": "test_event",
                    "event_data": {},
                    "context": {"user_id": [context_with_user.user_id, "another id"]},
                },
                "action": {"service": "test.automation"},
            }
        },
    )

    hass.bus.async_fire("test_event", {}, context=context_with_user)
    await hass.async_block_till_done()
    assert len(calls) == 1
```


Overlapping Code:
```
f_fires_on_multiple_user_ids(hass, calls, context_with_user):
"""Test the firing of event when the trigger has multiple user ids."""
assert await async_setup_component(
hass,
automation.DOMAIN,
{
automation.DOMAIN: {
"trigger": {
"platform": "event",
"event_type": "test_event",
"event_data": {},
"context": {"user_id": [context_with_user.user_id, "another id"]},
},
"action": {"service": "test.automation"},
}
},
)
hass.bus.async_fire("test_event", {}, context=context_with_user)
await hass.async_block_till_done()
assert len(calls) == 1
```
<Overlap Ratio: 0.9711191335740073>

---

--- 246 --
Question ID: bc4dffea4962fa9f87f4fd8f0dccf1e491ba0a0f_3
Original Code:
```
def write_shah_data(data, questiondict, targetpath):
    corpus = {}

    #write train.pos.txt and train.neg.txt
    with io.open(targetpath + "/train.pos.txt", 'w', encoding="utf-8") as pos:
        with io.open(targetpath + "/train.neg.txt", 'w', encoding="utf-8") as neg:
            for d in data:
                pos.write(d[0] + '\t' + d[1] + '\n')
                for n in d[2]:
                    neg.write(d[0] + '\t' + n + '\n')

    # d question, answer, list(100 random neg answers)  
    with gzip.open(targetpath + "/corpus.tsv.gz", 'wt', encoding='utf-8') as c:
        for d in data:
            if not d[0] in corpus:
                title = prep(questiondict[d[0]]['TITLE'])
                body = prep(questiondict[d[0]]['BODY'])
                line = d[0] + '\t' + title + '\t' + body + '\n'
                c.write(line)
                corpus[d[0]] = {'TITLE': title, 'BODY': body}

            for n in d[2]:
                if not n in corpus:
                    title = prep(questiondict[n]['TITLE'])
                    body = prep(questiondict[n]['BODY'])
                    line = n + '\t' + title + '\t' + body + '\n'
                    c.write(line)
                    corpus[n] = {'TITLE': title, 'BODY': body}
```


Overlapping Code:
```
iondict, targetpath):
corpus = {}
#write train.pos.txt and train.neg.txt
with io.open(targetpath + "/train.pos.txt", 'w', encoding="utf-8") as pos:
with io.open(targetpath + "/train.neg.txt", 'w', encoding="utf-8") as neg:
for d in data:
pos.write(d[0] + '\t' + d[1] + '\n')
for n in d[2]:
neg.write(d[0] + '\t' + n + '\n')
# d question, answer, list(100 random neg answers) 
with gzip.open(targetpath + "/corpus.tsv.gz", 'wt', encoding='utf-8') as c:
for d in data:
if not d[0] in corpus:
title = prep(questiondict[d[0]]['TITLE'])
body = prep(questiondict[d[0]]['BODY'])
line = d[0] + '\t' + title + '\t' + body + '\n'
c.write(line)
corpus[d[0]] = {'TITLE': title, 'BODY': body}
for n in d[2]:
if not n in corpus:
title = prep(questiondict[n]['TITLE'])
body = prep(questiondict[n]['BODY'])
line = n + '\t' + title + '\t' + body + '\n'
c.write(line)
corpus[n] = {'TITLE': title, 'BODY': body
```
<Overlap Ratio: 0.9653304442036836>

---

--- 247 --
Question ID: 4afdd419dc71244616cb7fa1fa0d29c8e678751a_3
Original Code:
```
def process_format(variant, my_sample_idx, format_ids_values, write_block):

    format_ids_in_variant = str(variant).split('\t')[8].split(':')
    sample_data_in_variant = str(variant).rstrip('\n').split('\t')[9::]


    for nth in my_sample_idx:
        nth_sample = sample_data_in_variant[nth].split(':')

        # update the values for each keys in the dictionary
        format_ids_vals = list(zip(format_ids_in_variant, nth_sample))

        for ks, vs in format_ids_vals:
            if ks in format_ids_values.keys():
                format_ids_values[ks] = vs    # condition: if ks in format_ids_values.keys()


        # Now, write the values to each FORMAT field, for each sample
        for vs in format_ids_values.values():
            write_block.write('\t' + str(vs))
```


Overlapping Code:
```
ample_idx, format_ids_values, write_block):
format_ids_in_variant = str(variant).split('\t')[8].split(':')
sample_data_in_variant = str(variant).rstrip('\n').split('\t')[9::]
for nth in my_sample_idx:
nth_sample = sample_data_in_variant[nth].split(':')
# update the values for each keys in the dictionary
format_ids_vals = list(zip(format_ids_in_variant, nth_sample))
for ks, vs in format_ids_vals:
if ks in format_ids_values.keys():
format_ids_values[ks] = vs # condition: if ks in format_ids_values.keys()
# Now, write the values to each FORMAT field, for each sample
for vs in format_ids_values.va
```
<Overlap Ratio: 0.8915304606240714>

---

--- 248 --
Question ID: c827dcf472f9c3a54a8d6e47ea5c2911bca311fc_2
Original Code:
```
def cancelChainlinkRequest(sender, requestId, payment, callbackFunctionId, expiration):
    RequireWitness(sender)
    oracle = Get(GetContext(), concatKey(PENDING_REQUESTS_PREFIX, requestId))
    params = [sender, requestId, payment, GetCallingScriptHash(), callbackFunctionId, expiration]
    assert (DynamicCallFunction(bytearray_reverse(oracle), "cancelOracleRequest", params))
    Delete(GetContext(), concatKey(PENDING_REQUESTS_PREFIX, requestId))
    ChainlinkCancelledEvent(requestId)
    return True
```


Overlapping Code:
```
(sender, requestId, payment, callbackFunctionId, expiration):
RequireWitness(sender)
oracle = Get(GetContext(), concatKey(PENDING_REQUESTS_PREFIX, requestId))
params = [sender, requestId, payment, GetCallingScriptHash(), callbackFunctionId, expiration]
assert (DynamicCallFunction(bytearray_reverse(oracle), "cancelOracleRequest", params))
Delete(GetContext(), concatKey(PENDING_REQUESTS_PREFIX, requestId))
ChainlinkCancelledEvent(requestId)
return 
```
<Overlap Ratio: 0.9375>

---

--- 249 --
Question ID: 0aa09b2ae31833ea0394320b6b1c3a156b07dad0_0
Original Code:
```
def __getParam__(line: str):
    key = aigpy.string.getSub(line, "--", ":")
    value = aigpy.string.getSub(line, ":", ";")
    return key, value
```


Overlapping Code:
```
: str):
key = aigpy.string.getSub(line, "--", ":")
value = aigpy.string.getSub(line, ":", ";")
return key,
```
<Overlap Ratio: 0.7969924812030075>

---

--- 250 --
Question ID: 226217e2534a16358b206e917692823728cf2c67_6
Original Code:
```
def test_get_case_no_allegations():
    """
    Test case detail without allegation info.
    :return:
    """
    case_info = get_case("02-RC-023360")
    assert_equal(len(case_info["allegations"]), 0)
```


Overlapping Code:
```
st_get_case_no_allegations():
"""
Test case detail without allegation info.
:return:
"""
case_info = get_case("02-RC-023360")
assert_equal(len(case_in
```
<Overlap Ratio: 0.8426966292134831>

---

--- 251 --
Question ID: bad8a37752f433d0f64e35629b026451a9b7f31b_4
Original Code:
```
def read_info(file_name):
    info = []
    with open(file_name, "rb") as file:
        if file.read(4) != b'\x00\xA0\x00\x00':
            print("Incorrect magic!")
            return
        else:
            file.seek(0)

        read_node(info, file)            

    return info
```


Overlapping Code:
```
read_info(file_name):
info = []
with open(file_name, "rb") as file:
if file.read(4) != b'\x00\xA0\x00\x00':
print("Incorrect magic!")
return
else:
file.seek(0)
read_node
```
<Overlap Ratio: 0.8535353535353535>

---

--- 252 --
Question ID: 3a3eac2f902dedabd9a07416e292de3a98f668b5_27
Original Code:
```
def run_worker_build(client,
    host,
    artifact,
    build_number,
    environment,
    dependency,
    provider,
    component,
    command,
    previous_outputs,
    builds):
    class Worker(Thread):
        def run(self):
            self.success = False
            exit_code_path = get_exit_code_path(project_directory, environment, provider, component, command, build_number)
            if os.path.exists(exit_code_path):
                os.remove(exit_code_path)
            self.error = False
            lock.acquire()
            builds_filename = get_builds_filename(environment, provider, component, command)
            ensure_file(builds_filename)
            build_data = json.loads(open(builds_filename).read())
            this_build = {
                "success": False,
                "remote": True,
                "build_number": build_number,
                "reference": dependency,
                "status": "running",
                "environment": environment
            }
            state["running"].append(this_build)
            mark_dependency_as_running(dependency, environment)
            build_data["builds"].append(this_build)
            write_builds_file(builds_filename, build_data)
            lock.release()
            last_successful_build = find_last_successful_build(builds)
            if last_successful_build:
                try:
                    last_logfile = "builds/logs/{:03d}-{}-{}-{}-{}.log".format(last_successful_build["build_number"],
                        environment,
                        provider, component,
                        command)

                    last_size = os.stat(last_logfile).st_size
                    this_build["last_size"] = last_size
                    this_build["log_file"] = log_filename
                except Exception as e:
                    pass


            print("Creating remote directories")
            work_directory = "builds"
            cmd = client.run_command("mkdir -p builds/exits")
            client.join(cmd)
            cmd = client.run_command("mkdir -p builds/outputs")
            client.join(cmd)
            cmd = client.run_command("mkdir -p builds/envs")
            client.join(cmd)
            cmd = client.run_command("mkdir -p builds/logs")
            client.join(cmd)
            cmd = client.run_command("mkdir -p builds/published")
            client.join(cmd)


            env = construct_environment("", build_number, environment, provider, component, command, previous_outputs)
            work_path = "builds/work/{}.{}.{}.{}".format(environment, provider, component, command)
            make_work_path = client.run_command("mkdir -p {}".format(work_path))
            client.join(make_work_path)

            print("Unpacking artifact remotely")
            unpack_command = "tar -xvf {} -C {}".format(artifact, work_path)

            extract = client.run_command(unpack_command)
            client.join(extract)
            env_file_name = "{}.{}.{}.{}.{}".format(environment, provider, component, command, build_number)
            env_file_path = os.path.join(project_directory, "builds/envs/", env_file_name)
            env_file = open(env_file_path, "w")
            for key, value in env.items():
                escaped_value = value
                if " " in value:
                    escaped_value = "\"" + value + "\""
                env_file.write("{}={}".format(key, escaped_value))

                env_file.write("\n")
            env_file.flush()
            env_file.close()
            print("Sending envs file to worker")
            remote_env_path = os.path.join("builds/envs", env_file_name)
            cmds = client.scp_send(env_file_path, remote_env_path, True)
            joinall(cmds, raise_error=True)
            ran_remotely = False
            build_stdout = None
            build_stderr = None
            test_command = "test -f {}/{}/{}".format(work_path, provider, command)
            print(test_command)
            print(work_path)
            test_exists = client.run_command(test_command)
            client.join(test_exists)

            print("Running require")
            chosen_file = run_require("", environment, provider, component, command)
            if chosen_file:
                 print(chosen_file)
                 destination_path = os.path.join(work_path, os.path.basename(chosen_file))
                 print("Copying {} to remote {}".format(chosen_file, destination_path))

                 # cmds = client.scp_send(chosen_file, destination_path, True)
                 # joinall(cmds, raise_error=True)
                 rsync_command = "rsync -Pav -e \"ssh -i {}\" {} {}@{}:{}".format(
                    args.workers_key[0],
                    chosen_file,
                    args.workers_user,
                    host,
                    destination_path)
                 print(rsync_command)
                 rsync = Popen(["bash", "-c", rsync_command])
                 rsync.communicate()

                 unzip_command = "cd {} ; tar -xf {}".format(work_path, os.path.basename(chosen_file))
                 unzip_artifact = client.run_command(unzip_command)
                 print(unzip_command)
                 client.join(unzip_artifact)


            if test_exists[host]["exit_code"] == 0:
                print("Running build remotely")
                ran_remotely = True
                remote_log_filename = "builds/logs/{}-{}.{}.{}.{}.log".format(build_number, environment, provider, component, command)
                build_command = client.run_command("""set -a ;
                    source {} ;
                    export OUTPUT_PATH=$(readlink -f ${{OUTPUT_PATH}}) ;
                    export EXIT_CODE_PATH=$(readlink -f ${{EXIT_CODE_PATH}}) ;
                    export ARTIFACT_PATH=$(readlink -f ${{ARTIFACT_PATH}}) ;
                    cd {}/{} ;
                    ./{} {} {}""".format(remote_env_path, work_path, provider, command, environment, component, remote_log_filename))

            else:
                lock.acquire()
                builds_filename = get_builds_filename(environment, provider, component, command)
                ensure_file(builds_filename)
                build_data = json.loads(open(builds_filename).read())
                this_build = build_data["builds"][-1]
                print("Not implemented")
                this_build["success"] = True
                self.success = True
                open(os.path.join(project_directory,
                    "builds/outputs/{}.{}.{}.{}.outputs.json"
                    .format(environment, provider, component, command)), 'w').write("{}")
                write_builds_file(builds_filename, build_data)
                lock.release()
                remove_from_running(dependency, environment)
                open(os.path.join(get_last_run_path(environment, provider, component, command)), 'w').write(':)')
                return
            if ran_remotely:
                print("Setting up logs...")
                logfile = open("builds/logs/{}-{}.{}.{}.{}.log".format(build_number, environment, provider, component, command), "w")

                client.join(build_command)
                for line in build_command[host]["stdout"]:
                     logfile.write(line + "\n")
                for line in build_command[host]["stderr"]:
                    logfile.write(line + "\n")
                print("Remote build finished")
                remove_from_running(dependency, environment)

                print("Downloading outputs...")
                dest_output = os.path.join(project_directory, "builds/outputs/{}.{}.{}.{}.outputs.json".format(environment, provider, component, command))

                receive_outputs = client.copy_remote_file(os.path.join("builds/outputs/{}.{}.{}.{}.outputs.json".format(environment, provider, component, command)), dest_output)

                joinall(receive_outputs)
                os.rename("{}_{}".format(dest_output, host), dest_output)

                print("Downloading exit code...")
                dest_exit_code = os.path.join(project_directory, "builds/exits/{}.{}.{}.{}.{}.exitcode".format(environment, provider, component, command, build_number))
                receive_exit_code = client.copy_remote_file(os.path.join("builds/exits/{}.{}.{}.{}.{}.exitcode".format(environment, provider, component, command, build_number)), dest_exit_code)

                joinall(receive_exit_code, raise_error=True)
                os.rename("{}_{}".format(dest_exit_code, host), dest_exit_code)

                print("Downloading artifact...")
                check_for_artifacts = client.run_command("test -f {}".format("builds/published/{}.{}.{}.{}.{}.tgz".format(environment, provider, component, command, build_number)))
                client.join(check_for_artifacts)

                if check_for_artifacts[host]["exit_code"] == 0:
                    dest_artifact = os.path.join(project_directory, "builds/published/{}.{}.{}.{}.{}.tgz".format(environment, provider, component, command, build_number))
                    receive_artifact = client.copy_remote_file(os.path.join("builds/published/{}.{}.{}.{}.{}.tgz".format(environment, provider, component, command, build_number)), dest_artifact)
                    joinall(receive_artifact, raise_error=True)
                    os.rename("{}_{}".format(dest_artifact, host), dest_artifact)

                last_running_build = Component(dependency, environment, provider, component, command, args).calculate_state()
                if last_running_build:
                    self.success = last_running_build["success"]

    worker = Worker()
    worker.start()
    return worker
```


Overlapping Code:
```
f run_worker_build(client,
host,
artifact,
build_number,
environment,
dependency,
provider,
component,
command,
previous_outputs,
builds):
class Worker(Thread):
def run(self):
self.success = False
exit_code_path = get_exit_code_path(project_directory, environment, provider, component, command, build_number)
if os.path.exists(exit_code_path):
os.remove(exit_code_path)
self.error = False
lock.acquire()
builds_filename = get_builds_filename(environment, provider, component, command)
ensure_file(builds_filename)
build_data = json.loads(open(builds_filename).read())
this_build = {
"success": False,
"remote": True,
"build_number": build_number,
"reference": dependency,
"status": "running",
"environment": environment
}
state["running"].append(this_build)
mark_dependency_as_running(dependency, environment)
build_data["builds"].append(this_build)
write_builds_file(builds_filename, build_data)
lock.release()
last_successful_build = find_last_successful_build(builds)
if last_successful_build:
try:
last_logfile = "builds/logs/{:03d}-{}-{}-{}-{}.log".format(last_successful_build["build_number"],
environment,
provider, component,
command)
last_size = os.stat(last_logfile).st_size
this_build["last_size"] = last_size
this_build["log_file"] = log_filename
except Exception as e:
pass
print("Creating remote directories")
work_directory = "builds"
cmd = client.run_command("mkdir -p builds/exits")
client.join(cmd)
cmd = client.run_command("mkdir -p builds/outputs")
client.join(cmd)
cmd = client.run_command("mkdir -p builds/envs")
client.join(cmd)
cmd = client.run_command("mkdir -p builds/logs")
client.join(cmd)
cmd = client.run_command("mkdir -p builds/published")
client.join(cmd)
env = construct_environment("", build_number, environment, pr
```
<Overlap Ratio: 0.9909399773499433>

---

--- 253 --
Question ID: e1cc0ef5ec4e5be0789aef291bf492822295e7a1_2
Original Code:
```
def convert_setplot(setplot_file='setplot.py'):

    setplot_text = open(setplot_file).read()
    setplot_text = setplot_text.replace('pyclaw.plotters','clawpack.visclaw')
    setplot_text = setplot_text.replace('2d_grid','2d_patch')
    setplot_text = setplot_text.replace('gridedges','patchedges')
    setplot_text = setplot_text.replace('gridlines','celledges')
    setplot_text = setplot_text.replace('grid_bgcolor','patch_bgcolor')

    os.system("mv %s %s_4.x" % (setplot_file,setplot_file))
    print('Moved %s to %s_4.x ' % (setplot_file,setplot_file))
    open(setplot_file,'w').write(setplot_text)
    print('===> Created ', setplot_file)
```


Overlapping Code:
```
(setplot_file='setplot.py'):
setplot_text = open(setplot_file).read()
setplot_text = setplot_text.replace('pyclaw.plotters','clawpack.visclaw')
setplot_text = setplot_text.replace('2d_grid','2d_patch')
setplot_text = setplot_text.replace('gridedges','patchedges')
setplot_text = setplot_text.replace('gridlines','celledges')
setplot_text = setplot_text.replace('grid_bgcolor','patch_bgcolor')
os.system("mv %s %s_4.x" % (setplot_file,setplot_file))
print('Moved %s to %s_4.x ' % (setplot_file,setplot_file))
open(setplot_file,'w').write(setplot_text)
```
<Overlap Ratio: 0.9075907590759076>

---

--- 254 --
Question ID: 71a185bd30d18da2519ede069f3fb3bae056479b_1
Original Code:
```
def Kaggle_IoU_Precision(y_true, y_pred, threshold=0.5):
    y_pred = K.squeeze(tf.cast(y_pred > threshold, tf.int32), -1)
    y_true = K.cast(y_true[..., 0], K.floatx())
    y_pred = K.cast(y_pred, K.floatx())
    truth_areas = K.sum(y_true, axis=[1, 2])
    pred_areas = K.sum(y_pred, axis=[1, 2])
    intersection = K.sum(y_true * y_pred, axis=[1, 2])
    union = K.clip(truth_areas + pred_areas - intersection, 1e-9, 128 * 128)
    check = K.map_fn(lambda x: K.equal(x, 0),
                     truth_areas + pred_areas, dtype=tf.bool)
    p = intersection / union
    iou = K.switch(check, p + 1., p)

    prec = K.map_fn(lambda x: K.mean(
        K.greater(x, np.arange(0.5, 1.0, 0.05))), iou, dtype=tf.float32)

    prec_iou = K.mean(prec)
    return prec_iou
```


Overlapping Code:
```
Kaggle_IoU_Precision(y_true, y_pred, threshold=0.5):
y_pred = K.squeeze(tf.cast(y_pred > threshold, tf.int32), -1)
y_true = K.cast(y_true[..., 0], K.floatx())
y_pred = K.cast(y_pred, K.floatx())
truth_areas = K.sum(y_true, axis=[1, 2])
pred_areas = K.sum(y_pred, axis=[1, 2])
intersection = K.sum(y_true * y_pred, axis=[1, 2])
union = K.clip(truth_areas + pred_areas - intersection, 1e-9, 128 * 128)
check = K.map_fn(lambda x: K.equal(x, 0),
truth_areas + pred_areas, dtype=tf.bool)
p = intersection / union
iou = K.switch(check, p + 1., p)
prec = K.map_fn(lambda x: K.mean(
K.greater(x, np.arange(0.5, 1.0, 0.05))), iou, dtype=tf.float32)
prec_iou = K.mean(prec)
return prec_
```
<Overlap Ratio: 0.9897510980966325>

---

--- 255 --
Question ID: 1a33654f19204ceeb8cfb59ff9ad6cdec7391d7d_0
Original Code:
```
def rename_feature(feature, feature_type):
    try:
        if feature[feature_type] in non_uniq[feature_type]:
            feature[feature_type] = "_".join([
                feature[feature_type], feature['sequence'],
                feature['start'], feature['end'], feature['strand']
            ])
    except KeyError:
        pass
    return feature
```


Overlapping Code:
```
f rename_feature(feature, feature_type):
try:
if feature[feature_type] in non_uniq[feature_type]:
feature[feature_type] = "_".join([
feature[feature_type], feature['sequence'],
feature['start'], feature['end'], feature['strand']
])
except KeyError:
p
```
<Overlap Ratio: 0.9259259259259259>

---

--- 256 --
Question ID: c82e7167822ec2d042db7a5b1c4ad889f7a70e6d_1
Original Code:
```
def formatString(value, length, fLen):
    fString = '{0:0.'+str(fLen) +'f}'
    string = fString.format(value)
    padding = length - len(string)
    if padding != 0:
        for i in range(padding):
            string = ' ' + string
    return string    
```


Overlapping Code:
```
en):
fString = '{0:0.'+str(fLen) +'f}'
string = fString.format(value)
padding = length - len(string)
if padding != 0:
for i in range(padding):
string 
```
<Overlap Ratio: 0.7075471698113207>

---

--- 257 --
Question ID: 665fb9b308ef46f86b3755679d32ccd7e787916a_8
Original Code:
```
def get_action_form(request, obj, action):
    action_function = action['function']
    action_verbose_name = action['verbose_name']
    initial = action['initial']
    action_input = action['input']
    action_choices = action['choices']
    action_display = action['display']
    app_label = get_metadata(type(obj), 'app_label')
    func = getattr(obj, action_function.__name__, action_function)

    if initial and hasattr(obj, initial):
        initial = getattr(obj, initial)()
    else:
        initial = {}
    if action_choices and hasattr(obj, action_choices):
        action_choices = getattr(obj, action_choices)()
    else:
        action_choices = {}

    if action_input:
        # it is a form name
        if type(action_input) in [str, str] and '.' not in action_input:
            full_app_name = settings.APP_MAPPING.get(app_label, app_label)
            fromlist = app_label
            module = __import__('{}.forms'.format(full_app_name), fromlist=list(map(str, [app_label])))
            form_cls = getattr(module, action_input)

        # it is a model or model name
        else:
            if type(action_input) in [str, str]:
                app_name, class_name = action_input.split('.')
                action_input = apps.get_model(app_name, class_name)

            class Form(forms.ModelForm):
                class Meta:
                    model = action_input
                    fields = get_parameters_names(func)
                    title = action_verbose_name
                    submit_label = action_verbose_name

            form_cls = Form
    else:
        class Form(forms.ModelForm):
            class Meta:
                model = func.__self__.__class__
                fields = get_parameters_names(func)
                title = action_verbose_name
                submit_label = action_verbose_name

        form_cls = Form

    if issubclass(form_cls, forms.ModelForm):
        for key in list(initial.keys()):
            if hasattr(obj, key) and obj.pk and getattr(obj, key):
                del (initial[key])
        form = form_cls(request, instance=obj, initial=initial)
    else:
        form = form_cls(request, initial=initial)

    if action_display:
        for lookup in action_display:
            label = get_fiendly_name(func.__self__.__class__, lookup)
            value = getattr2(obj, lookup)
            form.fields[lookup] = forms.CharField(
                label=label, initial=value, required=False, widget=forms.widgets.DisplayInput(value)
            )

    if action_choices:
        for field_name in action_choices:
            form.fields[field_name].queryset = action_choices[field_name]

    if not obj.pk:
        verbose_name = get_metadata(obj.__class__, 'verbose_name')
        form.fields['instance'] = forms.ModelChoiceField(type(obj).objects.all(), label=verbose_name)
        if form.fieldsets:
            form.fieldsets = ((verbose_name, {'fields': ('instance',)}),) + form.fieldsets

    return form
```


Overlapping Code:
```
request, obj, action):
action_function = action['function']
action_verbose_name = action['verbose_name']
initial = action['initial']
action_input = action['input']
action_choices = action['choices']
action_display = action['display']
app_label = get_metadata(type(obj), 'app_label')
func = getattr(obj, action_function.__name__, action_function)
if initial and hasattr(obj, initial):
initial = getattr(obj, initial)()
else:
initial = {}
if action_choices and hasattr(obj, action_choices):
action_choices = getattr(obj, action_choices)()
else:
action_choices = {}
if action_input:
# it is a form name
if type(action_input) in [str, str] and '.' not in action_input:
full_app_name = settings.APP_MAPPING.get(app_label, app_label)
fromlist = app_label
module = __import__('{}.forms'.format(full_app_name), fromlist=list(map(str, [app_label])))
form_cls = getattr(module, action_input)
# it is a model or model name
else:
if type(action_input) in [str, str]:
app_name, class_name = action_input.split('.')
action_input = apps.get_model(app_name, class_name)
class Form(forms.ModelForm):
class Meta:
model = action_input
fields = get_parameters_names(func)
title = action_verbose_name
submit_label = action_verbose_name
form_cls = Form
else:
class Form(forms.ModelForm):
class Meta:
model = func.__self__.__class__
fields = get_parameters_names(func)
title = action_verbose_name
submit_label = action_verbose_name
form_cls = Form
if issubclass(form_cls, forms.ModelForm):
for key in list(initial.keys()):
if hasattr(obj, key) and obj.pk and getattr(obj, key):
del (initial[key])
form = form_cls(request, instance=obj, initial=initial)
else:
form = form_cls(request, initial=initial)
if action_display:
for lookup in action_display:
label = get_fiendly_name(func.__self__.__class__, lookup)
value = getattr2(obj, lookup)
form.fields[lookup] = forms.CharField(
label=label, initial=value, required=False, wi
```
<Overlap Ratio: 0.9778692743180648>

---

--- 258 --
Question ID: 68dae762b1051faf827e121f6af8f11505a356e5_6
Original Code:
```
def _MakeEagerLogicalBlob(op_attribute, obn, blob_register):
    lbi = op_attribute.arg_signature.bn_in_op2lbi[obn]
    blob_object = blob_register.GetObject4BlobName(
        "%s/%s" % (lbi.op_name, lbi.blob_name)
    )
    mirrored_sig_map = op_attribute.mirrored_signature.bn_in_op2opt_mirrored_parallel
    if mirrored_sig_map[obn].HasField("mirrored_parallel"):
        return remote_blob_util.EagerMirroredBlob(lbi, blob_object)
    else:
        return remote_blob_util.EagerConsistentBlob(lbi, blob_object)
```


Overlapping Code:
```
agerLogicalBlob(op_attribute, obn, blob_register):
lbi = op_attribute.arg_signature.bn_in_op2lbi[obn]
blob_object = blob_register.GetObject4BlobName(
"%s/%s" % (lbi.op_name, lbi.blob_name)
)
mirrored_sig_map = op_attribute.mirrored_signature.bn_in_op2opt_mirrored_parallel
if mirrored_sig_map[obn].HasField("mirrored_parallel"):
return remote_blob_util.EagerMirroredBlob(lbi, blob_object)
else:
return remote_blob_util.EagerConsistentBlob(lbi, blob_o
```
<Overlap Ratio: 0.9656652360515021>

---

--- 259 --
Question ID: 1d605b197cfb37e2bb233810d218c66618736ab9_2
Original Code:
```
def parse_degmin(degmin,sign):
    if sign in "EW":
        if len(degmin) < 5:
            return np.nan
        dec=my_float(degmin[:3]) + my_float(degmin[3:])/60
    else:
        if len(degmin) < 4:
            return np.nan
        dec=my_float(degmin[:2]) + my_float(degmin[2:])/60
    if sign in "WS":
        dec*=-1
    elif sign in 'NE':
        pass
    else:
        return np.nan
    return dec
```


Overlapping Code:
```
degmin(degmin,sign):
if sign in "EW":
if len(degmin) < 5:
return np.nan
dec=my_float(degmin[:3]) + my_float(degmin[3:])/60
else:
if len(degmin) < 4:
return np.nan
dec=my_float(degmin[:2]) + my_float(degmin[2:])/60
if sign in "WS":
dec*=-1
elif sign i
```
<Overlap Ratio: 0.8250825082508251>

---

--- 260 --
Question ID: 7395848f17a2b2be80fce364d79347b0bafb3b67_7
Original Code:
```
def highestship(f):
    highest = "freighters"
    for ship in v.shipindices:
        if f.__dict__[ship] > 0:
            highest = ship
    return highest
```


Overlapping Code:
```
highest = "freighters"
for ship in v.shipindices:
if f.__dict__[ship] > 0:
highest = ship
return hig
```
<Overlap Ratio: 0.8064516129032258>

---

--- 261 --
Question ID: fe824b9c43aecc8116bb749db31107476d4f2f42_0
Original Code:
```
def generate_batches(dataset, batch_size, shuffle=True,
                     drop_last=True, device="cpu"):
    """
    A generator function which wraps the PyTorch DataLoader. It will 
      ensure each tensor is on the write device location.
    """
    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,
                            shuffle=shuffle, drop_last=drop_last)

    for data_dict in dataloader:
        out_data_dict = {}
        for name, tensor in data_dict.items():
            out_data_dict[name] = data_dict[name].to(device)
        yield out_data_dict
```


Overlapping Code:
```
ef generate_batches(dataset, batch_size, shuffle=True,
drop_last=True, device="cpu"):
"""
A generator function which wraps the PyTorch DataLoader. It will 
ensure each tensor is on the write device location.
"""
dataloader = DataLoader(dataset=dataset, batch_size=batch_size,
shuffle=shuffle, drop_last=drop_last)
for data_dict in dataloader:
out_data_dict = {}
for name, tensor in data_dict.items():
out_data_dict[name] = data_dict[name].to(device)
yield out_data_dict
```
<Overlap Ratio: 0.997872340425532>

---

--- 262 --
Question ID: 36aaad06482e7c8d04fa1bb1da3d6a25d88ddc45_0
Original Code:
```
def get_autoincrement(sqlite):
    if (sqlite):
        return "AUTOINCREMENT"
    else:
        return "AUTO_INCREMENT"
```


Overlapping Code:
```
f get_autoincrement(sqlite):
if (sqlite):
return "
```
<Overlap Ratio: 0.5208333333333334>

---

--- 263 --
Question ID: ceee44d934e8b84acda78f0b31d76ab5898ee66e_1
Original Code:
```
@cli.group()
@click.pass_context
def env(ctx):
    """
    super.AI Config operations
    """
    pass
```


Overlapping Code:
```
t
def env(ctx):
"""
super.AI Config operations
"""
```
<Overlap Ratio: 0.5813953488372093>

---

--- 264 --
Question ID: e0907fabe3ceff995bc4eb0c5c4499e4c71d9301_1
Original Code:
```
def isdigit(c: str) -> bool:
    try:
        c = int(c)
    except:
        return False
    return True
```


Overlapping Code:
```
r) -> bool:
try:
c = int(c)
except:
return False
r
```
<Overlap Ratio: 0.6493506493506493>

---

--- 265 --
Question ID: b36010a1c05ae0de61fa1341d49cbcdd5a6a14c0_1
Original Code:
```
def _get_build_number(build_folder: str, build_def: BuildDef) -> Optional[str]:
    """Get the build number of a completed build by inspecting the produced
    assembly.

    Will return None if an error occurred.
    """
    assembly_path = _get_assembly_path(build_def)
    if assembly_path is None:
        return None

    assembly_path = path.join(build_folder, assembly_path)

    # we're going whole-hog here and loading the PE file itself to figure out
    # the version number... writing it to a TXT file during the Unity build
    # wasn't producing consistent version numbers as it seemed to be building
    # the assembly twice and only writing the number after the first build
    try:
        pe = pefile.PE(assembly_path)
        ver_info = pe.VS_FIXEDFILEINFO[0]
        ver_ms = ver_info.ProductVersionMS
        ver_ls = ver_info.ProductVersionLS
        major = ver_ms >> 16  # version numbers are 2 bytes each - grab em like this
        minor = ver_ms & 0xFFFF
        build = ver_ls >> 16
        patch = ver_ls & 0xFFFF
        return f"{major}.{minor}.{build}.{patch}"
    except OSError as err:
        logging.error(f"Could not open built assembly: {err}")
        return None
```


Overlapping Code:
```
ef _get_build_number(build_folder: str, build_def: BuildDef) -> Optional[str]:
"""Get the build number of a completed build by inspecting the produced
assembly.
Will return None if an error occurred.
"""
assembly_path = _get_assembly_path(build_def)
if assembly_path is None:
return None
assembly_path = path.join(build_folder, assembly_path)
# we're going whole-hog here and loading the PE file itself to figure out
# the version number... writing it to a TXT file during the Unity build
# wasn't producing consistent version numbers as it seemed to be building
# the assembly twice and only writing the number after the first build
try:
pe = pefile.PE(assembly_path)
ver_info = pe.VS_FIXEDFILEINFO[0]
ver_ms = ver_info.ProductVersionMS
ver_ls = ver_info.ProductVersionLS
major = ver_ms >> 16 # version numbers are 2 bytes each - grab em like this
minor = ver_ms & 0xFFFF
build = ver_ls >> 16
patch = ver_ls & 0xFFFF
return f"{major}.{minor}.{build}.{patch}"
except OSError as err:
logging.error(f"C
```
<Overlap Ratio: 0.9523809523809523>

---

--- 266 --
Question ID: 075b83f5bee7dd380ccf7b6290b97b9f0f35720b_16
Original Code:
```
def test_get_next_with_no_child_and_some_open_neighbors_returns_first_available():
    """
    Verifies that `get_next` returns the first available neighbor if some are open.
    """
    grid = Grid(((3, -1, -1), (-1, 2, -1), (0, -1, -1)))
    # Set partial path from "3"
    draw_path(grid, ((0, 0), (0, 1), (0, 2), (1, 2)))
    assert get_next(grid[1][1]).location == (2, 1)
```


Overlapping Code:
```
pen_neighbors_returns_first_available():
"""
Verifies that `get_next` returns the first available neighbor if some are open.
"""
grid = Grid(((3, -1, -1), (-1, 2, -1), (0, -1, -1)))
# Set partial path from "3"
draw_path(grid, ((0, 0), (0, 1), (0, 2), (1, 2)))
assert get_next(grid[1][1]).location == 
```
<Overlap Ratio: 0.8620689655172413>

---

--- 267 --
Question ID: 04d4a0d0403cc682e167109d526d0965af180363_0
Original Code:
```
@eel.expose
def set_known_folder(folder):
    if folder:
        try:
            global sr
            sr = FaceRecogniser(folder)
            if sr:
                return "Success initializing"
        except Exception as ex:
            return f"Error initializing -- {ex}"

    else:
        return "Error initializing -- No folder"
```


Overlapping Code:
```
older(folder):
if folder:
try:
global sr
sr = FaceRecogniser(folder)
if sr:
return "Success initializing"
except Exception as ex:
return f"Error initializing -- {ex}"
else:
return "Error initializing 
```
<Overlap Ratio: 0.8333333333333334>

---

--- 268 --
Question ID: 210f4184067b1376ef51ab6ca1f142db872e3747_0
Original Code:
```
def get_schemas():
    schemas = list()
    schemas.append(hello_tablelike.generate_schema())
    schemas.append(hello_tablelike_sub.generate_schema())
    return schemas
```


Overlapping Code:
```
pend(hello_tablelike.generate_schema())
schemas.append(hello_tablelike_sub.generate_schema())
return
```
<Overlap Ratio: 0.6493506493506493>

---

--- 269 --
Question ID: 0b239bbc8646acaed661338bd322cb9b63d610ab_4
Original Code:
```
def cache_data(fn, fun):
    if os.path.exists(fn):
        print(f'{fn} exists, using cached version')
        return pd.read_csv(fn)
    else:
        print(f'{fn} does not exist, creating file')
        df = fun()
        df.to_csv(fn, index=False)
        return df
```


Overlapping Code:
```
 fun):
if os.path.exists(fn):
print(f'{fn} exists, using cached version')
return pd.read_csv(fn)
else:
print(f'{fn} does not exist, creating file')
df 
```
<Overlap Ratio: 0.7089201877934272>

---

--- 270 --
Question ID: 7fb76cf201d9a53c275ed7ab989adb6c7fc32445_1
Original Code:
```
def _create_hvplot():
    # Generate some data
    cl1 = np.random.normal(loc=2, scale=0.2, size=(200, 200))
    cl2x = np.random.normal(loc=-2, scale=0.6, size=200)
    cl2y = np.random.normal(loc=-2, scale=0.1, size=200)
    cl3 = np.random.normal(loc=0, scale=1.5, size=(400, 400))
    # Create an overlay of points and ellipses
    clusters = (
        hv.Points(cl1).opts(color="blue")
        * hv.Points((cl2x, cl2y)).opts(color="green")
        * hv.Points(cl3).opts(color="#FDDC22")
    )
    plot = clusters * hv.Ellipse(2, 2, 2) * hv.Ellipse(-2, -2, (4, 2))
    return plot
```


Overlapping Code:
```
eate_hvplot():
# Generate some data
cl1 = np.random.normal(loc=2, scale=0.2, size=(200, 200))
cl2x = np.random.normal(loc=-2, scale=0.6, size=200)
cl2y = np.random.normal(loc=-2, scale=0.1, size=200)
cl3 = np.random.normal(loc=0, scale=1.5, size=(400, 400))
# Create an overlay of points and ellipses
clusters = (
hv.Points(cl1).opts(color="blue")
* hv.Points((cl2x, cl2y)).opts(color="green")
* hv.Points(cl3).opts(color="#FDDC22")
)
plot = clusters * hv.Ellipse(2, 2, 2) * hv.Ellipse(-
```
<Overlap Ratio: 0.9365384615384615>

---

--- 271 --
Question ID: 18f5db71e32cae282455bb290f90114661bab366_5
Original Code:
```
def main():
    fields = {
        "host": {"required": True, "type": "str"},
        "username": {"required": True, "type": "str"},
        "password": {"required": False, "type": "str", "no_log": True},
        "vdom": {"required": False, "type": "str", "default": "root"},
        "https": {"required": False, "type": "bool", "default": True},
        "system_dhcp_server": {
            "required": False, "type": "dict",
            "options": {
                "state": {"required": True, "type": "str",
                          "choices": ["present", "absent"]},
                "auto-configuration": {"required": False, "type": "str",
                                       "choices": ["disable", "enable"]},
                "conflicted-ip-timeout": {"required": False, "type": "int"},
                "ddns-auth": {"required": False, "type": "str",
                              "choices": ["disable", "tsig"]},
                "ddns-key": {"required": False, "type": "str"},
                "ddns-keyname": {"required": False, "type": "str"},
                "ddns-server-ip": {"required": False, "type": "str"},
                "ddns-ttl": {"required": False, "type": "int"},
                "ddns-update": {"required": False, "type": "str",
                                "choices": ["disable", "enable"]},
                "ddns-update-override": {"required": False, "type": "str",
                                         "choices": ["disable", "enable"]},
                "ddns-zone": {"required": False, "type": "str"},
                "default-gateway": {"required": False, "type": "str"},
                "dns-server1": {"required": False, "type": "str"},
                "dns-server2": {"required": False, "type": "str"},
                "dns-server3": {"required": False, "type": "str"},
                "dns-service": {"required": False, "type": "str",
                                "choices": ["local", "default", "specify"]},
                "domain": {"required": False, "type": "str"},
                "exclude-range": {"required": False, "type": "list",
                                  "options": {
                                      "end-ip": {"required": False, "type": "str"},
                                      "id": {"required": True, "type": "int"},
                                      "start-ip": {"required": False, "type": "str"}
                                  }},
                "filename": {"required": False, "type": "str"},
                "forticlient-on-net-status": {"required": False, "type": "str",
                                              "choices": ["disable", "enable"]},
                "id": {"required": True, "type": "int"},
                "interface": {"required": False, "type": "str"},
                "ip-mode": {"required": False, "type": "str",
                            "choices": ["range", "usrgrp"]},
                "ip-range": {"required": False, "type": "list",
                             "options": {
                                 "end-ip": {"required": False, "type": "str"},
                                 "id": {"required": True, "type": "int"},
                                 "start-ip": {"required": False, "type": "str"}
                             }},
                "ipsec-lease-hold": {"required": False, "type": "int"},
                "lease-time": {"required": False, "type": "int"},
                "mac-acl-default-action": {"required": False, "type": "str",
                                           "choices": ["assign", "block"]},
                "netmask": {"required": False, "type": "str"},
                "next-server": {"required": False, "type": "str"},
                "ntp-server1": {"required": False, "type": "str"},
                "ntp-server2": {"required": False, "type": "str"},
                "ntp-server3": {"required": False, "type": "str"},
                "ntp-service": {"required": False, "type": "str",
                                "choices": ["local", "default", "specify"]},
                "options": {"required": False, "type": "list",
                            "options": {
                                "code": {"required": False, "type": "int"},
                                "id": {"required": True, "type": "int"},
                                "ip": {"required": False, "type": "str"},
                                "type": {"required": False, "type": "str",
                                         "choices": ["hex", "string", "ip"]},
                                "value": {"required": False, "type": "str"}
                            }},
                "reserved-address": {"required": False, "type": "list",
                                     "options": {
                                         "action": {"required": False, "type": "str",
                                                    "choices": ["assign", "block", "reserved"]},
                                         "description": {"required": False, "type": "str"},
                                         "id": {"required": True, "type": "int"},
                                         "ip": {"required": False, "type": "str"},
                                         "mac": {"required": False, "type": "str"}
                                     }},
                "server-type": {"required": False, "type": "str",
                                "choices": ["regular", "ipsec"]},
                "status": {"required": False, "type": "str",
                           "choices": ["disable", "enable"]},
                "tftp-server": {"required": False, "type": "list",
                                "options": {
                                    "tftp-server": {"required": True, "type": "str"}
                                }},
                "timezone": {"required": False, "type": "str",
                             "choices": ["01", "02", "03",
                                         "04", "05", "81",
                                         "06", "07", "08",
                                         "09", "10", "11",
                                         "12", "13", "74",
                                         "14", "77", "15",
                                         "87", "16", "17",
                                         "18", "19", "20",
                                         "75", "21", "22",
                                         "23", "24", "80",
                                         "79", "25", "26",
                                         "27", "28", "78",
                                         "29", "30", "31",
                                         "32", "33", "34",
                                         "35", "36", "37",
                                         "38", "83", "84",
                                         "40", "85", "41",
                                         "42", "43", "39",
                                         "44", "46", "47",
                                         "51", "48", "45",
                                         "49", "50", "52",
                                         "53", "54", "55",
                                         "56", "57", "58",
                                         "59", "60", "62",
                                         "63", "61", "64",
                                         "65", "66", "67",
                                         "68", "69", "70",
                                         "71", "72", "00",
                                         "82", "73", "86",
                                         "76"]},
                "timezone-option": {"required": False, "type": "str",
                                    "choices": ["disable", "default", "specify"]},
                "vci-match": {"required": False, "type": "str",
                              "choices": ["disable", "enable"]},
                "vci-string": {"required": False, "type": "list",
                               "options": {
                                   "vci-string": {"required": True, "type": "str"}
                               }},
                "wifi-ac1": {"required": False, "type": "str"},
                "wifi-ac2": {"required": False, "type": "str"},
                "wifi-ac3": {"required": False, "type": "str"},
                "wins-server1": {"required": False, "type": "str"},
                "wins-server2": {"required": False, "type": "str"}

            }
        }
    }

    module = AnsibleModule(argument_spec=fields,
                           supports_check_mode=False)
    try:
        from fortiosapi import FortiOSAPI
    except ImportError:
        module.fail_json(msg="fortiosapi module is required")

    global fos
    fos = FortiOSAPI()

    is_error, has_changed, result = fortios_system_dhcp(module.params, fos)

    if not is_error:
        module.exit_json(changed=has_changed, meta=result)
    else:
        module.fail_json(msg="Error in repo", meta=result)
```


Overlapping Code:
```
def main():
fields = {
"host": {"required": True, "type": "str"},
"username": {"required": True, "type": "str"},
"password": {"required": False, "type": "str", "no_log": True},
"vdom": {"required": False, "type": "str", "default": "root"},
"https": {"required": False, "type": "bool", "default": True},
"system_dhcp_server": {
"required": False, "type": "dict",
"options": {
"state": {"required": True, "type": "str",
"choices": ["present", "absent"]},
"auto-configuration": {"required": False, "type": "str",
"choices": ["disable", "enable"]},
"conflicted-ip-timeout": {"required": False, "type": "int"},
"ddns-auth": {"required": False, "type": "str",
"choices": ["disable", "tsig"]},
"ddns-key": {"required": False, "type": "str"},
"ddns-keyname": {"required": False, "type": "str"},
"ddns-server-ip": {"required": False, "type": "str"},
"ddns-ttl": {"required": False, "type": "int"},
"ddns-update": {"required": False, "type": "str",
"choices": ["disable", "enable"]},
"ddns-update-override": {"required": False, "type": "str",
"choices": ["disable", "enable"]},
"ddns-zone": {"required": False, "type": "str"},
"default-gateway": {"required": False, "type": "str"},
"dns-server1": {"required": False, "type": "str"},
"dns-server2": {"required": False, "type": "str"},
"dns-server3": {"required": False, "type": "str"},
"dns-service": {"required": False, "type": "str",
"choices": ["local", "default", "specify"]},
"domain": {"required": False, "type": "str"},
"exclude-range": {"required": False, "type": "list",
"options": {
"end-ip": {"required": False, "type": "str"},
"id": {"required": True, "type": "int"},
"start-ip": {"required": False, "type": "str"}
}},
"filename": {"required": Fals
```
<Overlap Ratio: 0.9872167344567112>

---

--- 272 --
Question ID: 7b51f5033b7fb5ec575deb4c087d1c4e4eb32f93_1
Original Code:
```
def construct_payload(pipeline_name: str, pipeline_state: str, pipeline_stage: str):
    payload = {
        "state": "error",
        "context": "default"
    }
    codepipeline_url = "https://{region}.console.aws.amazon.com/codepipeline/home?region={region}#/view/{pipeline_name}"
    payload["context"] = pipeline_stage
    payload["target_url"] = codepipeline_url.format(region="us-east-1",pipeline_name=pipeline_name)
    if pipeline_state == "STARTED":
        payload["state"] = "pending"
        payload["description"] = "Running " + pipeline_stage
    if pipeline_state == "FAILED":
        payload["state"] = "failure"
        payload["description"] = "Failed to run " + pipeline_stage
    if pipeline_state == "SUCCEEDED":
        payload["state"] = "success"
        payload["description"] = "Successfully ran " + pipeline_stage
    return payload
```


Overlapping Code:
```
ine_name: str, pipeline_state: str, pipeline_stage: str):
payload = {
"state": "error",
"context": "default"
}
codepipeline_url = "https://{region}.console.aws.amazon.com/codepipeline/home?region={region}#/view/{pipeline_name}"
payload["context"] = pipeline_stage
payload["target_url"] = codepipeline_url.format(region="us-east-1",pipeline_name=pipeline_name)
if pipeline_state == "STARTED":
payload["state"] = "pending"
payload["description"] = "Running " + pipeline_stage
if pipeline_state == "FAILED":
payload["state"] = "failure"
payload["description"] = "Failed to run " + pipeline_stage
if pipeline_state == "SUCCEEDED":
payload["state"] = "success"
payload["description"] = "Successfully ran "
```
<Overlap Ratio: 0.922266139657444>

---

--- 273 --
Question ID: 7271352807c2c5854d5b0bb5b3cd4fd112210bfc_0
Original Code:
```
def readPower():
    try:
        requestP = modbus_client.read_holding_registers(10, 2, unit=1)
        print(requestP.registers)
        power_demand = requestP.registers[0]
        power_supply = requestP.registers[1]
    except Exception as ex:
        # logger.exception("READPOWER Exception")
        print(ex)
        print("Exception in Modbus Reading")
        requestP = modbus_client.read_holding_registers(10, 2, unit=1)
        power_demand = requestP.registers[0]
        power_supply = requestP.registers[1]
    return power_demand, power_supply
```


Overlapping Code:
```
ower():
try:
requestP = modbus_client.read_holding_registers(10, 2, unit=1)
print(requestP.registers)
power_demand = requestP.registers[0]
power_supply = requestP.registers[1]
except Exception as ex:
# logger.exception("READPOWER Exception")
print(ex)
print("Exception in Modbus Reading")
requestP = modbus_client.read_holding_registers(10, 2, unit=1)
power_demand = requestP.registers[0]
power_supply = requestP.registers[1]
return power_demand, pow
```
<Overlap Ratio: 0.9615384615384616>

---

--- 274 --
Question ID: 4de4aba63f64dfbd07fe311092dd95e306a0d5a0_0
Original Code:
```
@pytest.fixture(scope="module")
def start_cluster():
    try:
        cluster.start()

        yield cluster
    finally:
        cluster.shutdown()
```


Overlapping Code:
```
@pytest.fixture(scope="module")
def start_cluster():
try:
cluster.start()
yield cluster
finally:
cluster.shutdown()
```
<Overlap Ratio: 1.0>

---

--- 275 --
Question ID: e9e1490a0f09f2b18bb81b050e808323b4018b62_0
Original Code:
```
def parse_industry_code_dict(industry):
    data_container = {}
    for order_book_id, industry_dict in industry.items():
        industry_container = {}
        for criterion, industry_code_dict in industry_dict.items():
            industry_container[criterion] = industry_code_dict["industry_code"]
        data_container[order_book_id] = industry_container
    data_df = pd.DataFrame(data_container).transpose()
    data_df.index.name = "order_book_id"
    return data_df
```


Overlapping Code:
```
dustry_code_dict(industry):
data_container = {}
for order_book_id, industry_dict in industry.items():
industry_container = {}
for criterion, industry_code_dict in industry_dict.items():
industry_container[criterion] = industry_code_dict["industry_code"]
data_container[order_book_id] = industry_container
data_df = pd.DataFrame(data_container).transpose()
data_df.index.name = "order_book_id"
return 
```
<Overlap Ratio: 0.954653937947494>

---

--- 276 --
Question ID: b3427a061396ce44eb1ffe4b3906a8a3af55e5a1_2
Original Code:
```
def test_should_return_failure_when_find_stream_by_stream_id_fails(
    get_stream_details_use_case: GetStreamDetailsUseCase, find_stream_by_stream_id: Mock
) -> None:
    stream_id = uuid4()
    failure = FailureDetails(reason="TEST_FIND_STREAM_FAILS")
    find_stream_by_stream_id.return_value = Failure(failure)

    actual = get_stream_details_use_case(stream_id)

    find_stream_by_stream_id.assert_called_once()
    find_stream_by_stream_id.assert_called_with(stream_id)
    assert isinstance(actual, Result.failure_type)
    assert isinstance(actual.failure(), FailureDetails)
    assert failure == actual.failure()
```


Overlapping Code:
```
when_find_stream_by_stream_id_fails(
get_stream_details_use_case: GetStreamDetailsUseCase, find_stream_by_stream_id: Mock
) -> None:
stream_id = uuid4()
failure = FailureDetails(reason="TEST_FIND_STREAM_FAILS")
find_stream_by_stream_id.return_value = Failure(failure)
actual = get_stream_details_use_case(stream_id)
find_stream_by_stream_id.assert_called_once()
find_stream_by_stream_id.assert_called_with(stream_id)
assert isinstance(actual, Result.failure_type)
assert isinstance(actual.failure(), FailureDetails)
assert failure == actual.failure()
```
<Overlap Ratio: 0.9466437177280551>

---

--- 277 --
Question ID: 16d23cdf8171322b2fc9c1e4894149bde6d3cf03_6
Original Code:
```
def validate_height(height: str):
    if height[-2:] == 'cm':
        nr = int(height[:height.index('cm')])
        if 150 <= nr <= 193:
            return True
    if height[-2:] == 'in':
        nr = int(height[:height.index('in')])
        if 59 <= nr <= 76:
            return True
    return False
```


Overlapping Code:
```
alidate_height(height: str):
if height[-2:] == 'cm':
nr = int(height[:height.index('cm')])
if 150 <= nr <= 193:
return True
if height[-2:] == 'in':
nr = int(height[:height.index('in')])
if 59 <= nr <=
```
<Overlap Ratio: 0.8547008547008547>

---

--- 278 --
Question ID: fd8f516ba68ea0a417c745ad518f1d606a0edcf4_0
Original Code:
```
def digraph_of_expectation(G, theta=0.005, iterations=1000, seed=None):
	if (seed == None):
		seed = G.nodes()
	tau = {}
	edge_list = []
	for v in tqdm(seed):
		seed_v = {v}
		exp = graph_of_expectation(G=G, iterations=iterations, seed=seed_v)
		remove = [v]
		for i in exp:
			exp[i] /= iterations
			if exp[i] < theta :
				remove.append(i)
		for rem in remove:
			del exp[rem]
		tau[v] = exp
		for u in tau[v]:
			edge_list.append((v,u))
	DG = nx.DiGraph()
	DG.add_edges_from(edge_list)
	return DG, tau
```


Overlapping Code:
```
005, iterations=1000, seed=None):
if (seed == None):
seed = G.nodes()
tau = {}
edge_list = []
for v in tqdm(seed):
seed_v = {v}
exp = graph_of_expectation(G=G, iterations=iterations, seed=seed_v)
remove = [v]
for i in exp:
exp[i] /= iterations
if exp[i] < theta :
remove.append(i)
for rem in remove:
del exp[rem]
tau[v] = exp
for u in tau[v]:
edge_list.append((v,u))
DG = nx.DiGraph()
DG.add_edges_fr
```
<Overlap Ratio: 0.8583690987124464>

---

--- 279 --
Question ID: 4a241796f5fc6483411704dc8ffbed36257a96b5_0
Original Code:
```
def get_window():
    '''Returns windows in z-order (top first)'''
    user32 = ctypes.windll.user32
    lst = []
    top = user32.GetTopWindow(None)
    if not top:
        return lst
    lst.append(top)
    while True:
        next = user32.GetWindow(lst[-1], win32con.GW_HWNDNEXT)
        if not next:
            break
        lst.append(next)
    return lst
```


Overlapping Code:
```
''Returns windows in z-order (top first)'''
user32 = ctypes.windll.user32
lst = []
top = user32.GetTopWindow(None)
if not top:
return lst
lst.append(top)
while True:
next = user32.GetWindow(lst[-1], win32con.GW_HWNDNEXT)
if not next:
break
lst.append(next)
re
```
<Overlap Ratio: 0.9055944055944056>

---

--- 280 --
Question ID: 4768f9a389d6ec9e2fd064176285deb25c0d877d_1
Original Code:
```
def demo_distance():
    from ..datasets import public_dataset
    data = public_dataset(name="iris")
    x = data.iloc[:, 0]
    y = data.iloc[:, 1]
    print("Minkowski distance of the 'sepal length(cm)' and 'sepal width(cm)' from the iris data:")
    for p_power in [-2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]:
        p = 2**p_power
        print(f"order p={p: .3f}, Minkowski distance = {distance(p=p).Minkowski(x=x, y=y): .3f}")
    print(f"Euclidean distance = {distance().Euclidean(x=x, y=y): .3f}")
```


Overlapping Code:
```
():
from ..datasets import public_dataset
data = public_dataset(name="iris")
x = data.iloc[:, 0]
y = data.iloc[:, 1]
print("Minkowski distance of the 'sepal length(cm)' and 'sepal width(cm)' from the iris data:")
for p_power in [-2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]:
p = 2**p_power
print(f"order p={p: .3f}, Minkowski distance = {distance(p=p).Minkowski(x=x, y=y): .3f}")
print(f"Euclidean distance = {distance(
```
<Overlap Ratio: 0.8997821350762527>

---

--- 281 --
Question ID: 143202c13cf0a8902db0774b74811a68d447cd4b_1
Original Code:
```
def GTF(gtf):
	d_name = {}
	d_id = {}

	sList = []
	with open(gtf) as f:
		for line in f:
			if not line.startswith('#'):
				(chrid, source, genetype, start, end, score, strand, phase, attribute_string) = line.rstrip().split('\t')
				attributes = {}
	
				if genetype=='gene':
					key_value_pair_set = attribute_string.split('; ')
					for key_value_pair in key_value_pair_set[: -1]:
					    key, value = key_value_pair.split(' ')
					    attributes[key] = value[1: -1] ### remove the first str and last str
					### specially for last field
					key, value = key_value_pair_set[-1][: -1].split(' ')
					attributes[key] = value[1: -1]
					
					gene_id = attributes.get('gene_id')
					gene_name = attributes.get('gene_name')
					gene_type = attributes.get('gene_biotype')
	
					#print(gene_id, gene_name, gene_type)
					d_name[gene_name] = gene_type
					d_id[gene_id] = gene_type

					sList.append([gene_id, gene_name, gene_type])
	'''
	df_name =pd.Series(d_name)
	df_name_csv = "gene_name.type.txt"
	df_name.to_csv(df_name_csv, header=False, sep='\t')

	df_id=pd.Series(d_id)
	df_id_csv = "gene_id.type.txt"
	df_id.to_csv(df_id_csv, header=False, sep='\t')
	'''
	df = pd.DataFrame(sList)
	df_csv = "gene_type.txt"
	df.to_csv(df_csv, header=False, index=True, sep='\t')
```


Overlapping Code:
```
d_name = {}
d_id = {}
sList = []
with open(gtf) as f:
for line in f:
if not line.startswith('#'):
(chrid, source, genetype, start, end, score, strand, phase, attribute_string) = line.rstrip().split('\t')
attributes = {}

if genetype=='gene':
key_value_pair_set = attribute_string.split('; ')
for key_value_pair in key_value_pair_set[: -1]:
key, value = key_value_pair.split(' ')
attributes[key] = value[1: -1] ### remove the first str and last str
### specially for last field
key, value = key_value_pair_set[-1][: -1].split(' ')
attributes[key] = value[1: -1]

gene_id = attributes.get('gene_id')
gene_name = attributes.get('gene_name')
gene_type = attributes.get('gene_biotype')

#print(gene_id, gene_name, gene_type)
d_name[gene_name] = gene_type
d_id[gene_id] = gene_type
sList.append([gene_id, gene_name, gene_type])
'''
df_name =pd.Series(d_name)
df_name_csv = "gene_name.type.txt"
df_name.to_csv(df_name_csv, header=False, sep='\t')
df_id=pd.Series(d_id)
df_id_csv = "gene_id.type.txt"
df_id.to_csv(df_id_csv, header=False, sep='\t')
'''
df = pd.DataFrame(sList)
df_csv = "gene_type.txt"
df.to
```
<Overlap Ratio: 0.9466437177280551>

---

--- 282 --
Question ID: c57dccfa7699fdddff62f1aa8b166a2bff1de595_1
Original Code:
```
def downgrade():
    bind = op.get_bind()
    session = db.Session(bind=bind)

    for slc in session.query(Slice).filter(Slice.viz_type == 'pie').all():
        try:
            params = json.loads(slc.params)

            if 'metric' in params:
                if params['metric']:
                    params['metrics'] = [params['metric']]

                del params['metric']
                slc.params = json.dumps(params, sort_keys=True)
        except Exception:
            pass

    session.commit()
    session.close()
```


Overlapping Code:
```
def downgrade():
bind = op.get_bind()
session = db.Session(bind=bind)
for slc in session.query(Slice).filter(Slice.viz_type == 'pie').all():
try:
params = json.loads(slc.params)
if 'metric' in params:
if params['metric']:
params['metrics'] = [params['metric']]
del params['metric']
slc.params = json.dumps(params, sort_keys=True)
except Exception:
pass
session.commit()
session.close(
```
<Overlap Ratio: 0.9974025974025974>

---

--- 283 --
Question ID: d3f2ff996811b092cea80924609eb5edc78d47dd_2
Original Code:
```
def check_interactive_message_manager(payload):
    user_id = payload["original_message"]["attachments"][0]["callback_id"]
    user_info = get_user_info(user_id)
    user_profile = user_info["profile"]
    tool = payload["actions"][0]["value"]
    
    if payload["actions"][0]["name"] == "approve":
        #tool_access = check_record(user_profile["email"], "jira", "tool_status")
        #tool_access = check_record("test2@gmail.com", tool, "tool_status")
        tool_access = "no"
        
        if tool_access == "no":
            request_admin(payload)
            message = "Request forwarded to <@" + "U015QP5QHN0>" + " for `" + tool + "` access request by <@" + user_id + ">"
        else:
            message = "`" + tool + "` access already given to <@" + user_id + ">"
            
        return response_message(message)
    else:
        disapproval_message(payload["callback_id"], "Manager", tool)
        message = "`" + tool + "` access request by <@" + user_id + "> disapproved"
        return response_message(message)
```


Overlapping Code:
```
check_interactive_message_manager(payload):
user_id = payload["original_message"]["attachments"][0]["callback_id"]
user_info = get_user_info(user_id)
user_profile = user_info["profile"]
tool = payload["actions"][0]["value"]

if payload["actions"][0]["name"] == "approve":
#tool_access = check_record(user_profile["email"], "jira", "tool_status")
#tootus")
tool_access = "no"

if tool_access == "no":
request_admin(payload)
message = "Request forwarded to <@" + "U015QP5QHN0>" + " for `" + tool + "` access request by <@" + user_id + ">"
else:
message = "`" + tool + "` access already given to <@" + user_id + ">"

return response_message(message)
else:
disapproval_message(payload["callback_id"], "Manager", tool)
message = "`" + tool + "` access request by <@" + user_id + "> disapproved"
return res
```
<Overlap Ratio: 0.9049773755656109>

---

--- 284 --
Question ID: b3fbe2bd9224fb8aefbcc528ef7c2e8b818e74b7_2
Original Code:
```
def all_blocks(session) -> Dict[str, Any]:
    result = {}
    for type_ in [Junction, BLK, BLM, Signal, Disconnector, Railway, Track_Section, Control_Area]:
        result.update({block.id: block for block in session.query(type_).all()})
    return result
```


Overlapping Code:
```
all_blocks(session) -> Dict[str, Any]:
result = {}
for type_ in [Junction, BLK, BLM, Signal, Disconnector, Railway, Track_Section, Control_Area]:
result.update({block.id: block for block in session.qu
```
<Overlap Ratio: 0.847457627118644>

---

--- 285 --
Question ID: 3b5b9dbc619f35ba55f85d5a86864732ec73d976_2
Original Code:
```
def get_or_create(cls, **kwargs):
    """Get or create a ``cls`` instance using the ``kwargs`` provided."""
    instance = cls.query.filter_by(**kwargs).first()
    if not instance:
        instance = cls(**kwargs)
    return instance
```


Overlapping Code:
```
get_or_create(cls, **kwargs):
"""Get or create a ``cls`` instance using the ``kwargs`` provided."""
instance = cls.query.filter_by(**kwargs).first()
if not instance:
instance = cls(**
```
<Overlap Ratio: 0.8714285714285714>

---

--- 286 --
Question ID: 6c15f275758549ce70287b55bb9e3ca32cb95422_0
Original Code:
```
@classmethod
def Set(cls, instance, overwrite=False):
  assert cls._instance is None or overwrite is True
  assert isinstance(instance, cls)
  cls._instance = instance
```


Overlapping Code:
```
@classmethod
def Set(cls, instance, overwrite=False):
assert cls._instance is None or overwrite is True
assert isinstance(instance, cls)
cls._instance
```
<Overlap Ratio: 0.9316770186335404>

---

--- 287 --
Question ID: 00e841561e2b3c6e15cb5682b194723ed7aaacc5_0
Original Code:
```
def segment_graph(num_vertices,num_edges,edges,c):
    edges = sorted(edges,key=lambda edges:edges[2])
    print(len(edges))
    u = universe(num_vertices)
    threshold = [0 for i in range(num_vertices)]
    for i in range(num_vertices):
        threshold[i] = THRESHOLD(1,c)
    for i in range(num_edges):
        pedge = edges[i]
        # point a from list
        a = u.find(pedge[0])
        # point b from list
        b = u.find(pedge[1])
        # a!=b prevent to cause connected graph
        if a != b:
            if pedge[2] <= threshold[a] and pedge[2] <= threshold[b]:
                u.join(a,b)
                a = u.find(a)
                threshold[a] = pedge[2] + THRESHOLD(u.get_size(a),c)

    return u
```


Overlapping Code:
```
,c):
edges = sorted(edges,key=lambda edges:edges[2])
print(len(edges))
u = universe(num_vertices)
threshold = [0 for i in range(num_vertices)]
for i in range(num_vertices):
threshold[i] = THRESHOLD(1,c)
for i in range(num_edges):
pedge = edges[i]
# point a from list
a = u.find(pedge[0])
# point b from list
b = u.find(pedge[1])
# a!=b prevent to cause connected graph
if a != b:
if pedge[2] <= threshold[a] and pedge[2] <= threshold[b]:
u.join(a,b)
a = u.find(a)
threshold[a] = pedge[2] + THRESHOLD(
```
<Overlap Ratio: 0.8756567425569177>

---

--- 288 --
Question ID: a3d9a8469493ee5f200d6a9d8f927097f95bfa6e_1
Original Code:
```
def start(player):
    clear()
    lawan = random_lawan()
    hp_player = player.hp
    menang = True
    turn = 1

    while player.hp > 0:
        bar()
        print('[ Turn %d ]\n' % turn)

        # player menyerang lawan
        hp_awal_lawan = lawan.hp
        player.attack(lawan)
        
        print('> %s menyerang lawan! (Damage: %d)' % (player.name, player.atk))
        print('HP lawan: %d -> %d' % (hp_awal_lawan, lawan.hp))

        if lawan.hp == 0:
            menang = True
            print('\nKamu menang!'); bar();
            break

        print()

        # lawan menyerang player
        hp_awal_player = player.hp
        lawan.attack(player)

        print('< Lawan menyerang %s! (Damage: %d)' % (player.name, lawan.atk))
        print('HP %s: %d -> %d' % 
            (player.name, hp_awal_player, player.hp))
        
        if player.hp == 0:
            menang = False
            print('\nKamu kalah!'); bar()
            break

        turn += 1
        bar()
        wait()

    player.hp = hp_player
    wait()
    clear()
    if menang:
        player.gain_exp(10)
```


Overlapping Code:
```
an = random_lawan()
hp_player = player.hp
menang = True
turn = 1
while player.hp > 0:
bar()
print('[ Turn %d ]\n' % turn)
# player menyerang lawan
hp_awal_lawan = lawan.hp
player.attack(lawan)

print('> %s menyerang lawan! (Damage: %d)' % (player.name, player.atk))
print('HP lawan: %d -> %d' % (hp_awal_lawan, lawan.hp))
if lawan.hp == 0:
menang = True
print('\nKamu menang!'); bar();
break
print()
# lawan menyerang player
hp_awal_player = player.hp
lawan.attack(player)
print('< Lawan menyerang %s! (Damage: %d)' % (player.name, lawan.atk))
print('HP %s: %d -> %d' % 
(player.name, hp_awal_player, player.hp))

if player.hp == 0:
menang = False
print('\nKamu kalah!'); bar()
break
turn += 1
bar()
wait()
player.hp = hp_player
wait()
clear()
if men
```
<Overlap Ratio: 0.9328358208955224>

---

--- 289 --
Question ID: e9e8e6f9f31ce619785842ccb9a485d745c27002_2
Original Code:
```
@pytest.fixture
def csv_file_with_image(tmp_path, image_file):
    filename = tmp_path / "csv_with_image.csv"
    data = textwrap.dedent(
        f"""\
        image
        {image_file}
        """
    )
    with open(filename, "w") as f:
        f.write(data)
    return str(filename)
```


Overlapping Code:
```
e_with_image(tmp_path, image_file):
filename = tmp_path / "csv_with_image.csv"
data = textwrap.dedent(
f"""\
image
{image_file}
"""
)
with open(filename, "w") as f:
f.write(data)
return str(filename
```
<Overlap Ratio: 0.8761061946902655>

---

--- 290 --
Question ID: 363e5efe809fc0b49d474572ce8794fb6dc275cd_1
Original Code:
```
def access_ext(name):
    #用于扩展名称的获取
    ALLOW_SET = ['jpg', 'jpeg', 'png', 'webp', 'gif']
    li = name.split('.')
    if  len(li) > 1 and li[-1] in ALLOW_SET:
        return "." + li[-1] #返回扩展名

    else:
        #不合法的文件默认保存为无扩展的文件
        #还需要在前端做文件类型的判断
        return ''
```


Overlapping Code:
```
SET = ['jpg', 'jpeg', 'png', 'webp', 'gif']
li = name.split('.')
if len(li) > 1 and li[-1] in ALLOW_SET:
return "." + li[-1] #返回扩展名
else:
#不合法的文件默认保存为无扩展的文
```
<Overlap Ratio: 0.7013574660633484>

---

--- 291 --
Question ID: 66cbba55d8f68c58d685cc45617c4365d6588dda_8
Original Code:
```
@app.delete('/strava/athletes/{strava_athlete_id}')
async def delete_strava_athletes(strava_athlete_id: int, admin: bool = Depends(is_admin)):
    strava_athlete = await StravaAthlete.objects.get(id=strava_athlete_id)

    client = Client(strava_athlete.access_token)
    try:
        client.deauthorize()
    except stravalib_exceptions.AccessUnauthorized:
        strava_athlete = await refresh_access_token(strava_athlete)
        client = Client(strava_athlete.access_token)
        client.deauthorize()

    await strava_athlete.delete()

    return strava_athlete
```


Overlapping Code:
```
pp.delete('/strava/athletes/{strava_athlete_id}')
async def delete_strava_athletes(strava_athlete_id: int, admin: bool = Depends(is_admin)):
strava_athlete = await StravaAthlete.objects.get(id=strava_athlete_id)
client = Client(strava_athlete.access_token)
try:
client.deauthorize()
except stravalib_exceptions.AccessUnauthorized:
strava_athlete = await refresh_access_token(strava_athlete)
client = Client(strava_athlete.access_token)
client.deauthorize()
await strava_athlete.delete()
return strava
```
<Overlap Ratio: 0.9803921568627451>

---

--- 292 --
Question ID: a1f97b80e5bb1a4780c222371d34da0ad9e5298d_0
Original Code:
```
def load_data():
    train_dataset = h5py.File('../Datasets/train_catvnoncat.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File('../Datasets/test_catvnoncat.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes
```


Overlapping Code:
```
d_data():
train_dataset = h5py.File('../Datasets/train_catvnoncat.h5', "r")
train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels
test_dataset = h5py.File('../Datasets/test_catvnoncat.h5', "r")
test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels
classes = np.array(test_dataset["list_classes"][:]) # the list of classes
train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))
return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes
```
<Overlap Ratio: 0.9911392405063291>

---

--- 293 --
Question ID: ce04b0b29c2afca534c792a20617760365209ee1_2
Original Code:
```
@timeit('Part 1')
def part_one(x):
    '''Solves part one'''
    im = format_image(25, 6, x)

    min_zeros = (10000, -1) # N, layer
    print(im.shape)
    for i in range(im.shape[0]):
        n = sum(im[i,:,:].flatten() == 0)
        # print(n)
        if n < min_zeros[0]:
            min_zeros = (n, i)

    n_ones = sum(im[min_zeros[1],:,:].flatten() == 1)
    n_twos = sum(im[min_zeros[1],:,:].flatten() == 2)

    return n_ones*n_twos
```


Overlapping Code:
```
@timeit('Part 1')
def part_one(x):
'''Solves part one'''
im = format_image(25, 6, x)
min_zeros = (10000, -1) # N, layer
print(im.shape)
for i in range(im.shape[0]):
n = sum(im[i,:,:].flatten() == 0)
# print(n)
if n < min_zeros[0]:
min_zeros = (n, i)
n_ones = sum(im[min_zeros[1],:,:].flatten() == 1)
n_twos = sum(im[min_zeros[1],:,:].flatten() == 2)
return n_ones
```
<Overlap Ratio: 0.981081081081081>

---

--- 294 --
Question ID: b0555e848b5dc855d58ac8f786b92bf8e9a5e7e8_3
Original Code:
```
def split_individual_events(
        signal,
        sampling_rate,
        expected_call_max_duration=1.0,
        max_tries=10,
        scale_factor=1.25,
        amp_env_mode="broadband",
    ):
    """Divide a signal interval into individual putative events

    This function assumes that the input is a signal that already contains a lot
    of sound (detected by thresholding) and wants to split it up into individual
    events by creating a second, more conservative threshold.

    It is recommended to include some padding in the signal so that the detector
    can better find a baseline (e.g. include the period between 1.0s and 4.0s
    for a vocal period detected at 2.0s to 3.0s)

    TODO: do this across two channels?
    """
    if signal.ndim == 1:
        signal = signal - np.mean(signal)
    else:
        signal = (signal - np.mean(signal, axis=0))[:, 0]

    amp_env = get_amplitude_envelope(
        signal,
        sampling_rate,
        highpass=1000,
        lowpass=8000,
        mode=amp_env_mode,
    )

    threshold = compute_smart_threshold(amp_env, sampling_rate)
    # Compute intervals within this period, preferentially separating sounds
    # that are separated by more than 20ms of silence.
    # Then, gradually raise the threshold until the longest period detected is
    # no greater than the defined max_length (default 1s)

    idx = 0
    while idx < max_tries:
        intervals = threshold_events(
            amp_env,
            threshold,
            polarity=1,
            sampling_rate=sampling_rate,
            ignore_width=0.02,
            min_size=0.01,
            fuse_duration=0.02,
        )
        durations = [np.diff(x) / sampling_rate for x in intervals]
        if len(durations) and np.max(durations) > expected_call_max_duration:
            threshold *= scale_factor
        else:
            break

        idx += 1

    if not len(intervals):
        return [[0, len(signal)]]
    else:
        return intervals
```


Overlapping Code:
```
ividual_events(
signal,
sampling_rate,
expected_call_max_duration=1.0,
max_tries=10,
scale_factor=1.25,
amp_env_mode="broadband",
):
"""Divide a signal interval into individual putative events
This function assumes that the input is a signal that already contains a lot
of sound (detected by thresholding) and wants to split it up into individual
events by creating a second, more conservative threshold.
It is recommended to include some padding in the signal so that the detector
can better find a baseline (e.g. include the period between 1.0s and 4.0s
for a vocal period detected at 2.0s to 3.0s)
TODO: do this across two channels?
"""
if signal.ndim == 1:
signal = signal - np.mean(signal)
else:
signal = (signal - np.mean(signal, axis=0))[:, 0]
amp_env = get_amplitude_envelope(
signal,
sampling_rate,
highpass=1000,
lowpass=8000,
mode=amp_env_mode,
)
threshold = compute_smart_threshold(amp_env, sampling_rate)
# Compute intervals within this period, preferentially separating sounds
# that are separated by more than 20ms of silence.
# Then, gradually raise the threshold until the longest period detected is
# no greater than the defined max_length (default 1s)
idx = 0
while idx < max_tries:
intervals = threshold_events(
amp_env,
threshold,
polarity=1,
sampling_rate=sampling_rate,
ignore_width=0.02,
min_size=0.01,
fuse_duration=0.02,
)
durations = [np.diff(x) / sampling_rate for x in intervals]
if len(durations) and np.max(durations) > expected_call_max_duration:
threshold *= scale_factor
else:
break
idx += 1
if not len(intervals):
r
```
<Overlap Ratio: 0.9627329192546584>

---

--- 295 --
Question ID: 656a48b4f8e679db86c0772a59cd008ea8429109_0
Original Code:
```
@dirichlet
def _zero_boundary(t, locations):
  del t, locations
  return 0.0
```


Overlapping Code:
```

def _zero_boundary(t, locations):
del t, location
```
<Overlap Ratio: 0.6944444444444444>

---

--- 296 --
Question ID: f7c307658c3455c313677d269a8648f6697b4a82_0
Original Code:
```
def encode_key(key: str) -> str:
    """ convert to a file-stystem safe representation of a URL """

    result = re.sub("https?:.+/", "", key)
    result = re.sub("[/?=&]", "_", result)
    result = result.replace(".aspx", ".html")
    return result
```


Overlapping Code:
```
f encode_key(key: str) -> str:
""" convert to a file-stystem safe representation of a URL """
result = re.sub("https?:.+/", "", key)
result = re.sub("[/?=&]", "_", result)
result = result.replace(".as
```
<Overlap Ratio: 0.8733624454148472>

---

--- 297 --
Question ID: 648e3527d02cb41be6ee1553226a814bc0e72991_4
Original Code:
```
def crt_table_dbs(db_name, sql):
    """
    连接数据库，若不存在则创建，创建表
    :param db_name: 'test.db'
    :param sql: CREATE TABLE COMPANY
                (ID INT PRIMARY KEY     NOT NULL,
                NAME           TEXT    NOT NULL);
    :return: void
    """
    debug_print("crt_table_dbs")
    conn = sqlite3.connect(db_name)
    c = conn.cursor()
    c.execute(sql)
    debug_print("Table created successfully")
    conn.commit()
    conn.close()
```


Overlapping Code:
```
t_table_dbs(db_name, sql):
"""
连接数据库，若不存在则创建，创建表
:param db_name: 'test.db'
:param sql: CREATE TABLE COMPANY
(ID INT PRIMARY KEY NOT NULL,
NAME TEXT NOT NULL);
:return: void
"""
debug_print("crt_table_dbs")
conn = sqlite3.connect(db_name)
c = conn.cursor()
c.execute(sql)
debug_print("Table created successfully")
conn.commit()
conn.close()
```
<Overlap Ratio: 0.9826086956521739>

---

--- 298 --
Question ID: 354aad468386d2ab99eadd09fc2be4ebde3051fe_1
Original Code:
```
@app.route('/station_detail/station_id=<station_id>')
def station_detail(station_id: str):
    url_back = url_for('index')
    logging.debug(f"station_id: {station_id}")
    station = stations.loc[station_id, :]
    stlabel = utils.station_label(station)
    rdf, cprcp, curr_drought_rate, curr_fillrate, curr_fillrate_cdf = \
        utils.drought_rate_data(station_id, current_year, engine=engine)
    dft = totals.loc[totals['station'] == station_id, :]
    f_prcp = utils.cum_prcp_plot(stlabel, rdf, cprcp, curr_drought_rate)
    f_totals = utils.totals_barchart(dft)
    script, div = components(f_prcp)
    script_totals, div_totals = components(f_totals)
    html = render_template(
        'station_detail.html',
        js_resources=js_resources,
        css_resources=css_resources,
        plot_script=script,
        plot_script_totals=script_totals,
        plot_div=div,
        plot_totals=div_totals,
        station=station,
        url_back=url_back)
    return html
```


Overlapping Code:
```
station_id>')
def station_detail(station_id: str):
url_back = url_for('index')
logging.debug(f"station_id: {station_id}")
station = stations.loc[station_id, :]
stlabel = utils.station_label(station)
rdf, cprcp, curr_drought_rate, curr_fillrate, curr_fillrate_cdf = \
utils.drought_rate_data(station_id, current_year, engine=engine)
dft = totals.loc[totals['station'] == station_id, :]
f_prcp = utils.cum_prcp_plot(stlabel, rdf, cprcp, curr_drought_rate)
f_totals = utils.totals_barchart(dft)
script, div = components(f_prcp)
script_totals, div_totals = components(f_totals)
html = render_template(
'station_detail.html',
js_resources=js_resources,
css_resources=css_resources,
plot_script=script,
plot_script_totals=script_totals,
plot_div=div,
plot_totals=div_totals,
station=station,
url_back=url_b
```
<Overlap Ratio: 0.9345794392523364>

---

--- 299 --
Question ID: ed887bb6049ff4625eb036c565dd9926a4a7d433_0
Original Code:
```
async def test_form(hass: HomeAssistant) -> None:
    """Test we get the form."""
    result = await hass.config_entries.flow.async_init(
        DOMAIN, context={"source": config_entries.SOURCE_USER}
    )
    assert result["type"] == RESULT_TYPE_FORM
    assert result["errors"] == {}

    with _patch_discovery(no_device=True), patch(
        "homeassistant.components.steamist.config_flow.Steamist.async_get_status"
    ), patch(
        "homeassistant.components.steamist.async_setup_entry",
        return_value=True,
    ) as mock_setup_entry:
        result2 = await hass.config_entries.flow.async_configure(
            result["flow_id"],
            {
                "host": "127.0.0.1",
            },
        )
        await hass.async_block_till_done()

    assert result2["type"] == RESULT_TYPE_CREATE_ENTRY
    assert result2["title"] == "127.0.0.1"
    assert result2["data"] == {
        "host": "127.0.0.1",
    }
    assert len(mock_setup_entry.mock_calls) == 1
```


Overlapping Code:
```
async def test_form(hass: HomeAssistant) -> None:
"""Test we get the form."""
result = await hass.config_entries.flow.async_init(
DOMAIN, context={"source": config_entries.SOURCE_USER}
)
assert result["type"] == RESULT_TYPE_FORM
assert result["errors"] == {}
with _patch_discovery(no_device=True), patch(
"homeassistant.components.steamist.config_flow.Steamist.async_get_status"
), patch(
"homeassistant.components.steamist.async_setup_entry",
return_value=True,
) as mock_setup_entry:
result2 = await hass.config_entries.flow.async_configure(
result["flow_id"],
{
"host": "127.0.0.1",
},
)
await hass.async_block_till_done()
assert result2["type"] == RESULT_TYPE_CREATE_ENTRY
assert result2["title"] == "127.0.0.1"
assert result2["data"] == {
"host": "127.0.0.1",
}
assert len(mock_setup_entry.mock_calls)
```
<Overlap Ratio: 0.9938347718865598>

---

--- 300 --
Question ID: ad285f43cf253d1f2ca927b49f3a5a07545a593e_10
Original Code:
```
@with_altair_and_later
@with_presets([MAINNET], reason="to create duplicate committee")
@spec_state_test
def test_sync_committee_rewards_duplicate_committee_full_participation(spec, state):
    committee_indices = get_committee_indices(spec, state, duplicates=True)
    committee_size = len(committee_indices)
    committee_bits = [True] * committee_size
    active_validator_count = len(spec.get_active_validator_indices(state, spec.get_current_epoch(state)))

    # Preconditions of this test case
    assert active_validator_count < spec.SYNC_COMMITTEE_SIZE
    assert committee_size > len(set(committee_indices))

    yield from run_successful_sync_committee_test(spec, state, committee_indices, committee_bits)
```


Overlapping Code:
```
with_altair_and_later
@with_presets([MAINNET], reason="to create duplicate committee")
@spec_state_test
def test_sync_committee_rewards_duplicate_committee_full_participation(spec, state):
committee_indices = get_committee_indices(spec, state, duplicates=True)
committee_size = len(committee_indices)
committee_bits = [True] * committee_size
active_validator_count = len(spec.get_active_validator_indices(state, spec.get_current_epoch(state)))
# Preconditions of this test case
assert active_validator_count < spec.SYNC_COMMITTEE_SIZE
assert committee_size > len(set(committee_indices))
yield from run_successful_sync_committee_test(spec, state, committee_indices, committee_bi
```
<Overlap Ratio: 0.9941262848751835>

---

--- 301 --
Question ID: a0490b32bcf53291170b831faf8cdb96ba01d501_24
Original Code:
```
def xy_to_ABC(xy, xscale=1.0, yscale=1.0):
    """
    Convert x-y coordinates within a triangle to compositional ternary coordinates.

    Parameters
    -----------
    xy : :class:`numpy.ndarray`
        XY array (:code:`samples, 2`).
    xscale : :class:`float`
        Scale for x-axis.
    yscale : :class:`float`
        Scale for y-axis.

    Returns
    --------
    :class:`numpy.ndarray`
        Array of ternary coordinates (:code:`samples, 3`)
    """
    assert xy.shape[-1] == 2
    # transform from xy cartesian to ternary
    scale = affine_transform(
        np.array([[1 / xscale, 0, 0], [0, 1 / yscale, 0], [0, 0, 1]])
    )
    shear = affine_transform(np.array([[1, -1 / 2, 0], [0, 1, 0], [0, 0, 1]]))
    A, B = shear(scale(xy).T)
    C = 1.0 - (A + B)  # + (xscale-1) + (yscale-1)
    return np.vstack([A, B, C]).T
```


Overlapping Code:
```
, yscale=1.0):
"""
Convert x-y coordinates within a triangle to compositional ternary coordinates.
Parameters
-----------
xy : :class:`numpy.ndarray`
XY array (:code:`samples, 2`).
xscale : :class:`float`
Scale for x-axis.
yscale : :class:`float`
Scale for y-axis.
Returns
--------
:class:`numpy.ndarray`
Array of ternary coordinates (:code:`samples, 3`)
"""
assert xy.shape[-1] == 2
# transform from xy cartesian to ternary
scale = affine_transform(
np.array([[1 / xscale, 0, 0], [0, 1 / yscale, 0], [0, 0, 1]])
)
shear = affine_transform(np.array([[1, -1 / 2, 0], [0, 1, 0], [0, 0, 1]]))
A, B = shear(scale(xy).T)
C = 1.0 - (A + B) # + (xscale-1) + (yscale-1)
return 
```
<Overlap Ratio: 0.9304589707927677>

---

--- 302 --
Question ID: 0b69bc56064c9d36d3700fefed89530bd5049f63_0
Original Code:
```
def find_host_name(indice):
    search_first = es_client.search(
        index=indice,
        body={
            "sort": [
                {"@timestamp": "desc"}
            ],
            "size": 1,
        }
    )
    host_name = search_first['hits']['hits'][0]['_source']['host']['name']
    return host_name
```


Overlapping Code:
```
earch_first = es_client.search(
index=indice,
body={
"sort": [
{"@timestamp": "desc"}
],
"size": 1,
}
)
host_name = search_first['hits']['hits'][0]['_
```
<Overlap Ratio: 0.6818181818181818>

---

--- 303 --
Question ID: 6aa4e08ed331f4b42d4c62d5c6a6e50b5168594d_10
Original Code:
```
def test_stabilized_vs_sinkhorn_multidim():
    # test if stable version matches sinkhorn
    # for multidimensional inputs
    n = 100

    # Gaussian distributions
    a = ot.datasets.make_1D_gauss(n, m=20, s=5)  # m= mean, s= std
    b1 = ot.datasets.make_1D_gauss(n, m=60, s=8)
    b2 = ot.datasets.make_1D_gauss(n, m=30, s=4)

    # creating matrix A containing all distributions
    b = np.vstack((b1, b2)).T

    M = ot.utils.dist0(n)
    M /= np.median(M)
    epsilon = 0.1
    G, log = ot.bregman.sinkhorn(a, b, M, reg=epsilon,
                                 method="sinkhorn_stabilized",
                                 log=True)
    G2, log2 = ot.bregman.sinkhorn(a, b, M, epsilon,
                                   method="sinkhorn", log=True)

    np.testing.assert_allclose(G, G2)
```


Overlapping Code:
```
ilized_vs_sinkhorn_multidim():
# test if stable version matches sinkhorn
# for multidimensional inputs
n = 100
# Gaussian distributions
a = ot.datasets.make_1D_gauss(n, m=20, s=5) # m= mean, s= std
b1 = ot.datasets.make_1D_gauss(n, m=60, s=8)
b2 = ot.datasets.make_1D_gauss(n, m=30, s=4)
# creating matrix A containing all distributions
b = np.vstack((b1, b2)).T
M = ot.utils.dist0(n)
M /= np.median(M)
epsilon = 0.1
G, log = ot.bregman.sinkhorn(a, b, M, reg=epsilon,
method="sinkhorn_stabilized",
log=True)
G2, log2 = ot.bregman.sinkhorn(a, b, M, epsilon,
method="sinkhorn", log=True)
np.testing.ass
```
<Overlap Ratio: 0.9493670886075949>

---

--- 304 --
Question ID: e7403d8ed4ba6476c7e5720395f5268978927c4e_0
Original Code:
```
def convert(lines):
    seen_noncomment = False
    for line in lines:
        if line[0] == '#' and not seen_noncomment:
            # skip comments in beginning of file
            continue
        else:
            seen_noncomment = True
        c, w = line.rstrip('\n').split(None, 1)
        c = int(float(c)) + 1
        yield c, w 
```


Overlapping Code:
```
f convert(lines):
seen_noncomment = False
for line in lines:
if line[0] == '#' and not seen_noncomment:
# skip comments in beginning of file
continue
else:
seen_noncomment = True
c, w = line.rstrip('\n').split(None, 1)
c = int(float(c)) + 1
yield c, 
```
<Overlap Ratio: 0.9881422924901185>

---

--- 305 --
Question ID: 5960e0ee8becb1e1a225115427a9de9b595480fa_1
Original Code:
```
def metric_to_image_radial_length(length, affine):
    """
        Only useful for image + mesh cases. Not implemented yet.
        """
    return length
```


Overlapping Code:
```
, affine):
"""
Only useful for image + mesh cases. Not implemented yet.
"""
retu
```
<Overlap Ratio: 0.6201550387596899>

---

--- 306 --
Question ID: c03ef307f1e8703eb8538ab36dc9b3b3af98024c_1
Original Code:
```
def get_tag2index():
    if TASK == 0: return {"O": 0, "B-PER": 1, "I-PER": 2, "B-LOC": 3, "I-LOC": 4, "B-ORG": 5, "I-ORG": 6}         # NER 标注的标签
    #else: return {"O": 0, "B-R": 1, "I-R": 2, "B-M": 3, "I-M": 4, "B-S": 5, "I-S": 6, "B-W": 7, "I-W": 8}         # query 纠错标注的标签1
    else: return {"O": 0, "B-SP": 1, "I-SP": 2, "B-SS": 3, "I-SS": 4}  # query 纠错标注的标签
```


Overlapping Code:
```
2index():
if TASK == 0: return {"O": 0, "B-PER": 1, "I-PER": 2, "B-LOC": 3, "I-LOC": 4, "B-ORG": 5, "I-ORG": 6} # NER 标注的标签
#else: return {"O": 0, "B-R": 1, "I-R": 2, "B-M": 3, "I-M": 4, "B-S": 5, "I-S": 6, "B-W": 7, "I-W": 8} # query 纠错标注的标签1
else: return {"O": 0, "B-SP": 1, "I-SP": 2, "B-SS": 3, "
```
<Overlap Ratio: 0.8928571428571429>

---

--- 307 --
Question ID: 1cf8634ccc9b149becd266d49f008d61337f0553_9
Original Code:
```
def export_table(data, path):
    """
    Export final table as LaTeX table.
    """
    # Keep columns we care about
    data['time_base'] = data['time_base'].map('{:.3f}'.format) + ' ' + data['stdev_base'].map('({:.4f})'.format)
    data['time_ebph'] = data['time_ebph'].map('{:.3f}'.format) + ' ' + data['stdev_ebph'].map('({:.4f})'.format)
    data['overhead'] = data['overhead'].map('{:.3f}'.format) + ' ' + data['stdev_overhead'].map('({:.4f})'.format)
    data = data[['syscall', 'time_base', 'time_ebph', 'overhead']]
    # Export table, after renaming columns
    if args.out:
        data[:]['syscall'] =  data[:]['syscall'].str.replace('_', r'\_')
        data = data.rename(columns={
            'syscall':r'\multicolumn{1}{l}{System Call}',
            'time_base':r'$T_{\text{base}}$ ($\mu$s)',
            'time_ebph':r'$T_{\text{ebpH}}$ ($\mu$s)',
            'overhead':r'\% Overhead'})
        data.to_latex(index=0, escape=0, buf=path, column_format=r'>{\ttfamily}lrrrr')
    else:
        print(data)
```


Overlapping Code:
```
th):
"""
Export final table as LaTeX table.
"""
# Keep columns we care about
data['time_base'] = data['time_base'].map('{:.3f}'.format) + ' ' + data['stdev_base'].map('({:.4f})'.format)
data['time_ebph'] = data['time_ebph'].map('{:.3f}'.format) + ' ' + data['stdev_ebph'].map('({:.4f})'.format)
data['overhead'] = data['overhead'].map('{:.3f}'.format) + ' ' + data['stdev_overhead'].map('({:.4f})'.format)
data = data[['syscall', 'time_base', 'time_ebph', 'overhead']]
# Export table, after renaming columns
if args.out:
data[:]['syscall'] = data[:]['syscall'].str.replace('_', r'\_')
data = data.rename(columns={
'syscall':r'\multicolumn{1}{l}{System Call}',
'time_base':r'$T_{\text{base}}$ ($\mu$s)',
'time_ebph':r'$T_{\text{ebpH}}$ ($\mu$s)',
'overhead':r'\% Overhead'})
data.to_latex(index=0, escape=0, buf=path, column_format=r'>{\ttfamily}lrrrr
```
<Overlap Ratio: 0.9497206703910615>

---

--- 308 --
Question ID: ce6d28800f6264762b35182834863c5269a488ae_2
Original Code:
```
async def join_game(request):
    logging.info(f"Request: {request.url}")
    username = request.query['username'] if 'username' in request.query else None
    round_id = request.match_info['round_id']
    return add_player_to_round(request, round_id, username)
```


Overlapping Code:
```
request):
logging.info(f"Request: {request.url}")
username = request.query['username'] if 'username' in request.query else None
round_id = request.match_info['round_id']
return add_player_to_round(request, round_id, u
```
<Overlap Ratio: 0.8857142857142857>

---

--- 309 --
Question ID: 15ae329b1bc41d195af092a2b11437a91c641074_5
Original Code:
```
def isString(hou_attrib):
    """Convenience filter function for string attributes.
    """
    assert type(hou_attrib) is hou.Attrib, "invalid argument"
    return \
        hou_attrib.dataType()==hou.attribData.String and \
        hou_attrib.size()==1
```


Overlapping Code:
```
ttrib):
"""Convenience filter function for string attributes.
"""
assert type(hou_attrib) is hou.Attrib, "invalid argument"
return \
hou_attrib.dataType()==hou.attribData.String and \
hou_attrib.size(
```
<Overlap Ratio: 0.9009009009009009>

---

--- 310 --
Question ID: 775e151e6316a16f75b1eb8a5a671b8d74cdea9f_1
Original Code:
```
def profile_information(profile):
    """
    Returns the profile information about a given connection
    
    =====================================================================     ====================================================================
    **Parameter**                                                             **Description**
    ---------------------------------------------------------------------     --------------------------------------------------------------------           
    profile                                                                   Required String. The name of the profile to get the information about.
    =====================================================================     ====================================================================
    
    :returns: Dict
    
    """
    home_dir = str(Path.home())
    profile_file = os.path.join(home_dir, ".arcgisprofile")
    if os.path.isfile(profile_file):
        config = configparser.ConfigParser()
        config.read(profile_file)
        keys = config.options(profile)
        values = {}
        for key in keys:
            try:
                if key == 'date_modified':
                    import datetime as _datetime
                    values[key] = _datetime.datetime.strptime(
                        config.get(
                            profile, 
                            key), 
                        "%Y-%m-%d %H:%M:%S.%f")
                else:
                    values[key] = config.get(profile, key)
            except:
                values[key] = None
        return values
    return None  
```


Overlapping Code:
```
le_information(profile):
"""
Returns the profile information about a given connection

===================================================================== ====================================================================
**Parameter** **Description**
--------------------------------------------------------------------- -------------------------------------------------------------------- 
profile Required String. The name of the profile to get the information about.
===================================================================== ====================================================================

:returns: Dict

"""
home_dir = str(Path.home())
profile_file = os.path.join(home_dir, ".arcgisprofile")
if os.path.isfile(profile_file):
config = configparser.ConfigParser()
config.read(profile_file)
keys = config.options(profile)
values = {}
for key in keys:
try:
if key == 'date_modified':
import datetime as _datetime
values[key] = _datetime.datetime.strptime(
config.get(
profile, 
key), 
"%Y-%m-%d %H:%M:%S.%f")
else:
values[key] = config.get(profile, key)
except:
values[key] = N
```
<Overlap Ratio: 0.9666080843585237>

---

--- 311 --
Question ID: 9f82107efc808c49ec52025faefacdfb9a2bee12_3
Original Code:
```
def form_another_2d_array(df_list):
    new_list = np.zeros(len(df_list)+1)
    final_table = np.array([trisect_ages(x_df) for x_df in df_list])

    
    return final_table
```


Overlapping Code:
```
ef form_another_2d_array(df_list):
new_list = np.zeros(len(df_list)+1)
final_table = np.array([trisect_ages(x_df) for x_df in df_list])

return final_tabl
```
<Overlap Ratio: 0.9871794871794872>

---

--- 312 --
Question ID: c4c34a0832f4b7fc813d99283c4adf2da0eecb2b_5
Original Code:
```
@pytest.mark.asyncio
async def test_os(redis_conn, small_graph):
    subjects = await graph.os(redis_conn, small_graph["b"])
    assert len(subjects) == 2
    assert small_graph["a"] in subjects
    assert small_graph["d"] in subjects
```


Overlapping Code:
```
pytest.mark.asyncio
async def test_os(redis_conn, small_graph):
subjects = await graph.os(redis_conn, small_graph["b"])
assert len(subjects) == 2
assert small_graph["a"] in subjects
assert small_graph
```
<Overlap Ratio: 0.9174311926605505>

---

--- 313 --
Question ID: 13d20c58618bae1fd9184241e64ff9b913dd727d_1
Original Code:
```
def check_adb_alive():
    try:
        sess = ADBClientSession(config.ADB_SERVER)
        version = int(sess.service('host:version').read_response().decode(), 16)
        logger.debug('ADB server version %d', version)
        return True
    except ConnectionRefusedError:
        return False
    except RuntimeError:
        return False
```


Overlapping Code:
```
:
sess = ADBClientSession(config.ADB_SERVER)
version = int(sess.service('host:version').read_response().decode(), 16)
logger.debug('ADB server version %d', version)
return True
except ConnectionRefusedError:
return False
except RuntimeError:
return Fa
```
<Overlap Ratio: 0.8964285714285715>

---

--- 314 --
Question ID: 57d9b09605aa2dbd63885c26adbd8139e92f01f7_2
Original Code:
```
def calculate_kld(pe, qe, vb=True):
    """
    Calculates the Kullback-Leibler Divergence between two PDFs.

    Parameters
    ----------
    pe: numpy.ndarray, float
        probability distribution evaluated on a grid whose distance from `q`
        will be calculated.
    qe: numpy.ndarray, float
        probability distribution evaluated on a grid whose distance to `p` will
        be calculated.
    vb: boolean
        report on progress to stdout?

    Returns
    -------
    Dpq: float
        the value of the Kullback-Leibler Divergence from `q` to `p`
    """
    # Normalize the evaluations, so that the integrals can be done
    # (very approximately!) by simple summation:
    pn = pe / np.sum(pe)
    qn = qe / np.sum(qe)
    # Compute the log of the normalized PDFs
    logp = u.safe_log(pn)
    logq = u.safe_log(qn)
    # Calculate the KLD from q to p
    Dpq = np.sum(pn * (logp - logq))
    return Dpq
```


Overlapping Code:
```
ulate_kld(pe, qe, vb=True):
"""
Calculates the Kullback-Leibler Divergence between two PDFs.
Parameters
----------
pe: numpy.ndarray, float
probability distribution evaluated on a grid whose distance from `q`
will be calculated.
qe: numpy.ndarray, float
probability distribution evaluated on a grid whose distance to `p` will
be calculated.
vb: boolean
report on progress to stdout?
Returns
-------
Dpq: float
the value of the Kullback-Leibler Divergence from `q` to `p`
"""
# Normalize the evaluations, so that the integrals can be done
# (very approximately!) by simple summation:
pn = pe / np.sum(pe)
qn = qe / np.sum(qe)
# Compute the log of the normalized PDFs
logp = u.safe_log(pn)
logq = u.safe_log(qn)
# Calculate the KLD from q to p
Dpq = np.sum(pn * (logp - logq))
return 
```
<Overlap Ratio: 0.9861286254728878>

---

--- 315 --
Question ID: 366b0343ba359792a3cdf165d3849e31b0299bf2_0
Original Code:
```
def get_data(file_obj):
    row_0 = np.genfromtxt(file_obj, delimiter=',', usecols=(0))
    row_1 = np.genfromtxt(file_obj, delimiter=',', usecols=(1))
    row_2 = np.genfromtxt(file_obj, delimiter=',', usecols=(2))
    row_3 = np.genfromtxt(file_obj, delimiter=',', usecols=(3))
    return row_0, row_1, row_2, row_3
```


Overlapping Code:
```
(file_obj):
row_0 = np.genfromtxt(file_obj, delimiter=',', usecols=(0))
row_1 = np.genfromtxt(file_obj, delimiter=',', usecols=(1))
row_2 = np.genfromtxt(file_obj, delimiter=',', usecols=(2))
row_3 = np.genfromtxt(file_obj, delimiter=',', usecols=(3)
```
<Overlap Ratio: 0.8417508417508418>

---

--- 316 --
Question ID: 3f75ec23874d95ee0b08b0fdd08aaf8b860c3f7a_0
Original Code:
```
def mute_spotify(mute_val: bool):
    '''
    Finds spotify application and mutes or unmutes it based on the value of mute_val.
    The input argument to the function sets the mute state of Spotify. The value
    is True for muted False for unmuted.
    Returns: nothing.
    '''

    if platform == 'linux' or platform == 'linux2':
        _mute_spotify_linux(mute_val)
    elif platform == 'darwin':
        pass
    elif platform == 'win32':
        _mute_spotify_win32(mute_val)
```


Overlapping Code:
```
s spotify application and mutes or unmutes it based on the value of mute_val.
The input argument to the function sets the mute state of Spotify. The value
is True for muted False for unmuted.
Returns: nothing.
'''
if platform == 'linux' or platform == 'linux2':
_mute_spotify_linux(mute_val)
elif platform == 'darwin':
pass
elif platform == 'win32':

```
<Overlap Ratio: 0.831353919239905>

---

--- 317 --
Question ID: 204fe53dee63a8492d4a3b958826dede8b213446_0
Original Code:
```
def read_data():
    rules = dict()
    your_ticket = None
    nearby_tickets = []
    state = 0
    with open('in') as f:
        for line in map(lambda x: x.strip(), f.readlines()):
            if line == '':
                state += 1
                continue
            if line == 'your ticket:':
                continue
            if line == 'nearby tickets:':
                continue

            if state == 0:
                parts = line.split(':')
                ranges = parts[1].split('or')
                rules[parts[0]] = []
                for r in ranges:
                    nums = r.split('-')
                    rules[parts[0]].append((int(nums[0]), int(nums[1])))
            if state == 1:
                your_ticket = list(map(int, line.split(',')))

            if state == 2:
                nearby_tickets.append(list(map(int, line.split(','))))

    return rules, your_ticket, nearby_tickets
```


Overlapping Code:
```
icket = None
nearby_tickets = []
state = 0
with open('in') as f:
for line in map(lambda x: x.strip(), f.readlines()):
if line == '':
state += 1
continue
if line == 'your ticket:':
continue
if line == 'nearby tickets:':
continue
if state == 0:
parts = line.split(':')
ranges = parts[1].split('or')
rules[parts[0]] = []
for r in ranges:
nums = r.split('-')
rules[parts[0]].append((int(nums[0]), int(nums[1])))
if state == 1:
your_ticket = list(map(int, line.split(',')))
if state == 2:
nearby_tickets.append(list(map(int, line.split(','))))
return rule
```
<Overlap Ratio: 0.889967637540453>

---

--- 318 --
Question ID: 2be0fb16e9aa1df2dc2f258d1f03b85088b475eb_0
Original Code:
```
def verify_test_message(cls, reader):
    cmd = reader.read_int()
    for j in range(cmd % 10):
        s = j % 5
        if s == 0:
            cls.assertEqual(1/cmd, reader.read_double())
        elif s == 1:
            expected_size = cmd % (1024*10)
            cls.assertEqual(len(reader.read_string()), expected_size)
        elif s == 2:
            cls.assertEqual((cmd % 256), reader.read_byte())
        elif s == 3:
            cls.assertEqual(cmd, (reader.read_long() >> 32))
        elif s == 4:
            cls.assertEqual('톩', reader.read_char())
        else:
            pass
```


Overlapping Code:
```
):
cmd = reader.read_int()
for j in range(cmd % 10):
s = j % 5
if s == 0:
cls.assertEqual(1/cmd, reader.read_double())
elif s == 1:
expected_size = cmd % (1024*10)
cls.assertEqual(len(reader.read_string()), expected_size)
elif s == 2:
cls.assertEqual((cmd % 256), reader.read_byte())
elif s == 3:
cls.assertEqual(cmd, (reader.read_long() >> 32))
elif s == 4:
cls.assertEqual('톩', reader.read_char())

```
<Overlap Ratio: 0.898876404494382>

---

--- 319 --
Question ID: aca7d17340fb56c81511cd04150bbc861e06ca70_9
Original Code:
```
def test_mark_complete_existing_todo(repository: Repository) -> None:
    id_1, text_1 = _insert_todo(repository, "This is a Todo!")
    id_2, text_2 = _insert_todo(repository, "This is a ANOTHER Todo!")

    result = repository.deactivate(id_2)
    assert result

    todos = repository.list()
    assert todos == tuple(
        sorted(
            (
                Todo(id=id_1, text=text_1, active=True),
                Todo(id=id_2, text=text_2, active=False),
            )
        )
    )
```


Overlapping Code:
```
_complete_existing_todo(repository: Repository) -> None:
id_1, text_1 = _insert_todo(repository, "This is a Todo!")
id_2, text_2 = _insert_todo(repository, "This is a ANOTHER Todo!")
result = repository.deactivate(id_2)
assert result
todos = repository.list()
assert todos == tuple(
sorted(
(
Todo(id=id_1, text=text_1, active=True),
Todo(id=id_2, text=te
```
<Overlap Ratio: 0.9010152284263959>

---

--- 320 --
Question ID: 642b901db4811563f00b0cc99e370ef5f380dd0b_1
Original Code:
```
def save(pi, sess, name):
    saver = tf.train.Saver(pi.get_variables())
    save_dir = os.path.join(logger.get_dir(), name)
    saver.save(sess, save_dir)
```


Overlapping Code:
```
aver(pi.get_variables())
save_dir = os.path.join(l
```
<Overlap Ratio: 0.34965034965034963>

---

--- 321 --
Question ID: 0e313b0129840a9db7ca51c374e69f7035db7c71_0
Original Code:
```
async def canvas_fetch(canvas_url, canvas_key, course_id, destination):
    from canvasapi import Canvas

    canvas = Canvas(canvas_url, canvas_key)
    course = canvas.get_course(course_id)

    for folder in course.get_folders():
        for file in folder.get_files():
            dest_path = os.path.join(destination, folder.full_name, file.filename)
            if os.path.exists(dest_path):
                file_mtime = datetime.fromtimestamp(os.path.getmtime(dest_path)).astimezone(pytz.utc)
                if file.updated_at_date <= file_mtime:
                    print(f'{dest_path} up to date, skipping')
                    continue
            # FIXME: protect against path traversal attacks here
            os.makedirs(os.path.dirname(dest_path), exist_ok=True)
            file.download(dest_path)
            print(f'downloaded {dest_path}')
```


Overlapping Code:
```
s_key, course_id, destination):
from canvasapi import Canvas
canvas = Canvas(canvas_url, canvas_key)
course = canvas.get_course(course_id)
for folder in course.get_folders():
for file in folder.get_files():
dest_path = os.path.join(destination, folder.full_name, file.filename)
if os.path.exists(dest_path):
file_mtime = datetime.fromtimestamp(os.path.getmtime(dest_path)).astimezone(pytz.utc)
if file.updated_at_date <= file_mtime:
print(f'{dest_path} up to date, skipping')
continue
# FIXME: protect against path traversal attacks here
os.makedirs(os.path.dirname(dest_path), exist_ok=True)
file.download(dest_path)
print(f'downloaded {dest_path}')
```
<Overlap Ratio: 0.9420289855072463>

---

--- 322 --
Question ID: dade6b7690cf9c0ec954b82fa6bd23cceaf873e5_1
Original Code:
```
def prereq():
    with open('configure.json') as configure_file:
        json_text = json.load(configure_file)

    # Create a Thing
    thing_name = json_text['thing_name']
    thing_obj = thing.Thing(thing_name)
    if not thing_obj.create():

        # Create a Certificate
        cert_obj = certs.Certificate()
        result = cert_obj.create()

        # Store certId
        cert_id = result['certificateId']
        cert_id_filename = thing_name + '_cert_id_file.txt'
        cert_id_file = open(cert_id_filename, 'w')
        cert_id_file.write(cert_id)
        cert_id_file_path = os.path.abspath(cert_id_filename)
        os.chmod(cert_id_file_path, 0o444)
        cert_id_file.close()

        # Store cert_pem as file
        cert_pem = result['certificatePem']
        cert_pem_filename = thing_name + '_cert_pem_file.pem'
        cert_pem_file = open(cert_pem_filename, 'w')
        cert_pem_file.write(cert_pem)
        cert_pem_file_path = os.path.abspath(cert_pem_filename)
        os.chmod(cert_pem_file_path, 0o444)
        cert_pem_file.close()

        # Store private key PEM as file
        private_key_pem = result['keyPair']['PrivateKey']
        private_key_pem_filename = thing_name + '_private_key_pem_file.pem'
        private_key_pem_file = open(private_key_pem_filename, 'w')
        private_key_pem_file.write(private_key_pem)
        private_key_pem_file_path = os.path.abspath(private_key_pem_filename)
        os.chmod(private_key_pem_file_path, 0o444)
        private_key_pem_file.close()

        # Create a Policy
        policy_document = misc.create_policy_document()
        policy_name = thing_name + '_amazon_freertos_policy'
        policy_obj = policy.Policy(policy_name, policy_document)
        policy_obj.create()

        # Attach certificate to Thing
        cert_obj.attach_thing(thing_name)

        # Attach policy to certificate
        cert_obj.attach_policy(policy_name)
```


Overlapping Code:
```
:
with open('configure.json') as configure_file:
json_text = json.load(configure_file)
# Create a Thing
thing_name = json_text['thing_name']
thing_obj = thing.Thing(thing_name)
if not thing_obj.create():
# Create a Certificate
cert_obj = certs.Certificate()
result = cert_obj.create()
# Store certId
cert_id = result['certificateId']
cert_id_filename = thing_name + '_cert_id_file.txt'
cert_id_file = open(cert_id_filename, 'w')
cert_id_file.write(cert_id)
cert_id_file_path = os.path.abspath(cert_id_filename)
os.chmod(cert_id_file_path, 0o444)
cert_id_file.close()
# Store cert_pem as file
cert_pem = result['certificatePem']
cert_pem_filename = thing_name + '_cert_pem_file.pem'
cert_pem_file = open(cert_pem_filename, 'w')
cert_pem_file.write(cert_pem)
cert_pem_file_path = os.path.abspath(cert_pem_filename)
os.chmod(cert_pem_file_path, 0o444)
cert_pem_file.close()
# Store private key PEM as file
private_key_pem = result['keyPair']['PrivateKey']
private_key_pem_filename = thing_name + '_private_key_pem_file.pem'
private_key_pem_file = open(private_key_pem_filename, 'w')
private_key_pem_file.write(private_key_pem)
private_key_pem_file_path = os.path.abspath(private_key_pem_filename)
os.chmod(private_key_pem_file_path, 0o444)
private_key_pem_file.close()
# Create a Policy
policy_document = misc.create_policy_document()
policy_name = thing_name + '_amazon_freertos_policy'
policy_obj = policy.Policy(policy_name, policy_document)
policy_obj.create()
# Attach certificate to Thing
cert_obj.attach_thing(thing_name)
# Attach policy to certificate
cert_obj.attach_policy(policy_name)
```
<Overlap Ratio: 0.9925187032418953>

---

--- 323 --
Question ID: e3cea6ee2cccf733622fb86e322c3efc46d63f5c_5
Original Code:
```
def check_helm_install(kube_config, kube_context):
    cmd_helm_installed = ["helm", "--kubeconfig", kube_config, "--debug"]
    if kube_context:
        cmd_helm_installed.extend(["--kube-context", kube_context])
    try:
        response_helm_installed = Popen(cmd_helm_installed, stdout=PIPE, stderr=PIPE)
        _, error_helm_installed = response_helm_installed.communicate()
        if response_helm_installed.returncode != 0:
            if "unknown flag" in error_helm_installed.decode("ascii"):
                telemetry.set_user_fault()
                telemetry.set_exception(exception='Helm 3 not found', fault_type=Helm_Version_Fault_Type,
                                        summary='Helm3 not found on the machine')
                raise CLIError("Please install the latest version of Helm. " +
                               "Learn more at https://aka.ms/arc/k8s/onboarding-helm-install")
            telemetry.set_user_fault()
            telemetry.set_exception(exception=error_helm_installed.decode("ascii"), fault_type=Helm_Installation_Fault_Type,
                                    summary='Helm3 not installed on the machine')
            raise CLIError(error_helm_installed.decode("ascii"))
    except FileNotFoundError as e:
        telemetry.set_exception(exception=e, fault_type=Check_HelmInstallation_Fault_Type,
                                summary='Unable to verify helm installation')
        raise CLIError("Helm is not installed or requires elevated permissions. " +
                       "Ensure that you have the latest version of Helm installed on your machine. " +
                       "Learn more at https://aka.ms/arc/k8s/onboarding-helm-install")
    except subprocess.CalledProcessError as e2:
        e2.output = e2.output.decode("ascii")
        print(e2.output)
```


Overlapping Code:
```
_helm_install(kube_config, kube_context):
cmd_helm_installed = ["helm", "--kubeconfig", kube_config, "--debug"]
if kube_context:
cmd_helm_installed.extend(["--kube-context", kube_context])
try:
response_helm_installed = Popen(cmd_helm_installed, stdout=PIPE, stderr=PIPE)
_, error_helm_installed = response_helm_installed.communicate()
if response_helm_installed.returncode != 0:
if "unknown flag" in error_helm_installed.decode("ascii"):
telemetry.set_user_fault()
telemetry.set_exception(exception='Helm 3 not found', fault_type=Helm_Version_Fault_Type,
summary='Helm3 not found on the machine')
raise CLIError("Please install the latest version of Helm. " +
"Learn more at https://aka.ms/arc/k8s/onboarding-helm-install")
telemetry.set_user_fault()
telemetry.set_exception(exception=error_helm_installed.decode("ascii"), fault_type=Helm_Installation_Fault_Type,
summary='Helm3 not installed on the machine')
raise CLIError(error_helm_installed.decode("ascii"))
except FileNotFoundError as e:
telemetry.set_exception(exception=e, fault_type=Check_HelmInstallation_Fault_Type,
summary='Unable to verify helm installation')
raise CLIError("Helm is not installed or requires elevated permissions. " +
"Ensure that you have the latest version of Helm installed on your machine. " +
"Learn more at https://aka.ms/arc/k8s/onboarding-helm-install")
except subprocess.CalledProcessError as e2:
e2.output = 
```
<Overlap Ratio: 0.9648518263266712>

---

--- 324 --
Question ID: 71eb6a1a78e7e9a50cf137956f87e7e06f2d3af1_10
Original Code:
```
def get_param(entry,paramname):
    '''
    Returns parameter value.
    Takes TaskEntry object.
    '''
    return json.loads(entry.params)[paramname]
```


Overlapping Code:
```
ramname):
'''
Returns parameter value.
Takes TaskEntry object.
'''
return json.loads(entry.params)[p
```
<Overlap Ratio: 0.7633587786259542>

---

--- 325 --
Question ID: f46a51432651c4e321748bc817ea07abcffb4985_0
Original Code:
```
def print_graph(relationships, site_nodes):
    project_keys = []
    for node in site_nodes:
        project_keys.append(node)

    print(tabulate(relationships,
                   headers=project_keys,
                   showindex=project_keys,
                   tablefmt='fancy_grid'))
```


Overlapping Code:
```
):
project_keys = []
for node in site_nodes:
project_keys.append(node)
print(tabulate(relationships,
headers=project_keys,
showindex=project_keys,
tab
```
<Overlap Ratio: 0.7109004739336493>

---

--- 326 --
Question ID: 4b63530da8cc0deb58044c8935aabba9d812357c_8
Original Code:
```
async def device_firmware_upgrade(dfu_addr, package):

    async with DfuDevice(address=str(dfu_addr)) as dev:
        imgpkg = DfuImagePkg(package)
        await dev.send_image_package(imgpkg)
```


Overlapping Code:
```
 device_firmware_upgrade(dfu_addr, package):
async with DfuDevice(address=str(dfu_addr)) as dev:
imgpkg = DfuImagePkg(package)
await dev.send_image_pa
```
<Overlap Ratio: 0.872093023255814>

---

--- 327 --
Question ID: a71d65d89e1dae351ffd4427733e54cfe675a681_0
Original Code:
```
def upload_file(request):
    """
    Upload the ReID video and the ReID query picture
    :param request: HTTP request
    :return: HTTP Web Page and ReID query result url
    """

    # the url and url_name for query failed
    url = ['/static/fail.jpg']
    url_name = ['fail']

    if request.method == 'POST':

        reid_video_file = request.FILES.getlist('reid_video')[0]
        reid_pic_file = request.FILES.getlist('reid_pic')[0]

        # Handle the uploading action
        handle_uploaded_file(reid_video_file)
        handle_uploaded_file(reid_pic_file)

        # ReID service
        reid_result = PersonReID(reid_video=os.path.join('reid_query', str(reid_video_file)),
                                 reid_pic=os.path.join('reid_query', str(reid_pic_file)))
        url, url_name = reid_result.reid_result()

    context = {'reid_url': zip(url, url_name)}  # API dictionary for showing ReID query result

    return render(request, 'result.html', context)
```


Overlapping Code:
```
ideo and the ReID query picture
:param request: HTTP request
:return: HTTP Web Page and ReID query result url
"""
# the url and url_name for query failed
url = ['/static/fail.jpg']
url_name = ['fail']
if request.method == 'POST':
reid_video_file = request.FILES.getlist('reid_video')[0]
reid_pic_file = request.FILES.getlist('reid_pic')[0]
# Handle the uploading action
handle_uploaded_file(reid_video_file)
handle_uploaded_file(reid_pic_file)
# ReID service
reid_result = PersonReID(reid_video=os.path.join('reid_query', str(reid_video_file)),
reid_pic=os.path.join('reid_query', str(reid_pic_file)))
url, url_name = reid_result.reid_result()
context = {'reid_url': zip(url, url_name)} # API dictionary for showing ReID query result
return render(re
```
<Overlap Ratio: 0.9068923821039904>

---

--- 328 --
Question ID: 7c2914bd86e1bf64d5c886500d73b7b98ed085a2_1
Original Code:
```
@pytest.mark.unit
def test_check_url_good(client):
    response = client.post(
        fileinfo_url,
        json={
            "url": "https://s3.amazonaws.com/testfiles.oceanprotocol.com/info.0.json",
            "type": "url",
            "method": "GET",
        },
    )
    result = response.get_json()
    assert response.status == "200 OK"
    for file_info in result:
        assert file_info["contentLength"] == "1161"
        assert file_info["contentType"] == "application/json"
        assert file_info["valid"] is True
```


Overlapping Code:
```
mark.unit
def test_check_url_good(client):
response = client.post(
fileinfo_url,
json={
"url": "https://s3.amazonaws.com/testfiles.oceanprotocol.com/info.0.json",
"type": "url",
"method": "GET",
},
)
result = response.get_json()
assert response.status == "200 OK"
for file_info in result:
assert file_info["contentLength"] == "1161"
assert file_info["contentType"] == "application/json"
assert file_i
```
<Overlap Ratio: 0.9345794392523364>

---

--- 329 --
Question ID: d9d06e270311a385c2b952d8f3d78341b877e635_11
Original Code:
```
def make_batched_features_dataset(file_pattern,
                                  batch_size,
                                  features,
                                  reader=core_readers.TFRecordDataset,
                                  label_key=None,
                                  reader_args=None,
                                  num_epochs=None,
                                  shuffle=True,
                                  shuffle_buffer_size=10000,
                                  shuffle_seed=None,
                                  prefetch_buffer_size=optimization.AUTOTUNE,
                                  reader_num_threads=1,
                                  parser_num_threads=2,
                                  sloppy_ordering=False,
                                  drop_final_batch=False):
  """Returns a `Dataset` of feature dictionaries from `Example` protos.

  If label_key argument is provided, returns a `Dataset` of tuple
  comprising of feature dictionaries and label.

  Example:

  ```
  serialized_examples = [
    features {
      feature { key: "age" value { int64_list { value: [ 0 ] } } }
      feature { key: "gender" value { bytes_list { value: [ "f" ] } } }
      feature { key: "kws" value { bytes_list { value: [ "code", "art" ] } } }
    },
    features {
      feature { key: "age" value { int64_list { value: [] } } }
      feature { key: "gender" value { bytes_list { value: [ "f" ] } } }
      feature { key: "kws" value { bytes_list { value: [ "sports" ] } } }
    }
  ]
  ```

  We can use arguments:

  ```
  features: {
    "age": FixedLenFeature([], dtype=tf.int64, default_value=-1),
    "gender": FixedLenFeature([], dtype=tf.string),
    "kws": VarLenFeature(dtype=tf.string),
  }
  ```

  And the expected output is:

  ```python
  {
    "age": [[0], [-1]],
    "gender": [["f"], ["f"]],
    "kws": SparseTensor(
      indices=[[0, 0], [0, 1], [1, 0]],
      values=["code", "art", "sports"]
      dense_shape=[2, 2]),
  }
  ```

  Args:
    file_pattern: List of files or patterns of file paths containing
      `Example` records. See `tf.gfile.Glob` for pattern rules.
    batch_size: An int representing the number of records to combine
      in a single batch.
    features: A `dict` mapping feature keys to `FixedLenFeature` or
      `VarLenFeature` values. See `tf.parse_example`.
    reader: A function or class that can be
      called with a `filenames` tensor and (optional) `reader_args` and returns
      a `Dataset` of `Example` tensors. Defaults to `tf.data.TFRecordDataset`.
    label_key: (Optional) A string corresponding to the key labels are stored in
      `tf.Examples`. If provided, it must be one of the `features` key,
      otherwise results in `ValueError`.
    reader_args: Additional arguments to pass to the reader class.
    num_epochs: Integer specifying the number of times to read through the
      dataset. If None, cycles through the dataset forever. Defaults to `None`.
    shuffle: A boolean, indicates whether the input should be shuffled. Defaults
      to `True`.
    shuffle_buffer_size: Buffer size of the ShuffleDataset. A large capacity
      ensures better shuffling but would increase memory usage and startup time.
    shuffle_seed: Randomization seed to use for shuffling.
    prefetch_buffer_size: Number of feature batches to prefetch in order to
      improve performance. Recommended value is the number of batches consumed
      per training step. Defaults to auto-tune.
    reader_num_threads: Number of threads used to read `Example` records. If >1,
      the results will be interleaved.
    parser_num_threads: Number of threads to use for parsing `Example` tensors
      into a dictionary of `Feature` tensors.
    sloppy_ordering: If `True`, reading performance will be improved at
      the cost of non-deterministic ordering. If `False`, the order of elements
      produced is deterministic prior to shuffling (elements are still
      randomized if `shuffle=True`. Note that if the seed is set, then order
      of elements after shuffling is deterministic). Defaults to `False`.
    drop_final_batch: If `True`, and the batch size does not evenly divide the
      input dataset size, the final smaller batch will be dropped. Defaults to
      `False`.

  Returns:
    A dataset of `dict` elements, (or a tuple of `dict` elements and label).
    Each `dict` maps feature keys to `Tensor` or `SparseTensor` objects.

  Raises:
    ValueError: If `label_key` is not one of the `features` keys.
  """
  # Create dataset of all matching filenames
  filenames = _get_file_names(file_pattern, False)
  dataset = dataset_ops.Dataset.from_tensor_slices(filenames)
  if shuffle:
    dataset = dataset.shuffle(len(filenames), shuffle_seed)

  # Read `Example` records from files as tensor objects.
  if reader_args is None:
    reader_args = []

  # Read files sequentially (if reader_num_threads=1) or in parallel
  dataset = dataset.apply(
      interleave_ops.parallel_interleave(
          lambda filename: reader(filename, *reader_args),
          cycle_length=reader_num_threads,
          sloppy=sloppy_ordering))

  # Extract values if the `Example` tensors are stored as key-value tuples.
  if dataset.output_types == (dtypes.string, dtypes.string):
    dataset = dataset_ops.MapDataset(
        dataset, lambda _, v: v, use_inter_op_parallelism=False)

  # Apply dataset repeat and shuffle transformations.
  dataset = _maybe_shuffle_and_repeat(
      dataset, num_epochs, shuffle, shuffle_buffer_size, shuffle_seed)

  # NOTE(mrry): We set `drop_remainder=True` when `num_epochs is None` to
  # improve the shape inference, because it makes the batch dimension static.
  # It is safe to do this because in that case we are repeating the input
  # indefinitely, and all batches will be full-sized.
  dataset = dataset.batch(
      batch_size, drop_remainder=drop_final_batch or num_epochs is None)

  # Parse `Example` tensors to a dictionary of `Feature` tensors.
  dataset = dataset.apply(
      parsing_ops.parse_example_dataset(
          features, num_parallel_calls=parser_num_threads))

  if label_key:
    if label_key not in features:
      raise ValueError(
          "The `label_key` provided (%r) must be one of the `features` keys." %
          label_key)
    dataset = dataset.map(lambda x: (x, x.pop(label_key)))

  dataset = dataset.prefetch(prefetch_buffer_size)
  return dataset
```


Overlapping Code:
```
le_pattern,
batch_size,
features,
reader=core_readers.TFRecordDataset,
label_key=None,
reader_args=None,
num_epochs=None,
shuffle=True,
shuffle_buffer_size=10000,
shuffle_seed=None,
prefetch_buffer_size=optimization.AUTOTUNE,
reader_num_threads=1,
parser_num_threads=2,
sloppy_ordering=False,
drop_final_batch=False):
"""Returns a `Dataset` of feature dictionaries from `Example` protos.
If label_key argument is provided, returns a `Dataset` of tuple
comprising of feature dictionaries and label.
Example:
```
serialized_examples = [
features {
feature { key: "age" value { int64_list { value: [ 0 ] } } }
feature { key: "gender" value { bytes_list { value: [ "f" ] } } }
feature { key: "kws" value { bytes_list { value: [ "code", "art" ] } } }
},
features {
feature { key: "age" value { int64_list { value: [] } } }
feature { key: "gender" value { bytes_list { value: [ "f" ] } } }
feature { key: "kws" value { bytes_list { value: [ "sports" ] } } }
}
]
```
We can use arguments:
```
features: {
"age": FixedLenFeature([], dtype=tf.int64, default_value=-1),
"gender": FixedLenFeature([], dtype=tf.string),
"kws": VarLenFeature(dtype=tf.string),
}
```
And the expected output is:
```python
{
"age": [[0], [-1]],
"gender": [["f"], ["f"]],
"kws": SparseTensor(
indices=[[0, 0], [0, 1], [1, 0]],
values=["code", "art", "sports"]
dense_shape=[2, 2]),
}
```
Args:
file_pattern: List of files or patterns of file paths containing
`Example` records. See `tf.gfile.Glob` for pattern rules.
batch_size: An int representing the number of records to combine
in a single batch.
features: A `dict` mapping feature keys to `FixedLenFeature` or
`VarLenFeature` values. See `tf.parse_example`.
reader: A function or class that can be
called with a `filenames` tensor and (optional) 
```
<Overlap Ratio: 0.9618943930321175>

---

--- 330 --
Question ID: 305a54370db17c2200ccd40a8347199a7b89a58f_1
Original Code:
```
def filter_files(predicate, files):
    for filenames in files.values():
        new_filenames = tuple(filter(predicate, filenames))
        filenames.clear()
        filenames.extend(new_filenames)
```


Overlapping Code:
```
lenames in files.values():
new_filenames = tuple(filter(predicate, filenames))
filenames.clear()
fil
```
<Overlap Ratio: 0.5882352941176471>

---

--- 331 --
Question ID: e79fce9f9693f141e9b49b0e400cbc25938a6035_0
Original Code:
```
def create_window():
    # Pretend the process
    window = tk.Tk()
    window.title('My Window')
    window.geometry('0x0')
    window.mainloop()
```


Overlapping Code:
```
indow = tk.Tk()
window.title('My Window')
window.geometry
```
<Overlap Ratio: 0.4523809523809524>

---

--- 332 --
Question ID: 6e2c44a9819f248e0d8243bd043775df65f71567_5
Original Code:
```
def ecs_init():
    # Setup AWS connection
    aws_region = os.getenv('REGION', 'us-east-1')
    logger.info("Connecting ecs to region \"%s\"", aws_region)
    
    global ecs, dynamodb_client, autoscaling
    ecs = boto3.client('ecs', region_name=aws_region)
    autoscaling = boto3.client("application-autoscaling", region_name=aws_region)
    dynamodb_client = boto3.client('dynamodb', region_name=aws_region)

    logger.info("Connected ecs to region \"%s\"", aws_region)
```


Overlapping Code:
```
AWS connection
aws_region = os.getenv('REGION', 'us-east-1')
logger.info("Connecting ecs to region \"%s\"", aws_region)

global ecs, dynamodb_client, autoscaling
ecs = boto3.client('ecs', region_name=aws_region)
autoscaling = boto3.client("application-autoscaling", region_name=aws_region)
dynamodb_client = boto3.client('dynamodb', region_name=aws_region)
logger.info("Connected ecs to region \"%s\"
```
<Overlap Ratio: 0.91324200913242>

---

--- 333 --
Question ID: d3c42b61b2cc3897bab7a120cbae7757dba57eeb_1
Original Code:
```
def reverse_replay_path(path: _PathType) -> Iterator[AccessAssignment]:
	with open(path, mode='rb') as file:
		for assgnm in reverse_replay(file):
			yield assgnm
```


Overlapping Code:
```
_path(path: _PathType) -> Iterator[AccessAssignment]:
with open(path, mode='rb') as file:
for assgnm 
```
<Overlap Ratio: 0.6474358974358975>

---

--- 334 --
Question ID: 5a3b2f1a78ce0acfc63139c48cf2862a4a7bfbad_1
Original Code:
```
def normalize_angle(x):
    x = x % (2 * np.pi)  # force in range [0, 2 pi)
    if x > np.pi:  # move to [-pi, pi)
        x -= 2 * np.pi
    return x
```


Overlapping Code:
```
def normalize_angle(x):
x = x % (2 * np.pi) # force in range [0, 2 pi)
if x > np.pi: # move to [-pi, pi)
x -= 2 * np.pi
retu
```
<Overlap Ratio: 0.96875>

---

--- 335 --
Question ID: ff59713865db17e9ed5fd0305bb8a8400e06518d_12
Original Code:
```
def display_cols(target_table, fields, show_disabled=True, order=None,
                 output_format=None):
    """
    Display the columns of data defined by the fields parameter.

    This gets the data from the targets data based on the col_list and prepares
    and displays a table based on those targets_tbl colums.

    Parameters:
      fields: list of strings defining the targets_data columns to be
        displayed.

      target_table: The targets table from the database

      order (:term: `string`): None or name of field upon which the table will
        be sorted for output

      show_disabled(:class:`py:bool`)
        If True, show disabled entries. If not True, entries marked disabled
        are ignored

    """
    if show_disabled:
        if 'ScanEnabled' not in fields:
            fields.append('ScanEnabled')

    table_width = target_table.get_output_width(fields) + len(fields)
    # TODO. the above is incorrect in that some fields are of indeterminate
    # length.  The definition in targetstable is not correct.
    fold = False if table_width < 80 else True

    if show_disabled:
        target_ids = sorted(target_table.keys())
    else:
        target_ids = sorted(target_table.get_enabled_targetids())

    # If order defined check to see if valid field
    if order:
        if order not in target_table.get_field_list():
            raise click.ClickException("--order option defines invalid field %s"
                                       % order)

        # create dictionary with order value as key and list of targetids as
        # value. List because the order fields are not unique
        order_dict = defaultdict(list)
        for targetid in target_ids:
            order_dict[target_table[targetid][order]].append(targetid)
        # order_dict = {target_table[targetid][order]: targetid
        #               for targetid in target_ids}
        # TODO this may be inefficient means to sort by keys and get values
        # into list
        target_ids = []
        for key in sorted(order_dict.keys()):
            target_ids.extend(order_dict[key])

    rows = []
    for targetid in target_ids:
        rows.append(target_table.format_record(targetid, fields, fold))

    headers = target_table.tbl_hdr(fields)
    title = 'Target Providers Overview: %s:' % \
            datetime_display_str(datetime.datetime.now())
    if show_disabled:
        title = '%s including disabled targets' % title

    print_table(rows, headers=headers, title=title,
                table_format=output_format)
```


Overlapping Code:
```
splay_cols(target_table, fields, show_disabled=True, order=None,
output_format=None):
"""
Display the columns of data defined by the fields parameter.
This gets the data from the targets data based on the col_list and prepares
and displays a table based on those targets_tbl colums.
Parameters:
fields: list of strings defining the targets_data columns to be
displayed.
target_table: The targets table from the database
order (:term: `string`): None or name of field upon which the table will
be sorted for output
show_disabled(:class:`py:bool`)
If True, show disabled entries. If not True, entries marked disabled
are ignored
"""
if show_disabled:
if 'ScanEnabled' not in fields:
fields.append('ScanEnabled')
table_width = target_table.get_output_width(fields) + len(fields)
# TODO. the above is incorrect in that some fields are of indeterminate
# length. The definition in targetstable is not correct.
fold = False if table_width < 80 else True
if show_disabled:
target_ids = sorted(target_table.keys())
else:
target_ids = sorted(target_table.get_enabled_targetids())
# If order defined check to see if valid field
if order:
if order not in target_table.get_field_list():
raise click.ClickException("--order option defines invalid field %s"
% order)
# create dictionary with order value as key and list of targetids as
# value. List because the order fields are not unique
order_dict = defaultdict(list)
for targetid in target_ids:
order_dict[target_table[targetid][order]].append(targetid)
# order_dict = {target_table[targetid][order]: targetid
# for targetid in target_ids}
# TODO this may be inefficient means to sort by keys and get values
# into list
target_ids = []
for key in sorted(order_dict.keys()):
target_ids.extend(order_dict[key])
rows = []
for targetid in target_ids:
rows.append(target_table.format_record(targetid, fields, fold))
headers = target_table.tbl_hdr(fields)
title = 'Target Providers Overview: %s:' % \
datetime_display_str(datetime.datetime.now())
if show_disabled:
title = '%s including disabled targets' % title
pri
```
<Overlap Ratio: 0.9822712026832774>

---

--- 336 --
Question ID: 0220ca6c530af678b249eb7ed0726942eed3cd3b_2
Original Code:
```
def palindrom_bul():
    global islem_sayisi
    palindrom_sayisi = 0

    # her satırda kontrol edilecek.
    for satir in range(len(list)):

        '''10 ve daha fazla kelime palindrom mu kontrolü.
        her sütuna bakılacak'''
        for sınır in range(10, len(list[0]) + 1):
            baslangic = 0

            # satır sonuna geldi mi kontrolü.
            while (sınır != len(list[0]) + 1):

                # polindrom kontrolü
                word = "".join(list[satir][baslangic:sınır])
                palindrom = str(word) == str(word)[::-1]

                islem_sayisi += 1
                # palindrom bulundu ise yazdır.
                if (palindrom == True):
                    palindrom_sayisi += 1
                    print(palindrom, word)

                baslangic += 1
                sınır += 1

    print("palindrom sayısı ->", palindrom_sayisi)
```


Overlapping Code:
```
_bul():
global islem_sayisi
palindrom_sayisi = 0
# her satırda kontrol edilecek.
for satir in range(len(list)):
'''10 ve daha fazla kelime palindrom mu kontrolü.
her sütuna bakılacak'''
for sınır in range(10, len(list[0]) + 1):
baslangic = 0
# satır sonuna geldi mi kontrolü.
while (sınır != len(list[0]) + 1):
# polindrom kontrolü
word = "".join(list[satir][baslangic:sınır])
palindrom = str(word) == str(word)[::-1]
islem_sayisi += 1
# palindrom bulundu ise yazdır.
if (palindrom == True):
palindrom_sayisi += 1
print(palindrom, word)
baslangic += 1
sınır += 1
print("palindrom sayısı ->", palindro
```
<Overlap Ratio: 0.9646302250803859>

---

--- 337 --
Question ID: 8a9b490b8dead69bfb2c9b3ca3c5e6dff05a38f7_0
Original Code:
```
def log_zinb_positive(
    x: torch.Tensor, mu: torch.Tensor, theta: torch.Tensor, pi: torch.Tensor, eps=1e-8
):
    """
    Log likelihood (scalar) of a minibatch according to a zinb model.

    Parameters
    ----------
    x
        Data
    mu
        mean of the negative binomial (has to be positive support) (shape: minibatch x vars)
    theta
        inverse dispersion parameter (has to be positive support) (shape: minibatch x vars)
    pi
        logit of the dropout parameter (real support) (shape: minibatch x vars)
    eps
        numerical stability constant

    Notes
    -----
    We parametrize the bernoulli using the logits, hence the softplus functions appearing.
    """
    # theta is the dispersion rate. If .ndimension() == 1, it is shared for all cells (regardless of batch or labels)
    if theta.ndimension() == 1:
        theta = theta.view(
            1, theta.size(0)
        )  # In this case, we reshape theta for broadcasting

    softplus_pi = F.softplus(-pi)  #  uses log(sigmoid(x)) = -softplus(-x)
    log_theta_eps = torch.log(theta + eps)
    log_theta_mu_eps = torch.log(theta + mu + eps)
    pi_theta_log = -pi + theta * (log_theta_eps - log_theta_mu_eps)

    case_zero = F.softplus(pi_theta_log) - softplus_pi
    mul_case_zero = torch.mul((x < eps).type(torch.float32), case_zero)

    case_non_zero = (
        -softplus_pi
        + pi_theta_log
        + x * (torch.log(mu + eps) - log_theta_mu_eps)
        + torch.lgamma(x + theta)
        - torch.lgamma(theta)
        - torch.lgamma(x + 1)
    )
    mul_case_non_zero = torch.mul((x > eps).type(torch.float32), case_non_zero)

    res = mul_case_zero + mul_case_non_zero

    return res
```


Overlapping Code:
```
def log_zinb_positive(
x: torch.Tensor, mu: torch.Tensor, theta: torch.Tensor, pi: torch.Tensor, eps=1e-8
):
"""
Log likelihood (scalar) of a minibatch according to a zinb model.
Parameters
----------
x
Data
mu
mean of the negative binomial (has to be positive support) (shape: minibatch x vars)
theta
inverse dispersion parameter (has to be positive support) (shape: minibatch x vars)
pi
logit of the dropout parameter (real support) (shape: minibatch x vars)
eps
numerical stability constant
Notes
-----
We parametrize the bernoulli using the logits, hence the softplus functions appearing.
"""
# theta is the dispersion rate. If .ndimension() == 1, it is shared for all cells (regardless of batch or labels)
if theta.ndimension() == 1:
theta = theta.view(
1, theta.size(0)
) # In this case, we reshape theta for broadcasting
softplus_pi = F.softplus(-pi) # uses log(sigmoid(x)) = -softplus(-x)
log_theta_eps = torch.log(theta + eps)
log_theta_mu_eps = torch.log(theta + mu + eps)
pi_theta_log = -pi + theta * (log_theta_eps - log_theta_mu_eps)
case_zero = F.softplus(pi_theta_log) - softplus_pi
mul_case_zero = torch.mul((x < eps).type(torch.float32), case_zero)
case_non_zero = (
-softplus_pi
+ pi_theta_log
+ x * (torch.log(mu + eps) - log_theta_mu_eps)
+ torch.lgamma(x + theta)
- torch.lgamma(theta)
- torch.lgamma(x + 1)
)
mul_case_non_zero = torch.mul((x > eps).type(torch.float32), case_non_zero)
res = mul_case_zero + mul_case_non_zero
return res
```
<Overlap Ratio: 1.0>

---

--- 338 --
Question ID: 666971ee0009d0bafb0de9d0ae6601b14f216e6d_2
Original Code:
```
@scheme
def main_path(source_path, download_yt_captions, convert_wav):
    if convert_wav:
        src = AudioFileClip(source_path)
        filename = os.path.basename(source_path)
        name, _ = os.path.splitext(filename)
        wav_path = os.path.join(tempdir_path(), name + '.wav')
        src.write_audiofile(wav_path)
        return wav_path
    else:
        return source_path
```


Overlapping Code:
```
 download_yt_captions, convert_wav):
if convert_wav:
src = AudioFileClip(source_path)
filename = os.path.basename(source_path)
name, _ = os.path.splitext(filename)
wav_path = os.path.join(tempdir_path(), name + '.wav')
src.write_audiofile(wav_path)
r
```
<Overlap Ratio: 0.7739938080495357>

---

--- 339 --
Question ID: c49375650ec8435cc5750eabedccb5b6e7df7aae_0
Original Code:
```
def result_path_to_dict(result_path):
    result_basename = os.path.basename(result_path).split('.')[0]
    temp_result_dict = {}

    with open(result_path,'r') as f:
        for line in f:
            if not line.startswith("#"):
                temp_line = [result_basename]
                temp_line += split_str_rem_whitespace(line)[:22]
                temp_line[5] = temp_line[5].split(".")[0]

                if temp_line[1] in temp_result_dict.keys():
                    temp_result_dict[temp_line[1]].append(temp_line)
                else:
                    temp_result_dict[temp_line[1]] = [temp_line]
    return temp_result_dict
```


Overlapping Code:
```
to_dict(result_path):
result_basename = os.path.basename(result_path).split('.')[0]
temp_result_dict = {}
with open(result_path,'r') as f:
for line in f:
if not line.startswith("#"):
temp_line = [result_basename]
temp_line += split_str_rem_whitespace(line)[:22]
temp_line[5] = temp_line[5].split(".")[0]
if temp_line[1] in temp_result_dict.keys():
temp_result_dict[temp_line[1]].append(temp_line)
else:
temp_result_dict[temp_line[1]] = [temp_line]
re
```
<Overlap Ratio: 0.9240246406570842>

---

--- 340 --
Question ID: 113d12a47d2ffbce2673b6b05a7a1596f6ef4aaa_5
Original Code:
```
def print_list_to_user():
        with open('countries-json/country-by-abbreviation.json') as json_file:
            number = 0
            string = ""
            for line in yaml.safe_load(json_file):
                string += "{}. {}:{}".format(number, line['COUNTRY'],\
                              line['ABBREVIATION'] + '\n')
                number += 1
        number = 0
        pager(string)
```


Overlapping Code:
```
open('countries-json/country-by-abbreviation.json') as json_file:
number = 0
string = ""
for line in yaml.safe_load(json_file):
string += "{}. {}:{}".format(number, line['COUNTRY'],\
line['ABBREVIATIO
```
<Overlap Ratio: 0.7168458781362007>

---

--- 341 --
Question ID: 42a6acec6cd2a414251e61ed9b36a5f1d2cbed99_7
Original Code:
```
def test_clarification_edit_diff():
    question = {
        "OwnerUserId": "1",
        "Comments": [{
            "Id": "1",
            "Text": "is this a question? some unrelated text.",
            "UserId": "2",
            "CreationDate": "2010-07-22T07:00:00.000",
        }],
        "Edits": initial_edits + [{
            "Id": "5",
            "PostHistoryTypeId": "5",  # edit body
            "RevisionGUID": "guid3",
            "CreationDate": "2010-07-22T08:00:00.000",
            "UserId": "1",
            "Text": "question body, answer to question"
        }]
    }

    cq = ClarificationQuestion("is this a question ?",
                               parse_time("2010-07-22T07:00:00.000"), entity_id="1")
    ce = annotator._clarification_edit(question, cq)

    assert ce.text == "question body , answer to question"
    assert ce.diff.insert == " , answer to question"
    assert ce.diff.start_offset == 13
    assert ce.diff.end_offset == 34
```


Overlapping Code:
```
est_clarification_edit_diff():
question = {
"OwnerUserId": "1",
"Comments": [{
"Id": "1",
"Text": "is this a question? some unrelated text.",
"UserId": "2",
"CreationDate": "2010-07-22T07:00:00.000",
}],
"Edits": initial_edits + [{
"Id": "5",
"PostHistoryTypeId": "5", # edit body
"RevisionGUID": "guid3",
"CreationDate": "2010-07-22T08:00:00.000",
"UserId": "1",
"Text": "question body, answer to question"
}]
}
cq = ClarificationQuestion("is this a question ?",
parse_time("2010-07-22T07:00:00.000"), entity_id="1")
ce = annotator._clarification_edit(question, cq)
assert ce.text == "question body , answer to question"
assert ce.diff.insert == " , answer to question"
assert ce.diff.start_offset =
```
<Overlap Ratio: 0.9446693657219973>

---

--- 342 --
Question ID: 8e0c36be28eea50c71e90b501763b6ebed13be18_0
Original Code:
```
def _get_columns(item):
    column_map = {
    }
    inv_columns = ['']
    return sdk_utils.get_osc_show_columns_for_sdk_resource(item, column_map,
                                                           inv_columns)
```


Overlapping Code:
```
v_columns = ['']
return sdk_utils.get_osc_show_columns_for_sdk_resource(item, column_map,
inv_column
```
<Overlap Ratio: 0.6896551724137931>

---

--- 343 --
Question ID: 33c7add7e4d859c7c7182c4d4fd999748c1ef12c_0
Original Code:
```
def clearWebhooks():
  """Will clear all webhooks associated with the current token."""

  api = WebexTeamsAPI(access_token=teams_token)
  webhooks = api.webhooks.list()

  # no need to iterate through all the webhooks if the list is empty
  if len(list(webhooks)) == 0:
    print('no webhooks registered')
    return

  print('clearing webhooks...')

  # iterate through all webhooks and delete them
  for webhook in webhooks:
    print('deleting webhook "%(webhookName)s"...' % {'webhookName' : webhook.name})
    api.webhooks.delete(webhook.id)

  print('done')
```


Overlapping Code:
```
ebhooks associated with the current token."""
api = WebexTeamsAPI(access_token=teams_token)
webhooks = api.webhooks.list()
# no need to iterate through all the webhooks if the list is empty
if len(list(webhooks)) == 0:
print('no webhooks registered')
return
print('clearing webhooks...')
# iterate through all webhooks and delete them
for webhook in webhooks:
print('deleting webhook "%(webhookName)s"...' % {'webhookName' : webhook.name})
api.webhoo
```
<Overlap Ratio: 0.8571428571428571>

---

--- 344 --
Question ID: a982b089080491ccbb04052acf293edc7cb2a2f1_0
Original Code:
```
def preview_element(my_app: "sqlApp"):
    help_text = """
    Press Enter in the input box to page through the table.
    Alternatively, enter a filtering SQL statement and then press Enter
    to page through the results.
    """
    formatter = TabularOutputFormatter()
    input_buffer = Buffer(
            name = "previewbuffer",
            tempfile_suffix = ".sql",
            multiline = False
            )

    input_control = BufferControl(
            buffer = input_buffer,
            include_default_input_processors = False,
            preview_search = False
    )
    input_window = Window(
            input_control,
        )

    search_buffer = Buffer(name = "previewsearchbuffer")
    search_field = SearchToolbar(search_buffer)
    output_field = TextArea(style = "class:preview-output-field",
            text = help_text,
            height = D(preferred = 50),
            search_field=search_field,
            wrap_lines = False,
            focusable = True,
            read_only = True,
            preview_search = True,
            input_processors = [
                ConditionalProcessor(
                    processor=HighlightIncrementalSearchProcessor(),
                    filter=has_focus("previewsearchbuffer")
                    | has_focus(search_field.control),
                    ),
                HighlightSelectionProcessor(),
            ]
            )

    def refresh_results(window_height) -> bool:
        sql_conn = my_app.selected_object.conn

        if sql_conn.execution_status == executionStatus.FAIL:
            # Let's display the error message to the user
            output = sql_conn.execution_err
        else:
            crsr = sql_conn.cursor
            if crsr.description:
                cols = [col.name for col in crsr.description]
            else:
                cols = []
            if len(cols):
                sql_conn.status = connStatus.FETCHING
                res = sql_conn.async_fetchmany(size = window_height - 4)
                output = formatter.format_output(res, cols, format_name = "psql")
                output = "\n".join(output)
            else:
                sql_conn.status = connStatus.IDLE
                output = "No rows returned\n"

        # Add text to output buffer.
        output_field.buffer.set_document(Document(
            text = output, cursor_position = 0), True)

        return True

    def accept(buff: Buffer) -> bool:
        obj = my_app.selected_object
        sql_conn = obj.conn
        catalog = None
        schema = None
        # TODO: Verify connected
        if obj.parent is not None:
            if type(obj.parent).__name__ == "myDBSchema":
                schema = obj.parent.name
            elif type(obj.parent).__name__ == "myDBCatalog":
                catalog = obj.parent.name
            if obj.parent.parent is not None:
                if type(obj.parent.parent).__name__ == "myDBCatalog":
                    catalog = obj.parent.parent.name

        if catalog:
            catalog =  (sql_conn.quotechar + "%s" + sql_conn.quotechar) % catalog
        if schema:
            schema =  (sql_conn.quotechar + "%s" + sql_conn.quotechar) % schema
        name = (sql_conn.quotechar + "%s" + sql_conn.quotechar) % obj.name
        identifier = ".".join(list(filter(None, [catalog, schema, obj.name])))
        query = sql_conn.preview_query(table = identifier, filter_query = buff.text,
                limit = my_app.preview_limit_rows)

        func = partial(refresh_results,
                window_height = output_field.window.render_info.window_height)
        # If status is IDLE, this is the first time we are executing.
        if sql_conn.query != query or sql_conn.status == connStatus.IDLE:
            # Exit the app to execute the query
            my_app.application.exit(result = ["preview", query])
            my_app.application.pre_run_callables.append(func)
        else:
            # No need to exit let's just go and fetch
            func()
        return True # Keep filter text

    input_buffer.accept_handler = accept

    def cancel_handler() -> None:
        sql_conn = my_app.selected_object.conn
        sql_conn.close_cursor()
        sql_conn.status = connStatus.IDLE
        input_buffer.text = ""
        output_field.buffer.set_document(Document(
            text = help_text, cursor_position = 0
        ), True)
        my_app.show_preview = False
        my_app.show_sidebar = True
        my_app.application.layout.focus(input_buffer)
        my_app.application.layout.focus("sidebarbuffer")
        return None

    cancel_button = Button(text = "Done", handler = cancel_handler)

    container = HSplit(
            [
                Box(
                    body = VSplit(
                        [input_window, cancel_button],
                        padding=1
                    ),
                    padding=1,
                    style="class:preview-input-field"
                ),
                Window(height=1, char="-", style="class:preview-divider-line"),
                output_field,
                search_field,
                ]
            )

    frame = Shadow(
            body = Frame(
                title = "Table Preview",
                body = container,
                style="class:dialog.body",
                width = D(preferred = 180, min = 30),
                modal = True
            )
    )


    return ConditionalContainer(
            content = frame,
            filter = ShowPreview(my_app) & ~is_done
    )
```


Overlapping Code:
```
"sqlApp"):
help_text = """
Press Enter in the input box to page through the table.
Alternatively, enter a filtering SQL statement and then press Enter
to page through the results.
"""
formatter = TabularOutputFormatter()
input_buffer = Buffer(
name = "previewbuffer",
tempfile_suffix = ".sql",
multiline = False
)
input_control = BufferControl(
buffer = input_buffer,
include_default_input_processors = False,
preview_search = False
)
input_window = Window(
input_control,
)
search_buffer = Buffer(name = "previewsearchbuffer")
search_field = SearchToolbar(search_buffer)
output_field = TextArea(style = "class:preview-output-field",
text = help_text,
height = D(preferred = 50),
search_field=search_field,
wrap_lines = False,
focusable = True,
read_only = True,
preview_search = True,
input_processors = [
ConditionalProcessor(
processor=HighlightIncrementalSearchProcessor(),
filter=has_focus("previewsearchbuffer")
| has_focus(search_field.control),
),
HighlightSelectionProcessor(),
]
)
def refresh_results(window_height) -> bool:
sql_conn = my_app.selected_object.conn
if sql_conn.execution_status == executionStatus.FAIL:
# Let's display the error message to the user
output = sql_conn.execution_err
else:
crsr = sql_conn.cursor
if crsr.description:
cols = [col.name for col in crsr.description]
else:
cols = []
if len(cols):
sql_conn.status = connStatus.FETCHING
res = sql_conn.async_fetchmany(size = window_height - 4)
output = formatter.format_output(res, cols, format_name = "psql")
output = "\n".join(output)
else:
sql_conn.status = connStatus.IDLE
output = "No rows returned\n"
# Add text to output buffer.
output_field.buffer.set_document(Document(
text = output, cursor_position = 0), True)
return True
def accept(buff: Buffer) -> bool:
```
<Overlap Ratio: 0.9684560044272275>

---

--- 345 --
Question ID: 225edfcc9f8e7320473f2236c21115016fff4784_6
Original Code:
```
@Resolver.register
@isLoggedIn
def play(plugin, channel_id, showtime=None, srno=None):
    with open(EXTRA_CHANNELS, "r") as f:
        extra = json.load(f)
    if showtime is None and extra.get(str(channel_id)):
        return PLAY_EX_URL + extra.get(str(channel_id)).get("data")

    rjson = {
        "channel_id": int(channel_id),
        "stream_type": "Seek"
    }
    if showtime and srno:
        rjson["showtime"] = showtime
        rjson["srno"] = srno
        rjson["stream_type"] = "Catchup"

    resp = urlquick.post(GET_CHANNEL_URL, json=rjson).json()
    return Listitem().from_dict(**{
        "label": plugin._title,
        "callback": resp.get("result", "") + "?" + urlencode(getTokenParams()),
        "properties": {
            "IsPlayable": True,
            "inputstreamaddon": "inputstream.adaptive",
            "inputstream.adaptive.stream_headers": "User-Agent=KAIOS",
            "inputstream.adaptive.manifest_type": "hls",
            "inputstream.adaptive.license_key": urlencode(getTokenParams()) + "|" + urlencode(getHeaders()) + "|R{SSM}|",
        }
    })
```


Overlapping Code:
```
f play(plugin, channel_id, showtime=None, srno=None):
with open(EXTRA_CHANNELS, "r") as f:
extra = json.load(f)
if showtime is None and extra.get(str(channel_id)):
return PLAY_EX_URL + extra.get(str(channel_id)).get("data")
rjson = {
"channel_id": int(channel_id),
"stream_type": "Seek"
}
if showtime and srno:
rjson["showtime"] = showtime
rjson["srno"] = srno
rjson["stream_type"] = "Catchup"
resp = urlquick.post(GET_CHANNEL_URL, json=rjson).json()
return Listitem().from_dict(**{
"label": plugin._title,
"callback": resp.get("result", "") + "?" + urlencode(getTokenParams()),
"properties": {
"IsPlayable": True,
"inputstreamaddon": "inputstream.adaptive",
"inputstream.adaptive.stream_headers": "User-Agent=KAIOS",
"inputstream.adaptive.manifest_type": "hls",
"inputstream.adaptive.license_key": urlencode(getTokenParams()) + "|" + urlencode(getHea
```
<Overlap Ratio: 0.9351648351648352>

---

--- 346 --
Question ID: b268a992fd9a0747d77e55dac4560f07ec08c871_0
Original Code:
```
def test_rank(candidates):
    model_name = "LR_15_components"
    result = rank(candidates, model_name)
    assert isinstance(result, list)
```


Overlapping Code:
```
_components"
result = rank(candidates, model_name)
```
<Overlap Ratio: 0.390625>

---

--- 347 --
Question ID: 5a75e5a14da8c33d6eba2f02a6e9aa18f45849f6_4
Original Code:
```
@app.callback(
    dash.dependencies.Output('logfield','children'),
    [dash.dependencies.Input('interval_log','n_intervals')]
)

def logupdate(n):
    with open("datalogger.log", "r") as file:
        i=0
        lines_size = 20
        last_lines = []
        for line in file:
            if i < lines_size:
                last_lines.append(line)
            else:
                last_lines[i%lines_size] = line
            i = i + 1
 
    last_lines = last_lines[(i%lines_size):] + last_lines[:(i%lines_size)]

    output=""

    for line in last_lines:
        output+=line
       
    
    return output
```


Overlapping Code:
```
callback(
dash.dependencies.Output('logfield','children'),
[dash.dependencies.Input('interval_log','n_intervals')]
)
def logupdate(n):
with open("datalogger.log", "r") as file:
i=0
lines_size = 20
last_lines = []
for line in file:
if i < lines_size:
last_lines.append(line)
else:
last_lines[i%lines_size] = line
i = i + 1

last_lines = last_lines[(i%lines_size):] + last_lines[:(i%lines_size)]
output=""
for line in last_lines:
output+=line


return 
```
<Overlap Ratio: 0.9761388286334056>

---

--- 348 --
Question ID: d542abf61ad1a206d0eafc9e06cad3b3673edccc_1
Original Code:
```
def plot_related_features(spn, featureId_x, featureId_y, detail=100, dictionary=None, evidence=None, fname=None):
    """
    Plots a 2d representation of the joint marginal probability of these two
    features.

    :param spn: the root node of the spn
    :param featureId_x: featureid of the first feature
    :param featureId_y: featureid of the second feature
    :param detail: granularity of the plotting grid
    :param dictionary: the data dictionary to extract meta information
    :param evidence: evidence to condition the plot on
    :param fname: file name to save the resulting plot
    :return: a plotly dictionary containing data and context
    """

    # construct the grid
    num_features = len(spn.scope)
    context = dictionary['context']
    categoricals = get_categoricals(spn, context)
    domain_x = context.get_domains_by_scope([featureId_x])[0]
    domain_y = context.get_domains_by_scope([featureId_y])[0]
    feature_names = context.feature_names
    x_range = (domain_x[0],
               domain_x[-1])
    y_range = (domain_y[0],
               domain_y[-1])
    x_detail = detail
    y_detail = detail
    x_cat = False
    y_cat = False
    if featureId_x in categoricals:
        x_cat = True
        x_detail = len(domain_x)
        enc = dictionary['features'][featureId_x]['encoder'].inverse_transform
        x_range = domain_x
        x_names = enc([int(x) for x in x_range])
    if featureId_y in categoricals:
        y_cat = True
        y_detail = len(domain_y)
        enc = dictionary['features'][featureId_y]['encoder'].inverse_transform
        y_range = domain_y
        y_names = enc([int(y) for y in y_range])
    grid = np.mgrid[x_range[0]:x_range[-1]:x_detail*1j, y_range[0]:y_range[-1]:y_detail*1j]
    grid = grid.reshape(2,-1).T

    # construct query
    query = np.zeros((1,num_features))
    query[:] = np.nan
    query = np.repeat(query, grid.shape[0], axis=0)
    query[:, featureId_x] = grid[:, 0]
    query[:, featureId_y] = grid[:, 1]

    # calculate the probability and shape the array
    result = likelihood(spn, query)

    result.shape = (x_detail, y_detail)
    
    # plot
    data = [Heatmap(z=result,
            x=np.linspace(domain_y[0], domain_y[-1], y_detail) if not y_cat else y_names,
            y=np.linspace(domain_x[0], domain_x[-1], x_detail) if not x_cat else x_names,
            colorbar=ColorBar(
                title='Colorbar'
            ),
            colorscale='Hot')]
    layout = dict(width=450, 
                  height=450,
                  xaxis=dict(title=feature_names[featureId_y], autotick=True),
                  yaxis=dict(title=feature_names[featureId_x], autotick=True)
                 )

    if fname is None:
        return {'data': data, 'layout': layout}
    else:
        raise NotImplementedError
```


Overlapping Code:
```
res(spn, featureId_x, featureId_y, detail=100, dictionary=None, evidence=None, fname=None):
"""
Plots a 2d representation of the joint marginal probability of these two
features.
:param spn: the root node of the spn
:param featureId_x: featureid of the first feature
:param featureId_y: featureid of the second feature
:param detail: granularity of the plotting grid
:param dictionary: the data dictionary to extract meta information
:param evidence: evidence to condition the plot on
:param fname: file name to save the resulting plot
:return: a plotly dictionary containing data and context
"""
# construct the grid
num_features = len(spn.scope)
context = dictionary['context']
categoricals = get_categoricals(spn, context)
domain_x = context.get_domains_by_scope([featureId_x])[0]
domain_y = context.get_domains_by_scope([featureId_y])[0]
feature_names = context.feature_names
x_range = (domain_x[0],
domain_x[-1])
y_range = (domain_y[0],
domain_y[-1])
x_detail = detail
y_detail = detail
x_cat = False
y_cat = False
if featureId_x in categoricals:
x_cat = True
x_detail = len(domain_x)
enc = dictionary['features'][featureId_x]['encoder'].inverse_transform
x_range = domain_x
x_names = enc([int(x) for x in x_range])
if featureId_y in categoricals:
y_cat = True
y_detail = len(domain_y)
enc = dictionary['features'][featureId_y]['encoder'].inverse_transform
y_range = domain_y
y_names = enc([int(y) for y in y_range])
grid = np.mgrid[x_range[0]:x_range[-1]:x_detail*1j, y_range[0]:y_range[-1]:y_detail*1j]
grid = grid.reshape(2,-1).T
# construct query
query = np.zeros((1,num_features))
query[:] = np.nan
query = np.repeat(query, grid.shape[0], axis=0)
query[:, featureId_x] = grid[:, 0]
query[:, featureId_y] = grid[:, 1]
# calculate the probability and shape the array
result = likelihood(spn, query)
result.shape = (x_detail, y_detail)

# plot
data = [Heatmap(z=result,
x=np.linspace(domain_y[0], domain_y[-1], y_detail) if not y_cat else y_names,
y=np.linspace(domain_x[0], domain_x[-1], x_detail) if not x_cat else x_names,
colorbar=ColorBar(
title='Colorbar'
),
colorscale='Hot')]
layout = 
```
<Overlap Ratio: 0.9826860084230229>

---

--- 349 --
Question ID: 51eac0c750ecb1ecd08b99a51d74d7a88d097d37_1
Original Code:
```
def breadth_first_search(tree, target):
    """
    Exercise 2: Implement your depth first search algorithm here
    """
    nodes = [tree]
    while nodes:
        node = nodes.pop(0)
        print("BFS: Checking node with value '{}'".format(node.value))
        if node.value == target:
            return node
        if node.left:
            nodes.append(node.left)
        if node.right:
            nodes.append(node.right)
```


Overlapping Code:
```
t):
"""
Exercise 2: Implement your depth first search algorithm here
"""
nodes = [tree]
while nodes:
node = nodes.pop(0)
print("BFS: Checking node with value '{}'".format(node.value))
if node.value == target:
return node
if node.left:
nodes.append(node.left)
if node.right:
nodes.append(node.right
```
<Overlap Ratio: 0.8892215568862275>

---

--- 350 --
Question ID: 306e4a4f0346704936d6b27a3e2b1fb84cabd3e1_0
Original Code:
```
def d_transform(img,gt_bboxes,gt_masks=None):
    img=torch.tensor(img)
    assert gt_bboxes.shape==(1,4)
    box=gt_bboxes[0].astype(np.int32)
    box_img=np.zeros((512,512),dtype=np.uint8)
    box_img[box[1]:box[3],box[0]:box[2]]=1
    img=TF.to_pil_image(img)
    box_img=TF.to_pil_image(box_img)

    angle=random.randint(-10,10)
    #angle=90
    m_rotate= lambda x: TF.rotate(x,angle)

    if gt_masks is not None:
        mask_img=TF.to_pil_image(torch.tensor(gt_masks))
        r_img,r_box,r_mask=map(m_rotate,[img,box_img,mask_img])
        return unpack_pil(r_img,r_box,r_mask)
    else:
        r_img,r_box=map(m_rotate,[img,box_img])
        return unpack_pil(r_img,r_box)
```


Overlapping Code:
```
s=None):
img=torch.tensor(img)
assert gt_bboxes.shape==(1,4)
box=gt_bboxes[0].astype(np.int32)
box_img=np.zeros((512,512),dtype=np.uint8)
box_img[box[1]:box[3],box[0]:box[2]]=1
img=TF.to_pil_image(img)
box_img=TF.to_pil_image(box_img)
angle=random.randint(-10,10)
#angle=90
m_rotate= lambda x: TF.rotate(x,angle)
if gt_masks is not None:
mask_img=TF.to_pil_image(torch.tensor(gt_masks))
r_img,r_box,r_mask=map(m_rotate,[img,box_img,mask_img])
return unpack_pil(r_img,r_box,r_mask)
else:
r_img,r_box=map(m_rotate,[img,box_img])
return unpack_pil(r_img
```
<Overlap Ratio: 0.9259259259259259>

---

--- 351 --
Question ID: d6b0fb2d72085fe98785f68d017e42639962caa0_1
Original Code:
```
def find_svf(n_states, trajectories):
    """
    Find the state visitation frequency from trajectories.

    n_states: Number of states. int.
    trajectories: 3D array of state/action pairs. States are ints, actions
        are ints. NumPy array with shape (T, L, 2) where T is the number of
        trajectories and L is the trajectory length.
    -> State visitation frequencies vector with shape (N,).
    """

    svf = np.zeros(n_states)

    for trajectory in trajectories:
        for state, _, _ in trajectory:
            svf[state] += 1

    svf /= trajectories.shape[0]

    return svf
```


Overlapping Code:
```
_svf(n_states, trajectories):
"""
Find the state visitation frequency from trajectories.
n_states: Number of states. int.
trajectories: 3D array of state/action pairs. States are ints, actions
are ints. NumPy array with shape (T, L, 2) where T is the number of
trajectories and L is the trajectory length.
-> State visitation frequencies vector with shape (N,).
"""
svf = np.zeros(n_states)
for trajectory in trajectories:
for state, _, _ in trajectory:
svf[state] += 1
svf /= trajectories.shape[0]
return svf
```
<Overlap Ratio: 0.9845261121856866>

---

--- 352 --
Question ID: d1d875c17a52a7299af027dd66b0ae884c4b58cc_0
Original Code:
```
def main(args):
    """The main Command Line Interface for Segmentify"""

    # parse in images
    imgs = [util.parse_img(img) for img in args.images]

    if len(imgs) > 1:
        imgs = np.stack(imgs, axis=0)
    else:
        imgs = np.array(imgs)

    with gui_qt():
        viewer = Viewer(imgs, heatmap=args.heatmap)
```


Overlapping Code:
```
s):
"""The main Command Line Interface for Segmentify"""
# parse in images
imgs = [util.parse_img(img) for img in args.images]
if len(imgs) > 1:
imgs = np.stack(imgs, axis=0)
else:
imgs = np.array(imgs)
with gui_qt():
viewer = Viewer(img
```
<Overlap Ratio: 0.8681318681318682>

---

--- 353 --
Question ID: 78899763761ba1ba1832a56a870e4aa4b4167d4e_3
Original Code:
```
def move_border(old_center, old_border_diameter, center, border_diameter, delta_time, steps=1):
    global server_ip, server_password, stats_control_ok
    # waiting 10 seconds to avoid any problems with other orders at the round start
    sleep(10)
    # connect to server
    with MCRcon(server_ip, server_password) as mcr:
        # one step per second (starting by 1; ending by delta_time)
        for n in range(1, delta_time + 1):
            # end shrink when someone won
            if not stats_control_ok:
                break

            # calculating new diameter with a linear equation
            # y=m*x+n
            # m=gradient=(delta y)/(delta x)=(border_diameter - old_border_diameter) / delta_time
            # x=current step=n
            # n=offset=old_border_diameter
            temp_diameter = ((border_diameter - old_border_diameter) / delta_time) * n + old_border_diameter

            # calculation a new temporary center, similar t temp_diameter
            temp_x = ((center[0] - old_center[0]) / delta_time) * n + old_center[0]
            temp_z = ((center[1] - old_center[1]) / delta_time) * n + old_center[1]

            # apply new values
            mcr.command("/worldborder center {} {}".format(temp_x, temp_z))
            mcr.command("/worldborder set {} {}".format(str(temp_diameter), str(int(steps))))

            # wait for one step; default is 1sec.
            sleep(int(steps))
```


Overlapping Code:
```
, center, border_diameter, delta_time, steps=1):
global server_ip, server_password, stats_control_ok
# waiting 10 seconds to avoid any problems with other orders at the round start
sleep(10)
# connect to server
with MCRcon(server_ip, server_password) as mcr:
# one step per second (starting by 1; ending by delta_time)
for n in range(1, delta_time + 1):
# end shrink when someone won
if not stats_control_ok:
break
# calculating new diameter with a linear equation
# y=m*x+n
# m=gradient=(delta y)/(delta x)=(border_diameter - old_border_diameter) / delta_time
# x=current step=n
# n=offset=old_border_diameter
temp_diameter = ((border_diameter - old_border_diameter) / delta_time) * n + old_border_diameter
# calculation a new temporary center, similar t temp_diameter
temp_x = ((center[0] - old_center[0]) / delta_time) * n + old_center[0]
temp_z = ((center[1] - old_center[1]) / delta_time) * n + old_center[1]
# apply new values
mcr.command("/worldborder center {} {}".format(temp_x, temp_z))
mcr.command("/worldborder set {} {}".format(str(temp_diameter), str(int(steps))))
# wait for one step; 
```
<Overlap Ratio: 0.9314140558848434>

---

--- 354 --
Question ID: c8939b6523ddc3e7a9fd8625c26176a2ef5dd22a_7
Original Code:
```
@pytest.mark.parametrize(
    ["inlets", "outlets"],
    [
        pytest.param(
            # Airflow 1.10.x uses a dictionary structure for inlets and outlets.
            # We want the lineage backend to support this structure for backwards
            # compatability reasons, so this test is not conditional.
            {"datasets": [Dataset("snowflake", "mydb.schema.tableConsumed")]},
            {"datasets": [Dataset("snowflake", "mydb.schema.tableProduced")]},
            id="airflow-1-10-lineage-syntax",
        ),
        pytest.param(
            # Airflow 2.x also supports a flattened list for inlets and outlets.
            # We want to test this capability.
            [Dataset("snowflake", "mydb.schema.tableConsumed")],
            [Dataset("snowflake", "mydb.schema.tableProduced")],
            marks=pytest.mark.skipif(
                AIRFLOW_VERSION < packaging.version.parse("2.0.0"),
                reason="list-style lineage is only supported in Airflow 2.x",
            ),
            id="airflow-2-lineage-syntax",
        ),
    ],
)
@mock.patch("datahub_provider.hooks.datahub.DatahubRestHook.make_emitter")
def test_lineage_backend(mock_emit, inlets, outlets):
    DEFAULT_DATE = days_ago(2)
    mock_emitter = Mock()
    mock_emit.return_value = mock_emitter
    # Using autospec on xcom_pull and xcom_push methods fails on Python 3.6.
    with mock.patch.dict(
        os.environ,
        {
            "AIRFLOW__LINEAGE__BACKEND": "datahub_provider.lineage.datahub.DatahubLineageBackend",
            "AIRFLOW__LINEAGE__DATAHUB_CONN_ID": datahub_rest_connection_config.conn_id,
            "AIRFLOW__LINEAGE__DATAHUB_KWARGS": json.dumps(
                {"graceful_exceptions": False, "capture_executions": False}
            ),
        },
    ), mock.patch("airflow.models.BaseOperator.xcom_pull"), mock.patch(
        "airflow.models.BaseOperator.xcom_push"
    ), patch_airflow_connection(
        datahub_rest_connection_config
    ):
        func = mock.Mock()
        func.__name__ = "foo"

        dag = DAG(dag_id="test_lineage_is_sent_to_backend", start_date=DEFAULT_DATE)

        with dag:
            op1 = DummyOperator(
                task_id="task1_upstream",
                inlets=inlets,
                outlets=outlets,
            )
            op2 = DummyOperator(
                task_id="task2",
                inlets=inlets,
                outlets=outlets,
            )
            op1 >> op2

        # Airflow < 2.2 requires the execution_date parameter. Newer Airflow
        # versions do not require it, but will attempt to find the associated
        # run_id in the database if execution_date is provided. As such, we
        # must fake the run_id parameter for newer Airflow versions.
        if AIRFLOW_VERSION < packaging.version.parse("2.2.0"):
            ti = TaskInstance(task=op2, execution_date=DEFAULT_DATE)
        else:
            ti = TaskInstance(task=op2, run_id=f"test_airflow-{DEFAULT_DATE}")
        ctx1 = {
            "dag": dag,
            "task": op2,
            "ti": ti,
            "task_instance": ti,
            "execution_date": DEFAULT_DATE,
            "ts": "2021-04-08T00:54:25.771575+00:00",
        }

        prep = prepare_lineage(func)
        prep(op2, ctx1)
        post = apply_lineage(func)
        post(op2, ctx1)

        # Verify that the inlets and outlets are registered and recognized by Airflow correctly,
        # or that our lineage backend forces it to.
        assert len(op2.inlets) == 1
        assert len(op2.outlets) == 1
        assert all(map(lambda let: isinstance(let, Dataset), op2.inlets))
        assert all(map(lambda let: isinstance(let, Dataset), op2.outlets))

        # Check that the right things were emitted.
        assert mock_emitter.emit.call_count == 9
        # Running further checks based on python version because args only exists in python 3.7+
        if sys.version_info[:3] > (3, 7):
            assert mock_emitter.method_calls[0].args[0].aspectName == "dataFlowInfo"
            assert (
                mock_emitter.method_calls[0].args[0].entityUrn
                == "urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod)"
            )

            assert mock_emitter.method_calls[1].args[0].aspectName == "ownership"
            assert (
                mock_emitter.method_calls[1].args[0].entityUrn
                == "urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod)"
            )

            assert mock_emitter.method_calls[2].args[0].aspectName == "globalTags"
            assert (
                mock_emitter.method_calls[2].args[0].entityUrn
                == "urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod)"
            )

            assert mock_emitter.method_calls[3].args[0].aspectName == "dataJobInfo"
            assert (
                mock_emitter.method_calls[3].args[0].entityUrn
                == "urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task2)"
            )

            assert (
                mock_emitter.method_calls[4].args[0].aspectName == "dataJobInputOutput"
            )
            assert (
                mock_emitter.method_calls[4].args[0].entityUrn
                == "urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task2)"
            )
            assert (
                mock_emitter.method_calls[4].args[0].aspect.inputDatajobs[0]
                == "urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task1_upstream)"
            )
            assert (
                mock_emitter.method_calls[4].args[0].aspect.inputDatasets[0]
                == "urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableConsumed,PROD)"
            )
            assert (
                mock_emitter.method_calls[4].args[0].aspect.outputDatasets[0]
                == "urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableProduced,PROD)"
            )

            assert mock_emitter.method_calls[5].args[0].aspectName == "status"
            assert (
                mock_emitter.method_calls[5].args[0].entityUrn
                == "urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableConsumed,PROD)"
            )

            assert mock_emitter.method_calls[6].args[0].aspectName == "status"
            assert (
                mock_emitter.method_calls[6].args[0].entityUrn
                == "urn:li:dataset:(urn:li:dataPlatform:snowflake,mydb.schema.tableProduced,PROD)"
            )

            assert mock_emitter.method_calls[7].args[0].aspectName == "ownership"
            assert (
                mock_emitter.method_calls[7].args[0].entityUrn
                == "urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task2)"
            )

            assert mock_emitter.method_calls[8].args[0].aspectName == "globalTags"
            assert (
                mock_emitter.method_calls[8].args[0].entityUrn
                == "urn:li:dataJob:(urn:li:dataFlow:(airflow,test_lineage_is_sent_to_backend,prod),task2)"
            )
```


Overlapping Code:
```
s", "outlets"],
[
pytest.param(
# Airflow 1.10.x uses a dictionary structure for inlets and outlets.
# We want the lineage backend to support this structure for backwards
# compatability reasons, so this test is not conditional.
{"datasets": [Dataset("snowflake", "mydb.schema.tableConsumed")]},
{"datasets": [Dataset("snowflake", "mydb.schema.tableProduced")]},
id="airflow-1-10-lineage-syntax",
),
pytest.param(
# Airflow 2.x also supports a flattened list for inlets and outlets.
# We want to test this capability.
[Dataset("snowflake", "mydb.schema.tableConsumed")],
[Dataset("snowflake", "mydb.schema.tableProduced")],
marks=pytest.mark.skipif(
AIRFLOW_VERSION < packaging.version.parse("2.0.0"),
reason="list-style lineage is only supported in Airflow 2.x",
),
id="airflow-2-lineage-syntax",
),
],
)
@mock.patch("datahub_provider.hooks.datahub.DatahubRestHook.make_emitter")
def test_lineage_backend(mock_emit, inlets, outlets):
DEFAULT_DATE = days_ago(2)
mock_emitter = Mock()
mock_emit.return_value = mock_emitter
# Using autospec on xcom_pull and xcom_push methods fails on Python 3.6.
with mock.patch.dict(
os.environ,
{
"AIRFLOW__LINEAGE__BACKEND": "datahub_provider.lineage.datahub.DatahubLineageBackend",
"AIRFLOW__LINEAGE__DATAHUB_CONN_ID": datahub_rest_connection_config.conn_id,
"AIRFLOW__LINEAGE__DATAHUB_KWARGS": json.dumps(
{"graceful_exceptions": False, "capture_executions": False}
),
},
), mock.patch("airflow.models.BaseOperator.xcom_pull"), mock.patch(
"airflow.models.BaseOperator.xcom_push"
), patch_airflow_connection(
datahub_rest_connection_config
):
func = mock.Mock()
func.__name__ = "foo"
dag = DAG(dag_id="test_lineage_is_sent_to_backend", start_date=DEFAULT_DATE)
with dag:
op1 = DummyOperator(
task_id="task1_upstream",
inlets=inlets,
outlets=outlets,
)
op2 = DummyOperator(
task_id="task2",
inlets=inlets,
outlets=outlets,
)
op
```
<Overlap Ratio: 0.9642857142857143>

---

--- 355 --
Question ID: d045ca13be848bf190eb3eb9ed327a8ca7bb52a5_3
Original Code:
```
def find_triangles_v004(G):
    """
    Attempt to use parallelism - is much much slower, 
    because I can't find a good variable scope for adj.
    """
    nodes = [u for u in G]
    node2deg = {u: len(G[u]) for u in G}
    node2deg = {u: (deg, i) for i, (u, deg) in enumerate(sorted(node2deg.items(), key=lambda x: x[1]))}
    adj = {} # adj[u] neighbours of u with higher degree
    for u in node2deg:
        d = node2deg[u]
        adj[u] = {v for v in G[u] if node2deg[v] > d}

    adjs = [(adj[u], adj) for u in G]

    with Pool(4) as p:
        return sum(p.map(u2ntriangles, adjs))
```


Overlapping Code:
```
mpt to use parallelism - is much much slower, 
because I can't find a good variable scope for adj.
"""
nodes = [u for u in G]
node2deg = {u: len(G[u]) for u in G}
node2deg = {u: (deg, i) for i, (u, deg) in enumerate(sorted(node2deg.items(), key=lambda x: x[1]))}
adj = {} # adj[u] neighbours of u with higher degree
for u in node2deg:
d = node2deg[u]
adj[u] = {v for v in G[u] if node2deg[v] > d}
adjs = [(adj[u], adj) for u in G]
with Pool(4) as p:

```
<Overlap Ratio: 0.8604206500956023>

---

--- 356 --
Question ID: c28d41e68dc39189efb6db08756cc4158587d229_2
Original Code:
```
@app.route('/frame', methods=['POST','GET'])
def frame():
    # os.system('python iPhoneFrame.py')
    os.chdir(app.root_path)
    import iPhoneFrame
    outputpath = os.path.join(app.root_path, 'output', 'image.png') 
    if os.path.exists(outputpath):
        print('exists')
        return render_template('frame.html')
    else:
        return redirect(url_for('upload'))
```


Overlapping Code:
```
.route('/frame', methods=['POST','GET'])
def frame():
# os.system('python iPhoneFrame.py')
os.chdir(app.root_path)
import iPhoneFrame
outputpath = os.path.join(app.root_path, 'output', 'image.png') 
if os.path.exists(outputpath):
print('exists')
return render_template('frame.html')
else:
return redi
```
<Overlap Ratio: 0.9174311926605505>

---

--- 357 --
Question ID: 53196e29960a327c8c1ad1da91ce1840a4fd68bd_3
Original Code:
```
def leave_comment(api, urn, pr, body):
    path = "/repos/{urn}/issues/{pr}/comments".format(urn=urn, pr=pr)
    data = {"body": body}
    resp = api("post", path, json=data)
    return resp
```


Overlapping Code:
```
def leave_comment(api, urn, pr, body):
path = "/repos/{urn}/issues/{pr}/comments".format(urn=urn, pr=pr)
data = {"body": body}
resp = api("post", path, json=data)
retu
```
<Overlap Ratio: 0.9597701149425287>

---

--- 358 --
Question ID: 8769e337c40a0cdc510c62136493b4463c60c833_0
Original Code:
```
def power(verbose, config=None, filename=None):
    if config == None:
        if filename == None:
            print("config needed")
            exit(1)
        else:
            config = load_config(filename)

    clock_power = clock(verbose,config)
    logic_power = logic(verbose, config)
    BRAM_power = BRAM(verbose, config)
    DSP_power = DSP(verbose, config)
    static_power = static(verbose, config)
    
    return round(clock_power + BRAM_power + DSP_power + logic_power + static_power, 3), clock_power, logic_power, BRAM_power, DSP_power, static_power
```


Overlapping Code:
```
r(verbose, config=None, filename=None):
if config == None:
if filename == None:
print("config needed")
exit(1)
else:
config = load_config(filename)
clock_power = clock(verbose,config)
logic_power = logic(verbose, config)
BRAM_power = BRAM(verbose, config)
DSP_power = DSP(verbose, config)
static_power = static(verbose, config)

return round(clock_power + BRAM_power + DSP_power + logic_power + static_power, 3), clock_power, logic_power, BRAM_power,
```
<Overlap Ratio: 0.9336099585062241>

---

--- 359 --
Question ID: 7ca833a58757ca3c989bc7d016be73824d7526c6_6
Original Code:
```
def get_all_text_(node):
    if node.text is not None:
        text = node.text
    else:
        text = ''
    for child in node:
        if child.tail is not None:
            text += child.tail
    return text
```


Overlapping Code:
```
def get_all_text_(node):
if node.text is not None:
text = node.text
else:
text = ''
for child in node:
if child.tail is not None:
text += child.tail
return text
```
<Overlap Ratio: 1.0>

---

--- 360 --
Question ID: 2f5ecad524d0197183b72ab76a60c9f291d1a06f_0
Original Code:
```
def load_base_config():
    ''' Define initial config '''
    for key in CONFIG.keys():
        if key.upper() in os.environ:
            CONFIG[key.lower()] = os.getenv(key.upper())
        else:
            print("ERROR: You need to set {} as environment variable or .env entry".format(key.upper()))
            exit(1)

    CONFIG['session_id'] = str(uuid.uuid4())
    CONFIG['oauth_token'] = get_oauth_token()
```


Overlapping Code:
```
oad_base_config():
''' Define initial config '''
for key in CONFIG.keys():
if key.upper() in os.environ:
CONFIG[key.lower()] = os.getenv(key.upper())
else:
print("ERROR: You need to set {} as environment variable or .env entry".format(key.upper()))
exit(1)
CONFIG['session_id'] = str(uuid.uuid4())
CONFIG['oauth_token'] = get_oauth_tok
```
<Overlap Ratio: 0.9738372093023255>

---

--- 361 --
Question ID: 42067e8baf858341c173b42c0f164a10dc6a1734_3
Original Code:
```
def bitmask_from_text(mask, text):
    """Initialize a bitmask from text.

    Builds an integer value from text containing bit names that should be set. The
    complement of :func:`decode_bitmask`. For example::

        >>> COLORS = define_bitmask('COLORS','Primary colors',RED=0,BLUE=1,GREEN=4)
        >>> '{0:b}'.format(bitmask_from_text(COLORS,'GREEN|BLUE'))
        '10010'

    Args:
        mask: A bitmask type, normally created with :func:`create_bitmask`, that defines
            the symbolic bit names that are allowed.
        text: A list of bit names separated by '|'.

    Returns:
        int: Integer with bits set for each bit name appearing in the text.

    Raises:
        ValueError: invalid text specification.
    """
    if not hasattr(mask, '__dict__'):
        raise ValueError('Invalid bitmask.')
    value = int(0)
    for bit_name in text.split('|'):
        if bit_name not in mask.__dict__:
            raise ValueError('Invalid bit name: {0}.'.format(bit_name))
        value = value | mask.__dict__[bit_name]
    return value
```


Overlapping Code:
```
tmask_from_text(mask, text):
"""Initialize a bitmask from text.
Builds an integer value from text containing bit names that should be set. The
complement of :func:`decode_bitmask`. For example::
>>> COLORS = define_bitmask('COLORS','Primary colors',RED=0,BLUE=1,GREEN=4)
>>> '{0:b}'.format(bitmask_from_text(COLORS,'GREEN|BLUE'))
'10010'
Args:
mask: A bitmask type, normally created with :func:`create_bitmask`, that defines
the symbolic bit names that are allowed.
text: A list of bit names separated by '|'.
Returns:
int: Integer with bits set for each bit name appearing in the text.
Raises:
ValueError: invalid text specification.
"""
if not hasattr(mask, '__dict__'):
raise ValueError('Invalid bitmask.')
value = int(0)
for bit_name in text.split('|'):
if bit_name not in mask.__dict__:
raise ValueError('Invalid bit name: {0}.'.format(bit_name))
value = value | mask.__dict__[bit_name]
return v
```
<Overlap Ratio: 0.989010989010989>

---

--- 362 --
Question ID: fa7225e5d07b9d807a5cf5efa399a86caa12943a_2
Original Code:
```
async def vk(user_id: str) -> ParsedData:
    page_link = _get_page_link(user_id)

    page = BeautifulSoup(await get_page(page_link), features="html.parser")

    if page.find("img", {"src": "/images/pics/spamfight.gif"}) or (
            page.find("div", {"class": "message_page_title"}) and page.title == "Information"):
        logger.debug(f"Bad user {page_link}")
        raise BadID

    image_element = page.find("img", {"class": "page_avatar_img"})

    if image_element is None or "alt" not in image_element.attrs:
        logger.debug("Группа или недоступная страница")
        raise BadID

    user_name = image_element["alt"]
    image: bytes

    try:
        image = await _get_quality_photo(page)
    except Exception as e:
        logger.debug(f"Failed to fetch hires image for {page_link}")
        image = await get_file(image_element["src"])

    return ParsedData(face=image, traits={"name": [user_name], "vk_url": [page_link]})
```


Overlapping Code:
```
d: str) -> ParsedData:
page_link = _get_page_link(user_id)
page = BeautifulSoup(await get_page(page_link), features="html.parser")
if page.find("img", {"src": "/images/pics/spamfight.gif"}) or (
page.find("div", {"class": "message_page_title"}) and page.title == "Information"):
logger.debug(f"Bad user {page_link}")
raise BadID
image_element = page.find("img", {"class": "page_avatar_img"})
if image_element is None or "alt" not in image_element.attrs:
logger.debug("Группа или недоступная страница")
raise BadID
user_name = image_element["alt"]
image: bytes
try:
image = await _get_quality_photo(page)
except Exception as e:
logger.debug(f"Failed to fetch hires image for {page_link}")
image = await get_file(image_element["src"])
return ParsedData(face=image, traits={"name": [user_name], "vk_url"
```
<Overlap Ratio: 0.9592326139088729>

---

--- 363 --
Question ID: b3d7f177513de669b8b68077fe962eb68f228b7f_8
Original Code:
```
def align_session(session, audiopath, outpath, chans=None):
    """Align all channels within a given session."""
    chime_data = tu.chime_data()

    # The first binaural recorder is taken as the reference
    ref_chan = chime_data[session]['pids'][0]

    # If chans not specified then use all channels available
    if chans is None:  
        pids = chime_data[session]['pids']
        kinects = chime_data[session]['kinects']
        chans = pids[1:] + kinects

    all_results = dict()  # Empty dictionary for storing results

    for target_chan in chans:
        print(target_chan)

        # For dealing with channels with big missing audio segments
        missing = None
        if (('missing' in chime_data[session] and
             target_chan in chime_data[session]['missing'])):
            missing = chime_data[session]['missing'][target_chan]

        # Parameters for alignment depend on whether target is
        # a binaural mic ('P') or a kinect mic
        if target_chan[0] == 'P':
            search_duration = BINAURAL_SEARCH_DURATION
            template_duration = BINAURAL_TEMPLATE_DURATION
            alignment_resolution = BINAURAL_RESOLUTION
            target_chan_name = target_chan
        else:
            search_duration = KINECT_SEARCH_DURATION
            template_duration = KINECT_TEMPLATE_DURATION
            alignment_resolution = KINECT_RESOLUTION
            target_chan_name = target_chan + '.CH1'

        # Place it try-except block so that can continue
        # if a channel fails. This shouldn't happen unless
        # there is some problem reading the audio data.
        try:
            offset = 0
            if missing is not None:
                _, offset = missing

            ref_fn = f'{audiopath}/{session}_{ref_chan}.wav'
            target_fn = f'{audiopath}/{session}_{target_chan_name}.wav'

            # Will analyse the alignment offset at regular intervals
            session_duration = int(min(wavfile_duration(ref_fn) - offset,
                                   wavfile_duration(target_fn))
                                   - template_duration - search_duration)
            analysis_times = range(alignment_resolution, session_duration, alignment_resolution)

            # Run the alignment code and store results in dictionary
            all_results[target_chan] = \
                align_channels(ref_fn, 
                               target_fn,
                               analysis_times, 
                               search_duration,
                               template_duration, 
                               missing=missing)
        except:
            traceback.print_exc()

    pickle.dump(all_results, open(f'{outpath}/align.{session}.p', "wb"))
```


Overlapping Code:
```
udiopath, outpath, chans=None):
"""Align all channels within a given session."""
chime_data = tu.chime_data()
# The first binaural recorder is taken as the reference
ref_chan = chime_data[session]['pids'][0]
# If chans not specified then use all channels available
if chans is None: 
pids = chime_data[session]['pids']
kinects = chime_data[session]['kinects']
chans = pids[1:] + kinects
all_results = dict() # Empty dictionary for storing results
for target_chan in chans:
print(target_chan)
# For dealing with channels with big missing audio segments
missing = None
if (('missing' in chime_data[session] and
target_chan in chime_data[session]['missing'])):
missing = chime_data[session]['missing'][target_chan]
# Parameters for alignment depend on whether target is
# a binaural mic ('P') or a kinect mic
if target_chan[0] == 'P':
search_duration = BINAURAL_SEARCH_DURATION
template_duration = BINAURAL_TEMPLATE_DURATION
alignment_resolution = BINAURAL_RESOLUTION
target_chan_name = target_chan
else:
search_duration = KINECT_SEARCH_DURATION
template_duration = KINECT_TEMPLATE_DURATION
alignment_resolution = KINECT_RESOLUTION
target_chan_name = target_chan + '.CH1'
# Place it try-except block so that can continue
# if a channel fails. This shouldn't happen unless
# there is some problem reading the audio data.
try:
offset = 0
if missing is not None:
_, offset = missing
ref_fn = f'{audiopath}/{session}_{ref_chan}.wav'
target_fn = f'{audiopath}/{session}_{target_chan_name}.wav'
# Will analyse the alignment offset at regular intervals
session_duration = int(min(wavfile_duration(ref_fn) - offset,
wavfile_duration(target_fn))
- template_duration - search_duration)
analysis_times = range(alignment_resolution, session_duration, alignment_resolution)
# Run the alignment code and store results in dictionary
all_results[target_chan] = \
align_
```
<Overlap Ratio: 0.9625390218522373>

---

--- 364 --
Question ID: cb9ecbe044f947ca511e578919296e63703a76e7_16
Original Code:
```
def concat(table1, table2):
    table1 = database[table1]
    table2 = database[table2]
    start_time = time.time()
    header1 = [i for i in table1[0]]
    header2 = [i for i in table2[0]]
    if set(header1) != set(header2):
        print("Error: two tables don't share the same schema!")
        return 
    end_time = time.time()
    print("Time elapsed for concatenation is " + str(end_time - start_time) + " sec.")
    print("lines: " + str(len(table1 + table2[1:])))
    return table1 + table2[1:]
```


Overlapping Code:
```
le1, table2):
table1 = database[table1]
table2 = database[table2]
start_time = time.time()
header1 = [i for i in table1[0]]
header2 = [i for i in table2[0]]
if set(header1) != set(header2):
print("Error: two tables don't share the same schema!")
return 
end_time = time.time()
print("Time elapsed for concatenation is " + str(end_time - start_time) + " sec.")
print("lines: " + str(len(table1 + table
```
<Overlap Ratio: 0.89086859688196>

---

--- 365 --
Question ID: 2a8da1237d3594a5b0825382e5e9eee1aee55e77_26
Original Code:
```
@pytest.mark.parametrize(
    "endpoint",
    [
        "http://1.2.3.4",
        "http://1.2.3.4:8080",
        "http://host.com",
        "http://host.com:8080",
    ],
)
def test_external_endpoint_validation_valid(hooks_config, endpoint):
    """Test the validation of the external endpoint used in the "complete" hook with
    URL that are valid.
    """
    config_dict = hooks_config.serialize()

    config_dict["complete"]["external_endpoint"] = endpoint
    HooksConfiguration.deserialize(config_dict)
```


Overlapping Code:
```
.2.3.4",
"http://1.2.3.4:8080",
"http://host.com",
"http://host.com:8080",
],
)
def test_external_endpoint_validation_valid(hooks_config, endpoint):
"""Test the validation of the external endpoint used in the "complete" hook with
URL that are valid.
"""
config_dict = hooks_config.serialize()
config_dict["complete"]["external_endpoint"] = endpoint
H
```
<Overlap Ratio: 0.7936507936507936>

---

--- 366 --
Question ID: dc12eb19ca637e291bc5f601e2db96e7671e7103_16
Original Code:
```
@app.route('/My_Cart')
def my_cart():
    username = request.args.get('username')
    products, cnt, cart_cost = my_items_in_cart(username) # gets all the products in cart of this current customer
    return render_template('mycart.html', products=products, cnt=cnt, username=username, cart_cost=cart_cost)
```


Overlapping Code:
```
rname = request.args.get('username')
products, cnt, cart_cost = my_items_in_cart(username) # gets all the products in cart of this current customer
return render_template('mycart.html', products=products, cnt=cnt, username=username, cart_cost=cart_co
```
<Overlap Ratio: 0.8503401360544217>

---

--- 367 --
Question ID: 32bec60d9a52bbdd54f6335566d470bc05e2e7aa_0
Original Code:
```
def execute():
    print("Initializing Stock Settings Custom Fields")
    doc = frappe.get_doc("Stock Settings")
    if not doc.get("cogs_on_invoice"):
        doc.cogs_on_invoice = 1
    doc.save()
```


Overlapping Code:
```
tings Custom Fields")
doc = frappe.get_doc("Stock Settings")
if not doc.get("cogs_on_invoice"):
doc.
```
<Overlap Ratio: 0.5747126436781609>

---

--- 368 --
Question ID: fb45c17955be14892f31d632e1f120bae095d667_1
Original Code:
```
def Evaluate(context, string):
    """
    The dyn:evaluate function evaluates a string as an XPath expression and
    returns the resulting value, which might be a boolean, number, string,
    node set, result tree fragment or external object. The sole argument is
    the string to be evaluated. If the string is an invalid XPath expression,
    an empty node-set is returned.

    http://www.exslt.org/dyn/functions/evaluate/index.html
    """
    string = Conversions.StringValue(string)
    p = parser.new()
    try:
        result = p.parse(string).evaluate(context)
    except SyntaxError:
        tb = handle_traceback()
        msg = 'Syntax error in XPath "%s", masked by empty node set return:\n%s' % (string, tb.getvalue())
        context.processor.warning(msg)
        result = []
    except:
        import traceback
        traceback.print_exc()
        result = []
    return result
```


Overlapping Code:
```
string):
"""
The dyn:evaluate function evaluates a string as an XPath expression and
returns the resulting value, which might be a boolean, number, string,
node set, result tree fragment or external object. The sole argument is
the string to be evaluated. If the string is an invalid XPath expression,
an empty node-set is returned.
http://www.exslt.org/dyn/functions/evaluate/index.html
"""
string = Conversions.StringValue(string)
p = parser.new()
try:
result = p.parse(string).evaluate(context)
except SyntaxError:
tb = handle_traceback()
msg = 'Syntax error in XPath "%s", masked by empty node set return:\n%s' % (string, tb.getvalue())
context.processor.warning(msg)
result = []
except:
import traceback
traceback.print_exc()
result = []
return 
```
<Overlap Ratio: 0.9640102827763496>

---

--- 369 --
Question ID: 750c4afc515bd1464204757357d9abb97ef72e1f_0
Original Code:
```
def prog_bar(total, done, present="In Progress", past="complete"):
    ratio = int((done / total) * 40)
    black = ratio * "■"
    white = (40 - ratio) * " "
    print(f"{present}... {black}{white}| {done}/{total} {past}", flush=True, end="\r")
```


Overlapping Code:
```
total, done, present="In Progress", past="complete"):
ratio = int((done / total) * 40)
black = ratio * "■"
white = (40 - ratio) * " "
print(f"{present}... {black}{white}| {done}/{total} {past}", flush
```
<Overlap Ratio: 0.8733624454148472>

---

--- 370 --
Question ID: aa9c59ada03bc4b765d0b0464fdd75b2a97d8859_0
Original Code:
```
def generate_ios_json():
    result = []

    for key, value in ios_icon_mapping.items():
        for multiplier in value:
            effective_size = int(float(key) * float(multiplier))
            result.append({
                "absoluteSize": effective_size,
                "fileFormat": "png",
                "name": f'_{key}@{multiplier}x',
                "namingScheme": 0,
                "scale": 0,
                "visibleScaleType": 1
            })
    return [{
        "name": "iOS App Icon (noahgilmore.com)",
        "shouldApplyAutomatically": True,
        "exportFormats": result
    }]
```


Overlapping Code:
```
ate_ios_json():
result = []
for key, value in ios_icon_mapping.items():
for multiplier in value:
effective_size = int(float(key) * float(multiplier))
result.append({
"absoluteSize": effective_size,
"fileFormat": "png",
"name": f'_{key}@{multiplier}x',
"namingScheme": 0,
"scale": 0,
"visibleScaleType": 1
})
return [{
"name": "iOS App Icon (noahgilmore.com)",
"shouldApplyAutomatically": True,
"expor
```
<Overlap Ratio: 0.9324009324009324>

---

--- 371 --
Question ID: db3e020cf0be41f2bc94999eecbb7411fe3a784b_1
Original Code:
```
@bot.command()
async def bestGirl(ctx, arx: str, arx2: str):
    """Tells who best girl is"""
    random.seed()
    if random.randint(1, 100) < 50:
        await ctx.send(arx)
    else:
        await ctx.send(arx2)
```


Overlapping Code:
```
tx, arx: str, arx2: str):
"""Tells who best girl is"""
random.seed()
if random.randint(1, 100) < 50:
```
<Overlap Ratio: 0.5494505494505495>

---

--- 372 --
Question ID: b0fa61e7cbf4fc40d49289abb65d59ba663994d7_0
Original Code:
```
def create_tables():

    ''' Creates the table if it doesn't exists already.'''

    try:

        # Stablishes connection with our db



        connection = psycopg2.connect(user=os.environ.get('db_user'),
                                    password=os.environ.get('db_password'),
                                    host=os.environ.get('db_host'),
                                    port=os.environ.get('db_port'),
                                    database=os.environ.get('db_name'))



        # Create the cursor.

        cursor = connection.cursor()


        query_country_table='''
                            CREATE TABLE IF NOT EXISTS countries (
                                id SERIAL NOT NULL,
                                country_code VARCHAR(3) NOT NULL UNIQUE,
                                country_name CHAR(99),
                                PRIMARY KEY(country_code)
                            );
        '''

        cursor.execute(query_country_table)
        connection.commit()

        query_market_country_table= '''
                            CREATE TABLE IF NOT EXISTS markets (
                                id SERIAL,
                                market_id VARCHAR(99),
                                market_name VARCHAR(99),
                                country_code VARCHAR(3) REFERENCES countries(country_code),
                                PRIMARY KEY (market_id)
                            );
        '''

        cursor.execute(query_market_country_table)
        connection.commit()

        query_currency_table = '''
                            CREATE TABLE IF NOT EXISTS currencies (
                                id SERIAL NOT NULL,
                                currency_name VARCHAR(99),
                                currency_code VARCHAR(3) NOT NULL UNIQUE,
                                is_in_uganda BOOLEAN,
                                is_in_kenya BOOLEAN,
                                is_in_congo BOOLEAN,
                                is_in_burundi BOOLEAN,
                                is_in_tanzania BOOLEAN,
                                is_in_south_sudan BOOLEAN,
                                is_in_rwanda BOOLEAN,
                                is_in_malawi BOOLEAN,
                                PRIMARY KEY(currency_code)
                            );
        '''

        cursor.execute(query_currency_table)
        connection.commit()

        query_sources_table = '''
                            CREATE TABLE IF NOT EXISTS sources (
                                id SERIAL NOT NULL,
                                source_name VARCHAR(99) NOT NULL,
                                is_in_uganda BOOLEAN,
                                is_in_kenya BOOLEAN,
                                is_in_congo BOOLEAN,
                                is_in_burundi BOOLEAN,
                                is_in_tanzania BOOLEAN,
                                is_in_south_sudan BOOLEAN,
                                is_in_rwanda BOOLEAN,
                                is_in_malawi BOOLEAN,
                                PRIMARY KEY (id)
                            );
        '''

        cursor.execute(query_sources_table)
        connection.commit()

        query_categories_table = '''
                            CREATE TABLE IF NOT EXISTS categories (
                                id SERIAL NOT NULL,
                                category_name VARCHAR(99) UNIQUE,
                                PRIMARY KEY(id)
                            );
        '''

        cursor.execute(query_categories_table)
        connection.commit()

        # To be discused. ####

        # query_products_table = '''
        #                     CREATE TABLE IF NOT EXISTS products (
        #                         id SERIAL NOT NULL UNIQUE,
        #                         product_name VARCHAR(99) UNIQUE,
        #                         PRIMARY KEY(id, product_name)
        #                     );
        # '''

        # cursor.execute(query_products_table)
        # connection.commit()


        query_product_category_pair_table = '''
                            CREATE TABLE IF NOT EXISTS prod_cat_pair (
                                id SERIAL NOT NULL,
                                product_name VARCHAR(99) UNIQUE,
                                category_id INT REFERENCES categories(id),
                                PRIMARY KEY(product_name)
                            );
        '''

        cursor.execute(query_product_category_pair_table)
        connection.commit()

        query_product_raw_info_table = '''
                            CREATE TABLE IF NOT EXISTS product_raw_info (
                                product_name VARCHAR(99) REFERENCES prod_cat_pair(product_name),
                                market_id VARCHAR(99) REFERENCES markets(market_id),
                                unit_scale VARCHAR(32),
                                source_id INT REFERENCES sources(id),
                                currency_code VARCHAR(3) REFERENCES currencies(currency_code),
                                date_price DATE,
                                retail_observed_price float4,
                                wholesale_observed_price float4
                            );
        '''

        cursor.execute(query_product_raw_info_table)
        connection.commit()


        query_product_clean_retail_info_table = '''
                            CREATE TABLE IF NOT EXISTS product_clean_retail_info (
                                product_name VARCHAR(99) REFERENCES prod_cat_pair(product_name),
                                market_id VARCHAR(99) REFERENCES markets(market_id),
                                source_id INT REFERENCES sources(id),
                                currency_code VARCHAR(3) REFERENCES currencies(currency_code),
                                date_price DATE,
                                observed_price float4,
                                observed_class VARCHAR(9),
                                forecasted_price_1 float4,
                                forecasted_class_1 VARCHAR(9),
                                forecasted_price_2 float4,
                                forecasted_class_2 VARCHAR(9),
                                forecasted_price_3 float4,
                                forecasted_class_3 VARCHAR(9),
                                forecasted_price_4 float4,
                                forecasted_class_4 VARCHAR(9),
                                used_model VARCHAR(99),
                                date_run_model DATE,
                                normal_band_limit float8,
                                stress_band_limit float8,
                                alert_band_limit float8,
                                stressness float8
                            );
        '''

        cursor.execute(query_product_clean_retail_info_table)
        connection.commit()

        query_product_clean_wholesale_info_table = '''
                            CREATE TABLE IF NOT EXISTS product_clean_wholesale_info (
                                product_name VARCHAR(99) REFERENCES prod_cat_pair(product_name),
                                market_id VARCHAR(99) REFERENCES markets(market_id),
                                source_id INT REFERENCES sources(id),
                                currency_code VARCHAR(3) REFERENCES currencies(currency_code),
                                date_price DATE,
                                observed_price float4,
                                observed_class VARCHAR(9),
                                forecasted_price_1 float4,
                                forecasted_class_1 VARCHAR(9),
                                forecasted_price_2 float4,
                                forecasted_class_2 VARCHAR(9),
                                forecasted_price_3 float4,
                                forecasted_class_3 VARCHAR(9),
                                forecasted_price_4 float4,
                                forecasted_class_4 VARCHAR(9),
                                used_model VARCHAR(99),
                                date_run_model DATE,
                                normal_band_limit float8,
                                stress_band_limit float8,
                                alert_band_limit float8,
                                stressness float8
                            );
        '''

        cursor.execute(query_product_clean_wholesale_info_table)
        connection.commit()

        return 'Success'

    except (Exception, psycopg2.Error) as error:
        print('Error verifying or creating the table.')

    finally:

        if (connection):
            cursor.close()
            connection.close()
```


Overlapping Code:
```
te_tables():
''' Creates the table if it doesn't exists already.'''
try:
# Stablishes connection with our db
connection = psycopg2.connect(user=os.environ.get('db_user'),
password=os.environ.get('db_password'),
host=os.environ.get('db_host'),
port=os.environ.get('db_port'),
database=os.environ.get('db_name'))
# Create the cursor.
cursor = connection.cursor()
query_country_table='''
CREATE TABLE IF NOT EXISTS countries (
id SERIAL NOT NULL,
country_code VARCHAR(3) NOT NULL UNIQUE,
country_name CHAR(99),
PRIMARY KEY(country_code)
);
'''
cursor.execute(query_country_table)
connection.commit()
query_market_country_table= '''
CREATE TABLE IF NOT EXISTS markets (
id SERIAL,
market_id VARCHAR(99),
market_name VARCHAR(99),
country_code VARCHAR(3) REFERENCES countries(country_code),
PRIMARY KEY (market_id)
);
'''
cursor.execute(query_market_country_table)
connection.commit()
query_currency_table = '''
CREATE TABLE IF NOT EXISTS currencies (
id SERIAL NOT NULL,
currency_name VARCHAR(99),
currency_code VARCHAR(3) NOT NULL UNIQUE,
is_in_uganda BOOLEAN,
is_in_kenya BOOLEAN,
is_in_congo BOOLEAN,
is_in_burundi BOOLEAN,
is_in_tanzania BOOLEAN,
is_in_south_sudan BOOLEAN,
is_in_rwanda BOOLEAN,
is_in_malawi BOOLEAN,
PRIMARY KEY(currency_code)
);
'''
cursor.execute(query_currency_table)
connection.commit()
query_sources_table = '
```
<Overlap Ratio: 0.9925428784489188>

---

--- 373 --
Question ID: fb7e7f906e0c564022344bbeb3b0342b50ea10b7_0
Original Code:
```
def voice():
  gsp = gspeech2.Gspeech()
  while True:
      stt = gsp.getText()
      if stt is None:
         break
      print(stt)
      time.sleep(0.01)
      if ('좌' in stt):
        return 'a'
        break
      if ('우' in stt):
        return 'd'
        break
      if ('하' in stt):
        return 's'
        break
      if ('끝' in stt):
        return ' '
        break
      if ('빨강' in stt):
        return 'r'
        break
      if ('노랑' in stt):
        return 'o'
        break
      if ('초록' in stt):
        return 'g'
        break
      if ('지워' in stt):
        return 'e'
        break
      if ('열기' in stt):
        return 'o'
        break
      if ('힌트' in stt):
        return 'h'
        break
```


Overlapping Code:
```
):
gsp = gspeech2.Gspeech()
while True:
stt = gsp.getText()
if stt is None:
break
print(stt)
time.sleep(0.01)
if ('좌' in stt):
return 'a'
break
if ('우' in stt):
return 'd'
break
if ('하' in stt):
return 's'
break
if ('끝' in stt):
return ' '
break
if ('빨강' in stt):
return 'r'
break
if ('노랑' in stt):
return 'o'
break
if ('초록' in stt):
return 'g'
break
if ('지워' in stt):
return 'e'
break
if ('열기' in stt):
return 'o'
break
if ('힌트' in stt):
return 'h'

```
<Overlap Ratio: 0.967741935483871>

---

--- 374 --
Question ID: 26281d1a85f723050fb16e7013598656c1fc13ee_2
Original Code:
```
def recieverStatus():
    ser = openConnection()

    # send random (but existing!) code end evaluate response to get power state
    ser.write(formatCommand("07eb1"))
    response = ser.read(200)
    if b'\x02002B01\x03' in response:
        return True
    elif b'\x02310002\x03' in response:
        return False    
    else:
        raise("ERROR: Not recieved any response")
```


Overlapping Code:
```
= openConnection()
# send random (but existing!) code end evaluate response to get power state
ser.write(formatCommand("07eb1"))
response = ser.read(200)
if b'\x02002B01\x03' in response:
return True
elif b'\x02310002\x03' in response:
return False 

```
<Overlap Ratio: 0.7739938080495357>

---

--- 375 --
Question ID: ebcf8d7d946847179d73bc39dd12acec145d722b_2
Original Code:
```
def create_cog(cog_name: str, *, allow_erasing: bool = False):
    if not cog_name.isidentifier():
        print("{!r} is not a valid identifier, not creating.".format(cog_name))
        return
    elif keyword.iskeyword(cog_name.lower()) or cog_name.lower() in ["async", "await"]:
        print("{!r} is a reserved Python keyword, not creating.".format(cog_name))
        return

    cog_dir = root / cog_name.lower()

    if cog_dir.exists():
        if not allow_erasing and (
            not input(
                "Please confirm that you wish to overwrite the following "
                "directory (y/N): '{}'\nThis will irreversibly remove this directory, and all "
                "files and/or directories contained within."
                "\n".format(cog_dir)
            )
            .lower()
            .startswith("y")
        ):
            print("Not overwriting.")
            return

        print("Removing directory '{}' and all it's contents".format(cog_dir))
        shutil.rmtree(str(cog_dir))

    cog_dir = mkdir(cog_name.lower())
    mkdir("locales", root_dir=cog_dir)

    info = deepcopy(template_info)
    info["tags"] = args.tag or []
    info["short"] = args.description if args.short is ... else args.short
    info["description"] = args.description
    info = json.dumps(info, indent=2)

    write_file(
        cog_dir, "__init__.py", template_init.format(cog_lower=cog_name.lower(), cog=cog_name)
    )
    write_file(
        cog_dir,
        "{}.py".format(cog_name.lower()),
        template_cog.format(cog=cog_name, randint=random.randint(1000, 10000000)),
    )
    write_file(cog_dir, "info.json", info)
```


Overlapping Code:
```
low_erasing: bool = False):
if not cog_name.isidentifier():
print("{!r} is not a valid identifier, not creating.".format(cog_name))
return
elif keyword.iskeyword(cog_name.lower()) or cog_name.lower() in ["async", "await"]:
print("{!r} is a reserved Python keyword, not creating.".format(cog_name))
return
cog_dir = root / cog_name.lower()
if cog_dir.exists():
if not allow_erasing and (
not input(
"Please confirm that you wish to overwrite the following "
"directory (y/N): '{}'\nThis will irreversibly remove this directory, and all "
"files and/or directories contained within."
"\n".format(cog_dir)
)
.lower()
.startswith("y")
):
print("Not overwriting.")
return
print("Removing directory '{}' and all it's contents".format(cog_dir))
shutil.rmtree(str(cog_dir))
cog_dir = mkdir(cog_name.lower())
mkdir("locales", root_dir=cog_dir)
info = deepcopy(template_info)
info["tags"] = args.tag or []
info["short"] = args.description if args.short is ... else args.short
info["description"] = args.description
info = json.dumps(info, indent=2)
write_file(
cog_dir, "__init__.py", template_init.format(cog_lower=cog_name.lower(), cog=cog_name)
)
write_file(
cog_dir,
"{}.py".format(cog_name.lower()),
template_cog.format(cog=cog_name, randint=random.randint(1000, 10000000)),
)
write_file(cog_dir, "info.js
```
<Overlap Ratio: 0.966542750929368>

---

--- 376 --
Question ID: 50e165c10837556e835d32bc880e3e8dc4d29c62_0
Original Code:
```
def secant(a, b, E, n):
    k = 0
    while k < n:
        t_x = ((a * f(b)) - (b * f(a))) \
        / (f(b) - f(a))
        if abs(b-a) < E or abs(f(b)) < E:
            print ("Raiz =  {}".format(t_x))
            break
        a = b
        b = t_x
        
        k += 1
```


Overlapping Code:
```
def secant(a, b, E, n):
k = 0
while k < n:
t_x = ((a * f(b)) - (b * f(a))) \
/ (f(b) - f(a))
if abs(b-a) < E or abs(f(b)) < E:
print ("Raiz = {}".form
```
<Overlap Ratio: 0.8064516129032258>

---

--- 377 --
Question ID: 68b18f9cc116cc175112bf5e4f150f5fcec6e3e6_2
Original Code:
```
def evaluate(model: nn.Module,
             data: MolPairDataset,
             loss_func: Callable,
             num_tasks: int,
             metric_func: Callable,
             batch_size: int,
             dataset_type: str,
             scaler: StandardScaler = None,
             logger: logging.Logger = None) -> List[float]:
    """
    Evaluates an ensemble of models on a dataset.

    :param model: A model.
    :param data: A MolPairDataset.
    :param loss_func: Loss function.
    :param num_tasks: Number of tasks.
    :param metric_func: Metric function which takes in a list of targets and a list of predictions.
    :param batch_size: Batch size.
    :param dataset_type: Dataset type.
    :param scaler: A StandardScaler object fit on the training targets.
    :param logger: Logger.
    :return: A list with the score for each task based on `metric_func`.
    """
    loss = val_loss(model, data, loss_func, batch_size, dataset_type, scaler)

    preds = predict(
        model=model,
        data=data,
        batch_size=batch_size,
        scaler=scaler
    )

    targets = data.targets()

    results = evaluate_predictions(
        preds=preds,
        targets=targets,
        num_tasks=num_tasks,
        metric_func=metric_func,
        dataset_type=dataset_type,
        logger=logger
    )

    return results, loss
```


Overlapping Code:
```
f evaluate(model: nn.Module,
data: MolPairDataset,
loss_func: Callable,
num_tasks: int,
metric_func: Callable,
batch_size: int,
dataset_type: str,
scaler: StandardScaler = None,
logger: logging.Logger = None) -> List[float]:
"""
Evaluates an ensemble of models on a dataset.
:param model: A model.
:param data: A MolPairDataset.
:param loss_func: Loss function.
:param num_tasks: Number of tasks.
:param metric_func: Metric function which takes in a list of targets and a list of predictions.
:param batch_size: Batch size.
:param dataset_type: Dataset type.
:param scaler: A StandardScaler object fit on the training targets.
:param logger: Logger.
:return: A list with the score for each task based on `metric_func`.
"""
loss = val_loss(model, data, loss_func, batch_size, dataset_type, scaler)
preds = predict(
model=model,
data=data,
batch_size=batch_size,
scaler=scaler
)
targets = data.targets()
results = evaluate_predictions(
preds=preds,
targets=targets,
num_tasks=num_tasks,
metric_func=metric_func,
dataset_type=dataset_type,
logger=logger
)
return 
```
<Overlap Ratio: 0.986046511627907>

---

--- 378 --
Question ID: 2dbffca75bd9e6751adc440df79bb9eb9723f603_0
Original Code:
```
def swap(l, idx1, idx2):
    temp = l[idx1]
    l[idx1] = l[idx2]
    l[idx2] = temp
```


Overlapping Code:
```
x1, idx2):
temp = l[idx1]
l[idx1] = l[idx2]
l[idx2
```
<Overlap Ratio: 0.6944444444444444>

---

--- 379 --
Question ID: 317e5e31316e3a1b047cdee48b3a29e36a75588a_1
Original Code:
```
def check_collision(players, balls):
    """
    checks if any of the player have collided with any of the balls

    :param players: a dictonary of players
    :param balls: a list of balls
    :return: None
    """
    to_delete = []
    for player in players:
        p = players[player]
        x = p["x"]
        y = p["y"]
        for ball in balls:
            bx = ball[0]
            by = ball[1]

            dis = math.sqrt((x - bx) ** 2 + (y - by) ** 2)
            if dis <= START_RADIUS + p["score"]:
                p["score"] = p["score"] + 0.5
                balls.remove(ball)
```


Overlapping Code:
```
ef check_collision(players, balls):
"""
checks if any of the player have collided with any of the balls
:param players: a dictonary of players
:param balls: a list of balls
:return: None
"""
to_delete = []
for player in players:
p = players[player]
x = p["x"]
y = p["y"]
for ball in balls:
bx = ball[0]
by = ball[1]
dis = math.sqrt((x - bx) ** 2 + (y - by) ** 2)
if dis <= START_RADIUS + p["score"]:

```
<Overlap Ratio: 0.89086859688196>

---

--- 380 --
Question ID: 2d5a95d03fe34251466cc2dffbed1aac34a8714b_0
Original Code:
```
def parse_duration(duration: str, /) -> pendulum.Duration:
    duration = duration.strip()

    if not duration:
        raise ValueError('No duration provided.')

    args: dict[str, int] = {}
    digits = ''

    for char in duration:
        if char.isdigit():
            digits += char
            continue

        if char == ' ':
            if len(digits) > 0:
                raise ValueError('Invalid duration')

            continue  # pragma: no cover

        if char not in UNITS or not digits:
            raise ValueError('Invalid duration')

        args[UNITS[char]] = int(digits)
        digits = ''

    return pendulum.duration(**args)
```


Overlapping Code:
```
 str, /) -> pendulum.Duration:
duration = duration.strip()
if not duration:
raise ValueError('No duration provided.')
args: dict[str, int] = {}
digits = ''
for char in duration:
if char.isdigit():
digits += char
continue
if char == ' ':
if len(digits) > 0:
raise ValueError('Invalid duration')
continue # pragma: no cover
if char not in UNITS or not digits:
raise ValueError('Invalid duration')
args[UNITS[char]] = int(digits)
digits = ''
return pend
```
<Overlap Ratio: 0.9018036072144289>

---

--- 381 --
Question ID: 7c12bb92dca6e07d7eeebda40503fbdb25d98519_4
Original Code:
```
def ATL06_2_gdf(ATL06_fn,dataset_dict):
    """
    function to convert ATL06 hdf5 to geopandas dataframe, containing columns as passed in dataset dict
    Used Ben's ATL06_to_dict function
    """
    if ('latitude' in dataset_dict['land_ice_segments']) != True:
        dataset_dict['land_ice_segments'].append('latitude')
    if ('longitude' in dataset_dict['land_ice_segments']) != True:
        dataset_dict['land_ice_segments'].append('longitude')
    #use Ben's Scripts to convert to dict
    data_dict = ATL06_to_dict(ATL06_fn,dataset_dict)
    #this will give us 6 tracks
    i = 0
    for track in data_dict:
        #1 track
        #convert to datafrmae
        df = pd.DataFrame(track)
        df['p_b'] = str(track['pair'][0])+'_'+str(track['beam'][0])
        df['geometry'] = df.apply(point_covert,axis=1)
        if i==0:
            df_final = df.copy()
        else:
            df_final = df_final.append(df)
        i = i+1
    gdf_final = gpd.GeoDataFrame(df_final,geometry='geometry',crs={'init':'epsg:4326'})
    return gdf_final
```


Overlapping Code:
```
6_2_gdf(ATL06_fn,dataset_dict):
"""
function to convert ATL06 hdf5 to geopandas dataframe, containing columns as passed in dataset dict
Used Ben's ATL06_to_dict function
"""
if ('latitude' in dataset_dict['land_ice_segments']) != True:
dataset_dict['land_ice_segments'].append('latitude')
if ('longitude' in dataset_dict['land_ice_segments']) != True:
dataset_dict['land_ice_segments'].append('longitude')
#use Ben's Scripts to convert to dict
data_dict = ATL06_to_dict(ATL06_fn,dataset_dict)
#this will give us 6 tracks
i = 0
for track in data_dict:
#1 track
#convert to datafrmae
df = pd.DataFrame(track)
df['p_b'] = str(track['pair'][0])+'_'+str(track['beam'][0])
df['geometry'] = df.apply(point_covert,axis=1)
if i==0:
df_final = df.copy()
else:
df_final = df_final.append(df)
i = i+1
gdf_final = gpd.GeoDataFrame(df_final,geometry='geometry',crs={'init':'epsg:4326'})
return g
```
<Overlap Ratio: 0.9821627647714605>

---

--- 382 --
Question ID: 212b3becef06107f0a212d0613552ab99907b0b8_1
Original Code:
```
def test_build_autorest_options():
    line = build_autorest_options({"autorest_options": {"A": "value"}}, {"autorest_options": {"B": "value value"}})
    assert line == ["--a=value", "--b='value value'"]

    line = build_autorest_options({"autorest_options": {"A": "value"}}, {"autorest_options": {"B": ["value1", "value2"]}})
    assert line == ["--a=value", "--b=value1", "--b=value2"]

    line = build_autorest_options({"autorest_options": {"A": "value"}}, {"autorest_options": {"A": "newvalue"}})
    assert line == ["--a=newvalue"]

    line = build_autorest_options({}, {})
    assert line == []

    line = build_autorest_options({"autorest_options": {"A": 12, "B": True, "C": ''}}, {})
    assert line == ["--a=12", "--b=True", "--c"]
```


Overlapping Code:
```
e = build_autorest_options({"autorest_options": {"A": "value"}}, {"autorest_options": {"B": "value value"}})
assert line == ["--a=value", "--b='value value'"]
line = build_autorest_options({"autorest_options": {"A": "value"}}, {"autorest_options": {"B": ["value1", "value2"]}})
assert line == ["--a=value", "--b=value1", "--b=value2"]
line = build_autorest_options({"autorest_options": {"A": "value"}}, {"autorest_options": {"A": "newvalue"}})
assert line == ["--a=newvalue"]
line = build_autorest_options({}, {})
assert line == []
line = build_autorest_options({"autorest_options": {"A": 12, "B": True, "C": ''}}, {})
assert line == ["--a=12", "--b=
```
<Overlap Ratio: 0.927246790299572>

---

--- 383 --
Question ID: b2ef62d79752c2a4f8feb0b371165d2f6dc4ee4b_23
Original Code:
```
def test_mongo_config_with_data():
    """Test creation of the MongoConfig model with data."""
    res = MongoConfig(**MONGO_CONFIG)
    assert isinstance(res, MongoConfig)
```


Overlapping Code:
```
eation of the MongoConfig model with data."""
res = MongoConfig(**MONGO_CONFIG)
assert isinstance(re
```
<Overlap Ratio: 0.625>

---

