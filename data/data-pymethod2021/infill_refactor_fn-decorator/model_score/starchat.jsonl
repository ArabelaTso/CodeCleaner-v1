{"hexsha": "07bea0dee804f4a3dd1cb8de837a2c6f9ce9c771", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef pretty_print_post(url: str):\n    try:\n        text = parse_post(url)\n    except api2ch.Api2chError as e:\n        print('Request Error', e.code, e.reason)\n    else:\n        print(text)", "fn_id": 1, "class_fn": false, "repo": "uburuntu/api2ch", "file": "examples/complex.py", "last_update_at": "2021-07-26T09:40:57+00:00", "original_content": "def pretty_print_post(url: str):\n    try:\n        text = parse_post(url)\n    except api2ch.Api2chError as e:\n        print('Request Error', e.code, e.reason)\n    else:\n        print(text)", "refactored": true, "pred": {"ppl": 10.087967872619629, "ppl_lower": 11.04575252532959, "ppl/lowercase_ppl": -1.0392423526196863, "ppl/zlib": 0.0153069100261257, "Min_5.0% Prob": 11.318449020385742, "Min_10.0% Prob": 10.236943244934082, "Min_20.0% Prob": 8.237363851987398, "Min_30.0% Prob": 6.685976195335388, "Min_40.0% Prob": 5.524847794462134, "Min_50.0% Prob": 4.526617450749173, "Min_60.0% Prob": 3.891281943768263}}
{"hexsha": "6c31f616e579c190b006f0e8ba4f595d892ad988", "ext": "py", "lang": "Python", "content": "@card('Braids, Cabal Minion')\ndef braids_cabal_minion(card, abilities):\n\n    @timeing\n    @measure_memory_usage\n    def braids_cabal_minion():\n        return AbilityNotImplemented\n    return (braids_cabal_minion,)", "fn_id": 28, "class_fn": false, "repo": "Julian/cardboard", "file": "cardboard/cards/sets/odyssey.py", "last_update_at": "2021-05-29T06:00:40+00:00", "original_content": "@card('Braids, Cabal Minion')\ndef braids_cabal_minion(card, abilities):\n\n    def braids_cabal_minion():\n        return AbilityNotImplemented\n    return (braids_cabal_minion,)", "refactored": true, "pred": {"ppl": 9.477385520935059, "ppl_lower": 16.300935745239258, "ppl/lowercase_ppl": -1.2411454388337775, "ppl/zlib": 0.01641539043284138, "Min_5.0% Prob": 12.82346248626709, "Min_10.0% Prob": 10.626524289449057, "Min_20.0% Prob": 8.552176035367525, "Min_30.0% Prob": 7.054902189656308, "Min_40.0% Prob": 5.580356966990691, "Min_50.0% Prob": 4.477002944910165, "Min_60.0% Prob": 3.8012963335674543}}
{"hexsha": "fc7e94a2224575fc9faaa120b95dfa7c40d2f253", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_extended_projection(ra_module):\n    initial_set = ra_module.NamedRelationalAlgebraFrozenSet(('x', 'y'), [(7, 8), (9, 2)])\n    expected_sum = ra_module.NamedRelationalAlgebraFrozenSet(('z',), [(15,), (11,)])\n    expected_lambda = ra_module.NamedRelationalAlgebraFrozenSet(('z',), [(14,), (10,)])\n    expected_lambda2 = ra_module.NamedRelationalAlgebraFrozenSet(('z', 'x'), [(14, 8), (10, 10)])\n    expected_new_colum_str = ra_module.NamedRelationalAlgebraFrozenSet(('x', 'z'), [(7, 'a'), (9, 'a')])\n    expected_new_colum_int = ra_module.NamedRelationalAlgebraFrozenSet(('z',), [(1,), (1,)])\n    new_set = initial_set.extended_projection({'z': sum})\n    assert expected_sum == new_set\n    new_set = initial_set.extended_projection({'z': ra_module.RelationalAlgebraStringExpression('x+y')})\n    assert expected_sum == new_set\n    new_set = initial_set.extended_projection({'z': lambda r: r.x + r.y - 1})\n    assert expected_lambda == new_set\n    new_set = initial_set.extended_projection({'z': lambda r: r.x + r.y - 1, 'x': ra_module.RelationalAlgebraStringExpression('x+1')})\n    assert expected_lambda2 == new_set\n    new_set = initial_set.extended_projection({'z': 'a', 'x': ra_module.RelationalAlgebraStringExpression('x')})\n    assert expected_new_colum_str == new_set\n    new_set = initial_set.extended_projection({'z': 1})\n    assert expected_new_colum_int == new_set\n    new_set = initial_set.extended_projection({'x': ra_module.RelationalAlgebraColumnStr('x')})\n    assert initial_set.projection('x') == new_set\n    base_set = ra_module.NamedRelationalAlgebraFrozenSet((1, 2), [(7, 8), (9, 2)])\n    new_set = base_set.extended_projection({'x': ra_module.RelationalAlgebraColumnInt(1), 'y': ra_module.RelationalAlgebraColumnInt(2)})\n    assert initial_set == new_set", "fn_id": 31, "class_fn": false, "repo": "demianw/NeuroLang", "file": "neurolang/utils/tests/test_relational_algebra_set.py", "last_update_at": "2021-07-02T09:06:30+00:00", "original_content": "def test_extended_projection(ra_module):\n    initial_set = ra_module.NamedRelationalAlgebraFrozenSet(('x', 'y'), [(7, 8), (9, 2)])\n    expected_sum = ra_module.NamedRelationalAlgebraFrozenSet(('z',), [(15,), (11,)])\n    expected_lambda = ra_module.NamedRelationalAlgebraFrozenSet(('z',), [(14,), (10,)])\n    expected_lambda2 = ra_module.NamedRelationalAlgebraFrozenSet(('z', 'x'), [(14, 8), (10, 10)])\n    expected_new_colum_str = ra_module.NamedRelationalAlgebraFrozenSet(('x', 'z'), [(7, 'a'), (9, 'a')])\n    expected_new_colum_int = ra_module.NamedRelationalAlgebraFrozenSet(('z',), [(1,), (1,)])\n    new_set = initial_set.extended_projection({'z': sum})\n    assert expected_sum == new_set\n    new_set = initial_set.extended_projection({'z': ra_module.RelationalAlgebraStringExpression('x+y')})\n    assert expected_sum == new_set\n    new_set = initial_set.extended_projection({'z': lambda r: r.x + r.y - 1})\n    assert expected_lambda == new_set\n    new_set = initial_set.extended_projection({'z': lambda r: r.x + r.y - 1, 'x': ra_module.RelationalAlgebraStringExpression('x+1')})\n    assert expected_lambda2 == new_set\n    new_set = initial_set.extended_projection({'z': 'a', 'x': ra_module.RelationalAlgebraStringExpression('x')})\n    assert expected_new_colum_str == new_set\n    new_set = initial_set.extended_projection({'z': 1})\n    assert expected_new_colum_int == new_set\n    new_set = initial_set.extended_projection({'x': ra_module.RelationalAlgebraColumnStr('x')})\n    assert initial_set.projection('x') == new_set\n    base_set = ra_module.NamedRelationalAlgebraFrozenSet((1, 2), [(7, 8), (9, 2)])\n    new_set = base_set.extended_projection({'x': ra_module.RelationalAlgebraColumnInt(1), 'y': ra_module.RelationalAlgebraColumnInt(2)})\n    assert initial_set == new_set", "refactored": true, "pred": {"ppl": 2.3455545902252197, "ppl_lower": 2.377450704574585, "ppl/lowercase_ppl": -1.015843473980988, "ppl/zlib": 0.0021207011765340795, "Min_5.0% Prob": 8.38122798715319, "Min_10.0% Prob": 6.232189804315567, "Min_20.0% Prob": 3.9701838372027978, "Min_30.0% Prob": 2.796541459595456, "Min_40.0% Prob": 2.1260070574638075, "Min_50.0% Prob": 1.7059626201902267, "Min_60.0% Prob": 1.422185677181536}}
{"hexsha": "b41186cab3d777d1b3f6deea4852d5203576ebc3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef say(text):\n    hash = hashlib.md5(text.encode()).hexdigest()\n    file = 'speech-cache/{}.wav'.format(hash)\n    cmd = 'play {}'.format(file)\n    if not os.path.isfile(file):\n        cmd = 'pico2wave -w {} \"{}\" && {}'.format(file, text, cmd)\n    os.system(cmd)", "fn_id": 3, "class_fn": false, "repo": "yousefamar/sanctum", "file": "sanctum.py", "last_update_at": "2021-01-17T00:14:06+00:00", "original_content": "def say(text):\n    hash = hashlib.md5(text.encode()).hexdigest()\n    file = 'speech-cache/{}.wav'.format(hash)\n    cmd = 'play {}'.format(file)\n    if not os.path.isfile(file):\n        cmd = 'pico2wave -w {} \"{}\" && {}'.format(file, text, cmd)\n    os.system(cmd)", "refactored": true, "pred": {"ppl": 4.854525566101074, "ppl_lower": 4.854525566101074, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.007939253147180006, "Min_5.0% Prob": 11.305333614349365, "Min_10.0% Prob": 9.077955722808838, "Min_20.0% Prob": 6.589149399807579, "Min_30.0% Prob": 5.181923138243811, "Min_40.0% Prob": 3.957739654340242, "Min_50.0% Prob": 3.1556350962103656, "Min_60.0% Prob": 2.660370485661061}}
{"hexsha": "efba18d12458eb11af1a1ce332ee1fe30fca6aa2", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef stft(sig, frameSize=FRAMESIZE, overlapFac=OVERLAP, window=np.hanning):\n    hop = int(frameSize - np.floor(overlapFac * frameSize))\n    w = np.sqrt(window(frameSize))\n    out = np.array([np.fft.rfft(w * sig[i:i + frameSize]) for i in range(0, len(sig) - frameSize, hop)])\n    out = np.abs(out)\n    out -= np.mean(out)\n    return out", "fn_id": 0, "class_fn": false, "repo": "glkuzi/CountMTS", "file": "DataGenerator.py", "last_update_at": "2021-07-22T03:45:54+00:00", "original_content": "def stft(sig, frameSize=FRAMESIZE, overlapFac=OVERLAP, window=np.hanning):\n    hop = int(frameSize - np.floor(overlapFac * frameSize))\n    w = np.sqrt(window(frameSize))\n    out = np.array([np.fft.rfft(w * sig[i:i + frameSize]) for i in range(0, len(sig) - frameSize, hop)])\n    out = np.abs(out)\n    out -= np.mean(out)\n    return out", "refactored": true, "pred": {"ppl": 4.293637275695801, "ppl_lower": 4.151358127593994, "ppl/lowercase_ppl": -0.9768733157037932, "ppl/zlib": 0.0062005711639610385, "Min_5.0% Prob": 11.12176481882731, "Min_10.0% Prob": 8.809011141459147, "Min_20.0% Prob": 6.584643287658691, "Min_30.0% Prob": 4.819735957603705, "Min_40.0% Prob": 3.6567077601657196, "Min_50.0% Prob": 2.9320980682387017, "Min_60.0% Prob": 2.440332583562991}}
{"hexsha": "3daf2b90efe24d08876eb33fa7e809886b79aacb", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef write_float_list():\n    global result\n    with open('result_shared_memory.txt', 'w') as f:\n        f.write(str(result))\n        f.close()", "fn_id": 7, "class_fn": false, "repo": "alansouls/interop-pysharp", "file": "proof_of_concept/python/text_transfer/connector_file.py", "last_update_at": "2021-05-22T22:37:44+00:00", "original_content": "def write_float_list():\n    global result\n    with open('result_shared_memory.txt', 'w') as f:\n        f.write(str(result))\n        f.close()", "refactored": true, "pred": {"ppl": 8.364208221435547, "ppl_lower": 8.364208221435547, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.016464819195467675, "Min_5.0% Prob": 10.225103855133057, "Min_10.0% Prob": 9.199113845825195, "Min_20.0% Prob": 7.743890428543091, "Min_30.0% Prob": 6.362903626759847, "Min_40.0% Prob": 5.129241943359375, "Min_50.0% Prob": 4.1242444320366936, "Min_60.0% Prob": 3.5171047747135162}}
{"hexsha": "e13fa130c6f556d3b69fbf1da32dc9e4fc944723", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef report():\n    value = grp.value\n    try:\n        say(labels[value], 'selected')\n    except (TypeError, IndexError):\n        say('Value =', value)", "fn_id": 0, "class_fn": false, "repo": "gcewing/PyGUI", "file": "Tests/07-radiogroup.py", "last_update_at": "2021-11-24T19:50:02+00:00", "original_content": "def report():\n    value = grp.value\n    try:\n        say(labels[value], 'selected')\n    except (TypeError, IndexError):\n        say('Value =', value)", "refactored": true, "pred": {"ppl": 32.73969650268555, "ppl_lower": 39.42566680908203, "ppl/lowercase_ppl": -1.0532676051173564, "ppl/zlib": 0.025464148189605407, "Min_5.0% Prob": 15.209208965301514, "Min_10.0% Prob": 13.631213903427124, "Min_20.0% Prob": 11.325468275282118, "Min_30.0% Prob": 9.546820265906197, "Min_40.0% Prob": 8.310149510701498, "Min_50.0% Prob": 6.933463451655014, "Min_60.0% Prob": 5.798621227698667}}
{"hexsha": "d5c45875560e75035fb70d3ac4fa58d21599d27d", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_vsi_path_zip_plus_https():\n    \"\"\"A zip+https:// URLs vsi path is correct (see #1151)\"\"\"\n    url = 'zip+https://example.com/foo.zip!bar.tif'\n    assert vsi_path(parse_path(url)) == '/vsizip/vsicurl/https://example.com/foo.zip/bar.tif'", "fn_id": 16, "class_fn": false, "repo": "CloudNiner/rasterio", "file": "tests/test_path.py", "last_update_at": "2021-09-16T00:44:50+00:00", "original_content": "def test_vsi_path_zip_plus_https():\n    \"\"\"A zip+https:// URLs vsi path is correct (see #1151)\"\"\"\n    url = 'zip+https://example.com/foo.zip!bar.tif'\n    assert vsi_path(parse_path(url)) == '/vsizip/vsicurl/https://example.com/foo.zip/bar.tif'", "refactored": true, "pred": {"ppl": 9.333516120910645, "ppl_lower": 9.28604507446289, "ppl/lowercase_ppl": -0.9977171225205403, "ppl/zlib": 0.012073577328100571, "Min_5.0% Prob": 10.510348510742187, "Min_10.0% Prob": 9.415906286239624, "Min_20.0% Prob": 7.709419417381286, "Min_30.0% Prob": 6.42031303246816, "Min_40.0% Prob": 5.299529950793197, "Min_50.0% Prob": 4.440781877321355, "Min_60.0% Prob": 3.7582747269360746}}
{"hexsha": "995e126c50514275c11a5e83f7c9eaa1aa63e240", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef timestamp_unpack(seconds, timedelta=datetime.timedelta, relative_to=pg_epoch_datetime.__add__):\n    \"\"\"\n\tCreate a `datetime.datetime` instance from a (seconds, microseconds) pair.\n\t\"\"\"\n    return relative_to(timedelta(0, *seconds))", "fn_id": 1, "class_fn": false, "repo": "autolog/TRV_Controller", "file": "TRV.indigoPlugin/Contents/Server Plugin/postgresql/types/io/stdlib_datetime.py", "last_update_at": "2021-02-03T16:31:26+00:00", "original_content": "def timestamp_unpack(seconds, timedelta=datetime.timedelta, relative_to=pg_epoch_datetime.__add__):\n    \"\"\"\n\tCreate a `datetime.datetime` instance from a (seconds, microseconds) pair.\n\t\"\"\"\n    return relative_to(timedelta(0, *seconds))", "refactored": true, "pred": {"ppl": 17.147680282592773, "ppl_lower": 18.39974594116211, "ppl/lowercase_ppl": -1.0247985055515314, "ppl/zlib": 0.016522458744030268, "Min_5.0% Prob": 13.495926856994629, "Min_10.0% Prob": 11.369515419006348, "Min_20.0% Prob": 9.37775012425014, "Min_30.0% Prob": 7.588986505161632, "Min_40.0% Prob": 6.424025099852989, "Min_50.0% Prob": 5.4240407267132325, "Min_60.0% Prob": 4.708251470869238}}
{"hexsha": "6f8e9b82a7e7f7ef2e5696e6e11a9da88873c108", "ext": "py", "lang": "Python", "content": "@app.route('/ext/<api_key>/new_session/<session_name>', methods=['POST'])\n@timeing\n@measure_memory_usage\ndef ext_create_new_session_name(api_key, session_name):\n    logging.debug('EXTERNAL: Create new Session with name')\n    db = Database()\n    user = db.get_user_by_api_key(api_key)\n    if user is None:\n        return (\"Provided API key ('{}') is not associated with any registered user\".format(api_key), HTTPStatus.UNAUTHORIZED.value)\n    if db.does_session_name_exist(session_name):\n        return (\"Provided session name ('{}') already exists\".format(session_name), HTTPStatus.UNAUTHORIZED.value)\n    logging.debug('Creating new session: %s (userid: %s)', session_name, user['user_id'])\n    session_id = db.create_session(session_name, user['user_id'])\n    logging.debug('New session id: %s', session_id)\n    set_current_session(session_id, session_name)\n    return \"New Session ID: '{}'\".format(session_id)", "fn_id": 2, "class_fn": false, "repo": "Samiasa/UsabilityConsole", "file": "console/routes/external.py", "last_update_at": "2021-06-07T17:15:42+00:00", "original_content": "@app.route('/ext/<api_key>/new_session/<session_name>', methods=['POST'])\ndef ext_create_new_session_name(api_key, session_name):\n    logging.debug('EXTERNAL: Create new Session with name')\n    db = Database()\n    user = db.get_user_by_api_key(api_key)\n    if user is None:\n        return (\"Provided API key ('{}') is not associated with any registered user\".format(api_key), HTTPStatus.UNAUTHORIZED.value)\n    if db.does_session_name_exist(session_name):\n        return (\"Provided session name ('{}') already exists\".format(session_name), HTTPStatus.UNAUTHORIZED.value)\n    logging.debug('Creating new session: %s (userid: %s)', session_name, user['user_id'])\n    session_id = db.create_session(session_name, user['user_id'])\n    logging.debug('New session id: %s', session_id)\n    set_current_session(session_id, session_name)\n    return \"New Session ID: '{}'\".format(session_id)", "refactored": true, "pred": {"ppl": 3.6526968479156494, "ppl_lower": 4.183244705200195, "ppl/lowercase_ppl": -1.1046893232875425, "ppl/zlib": 0.002984944141042782, "Min_5.0% Prob": 9.412486333113451, "Min_10.0% Prob": 7.467903247246375, "Min_20.0% Prob": 5.400225472900103, "Min_30.0% Prob": 4.112725601166109, "Min_40.0% Prob": 3.1808666359141187, "Min_50.0% Prob": 2.587243511143959, "Min_60.0% Prob": 2.156681151076589}}
{"hexsha": "3b40f31f9e2306102a844f470092207c102189e3", "ext": "py", "lang": "Python", "content": "@pb.route('/detail/<id>', methods=['GET', 'POST'])\n@timeing\n@measure_memory_usage\ndef single_product(id):\n    product = Product.query.filter_by(id=id).first()\n    if product.out_of_stock == True:\n        status = 'Sold Out'\n    else:\n        status = 'In stock'\n    data = {'id': product.id, 'name': product.name, 'img_url': product.img_url, 'description': product.description, 'seller': User.query.filter_by(id=product.seller_id).first().name, 'seller_id': product.seller_id, 'created': product.created, 'price': product.price, 'status': status, 'seller_img': User.query.filter_by(id=product.seller_id).first().avatar_url}\n    return jsonify(data)", "fn_id": 3, "class_fn": false, "repo": "sori9088/mealplan-server", "file": "app/components/product/__init__.py", "last_update_at": "2021-06-02T00:44:12+00:00", "original_content": "@pb.route('/detail/<id>', methods=['GET', 'POST'])\ndef single_product(id):\n    product = Product.query.filter_by(id=id).first()\n    if product.out_of_stock == True:\n        status = 'Sold Out'\n    else:\n        status = 'In stock'\n    data = {'id': product.id, 'name': product.name, 'img_url': product.img_url, 'description': product.description, 'seller': User.query.filter_by(id=product.seller_id).first().name, 'seller_id': product.seller_id, 'created': product.created, 'price': product.price, 'status': status, 'seller_img': User.query.filter_by(id=product.seller_id).first().avatar_url}\n    return jsonify(data)", "refactored": true, "pred": {"ppl": 3.1610772609710693, "ppl_lower": 3.4679338932037354, "ppl/lowercase_ppl": -1.0804979455423227, "ppl/zlib": 0.0036770379388957808, "Min_5.0% Prob": 10.449463653564454, "Min_10.0% Prob": 8.228274631500245, "Min_20.0% Prob": 5.365259036421776, "Min_30.0% Prob": 3.795284986992677, "Min_40.0% Prob": 2.885602711793035, "Min_50.0% Prob": 2.2963236908585127, "Min_60.0% Prob": 1.919691768982871}}
{"hexsha": "fe240d6eafcfe9043656fc61dbcdc36afa2f5d4c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_feature(internel_layer, layers, filters, batch_norm=False):\n    \"\"\"Get VGG feature body as stacks of convoltions.\"\"\"\n    for i, num in enumerate(layers):\n        for j in range(num):\n            internel_layer = sym.conv2d(data=internel_layer, kernel_size=(3, 3), padding=(1, 1), channels=filters[i], name='conv%s_%s' % (i + 1, j + 1))\n            if batch_norm:\n                internel_layer = sym.batch_norm(data=internel_layer, name='bn%s_%s' % (i + 1, j + 1))\n            internel_layer = sym.relu(data=internel_layer, name='relu%s_%s' % (i + 1, j + 1))\n        internel_layer = sym.max_pool2d(data=internel_layer, pool_size=(2, 2), strides=(2, 2), name='pool%s' % (i + 1))\n    return internel_layer", "fn_id": 0, "class_fn": false, "repo": "CynthiaProtector/helo", "file": "nnvm/python/nnvm/testing/vgg.py", "last_update_at": "2021-12-25T06:09:46+00:00", "original_content": "def get_feature(internel_layer, layers, filters, batch_norm=False):\n    \"\"\"Get VGG feature body as stacks of convoltions.\"\"\"\n    for i, num in enumerate(layers):\n        for j in range(num):\n            internel_layer = sym.conv2d(data=internel_layer, kernel_size=(3, 3), padding=(1, 1), channels=filters[i], name='conv%s_%s' % (i + 1, j + 1))\n            if batch_norm:\n                internel_layer = sym.batch_norm(data=internel_layer, name='bn%s_%s' % (i + 1, j + 1))\n            internel_layer = sym.relu(data=internel_layer, name='relu%s_%s' % (i + 1, j + 1))\n        internel_layer = sym.max_pool2d(data=internel_layer, pool_size=(2, 2), strides=(2, 2), name='pool%s' % (i + 1))\n    return internel_layer", "refactored": true, "pred": {"ppl": 2.8355605602264404, "ppl_lower": 3.035755157470703, "ppl/lowercase_ppl": -1.0654557366030226, "ppl/zlib": 0.0032569988940904745, "Min_5.0% Prob": 10.395005079416128, "Min_10.0% Prob": 7.902915459412795, "Min_20.0% Prob": 5.060411583918792, "Min_30.0% Prob": 3.4762495120461936, "Min_40.0% Prob": 2.6142177684859442, "Min_50.0% Prob": 2.092236524645705, "Min_60.0% Prob": 1.7436860599862904}}
{"hexsha": "d1b6df0cf562b4a74a407607abec54818df78080", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef mech():\n    item = {0: 'mail', 1: 'password'}\n    try:\n        choice = int(input(bcolors.OKBLUE + 'Enter the option no. of data: \\n [0] Mail \\n [1] Password\\n [2] Exit\\nOption:' + bcolors.ENDC))\n        try:\n            pass_data = input(bcolors.OKBLUE + 'Enter the ' + str(item[choice]) + ' to check: ' + bcolors.ENDC)\n        except:\n            exit(0)\n        data = main(str(choice), pass_data)\n        print(bcolors.OKGREEN + data + bcolors.ENDC)\n        print(bcolors.OKBLUE + ' [0] Dump data in a file \\n [1] Go back \\n [2] Exit' + bcolors.ENDC)\n        choice_ = input(bcolors.OKBLUE + 'Option: ' + bcolors.ENDC)\n        if choice_ == '0':\n            raw_filename = input(bcolors.OKBLUE + 'File name to save: ' + bcolors.ENDC)\n            filename = raw_filename + '_' + str(item[choice]) + '.txt'\n            file = open(filename, 'w')\n            file.write(data)\n            file.close()\n            print(bcolors.OKGREEN + 'File is saved in same directory with name {}.'.format(raw_filename) + bcolors.ENDC)\n            print()\n        elif choice_ == '1':\n            print()\n            print()\n            mech()\n        else:\n            exit(0)\n    except Exception as e:\n        print(bcolors.FAIL + e + bcolors.ENDC)\n        exit(0)", "fn_id": 1, "class_fn": false, "repo": "StrinTH/checkleaks", "file": "leakcheck.py", "last_update_at": "2021-01-06T14:45:08+00:00", "original_content": "def mech():\n    item = {0: 'mail', 1: 'password'}\n    try:\n        choice = int(input(bcolors.OKBLUE + 'Enter the option no. of data: \\n [0] Mail \\n [1] Password\\n [2] Exit\\nOption:' + bcolors.ENDC))\n        try:\n            pass_data = input(bcolors.OKBLUE + 'Enter the ' + str(item[choice]) + ' to check: ' + bcolors.ENDC)\n        except:\n            exit(0)\n        data = main(str(choice), pass_data)\n        print(bcolors.OKGREEN + data + bcolors.ENDC)\n        print(bcolors.OKBLUE + ' [0] Dump data in a file \\n [1] Go back \\n [2] Exit' + bcolors.ENDC)\n        choice_ = input(bcolors.OKBLUE + 'Option: ' + bcolors.ENDC)\n        if choice_ == '0':\n            raw_filename = input(bcolors.OKBLUE + 'File name to save: ' + bcolors.ENDC)\n            filename = raw_filename + '_' + str(item[choice]) + '.txt'\n            file = open(filename, 'w')\n            file.write(data)\n            file.close()\n            print(bcolors.OKGREEN + 'File is saved in same directory with name {}.'.format(raw_filename) + bcolors.ENDC)\n            print()\n        elif choice_ == '1':\n            print()\n            print()\n            mech()\n        else:\n            exit(0)\n    except Exception as e:\n        print(bcolors.FAIL + e + bcolors.ENDC)\n        exit(0)", "refactored": true, "pred": {"ppl": 3.342710018157959, "ppl_lower": 3.652327060699463, "ppl/lowercase_ppl": -1.0734040322700507, "ppl/zlib": 0.0025352560098269995, "Min_5.0% Prob": 9.09364534679212, "Min_10.0% Prob": 7.276759988383243, "Min_20.0% Prob": 5.149652583258493, "Min_30.0% Prob": 3.862893290046988, "Min_40.0% Prob": 2.98576448809716, "Min_50.0% Prob": 2.4056110015695857, "Min_60.0% Prob": 2.0165308673047173}}
{"hexsha": "a113b237351d78e1a5fa0fd0d24d0327976b7f7a", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef process_data(dataframe: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"delete column that have most na, and fill na with mean\"\"\"\n    na_count = dataframe.isnull().sum(axis=0).tolist()\n    most_na_col = na_count.index(max(na_count))\n    result = dataframe.drop(dataframe.columns[most_na_col], axis=1)\n    return result.fillna(result.mean())", "fn_id": 1, "class_fn": false, "repo": "coookie89/Intern-Training", "file": "txya900619/Week1/ch2/2.2/exercise1.py", "last_update_at": "2021-08-24T12:14:46+00:00", "original_content": "def process_data(dataframe: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"delete column that have most na, and fill na with mean\"\"\"\n    na_count = dataframe.isnull().sum(axis=0).tolist()\n    most_na_col = na_count.index(max(na_count))\n    result = dataframe.drop(dataframe.columns[most_na_col], axis=1)\n    return result.fillna(result.mean())", "refactored": true, "pred": {"ppl": 4.229223251342773, "ppl_lower": 4.808315753936768, "ppl/lowercase_ppl": -1.088992293494233, "ppl/zlib": 0.00638061215784176, "Min_5.0% Prob": 10.900326347351074, "Min_10.0% Prob": 8.537366390228271, "Min_20.0% Prob": 6.038644654410226, "Min_30.0% Prob": 4.617737490683794, "Min_40.0% Prob": 3.5937837085058524, "Min_50.0% Prob": 2.8785560204199068, "Min_60.0% Prob": 2.432483275886625}}
{"hexsha": "be83fdd493ff018b94ad39f9ed435403d860ea8e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef generate_bio(spans, length):\n    tags = ['O'] * length\n    for span in spans:\n        tags[span.tokens[0]] = f'B-{span.type}'\n        for i in span.tokens[1:]:\n            tags[i] = f'I-{span.type}'\n    return tags", "fn_id": 4, "class_fn": false, "repo": "blester125/iobes", "file": "tests/utils.py", "last_update_at": "2021-12-06T23:15:18+00:00", "original_content": "def generate_bio(spans, length):\n    tags = ['O'] * length\n    for span in spans:\n        tags[span.tokens[0]] = f'B-{span.type}'\n        for i in span.tokens[1:]:\n            tags[i] = f'I-{span.type}'\n    return tags", "refactored": true, "pred": {"ppl": 5.614726543426514, "ppl_lower": 7.509453296661377, "ppl/lowercase_ppl": -1.1685238086116867, "ppl/zlib": 0.010851527584420229, "Min_5.0% Prob": 11.287869771321615, "Min_10.0% Prob": 10.161286899021693, "Min_20.0% Prob": 7.773333899180094, "Min_30.0% Prob": 5.776789066584214, "Min_40.0% Prob": 4.362451309157956, "Min_50.0% Prob": 3.4865784305028424, "Min_60.0% Prob": 2.8987589553791158}}
{"hexsha": "1a9febc6fc48fe142ca3755fceef4aa604dfb595", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef assemble(mappings, sequence_file, out_file, min_overlap, max_overhang, min_length):\n    graph, assembled = OverlapGraph.parse_paf(mappings, min_overlap, max_overhang)\n    paths = graph.max_paths()\n    sequences = parse_fasta(sequence_file)\n    with open(out_file, 'w') as f:\n        for path in paths:\n            seq, contained_reads, name = construct_merged_sequence(path, graph, sequences)\n            assembled += contained_reads\n            if len(seq) > min_length:\n                f.write('>%s\\n%s\\n' % (name, seq))\n        for n, s in sequences.items():\n            if n in assembled:\n                continue\n            if len(s) < min_length:\n                continue\n            f.write('>%s\\n%s\\n' % (n, s))", "fn_id": 2, "class_fn": false, "repo": "rikuu/hgga", "file": "asm_node.py", "last_update_at": "2021-04-09T21:56:00+00:00", "original_content": "def assemble(mappings, sequence_file, out_file, min_overlap, max_overhang, min_length):\n    graph, assembled = OverlapGraph.parse_paf(mappings, min_overlap, max_overhang)\n    paths = graph.max_paths()\n    sequences = parse_fasta(sequence_file)\n    with open(out_file, 'w') as f:\n        for path in paths:\n            seq, contained_reads, name = construct_merged_sequence(path, graph, sequences)\n            assembled += contained_reads\n            if len(seq) > min_length:\n                f.write('>%s\\n%s\\n' % (name, seq))\n        for n, s in sequences.items():\n            if n in assembled:\n                continue\n            if len(s) < min_length:\n                continue\n            f.write('>%s\\n%s\\n' % (n, s))", "refactored": true, "pred": {"ppl": 4.229791641235352, "ppl_lower": 4.156686305999756, "ppl/lowercase_ppl": -0.9879107543568547, "ppl/zlib": 0.0044510886865859, "Min_5.0% Prob": 10.391263580322265, "Min_10.0% Prob": 8.48236880983625, "Min_20.0% Prob": 6.234747409820557, "Min_30.0% Prob": 4.726897540546599, "Min_40.0% Prob": 3.597715523926651, "Min_50.0% Prob": 2.8955375496122353, "Min_60.0% Prob": 2.418325169314447}}
{"hexsha": "a2c6556c09f53bd180fe20a7e14114c3b4f5b12b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_mycroft_version():\n    try:\n        from mycroft.version import CORE_VERSION_STR\n        return CORE_VERSION_STR\n    except:\n        pass\n    root = search_mycroft_core_location()\n    if root:\n        version_file = join(root, 'version', '__init__.py')\n        if not isfile(version_file):\n            version_file = join(root, 'mycroft', 'version', '__init__.py')\n        if isfile(version_file):\n            version = []\n            with open(version_file) as f:\n                text = f.read()\n                version.append(text.split('CORE_VERSION_MAJOR =')[-1].split('\\n')[0].strip())\n                version.append(text.split('CORE_VERSION_MINOR =')[-1].split('\\n')[0].strip())\n                version.append(text.split('CORE_VERSION_BUILD =')[-1].split('\\n')[0].strip())\n                version = '.'.join(version)\n                if \"CORE_VERSION_STR = '.'.join(map(str, CORE_VERSION_TUPLE)) + \" in text:\n                    version += text.split(\"CORE_VERSION_STR = '.'.join(map(str, CORE_VERSION_TUPLE)) + \")[-1].split('\\n')[0][1:-1]\n                return version\n        return None", "fn_id": 3, "class_fn": false, "repo": "NeonJarbas/ovos_utils", "file": "ovos_utils/fingerprinting.py", "last_update_at": "2021-11-08T07:19:25+00:00", "original_content": "def get_mycroft_version():\n    try:\n        from mycroft.version import CORE_VERSION_STR\n        return CORE_VERSION_STR\n    except:\n        pass\n    root = search_mycroft_core_location()\n    if root:\n        version_file = join(root, 'version', '__init__.py')\n        if not isfile(version_file):\n            version_file = join(root, 'mycroft', 'version', '__init__.py')\n        if isfile(version_file):\n            version = []\n            with open(version_file) as f:\n                text = f.read()\n                version.append(text.split('CORE_VERSION_MAJOR =')[-1].split('\\n')[0].strip())\n                version.append(text.split('CORE_VERSION_MINOR =')[-1].split('\\n')[0].strip())\n                version.append(text.split('CORE_VERSION_BUILD =')[-1].split('\\n')[0].strip())\n                version = '.'.join(version)\n                if \"CORE_VERSION_STR = '.'.join(map(str, CORE_VERSION_TUPLE)) + \" in text:\n                    version += text.split(\"CORE_VERSION_STR = '.'.join(map(str, CORE_VERSION_TUPLE)) + \")[-1].split('\\n')[0][1:-1]\n                return version\n        return None", "refactored": true, "pred": {"ppl": 2.7446630001068115, "ppl_lower": 2.935319423675537, "ppl/lowercase_ppl": -1.0665155540673588, "ppl/zlib": 0.002678138723589948, "Min_5.0% Prob": 9.108300654093425, "Min_10.0% Prob": 7.275043602912657, "Min_20.0% Prob": 4.746934566232893, "Min_30.0% Prob": 3.3412088016246226, "Min_40.0% Prob": 2.517417196568752, "Min_50.0% Prob": 2.0246334722396107, "Min_60.0% Prob": 1.6826204318588935}}
{"hexsha": "d9fc432fc00d588e5c0efee2aee23a3f4e8a0c91", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef main(_):\n    tf.logging.set_verbosity(tf.logging.INFO)\n    usr_dir.import_usr_dir(FLAGS.t2t_usr_dir)\n    trainer_utils.log_registry()\n    trainer_utils.validate_flags()\n    output_dir = os.path.expanduser(FLAGS.output_dir)\n    tmp_dir = os.path.expanduser(FLAGS.tmp_dir)\n    if not FLAGS.data_dir:\n        raise ValueError('You must specify a --data_dir')\n    data_dir = os.path.expanduser(FLAGS.data_dir)\n    tf.gfile.MakeDirs(output_dir)\n    if FLAGS.generate_data:\n        tf.gfile.MakeDirs(data_dir)\n        tf.gfile.MakeDirs(tmp_dir)\n        for problem_name in FLAGS.problems.split('-'):\n            tf.logging.info('Generating data for %s' % problem_name)\n            problem = registry.problem(problem_name)\n            problem.generate_data(data_dir, tmp_dir)\n    trainer_utils.run(data_dir=data_dir, model=FLAGS.model, output_dir=output_dir, train_steps=FLAGS.train_steps, eval_steps=FLAGS.eval_steps, schedule=FLAGS.schedule)", "fn_id": 0, "class_fn": false, "repo": "ctuning/ck-ml", "file": "program/tensor2tensor-attention-tf/source/tensor2tensor/tensor2tensor/bin/t2t-trainer.py", "last_update_at": "2021-09-14T14:14:10+00:00", "original_content": "def main(_):\n    tf.logging.set_verbosity(tf.logging.INFO)\n    usr_dir.import_usr_dir(FLAGS.t2t_usr_dir)\n    trainer_utils.log_registry()\n    trainer_utils.validate_flags()\n    output_dir = os.path.expanduser(FLAGS.output_dir)\n    tmp_dir = os.path.expanduser(FLAGS.tmp_dir)\n    if not FLAGS.data_dir:\n        raise ValueError('You must specify a --data_dir')\n    data_dir = os.path.expanduser(FLAGS.data_dir)\n    tf.gfile.MakeDirs(output_dir)\n    if FLAGS.generate_data:\n        tf.gfile.MakeDirs(data_dir)\n        tf.gfile.MakeDirs(tmp_dir)\n        for problem_name in FLAGS.problems.split('-'):\n            tf.logging.info('Generating data for %s' % problem_name)\n            problem = registry.problem(problem_name)\n            problem.generate_data(data_dir, tmp_dir)\n    trainer_utils.run(data_dir=data_dir, model=FLAGS.model, output_dir=output_dir, train_steps=FLAGS.train_steps, eval_steps=FLAGS.eval_steps, schedule=FLAGS.schedule)", "refactored": true, "pred": {"ppl": 2.9577198028564453, "ppl_lower": 3.3201491832733154, "ppl/lowercase_ppl": -1.106592674037748, "ppl/zlib": 0.0027178411894767723, "Min_5.0% Prob": 8.819909731547037, "Min_10.0% Prob": 7.063490676879883, "Min_20.0% Prob": 4.971282674868902, "Min_30.0% Prob": 3.594738139708837, "Min_40.0% Prob": 2.7090750804617385, "Min_50.0% Prob": 2.174907951177844, "Min_60.0% Prob": 1.815203407916333}}
{"hexsha": "01b86d405494b9108be791ee779c96fe8f5fc81b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_code_point_of_name():\n    assert demojiprocess.code_point_of_name('yellow_heart') == '1f49b'\n    assert demojiprocess.code_point_of_name(':yellow_heart:') == '1f49b'\n    assert demojiprocess.code_point_of_name('woman_health_worker_tone2') == '1f469-1f3fc-200d-2695-fe0f'\n    assert demojiprocess.code_point_of_name('adult:skin-tone-4') == '1f9d1-1f3fe'", "fn_id": 4, "class_fn": false, "repo": "PommeBleue/discord-emoji", "file": "src/tests/tests.py", "last_update_at": "2021-06-18T09:37:53+00:00", "original_content": "def test_code_point_of_name():\n    assert demojiprocess.code_point_of_name('yellow_heart') == '1f49b'\n    assert demojiprocess.code_point_of_name(':yellow_heart:') == '1f49b'\n    assert demojiprocess.code_point_of_name('woman_health_worker_tone2') == '1f469-1f3fc-200d-2695-fe0f'\n    assert demojiprocess.code_point_of_name('adult:skin-tone-4') == '1f9d1-1f3fe'", "refactored": true, "pred": {"ppl": 3.4163057804107666, "ppl_lower": 3.4163057804107666, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.006713441454054312, "Min_5.0% Prob": 11.29744565486908, "Min_10.0% Prob": 8.98922935128212, "Min_20.0% Prob": 5.780122071504593, "Min_30.0% Prob": 4.057436504401267, "Min_40.0% Prob": 3.0646013806399424, "Min_50.0% Prob": 2.455730794556439, "Min_60.0% Prob": 2.047347941692957}}
{"hexsha": "b9a0444e34d229fe92febc414b59c043e61e95ce", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_blue_marble_image(month):\n    filename = 'world.2004%02i.3x21600x10800.jpg' % month\n    fullpath = os.path.join(os.path.expanduser('~'), '.cache/burned_area/' + filename)\n    if os.path.exists(fullpath):\n        with open(fullpath, 'rb') as f:\n            imgdata = f.read()\n    else:\n        code = _nasa_blue_marble_ng_monthly_image_url_codes[month - 1]\n        url = ('https://eoimages.gsfc.nasa.gov/images/imagerecords/' + '%s/' + filename) % code\n        print('Downloading', url)\n        req = requests.get(url)\n        req.raise_for_status()\n        imgdata = req.content\n        os.makedirs(os.path.dirname(fullpath), exist_ok=True)\n        with open(fullpath, 'wb') as f:\n            f.write(imgdata)\n    img = Image.open(io.BytesIO(imgdata))\n    return img", "fn_id": 4, "class_fn": false, "repo": "benbarsdell/burned-area-viz", "file": "burned_area_viz.py", "last_update_at": "2021-04-16T18:20:36+00:00", "original_content": "def get_blue_marble_image(month):\n    filename = 'world.2004%02i.3x21600x10800.jpg' % month\n    fullpath = os.path.join(os.path.expanduser('~'), '.cache/burned_area/' + filename)\n    if os.path.exists(fullpath):\n        with open(fullpath, 'rb') as f:\n            imgdata = f.read()\n    else:\n        code = _nasa_blue_marble_ng_monthly_image_url_codes[month - 1]\n        url = ('https://eoimages.gsfc.nasa.gov/images/imagerecords/' + '%s/' + filename) % code\n        print('Downloading', url)\n        req = requests.get(url)\n        req.raise_for_status()\n        imgdata = req.content\n        os.makedirs(os.path.dirname(fullpath), exist_ok=True)\n        with open(fullpath, 'wb') as f:\n            f.write(imgdata)\n    img = Image.open(io.BytesIO(imgdata))\n    return img", "refactored": true, "pred": {"ppl": 3.2154948711395264, "ppl_lower": 3.7435050010681152, "ppl/lowercase_ppl": -1.130174233762589, "ppl/zlib": 0.002703660350806944, "Min_5.0% Prob": 10.284119936136099, "Min_10.0% Prob": 8.046301988454966, "Min_20.0% Prob": 5.3475657274138255, "Min_30.0% Prob": 3.8554725106805563, "Min_40.0% Prob": 2.9281140245915016, "Min_50.0% Prob": 2.3433004878322357, "Min_60.0% Prob": 1.9513184617617694}}
{"hexsha": "965d7f77982b9eaf53ae37fba39562b4f8c0b34f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _split_points_into_batches(points: NumericArray, number_of_points_per_batch: int) -> tp.List[tp.List[NumericArray]]:\n    number_of_points = points.shape[1]\n    n_begin = 0\n    args_list = []\n    while n_begin < number_of_points:\n        n_end = min(n_begin + number_of_points_per_batch, number_of_points)\n        args_list.append([points[:, n_begin:n_end]])\n        n_begin = n_end\n    return args_list", "fn_id": 0, "class_fn": false, "repo": "michaelnowotny/cocos", "file": "cocos/scientific/kde.py", "last_update_at": "2021-11-27T09:09:40+00:00", "original_content": "def _split_points_into_batches(points: NumericArray, number_of_points_per_batch: int) -> tp.List[tp.List[NumericArray]]:\n    number_of_points = points.shape[1]\n    n_begin = 0\n    args_list = []\n    while n_begin < number_of_points:\n        n_end = min(n_begin + number_of_points_per_batch, number_of_points)\n        args_list.append([points[:, n_begin:n_end]])\n        n_begin = n_end\n    return args_list", "refactored": true, "pred": {"ppl": 3.1933255195617676, "ppl_lower": 3.617906332015991, "ppl/lowercase_ppl": -1.107515833532312, "ppl/zlib": 0.0052300128667132455, "Min_5.0% Prob": 11.690078462873187, "Min_10.0% Prob": 9.159900222505842, "Min_20.0% Prob": 5.737337738275528, "Min_30.0% Prob": 3.859464039289674, "Min_40.0% Prob": 2.9258345038090883, "Min_50.0% Prob": 2.3211132344634584, "Min_60.0% Prob": 1.9440203481475649}}
{"hexsha": "01eaa1599fa6952fceac0db02f61801ac8392d27", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef plot_correlation(py, correlated, minimum_arg, minimums):\n    plt.figure()\n    plt.plot(py, correlated[:, 0, 0])\n    plt.plot(py, correlated[:, -1, -1])\n    plt.legend(['pixel 0,0', 'pixel n,n', 'minimum 0 0', 'minimum n n'])\n    plt.title('cross correlation between calibration curve and pixel values')\n    plt.xlabel('Physical distance (im)')\n    plt.ylabel('Focus unit')", "fn_id": 17, "class_fn": false, "repo": "idiap/deepfocus2019", "file": "detection/calibration_fit.py", "last_update_at": "2021-11-26T09:33:14+00:00", "original_content": "def plot_correlation(py, correlated, minimum_arg, minimums):\n    plt.figure()\n    plt.plot(py, correlated[:, 0, 0])\n    plt.plot(py, correlated[:, -1, -1])\n    plt.legend(['pixel 0,0', 'pixel n,n', 'minimum 0 0', 'minimum n n'])\n    plt.title('cross correlation between calibration curve and pixel values')\n    plt.xlabel('Physical distance (im)')\n    plt.ylabel('Focus unit')", "refactored": true, "pred": {"ppl": 12.271068572998047, "ppl_lower": 11.916271209716797, "ppl/lowercase_ppl": -0.9882980896729449, "ppl/zlib": 0.011243248175761243, "Min_5.0% Prob": 13.642422835032145, "Min_10.0% Prob": 11.820472002029419, "Min_20.0% Prob": 9.109337711334229, "Min_30.0% Prob": 7.3565947816178605, "Min_40.0% Prob": 5.962470479011536, "Min_50.0% Prob": 4.960368067026138, "Min_60.0% Prob": 4.152451425790787}}
{"hexsha": "d4fedd0a36cd395e4706b0c47428c1465d2a2a9d", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef wl_predict(datasets, test_datasets):\n    \"\"\"\n\n    \"\"\"\n    log_dir = tfu.get_logdir()\n    datasets, test_datasets = get_wl_datasets()\n    x, y = tfu.get_example(datasets)\n    loss_fxn = tf.losses.BinaryCrossentropy()\n    optim = tf.keras.optimizers.Adam()\n    model = lstm.make_mlp_functional(x.shape[-2:], tf.size(y[0]), classify=True)\n    train_summary_writer, test_summary_writer = tfu.init_summary_writers(log_dir)\n    train_loss, train_accuracy, test_loss, test_accuracy = tfu.get_classification_metrics()\n    tr_step = 0\n    te_step = 0\n    for epoch, dataset in enumerate(datasets):\n        if not dataset:\n            continue\n        for xtr, ytr in dataset:\n            ytr = tf.reshape(ytr, (1, -1))\n            tl, ta, preds = fwd.train_step_classify(model, optim, loss_fxn, xtr, ytr, train_loss, train_accuracy)\n            tr_step += 1\n            with train_summary_writer.as_default():\n                tf.summary.scalar('loss', tl.numpy(), step=tr_step)\n                tf.summary.scalar('accuracy', ta.numpy(), step=tr_step)\n            maxed_pred = tf.argmax(preds, 1).numpy()[0]\n            maxed_true = tf.argmax(ytr).numpy()\n            correct = tf.equal(maxed_pred, maxed_true).numpy()\n        test_dataset = random.choice(test_datasets)\n        if not test_dataset:\n            continue\n        for xte, yte in test_dataset:\n            yte = tf.reshape(yte, (1, -1))\n            tel, tea = fwd.test_step(model, loss_fxn, xte, yte, test_loss, test_accuracy)\n            te_step += 1\n            with test_summary_writer.as_default():\n                tf.summary.scalar('loss', tel.numpy(), step=te_step)\n                tf.summary.scalar('accuracy', tea.numpy(), step=te_step)\n        template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}, Preds: {}, Acts: {}'\n        print(template.format(epoch + 1, train_loss.result(), train_accuracy.result() * 100, test_loss.result(), test_accuracy.result() * 100, preds, ytr))\n        train_loss.reset_states()\n        test_loss.reset_states()\n        train_accuracy.reset_states()\n        test_accuracy.reset_states()\n    tf.saved_model.save(model, tfm.WRITE_TO + 'win_loss/')\n    return datasets", "fn_id": 1, "class_fn": false, "repo": "AnandIJain/sip", "file": "sips/ml/tf_models/win_loss.py", "last_update_at": "2021-06-13T13:53:59+00:00", "original_content": "def wl_predict(datasets, test_datasets):\n    \"\"\"\n\n    \"\"\"\n    log_dir = tfu.get_logdir()\n    datasets, test_datasets = get_wl_datasets()\n    x, y = tfu.get_example(datasets)\n    loss_fxn = tf.losses.BinaryCrossentropy()\n    optim = tf.keras.optimizers.Adam()\n    model = lstm.make_mlp_functional(x.shape[-2:], tf.size(y[0]), classify=True)\n    train_summary_writer, test_summary_writer = tfu.init_summary_writers(log_dir)\n    train_loss, train_accuracy, test_loss, test_accuracy = tfu.get_classification_metrics()\n    tr_step = 0\n    te_step = 0\n    for epoch, dataset in enumerate(datasets):\n        if not dataset:\n            continue\n        for xtr, ytr in dataset:\n            ytr = tf.reshape(ytr, (1, -1))\n            tl, ta, preds = fwd.train_step_classify(model, optim, loss_fxn, xtr, ytr, train_loss, train_accuracy)\n            tr_step += 1\n            with train_summary_writer.as_default():\n                tf.summary.scalar('loss', tl.numpy(), step=tr_step)\n                tf.summary.scalar('accuracy', ta.numpy(), step=tr_step)\n            maxed_pred = tf.argmax(preds, 1).numpy()[0]\n            maxed_true = tf.argmax(ytr).numpy()\n            correct = tf.equal(maxed_pred, maxed_true).numpy()\n        test_dataset = random.choice(test_datasets)\n        if not test_dataset:\n            continue\n        for xte, yte in test_dataset:\n            yte = tf.reshape(yte, (1, -1))\n            tel, tea = fwd.test_step(model, loss_fxn, xte, yte, test_loss, test_accuracy)\n            te_step += 1\n            with test_summary_writer.as_default():\n                tf.summary.scalar('loss', tel.numpy(), step=te_step)\n                tf.summary.scalar('accuracy', tea.numpy(), step=te_step)\n        template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}, Preds: {}, Acts: {}'\n        print(template.format(epoch + 1, train_loss.result(), train_accuracy.result() * 100, test_loss.result(), test_accuracy.result() * 100, preds, ytr))\n        train_loss.reset_states()\n        test_loss.reset_states()\n        train_accuracy.reset_states()\n        test_accuracy.reset_states()\n    tf.saved_model.save(model, tfm.WRITE_TO + 'win_loss/')\n    return datasets", "refactored": true, "pred": {"ppl": 3.134838819503784, "ppl_lower": 3.4296302795410156, "ppl/lowercase_ppl": -1.0786595965750874, "ppl/zlib": 0.001505372542195976, "Min_5.0% Prob": 10.12645082762747, "Min_10.0% Prob": 7.912604310295799, "Min_20.0% Prob": 5.328420393394701, "Min_30.0% Prob": 3.751936909990694, "Min_40.0% Prob": 2.855340718969984, "Min_50.0% Prob": 2.2842027485785805, "Min_60.0% Prob": 1.906052194479572}}
{"hexsha": "982db6db807b1cdc32803139fd8028059a1a1f18", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef plotvfonsph(theta_rad, phi_rad, F_th, F_ph, freq=0.0, vcoordlist=['sph'], projection='orthographic', cmplx_rep='AbsAng', vfname='Unknown'):\n    \"\"\"Plot transverse vector field on sphere. Different projections are\n    supported as are different bases and complex value representations.\"\"\"\n    x, y, xyNames, nom_xticks, F_th, F_ph = projectdomain(theta_rad, phi_rad, F_th, F_ph, projection)\n    F0_c, F1_c, compNames = vcoordconvert(F_th, F_ph, phi_rad, vcoordlist=vcoordlist)\n    F0_2r, cmplxop0 = cmplx2realrep(F0_c, cmplx_rep)\n    F1_2r, cmplxop1 = cmplx2realrep(F1_c, cmplx_rep)\n    if projection == 'orthographic':\n        xyNames = [xyNames[0] + ' []', xyNames[1] + ' []']\n    if projection == 'azimuthal-equidistant':\n        x = numpy.rad2deg(x)\n        y = numpy.rad2deg(y)\n        xyNames = [xyNames[0] + ' [deg.]', xyNames[1] + ' [deg.]']\n    fig = plt.figure()\n    fig.suptitle(vfname + ' @ ' + str(freq / 1000000.0) + ' MHz' + ', ' + 'projection: ' + projection)\n\n    def plotcomp(vcmpi, cpi, zcomp, cmplxop, xyNames, nom_xticks):\n        if cmplxop[cpi] == 'Ang':\n            cmap = plt.get_cmap('hsv')\n        else:\n            cmap = plt.get_cmap('viridis')\n        plt.pcolormesh(x, y, zcomp[cpi], cmap=cmap)\n        if nom_xticks is not None:\n            plt.xticks(nom_xticks)\n        ax.set_title(cmplxop[cpi] + '(' + compNames[vcmpi] + ')')\n        plt.xlabel(xyNames[0])\n        plt.ylabel(xyNames[1])\n        plt.grid()\n        plt.colorbar()\n        if projection == 'equirectangular':\n            ax.invert_yaxis()\n    ax = plt.subplot(221, polar=False)\n    plotcomp(0, 0, F0_2r, cmplxop0, xyNames, nom_xticks)\n    ax = plt.subplot(222, polar=False)\n    plotcomp(0, 1, F0_2r, cmplxop0, xyNames, nom_xticks)\n    ax = plt.subplot(223, polar=False)\n    plotcomp(1, 0, F1_2r, cmplxop1, xyNames, nom_xticks)\n    ax = plt.subplot(224, polar=False)\n    plotcomp(1, 1, F1_2r, cmplxop1, xyNames, nom_xticks)\n    plt.show()", "fn_id": 12, "class_fn": false, "repo": "David-McKenna/AntPat", "file": "antpat/reps/sphgridfun/tvecfun.py", "last_update_at": "2021-04-06T06:23:25+00:00", "original_content": "def plotvfonsph(theta_rad, phi_rad, F_th, F_ph, freq=0.0, vcoordlist=['sph'], projection='orthographic', cmplx_rep='AbsAng', vfname='Unknown'):\n    \"\"\"Plot transverse vector field on sphere. Different projections are\n    supported as are different bases and complex value representations.\"\"\"\n    x, y, xyNames, nom_xticks, F_th, F_ph = projectdomain(theta_rad, phi_rad, F_th, F_ph, projection)\n    F0_c, F1_c, compNames = vcoordconvert(F_th, F_ph, phi_rad, vcoordlist=vcoordlist)\n    F0_2r, cmplxop0 = cmplx2realrep(F0_c, cmplx_rep)\n    F1_2r, cmplxop1 = cmplx2realrep(F1_c, cmplx_rep)\n    if projection == 'orthographic':\n        xyNames = [xyNames[0] + ' []', xyNames[1] + ' []']\n    if projection == 'azimuthal-equidistant':\n        x = numpy.rad2deg(x)\n        y = numpy.rad2deg(y)\n        xyNames = [xyNames[0] + ' [deg.]', xyNames[1] + ' [deg.]']\n    fig = plt.figure()\n    fig.suptitle(vfname + ' @ ' + str(freq / 1000000.0) + ' MHz' + ', ' + 'projection: ' + projection)\n\n    def plotcomp(vcmpi, cpi, zcomp, cmplxop, xyNames, nom_xticks):\n        if cmplxop[cpi] == 'Ang':\n            cmap = plt.get_cmap('hsv')\n        else:\n            cmap = plt.get_cmap('viridis')\n        plt.pcolormesh(x, y, zcomp[cpi], cmap=cmap)\n        if nom_xticks is not None:\n            plt.xticks(nom_xticks)\n        ax.set_title(cmplxop[cpi] + '(' + compNames[vcmpi] + ')')\n        plt.xlabel(xyNames[0])\n        plt.ylabel(xyNames[1])\n        plt.grid()\n        plt.colorbar()\n        if projection == 'equirectangular':\n            ax.invert_yaxis()\n    ax = plt.subplot(221, polar=False)\n    plotcomp(0, 0, F0_2r, cmplxop0, xyNames, nom_xticks)\n    ax = plt.subplot(222, polar=False)\n    plotcomp(0, 1, F0_2r, cmplxop0, xyNames, nom_xticks)\n    ax = plt.subplot(223, polar=False)\n    plotcomp(1, 0, F1_2r, cmplxop1, xyNames, nom_xticks)\n    ax = plt.subplot(224, polar=False)\n    plotcomp(1, 1, F1_2r, cmplxop1, xyNames, nom_xticks)\n    plt.show()", "refactored": true, "pred": {"ppl": 3.743678092956543, "ppl_lower": 3.843864917755127, "ppl/lowercase_ppl": -1.0200063651578974, "ppl/zlib": 0.0017935714340673009, "Min_5.0% Prob": 10.798707948790657, "Min_10.0% Prob": 8.549269080162048, "Min_20.0% Prob": 5.895367805328634, "Min_30.0% Prob": 4.284140476474056, "Min_40.0% Prob": 3.2894569773020015, "Min_50.0% Prob": 2.6423020840156823, "Min_60.0% Prob": 2.2029800582515437}}
{"hexsha": "acf70d67c75b3c219a4cc6fc3e0736e214b7ac7f", "ext": "py", "lang": "Python", "content": "@pytest.fixture\n@timeing\n@measure_memory_usage\ndef color_image(image):\n    image.ndim = 3\n    return image", "fn_id": 2, "class_fn": false, "repo": "dxxxm/opencv_wrapper", "file": "tests/conftest.py", "last_update_at": "2021-11-24T09:24:10+00:00", "original_content": "@pytest.fixture\ndef color_image(image):\n    image.ndim = 3\n    return image", "refactored": true, "pred": {"ppl": 24.60875701904297, "ppl_lower": 24.60875701904297, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.03371686690502755, "Min_5.0% Prob": 11.037071228027344, "Min_10.0% Prob": 10.74448267618815, "Min_20.0% Prob": 10.192871252695719, "Min_30.0% Prob": 9.005565378401014, "Min_40.0% Prob": 7.381370961666107, "Min_50.0% Prob": 6.036902755498886, "Min_60.0% Prob": 5.290461395916186}}
{"hexsha": "53f7bb295a7670c88ce420c9fc2d3c662110f9c1", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef AvgPooling(pytorch_layer):\n    layer = pb2.LayerParameter()\n    layer.type = 'Pooling'\n    layer.pooling_param.pool = pb2.PoolingParameter.AVE\n    CopyPoolingParameter(pytorch_layer, layer)\n    return layer", "fn_id": 14, "class_fn": false, "repo": "UltronAI/pytorch-caffe", "file": "pytorch2caffe/ConvertLayer_caffe.py", "last_update_at": "2021-11-12T07:39:36+00:00", "original_content": "def AvgPooling(pytorch_layer):\n    layer = pb2.LayerParameter()\n    layer.type = 'Pooling'\n    layer.pooling_param.pool = pb2.PoolingParameter.AVE\n    CopyPoolingParameter(pytorch_layer, layer)\n    return layer", "refactored": true, "pred": {"ppl": 7.997162818908691, "ppl_lower": 17.90610694885254, "ppl/lowercase_ppl": -1.3876966472847294, "ppl/zlib": 0.014047883994196464, "Min_5.0% Prob": 12.628250122070312, "Min_10.0% Prob": 11.114429791768393, "Min_20.0% Prob": 8.43767422896165, "Min_30.0% Prob": 6.454709434509278, "Min_40.0% Prob": 5.218671725346492, "Min_50.0% Prob": 4.185180818041165, "Min_60.0% Prob": 3.473654754832387}}
{"hexsha": "ed1e84994c73c427aad1a29aacc2fb28de4545f1", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize('transaction_args,method_args,method_kwargs,expected,skip_testrpc', (({}, (5,), {}, {'data': '0x7cf5dab00000000000000000000000000000000000000000000000000000000000000005', 'value': 0, 'gasPrice': 1, 'chainId': None}, False), ({'gas': 800000}, (5,), {}, {'data': '0x7cf5dab00000000000000000000000000000000000000000000000000000000000000005', 'value': 0, 'gasPrice': 1, 'chainId': None}, False), ({'gasPrice': 21000000000}, (5,), {}, {'data': '0x7cf5dab00000000000000000000000000000000000000000000000000000000000000005', 'value': 0, 'gasPrice': 21000000000, 'chainId': None}, False), ({'nonce': 7}, (5,), {}, {'data': '0x7cf5dab00000000000000000000000000000000000000000000000000000000000000005', 'value': 0, 'gasPrice': 1, 'nonce': 7, 'chainId': None}, True), ({'value': 20000}, (5,), {}, {'data': '0x7cf5dab00000000000000000000000000000000000000000000000000000000000000005', 'value': 20000, 'gasPrice': 1, 'chainId': None}, False)), ids=['Standard', 'Explicit Gas', 'Explicit Gas Price', 'Explicit Nonce', 'With Value'])\n@timeing\n@measure_memory_usage\ndef test_build_transaction_with_contract_with_arguments(web3, skip_if_testrpc, math_contract, transaction_args, method_args, method_kwargs, expected, skip_testrpc, buildTransaction):\n    if skip_testrpc:\n        skip_if_testrpc(web3)\n    txn = buildTransaction(contract=math_contract, contract_function='increment', func_args=method_args, func_kwargs=method_kwargs, tx_params=transaction_args)\n    expected['to'] = math_contract.address\n    assert txn is not None\n    if 'gas' in transaction_args:\n        assert txn['gas'] == transaction_args['gas']\n    else:\n        assert 'gas' in txn\n    assert dissoc(txn, 'gas') == expected", "fn_id": 4, "class_fn": false, "repo": "onlinedeal4unow/web3.py", "file": "tests/core/contracts/test_contract_buildTransaction.py", "last_update_at": "2021-11-12T00:38:42+00:00", "original_content": "@pytest.mark.parametrize('transaction_args,method_args,method_kwargs,expected,skip_testrpc', (({}, (5,), {}, {'data': '0x7cf5dab00000000000000000000000000000000000000000000000000000000000000005', 'value': 0, 'gasPrice': 1, 'chainId': None}, False), ({'gas': 800000}, (5,), {}, {'data': '0x7cf5dab00000000000000000000000000000000000000000000000000000000000000005', 'value': 0, 'gasPrice': 1, 'chainId': None}, False), ({'gasPrice': 21000000000}, (5,), {}, {'data': '0x7cf5dab00000000000000000000000000000000000000000000000000000000000000005', 'value': 0, 'gasPrice': 21000000000, 'chainId': None}, False), ({'nonce': 7}, (5,), {}, {'data': '0x7cf5dab00000000000000000000000000000000000000000000000000000000000000005', 'value': 0, 'gasPrice': 1, 'nonce': 7, 'chainId': None}, True), ({'value': 20000}, (5,), {}, {'data': '0x7cf5dab00000000000000000000000000000000000000000000000000000000000000005', 'value': 20000, 'gasPrice': 1, 'chainId': None}, False)), ids=['Standard', 'Explicit Gas', 'Explicit Gas Price', 'Explicit Nonce', 'With Value'])\ndef test_build_transaction_with_contract_with_arguments(web3, skip_if_testrpc, math_contract, transaction_args, method_args, method_kwargs, expected, skip_testrpc, buildTransaction):\n    if skip_testrpc:\n        skip_if_testrpc(web3)\n    txn = buildTransaction(contract=math_contract, contract_function='increment', func_args=method_args, func_kwargs=method_kwargs, tx_params=transaction_args)\n    expected['to'] = math_contract.address\n    assert txn is not None\n    if 'gas' in transaction_args:\n        assert txn['gas'] == transaction_args['gas']\n    else:\n        assert 'gas' in txn\n    assert dissoc(txn, 'gas') == expected", "refactored": true, "pred": {"ppl": 1.895586609840393, "ppl_lower": 1.9404711723327637, "ppl/lowercase_ppl": -1.036593324982499, "ppl/zlib": 0.0012739608511466616, "Min_5.0% Prob": 8.275681257247925, "Min_10.0% Prob": 5.624377955496311, "Min_20.0% Prob": 3.1693759514308124, "Min_30.0% Prob": 2.1343984369511815, "Min_40.0% Prob": 1.6014043107519256, "Min_50.0% Prob": 1.2806071631669034, "Min_60.0% Prob": 1.0667566670266206}}
{"hexsha": "6f98bbabb72a1fe6a11d10ff1c78a84452e1e916", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef add_preproc_params(parser):\n    parser.add_argument('--effdir_out_preproc', required=False, help='Output directory for preproc')\n    parser.add_argument('--preproc_mask', required=False, help='Preproc processing mask file')\n    parser.add_argument('--preproc_mask_adc', required=False, help='Preproc ADC NL processing mask file')\n    parser.add_argument('--preproc_dark', dest='preproc_dark', default=False, action='store_true', help='Preprocess dark data')\n    parser.add_argument('--preproc_common', dest='preproc_common', default=False, action='store_true', help='Preprocess common data')\n    parser.add_argument('--nbin', required=False, default=10000, type=np.int, help='Number of phase bins')\n    parser.add_argument('--jump_filter_len', required=False, default=40000, type=np.int, help='Jump filter length')\n    parser.add_argument('--jump_threshold', required=False, default=5.0, type=np.float, help='Jump detection threshold')\n    parser.add_argument('--preproc_timeout', required=False, default=120, type=np.int, help='Maximum time allowed for preprocessing a ring')\n    parser.add_argument('--preproc_timeout_intermediate', required=False, default=60, type=np.int, help='Maximum time allowed for preprocessing a ring before last iteration')\n    parser.add_argument('--adc_correction', required=False, help='Full (new) NL correction file.')\n    parser.add_argument('--measure_ADC', dest='measure_ADC', default=False, action='store_true', help='Measure ADC NL')\n    parser.add_argument('--niter_ADC', default=1, type=np.int, help='Number of ADC NL iterations')\n    parser.add_argument('--delta_ADC', default=1.0, type=np.float, help='Width of ADC bin in ADU')\n    parser.add_argument('--nphase4k', required=False, default=2, type=np.int, help='Number of 4K cooler phases to measure ADC NL for.')\n    parser.add_argument('--skip_preproc', dest='skip_preproc', default=False, action='store_true', help='Do not pre-process the TOD')\n    parser.add_argument('--flag_planets', dest='flag_planets', default=False, action='store_true', help='Derive planet flags')\n    parser.add_argument('--planet_flag_radius', required=False, default=2.0, type=np.float, help='New planet flag radius (in FWHM) when --flag_planets')\n    parser.add_argument('--detmask', required=False, type=np.int, help='Detector flag mask')\n    parser.add_argument('--intense_threshold', required=False, default=10000000000.0, type=np.float, help='Intense signal threshold [K_CMB]')\n    parser.add_argument('--preproc_async_time', required=False, default=1000, type=np.int, help='Initial asynchronous processing time before load balancing')\n    parser.add_argument('--preproc_async_time_intermediate', required=False, default=800, type=np.int, help='Initial asynchronous processing time before load balancing before last iteration')\n    return", "fn_id": 5, "class_fn": false, "repo": "planck-npipe/toast-npipe", "file": "pipelines/toast_planck_reduce.py", "last_update_at": "2021-04-20T08:09:35+00:00", "original_content": "def add_preproc_params(parser):\n    parser.add_argument('--effdir_out_preproc', required=False, help='Output directory for preproc')\n    parser.add_argument('--preproc_mask', required=False, help='Preproc processing mask file')\n    parser.add_argument('--preproc_mask_adc', required=False, help='Preproc ADC NL processing mask file')\n    parser.add_argument('--preproc_dark', dest='preproc_dark', default=False, action='store_true', help='Preprocess dark data')\n    parser.add_argument('--preproc_common', dest='preproc_common', default=False, action='store_true', help='Preprocess common data')\n    parser.add_argument('--nbin', required=False, default=10000, type=np.int, help='Number of phase bins')\n    parser.add_argument('--jump_filter_len', required=False, default=40000, type=np.int, help='Jump filter length')\n    parser.add_argument('--jump_threshold', required=False, default=5.0, type=np.float, help='Jump detection threshold')\n    parser.add_argument('--preproc_timeout', required=False, default=120, type=np.int, help='Maximum time allowed for preprocessing a ring')\n    parser.add_argument('--preproc_timeout_intermediate', required=False, default=60, type=np.int, help='Maximum time allowed for preprocessing a ring before last iteration')\n    parser.add_argument('--adc_correction', required=False, help='Full (new) NL correction file.')\n    parser.add_argument('--measure_ADC', dest='measure_ADC', default=False, action='store_true', help='Measure ADC NL')\n    parser.add_argument('--niter_ADC', default=1, type=np.int, help='Number of ADC NL iterations')\n    parser.add_argument('--delta_ADC', default=1.0, type=np.float, help='Width of ADC bin in ADU')\n    parser.add_argument('--nphase4k', required=False, default=2, type=np.int, help='Number of 4K cooler phases to measure ADC NL for.')\n    parser.add_argument('--skip_preproc', dest='skip_preproc', default=False, action='store_true', help='Do not pre-process the TOD')\n    parser.add_argument('--flag_planets', dest='flag_planets', default=False, action='store_true', help='Derive planet flags')\n    parser.add_argument('--planet_flag_radius', required=False, default=2.0, type=np.float, help='New planet flag radius (in FWHM) when --flag_planets')\n    parser.add_argument('--detmask', required=False, type=np.int, help='Detector flag mask')\n    parser.add_argument('--intense_threshold', required=False, default=10000000000.0, type=np.float, help='Intense signal threshold [K_CMB]')\n    parser.add_argument('--preproc_async_time', required=False, default=1000, type=np.int, help='Initial asynchronous processing time before load balancing')\n    parser.add_argument('--preproc_async_time_intermediate', required=False, default=800, type=np.int, help='Initial asynchronous processing time before load balancing before last iteration')\n    return", "refactored": true, "pred": {"ppl": 3.3811614513397217, "ppl_lower": 3.5011396408081055, "ppl/lowercase_ppl": -1.0286231327484316, "ppl/zlib": 0.001715801795952421, "Min_5.0% Prob": 9.691679418087006, "Min_10.0% Prob": 7.767941492795944, "Min_20.0% Prob": 5.332134685160951, "Min_30.0% Prob": 3.906229549941938, "Min_40.0% Prob": 3.0206359635934743, "Min_50.0% Prob": 2.4337899079822964, "Min_60.0% Prob": 2.031148671777439}}
{"hexsha": "c0b95f623054d53abb9621aff1a8155369c99b80", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef seed_everything(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    random.seed(seed)", "fn_id": 0, "class_fn": false, "repo": "boostcampaitech2/model-optimization-level3-cv-17", "file": "train.py", "last_update_at": "2021-12-22T04:52:08+00:00", "original_content": "def seed_everything(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    random.seed(seed)", "refactored": true, "pred": {"ppl": 2.855120897293091, "ppl_lower": 3.591003179550171, "ppl/lowercase_ppl": -1.2185819382294243, "ppl/zlib": 0.007602276724806925, "Min_5.0% Prob": 10.893981297810873, "Min_10.0% Prob": 8.984284162521362, "Min_20.0% Prob": 5.3109202201549826, "Min_30.0% Prob": 3.5037358861416577, "Min_40.0% Prob": 2.700977017959723, "Min_50.0% Prob": 2.1298884732620507, "Min_60.0% Prob": 1.7572568108598716}}
{"hexsha": "5ecff52b202fb2311f75b717f10ecda7192b0a5e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef extract_cands_in_generate(type, constraints=set()):\n    cands = []\n    for t in CONCEPTS[type]:\n        if all([is_consistent(t, c) for c in constraints]) or not constraints:\n            cands.append(t)\n    return cands", "fn_id": 5, "class_fn": false, "repo": "simon555/baby-ai-game", "file": "levels/instr_gen.py", "last_update_at": "2021-11-03T15:36:56+00:00", "original_content": "def extract_cands_in_generate(type, constraints=set()):\n    cands = []\n    for t in CONCEPTS[type]:\n        if all([is_consistent(t, c) for c in constraints]) or not constraints:\n            cands.append(t)\n    return cands", "refactored": true, "pred": {"ppl": 9.374397277832031, "ppl_lower": 9.580162048339844, "ppl/lowercase_ppl": -1.009701697770219, "ppl/zlib": 0.01308761566915308, "Min_5.0% Prob": 12.029558181762695, "Min_10.0% Prob": 10.3918548311506, "Min_20.0% Prob": 8.370724746159144, "Min_30.0% Prob": 6.714134454727173, "Min_40.0% Prob": 5.439589023590088, "Min_50.0% Prob": 4.403221457391171, "Min_60.0% Prob": 3.744855313815854}}
{"hexsha": "57728a0d7226be69507552ec44dac52e55264baf", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef make_test_data():\n    \"\"\" \"\"\"\n    tgrid = np.array([0.0, 86400.0])\n    xgrid = np.linspace(-8, 8, 120)\n    ygrid = np.linspace(-7, 7, 110)\n    zgrid = np.linspace(-6, 6, 100)\n    bdip = np.array([[[ptm_dipole.dipole_field([x, y, z]) for z in zgrid] for y in ygrid] for x in xgrid])\n    ex = np.zeros([xgrid.size, ygrid.size, zgrid.size])\n    ey = np.zeros_like(ex)\n    ez = np.zeros_like(ex)\n    pf = ptm_preprocessing.PTMfields()\n    pf.set_grid(xgrid, ygrid, zgrid)\n    pf.set_magnetic(bdip[:, :, :, 0], bdip[:, :, :, 1], bdip[:, :, :, 2])\n    pf.set_electric(ex, ey, ez)\n    pf.write_file('ptm_data/ptm_fields_0001.dat')\n    pf.write_file('ptm_data/ptm_fields_0002.dat')\n    np.savetxt('ptm_data/tgrid.dat', tgrid)", "fn_id": 1, "class_fn": false, "repo": "Pheosics/SHIELDS-PTM", "file": "ptm_python/ptm_test_data.py", "last_update_at": "2021-09-20T19:58:27+00:00", "original_content": "def make_test_data():\n    \"\"\" \"\"\"\n    tgrid = np.array([0.0, 86400.0])\n    xgrid = np.linspace(-8, 8, 120)\n    ygrid = np.linspace(-7, 7, 110)\n    zgrid = np.linspace(-6, 6, 100)\n    bdip = np.array([[[ptm_dipole.dipole_field([x, y, z]) for z in zgrid] for y in ygrid] for x in xgrid])\n    ex = np.zeros([xgrid.size, ygrid.size, zgrid.size])\n    ey = np.zeros_like(ex)\n    ez = np.zeros_like(ex)\n    pf = ptm_preprocessing.PTMfields()\n    pf.set_grid(xgrid, ygrid, zgrid)\n    pf.set_magnetic(bdip[:, :, :, 0], bdip[:, :, :, 1], bdip[:, :, :, 2])\n    pf.set_electric(ex, ey, ez)\n    pf.write_file('ptm_data/ptm_fields_0001.dat')\n    pf.write_file('ptm_data/ptm_fields_0002.dat')\n    np.savetxt('ptm_data/tgrid.dat', tgrid)", "refactored": true, "pred": {"ppl": 3.1499762535095215, "ppl_lower": 3.2509818077087402, "ppl/lowercase_ppl": -1.0275076442882909, "ppl/zlib": 0.0032230194220244962, "Min_5.0% Prob": 9.342468738555908, "Min_10.0% Prob": 7.220814980566502, "Min_20.0% Prob": 5.044542099879338, "Min_30.0% Prob": 3.7174051001514354, "Min_40.0% Prob": 2.850214489377462, "Min_50.0% Prob": 2.2887860354653164, "Min_60.0% Prob": 1.9167255409157429}}
{"hexsha": "bb68959040f9d60e5d44b71c03cdbfca7f860c20", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _read(fname):\n    try:\n        return open(path.join(path.dirname(__file__), fname)).read()\n    except IOError:\n        return ''", "fn_id": 0, "class_fn": false, "repo": "inveniosoftware/flask-collect", "file": "setup.py", "last_update_at": "2021-11-15T12:08:13+00:00", "original_content": "def _read(fname):\n    try:\n        return open(path.join(path.dirname(__file__), fname)).read()\n    except IOError:\n        return ''", "refactored": true, "pred": {"ppl": 8.421010971069336, "ppl_lower": 10.12161922454834, "ppl/lowercase_ppl": -1.0863289925096853, "ppl/zlib": 0.01704583911095706, "Min_5.0% Prob": 10.128919124603271, "Min_10.0% Prob": 9.344354629516602, "Min_20.0% Prob": 7.624955177307129, "Min_30.0% Prob": 6.129489458524263, "Min_40.0% Prob": 5.22784552153419, "Min_50.0% Prob": 4.18981631371108, "Min_60.0% Prob": 3.5847362062105765}}
{"hexsha": "9f339190f0ef239c8b99606a869e09ec3bd344b0", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef top(stack):\n    if len(stack):\n        return stack[-1]\n    return None", "fn_id": 2, "class_fn": false, "repo": "suvambasak/cp", "file": "others/stack.py", "last_update_at": "2021-02-28T20:17:32+00:00", "original_content": "def top(stack):\n    if len(stack):\n        return stack[-1]\n    return None", "refactored": true, "pred": {"ppl": 13.791037559509277, "ppl_lower": 18.655834197998047, "ppl/lowercase_ppl": -1.115143984098684, "ppl/zlib": 0.030161137114577533, "Min_5.0% Prob": 11.121386528015137, "Min_10.0% Prob": 9.51709016164144, "Min_20.0% Prob": 8.199105501174927, "Min_30.0% Prob": 7.191371652815077, "Min_40.0% Prob": 6.081981162230174, "Min_50.0% Prob": 5.114631215731303, "Min_60.0% Prob": 4.351563592751821}}
{"hexsha": "a7cb7c0cbe1316b9f471be562240d8e6e68b5ee9", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_items(list_, doc_type):\n    data_table_names = {'PO': 'fss.dbo.bsPRItem', 'SO': 'fss.dbo.bsSaleOrderItem'}\n    result_dict = {}\n    item_list = []\n    try:\n        table_name = data_table_names[doc_type]\n    except KeyError as e:\n        print(f'ERROR: DocType {e} not found')\n        sys.exit()\n        return ({}, [])\n    for p in list_:\n        doc_no = p[1]\n        statement_items = f\"SELECT * FROM {table_name}                            WHERE DocNo = '{doc_no}'\"\n        cursor.execute(statement_items)\n        package = []\n        for r in cursor:\n            item_list.append(r)\n            package.append(r)\n        result_dict[r[1]] = package\n    return (result_dict, item_list)", "fn_id": 1, "class_fn": false, "repo": "adadesions/tenzing-project", "file": "tenzing-mini/tenzing_mini.py", "last_update_at": "2021-01-25T04:45:56+00:00", "original_content": "def get_items(list_, doc_type):\n    data_table_names = {'PO': 'fss.dbo.bsPRItem', 'SO': 'fss.dbo.bsSaleOrderItem'}\n    result_dict = {}\n    item_list = []\n    try:\n        table_name = data_table_names[doc_type]\n    except KeyError as e:\n        print(f'ERROR: DocType {e} not found')\n        sys.exit()\n        return ({}, [])\n    for p in list_:\n        doc_no = p[1]\n        statement_items = f\"SELECT * FROM {table_name}                            WHERE DocNo = '{doc_no}'\"\n        cursor.execute(statement_items)\n        package = []\n        for r in cursor:\n            item_list.append(r)\n            package.append(r)\n        result_dict[r[1]] = package\n    return (result_dict, item_list)", "refactored": true, "pred": {"ppl": 6.4323577880859375, "ppl_lower": 7.084023952484131, "ppl/lowercase_ppl": -1.0518448454199805, "ppl/zlib": 0.005071774268628677, "Min_5.0% Prob": 12.154056072235107, "Min_10.0% Prob": 9.927848225548153, "Min_20.0% Prob": 7.398020738647098, "Min_30.0% Prob": 5.708180129528046, "Min_40.0% Prob": 4.538057330776663, "Min_50.0% Prob": 3.689345430826472, "Min_60.0% Prob": 3.104972557979636}}
{"hexsha": "e376bb9a7337aed835807b7a81d670cdfa095d2c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef plot_values(registration_method):\n    global metric_values, multires_iterations\n    metric_values.append(registration_method.GetMetricValue())\n    clear_output(wait=True)\n    plt.plot(metric_values, 'r')\n    plt.plot(multires_iterations, [metric_values[index] for index in multires_iterations], 'b*')\n    plt.xlabel('Iteration Number', fontsize=12)\n    plt.ylabel('Metric Value', fontsize=12)\n    plt.show()", "fn_id": 10, "class_fn": false, "repo": "neurodata/ndreg", "file": "ndreg/plotter.py", "last_update_at": "2021-09-27T01:06:33+00:00", "original_content": "def plot_values(registration_method):\n    global metric_values, multires_iterations\n    metric_values.append(registration_method.GetMetricValue())\n    clear_output(wait=True)\n    plt.plot(metric_values, 'r')\n    plt.plot(multires_iterations, [metric_values[index] for index in multires_iterations], 'b*')\n    plt.xlabel('Iteration Number', fontsize=12)\n    plt.ylabel('Metric Value', fontsize=12)\n    plt.show()", "refactored": true, "pred": {"ppl": 4.010064601898193, "ppl_lower": 4.921962261199951, "ppl/lowercase_ppl": -1.1475366133221907, "ppl/zlib": 0.005960546572479659, "Min_5.0% Prob": 10.559686024983725, "Min_10.0% Prob": 8.881585637728373, "Min_20.0% Prob": 6.108782396316529, "Min_30.0% Prob": 4.503962484565941, "Min_40.0% Prob": 3.4475900506973267, "Min_50.0% Prob": 2.7958655945596194, "Min_60.0% Prob": 2.313918809965253}}
{"hexsha": "56d66414c84d9d6eef940c2d2989f3d4532fde9e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef pose_to_transformation(pose):\n    \"\"\"\n    Convert poses to transformation matrix\n    \"\"\"\n    temp_0 = pose[0]\n    temp_1 = pose[1]\n    temp_2 = pose[2]\n    temp_x = pose[3]\n    temp_y = pose[4]\n    temp_z = pose[5]\n    temp_w = pose[6]\n    pose[4:6] *= -1\n    pose[0] *= -1\n    rot_mat = quat2mat(pose[3:])\n    translation_vector = np.array([[pose[0]], [pose[1]], [pose[2]]]) / 1000\n    print(translation_vector)\n    rot_mat_2 = np.array([[0, 1, 0, 0], [-1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n    flip_x = np.eye(4)\n    flip_x[0, 0] *= -1\n    trans = flip_x @ rot_mat_2\n    translation_offset = np.ones((3, 1)) * 1\n    transformation_mat = np.vstack((np.hstack((rot_mat, translation_vector + 0.5)), np.array([0, 0, 0, 1])))\n    print(transformation_mat.shape)\n    return transformation_mat @ trans", "fn_id": 8, "class_fn": false, "repo": "RahulSajnani/DRACO-Weakly-Supervised-Dense-Reconstruction-And-Canonicalization-of-Objects", "file": "DRACO/visualization-scripts/gen_point_cloud.py", "last_update_at": "2021-12-17T16:55:00+00:00", "original_content": "def pose_to_transformation(pose):\n    \"\"\"\n    Convert poses to transformation matrix\n    \"\"\"\n    temp_0 = pose[0]\n    temp_1 = pose[1]\n    temp_2 = pose[2]\n    temp_x = pose[3]\n    temp_y = pose[4]\n    temp_z = pose[5]\n    temp_w = pose[6]\n    pose[4:6] *= -1\n    pose[0] *= -1\n    rot_mat = quat2mat(pose[3:])\n    translation_vector = np.array([[pose[0]], [pose[1]], [pose[2]]]) / 1000\n    print(translation_vector)\n    rot_mat_2 = np.array([[0, 1, 0, 0], [-1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n    flip_x = np.eye(4)\n    flip_x[0, 0] *= -1\n    trans = flip_x @ rot_mat_2\n    translation_offset = np.ones((3, 1)) * 1\n    transformation_mat = np.vstack((np.hstack((rot_mat, translation_vector + 0.5)), np.array([0, 0, 0, 1])))\n    print(transformation_mat.shape)\n    return transformation_mat @ trans", "refactored": true, "pred": {"ppl": 3.033672571182251, "ppl_lower": 3.017296314239502, "ppl/lowercase_ppl": -0.9951226238344664, "ppl/zlib": 0.003179868066633042, "Min_5.0% Prob": 9.370275497436523, "Min_10.0% Prob": 7.239553032499371, "Min_20.0% Prob": 4.896964824560917, "Min_30.0% Prob": 3.5902424809908626, "Min_40.0% Prob": 2.7670658944795528, "Min_50.0% Prob": 2.223411417617039, "Min_60.0% Prob": 1.854741078575678}}
{"hexsha": "d7f4490b643639fa5528139f79a00b5f88a06a1f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef v_bool(Inout=InOut_t.Internal_t, Default=0, varSigConst=None):\n    value = Default\n    if type(Default).__name__ == 'int':\n        Default = \"'\" + str(Default) + \"'\"\n    return v_symbol(v_type='boolean', DefaultValue=Default, Inout=Inout, includes=slv_includes, value=value, varSigConst=varSigConst, Bitwidth=1, primitive_type='boolean')", "fn_id": 0, "class_fn": false, "repo": "HardwareDesignWithPython/HDPython", "file": "HDPython/v_symbol.py", "last_update_at": "2021-10-20T20:08:16+00:00", "original_content": "def v_bool(Inout=InOut_t.Internal_t, Default=0, varSigConst=None):\n    value = Default\n    if type(Default).__name__ == 'int':\n        Default = \"'\" + str(Default) + \"'\"\n    return v_symbol(v_type='boolean', DefaultValue=Default, Inout=Inout, includes=slv_includes, value=value, varSigConst=varSigConst, Bitwidth=1, primitive_type='boolean')", "refactored": true, "pred": {"ppl": 22.070558547973633, "ppl_lower": 24.188535690307617, "ppl/lowercase_ppl": -1.0296144213281893, "ppl/zlib": 0.013280019433335384, "Min_5.0% Prob": 13.235263061523437, "Min_10.0% Prob": 11.762839837507768, "Min_20.0% Prob": 9.851347156192945, "Min_30.0% Prob": 8.262654178282794, "Min_40.0% Prob": 7.0649422044339385, "Min_50.0% Prob": 6.0814332010453205, "Min_60.0% Prob": 5.119452046743338}}
{"hexsha": "db3168c7636e669ffaa426160820cfab9e45c298", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef event(self, event):\n    if event.type() == QEvent.KeyPress and event.key() == Qt.Key_Tab:\n        self.emit(SIGNAL('tabPressed'))\n        return True", "fn_id": 3, "class_fn": false, "repo": "anjanatiha/Generative-Open-Domain-Chatbot-Application-with-Deep-Learning", "file": "code/chat_gui.py", "last_update_at": "2021-11-19T07:31:14+00:00", "original_content": "def event(self, event):\n    if event.type() == QEvent.KeyPress and event.key() == Qt.Key_Tab:\n        self.emit(SIGNAL('tabPressed'))\n        return True", "refactored": true, "pred": {"ppl": 6.9152607917785645, "ppl_lower": 17.28562355041504, "ppl/lowercase_ppl": -1.4737704579632545, "ppl/zlib": 0.013812361988062264, "Min_5.0% Prob": 10.618678569793701, "Min_10.0% Prob": 9.314088535308837, "Min_20.0% Prob": 7.7533337593078615, "Min_30.0% Prob": 6.267183176676432, "Min_40.0% Prob": 4.763175473326728, "Min_50.0% Prob": 3.90797913647615, "Min_60.0% Prob": 3.2980822137286587}}
{"hexsha": "58b7827b7aa5bd50934dcfc83e24f6888bb30df8", "ext": "py", "lang": "Python", "content": "@pytest.fixture()\n@timeing\n@measure_memory_usage\ndef enrollment_data(user):\n    \"\"\"enrollment data for testing\"\"\"\n    bootcamps = BootcampFactory.create_batch(2)\n    enrollments = BootcampRunEnrollmentFactory.create_batch(3, user=user, active=factory.Iterator([False, True, True]), bootcamp_run__bootcamp=factory.Iterator([bootcamps[0], bootcamps[0], bootcamps[1]]))\n    unenrollable_run = BootcampRunFactory.create(end_date=now_in_utc() - timedelta(days=1))\n    order = OrderFactory.create(user=user)\n    return SimpleNamespace(bootcamps=bootcamps, enrollments=enrollments, unenrollable_run=unenrollable_run, order=order)", "fn_id": 3, "class_fn": false, "repo": "mitodl/bootcamp-ecommerce", "file": "klasses/api_test.py", "last_update_at": "2021-01-06T09:51:40+00:00", "original_content": "@pytest.fixture()\ndef enrollment_data(user):\n    \"\"\"enrollment data for testing\"\"\"\n    bootcamps = BootcampFactory.create_batch(2)\n    enrollments = BootcampRunEnrollmentFactory.create_batch(3, user=user, active=factory.Iterator([False, True, True]), bootcamp_run__bootcamp=factory.Iterator([bootcamps[0], bootcamps[0], bootcamps[1]]))\n    unenrollable_run = BootcampRunFactory.create(end_date=now_in_utc() - timedelta(days=1))\n    order = OrderFactory.create(user=user)\n    return SimpleNamespace(bootcamps=bootcamps, enrollments=enrollments, unenrollable_run=unenrollable_run, order=order)", "refactored": true, "pred": {"ppl": 4.057850360870361, "ppl_lower": 5.483251094818115, "ppl/lowercase_ppl": -1.2149317113586349, "ppl/zlib": 0.004607412386941338, "Min_5.0% Prob": 12.106640285915798, "Min_10.0% Prob": 9.725153817070854, "Min_20.0% Prob": 6.389479885230193, "Min_30.0% Prob": 4.647279558398507, "Min_40.0% Prob": 3.5023197325298914, "Min_50.0% Prob": 2.7974649286398323, "Min_60.0% Prob": 2.3463768219273233}}
{"hexsha": "d07c3f5d3f8bceba3730181638eb4ed2e60e2c51", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_toplevel_elements(str_, element, parsed=None):\n    if parsed is None:\n        parsed = bs4.BeautifulSoup(str_, 'html.parser')\n    skip, result = ([], [])\n    for l in parsed(element):\n        if str(l) not in skip:\n            result.append(l)\n        skip += [str(l_nested) for l_nested in l(element)]\n    return result", "fn_id": 0, "class_fn": false, "repo": "memri/pyintegrators", "file": "pyintegrators/indexers/notelist/util.py", "last_update_at": "2021-12-06T16:25:13+00:00", "original_content": "def get_toplevel_elements(str_, element, parsed=None):\n    if parsed is None:\n        parsed = bs4.BeautifulSoup(str_, 'html.parser')\n    skip, result = ([], [])\n    for l in parsed(element):\n        if str(l) not in skip:\n            result.append(l)\n        skip += [str(l_nested) for l_nested in l(element)]\n    return result", "refactored": true, "pred": {"ppl": 7.019493103027344, "ppl_lower": 8.926270484924316, "ppl/lowercase_ppl": -1.123317478209611, "ppl/zlib": 0.009148784074575901, "Min_5.0% Prob": 11.334596824645995, "Min_10.0% Prob": 10.127010440826416, "Min_20.0% Prob": 8.063227105140687, "Min_30.0% Prob": 6.306788623332977, "Min_40.0% Prob": 4.8156167784842046, "Min_50.0% Prob": 3.9160160745183625, "Min_60.0% Prob": 3.2847268235121594}}
{"hexsha": "60908cb818b31508f23a6d5114dd56b36a1c0b97", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef factory_create_activity(name: str=None) -> Activity:\n    record = RecordFactory()\n    now = timezone.localtime()\n    start = now\n    end = now + datetime.timedelta(hours=1)\n    spent_time = (end - start).seconds\n    return Activity.objects.create(record_id=record.id, name=name, start=start, end=end, spent_time=spent_time)", "fn_id": 0, "class_fn": false, "repo": "siruku6/life_recorder", "file": "tests/test_models.py", "last_update_at": "2021-05-08T10:04:20+00:00", "original_content": "def factory_create_activity(name: str=None) -> Activity:\n    record = RecordFactory()\n    now = timezone.localtime()\n    start = now\n    end = now + datetime.timedelta(hours=1)\n    spent_time = (end - start).seconds\n    return Activity.objects.create(record_id=record.id, name=name, start=start, end=end, spent_time=spent_time)", "refactored": true, "pred": {"ppl": 5.4465484619140625, "ppl_lower": 7.529036998748779, "ppl/lowercase_ppl": -1.1910256438900773, "ppl/zlib": 0.007704464083224047, "Min_5.0% Prob": 10.069892501831054, "Min_10.0% Prob": 9.235651397705078, "Min_20.0% Prob": 6.986744201183319, "Min_30.0% Prob": 5.3381178225240395, "Min_40.0% Prob": 4.2058454650204355, "Min_50.0% Prob": 3.367430929094553, "Min_60.0% Prob": 2.8370955484889206}}
{"hexsha": "109ffa951504bc25a96cc88a0b6aa9552c14314b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _create_nat_match_obj(**kwargs):\n    nat_match_obj = {'ethertype': 'IPv4'}\n    delta = set(kwargs.keys()) - set(MATCH_KEYS)\n    if delta:\n        raise Exception(_('Invalid keys for NAT match: %s'), delta)\n    nat_match_obj.update(kwargs)\n    return nat_match_obj", "fn_id": 51, "class_fn": false, "repo": "ericwanghp/quantum", "file": "quantum/plugins/nicira/nicira_nvp_plugin/nvplib.py", "last_update_at": "2021-04-18T15:23:19+00:00", "original_content": "def _create_nat_match_obj(**kwargs):\n    nat_match_obj = {'ethertype': 'IPv4'}\n    delta = set(kwargs.keys()) - set(MATCH_KEYS)\n    if delta:\n        raise Exception(_('Invalid keys for NAT match: %s'), delta)\n    nat_match_obj.update(kwargs)\n    return nat_match_obj", "refactored": true, "pred": {"ppl": 7.4968719482421875, "ppl_lower": 7.923337936401367, "ppl/lowercase_ppl": -1.0274644338681393, "ppl/zlib": 0.009638688325240119, "Min_5.0% Prob": 13.166837453842163, "Min_10.0% Prob": 10.887543784247505, "Min_20.0% Prob": 8.307678831948174, "Min_30.0% Prob": 6.308159775204128, "Min_40.0% Prob": 4.965604464213054, "Min_50.0% Prob": 4.035538362132178, "Min_60.0% Prob": 3.381950996540211}}
{"hexsha": "3db72119b5d317902b6f4df26947aae91390d2a3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef VAE(inputs, input_lengths, filters, kernel_size, strides, num_units, is_training, scope):\n    with tf.variable_scope(scope):\n        outputs = ReferenceEncoder(inputs=inputs, input_lengths=input_lengths, filters=filters, kernel_size=kernel_size, strides=strides, is_training=is_training)\n        mu = tf.layers.dense(outputs, num_units, name='mean', activation=tf.nn.relu)\n        log_var = tf.layers.dense(outputs, num_units, name='vari', activation=tf.nn.relu)\n        std = tf.exp(log_var * 0.5)\n        z = tf.random_normal(shape=[tf.shape(mu)[0], num_units], mean=0.0, stddev=1.0)\n        output = mu + z * std\n        style_embeddings = tf.layers.dense(output, hp.encoder_depth)\n        return (style_embeddings, mu, log_var)", "fn_id": 0, "class_fn": false, "repo": "GlitteringAu/vae_tacotron", "file": "models/modules.py", "last_update_at": "2021-09-02T06:04:46+00:00", "original_content": "def VAE(inputs, input_lengths, filters, kernel_size, strides, num_units, is_training, scope):\n    with tf.variable_scope(scope):\n        outputs = ReferenceEncoder(inputs=inputs, input_lengths=input_lengths, filters=filters, kernel_size=kernel_size, strides=strides, is_training=is_training)\n        mu = tf.layers.dense(outputs, num_units, name='mean', activation=tf.nn.relu)\n        log_var = tf.layers.dense(outputs, num_units, name='vari', activation=tf.nn.relu)\n        std = tf.exp(log_var * 0.5)\n        z = tf.random_normal(shape=[tf.shape(mu)[0], num_units], mean=0.0, stddev=1.0)\n        output = mu + z * std\n        style_embeddings = tf.layers.dense(output, hp.encoder_depth)\n        return (style_embeddings, mu, log_var)", "refactored": true, "pred": {"ppl": 3.534794330596924, "ppl_lower": 3.5439369678497314, "ppl/lowercase_ppl": -1.002045792375723, "ppl/zlib": 0.0036283193010866327, "Min_5.0% Prob": 11.976905129172586, "Min_10.0% Prob": 8.740988441135572, "Min_20.0% Prob": 5.93069876254873, "Min_30.0% Prob": 4.183130652971671, "Min_40.0% Prob": 3.1554344251359763, "Min_50.0% Prob": 2.5244485131472727, "Min_60.0% Prob": 2.116180197716179}}
{"hexsha": "8f760a85301ff2c85907784935bbde2796fc3fc2", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_current_commit_id() -> str:\n    \"\"\"Get current commit id\n\n    Returns:\n        str: Current commit id\n    \"\"\"\n    command = 'git rev-parse HEAD'\n    commit_id = subprocess.check_output(command.split()).strip().decode('utf-8')\n    return commit_id", "fn_id": 7, "class_fn": false, "repo": "shagunsodhani/torch-template", "file": "src/utils/utils.py", "last_update_at": "2021-01-12T10:02:16+00:00", "original_content": "def get_current_commit_id() -> str:\n    \"\"\"Get current commit id\n\n    Returns:\n        str: Current commit id\n    \"\"\"\n    command = 'git rev-parse HEAD'\n    commit_id = subprocess.check_output(command.split()).strip().decode('utf-8')\n    return commit_id", "refactored": true, "pred": {"ppl": 3.6268539428710938, "ppl_lower": 4.309873104095459, "ppl/lowercase_ppl": -1.1339238432933807, "ppl/zlib": 0.0067102374483633655, "Min_5.0% Prob": 9.517089207967123, "Min_10.0% Prob": 7.435742582593646, "Min_20.0% Prob": 5.510123904546102, "Min_30.0% Prob": 4.080528277417888, "Min_40.0% Prob": 3.239748775959015, "Min_50.0% Prob": 2.5951220252011953, "Min_60.0% Prob": 2.151197569930683}}
{"hexsha": "a1f55cf8e4ae2523b3711b8a9b769203a9a8a01e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef search(keywords, max_results=None):\n    url = 'https://html.duckduckgo.com/html/'\n    params = {'q': keywords}\n    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:86.0) Gecko/20100101 Firefox/86.0'}\n    yielded = 0\n    while True:\n        res = requests.post(url, data=params, headers=headers)\n        doc = html.fromstring(res.text)\n        results = [a.get('href') for a in doc.cssselect('#links .links_main a')]\n        for result in results:\n            yield result\n            time.sleep(0.1)\n            yielded += 1\n            if max_results and yielded >= max_results:\n                return\n        try:\n            form = doc.cssselect('.results_links_more form')[-1]\n        except IndexError:\n            return\n        params = dict(form.fields)", "fn_id": 0, "class_fn": false, "repo": "Jcorb08/programming-support-skill", "file": "duckduckgo.py", "last_update_at": "2021-05-26T11:08:27+00:00", "original_content": "def search(keywords, max_results=None):\n    url = 'https://html.duckduckgo.com/html/'\n    params = {'q': keywords}\n    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:86.0) Gecko/20100101 Firefox/86.0'}\n    yielded = 0\n    while True:\n        res = requests.post(url, data=params, headers=headers)\n        doc = html.fromstring(res.text)\n        results = [a.get('href') for a in doc.cssselect('#links .links_main a')]\n        for result in results:\n            yield result\n            time.sleep(0.1)\n            yielded += 1\n            if max_results and yielded >= max_results:\n                return\n        try:\n            form = doc.cssselect('.results_links_more form')[-1]\n        except IndexError:\n            return\n        params = dict(form.fields)", "refactored": true, "pred": {"ppl": 2.695141077041626, "ppl_lower": 3.244199514389038, "ppl/lowercase_ppl": -1.1870169784385656, "ppl/zlib": 0.002268765562213281, "Min_5.0% Prob": 9.921785874800248, "Min_10.0% Prob": 7.037791957025942, "Min_20.0% Prob": 4.51416302741842, "Min_30.0% Prob": 3.2582673711436136, "Min_40.0% Prob": 2.468713250566036, "Min_50.0% Prob": 1.9898200776014063, "Min_60.0% Prob": 1.6523204819445418}}
{"hexsha": "b97e6f9295654f19c6ba8f5fa580dfd29d44c69e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef edit_seller_selector(brief, sellers_to_invite):\n    if brief.lot.slug != 'atm' and sellers_to_invite:\n        seller_selector = brief.data.get('sellerSelector', '')\n        if len(sellers_to_invite.keys()) > 0 and seller_selector and (seller_selector == 'oneSeller'):\n            brief.data['sellerSelector'] = 'someSellers'", "fn_id": 5, "class_fn": false, "repo": "ArenaNetworks/dto-digitalmarketplace-api", "file": "app/api/business/brief/brief_edit_business.py", "last_update_at": "2021-08-23T06:05:06+00:00", "original_content": "def edit_seller_selector(brief, sellers_to_invite):\n    if brief.lot.slug != 'atm' and sellers_to_invite:\n        seller_selector = brief.data.get('sellerSelector', '')\n        if len(sellers_to_invite.keys()) > 0 and seller_selector and (seller_selector == 'oneSeller'):\n            brief.data['sellerSelector'] = 'someSellers'", "refactored": true, "pred": {"ppl": 8.120282173156738, "ppl_lower": 9.336545944213867, "ppl/lowercase_ppl": -1.0666414273688707, "ppl/zlib": 0.01126002636536864, "Min_5.0% Prob": 13.36648769378662, "Min_10.0% Prob": 10.74866361618042, "Min_20.0% Prob": 7.712375561396281, "Min_30.0% Prob": 6.035643704235554, "Min_40.0% Prob": 5.023799246265774, "Min_50.0% Prob": 4.157201515053803, "Min_60.0% Prob": 3.4880780302919447}}
{"hexsha": "299d845b4ecb3a12712dbe607958f777e7c89d76", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef dms2dec(dms_str):\n    \"\"\"Return decimal representation of DMS\n    \n    >>> dms2dec(utf8(48\u00b053'10.18\"N))\n    48.8866111111F\n    \n    >>> dms2dec(utf8(2\u00b020'35.09\"E))\n    2.34330555556F\n    \n    >>> dms2dec(utf8(48\u00b053'10.18\"S))\n    -48.8866111111F\n    \n    >>> dms2dec(utf8(2\u00b020'35.09\"W))\n    -2.34330555556F\n    \n    \"\"\"\n    dms_str = re.sub('\\\\s', '', dms_str)\n    sign = -1 if re.search('[swSW]', dms_str) else 1\n    numbers = list(filter(len, re.split('\\\\D+', dms_str, maxsplit=4)))\n    degree = numbers[0]\n    minute = numbers[1] if len(numbers) >= 2 else '0'\n    second = numbers[2] if len(numbers) >= 3 else '0'\n    frac_seconds = numbers[3] if len(numbers) >= 4 else '0'\n    second += '.' + frac_seconds\n    return sign * (int(degree) + float(minute) / 60 + float(second) / 3600)", "fn_id": 1, "class_fn": false, "repo": "Joel-hanson/Iceberg-locations", "file": "iceberg.py", "last_update_at": "2021-08-17T08:21:01+00:00", "original_content": "def dms2dec(dms_str):\n    \"\"\"Return decimal representation of DMS\n    \n    >>> dms2dec(utf8(48\u00b053'10.18\"N))\n    48.8866111111F\n    \n    >>> dms2dec(utf8(2\u00b020'35.09\"E))\n    2.34330555556F\n    \n    >>> dms2dec(utf8(48\u00b053'10.18\"S))\n    -48.8866111111F\n    \n    >>> dms2dec(utf8(2\u00b020'35.09\"W))\n    -2.34330555556F\n    \n    \"\"\"\n    dms_str = re.sub('\\\\s', '', dms_str)\n    sign = -1 if re.search('[swSW]', dms_str) else 1\n    numbers = list(filter(len, re.split('\\\\D+', dms_str, maxsplit=4)))\n    degree = numbers[0]\n    minute = numbers[1] if len(numbers) >= 2 else '0'\n    second = numbers[2] if len(numbers) >= 3 else '0'\n    frac_seconds = numbers[3] if len(numbers) >= 4 else '0'\n    second += '.' + frac_seconds\n    return sign * (int(degree) + float(minute) / 60 + float(second) / 3600)", "refactored": true, "pred": {"ppl": 2.7790939807891846, "ppl_lower": 3.0178027153015137, "ppl/lowercase_ppl": -1.0806202989012892, "ppl/zlib": 0.0026411497891282707, "Min_5.0% Prob": 10.031054777257582, "Min_10.0% Prob": 7.224325786318098, "Min_20.0% Prob": 4.689287318310267, "Min_30.0% Prob": 3.3684237034253353, "Min_40.0% Prob": 2.546728412240324, "Min_50.0% Prob": 2.048798710786564, "Min_60.0% Prob": 1.703362364848822}}
{"hexsha": "40cc7483e6638afc906172fe0592b4a30fce27e5", "ext": "py", "lang": "Python", "content": "@dp.message_handler(commands=['creds'])\n@timeing\n@measure_memory_usage\nasync def handler_creds(message: types.Message, state: FSMContext):\n    username = await db.get_username(message.from_user['id'])\n    if username is None:\n        await message.answer('You have to be registered to view creds! Use /reg')\n        return\n    username = username.decode()\n    password = (await db.get_secret(message.from_user['id'])).decode()\n    await message.answer(f'Username: {username}\\nPassword: {password}')", "fn_id": 5, "class_fn": false, "repo": "ne-bknn/CSSH", "file": "bot/bot.py", "last_update_at": "2021-05-12T19:38:55+00:00", "original_content": "@dp.message_handler(commands=['creds'])\nasync def handler_creds(message: types.Message, state: FSMContext):\n    username = await db.get_username(message.from_user['id'])\n    if username is None:\n        await message.answer('You have to be registered to view creds! Use /reg')\n        return\n    username = username.decode()\n    password = (await db.get_secret(message.from_user['id'])).decode()\n    await message.answer(f'Username: {username}\\nPassword: {password}')", "refactored": true, "pred": {"ppl": 4.379208087921143, "ppl_lower": 6.929024696350098, "ppl/lowercase_ppl": -1.3106920793101489, "ppl/zlib": 0.005469881134063863, "Min_5.0% Prob": 12.20445473988851, "Min_10.0% Prob": 9.365106435922476, "Min_20.0% Prob": 6.5228451031904955, "Min_30.0% Prob": 4.754042880657392, "Min_40.0% Prob": 3.6203687871402166, "Min_50.0% Prob": 2.948086118833585, "Min_60.0% Prob": 2.4787559512106676}}
{"hexsha": "13e50b7266dceea05106c82bd6e330375bd50ee2", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef timeline(request):\n    ts = Timeline.objects.all()\n    contaxt = {'ts': ts}\n    return render(request, 'other/timeline.html', contaxt)", "fn_id": 1, "class_fn": false, "repo": "jackyfzh/j_django_blog", "file": "other/views.py", "last_update_at": "2021-05-22T10:57:45+00:00", "original_content": "def timeline(request):\n    ts = Timeline.objects.all()\n    contaxt = {'ts': ts}\n    return render(request, 'other/timeline.html', contaxt)", "refactored": true, "pred": {"ppl": 9.749285697937012, "ppl_lower": 11.88258171081543, "ppl/lowercase_ppl": -1.0868962344385933, "ppl/zlib": 0.017251469852847975, "Min_5.0% Prob": 11.253743171691895, "Min_10.0% Prob": 10.93293113708496, "Min_20.0% Prob": 8.893823480606079, "Min_30.0% Prob": 7.112797896067302, "Min_40.0% Prob": 5.620270454883576, "Min_50.0% Prob": 4.591173419952392, "Min_60.0% Prob": 3.853710344682137}}
{"hexsha": "b167197574c1ae8ea60f5374ce76c84554e825b9", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef upgrade_rdr():\n    op.execute('ALTER TABLE `workbench_researcher` MODIFY `degree` JSON;')\n    op.execute('ALTER TABLE `workbench_researcher_history` MODIFY `degree` JSON;')\n    op.execute('ALTER TABLE `workbench_researcher` MODIFY `ethnicity` smallint(6);')\n    op.execute('ALTER TABLE `workbench_researcher_history` MODIFY `ethnicity` smallint(6);')", "fn_id": 0, "class_fn": false, "repo": "all-of-us/raw-data-repository", "file": "rdr_service/alembic/versions/01e685241414_change_degree_for_workbench_researcher__.py", "last_update_at": "2021-09-24T16:58:21+00:00", "original_content": "def upgrade_rdr():\n    op.execute('ALTER TABLE `workbench_researcher` MODIFY `degree` JSON;')\n    op.execute('ALTER TABLE `workbench_researcher_history` MODIFY `degree` JSON;')\n    op.execute('ALTER TABLE `workbench_researcher` MODIFY `ethnicity` smallint(6);')\n    op.execute('ALTER TABLE `workbench_researcher_history` MODIFY `ethnicity` smallint(6);')", "refactored": true, "pred": {"ppl": 4.32364559173584, "ppl_lower": 4.286853790283203, "ppl/lowercase_ppl": -0.9941630640529067, "ppl/zlib": 0.009093782194779444, "Min_5.0% Prob": 10.712566375732422, "Min_10.0% Prob": 8.779824516989969, "Min_20.0% Prob": 6.433544093912298, "Min_30.0% Prob": 4.672483250950322, "Min_40.0% Prob": 3.6146407713266937, "Min_50.0% Prob": 2.9220057969743554, "Min_60.0% Prob": 2.4391197513534943}}
{"hexsha": "c5d6e28510e9bfcfa523c2d73057ed4afa27c4d5", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _test_overlap(hits):\n    tester = np.zeros(len(hits), dtype=strax.time_fields)\n    tester['time'] = hits['time'] - (hits['left_integration'] - hits['left']) * hits['dt']\n    tester['endtime'] = hits['time'] + (hits['right_integration'] - hits['left']) * hits['dt']\n    for ch in np.unique(hits['channel']):\n        mask = hits['channel'] == ch\n        test_ch = np.all(tester[mask]['endtime'][:-1] - tester[mask]['time'][1:] <= 0)\n        assert np.all(test_ch), 'Hits overlap!'", "fn_id": 1, "class_fn": false, "repo": "RiceAstroparticleLab/strax", "file": "tests/test_lone_hit_integration.py", "last_update_at": "2021-11-16T18:20:27+00:00", "original_content": "def _test_overlap(hits):\n    tester = np.zeros(len(hits), dtype=strax.time_fields)\n    tester['time'] = hits['time'] - (hits['left_integration'] - hits['left']) * hits['dt']\n    tester['endtime'] = hits['time'] + (hits['right_integration'] - hits['left']) * hits['dt']\n    for ch in np.unique(hits['channel']):\n        mask = hits['channel'] == ch\n        test_ch = np.all(tester[mask]['endtime'][:-1] - tester[mask]['time'][1:] <= 0)\n        assert np.all(test_ch), 'Hits overlap!'", "refactored": true, "pred": {"ppl": 5.5735650062561035, "ppl_lower": 5.687180519104004, "ppl/lowercase_ppl": -1.0117458173656115, "ppl/zlib": 0.006790651724239797, "Min_5.0% Prob": 10.922416806221008, "Min_10.0% Prob": 9.018821626901627, "Min_20.0% Prob": 6.8854745849967, "Min_30.0% Prob": 5.4168919796744985, "Min_40.0% Prob": 4.285812791436911, "Min_50.0% Prob": 3.4260709137644296, "Min_60.0% Prob": 2.86711917197351}}
{"hexsha": "65850953a74773d8a1f36d8cdb7ca6cda7bf01af", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef message(flag=None):\n    exc_type, exc_obj, exc_tb = sys.exc_info()\n    if flag is None:\n        return exc_obj.message\n    return traceback.extract_tb(exc_tb)[0][3]", "fn_id": 10, "class_fn": false, "repo": "wroldwiedbwe/vfp2py", "file": "vfp2py/vfpfunc.py", "last_update_at": "2021-12-11T18:31:23+00:00", "original_content": "def message(flag=None):\n    exc_type, exc_obj, exc_tb = sys.exc_info()\n    if flag is None:\n        return exc_obj.message\n    return traceback.extract_tb(exc_tb)[0][3]", "refactored": true, "pred": {"ppl": 7.229671478271484, "ppl_lower": 8.702312469482422, "ppl/lowercase_ppl": -1.0937194394502896, "ppl/zlib": 0.013737455530891183, "Min_5.0% Prob": 10.997073491414389, "Min_10.0% Prob": 10.086547215779623, "Min_20.0% Prob": 7.386950126061072, "Min_30.0% Prob": 5.9462415419126815, "Min_40.0% Prob": 4.760947493406443, "Min_50.0% Prob": 3.9834392312914133, "Min_60.0% Prob": 3.2921323197392316}}
{"hexsha": "c1e6e718242cba1e86b4e900ac601b58a5f8729f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef count_byte_values(bss: Iterable[ByteString]) -> list[Counter[int]]:\n    \"\"\"Returns a list of Counters, each of which records the values of the\n    bytes at the corresponding index in the given ByteStrings.\"\"\"\n    counts_for_idx: list[Counter[int]] = []\n    bs_len = None\n    for bs in bss:\n        if bs_len is None:\n            bs_len = len(bs)\n            counts_for_idx = [Counter() for _ in range(bs_len)]\n        assert len(bs) == bs_len\n        for i, b in enumerate(bs):\n            counts_for_idx[i][b] += 1\n    return counts_for_idx", "fn_id": 2, "class_fn": false, "repo": "misterfifths/nis_mods", "file": "utils/mining.py", "last_update_at": "2021-10-18T13:42:09+00:00", "original_content": "def count_byte_values(bss: Iterable[ByteString]) -> list[Counter[int]]:\n    \"\"\"Returns a list of Counters, each of which records the values of the\n    bytes at the corresponding index in the given ByteStrings.\"\"\"\n    counts_for_idx: list[Counter[int]] = []\n    bs_len = None\n    for bs in bss:\n        if bs_len is None:\n            bs_len = len(bs)\n            counts_for_idx = [Counter() for _ in range(bs_len)]\n        assert len(bs) == bs_len\n        for i, b in enumerate(bs):\n            counts_for_idx[i][b] += 1\n    return counts_for_idx", "refactored": true, "pred": {"ppl": 3.770409107208252, "ppl_lower": 4.450997829437256, "ppl/lowercase_ppl": -1.1250353012928471, "ppl/zlib": 0.004380143604202285, "Min_5.0% Prob": 11.155575394630432, "Min_10.0% Prob": 9.192663729190826, "Min_20.0% Prob": 6.178652759641409, "Min_30.0% Prob": 4.361139087044463, "Min_40.0% Prob": 3.3332877856034497, "Min_50.0% Prob": 2.6520129090583904, "Min_60.0% Prob": 2.220816397866025}}
{"hexsha": "f3e180cb426f2d4f4869990002b54890e11a8ca1", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef exponential(db, utility, eps=1e-05):\n    \"\"\"\n    Goal:\n        The Exponential mechanism is a DP method for answering categorical queries,\n        by sampling from an exponential distribution over possible choices.\n    Guaranties:\n        1. (eps,0) Differential Privacy\n        2. Accuracy:\n           Let utility: N ^ |Universe| x categories -> R\n           Denote c* = exponential(db, utility, eps)\n                  s = utility.sensitivity\n                  R = utility.categories\n                  Opt(u,x) = max_{c in R} u(x,r)\n           Then forall t > 0\n                  P[u(x,c*) <= Opt(u,x) - (2s / epsilon) (ln(|R|) + t)] <= e^-t\n    \"\"\"\n    assert isinstance(db, Database)\n    assert isinstance(utility, Utility)\n    assert db.rep == 'histogram'\n    assert eps > 0\n    evals = np.array([utility.value(db, cat) for cat in utility.categories])\n    consts = eps / (2 * utility.sensitivity)\n    weights = np.exp(consts * evals)\n    res = np.random.choice(utility.categories, p=normalize(weights, ord=1))\n    return res", "fn_id": 1, "class_fn": false, "repo": "chuxuantinh/differential-privacy-ct", "file": "mechanism/basic.py", "last_update_at": "2021-04-17T04:55:59+00:00", "original_content": "def exponential(db, utility, eps=1e-05):\n    \"\"\"\n    Goal:\n        The Exponential mechanism is a DP method for answering categorical queries,\n        by sampling from an exponential distribution over possible choices.\n    Guaranties:\n        1. (eps,0) Differential Privacy\n        2. Accuracy:\n           Let utility: N ^ |Universe| x categories -> R\n           Denote c* = exponential(db, utility, eps)\n                  s = utility.sensitivity\n                  R = utility.categories\n                  Opt(u,x) = max_{c in R} u(x,r)\n           Then forall t > 0\n                  P[u(x,c*) <= Opt(u,x) - (2s / epsilon) (ln(|R|) + t)] <= e^-t\n    \"\"\"\n    assert isinstance(db, Database)\n    assert isinstance(utility, Utility)\n    assert db.rep == 'histogram'\n    assert eps > 0\n    evals = np.array([utility.value(db, cat) for cat in utility.categories])\n    consts = eps / (2 * utility.sensitivity)\n    weights = np.exp(consts * evals)\n    res = np.random.choice(utility.categories, p=normalize(weights, ord=1))\n    return res", "refactored": true, "pred": {"ppl": 14.041475296020508, "ppl_lower": 15.990340232849121, "ppl/lowercase_ppl": -1.049193252177882, "ppl/zlib": 0.005051654820538223, "Min_5.0% Prob": 11.216399465288434, "Min_10.0% Prob": 10.15653028159306, "Min_20.0% Prob": 8.48542595731801, "Min_30.0% Prob": 7.178590256592323, "Min_40.0% Prob": 6.031983917129451, "Min_50.0% Prob": 5.069036589909906, "Min_60.0% Prob": 4.347266290528434}}
{"hexsha": "748d69ab544fa46cb8d58415b5d688a6b34e3694", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef map_dists(dists: Dict[str, AbstractMessage], values: Dict[str, np.ndarray], _call: str='logpdf') -> Iterator[Tuple[str, np.ndarray]]:\n    \"\"\"\n    Calls a method (default: logpdf) for each Message in dists\n    on the corresponding value in values\n    \"\"\"\n    for v in dists.keys() & values.keys():\n        dist = dists[v]\n        if isinstance(dist, AbstractMessage):\n            yield (v, getattr(dist, _call)(values[v]))", "fn_id": 0, "class_fn": false, "repo": "arfon/PyAutoFit", "file": "autofit/graphical/messages/__init__.py", "last_update_at": "2021-01-18T23:20:31+00:00", "original_content": "def map_dists(dists: Dict[str, AbstractMessage], values: Dict[str, np.ndarray], _call: str='logpdf') -> Iterator[Tuple[str, np.ndarray]]:\n    \"\"\"\n    Calls a method (default: logpdf) for each Message in dists\n    on the corresponding value in values\n    \"\"\"\n    for v in dists.keys() & values.keys():\n        dist = dists[v]\n        if isinstance(dist, AbstractMessage):\n            yield (v, getattr(dist, _call)(values[v]))", "refactored": true, "pred": {"ppl": 6.476142883300781, "ppl_lower": 7.525058746337891, "ppl/lowercase_ppl": -1.0803551745099294, "ppl/zlib": 0.007049528675786946, "Min_5.0% Prob": 9.968142986297607, "Min_10.0% Prob": 8.981964031855265, "Min_20.0% Prob": 7.049145698547363, "Min_30.0% Prob": 5.563109115550392, "Min_40.0% Prob": 4.539632207155227, "Min_50.0% Prob": 3.7159125478494737, "Min_60.0% Prob": 3.1138866244766272}}
{"hexsha": "85ecf5da466f0df65d97156a925dd2cbb9cc304a", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef predict_labels_datasets(weight0, weight1, weight23, data, transform_x, degree):\n    \"\"\" Generate the predictions given the weigth of the data set with num jet 0, 1  or {2,3} \"\"\"\n    ids = np.arange(data.shape[0])\n    tx_0, tx_1, tx_23 = transform_x(data, degree)\n    ids0 = ids[data[:, 22] == 0]\n    y_pred0 = np.dot(tx_0, weight0)\n    ids1 = ids[data[:, 22] == 1]\n    y_pred1 = np.dot(tx_1, weight1)\n    ids23 = ids[data[:, 22] > 1]\n    y_pred23 = np.dot(tx_23, weight23)\n    y_pred = np.concatenate((np.concatenate((y_pred0, y_pred1), axis=None), y_pred23), axis=None)\n    ids = np.concatenate((np.concatenate((ids0, ids1), axis=None), ids23), axis=None)\n    y = np.transpose(np.array([ids, y_pred]))\n    y = y[y[:, 0].argsort()][:, 1]\n    y[np.where(y <= 0)] = -1\n    y[np.where(y > 0)] = 1\n    return y", "fn_id": 2, "class_fn": false, "repo": "yannvon/higgs-boson", "file": "scripts/split.py", "last_update_at": "2021-11-01T12:55:40+00:00", "original_content": "def predict_labels_datasets(weight0, weight1, weight23, data, transform_x, degree):\n    \"\"\" Generate the predictions given the weigth of the data set with num jet 0, 1  or {2,3} \"\"\"\n    ids = np.arange(data.shape[0])\n    tx_0, tx_1, tx_23 = transform_x(data, degree)\n    ids0 = ids[data[:, 22] == 0]\n    y_pred0 = np.dot(tx_0, weight0)\n    ids1 = ids[data[:, 22] == 1]\n    y_pred1 = np.dot(tx_1, weight1)\n    ids23 = ids[data[:, 22] > 1]\n    y_pred23 = np.dot(tx_23, weight23)\n    y_pred = np.concatenate((np.concatenate((y_pred0, y_pred1), axis=None), y_pred23), axis=None)\n    ids = np.concatenate((np.concatenate((ids0, ids1), axis=None), ids23), axis=None)\n    y = np.transpose(np.array([ids, y_pred]))\n    y = y[y[:, 0].argsort()][:, 1]\n    y[np.where(y <= 0)] = -1\n    y[np.where(y > 0)] = 1\n    return y", "refactored": true, "pred": {"ppl": 3.498335123062134, "ppl_lower": 3.661841869354248, "ppl/lowercase_ppl": -1.0364765275018926, "ppl/zlib": 0.003244267295852482, "Min_5.0% Prob": 10.437289535999298, "Min_10.0% Prob": 8.078308163267193, "Min_20.0% Prob": 5.634973789328959, "Min_30.0% Prob": 4.068324316256117, "Min_40.0% Prob": 3.1233192288608693, "Min_50.0% Prob": 2.5066799723232784, "Min_60.0% Prob": 2.088280167628891}}
{"hexsha": "9681618928c5641d65d743ec3954a60d579c7fcb", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef bezierSliceFromTo(points, minParam, maxParam):\n    fromP = bezierPointAt(points, minParam)\n    fromT = bezierTangentAt(points, minParam)\n    toP = bezierPointAt(points, maxParam)\n    toT = bezierTangentAt(points, maxParam)\n    paramDiff = maxParam - minParam\n    return [fromP, fromP + fromT * paramDiff, toP - toT * paramDiff, toP]", "fn_id": 15, "class_fn": false, "repo": "calculusrobotics/RNNs-for-Bayesian-State-Estimation", "file": "Blender 2.91/2.91/scripts/addons/curve_tools/internal.py", "last_update_at": "2021-06-30T00:39:40+00:00", "original_content": "def bezierSliceFromTo(points, minParam, maxParam):\n    fromP = bezierPointAt(points, minParam)\n    fromT = bezierTangentAt(points, minParam)\n    toP = bezierPointAt(points, maxParam)\n    toT = bezierTangentAt(points, maxParam)\n    paramDiff = maxParam - minParam\n    return [fromP, fromP + fromT * paramDiff, toP - toT * paramDiff, toP]", "refactored": true, "pred": {"ppl": 4.1668877601623535, "ppl_lower": 4.718620777130127, "ppl/lowercase_ppl": -1.0871285010295748, "ppl/zlib": 0.008395114215713819, "Min_5.0% Prob": 11.259636878967285, "Min_10.0% Prob": 9.504667715592818, "Min_20.0% Prob": 6.659723048624785, "Min_30.0% Prob": 4.709600438390459, "Min_40.0% Prob": 3.6202942724137204, "Min_50.0% Prob": 2.878396045994656, "Min_60.0% Prob": 2.3853338663175236}}
{"hexsha": "1bafe6e69cd8c367d829bb77f059218e660a5475", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef computeStarsItembased(corated, target_bid, model):\n    \"\"\"\n    corated - {bid: star, ...}\n    \"\"\"\n    if corated == None:\n        return None\n    corated.pop(target_bid, None)\n    bid_cor = list(corated.keys())\n    collect = []\n    for b in bid_cor:\n        pair = None\n        if b < target_bid:\n            pair = (b, target_bid)\n        else:\n            pair = (target_bid, b)\n        w = model.get(pair)\n        if w != None:\n            collect.append((pair, w, b))\n    collect.sort(key=lambda x: x[1], reverse=True)\n    neighbors = collect[:N_NEIGHBORS_ITEMBASED]\n    sum_w = 0\n    n = 0\n    for p, w, b in neighbors:\n        star = corated[b]\n        n += star * w\n        sum_w += w\n    if sum_w == 0:\n        return None\n    else:\n        return n / sum_w", "fn_id": 2, "class_fn": false, "repo": "maple1eaf/data_mining_inf553", "file": "assignment/assignment3/python/task3/task3predict_dev.py", "last_update_at": "2021-05-04T05:17:57+00:00", "original_content": "def computeStarsItembased(corated, target_bid, model):\n    \"\"\"\n    corated - {bid: star, ...}\n    \"\"\"\n    if corated == None:\n        return None\n    corated.pop(target_bid, None)\n    bid_cor = list(corated.keys())\n    collect = []\n    for b in bid_cor:\n        pair = None\n        if b < target_bid:\n            pair = (b, target_bid)\n        else:\n            pair = (target_bid, b)\n        w = model.get(pair)\n        if w != None:\n            collect.append((pair, w, b))\n    collect.sort(key=lambda x: x[1], reverse=True)\n    neighbors = collect[:N_NEIGHBORS_ITEMBASED]\n    sum_w = 0\n    n = 0\n    for p, w, b in neighbors:\n        star = corated[b]\n        n += star * w\n        sum_w += w\n    if sum_w == 0:\n        return None\n    else:\n        return n / sum_w", "refactored": true, "pred": {"ppl": 4.8960065841674805, "ppl_lower": 5.7083964347839355, "ppl/lowercase_ppl": -1.0966484118200142, "ppl/zlib": 0.004191081503996227, "Min_5.0% Prob": 11.332526048024496, "Min_10.0% Prob": 9.233919421831766, "Min_20.0% Prob": 6.809703393858307, "Min_30.0% Prob": 5.064430639550492, "Min_40.0% Prob": 3.944786365543093, "Min_50.0% Prob": 3.1737459707187443, "Min_60.0% Prob": 2.6468377724868826}}
{"hexsha": "e3a8b073f0f64789f2b3c0b201ed953476be70ca", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_quarter(month):\n    if month in ['January', 'April', 'July', 'October']:\n        return True\n    else:\n        return False", "fn_id": 2, "class_fn": false, "repo": "Mindhome/field_service", "file": "mindhome_alpha/erpnext/quality_management/doctype/quality_review/quality_review.py", "last_update_at": "2021-04-29T14:55:29+00:00", "original_content": "def get_quarter(month):\n    if month in ['January', 'April', 'July', 'October']:\n        return True\n    else:\n        return False", "refactored": true, "pred": {"ppl": 8.18301773071289, "ppl_lower": 10.401055335998535, "ppl/lowercase_ppl": -1.1141005315684684, "ppl/zlib": 0.016551661404071258, "Min_5.0% Prob": 10.786260604858398, "Min_10.0% Prob": 10.048863410949707, "Min_20.0% Prob": 8.398354530334473, "Min_30.0% Prob": 6.817823847134908, "Min_40.0% Prob": 5.258882894235499, "Min_50.0% Prob": 4.295710808819249, "Min_60.0% Prob": 3.61325134716928}}
{"hexsha": "c5b9d51ce8759bd19c31258626e669df3a3907b6", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef bias_variable(shape, bias=0.01):\n    \"\"\"Function to initialize the bias. For ReLUs, it MUST be > 0.0\n\n    :param shape: the shape of the bias variable\n    :param val: the value of the bias variable\n    \"\"\"\n    initial = tf.constant(bias, shape=shape)\n    return tf.Variable(initial)", "fn_id": 1, "class_fn": false, "repo": "yxw027/mmWave-localization-learning", "file": "bff_positioning/models/layer_functions.py", "last_update_at": "2021-05-29T20:28:23+00:00", "original_content": "def bias_variable(shape, bias=0.01):\n    \"\"\"Function to initialize the bias. For ReLUs, it MUST be > 0.0\n\n    :param shape: the shape of the bias variable\n    :param val: the value of the bias variable\n    \"\"\"\n    initial = tf.constant(bias, shape=shape)\n    return tf.Variable(initial)", "refactored": true, "pred": {"ppl": 7.069021701812744, "ppl_lower": 8.035490989685059, "ppl/lowercase_ppl": -1.0655236276645972, "ppl/zlib": 0.009778610485908317, "Min_5.0% Prob": 12.365195751190186, "Min_10.0% Prob": 9.908863597446018, "Min_20.0% Prob": 7.542445368236965, "Min_30.0% Prob": 5.885168786402102, "Min_40.0% Prob": 4.744759983486599, "Min_50.0% Prob": 3.876217319899135, "Min_60.0% Prob": 3.2503306127532765}}
{"hexsha": "5f30e702c766d9408e95e8e3b793cedd534e612e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_colorscale_values(cmap):\n    \"\"\"Get the colors composing a plotly colorscale.\n\n    Parameter\n    ---------\n    cmap : str\n        Name of the Plotly colorscale\n\n    Returns\n    -------\n    colorscale : array_like\n        Colors associated to the colormap\n    \"\"\"\n    import plotly\n    rev = '_r' if '_r' in cmap.lower() else ''\n    cmap = cmap.lower().replace('_r', '')\n    colorscales = plotly.colors.named_colorscales()\n    assert cmap in colorscales\n    ensembles = ['sequential', 'diverging', 'qualitative']\n    for e in ensembles:\n        cmaps = dir(eval(f'plotly.colors.{e}'))\n        cmaps_lower = [c.lower() for c in cmaps]\n        if cmap in cmaps_lower:\n            cmap_idx = cmaps_lower.index(cmap)\n            return eval(f'plotly.colors.{e}.{cmaps[cmap_idx]}{rev}')\n    assert ValueError(f'{cmap} is not a predefined colorscale {colorscales}')", "fn_id": 0, "class_fn": false, "repo": "brainets/netchos", "file": "netchos/utils/colors.py", "last_update_at": "2021-11-17T15:18:33+00:00", "original_content": "def get_colorscale_values(cmap):\n    \"\"\"Get the colors composing a plotly colorscale.\n\n    Parameter\n    ---------\n    cmap : str\n        Name of the Plotly colorscale\n\n    Returns\n    -------\n    colorscale : array_like\n        Colors associated to the colormap\n    \"\"\"\n    import plotly\n    rev = '_r' if '_r' in cmap.lower() else ''\n    cmap = cmap.lower().replace('_r', '')\n    colorscales = plotly.colors.named_colorscales()\n    assert cmap in colorscales\n    ensembles = ['sequential', 'diverging', 'qualitative']\n    for e in ensembles:\n        cmaps = dir(eval(f'plotly.colors.{e}'))\n        cmaps_lower = [c.lower() for c in cmaps]\n        if cmap in cmaps_lower:\n            cmap_idx = cmaps_lower.index(cmap)\n            return eval(f'plotly.colors.{e}.{cmaps[cmap_idx]}{rev}')\n    assert ValueError(f'{cmap} is not a predefined colorscale {colorscales}')", "refactored": true, "pred": {"ppl": 4.270056247711182, "ppl_lower": 4.6178297996521, "ppl/lowercase_ppl": -1.0539379981775727, "ppl/zlib": 0.003456254761713947, "Min_5.0% Prob": 10.417069435119629, "Min_10.0% Prob": 8.60523291428884, "Min_20.0% Prob": 6.298841471574744, "Min_30.0% Prob": 4.709481327501062, "Min_40.0% Prob": 3.6145524300482808, "Min_50.0% Prob": 2.896991016343236, "Min_60.0% Prob": 2.427640075953741}}
{"hexsha": "5ea5ff02510da4eb6075ea3e0e4184f4d733ca2e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef detectFacesByRekognition(image_binary: bytes) -> List[List[float]]:\n    client = boto3.client('rekognition')\n    response = client.detect_faces(Image={'Bytes': image_binary}, Attributes=['ALL'])\n    faces = list()\n    for face_info in response['FaceDetails']:\n        faces.append(face_info['BoundingBox'])\n        print(face_info['BoundingBox'])\n    return faces", "fn_id": 1, "class_fn": false, "repo": "p1ass/emojic.ch", "file": "lambda/detect_face.py", "last_update_at": "2021-08-12T04:24:43+00:00", "original_content": "def detectFacesByRekognition(image_binary: bytes) -> List[List[float]]:\n    client = boto3.client('rekognition')\n    response = client.detect_faces(Image={'Bytes': image_binary}, Attributes=['ALL'])\n    faces = list()\n    for face_info in response['FaceDetails']:\n        faces.append(face_info['BoundingBox'])\n        print(face_info['BoundingBox'])\n    return faces", "refactored": true, "pred": {"ppl": 4.498692989349365, "ppl_lower": 6.8384175300598145, "ppl/lowercase_ppl": -1.2784765844617685, "ppl/zlib": 0.0061630610974474495, "Min_5.0% Prob": 11.069679069519044, "Min_10.0% Prob": 9.014500617980957, "Min_20.0% Prob": 6.6993592262268065, "Min_30.0% Prob": 4.8358857516319524, "Min_40.0% Prob": 3.757953710672332, "Min_50.0% Prob": 2.9957199546580133, "Min_60.0% Prob": 2.519849611386176}}
{"hexsha": "4a4bfa3d96580a5d40c5b2e5e994ece0ab585163", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_event_graph_accumulated_time_before_start(mocker: Any) -> None:\n    message = DeferredMessage(MyMessage, 'unittest_args', kwargs_field='unittest_kwargs')\n    topic = Topic(MyMessage)\n    start = Event(message, topic, 0.0, 1.0)\n    graph = EventGraph(start)\n    parent = Event(message, topic, 0.0, 1.0)\n    child = Event(message, topic, -3.0, 1.0)\n    graph.add_event_at_end(parent, start)\n    with pytest.raises(LabGraphError):\n        graph.add_event_at_end(child, parent)", "fn_id": 10, "class_fn": false, "repo": "mofe64/labgraph", "file": "labgraph/events/tests/test_event_generator.py", "last_update_at": "2021-08-01T06:31:08+00:00", "original_content": "def test_event_graph_accumulated_time_before_start(mocker: Any) -> None:\n    message = DeferredMessage(MyMessage, 'unittest_args', kwargs_field='unittest_kwargs')\n    topic = Topic(MyMessage)\n    start = Event(message, topic, 0.0, 1.0)\n    graph = EventGraph(start)\n    parent = Event(message, topic, 0.0, 1.0)\n    child = Event(message, topic, -3.0, 1.0)\n    graph.add_event_at_end(parent, start)\n    with pytest.raises(LabGraphError):\n        graph.add_event_at_end(child, parent)", "refactored": true, "pred": {"ppl": 7.853204727172852, "ppl_lower": 10.297468185424805, "ppl/lowercase_ppl": -1.1314830954451789, "ppl/zlib": 0.007777062996168009, "Min_5.0% Prob": 11.407822728157043, "Min_10.0% Prob": 10.038767695426941, "Min_20.0% Prob": 7.8571547305945195, "Min_30.0% Prob": 6.099942324161529, "Min_40.0% Prob": 4.889375981110246, "Min_50.0% Prob": 4.043068069787252, "Min_60.0% Prob": 3.418537851902518}}
{"hexsha": "4ccaae78da187b459e1b8de6f377e49ceb0a7fea", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _get_files_from_arcs(extension, arc_list=None, arc_path=None):\n    use_concurrency = False\n    if arc_path:\n        arc_list = find_files(arc_path, '.arc')\n    elif arc_list:\n        arc_list = arc_list\n    else:\n        arc_list = ARC_FILES\n    to_export = [arc_file for arc_file in arc_list if arc_file not in CACHE_ARC]\n    if to_export and use_concurrency:\n        concurrent_unpack(arc_list, CACHE_ARC, extension)\n    elif to_export and (not use_concurrency):\n        for arc_file in to_export:\n            _unpack_arc_in_temp(arc_file, CACHE_ARC)\n    files, ids = _get_files_and_ids(extension, arc_list)\n    return (files, ids)", "fn_id": 0, "class_fn": false, "repo": "BlenderCN-Org/albam", "file": "tests/mtframework/conftest.py", "last_update_at": "2021-01-05T22:58:18+00:00", "original_content": "def _get_files_from_arcs(extension, arc_list=None, arc_path=None):\n    use_concurrency = False\n    if arc_path:\n        arc_list = find_files(arc_path, '.arc')\n    elif arc_list:\n        arc_list = arc_list\n    else:\n        arc_list = ARC_FILES\n    to_export = [arc_file for arc_file in arc_list if arc_file not in CACHE_ARC]\n    if to_export and use_concurrency:\n        concurrent_unpack(arc_list, CACHE_ARC, extension)\n    elif to_export and (not use_concurrency):\n        for arc_file in to_export:\n            _unpack_arc_in_temp(arc_file, CACHE_ARC)\n    files, ids = _get_files_and_ids(extension, arc_list)\n    return (files, ids)", "refactored": true, "pred": {"ppl": 5.388730049133301, "ppl_lower": 5.634693622589111, "ppl/lowercase_ppl": -1.0264993009520136, "ppl/zlib": 0.005728944710048458, "Min_5.0% Prob": 10.528752708435059, "Min_10.0% Prob": 8.953453612327575, "Min_20.0% Prob": 6.586307659381774, "Min_30.0% Prob": 5.136788510885395, "Min_40.0% Prob": 4.090497672557831, "Min_50.0% Prob": 3.337601787256963, "Min_60.0% Prob": 2.8155394837381396}}
{"hexsha": "66c3929472e5beb4f365dc128305de4970e9e7a7", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef extract_sentences_and_labels_from_files(files):\n    sentences = []\n    labels = []\n    for infile in files:\n        with open(infile) as file:\n            for line in file:\n                sentence, label = parse_infile_line(line)\n                labels.append(label)\n                sentences.append(sentence)\n            file.close()\n    return (sentences, labels)", "fn_id": 1, "class_fn": false, "repo": "arielrodrigues/nlp-relations-extraction-ptbr", "file": "input-data-preparation/utils.py", "last_update_at": "2021-02-09T16:27:10+00:00", "original_content": "def extract_sentences_and_labels_from_files(files):\n    sentences = []\n    labels = []\n    for infile in files:\n        with open(infile) as file:\n            for line in file:\n                sentence, label = parse_infile_line(line)\n                labels.append(label)\n                sentences.append(sentence)\n            file.close()\n    return (sentences, labels)", "refactored": true, "pred": {"ppl": 4.17502498626709, "ppl_lower": 4.17502498626709, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.007366599706286303, "Min_5.0% Prob": 10.3422212600708, "Min_10.0% Prob": 8.656579441494411, "Min_20.0% Prob": 6.509333027733697, "Min_30.0% Prob": 4.81715636783176, "Min_40.0% Prob": 3.576578997679659, "Min_50.0% Prob": 2.884750359610695, "Min_60.0% Prob": 2.4156231455335564}}
{"hexsha": "0924022c1119a245a7cbca668d233e67f604810f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_shifted_3D(tau, X, Y, Z, cam_left, cam_top, traj_left, traj_top, timespan):\n    corr_top, corr_left = shift_cam_coord(timespan, traj_top, traj_left, tau)\n    len_traj = len(corr_top)\n    x = np.zeros(np.shape(X)) * np.nan\n    y = np.zeros(np.shape(Y)) * np.nan\n    z = np.zeros(np.shape(Z)) * np.nan\n    for i in range(0, len_traj):\n        if not (np.isnan(X[i]) or np.isnan(Y[i]) or np.isnan(Z[i])):\n            A, B = make_system_mat(cam_top, cam_left, corr_left[i, :], corr_top[i, :])\n            x[i], y[i], z[i] = np.linalg.solve(np.matrix(A), np.matrix(B).T)\n    return (x, y, z, corr_top, corr_left)", "fn_id": 12, "class_fn": false, "repo": "simonBreumier/3Deye", "file": "data_treat/reconstruction_3d.py", "last_update_at": "2021-06-05T07:51:00+00:00", "original_content": "def get_shifted_3D(tau, X, Y, Z, cam_left, cam_top, traj_left, traj_top, timespan):\n    corr_top, corr_left = shift_cam_coord(timespan, traj_top, traj_left, tau)\n    len_traj = len(corr_top)\n    x = np.zeros(np.shape(X)) * np.nan\n    y = np.zeros(np.shape(Y)) * np.nan\n    z = np.zeros(np.shape(Z)) * np.nan\n    for i in range(0, len_traj):\n        if not (np.isnan(X[i]) or np.isnan(Y[i]) or np.isnan(Z[i])):\n            A, B = make_system_mat(cam_top, cam_left, corr_left[i, :], corr_top[i, :])\n            x[i], y[i], z[i] = np.linalg.solve(np.matrix(A), np.matrix(B).T)\n    return (x, y, z, corr_top, corr_left)", "refactored": true, "pred": {"ppl": 3.513568878173828, "ppl_lower": 3.656703472137451, "ppl/lowercase_ppl": -1.0317752103795583, "ppl/zlib": 0.004002013679367189, "Min_5.0% Prob": 9.847604458148663, "Min_10.0% Prob": 8.203838201669546, "Min_20.0% Prob": 5.626075719887355, "Min_30.0% Prob": 4.084585814913617, "Min_40.0% Prob": 3.1153620011963934, "Min_50.0% Prob": 2.5184667472812263, "Min_60.0% Prob": 2.0938424302598917}}
{"hexsha": "f441d0b6f5533ef1211ad198d6e281b4230affcd", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_class4_ex4():\n    base_path = '../class4/exercises/exercise4'\n    cmd_list = ['ansible-playbook', 'exercise4.yml']\n    std_out, std_err, return_code = subprocess_runner(cmd_list, exercise_dir=base_path)\n    assert std_err == ''\n    assert return_code == 0", "fn_id": 4, "class_fn": false, "repo": "kinther/ansible_course", "file": "tests/test_class4.py", "last_update_at": "2021-05-24T01:58:08+00:00", "original_content": "def test_class4_ex4():\n    base_path = '../class4/exercises/exercise4'\n    cmd_list = ['ansible-playbook', 'exercise4.yml']\n    std_out, std_err, return_code = subprocess_runner(cmd_list, exercise_dir=base_path)\n    assert std_err == ''\n    assert return_code == 0", "refactored": true, "pred": {"ppl": 6.499391555786133, "ppl_lower": 6.499391555786133, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.009748482113114937, "Min_5.0% Prob": 9.575757026672363, "Min_10.0% Prob": 8.60789230134752, "Min_20.0% Prob": 7.138174533843994, "Min_30.0% Prob": 5.8406671153174505, "Min_40.0% Prob": 4.569082406727043, "Min_50.0% Prob": 3.740505590710951, "Min_60.0% Prob": 3.149688573046164}}
{"hexsha": "f2310222e3119de7c2f2b3c8746814477e78d8d7", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef class_add_student(student_id, student_name):\n    \"\"\"\n    \u7528\u4e8e\u6dfb\u52a0\u5b66\u751f\n    \"\"\"\n    cur.execute(f\"insert into student_list values ({student_id},'{student_name}')\")\n    conn.commit()", "fn_id": 1, "class_fn": false, "repo": "XieJianCheng/ClassAdminSystem", "file": "module_new/class_admin_n.py", "last_update_at": "2021-11-11T12:41:09+00:00", "original_content": "def class_add_student(student_id, student_name):\n    \"\"\"\n    \u7528\u4e8e\u6dfb\u52a0\u5b66\u751f\n    \"\"\"\n    cur.execute(f\"insert into student_list values ({student_id},'{student_name}')\")\n    conn.commit()", "refactored": true, "pred": {"ppl": 7.0009684562683105, "ppl_lower": 7.0009684562683105, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.011447344061065786, "Min_5.0% Prob": 10.308099428812662, "Min_10.0% Prob": 9.249728838602701, "Min_20.0% Prob": 7.193026423454285, "Min_30.0% Prob": 5.754398518138462, "Min_40.0% Prob": 4.581636681556701, "Min_50.0% Prob": 3.848808798097795, "Min_60.0% Prob": 3.2869596259819494}}
{"hexsha": "78f066607cb84040dfcf6fa504ecdea051a0dfbc", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_calculate_raises_with_no_active_phases_passed():\n    \"\"\"Passing inactive phases to calculate() raises a ConditionError.\"\"\"\n    with pytest.raises(ConditionError):\n        calculate(ALFE_DBF, ['AL', 'VA'], ['AL13FE4'], T=1200, P=101325)", "fn_id": 3, "class_fn": false, "repo": "dschwen/pycalphad", "file": "pycalphad/tests/test_calculate.py", "last_update_at": "2021-07-19T14:25:43+00:00", "original_content": "def test_calculate_raises_with_no_active_phases_passed():\n    \"\"\"Passing inactive phases to calculate() raises a ConditionError.\"\"\"\n    with pytest.raises(ConditionError):\n        calculate(ALFE_DBF, ['AL', 'VA'], ['AL13FE4'], T=1200, P=101325)", "refactored": true, "pred": {"ppl": 17.589107513427734, "ppl_lower": 22.028722763061523, "ppl/lowercase_ppl": -1.0784950813990248, "ppl/zlib": 0.014481211207925896, "Min_5.0% Prob": 12.294562816619873, "Min_10.0% Prob": 11.409148216247559, "Min_20.0% Prob": 9.544931355644675, "Min_30.0% Prob": 7.932067238367521, "Min_40.0% Prob": 6.588895784105573, "Min_50.0% Prob": 5.546045444228432, "Min_60.0% Prob": 4.733211345267746}}
{"hexsha": "3546a7402fd9a0f1b6c55afe3610cdd62558560c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef __get_stock_bar(code, start, end, freq):\n    df = None\n    _start = start.format()\n    _end = end.format()\n    if freq == util.FREQ_DAY or freq == util.FREQ_WEEK or freq == util.FREQ_MONTH:\n        covert = {util.FREQ_DAY: 'D', util.FREQ_WEEK: 'W', util.FREQ_MONTH: 'M'}\n        df = ts.get_stock_bar(code=code, start=_start, end=_end, freq=covert[freq], factors=['vr', 'tor'])\n    else:\n        covert = {util.FREQ_1M: '1MIN', util.FREQ_5M: '5MIN', util.FREQ_15M: '15MIN', util.FREQ_30M: '30MIN'}\n        df = ts.tushare_bar(code=code, start=_start, end=_end, freq=covert[freq], factors=['vr', 'tor'])\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError('df is unknown type %s' % type(df))\n    elif df.empty:\n        log.warn('%s from %s to %s bar[%s] is empty' % (code, start, end, freq))\n    return df", "fn_id": 3, "class_fn": false, "repo": "lijielife/carp", "file": "carp/request.py", "last_update_at": "2021-03-02T15:48:57+00:00", "original_content": "def __get_stock_bar(code, start, end, freq):\n    df = None\n    _start = start.format()\n    _end = end.format()\n    if freq == util.FREQ_DAY or freq == util.FREQ_WEEK or freq == util.FREQ_MONTH:\n        covert = {util.FREQ_DAY: 'D', util.FREQ_WEEK: 'W', util.FREQ_MONTH: 'M'}\n        df = ts.get_stock_bar(code=code, start=_start, end=_end, freq=covert[freq], factors=['vr', 'tor'])\n    else:\n        covert = {util.FREQ_1M: '1MIN', util.FREQ_5M: '5MIN', util.FREQ_15M: '15MIN', util.FREQ_30M: '30MIN'}\n        df = ts.tushare_bar(code=code, start=_start, end=_end, freq=covert[freq], factors=['vr', 'tor'])\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError('df is unknown type %s' % type(df))\n    elif df.empty:\n        log.warn('%s from %s to %s bar[%s] is empty' % (code, start, end, freq))\n    return df", "refactored": true, "pred": {"ppl": 3.298616409301758, "ppl_lower": 3.437391757965088, "ppl/lowercase_ppl": -1.0345284925457563, "ppl/zlib": 0.0031161961113464046, "Min_5.0% Prob": 10.815905809402466, "Min_10.0% Prob": 8.500687204558274, "Min_20.0% Prob": 5.567446714740688, "Min_30.0% Prob": 3.93877341439215, "Min_40.0% Prob": 2.9881476808120224, "Min_50.0% Prob": 2.3930578060423167, "Min_60.0% Prob": 1.9931504642955085}}
{"hexsha": "8644f9b2a8f01c213caf5bcbd4d123b465cabeeb", "ext": "py", "lang": "Python", "content": "@leet2git.command()\n@click.option('--source-repository', '-s', default='', help='the path to the folder where the code will be saved')\n@click.option('--language', '-l', default='python3', help='the default language')\n@click.option('--soft/--hard', default=True, help='A soft reset only erases the database. A hard reset also erase the files.')\n@click.pass_obj\n@timeing\n@measure_memory_usage\ndef reset(cm: ConfigManager, source_repository: str, language: str, soft: bool):\n    \"\"\"Reset the configuration file\n    \\x0c\n    Args:\n        source_repository (str, optional): the path to the folder where the code will be saved.\n            Defaults to \"\".\n        language (str, optional): the default language. Defaults to \"python3\".\n        soft(bool, optional): the reset hardness. Defaults to soft.\n    \"\"\"\n    if not soft:\n        try:\n            click.confirm(f\"This will delete EVERY solution and test file inside                     the {cm.config['source_path']} folder.                      Still want to proceed?\", abort=True)\n        except Abort:\n            return\n        file_list = glob.glob(os.path.join(cm.config['source_path'], 'src', 'leetcode_*')) + glob.glob(os.path.join(cm.config['source_path'], 'tests', 'test_*'))\n        for file in file_list:\n            try:\n                os.remove(file)\n            except FileNotFoundError as e:\n                click.secho(e.args)\n    else:\n        try:\n            click.confirm('This will delete the question database. Still want to proceed?', abort=True)\n        except Abort:\n            return\n    reset_config(cm, source_repository, language)\n    cm.load_config()\n    qdb = QuestionDB(cm.config)\n    qdb.reset()\n    if not soft:\n        data = QuestionData(language=cm.config['language'])\n        file_handler = create_file_handler(data, cm.config)\n        file_handler.generate_repo(cm.config['source_path'])", "fn_id": 7, "class_fn": false, "repo": "sungho-joo/leetcode2github", "file": "src/leet2git/leet2git.py", "last_update_at": "2021-05-07T08:26:47+00:00", "original_content": "@leet2git.command()\n@click.option('--source-repository', '-s', default='', help='the path to the folder where the code will be saved')\n@click.option('--language', '-l', default='python3', help='the default language')\n@click.option('--soft/--hard', default=True, help='A soft reset only erases the database. A hard reset also erase the files.')\n@click.pass_obj\ndef reset(cm: ConfigManager, source_repository: str, language: str, soft: bool):\n    \"\"\"Reset the configuration file\n    \\x0c\n    Args:\n        source_repository (str, optional): the path to the folder where the code will be saved.\n            Defaults to \"\".\n        language (str, optional): the default language. Defaults to \"python3\".\n        soft(bool, optional): the reset hardness. Defaults to soft.\n    \"\"\"\n    if not soft:\n        try:\n            click.confirm(f\"This will delete EVERY solution and test file inside                     the {cm.config['source_path']} folder.                      Still want to proceed?\", abort=True)\n        except Abort:\n            return\n        file_list = glob.glob(os.path.join(cm.config['source_path'], 'src', 'leetcode_*')) + glob.glob(os.path.join(cm.config['source_path'], 'tests', 'test_*'))\n        for file in file_list:\n            try:\n                os.remove(file)\n            except FileNotFoundError as e:\n                click.secho(e.args)\n    else:\n        try:\n            click.confirm('This will delete the question database. Still want to proceed?', abort=True)\n        except Abort:\n            return\n    reset_config(cm, source_repository, language)\n    cm.load_config()\n    qdb = QuestionDB(cm.config)\n    qdb.reset()\n    if not soft:\n        data = QuestionData(language=cm.config['language'])\n        file_handler = create_file_handler(data, cm.config)\n        file_handler.generate_repo(cm.config['source_path'])", "refactored": true, "pred": {"ppl": 4.510154724121094, "ppl_lower": 5.045348644256592, "ppl/lowercase_ppl": -1.0744426452418663, "ppl/zlib": 0.0021008806971082048, "Min_5.0% Prob": 11.196052219556725, "Min_10.0% Prob": 8.931587127929038, "Min_20.0% Prob": 6.295951579479461, "Min_30.0% Prob": 4.7205130743308805, "Min_40.0% Prob": 3.7112910709683855, "Min_50.0% Prob": 2.9967603904788502, "Min_60.0% Prob": 2.510700087692641}}
{"hexsha": "d1aed9946c2cb489d711ae8a1fe207359b86da6f", "ext": "py", "lang": "Python", "content": "def buildStartEndBreakProb(fit, nSegments, tag):\n\n    @timeing\n    @measure_memory_usage\n    def sebBuilder(s):\n        return ee.String(s).cat('_' + tag)\n    segmentTag = buildSegmentTag(nSegments).map(sebBuilder)\n    zeros = ee.Array(0).repeat(0, nSegments)\n    magImg = fit.select(tag).arrayCat(zeros, 0).float().arraySlice(0, 0, nSegments)\n    return magImg.arrayFlatten([segmentTag])", "fn_id": 5, "class_fn": false, "repo": "rjsheperd/gee-gateway", "file": "gee/ccdc.py", "last_update_at": "2021-03-19T19:51:49+00:00", "original_content": "def buildStartEndBreakProb(fit, nSegments, tag):\n\n    def sebBuilder(s):\n        return ee.String(s).cat('_' + tag)\n    segmentTag = buildSegmentTag(nSegments).map(sebBuilder)\n    zeros = ee.Array(0).repeat(0, nSegments)\n    magImg = fit.select(tag).arrayCat(zeros, 0).float().arraySlice(0, 0, nSegments)\n    return magImg.arrayFlatten([segmentTag])", "refactored": true, "pred": {"ppl": 16.154796600341797, "ppl_lower": 21.43305778503418, "ppl/lowercase_ppl": -1.1016158972707875, "ppl/zlib": 0.01194084553085455, "Min_5.0% Prob": 12.071602980295816, "Min_10.0% Prob": 10.963527202606201, "Min_20.0% Prob": 9.412875751654306, "Min_30.0% Prob": 7.5832940273814735, "Min_40.0% Prob": 6.354231357574463, "Min_50.0% Prob": 5.384870529174805, "Min_60.0% Prob": 4.594404570344421}}
{"hexsha": "b735786ce4c39a03aae76e4daf78a4867d561247", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_subtraction_with_other_types():\n    price1 = Money(40, 'EUR')\n    price2 = Money(80, 'EUR')\n    price_range = MoneyRange(price1, price2)\n    with pytest.raises(TypeError):\n        price_range - 1", "fn_id": 6, "class_fn": false, "repo": "anton-shestakov/prices", "file": "tests/test_money_range.py", "last_update_at": "2021-11-17T14:38:52+00:00", "original_content": "def test_subtraction_with_other_types():\n    price1 = Money(40, 'EUR')\n    price2 = Money(80, 'EUR')\n    price_range = MoneyRange(price1, price2)\n    with pytest.raises(TypeError):\n        price_range - 1", "refactored": true, "pred": {"ppl": 6.513468265533447, "ppl_lower": 8.831731796264648, "ppl/lowercase_ppl": -1.162486570980243, "ppl/zlib": 0.011935490917432994, "Min_5.0% Prob": 10.700480779012045, "Min_10.0% Prob": 9.031860147203718, "Min_20.0% Prob": 6.738049125671386, "Min_30.0% Prob": 5.571303768591448, "Min_40.0% Prob": 4.440271763006846, "Min_50.0% Prob": 3.722482391305872, "Min_60.0% Prob": 3.1097118927372827}}
{"hexsha": "1869c8f32f7f9bd4afb8cbf0bd42e3c02a87fce7", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef Vector4CrossProduct(vectorLeft, vectorRight):\n    assert isinstance(vectorRight, Vector4)\n    v = Vector4()\n    v.x = vectorLeft.y * vectorRight.z - vectorLeft.z * vectorRight.y\n    v.y = vectorLeft.z * vectorRight.x - vectorLeft.x * vectorRight.z\n    v.z = vectorLeft.x * vectorRight.y - vectorLeft.y * vectorRight.x\n    v.w = 0\n    return v", "fn_id": 11, "class_fn": false, "repo": "pome-ta/draftPythonistaScripts", "file": "simd/vector4.py", "last_update_at": "2021-08-05T04:31:02+00:00", "original_content": "def Vector4CrossProduct(vectorLeft, vectorRight):\n    assert isinstance(vectorRight, Vector4)\n    v = Vector4()\n    v.x = vectorLeft.y * vectorRight.z - vectorLeft.z * vectorRight.y\n    v.y = vectorLeft.z * vectorRight.x - vectorLeft.x * vectorRight.z\n    v.z = vectorLeft.x * vectorRight.y - vectorLeft.y * vectorRight.x\n    v.w = 0\n    return v", "refactored": true, "pred": {"ppl": 2.587960958480835, "ppl_lower": 2.759579658508301, "ppl/lowercase_ppl": -1.0675255922610996, "ppl/zlib": 0.005464771786954793, "Min_5.0% Prob": 9.552156925201416, "Min_10.0% Prob": 7.935271978378296, "Min_20.0% Prob": 4.646331697702408, "Min_30.0% Prob": 3.1628517931741147, "Min_40.0% Prob": 2.3766973128028135, "Min_50.0% Prob": 1.9016927326039876, "Min_60.0% Prob": 1.5847729943546558}}
{"hexsha": "6780c64eb55bdd7710a2ca7590d41b28234c58e4", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef polyClipboard(*args, **kwargs):\n    \"\"\"\n    The command allows the user to copy and paste certain polygonal attributes to a clipboard. These attributes are:  1)\n    Shader (shading engine) assignment.  2) Texture coordinate (UV) assignment.  3) Color value assignment. Any combination\n    of attributes can be chosen for the copy or paste operation. If the attribute has not been copied to the clipboard, then\n    naturally it cannot be pasted from the clipboard. The copy option will copy the attribute assignments from a single\n    source polygonal dag object or polygon component. If the source does not have the either UV or color attributes, then\n    nothing will be copied to the clipboard. The paste option will paste the attribute assignments to one or more polygon\n    components or polygonal dag objects. If the destination does not have either UV or color attributes, then new values\n    will be assigned as needed. Additionally, there is the option to clear the clipboard contents\n    \n    Flags:\n      - clear : cl                     (bool)          [create]\n          When used, will mean to clear the specified attribute argument(s).\n    \n      - color : clr                    (bool)          [create]\n          When used, will be to copy or paste color attributes\n    \n      - copy : cp                      (bool)          [create]\n          When used, will mean to copy the specified attribute argument(s).\n    \n      - paste : ps                     (bool)          [create]\n          When used, will mean to paste the specified attribute argument(s).\n    \n      - shader : sh                    (bool)          [create]\n          When used, will be to copy or paste shader attributes\n    \n      - uvCoordinates : uv             (bool)          [create]\n          When used, will be to copy or paste texture coordinate attributes                                  Flag can have\n          multiple arguments, passed either as a tuple or a list.\n    \n    \n    Derived from mel command `maya.cmds.polyClipboard`\n    \"\"\"\n    pass", "fn_id": 220, "class_fn": false, "repo": "FXTD-ODYSSEY/vscode-mayapy", "file": "mayaSDK/pymel/core/modeling.py", "last_update_at": "2021-12-26T06:56:16+00:00", "original_content": "def polyClipboard(*args, **kwargs):\n    \"\"\"\n    The command allows the user to copy and paste certain polygonal attributes to a clipboard. These attributes are:  1)\n    Shader (shading engine) assignment.  2) Texture coordinate (UV) assignment.  3) Color value assignment. Any combination\n    of attributes can be chosen for the copy or paste operation. If the attribute has not been copied to the clipboard, then\n    naturally it cannot be pasted from the clipboard. The copy option will copy the attribute assignments from a single\n    source polygonal dag object or polygon component. If the source does not have the either UV or color attributes, then\n    nothing will be copied to the clipboard. The paste option will paste the attribute assignments to one or more polygon\n    components or polygonal dag objects. If the destination does not have either UV or color attributes, then new values\n    will be assigned as needed. Additionally, there is the option to clear the clipboard contents\n    \n    Flags:\n      - clear : cl                     (bool)          [create]\n          When used, will mean to clear the specified attribute argument(s).\n    \n      - color : clr                    (bool)          [create]\n          When used, will be to copy or paste color attributes\n    \n      - copy : cp                      (bool)          [create]\n          When used, will mean to copy the specified attribute argument(s).\n    \n      - paste : ps                     (bool)          [create]\n          When used, will mean to paste the specified attribute argument(s).\n    \n      - shader : sh                    (bool)          [create]\n          When used, will be to copy or paste shader attributes\n    \n      - uvCoordinates : uv             (bool)          [create]\n          When used, will be to copy or paste texture coordinate attributes                                  Flag can have\n          multiple arguments, passed either as a tuple or a list.\n    \n    \n    Derived from mel command `maya.cmds.polyClipboard`\n    \"\"\"\n    pass", "refactored": true, "pred": {"ppl": 6.271817207336426, "ppl_lower": 6.526154041290283, "ppl/lowercase_ppl": -1.0216504530635473, "ppl/zlib": 0.0027000972623308127, "Min_5.0% Prob": 12.348945379257202, "Min_10.0% Prob": 9.675145091080084, "Min_20.0% Prob": 7.030731704176926, "Min_30.0% Prob": 5.504004457132603, "Min_40.0% Prob": 4.415992269792208, "Min_50.0% Prob": 3.628249080152046, "Min_60.0% Prob": 3.0502379132601303}}
{"hexsha": "ce10512207649c9489cbc69c74570eefc9c463ff", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_data_int():\n    X = np.array([[2, 20, 3], [4, 50, 6], [7, 80, 9], [10, 110, 12]])\n    y_int = np.array([1, 2, 3, 4])\n    return (X, y_int)", "fn_id": 0, "class_fn": false, "repo": "gradientzero/dq0-sdk", "file": "tests/test_pipeline/test_pipeline.py", "last_update_at": "2021-03-18T21:26:29+00:00", "original_content": "def get_data_int():\n    X = np.array([[2, 20, 3], [4, 50, 6], [7, 80, 9], [10, 110, 12]])\n    y_int = np.array([1, 2, 3, 4])\n    return (X, y_int)", "refactored": true, "pred": {"ppl": 3.5388083457946777, "ppl_lower": 3.5478103160858154, "ppl/lowercase_ppl": -1.002010267312446, "ppl/zlib": 0.009157898876759985, "Min_5.0% Prob": 8.952462792396545, "Min_10.0% Prob": 7.3061389393276635, "Min_20.0% Prob": 5.2995319240971615, "Min_30.0% Prob": 3.999367039118494, "Min_40.0% Prob": 3.1009670268548164, "Min_50.0% Prob": 2.541860462661753, "Min_60.0% Prob": 2.1030182066259155}}
{"hexsha": "a0e82345e90e71c8a988186287df593c48a61aad", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\nasync def selectDB(userid):\n    conn = sqlite3.connect('ID_DATA.db')\n    c = conn.cursor()\n    cursor = c.execute('SELECT *  FROM UIDDATA WHERE USERID = ?', (userid,))\n    for row in cursor:\n        if row[0]:\n            if row[2]:\n                return [row[2], 3]\n            elif row[1]:\n                return [row[1], 2]\n            else:\n                return None\n        else:\n            return None", "fn_id": 1, "class_fn": false, "repo": "Twip-Emma/QQbot-Twip", "file": "bot_plugins/GenshinUID/getDB.py", "last_update_at": "2021-12-23T15:36:48+00:00", "original_content": "async def selectDB(userid):\n    conn = sqlite3.connect('ID_DATA.db')\n    c = conn.cursor()\n    cursor = c.execute('SELECT *  FROM UIDDATA WHERE USERID = ?', (userid,))\n    for row in cursor:\n        if row[0]:\n            if row[2]:\n                return [row[2], 3]\n            elif row[1]:\n                return [row[1], 2]\n            else:\n                return None\n        else:\n            return None", "refactored": true, "pred": {"ppl": 5.7493062019348145, "ppl_lower": 6.053836822509766, "ppl/lowercase_ppl": -1.0295087094732467, "ppl/zlib": 0.007442890157430279, "Min_5.0% Prob": 10.64793529510498, "Min_10.0% Prob": 9.176491043784402, "Min_20.0% Prob": 6.891278494959292, "Min_30.0% Prob": 5.447042605456184, "Min_40.0% Prob": 4.309217989444733, "Min_50.0% Prob": 3.4655241429291923, "Min_60.0% Prob": 2.9308200385691463}}
{"hexsha": "de5093d7a37e19119c9ed380921063f98f98a572", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef solve_all(l, r):\n    res.clear()\n    nl = l.copy()\n    for i in range(len(nl)):\n        nl[i] = (frac(nl[i]), str(nl[i]))\n    solve_all_rec(nl, frac(r))\n    return res", "fn_id": 1, "class_fn": false, "repo": "mateo-cv/KryptoCards", "file": "solver_all.py", "last_update_at": "2021-11-06T02:19:58+00:00", "original_content": "def solve_all(l, r):\n    res.clear()\n    nl = l.copy()\n    for i in range(len(nl)):\n        nl[i] = (frac(nl[i]), str(nl[i]))\n    solve_all_rec(nl, frac(r))\n    return res", "refactored": true, "pred": {"ppl": 7.629189968109131, "ppl_lower": 7.629189968109131, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.014411217557345691, "Min_5.0% Prob": 11.421210606892904, "Min_10.0% Prob": 10.095661980765206, "Min_20.0% Prob": 7.3191145896911625, "Min_30.0% Prob": 5.95479713786732, "Min_40.0% Prob": 4.858754086494446, "Min_50.0% Prob": 3.9838937011204267, "Min_60.0% Prob": 3.405349314212799}}
{"hexsha": "6a6c2ee6803eaaa2206bcf41874368968655c509", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef tsne_plot(model):\n    labels = []\n    tokens = []\n    print('Displaying t-SNE...')\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n    plt.figure(figsize=(16, 16))\n    for i in range(len(x)):\n        plt.scatter(x[i], y[i])\n        plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n    plt.title('t-SNE')\n    plt.show()", "fn_id": 3, "class_fn": false, "repo": "cjbayron/artist2lyrics", "file": "common/utils.py", "last_update_at": "2021-11-08T12:49:36+00:00", "original_content": "def tsne_plot(model):\n    labels = []\n    tokens = []\n    print('Displaying t-SNE...')\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n    plt.figure(figsize=(16, 16))\n    for i in range(len(x)):\n        plt.scatter(x[i], y[i])\n        plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n    plt.title('t-SNE')\n    plt.show()", "refactored": true, "pred": {"ppl": 1.6977424621582031, "ppl_lower": 1.9583008289337158, "ppl/lowercase_ppl": -1.2697485898455707, "ppl/zlib": 0.0013502535844412793, "Min_5.0% Prob": 7.453059037526448, "Min_10.0% Prob": 4.927589888374011, "Min_20.0% Prob": 2.6387849675149333, "Min_30.0% Prob": 1.7637093104799655, "Min_40.0% Prob": 1.3336112526654056, "Min_50.0% Prob": 1.0628353817969545, "Min_60.0% Prob": 0.8833509323764235}}
{"hexsha": "71850fd6a06c73964f1a9b9a4f6b911414e14aff", "ext": "py", "lang": "Python", "content": "@pytest.mark.usefixtures('aftersocialregister_app')\n@timeing\n@measure_memory_usage\ndef test_aftersocialregister(aftersocialregister_config, db_session):\n    \"\"\"Register fresh user and logs him in and check response if redirect from AfterSocialRegister.\"\"\"\n    profile = {'accounts': [{'domain': 'facebook.com', 'userid': '2343'}], 'displayName': 'teddy', 'verifiedEmail': 'we@po.pl', 'preferredUsername': 'teddy', 'emails': [{'value': 'aasd@bwwqwe.pl'}], 'name': 'ted'}\n    credentials = {'oauthAccessToken': '7897048593434'}\n    provider_name = 'facebook'\n    provider_type = 'facebook'\n    request = testing.DummyRequest()\n    request.user = None\n    request.registry = aftersocialregister_config.registry\n    request.remote_addr = '127.0.0.123'\n    request.context = AuthenticationComplete(profile, credentials, provider_name, provider_type)\n    request.login_perform = MagicMock(name='login_perform')\n    request.login_perform.return_value = {'status': True}\n    view = SocialLoginViews(request)\n    out = view()\n    assert out.location == EVENT_PATH.format(AfterSocialRegister)\n    transaction.commit()\n    user = db_session.query(User).one()\n    assert user.is_active\n    assert user.provider_id('facebook') == profile['accounts'][0]['userid']", "fn_id": 24, "class_fn": false, "repo": "fizyk/pyramid_fullauth", "file": "tests/views/test_events.py", "last_update_at": "2021-11-14T15:36:07+00:00", "original_content": "@pytest.mark.usefixtures('aftersocialregister_app')\ndef test_aftersocialregister(aftersocialregister_config, db_session):\n    \"\"\"Register fresh user and logs him in and check response if redirect from AfterSocialRegister.\"\"\"\n    profile = {'accounts': [{'domain': 'facebook.com', 'userid': '2343'}], 'displayName': 'teddy', 'verifiedEmail': 'we@po.pl', 'preferredUsername': 'teddy', 'emails': [{'value': 'aasd@bwwqwe.pl'}], 'name': 'ted'}\n    credentials = {'oauthAccessToken': '7897048593434'}\n    provider_name = 'facebook'\n    provider_type = 'facebook'\n    request = testing.DummyRequest()\n    request.user = None\n    request.registry = aftersocialregister_config.registry\n    request.remote_addr = '127.0.0.123'\n    request.context = AuthenticationComplete(profile, credentials, provider_name, provider_type)\n    request.login_perform = MagicMock(name='login_perform')\n    request.login_perform.return_value = {'status': True}\n    view = SocialLoginViews(request)\n    out = view()\n    assert out.location == EVENT_PATH.format(AfterSocialRegister)\n    transaction.commit()\n    user = db_session.query(User).one()\n    assert user.is_active\n    assert user.provider_id('facebook') == profile['accounts'][0]['userid']", "refactored": true, "pred": {"ppl": 7.898279190063477, "ppl_lower": 10.305120468139648, "ppl/lowercase_ppl": -1.1287090933355326, "ppl/zlib": 0.003327930614659059, "Min_5.0% Prob": 11.70961357565487, "Min_10.0% Prob": 9.66935853397145, "Min_20.0% Prob": 7.361261236494866, "Min_30.0% Prob": 5.934736161556058, "Min_40.0% Prob": 4.838390181893888, "Min_50.0% Prob": 4.045159471936004, "Min_60.0% Prob": 3.417778130962653}}
{"hexsha": "9109af01b75d406c726ca60fbd26a94a9e80f374", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef MultiscaleEntropy_mse(x, scale_factor=[i for i in range(1, 21)], m=[2], r=[0.15], return_type='dict', safe_mode=False):\n    \"\"\"[Multiscale Entropy]\n\n    Arguments:\n        x {[input signal]} -- [an iterator of numbers]\n\n    Keyword Arguments:\n        scale_factor {list} -- [scale factors of coarse graining] (default: {[i for i in range(1,21)]})\n        m {list} -- [m in sample entropy] (default: {[2]})\n        r {list} -- [r in sample entropy] (default: {[0.15]})\n        return_type {str} -- [can be dict or list] (default: {'dict'})\n        safe_mode {bool} -- [if set True, type checking will be skipped] (default: {False})\n\n    Raises:\n        ValueError -- [some values too big]\n\n    Returns:\n        [dict or list as return_type indicates] -- [if dict, nest as [scale_factor][m][r] for each value of scale_factor, m, r; if list nest as [i][j][k] for lengths of scale_factor, m, r]\n    \"\"\"\n    if not safe_mode:\n        m = MultiscaleEntropy_check_type(m, int, 'm')\n        r = MultiscaleEntropy_check_type(r, float, 'r')\n        scale_factor = MultiscaleEntropy_check_type(scale_factor, int, 'scale_factor')\n    try:\n        x = np.array(x)\n    except:\n        print('x should be a sequence of numbers')\n    if max(scale_factor) > len(x):\n        raise ValueError(\"the max scale_factor is bigger than x's length\")\n    sd = np.sqrt(np.var(x))\n    ms_en = MultiscaleEntropy_init_return_type(return_type)\n    for s_f in scale_factor:\n        y = MultiscaleEntropy_coarse_grain(x, s_f)\n        if return_type == 'dict':\n            ms_en[s_f] = MultiscaleEntropy_sample_entropy(y, m, r, sd, 'dict', True)\n        else:\n            ms_en.append(MultiscaleEntropy_sample_entropy(y, m, r, sd, 'list', True))\n    if return_type == 'list':\n        ms_en = [i[0] for i in ms_en]\n        ms_en = [i[0] for i in ms_en]\n    return ms_en", "fn_id": 18, "class_fn": false, "repo": "LRydin/NeuroKit", "file": "tests/tests_complexity.py", "last_update_at": "2021-06-10T03:27:15+00:00", "original_content": "def MultiscaleEntropy_mse(x, scale_factor=[i for i in range(1, 21)], m=[2], r=[0.15], return_type='dict', safe_mode=False):\n    \"\"\"[Multiscale Entropy]\n\n    Arguments:\n        x {[input signal]} -- [an iterator of numbers]\n\n    Keyword Arguments:\n        scale_factor {list} -- [scale factors of coarse graining] (default: {[i for i in range(1,21)]})\n        m {list} -- [m in sample entropy] (default: {[2]})\n        r {list} -- [r in sample entropy] (default: {[0.15]})\n        return_type {str} -- [can be dict or list] (default: {'dict'})\n        safe_mode {bool} -- [if set True, type checking will be skipped] (default: {False})\n\n    Raises:\n        ValueError -- [some values too big]\n\n    Returns:\n        [dict or list as return_type indicates] -- [if dict, nest as [scale_factor][m][r] for each value of scale_factor, m, r; if list nest as [i][j][k] for lengths of scale_factor, m, r]\n    \"\"\"\n    if not safe_mode:\n        m = MultiscaleEntropy_check_type(m, int, 'm')\n        r = MultiscaleEntropy_check_type(r, float, 'r')\n        scale_factor = MultiscaleEntropy_check_type(scale_factor, int, 'scale_factor')\n    try:\n        x = np.array(x)\n    except:\n        print('x should be a sequence of numbers')\n    if max(scale_factor) > len(x):\n        raise ValueError(\"the max scale_factor is bigger than x's length\")\n    sd = np.sqrt(np.var(x))\n    ms_en = MultiscaleEntropy_init_return_type(return_type)\n    for s_f in scale_factor:\n        y = MultiscaleEntropy_coarse_grain(x, s_f)\n        if return_type == 'dict':\n            ms_en[s_f] = MultiscaleEntropy_sample_entropy(y, m, r, sd, 'dict', True)\n        else:\n            ms_en.append(MultiscaleEntropy_sample_entropy(y, m, r, sd, 'list', True))\n    if return_type == 'list':\n        ms_en = [i[0] for i in ms_en]\n        ms_en = [i[0] for i in ms_en]\n    return ms_en", "refactored": true, "pred": {"ppl": 3.447228193283081, "ppl_lower": 3.8988916873931885, "ppl/lowercase_ppl": -1.0994867327091682, "ppl/zlib": 0.0017117157481187613, "Min_5.0% Prob": 10.282890056741648, "Min_10.0% Prob": 7.814135729256323, "Min_20.0% Prob": 5.371298089876014, "Min_30.0% Prob": 3.951019663258461, "Min_40.0% Prob": 3.058160476417984, "Min_50.0% Prob": 2.472992058214102, "Min_60.0% Prob": 2.0663148371502755}}
{"hexsha": "d2664e1063dfac0585b27173768ffad2c031b7e9", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _InceptionV3_blockH(x):\n    block1 = conv2d_bn(x, 320, (1, 1))\n    block2 = conv2d_bn(x, 384, (1, 1))\n    block2_1 = conv2d_bn(block2, 384, (1, 3))\n    block2_2 = conv2d_bn(block2, 384, (3, 1))\n    block2 = Concatenate()([block2_1, block2_2])\n    block3 = conv2d_bn(x, 448, (1, 1))\n    block3_1 = conv2d_bn(block3, 384, (3, 3))\n    block3_2 = conv2d_bn(block3, 384, (1, 3))\n    block3_3 = conv2d_bn(block3, 384, (3, 1))\n    block3 = Concatenate()([block3_1, block3_2, block3_3])\n    block4 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n    block4 = conv2d_bn(block4, 192, (1, 1))\n    x = Concatenate()([block1, block2, block3, block4])\n    return x", "fn_id": 8, "class_fn": false, "repo": "Hiroaki-Ozaki/modelib-classification", "file": "keras/models/InceptionV3.py", "last_update_at": "2021-04-14T08:56:01+00:00", "original_content": "def _InceptionV3_blockH(x):\n    block1 = conv2d_bn(x, 320, (1, 1))\n    block2 = conv2d_bn(x, 384, (1, 1))\n    block2_1 = conv2d_bn(block2, 384, (1, 3))\n    block2_2 = conv2d_bn(block2, 384, (3, 1))\n    block2 = Concatenate()([block2_1, block2_2])\n    block3 = conv2d_bn(x, 448, (1, 1))\n    block3_1 = conv2d_bn(block3, 384, (3, 3))\n    block3_2 = conv2d_bn(block3, 384, (1, 3))\n    block3_3 = conv2d_bn(block3, 384, (3, 1))\n    block3 = Concatenate()([block3_1, block3_2, block3_3])\n    block4 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n    block4 = conv2d_bn(block4, 192, (1, 1))\n    x = Concatenate()([block1, block2, block3, block4])\n    return x", "refactored": true, "pred": {"ppl": 1.7564259767532349, "ppl_lower": 1.8781999349594116, "ppl/lowercase_ppl": -1.1190041587015531, "ppl/zlib": 0.0021499276693387206, "Min_5.0% Prob": 7.413146466016769, "Min_10.0% Prob": 5.259237480886055, "Min_20.0% Prob": 2.8035787667578727, "Min_30.0% Prob": 1.8858573636878282, "Min_40.0% Prob": 1.4080178766388016, "Min_50.0% Prob": 1.1298966042489695, "Min_60.0% Prob": 0.9387926152878063}}
{"hexsha": "be392b1159ec0d0cbf136ec1807d49c872f36a16", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef plot_oi(ticker: str, expiry: str, min_sp: float, max_sp: float, calls_only: bool, puts_only: bool, export: str):\n    \"\"\"Plot open interest\n\n    Parameters\n    ----------\n    ticker: str\n        Ticker\n    expiry: str\n        Expiry date for options\n    min_sp: float\n        Min strike to consider\n    max_sp: float\n        Max strike to consider\n    calls_only: bool\n        Show calls only\n    puts_only: bool\n        Show puts only\n    export: str\n        Format to export file\n    \"\"\"\n    options = tradier_model.get_option_chains(ticker, expiry)\n    export_data(export, os.path.dirname(os.path.abspath(__file__)), 'oi_tr', options)\n    current_price = tradier_model.last_price(ticker)\n    if min_sp == -1:\n        min_strike = 0.75 * current_price\n    else:\n        min_strike = min_sp\n    if max_sp == -1:\n        max_strike = 1.25 * current_price\n    else:\n        max_strike = max_sp\n    if calls_only and puts_only:\n        print('Both flags selected, please select one', '\\n')\n        return\n    calls = options[options.option_type == 'call'][['strike', 'open_interest']]\n    puts = options[options.option_type == 'put'][['strike', 'open_interest']]\n    call_oi = calls.set_index('strike')['open_interest'] / 1000\n    put_oi = puts.set_index('strike')['open_interest'] / 1000\n    df_opt = pd.merge(call_oi, put_oi, left_index=True, right_index=True)\n    df_opt = df_opt.rename(columns={'open_interest_x': 'OI_call', 'open_interest_y': 'OI_put'})\n    max_pain = op_helpers.calculate_max_pain(df_opt)\n    plt.style.use('classic')\n    fig, ax = plt.subplots(figsize=plot_autoscale(), dpi=cfp.PLOT_DPI)\n    if not calls_only:\n        put_oi.plot(x='strike', y='open_interest', label='Puts', ax=ax, marker='o', ls='-', c='r')\n    if not puts_only:\n        call_oi.plot(x='strike', y='open_interest', label='Calls', ax=ax, marker='o', ls='-', c='g')\n        ax.axvline(current_price, lw=2, c='k', ls='--', label='Current Price', alpha=0.7)\n        ax.axvline(max_pain, lw=3, c='k', label=f'Max Pain: {max_pain}', alpha=0.7)\n        ax.grid('on')\n        ax.set_xlabel('Strike Price')\n        ax.set_ylabel('Open Interest (1k) ')\n        ax.set_xlim(min_strike, max_strike)\n        if gtff.USE_ION:\n            plt.ion()\n        ax.set_title(f'Open Interest for {ticker.upper()} expiring {expiry}')\n        plt.legend(loc=0)\n        fig.tight_layout(pad=1)\n    plt.show()\n    plt.style.use('default')\n    print('')", "fn_id": 4, "class_fn": false, "repo": "Aerex/GamestonkTerminal", "file": "gamestonk_terminal/stocks/options/tradier_view.py", "last_update_at": "2021-03-11T17:42:35+00:00", "original_content": "def plot_oi(ticker: str, expiry: str, min_sp: float, max_sp: float, calls_only: bool, puts_only: bool, export: str):\n    \"\"\"Plot open interest\n\n    Parameters\n    ----------\n    ticker: str\n        Ticker\n    expiry: str\n        Expiry date for options\n    min_sp: float\n        Min strike to consider\n    max_sp: float\n        Max strike to consider\n    calls_only: bool\n        Show calls only\n    puts_only: bool\n        Show puts only\n    export: str\n        Format to export file\n    \"\"\"\n    options = tradier_model.get_option_chains(ticker, expiry)\n    export_data(export, os.path.dirname(os.path.abspath(__file__)), 'oi_tr', options)\n    current_price = tradier_model.last_price(ticker)\n    if min_sp == -1:\n        min_strike = 0.75 * current_price\n    else:\n        min_strike = min_sp\n    if max_sp == -1:\n        max_strike = 1.25 * current_price\n    else:\n        max_strike = max_sp\n    if calls_only and puts_only:\n        print('Both flags selected, please select one', '\\n')\n        return\n    calls = options[options.option_type == 'call'][['strike', 'open_interest']]\n    puts = options[options.option_type == 'put'][['strike', 'open_interest']]\n    call_oi = calls.set_index('strike')['open_interest'] / 1000\n    put_oi = puts.set_index('strike')['open_interest'] / 1000\n    df_opt = pd.merge(call_oi, put_oi, left_index=True, right_index=True)\n    df_opt = df_opt.rename(columns={'open_interest_x': 'OI_call', 'open_interest_y': 'OI_put'})\n    max_pain = op_helpers.calculate_max_pain(df_opt)\n    plt.style.use('classic')\n    fig, ax = plt.subplots(figsize=plot_autoscale(), dpi=cfp.PLOT_DPI)\n    if not calls_only:\n        put_oi.plot(x='strike', y='open_interest', label='Puts', ax=ax, marker='o', ls='-', c='r')\n    if not puts_only:\n        call_oi.plot(x='strike', y='open_interest', label='Calls', ax=ax, marker='o', ls='-', c='g')\n        ax.axvline(current_price, lw=2, c='k', ls='--', label='Current Price', alpha=0.7)\n        ax.axvline(max_pain, lw=3, c='k', label=f'Max Pain: {max_pain}', alpha=0.7)\n        ax.grid('on')\n        ax.set_xlabel('Strike Price')\n        ax.set_ylabel('Open Interest (1k) ')\n        ax.set_xlim(min_strike, max_strike)\n        if gtff.USE_ION:\n            plt.ion()\n        ax.set_title(f'Open Interest for {ticker.upper()} expiring {expiry}')\n        plt.legend(loc=0)\n        fig.tight_layout(pad=1)\n    plt.show()\n    plt.style.use('default')\n    print('')", "refactored": true, "pred": {"ppl": 2.9419870376586914, "ppl_lower": 3.113521099090576, "ppl/lowercase_ppl": -1.0525158299790944, "ppl/zlib": 0.0011287502261581213, "Min_5.0% Prob": 9.529292583465576, "Min_10.0% Prob": 7.293772400199593, "Min_20.0% Prob": 4.897736659357625, "Min_30.0% Prob": 3.5288327909235297, "Min_40.0% Prob": 2.6875900483179476, "Min_50.0% Prob": 2.156091691388451, "Min_60.0% Prob": 1.8004734624282366}}
{"hexsha": "467014e3a54821c2e5ac7b33d2f939b0052d1b15", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef run_task(hosts, command, timeout=None):\n    \"\"\"Create a task to run a command on each host in parallel.\n\n    Args:\n        hosts (list): list of hosts\n        command (str): the command to run in parallel\n        timeout (int, optional): command timeout in seconds. Defaults to None.\n\n    Returns:\n        Task: a ClusterShell.Task.Task object for the executed command\n\n    \"\"\"\n    task = task_self()\n    task.set_info('ssh_options', '-oForwardAgent=yes')\n    kwargs = {'command': command, 'nodes': NodeSet.fromlist(hosts)}\n    if timeout is not None:\n        kwargs['timeout'] = timeout\n    task.run(**kwargs)\n    return task", "fn_id": 3, "class_fn": false, "repo": "kmajzero/daos", "file": "src/tests/ftest/util/general_utils.py", "last_update_at": "2021-04-13T16:04:21+00:00", "original_content": "def run_task(hosts, command, timeout=None):\n    \"\"\"Create a task to run a command on each host in parallel.\n\n    Args:\n        hosts (list): list of hosts\n        command (str): the command to run in parallel\n        timeout (int, optional): command timeout in seconds. Defaults to None.\n\n    Returns:\n        Task: a ClusterShell.Task.Task object for the executed command\n\n    \"\"\"\n    task = task_self()\n    task.set_info('ssh_options', '-oForwardAgent=yes')\n    kwargs = {'command': command, 'nodes': NodeSet.fromlist(hosts)}\n    if timeout is not None:\n        kwargs['timeout'] = timeout\n    task.run(**kwargs)\n    return task", "refactored": true, "pred": {"ppl": 5.83024787902832, "ppl_lower": 7.409806251525879, "ppl/lowercase_ppl": -1.1359822356639766, "ppl/zlib": 0.004897387548061571, "Min_5.0% Prob": 10.80514109134674, "Min_10.0% Prob": 9.007333904504776, "Min_20.0% Prob": 6.823966674506664, "Min_30.0% Prob": 5.3416711067666816, "Min_40.0% Prob": 4.27433290389868, "Min_50.0% Prob": 3.474276363122754, "Min_60.0% Prob": 2.9360186815717997}}
{"hexsha": "dac2bf672e1bd9ca92568b7538b014b4a37dd949", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef find_euler_random(n):\n    while True:\n        el = np.array([random.randint(0, 4) * 2 for x in range(n + 1)])\n        try:\n            g = Graph.from_sequence(el)\n            choose_biggest_comp(g)\n            if g.adjacency.shape[0] != n:\n                continue\n            print(g.adjacency)\n            for i in range(el.shape[0] * 3):\n                g.randomize_edges()\n            print(g.adjacency)\n            graph = nx.from_numpy_matrix(g.adjacency)\n            euler_list = []\n            euler(g.adjacency.tolist(), 0, euler_list)\n            print(euler_list)\n            plt.subplot(111)\n            nx.draw(graph, with_labels=True, font_weight='bold')\n            plt.show()\n            break\n        except NotGraphicSequenceException:\n            continue", "fn_id": 3, "class_fn": false, "repo": "timcki/graficiarze", "file": "02_project/main.py", "last_update_at": "2021-05-14T11:35:04+00:00", "original_content": "def find_euler_random(n):\n    while True:\n        el = np.array([random.randint(0, 4) * 2 for x in range(n + 1)])\n        try:\n            g = Graph.from_sequence(el)\n            choose_biggest_comp(g)\n            if g.adjacency.shape[0] != n:\n                continue\n            print(g.adjacency)\n            for i in range(el.shape[0] * 3):\n                g.randomize_edges()\n            print(g.adjacency)\n            graph = nx.from_numpy_matrix(g.adjacency)\n            euler_list = []\n            euler(g.adjacency.tolist(), 0, euler_list)\n            print(euler_list)\n            plt.subplot(111)\n            nx.draw(graph, with_labels=True, font_weight='bold')\n            plt.show()\n            break\n        except NotGraphicSequenceException:\n            continue", "refactored": true, "pred": {"ppl": 6.047945499420166, "ppl_lower": 6.885228633880615, "ppl/lowercase_ppl": -1.0720444349945415, "ppl/zlib": 0.004850993605114208, "Min_5.0% Prob": 11.433259530500932, "Min_10.0% Prob": 9.594882770018144, "Min_20.0% Prob": 7.1373582257164845, "Min_30.0% Prob": 5.613831872370706, "Min_40.0% Prob": 4.398570771349801, "Min_50.0% Prob": 3.587360176257789, "Min_60.0% Prob": 2.9926025260929707}}
{"hexsha": "b27331e2a40b51e2950c979f9bac602c12eef9c1", "ext": "py", "lang": "Python", "content": "@app.route('/')\n@app.route('/index')\n@timeing\n@measure_memory_usage\ndef index():\n    app.logger.debug('Entering index')\n    flask.session['user_id'] = 'creator'\n    flask.session['finished'] = check_completed()\n    flask.session['to_finish'] = PEOPLE_TO_INVITE\n    events = []\n    for record in collection.find({'user_id': 'creator'}):\n        events.append(record)\n    if len(events) > 0:\n        return render_template('waiting.html')\n    if 'begin_date' not in flask.session:\n        init_session_values()\n    return render_template('index.html')", "fn_id": 0, "class_fn": false, "repo": "Dream7hief/MeetMe", "file": "flask_main.py", "last_update_at": "2021-02-08T20:18:11+00:00", "original_content": "@app.route('/')\n@app.route('/index')\ndef index():\n    app.logger.debug('Entering index')\n    flask.session['user_id'] = 'creator'\n    flask.session['finished'] = check_completed()\n    flask.session['to_finish'] = PEOPLE_TO_INVITE\n    events = []\n    for record in collection.find({'user_id': 'creator'}):\n        events.append(record)\n    if len(events) > 0:\n        return render_template('waiting.html')\n    if 'begin_date' not in flask.session:\n        init_session_values()\n    return render_template('index.html')", "refactored": true, "pred": {"ppl": 6.361433506011963, "ppl_lower": 6.842510223388672, "ppl/lowercase_ppl": -1.0394004931537804, "ppl/zlib": 0.006106448006540104, "Min_5.0% Prob": 12.708497728620257, "Min_10.0% Prob": 10.542113399505615, "Min_20.0% Prob": 7.801424957090808, "Min_30.0% Prob": 5.827106067474852, "Min_40.0% Prob": 4.536898210408196, "Min_50.0% Prob": 3.6717012634005726, "Min_60.0% Prob": 3.1017569321742715}}
{"hexsha": "11ad143814ad6766582b44de612f20dfd8d0423f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_x86_direct():\n    insn = ms.ISA_X86.parse_instruction('mov eax, dword ptr [0x8000]')\n    assert insn.num_operands == 2\n    op = insn.operands[1]\n    assert op.is_direct\n    assert op.offset == 32768\n    assert op.base_reg is None\n    assert op.index_reg is None\n    assert op.scale == 1\n    assert str(op) == '0x8000'\n    assert repr(op) == '<MemoryOperand(offset=0x8000)>'", "fn_id": 11, "class_fn": false, "repo": "giltom/megastone", "file": "tests/test_disasm.py", "last_update_at": "2021-04-12T15:12:10+00:00", "original_content": "def test_x86_direct():\n    insn = ms.ISA_X86.parse_instruction('mov eax, dword ptr [0x8000]')\n    assert insn.num_operands == 2\n    op = insn.operands[1]\n    assert op.is_direct\n    assert op.offset == 32768\n    assert op.base_reg is None\n    assert op.index_reg is None\n    assert op.scale == 1\n    assert str(op) == '0x8000'\n    assert repr(op) == '<MemoryOperand(offset=0x8000)>'", "refactored": true, "pred": {"ppl": 4.620872974395752, "ppl_lower": 5.615571022033691, "ppl/lowercase_ppl": -1.127376009161247, "ppl/zlib": 0.006377431844753008, "Min_5.0% Prob": 10.18789005279541, "Min_10.0% Prob": 8.53301089150565, "Min_20.0% Prob": 6.094939207208568, "Min_30.0% Prob": 4.794035562060096, "Min_40.0% Prob": 3.7982854832862984, "Min_50.0% Prob": 3.0639102310962874, "Min_60.0% Prob": 2.553320471324365}}
{"hexsha": "12b9c9e1e1cd70be553e0f6bc5aeab33f9e284de", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef build_data_str(records):\n    records_data_str = []\n    for record in records:\n        records_data_str.append(tup_to_str(record))\n    return records_data_str", "fn_id": 0, "class_fn": false, "repo": "pangeon/Taskello", "file": "utils/str_utils.py", "last_update_at": "2021-06-22T21:18:37+00:00", "original_content": "def build_data_str(records):\n    records_data_str = []\n    for record in records:\n        records_data_str.append(tup_to_str(record))\n    return records_data_str", "refactored": true, "pred": {"ppl": 7.250081539154053, "ppl_lower": 7.250081539154053, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.01664716567716982, "Min_5.0% Prob": 12.181049346923828, "Min_10.0% Prob": 10.6011568069458, "Min_20.0% Prob": 8.085082791068338, "Min_30.0% Prob": 6.219313144683838, "Min_40.0% Prob": 4.998204255645925, "Min_50.0% Prob": 4.001722658851317, "Min_60.0% Prob": 3.313154343953904}}
{"hexsha": "e7dae2609f1a6ef8908cb001bd4912ee60a48a48", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef construct_tabs(selected_pipeline, window, prefs, include_eeg):\n    \"\"\"Constructs as set of tabs based on specifications and the\n    selected pipeline.\n\n    Parameters\n    ----------\n    selected_pipeline : str\n        ID of the selected pipeline\n    window : instance of main window\n        The main window.\n    prefs : Instance of PreferencesHandler\n        Stores e.g. active plugins.\n    include_eeg : bool\n        Whether to add EEG-related actions\n\n    Returns\n    -------\n    list of QDialog\n        Contains the constructed tabs relevant to the pipeline\n    \"\"\"\n    active_plugins = prefs.active_plugins\n    action_specs = find_all_action_specs()\n    datatype_specs = find_all_datatype_specs()\n    package_specs = find_all_package_specs()\n    tabs = []\n    pipelines = []\n    for source, package_spec in package_specs.items():\n        if source not in active_plugins and source != 'meggie':\n            continue\n        if 'tabs' in package_spec:\n            tabs.extend(package_spec['tabs'])\n        if 'pipelines' in package_spec:\n            pipelines.extend(package_spec['pipelines'])\n    for tab in tabs:\n        if 'id' not in tab:\n            raise Exception('Every tab specification must have id.')\n        if 'name' not in tab:\n            tab['name'] = tab['id']\n        if 'inputs' not in tab:\n            tab['inputs'] = []\n        if 'outputs' not in tab:\n            tab['outputs'] = []\n        if 'input_actions' not in tab:\n            tab['input_actions'] = []\n        if 'output_actions' not in tab:\n            tab['output_actions'] = []\n        if 'info' not in tab:\n            tab['info'] = []\n    for pipeline in pipelines:\n        if 'id' not in pipeline:\n            raise Exception('Every pipeline must have id.')\n        if 'name' not in pipeline:\n            pipeline['name'] = pipeline['id']\n    found = False\n    pipeline_spec = None\n    for pipeline in pipelines:\n        if pipeline['id'] == selected_pipeline:\n            found = True\n            pipeline_spec = pipeline\n            break\n    if not found:\n        pipeline_spec = {'id': 'classic', 'name': 'Include everything'}\n    combined_tabs = []\n    for tab_spec in tabs:\n        if pipeline_spec.get('include_tabs'):\n            if tab_spec['id'] not in pipeline_spec['include_tabs']:\n                continue\n        if tab_spec['id'] not in [tab['id'] for tab in combined_tabs]:\n            new_tab = {}\n            new_tab['id'] = tab_spec['id']\n            new_tab['name'] = tab_spec['name']\n            new_tab['inputs'] = []\n            new_tab['outputs'] = []\n            new_tab['input_actions'] = []\n            new_tab['output_actions'] = []\n            new_tab['info'] = []\n            idx = len(combined_tabs)\n            combined_tabs.append(new_tab)\n        else:\n            idx = [tab['id'] for tab in combined_tabs].index(tab_spec['id'])\n        for input_spec in tab_spec['inputs']:\n            if input_spec not in combined_tabs[idx]['inputs']:\n                combined_tabs[idx]['inputs'].append(input_spec)\n        for output_spec in tab_spec['outputs']:\n            if output_spec not in combined_tabs[idx]['outputs']:\n                combined_tabs[idx]['outputs'].append(output_spec)\n        for input_spec in tab_spec['input_actions']:\n            action_spec = action_specs.get(input_spec)\n            if not action_spec:\n                raise Exception('Cannot read action ' + input_spec + '.')\n            if not include_eeg and 'eeg' in action_spec[2].get('tags', []):\n                continue\n            if input_spec not in combined_tabs[idx]['input_actions']:\n                combined_tabs[idx]['input_actions'].append(input_spec)\n        for output_spec in tab_spec['output_actions']:\n            action_spec = action_specs.get(output_spec)\n            if not action_spec:\n                raise Exception('Cannot read action ' + output_spec + '.')\n            if not include_eeg and 'eeg' in action_spec[2].get('tags', []):\n                continue\n            if output_spec not in combined_tabs[idx]['output_actions']:\n                combined_tabs[idx]['output_actions'].append(output_spec)\n        for info_spec in tab_spec['info']:\n            action_spec = action_specs.get(info_spec)\n            if not action_spec:\n                raise Exception('Cannot read info item ' + info_spec + '.')\n            if info_spec not in combined_tabs[idx]['info']:\n                combined_tabs[idx]['info'].append(info_spec)\n    tabs = []\n    for tab_spec in combined_tabs:\n        tabs.append(construct_tab(tab_spec, action_specs, datatype_specs, window))\n    return tabs", "fn_id": 6, "class_fn": false, "repo": "Teekuningas/meggie", "file": "meggie/mainwindow/dynamic.py", "last_update_at": "2021-01-15T21:21:51+00:00", "original_content": "def construct_tabs(selected_pipeline, window, prefs, include_eeg):\n    \"\"\"Constructs as set of tabs based on specifications and the\n    selected pipeline.\n\n    Parameters\n    ----------\n    selected_pipeline : str\n        ID of the selected pipeline\n    window : instance of main window\n        The main window.\n    prefs : Instance of PreferencesHandler\n        Stores e.g. active plugins.\n    include_eeg : bool\n        Whether to add EEG-related actions\n\n    Returns\n    -------\n    list of QDialog\n        Contains the constructed tabs relevant to the pipeline\n    \"\"\"\n    active_plugins = prefs.active_plugins\n    action_specs = find_all_action_specs()\n    datatype_specs = find_all_datatype_specs()\n    package_specs = find_all_package_specs()\n    tabs = []\n    pipelines = []\n    for source, package_spec in package_specs.items():\n        if source not in active_plugins and source != 'meggie':\n            continue\n        if 'tabs' in package_spec:\n            tabs.extend(package_spec['tabs'])\n        if 'pipelines' in package_spec:\n            pipelines.extend(package_spec['pipelines'])\n    for tab in tabs:\n        if 'id' not in tab:\n            raise Exception('Every tab specification must have id.')\n        if 'name' not in tab:\n            tab['name'] = tab['id']\n        if 'inputs' not in tab:\n            tab['inputs'] = []\n        if 'outputs' not in tab:\n            tab['outputs'] = []\n        if 'input_actions' not in tab:\n            tab['input_actions'] = []\n        if 'output_actions' not in tab:\n            tab['output_actions'] = []\n        if 'info' not in tab:\n            tab['info'] = []\n    for pipeline in pipelines:\n        if 'id' not in pipeline:\n            raise Exception('Every pipeline must have id.')\n        if 'name' not in pipeline:\n            pipeline['name'] = pipeline['id']\n    found = False\n    pipeline_spec = None\n    for pipeline in pipelines:\n        if pipeline['id'] == selected_pipeline:\n            found = True\n            pipeline_spec = pipeline\n            break\n    if not found:\n        pipeline_spec = {'id': 'classic', 'name': 'Include everything'}\n    combined_tabs = []\n    for tab_spec in tabs:\n        if pipeline_spec.get('include_tabs'):\n            if tab_spec['id'] not in pipeline_spec['include_tabs']:\n                continue\n        if tab_spec['id'] not in [tab['id'] for tab in combined_tabs]:\n            new_tab = {}\n            new_tab['id'] = tab_spec['id']\n            new_tab['name'] = tab_spec['name']\n            new_tab['inputs'] = []\n            new_tab['outputs'] = []\n            new_tab['input_actions'] = []\n            new_tab['output_actions'] = []\n            new_tab['info'] = []\n            idx = len(combined_tabs)\n            combined_tabs.append(new_tab)\n        else:\n            idx = [tab['id'] for tab in combined_tabs].index(tab_spec['id'])\n        for input_spec in tab_spec['inputs']:\n            if input_spec not in combined_tabs[idx]['inputs']:\n                combined_tabs[idx]['inputs'].append(input_spec)\n        for output_spec in tab_spec['outputs']:\n            if output_spec not in combined_tabs[idx]['outputs']:\n                combined_tabs[idx]['outputs'].append(output_spec)\n        for input_spec in tab_spec['input_actions']:\n            action_spec = action_specs.get(input_spec)\n            if not action_spec:\n                raise Exception('Cannot read action ' + input_spec + '.')\n            if not include_eeg and 'eeg' in action_spec[2].get('tags', []):\n                continue\n            if input_spec not in combined_tabs[idx]['input_actions']:\n                combined_tabs[idx]['input_actions'].append(input_spec)\n        for output_spec in tab_spec['output_actions']:\n            action_spec = action_specs.get(output_spec)\n            if not action_spec:\n                raise Exception('Cannot read action ' + output_spec + '.')\n            if not include_eeg and 'eeg' in action_spec[2].get('tags', []):\n                continue\n            if output_spec not in combined_tabs[idx]['output_actions']:\n                combined_tabs[idx]['output_actions'].append(output_spec)\n        for info_spec in tab_spec['info']:\n            action_spec = action_specs.get(info_spec)\n            if not action_spec:\n                raise Exception('Cannot read info item ' + info_spec + '.')\n            if info_spec not in combined_tabs[idx]['info']:\n                combined_tabs[idx]['info'].append(info_spec)\n    tabs = []\n    for tab_spec in combined_tabs:\n        tabs.append(construct_tab(tab_spec, action_specs, datatype_specs, window))\n    return tabs", "refactored": true, "pred": {"ppl": 2.198784828186035, "ppl_lower": 2.2935054302215576, "ppl/lowercase_ppl": -1.0535299984511786, "ppl/zlib": 0.0007104642533253636, "Min_5.0% Prob": 8.824413505255007, "Min_10.0% Prob": 6.344770758759742, "Min_20.0% Prob": 3.8125311741057564, "Min_30.0% Prob": 2.6209938714122463, "Min_40.0% Prob": 1.969077338521485, "Min_50.0% Prob": 1.5771104084631271, "Min_60.0% Prob": 1.3148451631737863}}
{"hexsha": "8ce8096f58ca81419a2edfa556d2c57b16fdd66c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef deepcopy_and_sign(rlp_signable, privatekey):\n    msg = deepcopy(rlp_signable)\n    msg.sign(privatekey)\n    return msg", "fn_id": 0, "class_fn": false, "repo": "luehrsFred/raidex", "file": "raidex/tests/unit/commitment_service/old_test_swap_commitment.py", "last_update_at": "2021-06-22T05:01:56+00:00", "original_content": "def deepcopy_and_sign(rlp_signable, privatekey):\n    msg = deepcopy(rlp_signable)\n    msg.sign(privatekey)\n    return msg", "refactored": true, "pred": {"ppl": 12.742496490478516, "ppl_lower": 12.742496490478516, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.022927410701021837, "Min_5.0% Prob": 11.653153896331787, "Min_10.0% Prob": 11.089562892913818, "Min_20.0% Prob": 9.337432702382406, "Min_30.0% Prob": 7.66187001977648, "Min_40.0% Prob": 6.183063833337081, "Min_50.0% Prob": 5.059241997698943, "Min_60.0% Prob": 4.355959818565419}}
{"hexsha": "338af77e36625c6e736374eb318b1d3d29015cd3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef find_replacement_chord(chord):\n    t = chord.transpose(-chord.bass)\n    if t.pitches == (0, 4, 6, 10):\n        return Chord(bass=0, root=0, pitches=(0, 4, 10)).transpose(chord.bass)\n    if t.pitches == (0, 3, 8, 10):\n        return Chord(bass=0, root=8, pitches=(0, 3, 8)).transpose(chord.bass)\n    if t.pitches == (0, 4, 6, 11):\n        return Chord(bass=0, root=0, pitches=(0, 4, 6, 7, 11)).transpose(chord.bass)\n    if t.pitches == (0, 2, 4, 6, 10):\n        return Chord(bass=0, root=10, pitches=(0, 2, 6, 10)).transpose(chord.bass)\n    raise ValueError('no such chord: ' + str(chord))", "fn_id": 7, "class_fn": false, "repo": "andreasjansson/music-inpainting-bert", "file": "data.py", "last_update_at": "2021-11-25T22:59:11+00:00", "original_content": "def find_replacement_chord(chord):\n    t = chord.transpose(-chord.bass)\n    if t.pitches == (0, 4, 6, 10):\n        return Chord(bass=0, root=0, pitches=(0, 4, 10)).transpose(chord.bass)\n    if t.pitches == (0, 3, 8, 10):\n        return Chord(bass=0, root=8, pitches=(0, 3, 8)).transpose(chord.bass)\n    if t.pitches == (0, 4, 6, 11):\n        return Chord(bass=0, root=0, pitches=(0, 4, 6, 7, 11)).transpose(chord.bass)\n    if t.pitches == (0, 2, 4, 6, 10):\n        return Chord(bass=0, root=10, pitches=(0, 2, 6, 10)).transpose(chord.bass)\n    raise ValueError('no such chord: ' + str(chord))", "refactored": true, "pred": {"ppl": 2.5427231788635254, "ppl_lower": 2.770249843597412, "ppl/lowercase_ppl": -1.0918330652531978, "ppl/zlib": 0.0041662304660069465, "Min_5.0% Prob": 8.50049352645874, "Min_10.0% Prob": 6.671610452510692, "Min_20.0% Prob": 4.358440427346663, "Min_30.0% Prob": 3.089913859065757, "Min_40.0% Prob": 2.3361723546844884, "Min_50.0% Prob": 1.8710039158304819, "Min_60.0% Prob": 1.5587417769056193}}
{"hexsha": "969279abcd7947481d7b6dda02b897ac7a615367", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef main() -> typing.NoReturn:\n    h, w, d = map(int, input().split())\n    I = np.array(sys.stdin.read().split(), dtype=np.int64)\n    a = I[:h * w].reshape(h, w) - 1\n    lr = I[h * w + 1:].reshape(-1, 2) - 1\n    solve(a, d, lr)", "fn_id": 1, "class_fn": false, "repo": "kagemeka/competitive-programming", "file": "src/atcoder/abc089/d/sol_0.py", "last_update_at": "2021-07-11T03:20:10+00:00", "original_content": "def main() -> typing.NoReturn:\n    h, w, d = map(int, input().split())\n    I = np.array(sys.stdin.read().split(), dtype=np.int64)\n    a = I[:h * w].reshape(h, w) - 1\n    lr = I[h * w + 1:].reshape(-1, 2) - 1\n    solve(a, d, lr)", "refactored": true, "pred": {"ppl": 4.375502109527588, "ppl_lower": 5.001811981201172, "ppl/lowercase_ppl": -1.0906348462721607, "ppl/zlib": 0.007978493411439282, "Min_5.0% Prob": 11.119700050354004, "Min_10.0% Prob": 9.261602210998536, "Min_20.0% Prob": 6.644379070826939, "Min_30.0% Prob": 4.824499374255538, "Min_40.0% Prob": 3.730997871784937, "Min_50.0% Prob": 2.9734426880923084, "Min_60.0% Prob": 2.466188746300759}}
{"hexsha": "f25d845a5d072ddbe9a4ba42086800a674959571", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef func_18fc8c7e787b450bb9ba27326af7b455(x):\n    x = sorted(x, reverse=True)\n    rmax = 0.0\n    y = [x[i] for i in xrange(len(x))]\n    return y", "fn_id": 134, "class_fn": false, "repo": "DynamicCodeSearch/CodeSeer", "file": "projects/src/main/python/CodeJam/Y13R5P1/gepa/generated_py_84001afe6c4e45fca49f21e859b1c46f.py", "last_update_at": "2021-04-13T20:34:19+00:00", "original_content": "def func_18fc8c7e787b450bb9ba27326af7b455(x):\n    x = sorted(x, reverse=True)\n    rmax = 0.0\n    y = [x[i] for i in xrange(len(x))]\n    return y", "refactored": true, "pred": {"ppl": 15.030082702636719, "ppl_lower": 17.303396224975586, "ppl/lowercase_ppl": -1.0519728038009972, "ppl/zlib": 0.018435739498325516, "Min_5.0% Prob": 11.149450540542603, "Min_10.0% Prob": 9.761942505836487, "Min_20.0% Prob": 7.824187934398651, "Min_30.0% Prob": 6.396390408277512, "Min_40.0% Prob": 5.4659356781930635, "Min_50.0% Prob": 4.89837171391743, "Min_60.0% Prob": 4.4073852397957625}}
{"hexsha": "35f904677ad8fd1f812f7b5f6172973cf62eaed0", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef mixture_fit(x, K=3, x_w=None, n_itr=100, verbose=False, random_state=0, f_write=None, output_folder=None, suffix=None, fold_number=0):\n    \"\"\"Fit a slope+bump mixture using EM algorithm.\n\n    Args:\n        x ((n,d) ndarray): The covaraites.\n        K (int): The number of bump components.\n        x_w ((n,) ndarray): The weights for each sample.\n        n_itr (int): The maximum number of iterations for the EM algorithm\n        verbose (bool): Indicate if output the computation details.\n        random_state (int): The random seed.\n        f_write (file handler (write mode)): The output file.\n        output_folder (string): The output directory.\n        suffix (string): The suffix of the output file.\n        fold_number (int(0,1)): The fold number.\n\n    Returns:\n        a ((d,) ndarray): slope parameter.\n        mu,sigma ((k,d) ndarray): bump parameter.\n        w ((n,) ndarray): proportion of each component. \n    \"\"\"\n    np.random.seed(random_state)\n    if len(x.shape) == 1:\n        x = x.reshape([-1, 1])\n    n_samp, d = x.shape\n    if x_w is None:\n        x_w = np.ones([n_samp], dtype=float)\n    GMM = GaussianMixture(n_components=K, covariance_type='diag').fit(x)\n    w_old = np.zeros([K + 1])\n    w = 0.5 * np.ones([K + 1]) / K\n    w[0] = 0.5\n    a = ML_slope(x, x_w)\n    mu, sigma = (GMM.means_, GMM.covariances_ ** 0.5)\n    w_samp = np.zeros([K + 1, n_samp], dtype=float)\n    i = 0\n    if verbose:\n        if f_write is not None:\n            f_write.write('## mixture_fit: initialization parameters\\n')\n            f_write.write('# Slope: w=%0.4f, a=%s\\n' % (w[0], a))\n            for k in range(K):\n                f_write.write('# Bump %d: w=%0.4f\\n' % (k, w[k + 1]))\n                f_write.write('         mu=%s\\n' % mu[k])\n                f_write.write('      sigma=%s\\n' % sigma[k])\n            f_write.write('\\n')\n    while np.linalg.norm(w - w_old, 1) > 0.005 and i < n_itr:\n        w_old = w\n        w_samp[0, :] = w[0] * f_slope(x, a)\n        for k in range(K):\n            w_samp[k + 1, :] = w[k + 1] * f_bump(x, mu[k], sigma[k])\n        w_samp = w_samp / np.sum(w_samp, axis=0) * x_w\n        w = np.mean(w_samp, axis=1)\n        a = ML_slope(x, w_samp[0, :])\n        for k in range(K):\n            if w[k + 1] > 0.0001:\n                mu[k], sigma[k] = ML_bump(x, w_samp[k + 1, :])\n        sigma = sigma.clip(min=0.0001)\n        w[w < 0.001] = 0\n        w /= w.sum()\n        i += 1\n    if i >= n_itr and verbose:\n        print('Warning: the model does not converge, w_dif=%0.4f' % np.linalg.norm(w - w_old, 1))\n        if f_write is not None:\n            f_write.write('Warning: the model does not converge, w_dif=%0.4f\\n' % np.linalg.norm(w - w_old, 1))\n    if verbose and f_write is not None:\n        f_write.write('## mixture_fit: learned parameters\\n')\n        f_write.write('# Slope: w=%0.4f, a=%s\\n' % (w[0], a))\n        for k in range(K):\n            f_write.write('# Bump %d: w=%0.4f\\n' % (k, w[k + 1]))\n            f_write.write('         mu=%s\\n' % mu[k])\n            f_write.write('      sigma=%s\\n' % sigma[k])\n        f_write.write('\\n')\n    if output_folder is not None:\n        bins_ = np.linspace(0, 1, 101)\n        x_grid = bins_.reshape([-1, 1])\n        if d == 1:\n            plt.figure(figsize=[8, 5])\n            plt.hist(x, bins=bins_, weights=x_w / np.sum(x_w) * 100)\n            temp_p = f_all(x_grid, a, mu, sigma, w)\n            plt.plot(bins_, temp_p)\n            plt.savefig(output_folder + '/projection%s_fold_%d.png' % (suffix, fold_number))\n        else:\n            plt.figure(figsize=[8, 12])\n            n_figure = min(d, 5)\n            for i_dim in range(n_figure):\n                plt.subplot(str(n_figure) + '1' + str(i_dim + 1))\n                plt.hist(x[:, i_dim], bins=bins_, weights=x_w / np.sum(x_w) * 100)\n                temp_p = f_all(x_grid, a[[i_dim]], mu[:, [i_dim]], sigma[:, [i_dim]], w)\n                plt.plot(bins_, temp_p)\n                plt.title('Dimension %d' % (i_dim + 1))\n            plt.savefig(output_folder + '/projection%s_fold_%d.png' % (suffix, fold_number))\n        plt.close('all')\n    return (a, mu, sigma, w)", "fn_id": 14, "class_fn": false, "repo": "martinjzhang/adafdr", "file": "adafdr/method.py", "last_update_at": "2021-05-21T09:23:58+00:00", "original_content": "def mixture_fit(x, K=3, x_w=None, n_itr=100, verbose=False, random_state=0, f_write=None, output_folder=None, suffix=None, fold_number=0):\n    \"\"\"Fit a slope+bump mixture using EM algorithm.\n\n    Args:\n        x ((n,d) ndarray): The covaraites.\n        K (int): The number of bump components.\n        x_w ((n,) ndarray): The weights for each sample.\n        n_itr (int): The maximum number of iterations for the EM algorithm\n        verbose (bool): Indicate if output the computation details.\n        random_state (int): The random seed.\n        f_write (file handler (write mode)): The output file.\n        output_folder (string): The output directory.\n        suffix (string): The suffix of the output file.\n        fold_number (int(0,1)): The fold number.\n\n    Returns:\n        a ((d,) ndarray): slope parameter.\n        mu,sigma ((k,d) ndarray): bump parameter.\n        w ((n,) ndarray): proportion of each component. \n    \"\"\"\n    np.random.seed(random_state)\n    if len(x.shape) == 1:\n        x = x.reshape([-1, 1])\n    n_samp, d = x.shape\n    if x_w is None:\n        x_w = np.ones([n_samp], dtype=float)\n    GMM = GaussianMixture(n_components=K, covariance_type='diag').fit(x)\n    w_old = np.zeros([K + 1])\n    w = 0.5 * np.ones([K + 1]) / K\n    w[0] = 0.5\n    a = ML_slope(x, x_w)\n    mu, sigma = (GMM.means_, GMM.covariances_ ** 0.5)\n    w_samp = np.zeros([K + 1, n_samp], dtype=float)\n    i = 0\n    if verbose:\n        if f_write is not None:\n            f_write.write('## mixture_fit: initialization parameters\\n')\n            f_write.write('# Slope: w=%0.4f, a=%s\\n' % (w[0], a))\n            for k in range(K):\n                f_write.write('# Bump %d: w=%0.4f\\n' % (k, w[k + 1]))\n                f_write.write('         mu=%s\\n' % mu[k])\n                f_write.write('      sigma=%s\\n' % sigma[k])\n            f_write.write('\\n')\n    while np.linalg.norm(w - w_old, 1) > 0.005 and i < n_itr:\n        w_old = w\n        w_samp[0, :] = w[0] * f_slope(x, a)\n        for k in range(K):\n            w_samp[k + 1, :] = w[k + 1] * f_bump(x, mu[k], sigma[k])\n        w_samp = w_samp / np.sum(w_samp, axis=0) * x_w\n        w = np.mean(w_samp, axis=1)\n        a = ML_slope(x, w_samp[0, :])\n        for k in range(K):\n            if w[k + 1] > 0.0001:\n                mu[k], sigma[k] = ML_bump(x, w_samp[k + 1, :])\n        sigma = sigma.clip(min=0.0001)\n        w[w < 0.001] = 0\n        w /= w.sum()\n        i += 1\n    if i >= n_itr and verbose:\n        print('Warning: the model does not converge, w_dif=%0.4f' % np.linalg.norm(w - w_old, 1))\n        if f_write is not None:\n            f_write.write('Warning: the model does not converge, w_dif=%0.4f\\n' % np.linalg.norm(w - w_old, 1))\n    if verbose and f_write is not None:\n        f_write.write('## mixture_fit: learned parameters\\n')\n        f_write.write('# Slope: w=%0.4f, a=%s\\n' % (w[0], a))\n        for k in range(K):\n            f_write.write('# Bump %d: w=%0.4f\\n' % (k, w[k + 1]))\n            f_write.write('         mu=%s\\n' % mu[k])\n            f_write.write('      sigma=%s\\n' % sigma[k])\n        f_write.write('\\n')\n    if output_folder is not None:\n        bins_ = np.linspace(0, 1, 101)\n        x_grid = bins_.reshape([-1, 1])\n        if d == 1:\n            plt.figure(figsize=[8, 5])\n            plt.hist(x, bins=bins_, weights=x_w / np.sum(x_w) * 100)\n            temp_p = f_all(x_grid, a, mu, sigma, w)\n            plt.plot(bins_, temp_p)\n            plt.savefig(output_folder + '/projection%s_fold_%d.png' % (suffix, fold_number))\n        else:\n            plt.figure(figsize=[8, 12])\n            n_figure = min(d, 5)\n            for i_dim in range(n_figure):\n                plt.subplot(str(n_figure) + '1' + str(i_dim + 1))\n                plt.hist(x[:, i_dim], bins=bins_, weights=x_w / np.sum(x_w) * 100)\n                temp_p = f_all(x_grid, a[[i_dim]], mu[:, [i_dim]], sigma[:, [i_dim]], w)\n                plt.plot(bins_, temp_p)\n                plt.title('Dimension %d' % (i_dim + 1))\n            plt.savefig(output_folder + '/projection%s_fold_%d.png' % (suffix, fold_number))\n        plt.close('all')\n    return (a, mu, sigma, w)", "refactored": true, "pred": {"ppl": 2.91039776802063, "ppl_lower": 2.9611167907714844, "ppl/lowercase_ppl": -1.0161723252349517, "ppl/zlib": 0.0007803431423547277, "Min_5.0% Prob": 9.887503773558374, "Min_10.0% Prob": 7.459668456339369, "Min_20.0% Prob": 4.893873291272743, "Min_30.0% Prob": 3.4947325185038687, "Min_40.0% Prob": 2.6598932143929246, "Min_50.0% Prob": 2.136235346696877, "Min_60.0% Prob": 1.7823118573733792}}
{"hexsha": "184edbc2fe82843b354712c8ba7f81417e3fb3d2", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef main(arguments):\n    cap = cv2.VideoCapture(arguments.video)\n    model: tf.keras.Model = tf.keras.models.load_model(arguments.weights_path)\n    model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n    classes = get_classes(os.path.join(arguments.data_path, 'training_set'))\n    success, img = cap.read()\n    while success:\n        img_pp = preprocess_image(img, INPUT_SHAPE)\n        x = model.predict(np.expand_dims(img_pp, 0), batch_size=1)\n        img_out = write_class_on_img(img_pp, classes[int(np.argmax(np.array(x)))])\n        cv2.imshow('EfficientNet Prediction', img_out)\n        cv2.waitKey(10)\n        success, img = cap.read()", "fn_id": 0, "class_fn": false, "repo": "vincenzosantopietro/Action-Recognition-EfficientNet-TensorFlow", "file": "demo.py", "last_update_at": "2021-08-24T06:48:24+00:00", "original_content": "def main(arguments):\n    cap = cv2.VideoCapture(arguments.video)\n    model: tf.keras.Model = tf.keras.models.load_model(arguments.weights_path)\n    model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n    classes = get_classes(os.path.join(arguments.data_path, 'training_set'))\n    success, img = cap.read()\n    while success:\n        img_pp = preprocess_image(img, INPUT_SHAPE)\n        x = model.predict(np.expand_dims(img_pp, 0), batch_size=1)\n        img_out = write_class_on_img(img_pp, classes[int(np.argmax(np.array(x)))])\n        cv2.imshow('EfficientNet Prediction', img_out)\n        cv2.waitKey(10)\n        success, img = cap.read()", "refactored": true, "pred": {"ppl": 4.0056281089782715, "ppl_lower": 5.688302516937256, "ppl/lowercase_ppl": -1.2527285262130976, "ppl/zlib": 0.003265177410421543, "Min_5.0% Prob": 10.28216249292547, "Min_10.0% Prob": 8.529980052601207, "Min_20.0% Prob": 5.994178109698826, "Min_30.0% Prob": 4.459973862942527, "Min_40.0% Prob": 3.45272049668071, "Min_50.0% Prob": 2.7713528233335207, "Min_60.0% Prob": 2.3258355869637692}}
{"hexsha": "3836b014a35e2f166dd6e956235064be019753de", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef createSampleFile(sampleFilePath):\n    if not os.path.exists(sampleFilePath):\n        _dir = os.path.dirname(sampleFilePath)\n        if not os.path.exists(_dir):\n            os.makedirs(_dir, mode=493)\n        import uuid\n        with open(sampleFilePath, 'w') as f:\n            f.write(str(uuid.uuid1()) + '\\n')\n            f.write(str(uuid.uuid4()) + '\\n')\n    return sampleFilePath", "fn_id": 0, "class_fn": false, "repo": "Fanduzi/huaweicloud-sdk-python-obs", "file": "examples/download_sample.py", "last_update_at": "2021-12-13T07:21:48+00:00", "original_content": "def createSampleFile(sampleFilePath):\n    if not os.path.exists(sampleFilePath):\n        _dir = os.path.dirname(sampleFilePath)\n        if not os.path.exists(_dir):\n            os.makedirs(_dir, mode=493)\n        import uuid\n        with open(sampleFilePath, 'w') as f:\n            f.write(str(uuid.uuid1()) + '\\n')\n            f.write(str(uuid.uuid4()) + '\\n')\n    return sampleFilePath", "refactored": true, "pred": {"ppl": 3.5255463123321533, "ppl_lower": 3.5906219482421875, "ppl/lowercase_ppl": -1.0145154854848795, "ppl/zlib": 0.0063318362134553545, "Min_5.0% Prob": 9.976269340515136, "Min_10.0% Prob": 8.249173814600164, "Min_20.0% Prob": 5.858707552370817, "Min_30.0% Prob": 4.184755387476512, "Min_40.0% Prob": 3.1707010587796254, "Min_50.0% Prob": 2.538551805351498, "Min_60.0% Prob": 2.111272014231539}}
{"hexsha": "8ee4e5b90ef2f57ff31c13abfc6ea37b6aa9f151", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef cosine_similarity(A, B):\n    t1 = vector_multiply(A, B)\n    t2 = sqrt(vector_multiply(A, A))\n    t3 = sqrt(vector_multiply(B, B))\n    if t2 > 0 and t3 > 0:\n        return t1 / (t2 * t3)\n    else:\n        return 0", "fn_id": 0, "class_fn": false, "repo": "Pengeace/LncRNA-Disease-link", "file": "sample/parallel_prediction_CV.py", "last_update_at": "2021-12-12T13:15:20+00:00", "original_content": "def cosine_similarity(A, B):\n    t1 = vector_multiply(A, B)\n    t2 = sqrt(vector_multiply(A, A))\n    t3 = sqrt(vector_multiply(B, B))\n    if t2 > 0 and t3 > 0:\n        return t1 / (t2 * t3)\n    else:\n        return 0", "refactored": true, "pred": {"ppl": 3.400538921356201, "ppl_lower": 3.4197754859924316, "ppl/lowercase_ppl": -1.0046088894912035, "ppl/zlib": 0.008269823819882149, "Min_5.0% Prob": 10.175256729125977, "Min_10.0% Prob": 8.71565130021837, "Min_20.0% Prob": 6.0043589274088545, "Min_30.0% Prob": 4.148242824607426, "Min_40.0% Prob": 3.0585880416470603, "Min_50.0% Prob": 2.469167335687772, "Min_60.0% Prob": 2.0682631043886595}}
{"hexsha": "0a460dfb309ddfb8343e04a6b02b1cc8858e4c42", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef import_no_virt_driver_import_deps(physical_line, filename):\n    \"\"\"Check virt drivers' modules aren't imported by other drivers\n\n    Modules under each virt driver's directory are\n    considered private to that virt driver. Other drivers\n    in Nova must not access those drivers. Any code that\n    is to be shared should be refactored into a common\n    module\n\n    N311\n    \"\"\"\n    thisdriver = _get_virt_name(virt_file_re, filename)\n    thatdriver = _get_virt_name(virt_import_re, physical_line)\n    if thatdriver is not None and thisdriver is not None and (thisdriver != thatdriver):\n        return (0, 'N311: importing code from other virt drivers forbidden')", "fn_id": 4, "class_fn": false, "repo": "KevinWang2015/nova", "file": "nova/hacking/checks.py", "last_update_at": "2021-05-12T07:52:44+00:00", "original_content": "def import_no_virt_driver_import_deps(physical_line, filename):\n    \"\"\"Check virt drivers' modules aren't imported by other drivers\n\n    Modules under each virt driver's directory are\n    considered private to that virt driver. Other drivers\n    in Nova must not access those drivers. Any code that\n    is to be shared should be refactored into a common\n    module\n\n    N311\n    \"\"\"\n    thisdriver = _get_virt_name(virt_file_re, filename)\n    thatdriver = _get_virt_name(virt_import_re, physical_line)\n    if thatdriver is not None and thisdriver is not None and (thisdriver != thatdriver):\n        return (0, 'N311: importing code from other virt drivers forbidden')", "refactored": true, "pred": {"ppl": 14.035745620727539, "ppl_lower": 16.591821670532227, "ppl/lowercase_ppl": -1.0633336253773722, "ppl/zlib": 0.007569075456193678, "Min_5.0% Prob": 10.780670404434204, "Min_10.0% Prob": 9.639525357414694, "Min_20.0% Prob": 7.961468001774379, "Min_30.0% Prob": 6.831173149439005, "Min_40.0% Prob": 5.824087347303118, "Min_50.0% Prob": 5.04839273293813, "Min_60.0% Prob": 4.333345870176951}}
{"hexsha": "8a91b3f07767ea74f9a6d8cb283d9cb8dd32dbfa", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef task_factory(name, func=None, vars=None, color='GREEN', ext_in=[], ext_out=[], before=[], after=[], shell=False, scan=None):\n    params = {'vars': vars or [], 'color': color, 'name': name, 'ext_in': Utils.to_list(ext_in), 'ext_out': Utils.to_list(ext_out), 'before': Utils.to_list(before), 'after': Utils.to_list(after), 'shell': shell, 'scan': scan}\n    if isinstance(func, str):\n        params['run_str'] = func\n    else:\n        params['run'] = func\n    cls = type(Task)(name, (Task,), params)\n    global classes\n    classes[name] = cls\n    return cls", "fn_id": 8, "class_fn": false, "repo": "He-Ze/Distributed-System-SYSU", "file": "\u4f5c\u4e1a/\u4f5c\u4e1a5/nack-oriented-reliable-multicast-master/norm-1.5r6/.waf3-1.8.11-930623f783bdf2f01355c2002e2b4462/waflib/Task.py", "last_update_at": "2021-09-17T07:35:05+00:00", "original_content": "def task_factory(name, func=None, vars=None, color='GREEN', ext_in=[], ext_out=[], before=[], after=[], shell=False, scan=None):\n    params = {'vars': vars or [], 'color': color, 'name': name, 'ext_in': Utils.to_list(ext_in), 'ext_out': Utils.to_list(ext_out), 'before': Utils.to_list(before), 'after': Utils.to_list(after), 'shell': shell, 'scan': scan}\n    if isinstance(func, str):\n        params['run_str'] = func\n    else:\n        params['run'] = func\n    cls = type(Task)(name, (Task,), params)\n    global classes\n    classes[name] = cls\n    return cls", "refactored": true, "pred": {"ppl": 4.394602298736572, "ppl_lower": 4.841515064239502, "ppl/lowercase_ppl": -1.0654229709905076, "ppl/zlib": 0.0048536952051245474, "Min_5.0% Prob": 11.239799711439344, "Min_10.0% Prob": 9.524674574534098, "Min_20.0% Prob": 6.791176802582211, "Min_30.0% Prob": 4.804243247075514, "Min_40.0% Prob": 3.702998619169405, "Min_50.0% Prob": 2.953239951363724, "Min_60.0% Prob": 2.4743921569971876}}
{"hexsha": "1407f6d79ec90ec0ceecb5413d805601158c4cae", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_bot() -> Bot:\n    for bot in DRIVER.bots.values():\n        return bot\n    return None", "fn_id": 0, "class_fn": false, "repo": "7sDream/nonebot_plugin_bam", "file": "nonebot_plugin_bam/common.py", "last_update_at": "2021-12-28T07:13:51+00:00", "original_content": "def get_bot() -> Bot:\n    for bot in DRIVER.bots.values():\n        return bot\n    return None", "refactored": true, "pred": {"ppl": 16.406530380249023, "ppl_lower": 32.30416488647461, "ppl/lowercase_ppl": -1.2421709593655386, "ppl/zlib": 0.025433449540814706, "Min_5.0% Prob": 11.121382713317871, "Min_10.0% Prob": 9.706367174784342, "Min_20.0% Prob": 8.615139348166329, "Min_30.0% Prob": 7.821925830841065, "Min_40.0% Prob": 6.4721549068178446, "Min_50.0% Prob": 5.576921666369719, "Min_60.0% Prob": 4.639109378769284}}
{"hexsha": "8b6e4daf34e200f38605973d93690899eefdcfed", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef check_build_is_legit(build):\n    build_string = build.lower()\n    build_number = build_string.replace('build', '')\n    if build_number in BUILD_MAP.keys():\n        return True\n    return False", "fn_id": 1, "class_fn": false, "repo": "EBISPOT/gwas-sumstats-validator", "file": "validate/validator.py", "last_update_at": "2021-04-14T17:38:42+00:00", "original_content": "def check_build_is_legit(build):\n    build_string = build.lower()\n    build_number = build_string.replace('build', '')\n    if build_number in BUILD_MAP.keys():\n        return True\n    return False", "refactored": true, "pred": {"ppl": 9.568812370300293, "ppl_lower": 11.695999145507812, "ppl/lowercase_ppl": -1.0888806388042092, "ppl/zlib": 0.014477622426453503, "Min_5.0% Prob": 10.924391746520996, "Min_10.0% Prob": 9.632004022598267, "Min_20.0% Prob": 7.787576675415039, "Min_30.0% Prob": 6.454040699534946, "Min_40.0% Prob": 5.206667466163635, "Min_50.0% Prob": 4.4071478093824075, "Min_60.0% Prob": 3.786483163769181}}
{"hexsha": "eb4e6315d4870bd2d06b1d9423ac6fcf2195cdeb", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef SOIC(A, B, L, T, W, num_pins, pitch=1.27, profile=ipc.LandPatternSize.Nominal):\n    f = fp.Footprint(name='{0}-SOIC'.format(num_pins), description='{0}-pin SOIC'.format(num_pins))\n    f.from_ipc(lib.SOIC(profile, A, B, L, T, W, num_pins, pitch))\n    return f", "fn_id": 2, "class_fn": false, "repo": "TheWylieStCoyote/kidraw", "file": "kidraw/footprint/library.py", "last_update_at": "2021-01-11T20:14:23+00:00", "original_content": "def SOIC(A, B, L, T, W, num_pins, pitch=1.27, profile=ipc.LandPatternSize.Nominal):\n    f = fp.Footprint(name='{0}-SOIC'.format(num_pins), description='{0}-pin SOIC'.format(num_pins))\n    f.from_ipc(lib.SOIC(profile, A, B, L, T, W, num_pins, pitch))\n    return f", "refactored": true, "pred": {"ppl": 14.528409004211426, "ppl_lower": 16.365434646606445, "ppl/lowercase_ppl": -1.0444920694407185, "ppl/zlib": 0.013248049377651373, "Min_5.0% Prob": 14.934530639648438, "Min_10.0% Prob": 12.017322280190207, "Min_20.0% Prob": 9.659725709394975, "Min_30.0% Prob": 7.756672999438117, "Min_40.0% Prob": 6.410582587454054, "Min_50.0% Prob": 5.242296397686005, "Min_60.0% Prob": 4.4599988977698715}}
{"hexsha": "4f4f6f8ac9dec0a03002fe61bd6214effc3be501", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef change_request_wrapper(func):\n\n    @functools.wraps(func)\n    def _inner(prep_req, *args, **kwargs):\n        prep_req.url = prep_req.url + '?some=stuff'\n        return func(prep_req, *args, **kwargs)\n    return _inner", "fn_id": 0, "class_fn": false, "repo": "HappyEinara/pytest-vts", "file": "tests/test_request_wrapper.py", "last_update_at": "2021-12-13T23:15:20+00:00", "original_content": "def change_request_wrapper(func):\n\n    @functools.wraps(func)\n    def _inner(prep_req, *args, **kwargs):\n        prep_req.url = prep_req.url + '?some=stuff'\n        return func(prep_req, *args, **kwargs)\n    return _inner", "refactored": true, "pred": {"ppl": 6.288315296173096, "ppl_lower": 6.288315296173096, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.012096665765730118, "Min_5.0% Prob": 11.705751657485962, "Min_10.0% Prob": 9.670778214931488, "Min_20.0% Prob": 7.296301007270813, "Min_30.0% Prob": 5.832411706447601, "Min_40.0% Prob": 4.596815161406994, "Min_50.0% Prob": 3.7150547660887243, "Min_60.0% Prob": 3.1019529254796603}}
{"hexsha": "5ad02454c22c71588ac4c352b70a914a8fe47bc8", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_set_args_settings(mocker):\n    mocker.patch('json.load', new=lambda x: x())\n    args = Namespace()\n    set_args(args)\n    assert args.settings == {}\n    args = Namespace(settings=None)\n    set_args(args)\n    assert args.settings == {}\n    s = list(range(3))\n    mock = MagicMock(return_value=s)\n    args = Namespace(settings=mock)\n    set_args(args)\n    assert args.settings == s\n    mock.close.assert_called_with()", "fn_id": 10, "class_fn": false, "repo": "akx/markovchain", "file": "tests/cli/test_util.py", "last_update_at": "2021-06-05T15:38:34+00:00", "original_content": "def test_set_args_settings(mocker):\n    mocker.patch('json.load', new=lambda x: x())\n    args = Namespace()\n    set_args(args)\n    assert args.settings == {}\n    args = Namespace(settings=None)\n    set_args(args)\n    assert args.settings == {}\n    s = list(range(3))\n    mock = MagicMock(return_value=s)\n    args = Namespace(settings=mock)\n    set_args(args)\n    assert args.settings == s\n    mock.close.assert_called_with()", "refactored": true, "pred": {"ppl": 4.343976020812988, "ppl_lower": 5.162214279174805, "ppl/lowercase_ppl": -1.1174950411037012, "ppl/zlib": 0.006768617799749879, "Min_5.0% Prob": 10.415387312571207, "Min_10.0% Prob": 8.970945651714619, "Min_20.0% Prob": 6.316649582650927, "Min_30.0% Prob": 4.673718282580376, "Min_40.0% Prob": 3.599864783662337, "Min_50.0% Prob": 2.938120286411314, "Min_60.0% Prob": 2.444337601279035}}
{"hexsha": "714d3dfaa367d1733c257ba0266824ee7c931162", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef getAllClass(p, period):\n    data = {}\n    data['{}'.format(period)] = getRawAssignments(p, period)\n    data['assignments'] = data.pop('{}'.format(period))\n    data['info'] = getRawClass(p)['{}'.format(period)]\n    return data", "fn_id": 5, "class_fn": false, "repo": "Grant2464/powerschool_api", "file": "powerschool.py", "last_update_at": "2021-11-02T17:43:43+00:00", "original_content": "def getAllClass(p, period):\n    data = {}\n    data['{}'.format(period)] = getRawAssignments(p, period)\n    data['assignments'] = data.pop('{}'.format(period))\n    data['info'] = getRawClass(p)['{}'.format(period)]\n    return data", "refactored": true, "pred": {"ppl": 11.79649543762207, "ppl_lower": 11.728620529174805, "ppl/lowercase_ppl": -0.9976617104626914, "ppl/zlib": 0.016235542700971335, "Min_5.0% Prob": 12.971001307169596, "Min_10.0% Prob": 10.711329187665667, "Min_20.0% Prob": 8.83963918685913, "Min_30.0% Prob": 7.1790173487229785, "Min_40.0% Prob": 6.0484942732186155, "Min_50.0% Prob": 4.863715141206174, "Min_60.0% Prob": 4.1225409213114865}}
{"hexsha": "4bf042cd1a9ebeebeb1cce1bd259939ff32e8532", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef ret_a_dict(existing):\n    \"\"\"\n    Append a key-value to 'existing' (if it exists)\n    \"\"\"\n    existing = existing or {}\n    ret = {'secret': 42}\n    ret.update(existing)\n    return ret", "fn_id": 1, "class_fn": false, "repo": "siriobalmelli/replacement", "file": "tests/demo.py", "last_update_at": "2021-02-07T16:24:31+00:00", "original_content": "def ret_a_dict(existing):\n    \"\"\"\n    Append a key-value to 'existing' (if it exists)\n    \"\"\"\n    existing = existing or {}\n    ret = {'secret': 42}\n    ret.update(existing)\n    return ret", "refactored": true, "pred": {"ppl": 12.868851661682129, "ppl_lower": 13.522246360778809, "ppl/lowercase_ppl": -1.0193855590618437, "ppl/zlib": 0.017262228321963762, "Min_5.0% Prob": 11.661168734232584, "Min_10.0% Prob": 10.492083072662354, "Min_20.0% Prob": 8.640664537747702, "Min_30.0% Prob": 7.101945688850002, "Min_40.0% Prob": 5.9720472145080565, "Min_50.0% Prob": 4.956553269177675, "Min_60.0% Prob": 4.261049723938892}}
{"hexsha": "0e12d22364ddbf0f90693961db9219ea9d1c3443", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_uwsgi_emperor_opts(settings, port):\n    if settings['DATABASES']['default'].get('PASSWORD'):\n        os.environ['DB_PASSWORD'] = settings['DATABASES']['default']['PASSWORD']\n        pg_str = 'pg://host={HOST} port={PORT} user={USER} password=$(DB_PASSWORD) dbname={NAME};{query}'\n    else:\n        pg_str = 'pg://host={HOST} port={PORT} user={USER} dbname={NAME};{query}'\n    return get_uwsgi_regular_opts(settings, port) + ['http = 0.0.0.0:{}'.format(port or settings.get('PORT')), 'http-to = /tmp/fastrouter.sock', 'fastrouter = /tmp/fastrouter.sock', 'fastrouter-use-code-string = 0:{}:get'.format(os.path.join(HERE, 'fastrouter_lookup.py')), 'emperor = {}'.format(pg_str.format(query=VASSALS_SQL_QUERY, **settings['DATABASES']['default']))]", "fn_id": 3, "class_fn": false, "repo": "yakky/django-multisite-plus", "file": "django_multisite_plus/cli.py", "last_update_at": "2021-08-17T20:18:51+00:00", "original_content": "def get_uwsgi_emperor_opts(settings, port):\n    if settings['DATABASES']['default'].get('PASSWORD'):\n        os.environ['DB_PASSWORD'] = settings['DATABASES']['default']['PASSWORD']\n        pg_str = 'pg://host={HOST} port={PORT} user={USER} password=$(DB_PASSWORD) dbname={NAME};{query}'\n    else:\n        pg_str = 'pg://host={HOST} port={PORT} user={USER} dbname={NAME};{query}'\n    return get_uwsgi_regular_opts(settings, port) + ['http = 0.0.0.0:{}'.format(port or settings.get('PORT')), 'http-to = /tmp/fastrouter.sock', 'fastrouter = /tmp/fastrouter.sock', 'fastrouter-use-code-string = 0:{}:get'.format(os.path.join(HERE, 'fastrouter_lookup.py')), 'emperor = {}'.format(pg_str.format(query=VASSALS_SQL_QUERY, **settings['DATABASES']['default']))]", "refactored": true, "pred": {"ppl": 7.156440258026123, "ppl_lower": 7.17891788482666, "ppl/lowercase_ppl": -1.0015934717433097, "ppl/zlib": 0.005020440527233411, "Min_5.0% Prob": 12.5131729443868, "Min_10.0% Prob": 10.264414234161377, "Min_20.0% Prob": 7.79605393409729, "Min_30.0% Prob": 6.18478892978869, "Min_40.0% Prob": 4.902698812803419, "Min_50.0% Prob": 3.929267865231657, "Min_60.0% Prob": 3.287656984310352}}
{"hexsha": "d83d43b9c49703cda927b344f9e7b3b7da45f5b3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef read_kitti_annotations(label_paths, calib_paths):\n    all_labels = []\n    all_calib = []\n    for label_file, calib_file in zip(label_paths, calib_paths):\n        calib = o3d.ml.datasets.KITTI.read_calib(calib_file)\n        labels = o3d.ml.datasets.KITTI.read_label(label_file, calib)\n        all_labels.append(labels)\n        all_calib.append(calib)\n    return (all_labels, all_calib)", "fn_id": 1, "class_fn": false, "repo": "supervisely-ecosystem/Open3D-ML", "file": "supervisely/src_backup/convert_kitty3d_to_sly.py", "last_update_at": "2021-08-31T09:06:08+00:00", "original_content": "def read_kitti_annotations(label_paths, calib_paths):\n    all_labels = []\n    all_calib = []\n    for label_file, calib_file in zip(label_paths, calib_paths):\n        calib = o3d.ml.datasets.KITTI.read_calib(calib_file)\n        labels = o3d.ml.datasets.KITTI.read_label(label_file, calib)\n        all_labels.append(labels)\n        all_calib.append(calib)\n    return (all_labels, all_calib)", "refactored": true, "pred": {"ppl": 3.123892307281494, "ppl_lower": 3.17179274559021, "ppl/lowercase_ppl": -1.013359208714808, "ppl/zlib": 0.005901967661565057, "Min_5.0% Prob": 10.414536794026693, "Min_10.0% Prob": 8.36241626739502, "Min_20.0% Prob": 5.496811085277134, "Min_30.0% Prob": 3.8074057465646325, "Min_40.0% Prob": 2.853541292554953, "Min_50.0% Prob": 2.277367235350328, "Min_60.0% Prob": 1.9169139449006207}}
{"hexsha": "07fe364d937fe8b983df03d9c0471b986fbd990b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _add_workload_inner():\n    OpArgMngr.add_workload('inner', np.zeros(shape=(1, 80), dtype=np.float64), np.zeros(shape=(1, 80), dtype=np.float64))\n    for dt in [np.float32, np.float64]:\n        A = np.array([[1, 2], [3, 4]], dtype=dt)\n        B = np.array([[1, 3], [2, 4]], dtype=dt)\n        C = np.array([1, 1], dtype=dt)\n        OpArgMngr.add_workload('inner', A.T, C)\n        OpArgMngr.add_workload('inner', C, A.T)\n        OpArgMngr.add_workload('inner', B, C)\n        OpArgMngr.add_workload('inner', C, B)\n        OpArgMngr.add_workload('inner', A, B)\n        OpArgMngr.add_workload('inner', A, A)\n        OpArgMngr.add_workload('inner', A, A.copy())\n        a = np.arange(5).astype(dt)\n        b = a[::-1]\n        OpArgMngr.add_workload('inner', b, a)\n        a = np.arange(24).reshape(2, 3, 4).astype(dt)\n        b = np.arange(24, 48).reshape(2, 3, 4).astype(dt)\n        OpArgMngr.add_workload('inner', a, b)\n        OpArgMngr.add_workload('inner', b, a)", "fn_id": 74, "class_fn": false, "repo": "sxjscience/mxnet", "file": "tests/python/unittest/test_numpy_interoperability.py", "last_update_at": "2021-03-23T03:07:38+00:00", "original_content": "def _add_workload_inner():\n    OpArgMngr.add_workload('inner', np.zeros(shape=(1, 80), dtype=np.float64), np.zeros(shape=(1, 80), dtype=np.float64))\n    for dt in [np.float32, np.float64]:\n        A = np.array([[1, 2], [3, 4]], dtype=dt)\n        B = np.array([[1, 3], [2, 4]], dtype=dt)\n        C = np.array([1, 1], dtype=dt)\n        OpArgMngr.add_workload('inner', A.T, C)\n        OpArgMngr.add_workload('inner', C, A.T)\n        OpArgMngr.add_workload('inner', B, C)\n        OpArgMngr.add_workload('inner', C, B)\n        OpArgMngr.add_workload('inner', A, B)\n        OpArgMngr.add_workload('inner', A, A)\n        OpArgMngr.add_workload('inner', A, A.copy())\n        a = np.arange(5).astype(dt)\n        b = a[::-1]\n        OpArgMngr.add_workload('inner', b, a)\n        a = np.arange(24).reshape(2, 3, 4).astype(dt)\n        b = np.arange(24, 48).reshape(2, 3, 4).astype(dt)\n        OpArgMngr.add_workload('inner', a, b)\n        OpArgMngr.add_workload('inner', b, a)", "refactored": true, "pred": {"ppl": 2.3556835651397705, "ppl_lower": 2.4872453212738037, "ppl/lowercase_ppl": -1.0634254108857304, "ppl/zlib": 0.0029751074533215086, "Min_5.0% Prob": 8.188955783843994, "Min_10.0% Prob": 6.04066296627647, "Min_20.0% Prob": 3.933376326189413, "Min_30.0% Prob": 2.8063559239325317, "Min_40.0% Prob": 2.134259532556518, "Min_50.0% Prob": 1.7114009845500011, "Min_60.0% Prob": 1.4313670494291302}}
{"hexsha": "0460f6e42c23c688c2c6afb0f673aa2cf4d651bf", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef load_state(network, state_dict):\n    net_state_keys = list(network.state_dict().keys())\n    net_state_keys_copy = net_state_keys.copy()\n    sup_string = ''\n    for key in state_dict.keys():\n        if 'backbone' in key:\n            sup_string = 'backbone.'\n        elif 'module' in key:\n            sup_string = 'module.'\n    for i, _ in enumerate(net_state_keys_copy):\n        name = net_state_keys_copy[i]\n        if name.startswith('classifier') or name.startswith('fc'):\n            continue\n        if not sup_string:\n            name_pretrained = name\n        else:\n            name_pretrained = sup_string + name\n        if name_pretrained in state_dict.keys():\n            dst_param_shape = network.state_dict()[name].shape\n            if state_dict[name_pretrained].shape == dst_param_shape:\n                network.state_dict()[name].copy_(state_dict[name_pretrained].view(dst_param_shape))\n                net_state_keys.remove(name)\n    if net_state_keys:\n        num_batches_list = []\n        for i in range(len(net_state_keys)):\n            if 'num_batches_tracked' in net_state_keys[i]:\n                num_batches_list.append(net_state_keys[i])\n        pruned_additional_states = [x for x in net_state_keys if x not in num_batches_list]\n        if pruned_additional_states:\n            logging.info('There are layers in current network not initialized by pretrained')\n            logging.warning('>> Failed to load: {}'.format(pruned_additional_states))\n        return False\n    return True", "fn_id": 0, "class_fn": false, "repo": "xuyu0010/PATAN", "file": "network/util.py", "last_update_at": "2021-12-27T19:45:45+00:00", "original_content": "def load_state(network, state_dict):\n    net_state_keys = list(network.state_dict().keys())\n    net_state_keys_copy = net_state_keys.copy()\n    sup_string = ''\n    for key in state_dict.keys():\n        if 'backbone' in key:\n            sup_string = 'backbone.'\n        elif 'module' in key:\n            sup_string = 'module.'\n    for i, _ in enumerate(net_state_keys_copy):\n        name = net_state_keys_copy[i]\n        if name.startswith('classifier') or name.startswith('fc'):\n            continue\n        if not sup_string:\n            name_pretrained = name\n        else:\n            name_pretrained = sup_string + name\n        if name_pretrained in state_dict.keys():\n            dst_param_shape = network.state_dict()[name].shape\n            if state_dict[name_pretrained].shape == dst_param_shape:\n                network.state_dict()[name].copy_(state_dict[name_pretrained].view(dst_param_shape))\n                net_state_keys.remove(name)\n    if net_state_keys:\n        num_batches_list = []\n        for i in range(len(net_state_keys)):\n            if 'num_batches_tracked' in net_state_keys[i]:\n                num_batches_list.append(net_state_keys[i])\n        pruned_additional_states = [x for x in net_state_keys if x not in num_batches_list]\n        if pruned_additional_states:\n            logging.info('There are layers in current network not initialized by pretrained')\n            logging.warning('>> Failed to load: {}'.format(pruned_additional_states))\n        return False\n    return True", "refactored": true, "pred": {"ppl": 2.955505132675171, "ppl_lower": 3.0564143657684326, "ppl/lowercase_ppl": -1.0309807304332301, "ppl/zlib": 0.001999390365189745, "Min_5.0% Prob": 10.024560332298279, "Min_10.0% Prob": 7.565358680486679, "Min_20.0% Prob": 4.910208193957805, "Min_30.0% Prob": 3.5347688900927703, "Min_40.0% Prob": 2.6941595745753055, "Min_50.0% Prob": 2.169769463217377, "Min_60.0% Prob": 1.811668657790483}}
{"hexsha": "f6b4cf055a04d3fdf48e80ef79e23515af8b5c83", "ext": "py", "lang": "Python", "content": "@pytest.fixture(scope='function')\ndef boto_status():\n    status_mock = Mock()\n    status_mock.get_status.side_effect = ['PENDING', 'INSYNC']\n\n    @timeing\n    @measure_memory_usage\n    def update():\n        status_mock.status = status_mock.get_status()\n    status_mock.update.side_effect = update\n    status_mock.status = 'PENDING'\n    return status_mock", "fn_id": 0, "class_fn": false, "repo": "dron22/r53dyndns", "file": "r53dyndns/tests/test_r53updater.py", "last_update_at": "2021-05-13T21:14:44+00:00", "original_content": "@pytest.fixture(scope='function')\ndef boto_status():\n    status_mock = Mock()\n    status_mock.get_status.side_effect = ['PENDING', 'INSYNC']\n\n    def update():\n        status_mock.status = status_mock.get_status()\n    status_mock.update.side_effect = update\n    status_mock.status = 'PENDING'\n    return status_mock", "refactored": true, "pred": {"ppl": 7.027451515197754, "ppl_lower": 8.238126754760742, "ppl/lowercase_ppl": -1.081519587338741, "ppl/zlib": 0.010262232234098978, "Min_5.0% Prob": 12.946261882781982, "Min_10.0% Prob": 10.759313901265463, "Min_20.0% Prob": 7.722879284306576, "Min_30.0% Prob": 6.022141637473271, "Min_40.0% Prob": 4.778888216385474, "Min_50.0% Prob": 3.8757947218053195, "Min_60.0% Prob": 3.288956934008105}}
{"hexsha": "cc8172756946d110aef633b85d99fdb465e813dc", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef used_tracks_set(image, all=False):\n    \"\"\"Determine the set of tracks used by directory and files on the disk\"\"\"\n    bam = Disk.from_image(image).bam()\n    used_tracks = set()\n    for head in range(2):\n        for cyl in range(80):\n            track = head << 7 | cyl\n            offset = (80 * head + cyl - 4) * image.spt\n            if all or track <= 4 or bam[offset:offset + image.spt].any():\n                used_tracks.add(track)\n    return used_tracks", "fn_id": 1, "class_fn": false, "repo": "simonowen/writeusb", "file": "mgtwriteusb/writeusb.py", "last_update_at": "2021-08-03T21:10:03+00:00", "original_content": "def used_tracks_set(image, all=False):\n    \"\"\"Determine the set of tracks used by directory and files on the disk\"\"\"\n    bam = Disk.from_image(image).bam()\n    used_tracks = set()\n    for head in range(2):\n        for cyl in range(80):\n            track = head << 7 | cyl\n            offset = (80 * head + cyl - 4) * image.spt\n            if all or track <= 4 or bam[offset:offset + image.spt].any():\n                used_tracks.add(track)\n    return used_tracks", "refactored": true, "pred": {"ppl": 10.466621398925781, "ppl_lower": 11.721403121948242, "ppl/lowercase_ppl": -1.0482180557396665, "ppl/zlib": 0.008697004738123515, "Min_5.0% Prob": 12.947271347045898, "Min_10.0% Prob": 10.677603551319667, "Min_20.0% Prob": 8.294488758876406, "Min_30.0% Prob": 6.760615304458973, "Min_40.0% Prob": 5.561804582332742, "Min_50.0% Prob": 4.609199198549741, "Min_60.0% Prob": 3.9169789278644256}}
{"hexsha": "0261d48fadbcba94432255ef1726dce932739202", "ext": "py", "lang": "Python", "content": "@deprecated(version='0.2.0', reason='Use AnyLE instead')\n@timeing\n@measure_memory_usage\ndef any_le(bound: Any) -> Any:\n    \"\"\"\n    Returns a matcher that matches any value less than or equal to ``bound``\n    \"\"\"\n    return AnyLE(bound)", "fn_id": 13, "class_fn": false, "repo": "jwodder/anys", "file": "src/anys/__init__.py", "last_update_at": "2021-12-14T23:07:16+00:00", "original_content": "@deprecated(version='0.2.0', reason='Use AnyLE instead')\ndef any_le(bound: Any) -> Any:\n    \"\"\"\n    Returns a matcher that matches any value less than or equal to ``bound``\n    \"\"\"\n    return AnyLE(bound)", "refactored": true, "pred": {"ppl": 9.599372863769531, "ppl_lower": 15.667804718017578, "ppl/lowercase_ppl": -1.2166116927311694, "ppl/zlib": 0.01292398725513849, "Min_5.0% Prob": 13.312609354654947, "Min_10.0% Prob": 11.657188256581625, "Min_20.0% Prob": 7.902851434854361, "Min_30.0% Prob": 6.146986520290374, "Min_40.0% Prob": 5.0164151235863015, "Min_50.0% Prob": 4.242681888973012, "Min_60.0% Prob": 3.7322974257171153}}
{"hexsha": "8521926dba5b85612435ecc4000212db9efd4499", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_previous_benefit_put_unauthenticated(anonymous_client, previous_benefit):\n    data = PreviousBenefitSerializer(previous_benefit).data\n    data['monthly_amount'] = '1234.56'\n    response = anonymous_client.put(get_previous_benefits_detail_url(previous_benefit), data)\n    assert response.status_code == 403", "fn_id": 7, "class_fn": false, "repo": "City-of-Helsinki/kesaseteli", "file": "backend/benefit/calculator/tests/test_previous_benefits_api.py", "last_update_at": "2021-05-17T12:15:34+00:00", "original_content": "def test_previous_benefit_put_unauthenticated(anonymous_client, previous_benefit):\n    data = PreviousBenefitSerializer(previous_benefit).data\n    data['monthly_amount'] = '1234.56'\n    response = anonymous_client.put(get_previous_benefits_detail_url(previous_benefit), data)\n    assert response.status_code == 403", "refactored": true, "pred": {"ppl": 7.224177360534668, "ppl_lower": 8.609663963317871, "ppl/lowercase_ppl": -1.0887270967199658, "ppl/zlib": 0.009646016426255262, "Min_5.0% Prob": 13.869967699050903, "Min_10.0% Prob": 11.264585812886557, "Min_20.0% Prob": 8.173768683483726, "Min_30.0% Prob": 6.275518224157136, "Min_40.0% Prob": 4.978997635213952, "Min_50.0% Prob": 3.9773010965436697, "Min_60.0% Prob": 3.3035078991589875}}
{"hexsha": "9c1f3df34d1f27d3dbdd982cd272b1268f3600c6", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef inference(net, data_label, use_aux):\n    if use_aux:\n        img, cls_label, seg_label = data_label\n        img, cls_label, seg_label = (img.cuda(), cls_label.long().cuda(), seg_label.long().cuda())\n        cls_out, seg_out = net(img)\n        return {'cls_out': cls_out, 'cls_label': cls_label, 'seg_out': seg_out, 'seg_label': seg_label}\n    else:\n        img, cls_label = data_label\n        img, cls_label = (img.cuda(), cls_label.long().cuda())\n        cls_out = net(img)\n        return {'cls_out': cls_out, 'cls_label': cls_label}", "fn_id": 0, "class_fn": false, "repo": "kjannakh/Ultra-Fast-Lane-Detection", "file": "train.py", "last_update_at": "2021-09-12T20:02:53+00:00", "original_content": "def inference(net, data_label, use_aux):\n    if use_aux:\n        img, cls_label, seg_label = data_label\n        img, cls_label, seg_label = (img.cuda(), cls_label.long().cuda(), seg_label.long().cuda())\n        cls_out, seg_out = net(img)\n        return {'cls_out': cls_out, 'cls_label': cls_label, 'seg_out': seg_out, 'seg_label': seg_label}\n    else:\n        img, cls_label = data_label\n        img, cls_label = (img.cuda(), cls_label.long().cuda())\n        cls_out = net(img)\n        return {'cls_out': cls_out, 'cls_label': cls_label}", "refactored": true, "pred": {"ppl": 2.6090753078460693, "ppl_lower": 2.6090753078460693, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.0049432776823615165, "Min_5.0% Prob": 10.151822514004177, "Min_10.0% Prob": 7.9351758691999645, "Min_20.0% Prob": 4.780576148548642, "Min_30.0% Prob": 3.2113556596450508, "Min_40.0% Prob": 2.40280536102131, "Min_50.0% Prob": 1.917769688326479, "Min_60.0% Prob": 1.6096797568887788}}
{"hexsha": "9cf488ea53f16e45eefa82773cf77f48e6b49c9a", "ext": "py", "lang": "Python", "content": "@pytest.fixture\n@timeing\n@measure_memory_usage\ndef mock_trained_emulator(mock_emulator):\n    filename = os.path.join(test_base, 'data', 'emu.hdf5')\n    if os.path.exists(filename):\n        yield Emulator.load(filename)\n    else:\n        mock_emulator.train()\n        mock_emulator.save(filename)\n        yield mock_emulator", "fn_id": 6, "class_fn": false, "repo": "spencerhurt/Starfish", "file": "tests/conftest.py", "last_update_at": "2021-05-13T16:23:26+00:00", "original_content": "@pytest.fixture\ndef mock_trained_emulator(mock_emulator):\n    filename = os.path.join(test_base, 'data', 'emu.hdf5')\n    if os.path.exists(filename):\n        yield Emulator.load(filename)\n    else:\n        mock_emulator.train()\n        mock_emulator.save(filename)\n        yield mock_emulator", "refactored": true, "pred": {"ppl": 5.796689987182617, "ppl_lower": 7.008589267730713, "ppl/lowercase_ppl": -1.1080354919504394, "ppl/zlib": 0.00981724616078616, "Min_5.0% Prob": 11.476837873458862, "Min_10.0% Prob": 9.79405434926351, "Min_20.0% Prob": 7.316648827658759, "Min_30.0% Prob": 5.543178465631273, "Min_40.0% Prob": 4.350978637735049, "Min_50.0% Prob": 3.535142655836211, "Min_60.0% Prob": 2.9559936415128134}}
{"hexsha": "511d2698d35dc4f9fe7517171c56725c07784116", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef make_char_dict():\n    chars = string.ascii_lowercase\n    char_dict = {'<blank>': 0}\n    for idx, c in enumerate(chars):\n        char_dict[c] = idx + 1\n    current_len = len(list(char_dict.keys()))\n    char_dict['<eos>'] = current_len\n    print(char_dict)\n    return char_dict", "fn_id": 0, "class_fn": false, "repo": "duguqiankun/lips_reading", "file": "cnn2d/cnn2d_image_generator.py", "last_update_at": "2021-11-14T13:10:30+00:00", "original_content": "def make_char_dict():\n    chars = string.ascii_lowercase\n    char_dict = {'<blank>': 0}\n    for idx, c in enumerate(chars):\n        char_dict[c] = idx + 1\n    current_len = len(list(char_dict.keys()))\n    char_dict['<eos>'] = current_len\n    print(char_dict)\n    return char_dict", "refactored": true, "pred": {"ppl": 4.482478141784668, "ppl_lower": 4.482478141784668, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.007772932902282454, "Min_5.0% Prob": 11.528032302856445, "Min_10.0% Prob": 9.197116374969482, "Min_20.0% Prob": 6.660572874546051, "Min_30.0% Prob": 4.902154892683029, "Min_40.0% Prob": 3.7299376912415028, "Min_50.0% Prob": 2.996885491237044, "Min_60.0% Prob": 2.500000185876464}}
{"hexsha": "67a636a6cf8ea63426b938047e0e18deaa078baf", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_project_creation_without_pylint(cookies):\n    with generate_temporary_project(cookies, extra_context=NO_PLINT) as result:\n        assert_successful_creation(result)\n        assert_expected_files_exist(result, files=EXPECTED_PROJECT_FILES_NO_PYLINT)\n        assert_expected_files_do_not_exist(result, files=('pylintrc',))", "fn_id": 4, "class_fn": false, "repo": "mendix/python-project-template", "file": "tests/test_project_creation.py", "last_update_at": "2021-07-31T12:31:00+00:00", "original_content": "def test_project_creation_without_pylint(cookies):\n    with generate_temporary_project(cookies, extra_context=NO_PLINT) as result:\n        assert_successful_creation(result)\n        assert_expected_files_exist(result, files=EXPECTED_PROJECT_FILES_NO_PYLINT)\n        assert_expected_files_do_not_exist(result, files=('pylintrc',))", "refactored": true, "pred": {"ppl": 8.942277908325195, "ppl_lower": 9.968742370605469, "ppl/lowercase_ppl": -1.0496004001577992, "ppl/zlib": 0.00978031409070745, "Min_5.0% Prob": 10.65205135345459, "Min_10.0% Prob": 9.593924570083619, "Min_20.0% Prob": 7.62314624786377, "Min_30.0% Prob": 6.407758839925131, "Min_40.0% Prob": 5.296295312047005, "Min_50.0% Prob": 4.324269706127691, "Min_60.0% Prob": 3.6515812514746777}}
{"hexsha": "4ab2bfd86a6ca0acdb74024739a1d2b31d9cbebd", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef zero_shot_transform(image, caption, p=0.5, transform_to_apply='solarized'):\n    \"\"\"\n    Output PIL.Image of a two-panel style transfer image with an english word describing the transform included in the caption.\n    See OpenAI DALL-E blog post for more details:\n    The top half of the image is the original image, and the bottom half is a stylized image.\n    \"\"\"\n    if p >= 0.5:\n        friendly_transform = transform_to_apply.replace('_', ' ')\n        style_caption = 'Two panel image of the exact same picture.' + f'On the top {caption} and on the bottom the same image but with {friendly_transform} applied.' + f'The original image is on the top and the {friendly_transform} image on the bottom. The caption is {caption}.'\n        style_image_transform = transform_lookup[transform_to_apply]\n        return (two_panel_style_transfer(image, img_transform=style_image_transform, resize_ratio=1.0), style_caption)\n    return (image, style_caption)", "fn_id": 3, "class_fn": false, "repo": "afiaka87/ZeroShotTransform", "file": "transforms.py", "last_update_at": "2021-07-20T00:07:04+00:00", "original_content": "def zero_shot_transform(image, caption, p=0.5, transform_to_apply='solarized'):\n    \"\"\"\n    Output PIL.Image of a two-panel style transfer image with an english word describing the transform included in the caption.\n    See OpenAI DALL-E blog post for more details:\n    The top half of the image is the original image, and the bottom half is a stylized image.\n    \"\"\"\n    if p >= 0.5:\n        friendly_transform = transform_to_apply.replace('_', ' ')\n        style_caption = 'Two panel image of the exact same picture.' + f'On the top {caption} and on the bottom the same image but with {friendly_transform} applied.' + f'The original image is on the top and the {friendly_transform} image on the bottom. The caption is {caption}.'\n        style_image_transform = transform_lookup[transform_to_apply]\n        return (two_panel_style_transfer(image, img_transform=style_image_transform, resize_ratio=1.0), style_caption)\n    return (image, style_caption)", "refactored": true, "pred": {"ppl": 10.95877742767334, "ppl_lower": 11.572640419006348, "ppl/lowercase_ppl": -1.0227651617825329, "ppl/zlib": 0.005072332048152361, "Min_5.0% Prob": 12.075497070948282, "Min_10.0% Prob": 10.648797264099121, "Min_20.0% Prob": 8.246815550561045, "Min_30.0% Prob": 6.587897390514225, "Min_40.0% Prob": 5.463476585406883, "Min_50.0% Prob": 4.609749157447368, "Min_60.0% Prob": 3.935331475812119}}
{"hexsha": "b605c74be7fc59d0693eaf81e6720e58b5e14ac4", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef func_645e7dad5f4c4c1db11925e9517ff3b9(infile):\n    n, p, q, r, s = [int(x) for x in infile.readline().split()]\n    dev = [(i * p + q) % r + s for i in range(n)]\n    tot = sum(dev)\n    i = 0\n    return q", "fn_id": 88, "class_fn": false, "repo": "DynamicCodeSearch/CodeSeer", "file": "projects/src/main/python/CodeJam/Y14R5P1/Smithers/generated_py_263a1af396df4e8fa1f96950f5309feb.py", "last_update_at": "2021-04-13T20:34:19+00:00", "original_content": "def func_645e7dad5f4c4c1db11925e9517ff3b9(infile):\n    n, p, q, r, s = [int(x) for x in infile.readline().split()]\n    dev = [(i * p + q) % r + s for i in range(n)]\n    tot = sum(dev)\n    i = 0\n    return q", "refactored": true, "pred": {"ppl": 9.980995178222656, "ppl_lower": 9.980995178222656, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.012369262379616755, "Min_5.0% Prob": 12.2878812789917, "Min_10.0% Prob": 10.043873357772828, "Min_20.0% Prob": 7.430293525968279, "Min_30.0% Prob": 6.057229688090663, "Min_40.0% Prob": 5.157986771492731, "Min_50.0% Prob": 4.481246909269919, "Min_60.0% Prob": 3.805696114188149}}
{"hexsha": "fa024eaa7abd499c2cfb6beea3f81e7389c03dac", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_pblog(*args, **kwargs):\n    global pblog\n    if pblog is None:\n        pblog = ProgressBarLog(*args, **kwargs)\n    return pblog", "fn_id": 0, "class_fn": false, "repo": "youngyzzZ/Sonographic_Gallbladder_Diagnose", "file": "src/logger.py", "last_update_at": "2021-07-16T10:20:23+00:00", "original_content": "def get_pblog(*args, **kwargs):\n    global pblog\n    if pblog is None:\n        pblog = ProgressBarLog(*args, **kwargs)\n    return pblog", "refactored": true, "pred": {"ppl": 10.045306205749512, "ppl_lower": 16.865922927856445, "ppl/lowercase_ppl": -1.224605989837451, "ppl/zlib": 0.019066987447949924, "Min_5.0% Prob": 12.688464641571045, "Min_10.0% Prob": 11.564034223556519, "Min_20.0% Prob": 9.399347994062635, "Min_30.0% Prob": 7.582611049924578, "Min_40.0% Prob": 5.834349531876414, "Min_50.0% Prob": 4.676338247333963, "Min_60.0% Prob": 3.888911601027538}}
{"hexsha": "1183023f80519f66fa1c64d6437b4cc896c39a21", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef main():\n    args = arguments().parse_args()\n    sys.path.insert(0, os.getcwd())\n    conn = get_connection(args)\n    headers = None\n    editfile = tempfile.NamedTemporaryFile()\n    with open(editfile.name, 'w', newline='') as cfile:\n        headers = write_csv(cfile, conn, args.table)\n    reffile = tempfile.NamedTemporaryFile()\n    shutil.copy(editfile.name, reffile.name)\n    call_vim(editfile.name)\n    while True:\n        try:\n            process_changes(reffile, editfile, conn, args.table, headers)\n            break\n        except Exception as err:\n            print('Failed to save changes: ', err)\n            q = '(C)ancel, (r)etry save, (e)dit the file again?'\n            cmd = query_options(q, ('c', 'r', 'e'))\n            if cmd == 'c':\n                print('Changes cancelled')\n                break\n            elif cmd == 'r':\n                continue\n            elif cmd == 'e':\n                call_vim(editfile.name)\n            else:\n                raise ValueError('Not a command')", "fn_id": 14, "class_fn": false, "repo": "BnMcGn/vibase", "file": "src/vibase.py", "last_update_at": "2021-10-17T07:52:04+00:00", "original_content": "def main():\n    args = arguments().parse_args()\n    sys.path.insert(0, os.getcwd())\n    conn = get_connection(args)\n    headers = None\n    editfile = tempfile.NamedTemporaryFile()\n    with open(editfile.name, 'w', newline='') as cfile:\n        headers = write_csv(cfile, conn, args.table)\n    reffile = tempfile.NamedTemporaryFile()\n    shutil.copy(editfile.name, reffile.name)\n    call_vim(editfile.name)\n    while True:\n        try:\n            process_changes(reffile, editfile, conn, args.table, headers)\n            break\n        except Exception as err:\n            print('Failed to save changes: ', err)\n            q = '(C)ancel, (r)etry save, (e)dit the file again?'\n            cmd = query_options(q, ('c', 'r', 'e'))\n            if cmd == 'c':\n                print('Changes cancelled')\n                break\n            elif cmd == 'r':\n                continue\n            elif cmd == 'e':\n                call_vim(editfile.name)\n            else:\n                raise ValueError('Not a command')", "refactored": true, "pred": {"ppl": 4.8315043449401855, "ppl_lower": 5.976502418518066, "ppl/lowercase_ppl": -1.1350198873777009, "ppl/zlib": 0.0034542935915593755, "Min_5.0% Prob": 11.320458192091722, "Min_10.0% Prob": 9.370043919636654, "Min_20.0% Prob": 6.52076754480038, "Min_30.0% Prob": 4.94876217097044, "Min_40.0% Prob": 3.9074221683560677, "Min_50.0% Prob": 3.152027825159686, "Min_60.0% Prob": 2.627300355897751}}
{"hexsha": "93702be6e0d4de5b5113d03f1b9b9f088babbad8", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef macro_do_exist_op(macro: str, specs: typ.Dict[str, str]) -> str:\n    macro_no_encloser = macro[1:-1]\n    l, r = macro_no_encloser.split(MACRO_OP_EXIST)\n    if l in specs:\n        return specs[with_encloser(l)]\n    else:\n        return specs[with_encloser(r)]", "fn_id": 2, "class_fn": false, "repo": "Tootooroo/VerManager", "file": "manager/basic/macros.py", "last_update_at": "2021-03-18T12:03:54+00:00", "original_content": "def macro_do_exist_op(macro: str, specs: typ.Dict[str, str]) -> str:\n    macro_no_encloser = macro[1:-1]\n    l, r = macro_no_encloser.split(MACRO_OP_EXIST)\n    if l in specs:\n        return specs[with_encloser(l)]\n    else:\n        return specs[with_encloser(r)]", "refactored": true, "pred": {"ppl": 9.13167667388916, "ppl_lower": 10.87700080871582, "ppl/lowercase_ppl": -1.0790782293924535, "ppl/zlib": 0.012020376751102946, "Min_5.0% Prob": 13.827655601501466, "Min_10.0% Prob": 11.578487968444824, "Min_20.0% Prob": 8.541921070643834, "Min_30.0% Prob": 6.738284064877417, "Min_40.0% Prob": 5.3371691178707845, "Min_50.0% Prob": 4.413975041646224, "Min_60.0% Prob": 3.6746494838642696}}
{"hexsha": "0778557121cb9d3021b5bcd3f81aaa9f8612eb04", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef write_glottal_tier(utterance, tao):\n    return_str = ''\n    glottal_dur = utterance.duration\n    for ii in range(len(utterance.duration)):\n        return_str += f'    <gesture value=\"{utterance.glottal[ii]}\" slope=\"0.000000\" duration_s=\"{utterance.duration[ii]:.6f}\" time_constant_s=\"{tao:.6f}\" neutral=\"0\" />\\n'\n    return return_str", "fn_id": 4, "class_fn": false, "repo": "quantling/create_vtl_corpus", "file": "generate_gestural_score.py", "last_update_at": "2021-11-01T02:35:12+00:00", "original_content": "def write_glottal_tier(utterance, tao):\n    return_str = ''\n    glottal_dur = utterance.duration\n    for ii in range(len(utterance.duration)):\n        return_str += f'    <gesture value=\"{utterance.glottal[ii]}\" slope=\"0.000000\" duration_s=\"{utterance.duration[ii]:.6f}\" time_constant_s=\"{tao:.6f}\" neutral=\"0\" />\\n'\n    return return_str", "refactored": true, "pred": {"ppl": 10.024698257446289, "ppl_lower": 10.024698257446289, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.010771270437999053, "Min_5.0% Prob": 12.607287565867106, "Min_10.0% Prob": 10.928807814915976, "Min_20.0% Prob": 8.432599143981934, "Min_30.0% Prob": 6.788013548464389, "Min_40.0% Prob": 5.485315999984741, "Min_50.0% Prob": 4.528410175490001, "Min_60.0% Prob": 3.85331935107708}}
{"hexsha": "9ec5c4528bc5fd84b15a1a39d42df73c0372c253", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef make_parser():\n    parser = argparse.ArgumentParser('Extract frames from a video. If `-r` and `-n N` parameters are specified, then dump `N` randomly selected frames. If `-s START -i STRIDE` are specified then dump every `STRIDE`-th frame starting from `START` frame.')\n    parser.add_argument('-f', dest='fname', type=str, help='input filename')\n    parser.add_argument('-s', dest='start', default=0, type=int, help='starting frame')\n    parser.add_argument('-i', dest='stride', default=1, type=int, help='stride, interval between successive frames to save.')\n    parser.add_argument('-c', dest='cmap', default='', type=str, help='colormap to conevrt to, default same as original')\n    parser.add_argument('-x', dest='scale', default=1, type=float, help='factor by which to scale the images')\n    parser.add_argument('-r', dest='random', action='store_true', help='extract random frames')\n    parser.add_argument('-n', dest='num', default=-1, type=int, help='number of frames to extract.')\n    parser.add_argument('-o', dest='outdir', default='.', type=str, help='output directory')\n    return parser", "fn_id": 0, "class_fn": false, "repo": "subhacom/argos", "file": "argos/extract_frames.py", "last_update_at": "2021-05-18T01:07:16+00:00", "original_content": "def make_parser():\n    parser = argparse.ArgumentParser('Extract frames from a video. If `-r` and `-n N` parameters are specified, then dump `N` randomly selected frames. If `-s START -i STRIDE` are specified then dump every `STRIDE`-th frame starting from `START` frame.')\n    parser.add_argument('-f', dest='fname', type=str, help='input filename')\n    parser.add_argument('-s', dest='start', default=0, type=int, help='starting frame')\n    parser.add_argument('-i', dest='stride', default=1, type=int, help='stride, interval between successive frames to save.')\n    parser.add_argument('-c', dest='cmap', default='', type=str, help='colormap to conevrt to, default same as original')\n    parser.add_argument('-x', dest='scale', default=1, type=float, help='factor by which to scale the images')\n    parser.add_argument('-r', dest='random', action='store_true', help='extract random frames')\n    parser.add_argument('-n', dest='num', default=-1, type=int, help='number of frames to extract.')\n    parser.add_argument('-o', dest='outdir', default='.', type=str, help='output directory')\n    return parser", "refactored": true, "pred": {"ppl": 3.8723807334899902, "ppl_lower": 3.972990036010742, "ppl/lowercase_ppl": -1.018945307872714, "ppl/zlib": 0.0028990781467034816, "Min_5.0% Prob": 10.022900390625, "Min_10.0% Prob": 8.129606405893961, "Min_20.0% Prob": 5.968732901414236, "Min_30.0% Prob": 4.38216143650013, "Min_40.0% Prob": 3.3803395038424444, "Min_50.0% Prob": 2.704039701591491, "Min_60.0% Prob": 2.260780379928362}}
{"hexsha": "328dbe2b5b1e986ae40f5e053574fec17b83fbb1", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef trio_perf_counter():\n    \"\"\"Trio task-local equivalent of time.perf_counter().\n\n    For the current Trio task, return the value (in fractional seconds) of a\n    performance counter, i.e. a clock with the highest available resolution to\n    measure a short duration.  It includes time elapsed during time.sleep,\n    but not trio.sleep.  The reference point of the returned value is\n    undefined, so that only the difference between the results of consecutive\n    calls is valid.\n\n    Performance note: calling this function installs instrumentation on the\n    Trio scheduler which may affect application performance.  The\n    instrumentation is automatically removed when the corresponding tasks\n    have exited.\n    \"\"\"\n    trio_lowlevel.add_instrument(_instrument)\n    task = trio_lowlevel.current_task()\n    return perf_counter() - _instrument.get_elapsed_descheduled_time(task)", "fn_id": 0, "class_fn": false, "repo": "belm0/perf-timer", "file": "src/perf_timer/_trio.py", "last_update_at": "2021-05-29T02:50:31+00:00", "original_content": "def trio_perf_counter():\n    \"\"\"Trio task-local equivalent of time.perf_counter().\n\n    For the current Trio task, return the value (in fractional seconds) of a\n    performance counter, i.e. a clock with the highest available resolution to\n    measure a short duration.  It includes time elapsed during time.sleep,\n    but not trio.sleep.  The reference point of the returned value is\n    undefined, so that only the difference between the results of consecutive\n    calls is valid.\n\n    Performance note: calling this function installs instrumentation on the\n    Trio scheduler which may affect application performance.  The\n    instrumentation is automatically removed when the corresponding tasks\n    have exited.\n    \"\"\"\n    trio_lowlevel.add_instrument(_instrument)\n    task = trio_lowlevel.current_task()\n    return perf_counter() - _instrument.get_elapsed_descheduled_time(task)", "refactored": true, "pred": {"ppl": 12.810823440551758, "ppl_lower": 13.196624755859375, "ppl/lowercase_ppl": -1.011634244368668, "ppl/zlib": 0.005335335554218034, "Min_5.0% Prob": 11.55967025756836, "Min_10.0% Prob": 10.08360743522644, "Min_20.0% Prob": 8.217538066026641, "Min_30.0% Prob": 6.809454241106587, "Min_40.0% Prob": 5.746426387005542, "Min_50.0% Prob": 4.903829561976286, "Min_60.0% Prob": 4.227523425413716}}
{"hexsha": "1c1c76f7c4bea462b7291fe3d05a036ce7b22f07", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef main():\n    if not os.path.exists('benchmark.py'):\n        logging.warning('Please change current path to %s', PYARMOR_PATH)\n        return\n    output = '.benchtest'\n    name = 'bfoo'\n    filename = os.path.join(output, name + '.py')\n    obname = 'obfoo'\n    obfilename = os.path.join(output, obname + '.py')\n    if len(sys.argv) > 1 and 'bootstrap'.startswith(sys.argv[1]):\n        if len(sys.argv) < 5:\n            sys.argv.extend(['1', '1', '1'])\n        obf_mod, obf_code, wrap_mode = sys.argv[2:5]\n        if not os.path.exists(output):\n            logging.info('Create output path: %s', output)\n            os.makedirs(output)\n        else:\n            logging.info('Output path: %s', output)\n        logging.info('Generate test script %s ...', filename)\n        make_test_script(filename)\n        logging.info('Obffuscate test script ...')\n        shutil.copy(filename, obfilename)\n        obffuscate_scripts(output, os.path.basename(obfilename), obf_mod, obf_code, wrap_mode)\n        if not os.path.exists(obfilename):\n            logging.info('Something is wrong to obsfucate the script')\n            return\n        logging.info('Generate obffuscated script %s', obfilename)\n        logging.info('Copy benchmark.py to %s', output)\n        shutil.copy('benchmark.py', output)\n        logging.info('')\n        logging.info('Now change to \"%s\"', output)\n        logging.info('Run \"%s benchmark.py\".', sys.executable)\n        return\n    filename = os.path.basename(filename)\n    if os.path.exists(filename):\n        logging.info('Test script: %s', filename)\n    else:\n        logging.warning('Test script: %s not found', filename)\n        logging.info('Run \"%s benchmark.py bootstrap\" first.', sys.executable)\n        return\n    obfilename = os.path.basename(obfilename)\n    if os.path.exists(obfilename):\n        logging.info('Obfuscated script: %s', obfilename)\n    else:\n        logging.warning('Obfuscated script: %s not found', obfilename)\n        logging.info('Run \"%s benchmark.py bootstrap\" first.', sys.executable)\n        return\n    logging.info('--------------------------------------')\n    logging.info('')\n    total_extra_init_time()\n    logging.info('')\n    foo = import_no_obfuscated_module(name)\n    obfoo = import_obfuscated_module(obname)\n    logging.info('')\n    foo = re_import_no_obfuscated_module(name)\n    obfoo = re_import_obfuscated_module(obname)\n    logging.info('')\n    run_empty_no_obfuscated_code_object(foo)\n    run_empty_obfuscated_code_object(obfoo)\n    logging.info('')\n    run_one_thousand_no_obfuscated_bytecode(foo)\n    run_one_thousand_obfuscated_bytecode(obfoo)\n    logging.info('')\n    run_ten_thousand_no_obfuscated_bytecode(foo)\n    run_ten_thousand_obfuscated_bytecode(obfoo)\n    logging.info('')\n    logging.info('--------------------------------------')", "fn_id": 6, "class_fn": false, "repo": "HildeTeamTNT/pyarmor", "file": "src/benchmark.py", "last_update_at": "2021-04-23T16:33:54+00:00", "original_content": "def main():\n    if not os.path.exists('benchmark.py'):\n        logging.warning('Please change current path to %s', PYARMOR_PATH)\n        return\n    output = '.benchtest'\n    name = 'bfoo'\n    filename = os.path.join(output, name + '.py')\n    obname = 'obfoo'\n    obfilename = os.path.join(output, obname + '.py')\n    if len(sys.argv) > 1 and 'bootstrap'.startswith(sys.argv[1]):\n        if len(sys.argv) < 5:\n            sys.argv.extend(['1', '1', '1'])\n        obf_mod, obf_code, wrap_mode = sys.argv[2:5]\n        if not os.path.exists(output):\n            logging.info('Create output path: %s', output)\n            os.makedirs(output)\n        else:\n            logging.info('Output path: %s', output)\n        logging.info('Generate test script %s ...', filename)\n        make_test_script(filename)\n        logging.info('Obffuscate test script ...')\n        shutil.copy(filename, obfilename)\n        obffuscate_scripts(output, os.path.basename(obfilename), obf_mod, obf_code, wrap_mode)\n        if not os.path.exists(obfilename):\n            logging.info('Something is wrong to obsfucate the script')\n            return\n        logging.info('Generate obffuscated script %s', obfilename)\n        logging.info('Copy benchmark.py to %s', output)\n        shutil.copy('benchmark.py', output)\n        logging.info('')\n        logging.info('Now change to \"%s\"', output)\n        logging.info('Run \"%s benchmark.py\".', sys.executable)\n        return\n    filename = os.path.basename(filename)\n    if os.path.exists(filename):\n        logging.info('Test script: %s', filename)\n    else:\n        logging.warning('Test script: %s not found', filename)\n        logging.info('Run \"%s benchmark.py bootstrap\" first.', sys.executable)\n        return\n    obfilename = os.path.basename(obfilename)\n    if os.path.exists(obfilename):\n        logging.info('Obfuscated script: %s', obfilename)\n    else:\n        logging.warning('Obfuscated script: %s not found', obfilename)\n        logging.info('Run \"%s benchmark.py bootstrap\" first.', sys.executable)\n        return\n    logging.info('--------------------------------------')\n    logging.info('')\n    total_extra_init_time()\n    logging.info('')\n    foo = import_no_obfuscated_module(name)\n    obfoo = import_obfuscated_module(obname)\n    logging.info('')\n    foo = re_import_no_obfuscated_module(name)\n    obfoo = re_import_obfuscated_module(obname)\n    logging.info('')\n    run_empty_no_obfuscated_code_object(foo)\n    run_empty_obfuscated_code_object(obfoo)\n    logging.info('')\n    run_one_thousand_no_obfuscated_bytecode(foo)\n    run_one_thousand_obfuscated_bytecode(obfoo)\n    logging.info('')\n    run_ten_thousand_no_obfuscated_bytecode(foo)\n    run_ten_thousand_obfuscated_bytecode(obfoo)\n    logging.info('')\n    logging.info('--------------------------------------')", "refactored": true, "pred": {"ppl": 3.093369245529175, "ppl_lower": 2.9741225242614746, "ppl/lowercase_ppl": -0.9651880065332793, "ppl/zlib": 0.0014976934583669477, "Min_5.0% Prob": 9.810366508288261, "Min_10.0% Prob": 7.685530925408388, "Min_20.0% Prob": 5.163347778806261, "Min_30.0% Prob": 3.6971187094005487, "Min_40.0% Prob": 2.815734792157771, "Min_50.0% Prob": 2.2591228539001214, "Min_60.0% Prob": 1.8833073626477153}}
{"hexsha": "07d1a12410a8c57d20254595d70eb26c45d6f0ba", "ext": "py", "lang": "Python", "content": "@app.route('/robots.txt')\n@app.route('/AI_list.xml')\n@timeing\n@measure_memory_usage\ndef static_from_root():\n    return send_from_directory(app.static_folder, request.path[1:])", "fn_id": 0, "class_fn": false, "repo": "mattarderne/CommitLearn", "file": "src/app.py", "last_update_at": "2021-12-13T20:29:20+00:00", "original_content": "@app.route('/robots.txt')\n@app.route('/AI_list.xml')\ndef static_from_root():\n    return send_from_directory(app.static_folder, request.path[1:])", "refactored": true, "pred": {"ppl": 8.08746337890625, "ppl_lower": 7.907523155212402, "ppl/lowercase_ppl": -0.9892358205171871, "ppl/zlib": 0.014930822369330406, "Min_5.0% Prob": 16.068296909332275, "Min_10.0% Prob": 14.301995849609375, "Min_20.0% Prob": 9.740978457710959, "Min_30.0% Prob": 6.893701588406282, "Min_40.0% Prob": 5.288040906190872, "Min_50.0% Prob": 4.243675482427252, "Min_60.0% Prob": 3.5217371127967323}}
{"hexsha": "241dc5d98f8878f2c0b9e622971ebdf6ac69b24e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef dye(image, lw=5, rw=5, sh=3, value=255):\n    h, w = image.shape\n    mask = np.ones([h, w], np.uint8)\n    mask *= value\n    mask[2 * int(h / sh):h, int(w / lw):int(w - w / rw)] = 0\n    dst = cv.add(mask, image)\n    return dst", "fn_id": 5, "class_fn": false, "repo": "shinki-alice/-", "file": "V3.py", "last_update_at": "2021-12-08T10:42:49+00:00", "original_content": "def dye(image, lw=5, rw=5, sh=3, value=255):\n    h, w = image.shape\n    mask = np.ones([h, w], np.uint8)\n    mask *= value\n    mask[2 * int(h / sh):h, int(w / lw):int(w - w / rw)] = 0\n    dst = cv.add(mask, image)\n    return dst", "refactored": true, "pred": {"ppl": 7.031585216522217, "ppl_lower": 7.031585216522217, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.010775757864679452, "Min_5.0% Prob": 11.04528980255127, "Min_10.0% Prob": 9.409953498840332, "Min_20.0% Prob": 7.336007072812035, "Min_30.0% Prob": 5.823302768891858, "Min_40.0% Prob": 4.6946207994506475, "Min_50.0% Prob": 3.843674993739938, "Min_60.0% Prob": 3.2619599600632987}}
{"hexsha": "585c8c30c7d2aa1d2bbdf2724d7af40b9b92f226", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_straightline_path_to(target, robot_pos):\n    pts = []\n    cur_pos = robot_pos\n    while np.linalg.norm(target[:2] - cur_pos[:2]) > 0.5:\n        t = get_step_target_for_move(cur_pos, [target[0], CAMERA_HEIGHT, target[1]], step_size=0.5)\n        pts.append(t)\n        cur_pos = t\n    return np.asarray(pts)", "fn_id": 7, "class_fn": false, "repo": "Jigyasu/droidlet", "file": "droidlet/lowlevel/robot_mover_utils.py", "last_update_at": "2021-09-13T13:25:16+00:00", "original_content": "def get_straightline_path_to(target, robot_pos):\n    pts = []\n    cur_pos = robot_pos\n    while np.linalg.norm(target[:2] - cur_pos[:2]) > 0.5:\n        t = get_step_target_for_move(cur_pos, [target[0], CAMERA_HEIGHT, target[1]], step_size=0.5)\n        pts.append(t)\n        cur_pos = t\n    return np.asarray(pts)", "refactored": true, "pred": {"ppl": 5.954103469848633, "ppl_lower": 5.832284450531006, "ppl/lowercase_ppl": -0.9884131509721042, "ppl/zlib": 0.007824915090571417, "Min_5.0% Prob": 11.843291918436686, "Min_10.0% Prob": 9.670440157254538, "Min_20.0% Prob": 7.30714366833369, "Min_30.0% Prob": 5.64965714957263, "Min_40.0% Prob": 4.434232442354669, "Min_50.0% Prob": 3.5495632212729222, "Min_60.0% Prob": 2.9856606435685142}}
{"hexsha": "60f2593114913a3c96f097967b6e4f643d2df767", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef root_sum_squares(arr1, arr2):\n    \"\"\"\n    Function to calculate magnitude of two arrays of vectors.\n    \"\"\"\n    rss_arr = np.sqrt(arr1 ** 2 + arr2 ** 2)\n    v_line = np.median(rss_arr)\n    return (rss_arr, v_line)", "fn_id": 5, "class_fn": false, "repo": "geohackweek/ghw2019_planetpieces", "file": "contributors/matt/data_tools.py", "last_update_at": "2021-08-15T12:01:11+00:00", "original_content": "def root_sum_squares(arr1, arr2):\n    \"\"\"\n    Function to calculate magnitude of two arrays of vectors.\n    \"\"\"\n    rss_arr = np.sqrt(arr1 ** 2 + arr2 ** 2)\n    v_line = np.median(rss_arr)\n    return (rss_arr, v_line)", "refactored": true, "pred": {"ppl": 10.384047508239746, "ppl_lower": 10.676238059997559, "ppl/lowercase_ppl": -1.011857514774318, "ppl/zlib": 0.013766298441705403, "Min_5.0% Prob": 13.527760744094849, "Min_10.0% Prob": 11.552112698554993, "Min_20.0% Prob": 9.237564265727997, "Min_30.0% Prob": 7.2683381080627445, "Min_40.0% Prob": 5.8239036979097305, "Min_50.0% Prob": 4.647757312726407, "Min_60.0% Prob": 3.922477200627327}}
{"hexsha": "70e4742585e29853d1350672ced44f958ac66764", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _extract_metrics(out_lines) -> Tuple[pd.DataFrame, Dict[str, Optional[Union[str, int, float]]]]:\n    loss_table = {'i': [], 'loss': [], 'since_last': []}\n    metrics = {}\n    try:\n        record = False\n        for line in out_lines:\n            line = line.strip()\n            if record:\n                if line == '':\n                    record = False\n                else:\n                    counter_line = line.split()\n                    try:\n                        count, average_loss, since_last = (counter_line[2], counter_line[0], counter_line[1])\n                        average_loss_f = float(average_loss)\n                        since_last_f = float(since_last)\n                        loss_table['i'].append(count)\n                        loss_table['loss'].append(average_loss_f)\n                        loss_table['since_last'].append(since_last_f)\n                    except (ValueError, TypeError):\n                        ...\n            elif line.startswith('loss'):\n                fields = line.split()\n                if fields[0] == 'loss' and fields[1] == 'last' and (fields[2] == 'counter'):\n                    record = True\n            elif '=' in line:\n                key_value = [p.strip() for p in line.split('=')]\n                if key_value[0] == 'average loss':\n                    metrics[key_value[0]] = _parse_loss(key_value[1])\n                else:\n                    metrics[key_value[0]] = _to(key_value[1], [int, float])\n    finally:\n        return (pd.DataFrame(loss_table).set_index('i'), metrics)", "fn_id": 3, "class_fn": false, "repo": "ataymano/data-science", "file": "from_mwt_ds/DataScience/vw_executor/artifacts.py", "last_update_at": "2021-12-15T21:45:13+00:00", "original_content": "def _extract_metrics(out_lines) -> Tuple[pd.DataFrame, Dict[str, Optional[Union[str, int, float]]]]:\n    loss_table = {'i': [], 'loss': [], 'since_last': []}\n    metrics = {}\n    try:\n        record = False\n        for line in out_lines:\n            line = line.strip()\n            if record:\n                if line == '':\n                    record = False\n                else:\n                    counter_line = line.split()\n                    try:\n                        count, average_loss, since_last = (counter_line[2], counter_line[0], counter_line[1])\n                        average_loss_f = float(average_loss)\n                        since_last_f = float(since_last)\n                        loss_table['i'].append(count)\n                        loss_table['loss'].append(average_loss_f)\n                        loss_table['since_last'].append(since_last_f)\n                    except (ValueError, TypeError):\n                        ...\n            elif line.startswith('loss'):\n                fields = line.split()\n                if fields[0] == 'loss' and fields[1] == 'last' and (fields[2] == 'counter'):\n                    record = True\n            elif '=' in line:\n                key_value = [p.strip() for p in line.split('=')]\n                if key_value[0] == 'average loss':\n                    metrics[key_value[0]] = _parse_loss(key_value[1])\n                else:\n                    metrics[key_value[0]] = _to(key_value[1], [int, float])\n    finally:\n        return (pd.DataFrame(loss_table).set_index('i'), metrics)", "refactored": true, "pred": {"ppl": 3.359557628631592, "ppl_lower": 3.843317747116089, "ppl/lowercase_ppl": -1.111013079580308, "ppl/zlib": 0.0022950933847766266, "Min_5.0% Prob": 9.171896056125039, "Min_10.0% Prob": 7.5868301140634635, "Min_20.0% Prob": 5.346530142583345, "Min_30.0% Prob": 3.916202963967072, "Min_40.0% Prob": 3.0053803844279363, "Min_50.0% Prob": 2.41751035659721, "Min_60.0% Prob": 2.018659329148042}}
{"hexsha": "ee05a0d4fc4e0216722d78972336a384e87d7578", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef show_video():\n    mp4list = glob.glob('video/*.mp4')\n    if len(mp4list) > 0:\n        mp4 = mp4list[0]\n        video = io.open(mp4, 'r+b').read()\n        encoded = base64.b64encode(video)\n        ipythondisplay.display(HTML(data='<video alt=\"test\" autoplay\\n                 controls style=\"height: 400px;\">\\n                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\\n             </video>'.format(encoded.decode('ascii'))))\n    else:\n        print('Could not find video')", "fn_id": 6, "class_fn": false, "repo": "perceptualrobots/pct", "file": "pct/putils.py", "last_update_at": "2021-11-17T14:33:13+00:00", "original_content": "def show_video():\n    mp4list = glob.glob('video/*.mp4')\n    if len(mp4list) > 0:\n        mp4 = mp4list[0]\n        video = io.open(mp4, 'r+b').read()\n        encoded = base64.b64encode(video)\n        ipythondisplay.display(HTML(data='<video alt=\"test\" autoplay\\n                 controls style=\"height: 400px;\">\\n                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\\n             </video>'.format(encoded.decode('ascii'))))\n    else:\n        print('Could not find video')", "refactored": true, "pred": {"ppl": 2.6783158779144287, "ppl_lower": 2.998096227645874, "ppl/lowercase_ppl": -1.1144850334071108, "ppl/zlib": 0.0031475661125593976, "Min_5.0% Prob": 9.570905983448029, "Min_10.0% Prob": 7.401194125413895, "Min_20.0% Prob": 4.732094519066088, "Min_30.0% Prob": 3.310364986256677, "Min_40.0% Prob": 2.475248392784234, "Min_50.0% Prob": 1.9701931939350934, "Min_60.0% Prob": 1.6518958126276502}}
{"hexsha": "e2638a4bcd52d33f3d9e3b6ccc08e027bf129a22", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef build_random_ts(num_samples, length_of_ts):\n    data = {}\n    labels = []\n    for s in range(0, num_samples):\n        labels.append(np.random.choice([1, 2]))\n    data['y'] = labels\n    for col in range(0, length_of_ts):\n        key = 'feature_' + str(col + 1)\n        values = []\n        for s in range(0, num_samples):\n            values.append(np.random.normal())\n        data[key] = values\n    df = pd.DataFrame.from_dict(data)\n    X = df.iloc[:, 1:]\n    y = df.iloc[:, :1]\n    return (X, y)", "fn_id": 19, "class_fn": false, "repo": "AkashPushkar/dsitributed-time-series", "file": "src/full.py", "last_update_at": "2021-06-18T20:51:28+00:00", "original_content": "def build_random_ts(num_samples, length_of_ts):\n    data = {}\n    labels = []\n    for s in range(0, num_samples):\n        labels.append(np.random.choice([1, 2]))\n    data['y'] = labels\n    for col in range(0, length_of_ts):\n        key = 'feature_' + str(col + 1)\n        values = []\n        for s in range(0, num_samples):\n            values.append(np.random.normal())\n        data[key] = values\n    df = pd.DataFrame.from_dict(data)\n    X = df.iloc[:, 1:]\n    y = df.iloc[:, :1]\n    return (X, y)", "refactored": true, "pred": {"ppl": 3.603010892868042, "ppl_lower": 4.094422817230225, "ppl/lowercase_ppl": -1.0997495015632541, "ppl/zlib": 0.004660981291049697, "Min_5.0% Prob": 8.703149020671844, "Min_10.0% Prob": 7.236236516167136, "Min_20.0% Prob": 5.590360346962424, "Min_30.0% Prob": 4.177490315016578, "Min_40.0% Prob": 3.2234958778409397, "Min_50.0% Prob": 2.560701470302288, "Min_60.0% Prob": 2.1398975309753085}}
{"hexsha": "a2ffeaa240b71382db18bba9227270f5573e50fb", "ext": "py", "lang": "Python", "content": "@Transform_Wrapper()\n@timeing\n@measure_memory_usage\ndef Annotate_Script_Names(empty_diffs=0):\n    \"\"\"\n    For every ai script, annotate the pilot entity with the name\n    of the script running.\n    \"\"\"\n    aiscript_files = Load_Files('aiscripts/*.xml')\n    for game_file in aiscript_files:\n        xml_root = game_file.Get_Root()\n        changed = False\n        for tag in ['dock_masstraffic_drone', 'execute_custom_trade', 'execute_trade', 'move_approach_path', 'move_docking', 'move_undocking', 'move_gate', 'move_navmesh', 'move_strafe', 'move_target_points', 'move_waypoints', 'move_to', 'detach_from_masstraffic', 'wait_for_prev_script', 'wait']:\n            nodes = xml_root.xpath('.//{}'.format(tag))\n            if not nodes:\n                continue\n            changed = True\n            if empty_diffs:\n                continue\n            for node in nodes:\n                script_name = etree.Element('set_value', name='this.$script_name', exact=\"'{}'\".format(game_file.name.replace('.xml', '')))\n                node.addprevious(script_name)\n                element_name = etree.Element('set_value', name='this.$element_name', exact=\"'{}'\".format(tag))\n                node.addprevious(element_name)\n                if node.sourceline:\n                    name_line = \"'${} {}'\".format(game_file.name.replace('.xml', ''), node.sourceline)\n                    script_line_node = etree.Element('set_value', name='this.$script_line_name', exact=name_line)\n                    node.addprevious(script_line_node)\n                    record_group = [etree.fromstring('\\n                            <do_if value=\"not this.$script_line_counts?\">\\n                              <set_value name=\"this.$script_line_counts\" exact=\"table[]\"/>\\n                            </do_if>'), etree.fromstring('\\n                            <do_if value=\"not this.$script_line_counts.{FIELD}?\">\\n                              <set_value name=\"this.$script_line_counts.{FIELD}\" exact=\"0.0\"/>\\n                            </do_if>'.replace('FIELD', name_line)), etree.fromstring('\\n                            <set_value name=\"this.$script_line_counts.{FIELD}\" operation=\"add\"/>'.replace('FIELD', name_line))]\n                    for record_node in record_group:\n                        node.addprevious(record_node)\n        if changed:\n            game_file.Update_Root(xml_root)\n    return", "fn_id": 1, "class_fn": false, "repo": "abouquet/x4-projects", "file": "extensions/sn_measure_perf/Customizer_Script.py", "last_update_at": "2021-03-17T13:54:15+00:00", "original_content": "@Transform_Wrapper()\ndef Annotate_Script_Names(empty_diffs=0):\n    \"\"\"\n    For every ai script, annotate the pilot entity with the name\n    of the script running.\n    \"\"\"\n    aiscript_files = Load_Files('aiscripts/*.xml')\n    for game_file in aiscript_files:\n        xml_root = game_file.Get_Root()\n        changed = False\n        for tag in ['dock_masstraffic_drone', 'execute_custom_trade', 'execute_trade', 'move_approach_path', 'move_docking', 'move_undocking', 'move_gate', 'move_navmesh', 'move_strafe', 'move_target_points', 'move_waypoints', 'move_to', 'detach_from_masstraffic', 'wait_for_prev_script', 'wait']:\n            nodes = xml_root.xpath('.//{}'.format(tag))\n            if not nodes:\n                continue\n            changed = True\n            if empty_diffs:\n                continue\n            for node in nodes:\n                script_name = etree.Element('set_value', name='this.$script_name', exact=\"'{}'\".format(game_file.name.replace('.xml', '')))\n                node.addprevious(script_name)\n                element_name = etree.Element('set_value', name='this.$element_name', exact=\"'{}'\".format(tag))\n                node.addprevious(element_name)\n                if node.sourceline:\n                    name_line = \"'${} {}'\".format(game_file.name.replace('.xml', ''), node.sourceline)\n                    script_line_node = etree.Element('set_value', name='this.$script_line_name', exact=name_line)\n                    node.addprevious(script_line_node)\n                    record_group = [etree.fromstring('\\n                            <do_if value=\"not this.$script_line_counts?\">\\n                              <set_value name=\"this.$script_line_counts\" exact=\"table[]\"/>\\n                            </do_if>'), etree.fromstring('\\n                            <do_if value=\"not this.$script_line_counts.{FIELD}?\">\\n                              <set_value name=\"this.$script_line_counts.{FIELD}\" exact=\"0.0\"/>\\n                            </do_if>'.replace('FIELD', name_line)), etree.fromstring('\\n                            <set_value name=\"this.$script_line_counts.{FIELD}\" operation=\"add\"/>'.replace('FIELD', name_line))]\n                    for record_node in record_group:\n                        node.addprevious(record_node)\n        if changed:\n            game_file.Update_Root(xml_root)\n    return", "refactored": true, "pred": {"ppl": 5.116941452026367, "ppl_lower": 5.144630432128906, "ppl/lowercase_ppl": -1.0033056418149213, "ppl/zlib": 0.002134061291375404, "Min_5.0% Prob": 12.240635348904517, "Min_10.0% Prob": 9.964586965499386, "Min_20.0% Prob": 7.057025580636917, "Min_30.0% Prob": 5.216234965990949, "Min_40.0% Prob": 4.037905766478469, "Min_50.0% Prob": 3.2604515469122317, "Min_60.0% Prob": 2.7235713983154906}}
{"hexsha": "b2384578a7677f29cd7e0fe11a8a0c2097dd4658", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef export_result(request, uid):\n    if request.method == 'GET':\n        data = request.GET.get('data')\n        img = request.GET.get('image')\n        gif = request.GET.get('gif')\n        if data == '1':\n            data = True\n        else:\n            data = False\n        if img == '1':\n            img = True\n        else:\n            img = False\n        if gif == '1':\n            gif = True\n        else:\n            gif = False\n        work = Work(uid)\n        zip_path, file_name = work.export(data, img, gif)\n        response = StreamingHttpResponse(Tools.file_iterator(zip_path))\n        response['Content-Type'] = 'application/octet-stream'\n        response['Content-Disposition'] = 'attachment; filename={0}'.format(file_name)\n        response['Access-Control-Expose-Headers'] = 'Content-Disposition'\n        return response", "fn_id": 3, "class_fn": false, "repo": "Mr-Gump/Bamboo-Web", "file": "src/BambooReConstruct/BackEnd/views.py", "last_update_at": "2021-07-03T09:36:27+00:00", "original_content": "def export_result(request, uid):\n    if request.method == 'GET':\n        data = request.GET.get('data')\n        img = request.GET.get('image')\n        gif = request.GET.get('gif')\n        if data == '1':\n            data = True\n        else:\n            data = False\n        if img == '1':\n            img = True\n        else:\n            img = False\n        if gif == '1':\n            gif = True\n        else:\n            gif = False\n        work = Work(uid)\n        zip_path, file_name = work.export(data, img, gif)\n        response = StreamingHttpResponse(Tools.file_iterator(zip_path))\n        response['Content-Type'] = 'application/octet-stream'\n        response['Content-Disposition'] = 'attachment; filename={0}'.format(file_name)\n        response['Access-Control-Expose-Headers'] = 'Content-Disposition'\n        return response", "refactored": true, "pred": {"ppl": 2.8320541381835938, "ppl_lower": 3.6977312564849854, "ppl/lowercase_ppl": -1.2562118906064303, "ppl/zlib": 0.0028836628593878273, "Min_5.0% Prob": 9.827725982666015, "Min_10.0% Prob": 7.050756556647165, "Min_20.0% Prob": 4.797711168016706, "Min_30.0% Prob": 3.439787317363043, "Min_40.0% Prob": 2.5944106548148045, "Min_50.0% Prob": 2.089019852022658, "Min_60.0% Prob": 1.7453433218243437}}
{"hexsha": "521ca7e8f2b91498b1cecc645438b8f39926f276", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_haversine():\n    \"\"\"\n    TODO...\n    \"\"\"\n    pass", "fn_id": 1, "class_fn": false, "repo": "peterprescott/optimize-nn", "file": "tests/test_given.py", "last_update_at": "2021-10-12T04:46:20+00:00", "original_content": "def test_haversine():\n    \"\"\"\n    TODO...\n    \"\"\"\n    pass", "refactored": true, "pred": {"ppl": 25.323331832885742, "ppl_lower": 26.0963077545166, "ppl/lowercase_ppl": -1.0093039012990348, "ppl/zlib": 0.039897854044388094, "Min_5.0% Prob": 11.121386528015137, "Min_10.0% Prob": 10.725748062133789, "Min_20.0% Prob": 9.763505554199218, "Min_30.0% Prob": 9.112183094024658, "Min_40.0% Prob": 7.586359930038452, "Min_50.0% Prob": 6.242007347253653, "Min_60.0% Prob": 5.5369126041730246}}
{"hexsha": "aa06ab4636c128273f66fb5d6c82aabd460f2220", "ext": "py", "lang": "Python", "content": "@mock.patch('six.moves.builtins.super')\n@timeing\n@measure_memory_usage\ndef test_algosec_servers_http_adapter(mock_super, mocker):\n    adapter = AlgoSecServersHTTPAdapter()\n    adapter.send()\n    assert super.return_value.send.call_args == mocker.call(timeout=(AlgoSecServersHTTPAdapter.ALGOSEC_SERVER_CONNECT_TIMEOUT, AlgoSecServersHTTPAdapter.ALGOSEC_SERVER_READ_TIMEOUT))\n    assert mock_super(AlgoSecServersHTTPAdapter, adapter).calls[0]", "fn_id": 0, "class_fn": false, "repo": "chanilurya/algosec-python", "file": "tests/test_helpers.py", "last_update_at": "2021-09-25T13:15:19+00:00", "original_content": "@mock.patch('six.moves.builtins.super')\ndef test_algosec_servers_http_adapter(mock_super, mocker):\n    adapter = AlgoSecServersHTTPAdapter()\n    adapter.send()\n    assert super.return_value.send.call_args == mocker.call(timeout=(AlgoSecServersHTTPAdapter.ALGOSEC_SERVER_CONNECT_TIMEOUT, AlgoSecServersHTTPAdapter.ALGOSEC_SERVER_READ_TIMEOUT))\n    assert mock_super(AlgoSecServersHTTPAdapter, adapter).calls[0]", "refactored": true, "pred": {"ppl": 6.43383264541626, "ppl_lower": 7.8837456703186035, "ppl/lowercase_ppl": -1.109172723408929, "ppl/zlib": 0.007506332328661752, "Min_5.0% Prob": 12.059483051300049, "Min_10.0% Prob": 10.286448359489441, "Min_20.0% Prob": 7.563908424377441, "Min_30.0% Prob": 5.717267692089081, "Min_40.0% Prob": 4.524960028190239, "Min_50.0% Prob": 3.6842485952656716, "Min_60.0% Prob": 3.1256737984520826}}
{"hexsha": "1ac41f241705a181f54bcd99f611ca86824ed455", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef Hosts_Ping(context, nameSpace, className, methodName, instanceName, parameters):\n    be = BookEnd('Hosts_Ping')\n    outParams = context.NewParameters('Hosts', 'Ping')\n    outParams.SetValue('MIReturn', MI_Uint64(123))\n    context.PostInstance(outParams)\n    context.PostResult(MI_RESULT_OK)", "fn_id": 5, "class_fn": false, "repo": "Bhaskers-Blu-Org2/omi-script-provider", "file": "samples/hosts/_mi_main.py", "last_update_at": "2021-04-20T16:09:19+00:00", "original_content": "def Hosts_Ping(context, nameSpace, className, methodName, instanceName, parameters):\n    be = BookEnd('Hosts_Ping')\n    outParams = context.NewParameters('Hosts', 'Ping')\n    outParams.SetValue('MIReturn', MI_Uint64(123))\n    context.PostInstance(outParams)\n    context.PostResult(MI_RESULT_OK)", "refactored": true, "pred": {"ppl": 16.719831466674805, "ppl_lower": 23.832536697387695, "ppl/lowercase_ppl": -1.1258456188760932, "ppl/zlib": 0.013100444315634533, "Min_5.0% Prob": 13.915695667266846, "Min_10.0% Prob": 11.131445778740776, "Min_20.0% Prob": 9.150683244069418, "Min_30.0% Prob": 7.709248627935137, "Min_40.0% Prob": 6.612793671118246, "Min_50.0% Prob": 5.480624422113946, "Min_60.0% Prob": 4.675750024616718}}
{"hexsha": "73401fdbf7f3329cc097f204980f1eaf451bd20d", "ext": "py", "lang": "Python", "content": "def updateStateByKey(ssc):\n\n    @timeing\n    @measure_memory_usage\n    def processStateUpdateByKey(input_stream, state):\n        print('i', input_stream)\n        print('s', state)\n        return state if not input_stream else input_stream[-1]\n    ssc.checkpoint('checkpoints/')\n    ssc.queueStream([[('a', 1), ('b', 3)], [('a', 2), ('a', 5), ('c', 4)]]).updateStateByKey(processStateUpdateByKey).pprint()", "fn_id": 0, "class_fn": false, "repo": "svaningelgem/pyspark_fast_tester", "file": "src/main/scripts/pyspark_streaming.py", "last_update_at": "2021-08-02T13:59:46+00:00", "original_content": "def updateStateByKey(ssc):\n\n    def processStateUpdateByKey(input_stream, state):\n        print('i', input_stream)\n        print('s', state)\n        return state if not input_stream else input_stream[-1]\n    ssc.checkpoint('checkpoints/')\n    ssc.queueStream([[('a', 1), ('b', 3)], [('a', 2), ('a', 5), ('c', 4)]]).updateStateByKey(processStateUpdateByKey).pprint()", "refactored": true, "pred": {"ppl": 8.512813568115234, "ppl_lower": 8.648664474487305, "ppl/lowercase_ppl": -1.0073928881538934, "ppl/zlib": 0.009690373334809127, "Min_5.0% Prob": 10.239758968353271, "Min_10.0% Prob": 9.069594860076904, "Min_20.0% Prob": 7.625671088695526, "Min_30.0% Prob": 6.299938718477885, "Min_40.0% Prob": 5.142464237908523, "Min_50.0% Prob": 4.250056491295497, "Min_60.0% Prob": 3.5815891205436654}}
{"hexsha": "00cef5ab0b6c71719e9da7da7a1b9a6407ebc5d7", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef main():\n    N = 7\n    M = 10\n    key = gfdb.Management.retrieval.Retrieve.create_key(N, M)\n    print('key: ' + key)\n    r_configuration = 'rectangle'\n    r = gfdb.Management.retrieval.Retrieve(r_configuration)\n    library_boundaries = r.query_database()\n    r_data = r.retrieve(N, M)\n    print(r_data)\n    r_level = r.levels[r_configuration]\n    L_configuration = 'L'\n    L = gfdb.Management.retrieval.Retrieve(L_configuration)\n    library_boundaries = L.query_database()\n    L_data = L.retrieve(N, M)\n    print(L_data)\n    LopU_configuration = 'LopU'\n    LopU = gfdb.Management.retrieval.Retrieve(LopU_configuration)\n    LopU_data = LopU.retrieve(N, M)\n    print(LopU_data)\n    U_configuration = 'U'\n    U = gfdb.Management.retrieval.Retrieve(U_configuration)\n    U_data = U.retrieve(N, M)\n    print(U_data)\n    c_configuration = 'C'\n    c = gfdb.Management.retrieval.Retrieve(c_configuration)\n    library_boundaries = c.query_database()\n    c_data = c.retrieve(N, M)\n    print(c_data)\n    Open_configuration = 'Open'\n    Open = gfdb.Management.retrieval.Retrieve(Open_configuration)\n    Open_data = Open.retrieve(N, M)\n    print(Open_data)\n    zoned_configuration = 'zoned'\n    zoned = gfdb.Management.retrieval.Retrieve(zoned_configuration)\n    zoned_data = zoned.retrieve(N, M)\n    print(zoned_data)", "fn_id": 0, "class_fn": false, "repo": "j-c-cook/gFunctionDatabase", "file": "gFunctionDatabase/Management/examples/retreive.py", "last_update_at": "2021-03-13T11:23:49+00:00", "original_content": "def main():\n    N = 7\n    M = 10\n    key = gfdb.Management.retrieval.Retrieve.create_key(N, M)\n    print('key: ' + key)\n    r_configuration = 'rectangle'\n    r = gfdb.Management.retrieval.Retrieve(r_configuration)\n    library_boundaries = r.query_database()\n    r_data = r.retrieve(N, M)\n    print(r_data)\n    r_level = r.levels[r_configuration]\n    L_configuration = 'L'\n    L = gfdb.Management.retrieval.Retrieve(L_configuration)\n    library_boundaries = L.query_database()\n    L_data = L.retrieve(N, M)\n    print(L_data)\n    LopU_configuration = 'LopU'\n    LopU = gfdb.Management.retrieval.Retrieve(LopU_configuration)\n    LopU_data = LopU.retrieve(N, M)\n    print(LopU_data)\n    U_configuration = 'U'\n    U = gfdb.Management.retrieval.Retrieve(U_configuration)\n    U_data = U.retrieve(N, M)\n    print(U_data)\n    c_configuration = 'C'\n    c = gfdb.Management.retrieval.Retrieve(c_configuration)\n    library_boundaries = c.query_database()\n    c_data = c.retrieve(N, M)\n    print(c_data)\n    Open_configuration = 'Open'\n    Open = gfdb.Management.retrieval.Retrieve(Open_configuration)\n    Open_data = Open.retrieve(N, M)\n    print(Open_data)\n    zoned_configuration = 'zoned'\n    zoned = gfdb.Management.retrieval.Retrieve(zoned_configuration)\n    zoned_data = zoned.retrieve(N, M)\n    print(zoned_data)", "refactored": true, "pred": {"ppl": 2.697384834289551, "ppl_lower": 2.722482204437256, "ppl/lowercase_ppl": -1.0093333435749847, "ppl/zlib": 0.002827016306954159, "Min_5.0% Prob": 10.709322066534133, "Min_10.0% Prob": 7.778948667437532, "Min_20.0% Prob": 4.78971582377094, "Min_30.0% Prob": 3.297141295011717, "Min_40.0% Prob": 2.485309003197721, "Min_50.0% Prob": 1.9884416362691721, "Min_60.0% Prob": 1.6561365242531354}}
{"hexsha": "e357f5174eb4389ae8c75eadb4fb65ed5718efd2", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef initialize_routes(api, limiter):\n    RegisterApi.decorators = [limiter.limit('10/hour', methods=['POST'])]\n    LoginApi.decorators = [limiter.limit('30/minute', methods=['POST'])]\n    PasswordChangeApi.decorators = [limiter.limit('5/day', methods=['POST'])]\n    CaffSearchApi.decorators = [limiter.limit('1/second', methods=['GET'])]\n    CaffDownloadApi.decorators = [limiter.limit('10/minute', methods=['GET'])]\n    CaffUploadApi.decorators = [limiter.limit('10/minute', methods=['POST'])]\n    CommentApi.decorators = [limiter.limit('10/minute', methods=['POST'])]\n    UserDataApi.decorators = [limiter.limit('60/second', methods=['GET', 'DELETE'])]\n    CaffDataApi.decorators = [limiter.limit('60/second', methods=['GET', 'DELETE'])]\n    api.add_resource(RegisterApi, '/user/register')\n    api.add_resource(LoginApi, '/user/login')\n    api.add_resource(LogoutApi, '/user/logout')\n    api.add_resource(PasswordChangeApi, '/user/password')\n    api.add_resource(UsersListApi, '/user/')\n    api.add_resource(UserDataApi, '/user/<username>')\n    api.add_resource(CaffDataApi, '/caff/<caff_id>')\n    api.add_resource(CaffSearchApi, '/caff/search')\n    api.add_resource(CaffUploadApi, '/caff/upload')\n    api.add_resource(CaffDownloadApi, '/caff/download/<caff_id>')\n    api.add_resource(CommentApi, '/comment')", "fn_id": 0, "class_fn": false, "repo": "nemkrisz11/Computer-Security-Homework-2021-SHAjt", "file": "Backend/flaskapp/resources/routes.py", "last_update_at": "2021-12-03T09:58:04+00:00", "original_content": "def initialize_routes(api, limiter):\n    RegisterApi.decorators = [limiter.limit('10/hour', methods=['POST'])]\n    LoginApi.decorators = [limiter.limit('30/minute', methods=['POST'])]\n    PasswordChangeApi.decorators = [limiter.limit('5/day', methods=['POST'])]\n    CaffSearchApi.decorators = [limiter.limit('1/second', methods=['GET'])]\n    CaffDownloadApi.decorators = [limiter.limit('10/minute', methods=['GET'])]\n    CaffUploadApi.decorators = [limiter.limit('10/minute', methods=['POST'])]\n    CommentApi.decorators = [limiter.limit('10/minute', methods=['POST'])]\n    UserDataApi.decorators = [limiter.limit('60/second', methods=['GET', 'DELETE'])]\n    CaffDataApi.decorators = [limiter.limit('60/second', methods=['GET', 'DELETE'])]\n    api.add_resource(RegisterApi, '/user/register')\n    api.add_resource(LoginApi, '/user/login')\n    api.add_resource(LogoutApi, '/user/logout')\n    api.add_resource(PasswordChangeApi, '/user/password')\n    api.add_resource(UsersListApi, '/user/')\n    api.add_resource(UserDataApi, '/user/<username>')\n    api.add_resource(CaffDataApi, '/caff/<caff_id>')\n    api.add_resource(CaffSearchApi, '/caff/search')\n    api.add_resource(CaffUploadApi, '/caff/upload')\n    api.add_resource(CaffDownloadApi, '/caff/download/<caff_id>')\n    api.add_resource(CommentApi, '/comment')", "refactored": true, "pred": {"ppl": 2.2159159183502197, "ppl_lower": 2.3146018981933594, "ppl/lowercase_ppl": -1.0547615342593337, "ppl/zlib": 0.002156275949003879, "Min_5.0% Prob": 9.357504820823669, "Min_10.0% Prob": 6.728108668327332, "Min_20.0% Prob": 3.87785655480844, "Min_30.0% Prob": 2.657599804384157, "Min_40.0% Prob": 1.9928845236606805, "Min_50.0% Prob": 1.591068248703773, "Min_60.0% Prob": 1.3293125664680208}}
{"hexsha": "fe661c2264845a793c67f4141a080dbf2325c8ac", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_significance_matrix(df):\n    \"\"\"\n    returns matrix in which item[y,x] is test of whether y-x is center around zero\n    \"\"\"\n    return df.apply(lambda x: df.apply(lambda y: wilcox_test(x, y)))", "fn_id": 4, "class_fn": false, "repo": "nweir127/COD3S", "file": "src/utils/metrics.py", "last_update_at": "2021-12-10T16:33:52+00:00", "original_content": "def get_significance_matrix(df):\n    \"\"\"\n    returns matrix in which item[y,x] is test of whether y-x is center around zero\n    \"\"\"\n    return df.apply(lambda x: df.apply(lambda y: wilcox_test(x, y)))", "refactored": true, "pred": {"ppl": 23.06991958618164, "ppl_lower": 23.06991958618164, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.019373639423056165, "Min_5.0% Prob": 13.94912020365397, "Min_10.0% Prob": 11.738600594656807, "Min_20.0% Prob": 9.888387475694929, "Min_30.0% Prob": 8.277132922952825, "Min_40.0% Prob": 7.1378336117185395, "Min_50.0% Prob": 5.9915879256016495, "Min_60.0% Prob": 5.183019761334766}}
{"hexsha": "4aa6ab3efd6286da2d8af7db8b33a3b6c6534a0f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_object_beside_object():\n    ball = situation_object(BALL)\n    table = situation_object(TABLE)\n    situation = HighLevelSemanticsSituation(ontology=GAILA_PHASE_1_ONTOLOGY, salient_objects=[ball, table], always_relations=[Relation(IN_REGION, ball, Region(table, distance=PROXIMAL, direction=Direction(positive=True, relative_to_axis=HorizontalAxisOfObject(table, index=0))))])\n    assert generated_tokens(situation) == ('a', 'ball', 'beside', 'a', 'table')", "fn_id": 37, "class_fn": false, "repo": "Tubbz-alt/adam", "file": "tests/language_specific/english/test_english_language_generator.py", "last_update_at": "2021-04-26T23:59:57+00:00", "original_content": "def test_object_beside_object():\n    ball = situation_object(BALL)\n    table = situation_object(TABLE)\n    situation = HighLevelSemanticsSituation(ontology=GAILA_PHASE_1_ONTOLOGY, salient_objects=[ball, table], always_relations=[Relation(IN_REGION, ball, Region(table, distance=PROXIMAL, direction=Direction(positive=True, relative_to_axis=HorizontalAxisOfObject(table, index=0))))])\n    assert generated_tokens(situation) == ('a', 'ball', 'beside', 'a', 'table')", "refactored": true, "pred": {"ppl": 11.092504501342773, "ppl_lower": 16.44257164001465, "ppl/lowercase_ppl": -1.163574435494512, "ppl/zlib": 0.007590755867733156, "Min_5.0% Prob": 13.098639488220215, "Min_10.0% Prob": 11.138178178242274, "Min_20.0% Prob": 8.618093918109762, "Min_30.0% Prob": 7.0811441166456355, "Min_40.0% Prob": 5.783808408112361, "Min_50.0% Prob": 4.756954514000514, "Min_60.0% Prob": 4.0242694329427575}}
{"hexsha": "4fc83d3fdc1d39e0640dd521e048db83a30a09cb", "ext": "py", "lang": "Python", "content": "@task\n@timeing\n@measure_memory_usage\ndef prepare_python_packages():\n    local('mkdir -p {local_python_packages_dir}'.format(**env))\n    local('cp {local_project_root}/requirements.txt {local_python_packages_dir}/'.format(**env))\n    existing_files = set((filenameToRequirement(filename) for filename in os.listdir(env.local_python_packages_dir)))\n    missing_requirements = tempfile.NamedTemporaryFile()\n    for raw_line in open(os.path.join(env.local_project_root, 'requirements.txt')):\n        line = raw_line.strip()\n        if not line or line.startswith('#') or line not in existing_files:\n            missing_requirements.write(raw_line)\n    missing_requirements.flush()\n    local('pip install --no-use-wheel -d {env.local_python_packages_dir} --exists-action=i -r {missing_requirements_file}'.format(env=env, missing_requirements_file=missing_requirements.name))\n    missing_requirements.close()", "fn_id": 8, "class_fn": false, "repo": "jsavikko/futurice-ldap-user-manager", "file": "fabfile.py", "last_update_at": "2021-05-06T07:57:56+00:00", "original_content": "@task\ndef prepare_python_packages():\n    local('mkdir -p {local_python_packages_dir}'.format(**env))\n    local('cp {local_project_root}/requirements.txt {local_python_packages_dir}/'.format(**env))\n    existing_files = set((filenameToRequirement(filename) for filename in os.listdir(env.local_python_packages_dir)))\n    missing_requirements = tempfile.NamedTemporaryFile()\n    for raw_line in open(os.path.join(env.local_project_root, 'requirements.txt')):\n        line = raw_line.strip()\n        if not line or line.startswith('#') or line not in existing_files:\n            missing_requirements.write(raw_line)\n    missing_requirements.flush()\n    local('pip install --no-use-wheel -d {env.local_python_packages_dir} --exists-action=i -r {missing_requirements_file}'.format(env=env, missing_requirements_file=missing_requirements.name))\n    missing_requirements.close()", "refactored": true, "pred": {"ppl": 3.886455774307251, "ppl_lower": 4.357842445373535, "ppl/lowercase_ppl": -1.0843312356752797, "ppl/zlib": 0.0035351500789381554, "Min_5.0% Prob": 11.70486044883728, "Min_10.0% Prob": 8.96587630112966, "Min_20.0% Prob": 6.146595309178035, "Min_30.0% Prob": 4.439236094690349, "Min_40.0% Prob": 3.3987194334262427, "Min_50.0% Prob": 2.7124650275151505, "Min_60.0% Prob": 2.2681830803551137}}
{"hexsha": "073a2f50e72052b77841f9cbe00704769b1c08c3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef predict(pdb_file: Path) -> float:\n    \"\"\"\n    The function that puts it all together: parsing the PDB file, generating\n    features from it and performing inference with the ML model.\n    \"\"\"\n    parser = PDBParser()\n    structure = parser.get_structure(pdb_file.stem, pdb_file)\n    features = featurize(structure)\n    predicted_solubility = ml_inference(features)\n    return predicted_solubility", "fn_id": 0, "class_fn": false, "repo": "krmstrong322/cbh21-protein-solubility-challenge", "file": "predict.py", "last_update_at": "2021-05-03T20:23:32+00:00", "original_content": "def predict(pdb_file: Path) -> float:\n    \"\"\"\n    The function that puts it all together: parsing the PDB file, generating\n    features from it and performing inference with the ML model.\n    \"\"\"\n    parser = PDBParser()\n    structure = parser.get_structure(pdb_file.stem, pdb_file)\n    features = featurize(structure)\n    predicted_solubility = ml_inference(features)\n    return predicted_solubility", "refactored": true, "pred": {"ppl": 5.677285194396973, "ppl_lower": 7.527778625488281, "ppl/lowercase_ppl": -1.1624711753881254, "ppl/zlib": 0.0069737877901263, "Min_5.0% Prob": 10.374934768676757, "Min_10.0% Prob": 8.810973644256592, "Min_20.0% Prob": 6.7225216797419955, "Min_30.0% Prob": 5.280201476067305, "Min_40.0% Prob": 4.231677992399349, "Min_50.0% Prob": 3.481018770072195, "Min_60.0% Prob": 2.9075519723960985}}
{"hexsha": "6a51a788efc3cd04bcda0ebddae5641c0b5fd64a", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef AdvSrch_Type_f(AdvSrch_Type):\n    if AdvSrch_Type == 'Computer':\n        return ('advancedcomputersearches', 'advanced_computer_search', 'computers')\n    if AdvSrch_Type == 'Mobile':\n        return ('advancedmobiledevicesearches', 'advanced_mobile_device_search', 'mobile_devices')\n    else:\n        print('Failed to set AdvSrch_Type properly.\\rPlease uncomment AdvSrch_Type = \"Computer\" or AdvSrch_Type = \"Mobile\"')", "fn_id": 0, "class_fn": false, "repo": "distorted-fields/jamf-to-google-reporting", "file": "jamf-to-google.py", "last_update_at": "2021-11-29T16:15:17+00:00", "original_content": "def AdvSrch_Type_f(AdvSrch_Type):\n    if AdvSrch_Type == 'Computer':\n        return ('advancedcomputersearches', 'advanced_computer_search', 'computers')\n    if AdvSrch_Type == 'Mobile':\n        return ('advancedmobiledevicesearches', 'advanced_mobile_device_search', 'mobile_devices')\n    else:\n        print('Failed to set AdvSrch_Type properly.\\rPlease uncomment AdvSrch_Type = \"Computer\" or AdvSrch_Type = \"Mobile\"')", "refactored": true, "pred": {"ppl": 7.90571928024292, "ppl_lower": 7.81443452835083, "ppl/lowercase_ppl": -0.9943828936455452, "ppl/zlib": 0.0097527663069109, "Min_5.0% Prob": 12.437400023142496, "Min_10.0% Prob": 10.494046489397684, "Min_20.0% Prob": 7.992087001800537, "Min_30.0% Prob": 6.259758271669087, "Min_40.0% Prob": 5.04986064340554, "Min_50.0% Prob": 4.145093814935535, "Min_60.0% Prob": 3.460672583237484}}
{"hexsha": "35daad5aecd0ac3a78c0c2339cbf51e0014f3f74", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef Predict(Phi, b, mu, s, t, r, outd):\n    print(t, '--t')\n    dt = t[1] - t[0]\n    tmin = min(t)\n    tmax = max(t)\n    t2 = np.linspace(tmin, tmax, num_pred)\n    Psi = np.zeros([r, len(t2)], dtype='complex')\n    for i, _x in enumerate(t2):\n        print(_x, '_x')\n        print(b, 'b')\n        print(i)\n        print(shape(Psi))\n        Psi[:, i] = multiply(power(mu, _x / dt), b)\n    D2 = dot(Phi, Psi)\n    sigmaps = []\n    tps = []\n    for i in range(len(D2[0, :])):\n        print(str(i) + '--predicted...' + str(t2[i]))\n        F = D2[:, i]\n        if i == 0:\n            F0 = average(F)\n        eps = t2[i]\n        sigma = MakeImagePred(F - F0, i, s, eps, outd)\n        tps.append(t2[i])\n        sigmaps.append(sigma + eps)\n    return (tps, sigmaps)", "fn_id": 10, "class_fn": false, "repo": "PapStatMechMat/SeaPy", "file": "RunSEAmodes.py", "last_update_at": "2021-05-26T05:16:05+00:00", "original_content": "def Predict(Phi, b, mu, s, t, r, outd):\n    print(t, '--t')\n    dt = t[1] - t[0]\n    tmin = min(t)\n    tmax = max(t)\n    t2 = np.linspace(tmin, tmax, num_pred)\n    Psi = np.zeros([r, len(t2)], dtype='complex')\n    for i, _x in enumerate(t2):\n        print(_x, '_x')\n        print(b, 'b')\n        print(i)\n        print(shape(Psi))\n        Psi[:, i] = multiply(power(mu, _x / dt), b)\n    D2 = dot(Phi, Psi)\n    sigmaps = []\n    tps = []\n    for i in range(len(D2[0, :])):\n        print(str(i) + '--predicted...' + str(t2[i]))\n        F = D2[:, i]\n        if i == 0:\n            F0 = average(F)\n        eps = t2[i]\n        sigma = MakeImagePred(F - F0, i, s, eps, outd)\n        tps.append(t2[i])\n        sigmaps.append(sigma + eps)\n    return (tps, sigmaps)", "refactored": true, "pred": {"ppl": 7.730316638946533, "ppl_lower": 7.9484357833862305, "ppl/lowercase_ppl": -1.0136055207615466, "ppl/zlib": 0.004976033635299131, "Min_5.0% Prob": 11.630510534558978, "Min_10.0% Prob": 10.06412175606037, "Min_20.0% Prob": 7.781621621826948, "Min_30.0% Prob": 6.1867856085300446, "Min_40.0% Prob": 4.952599756798501, "Min_50.0% Prob": 4.065279066157179, "Min_60.0% Prob": 3.398351561257058}}
{"hexsha": "70f2b74641e7867f742d38f95ce15e96044fc424", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _predict(X_, theta_, model_):\n    n = X_.shape[0]\n    t_init = time()\n    Z_hat_ = _predict_proba(X_, model_)\n    W_hat_ = _classify(Z_hat_, prob=theta_[0], invert_label=model_[-1])\n    tm = time() - t_init\n    return (W_hat_, tm)", "fn_id": 2, "class_fn": false, "repo": "gterren/cloud_segmentation", "file": "GDA_segm.py", "last_update_at": "2021-11-18T07:50:33+00:00", "original_content": "def _predict(X_, theta_, model_):\n    n = X_.shape[0]\n    t_init = time()\n    Z_hat_ = _predict_proba(X_, model_)\n    W_hat_ = _classify(Z_hat_, prob=theta_[0], invert_label=model_[-1])\n    tm = time() - t_init\n    return (W_hat_, tm)", "refactored": true, "pred": {"ppl": 11.933221817016602, "ppl_lower": 10.36660099029541, "ppl/lowercase_ppl": -0.9432357625210344, "ppl/zlib": 0.014167578627155075, "Min_5.0% Prob": 10.60672640800476, "Min_10.0% Prob": 9.636540200975206, "Min_20.0% Prob": 8.114394865537944, "Min_30.0% Prob": 6.967765972532075, "Min_40.0% Prob": 5.842593312263489, "Min_50.0% Prob": 4.876850496749489, "Min_60.0% Prob": 4.165786643223516}}
{"hexsha": "0e47d06b09a13065c0203d222fb5c4959d610194", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef checkIfRange(request, response):\n    \"\"\"Checks for the If-Range header, and if it exists, checks if the\n    test passes. Returns true if the server should return partial data.\"\"\"\n    ifrange = request.headers.getHeader('if-range')\n    if ifrange is None:\n        return True\n    if isinstance(ifrange, http_headers.ETag):\n        return ifrange.match(response.headers.getHeader('etag'), strongCompare=True)\n    else:\n        return ifrange == response.headers.getHeader('last-modified')", "fn_id": 4, "class_fn": false, "repo": "twonds/twisted", "file": "twisted/web2/http.py", "last_update_at": "2021-01-27T19:11:21+00:00", "original_content": "def checkIfRange(request, response):\n    \"\"\"Checks for the If-Range header, and if it exists, checks if the\n    test passes. Returns true if the server should return partial data.\"\"\"\n    ifrange = request.headers.getHeader('if-range')\n    if ifrange is None:\n        return True\n    if isinstance(ifrange, http_headers.ETag):\n        return ifrange.match(response.headers.getHeader('etag'), strongCompare=True)\n    else:\n        return ifrange == response.headers.getHeader('last-modified')", "refactored": true, "pred": {"ppl": 7.361313819885254, "ppl_lower": 8.723114013671875, "ppl/lowercase_ppl": -1.0850288516458955, "ppl/zlib": 0.007078859662931513, "Min_5.0% Prob": 12.10970369974772, "Min_10.0% Prob": 10.296078562736511, "Min_20.0% Prob": 7.52304672241211, "Min_30.0% Prob": 5.872606823318883, "Min_40.0% Prob": 4.797304348945618, "Min_50.0% Prob": 3.94323754499829, "Min_60.0% Prob": 3.310614764886467}}
{"hexsha": "68c4a5c1dfedd1f00d406e741e54edfced34cff1", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef user_info_strlist(user: telegram.User) -> list[str]:\n    keys = ['User ID', 'First Name', 'Last Name', 'User Name', 'Language Code']\n    values = [user.id, user.first_name, user.last_name, user.username, user.language_code]\n    return combine_no_none(keys, values)", "fn_id": 5, "class_fn": false, "repo": "punch-dango/telegram-bot", "file": "dango_bot.py", "last_update_at": "2021-06-15T11:47:09+00:00", "original_content": "def user_info_strlist(user: telegram.User) -> list[str]:\n    keys = ['User ID', 'First Name', 'Last Name', 'User Name', 'Language Code']\n    values = [user.id, user.first_name, user.last_name, user.username, user.language_code]\n    return combine_no_none(keys, values)", "refactored": true, "pred": {"ppl": 5.587693691253662, "ppl_lower": 5.742030143737793, "ppl/lowercase_ppl": -1.0158356012205378, "ppl/zlib": 0.008914852976374057, "Min_5.0% Prob": 12.824129343032837, "Min_10.0% Prob": 10.944287088182238, "Min_20.0% Prob": 7.850301893133866, "Min_30.0% Prob": 5.669110208749771, "Min_40.0% Prob": 4.266603518081339, "Min_50.0% Prob": 3.4688167137668486, "Min_60.0% Prob": 2.8650108050358924}}
{"hexsha": "039644328490858f1aa80d268898976f6bf2523f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef deploy():\n    execute(checkout)\n    execute(dependencies)\n    execute(make_current)\n    execute(restart)\n    execute(cleanup)", "fn_id": 1, "class_fn": false, "repo": "anupam123148/shaaaaaaaaaaaaa", "file": "fabfile.py", "last_update_at": "2021-04-24T16:37:31+00:00", "original_content": "def deploy():\n    execute(checkout)\n    execute(dependencies)\n    execute(make_current)\n    execute(restart)\n    execute(cleanup)", "refactored": true, "pred": {"ppl": 15.785971641540527, "ppl_lower": 15.785971641540527, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.02705021249856486, "Min_5.0% Prob": 11.221434593200684, "Min_10.0% Prob": 10.000052213668823, "Min_20.0% Prob": 8.83938205242157, "Min_30.0% Prob": 7.7397752205530805, "Min_40.0% Prob": 6.558447167277336, "Min_50.0% Prob": 5.45633128285408, "Min_60.0% Prob": 4.579981699275474}}
{"hexsha": "d7d328944b6c85472fc3b3a7461b51d1e20de3b9", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef punctuation_count(documents: list) -> int:\n    \"\"\"Count number of punctuation characters in a list of textual documents\n\n    Keyword arguments:\n    documents -- the list of textual documents.\n    \"\"\"\n    __check_documents_param(documents)\n    char_cnt = char_count(documents, ignore_spaces=True)\n    char_wo_punctuation = letter_count(documents, ignore_spaces=True, ignore_digits=False)\n    return char_cnt - char_wo_punctuation", "fn_id": 4, "class_fn": false, "repo": "Perevalov/language_features", "file": "linguaf/descriptive_statistics.py", "last_update_at": "2021-09-21T00:47:55+00:00", "original_content": "def punctuation_count(documents: list) -> int:\n    \"\"\"Count number of punctuation characters in a list of textual documents\n\n    Keyword arguments:\n    documents -- the list of textual documents.\n    \"\"\"\n    __check_documents_param(documents)\n    char_cnt = char_count(documents, ignore_spaces=True)\n    char_wo_punctuation = letter_count(documents, ignore_spaces=True, ignore_digits=False)\n    return char_cnt - char_wo_punctuation", "refactored": true, "pred": {"ppl": 7.811148643493652, "ppl_lower": 9.257266998291016, "ppl/lowercase_ppl": -1.082633198219763, "ppl/zlib": 0.008898493621073781, "Min_5.0% Prob": 12.087713241577148, "Min_10.0% Prob": 10.463357318531383, "Min_20.0% Prob": 8.34623676797618, "Min_30.0% Prob": 6.441347755704608, "Min_40.0% Prob": 5.142583590486775, "Min_50.0% Prob": 4.128228941097341, "Min_60.0% Prob": 3.4324160640261003}}
{"hexsha": "8e9b1c035fd0fe23e9e05c70dd0f5fd3d8067103", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _read_version_file():\n    \"\"\"\n    Attempt to read the `version.py` file from the top level package.\n    \"\"\"\n    version_file = configuration.version_path()\n    logging.info(\"Attempting to open '%s' and read the current version.\", version_file)\n    loader = importlib.machinery.SourceFileLoader('version_module', version_file)\n    spec = importlib.util.spec_from_loader('version_module', loader)\n    version_module = importlib.util.module_from_spec(spec)\n    try:\n        loader.exec_module(version_module)\n        logging.info(\"Version file found. Using version '%s' found within.\", version_module.__version__)\n        return version_module.__version__\n    except FileNotFoundError:\n        logging.warning(\"Version file was not found at '%s'. Attempting to determine version another way.\", version_file)\n    except AttributeError as err:\n        logging.error(err)\n        logging.warning(\"Version file was found at '%s', however it did not contain the variable __version__. Attempting to determine version another way.\", version_file)\n    return None", "fn_id": 3, "class_fn": false, "repo": "timepieces141/dynamic-versioning", "file": "src/dynamic_versioning/utils.py", "last_update_at": "2021-10-20T21:33:32+00:00", "original_content": "def _read_version_file():\n    \"\"\"\n    Attempt to read the `version.py` file from the top level package.\n    \"\"\"\n    version_file = configuration.version_path()\n    logging.info(\"Attempting to open '%s' and read the current version.\", version_file)\n    loader = importlib.machinery.SourceFileLoader('version_module', version_file)\n    spec = importlib.util.spec_from_loader('version_module', loader)\n    version_module = importlib.util.module_from_spec(spec)\n    try:\n        loader.exec_module(version_module)\n        logging.info(\"Version file found. Using version '%s' found within.\", version_module.__version__)\n        return version_module.__version__\n    except FileNotFoundError:\n        logging.warning(\"Version file was not found at '%s'. Attempting to determine version another way.\", version_file)\n    except AttributeError as err:\n        logging.error(err)\n        logging.warning(\"Version file was found at '%s', however it did not contain the variable __version__. Attempting to determine version another way.\", version_file)\n    return None", "refactored": true, "pred": {"ppl": 3.255758285522461, "ppl_lower": 4.061853408813477, "ppl/lowercase_ppl": -1.187402101962455, "ppl/zlib": 0.0026827845653442446, "Min_5.0% Prob": 9.050137201944986, "Min_10.0% Prob": 6.786374416351318, "Min_20.0% Prob": 4.920882096290589, "Min_30.0% Prob": 3.7173012291130267, "Min_40.0% Prob": 2.9297520836981215, "Min_50.0% Prob": 2.356418863408209, "Min_60.0% Prob": 1.9720249125384726}}
{"hexsha": "ac9620fc76910d66d5eadf9c78f39e41ea6edc77", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef plot_14(id, query):\n    if 'category__in' in query:\n        del query['category__in']\n    query['category'] = 'B'\n    data = data_plot_groupby_category(query, values=['category', 'cloud'], sum_param='jobdefcount', label_cols=['cloud'], label_translation=False)\n    colors = prepare_colors_for_piechart(data)\n    title = PLOT_TITLES['title' + id]\n    unit = PLOT_UNITS[id]\n    return (data, colors, title, unit)", "fn_id": 16, "class_fn": false, "repo": "PanDAWMS/panda-bigmon-core-new", "file": "core/pbm/utils.py", "last_update_at": "2021-11-18T04:57:18+00:00", "original_content": "def plot_14(id, query):\n    if 'category__in' in query:\n        del query['category__in']\n    query['category'] = 'B'\n    data = data_plot_groupby_category(query, values=['category', 'cloud'], sum_param='jobdefcount', label_cols=['cloud'], label_translation=False)\n    colors = prepare_colors_for_piechart(data)\n    title = PLOT_TITLES['title' + id]\n    unit = PLOT_UNITS[id]\n    return (data, colors, title, unit)", "refactored": true, "pred": {"ppl": 14.345026016235352, "ppl_lower": 16.19989585876465, "ppl/lowercase_ppl": -1.0456564548042977, "ppl/zlib": 0.009864456529783988, "Min_5.0% Prob": 11.858320395151773, "Min_10.0% Prob": 10.11182322868934, "Min_20.0% Prob": 8.459995022526494, "Min_30.0% Prob": 7.203759723901749, "Min_40.0% Prob": 6.082081176616527, "Min_50.0% Prob": 5.139971838277929, "Min_60.0% Prob": 4.427739837655315}}
{"hexsha": "8502057a4b3bf916aba610da48747e7b8d5604e4", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef load_data():\n    moon_data = np.load('moon_data.npz')\n    x_s = moon_data['x_s']\n    y_s = moon_data['y_s']\n    x_t = moon_data['x_t']\n    return (torch.from_numpy(x_s).float(), torch.from_numpy(y_s).float(), torch.from_numpy(x_t).float())", "fn_id": 2, "class_fn": false, "repo": "krumo/swd_pytorch", "file": "swd_pytorch.py", "last_update_at": "2021-11-09T07:08:23+00:00", "original_content": "def load_data():\n    moon_data = np.load('moon_data.npz')\n    x_s = moon_data['x_s']\n    y_s = moon_data['y_s']\n    x_t = moon_data['x_t']\n    return (torch.from_numpy(x_s).float(), torch.from_numpy(y_s).float(), torch.from_numpy(x_t).float())", "refactored": true, "pred": {"ppl": 2.9995791912078857, "ppl_lower": 2.9995791912078857, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.007472598702258763, "Min_5.0% Prob": 11.688112258911133, "Min_10.0% Prob": 8.757487583160401, "Min_20.0% Prob": 5.333169216201419, "Min_30.0% Prob": 3.681798313278705, "Min_40.0% Prob": 2.7547311478749266, "Min_50.0% Prob": 2.196364691722448, "Min_60.0% Prob": 1.85359332031112}}
{"hexsha": "85d7907789cbbb8ce6e2019eb09711694ee29577", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_main():\n\n    def cmd(*args, **kwargs):\n        try:\n            main.callback(*args, **kwargs)\n            return 0\n        except SystemExit as e:\n            return e.code\n    inpath = 'examples/test.py'\n    outpath = 'examples/test.xml'\n    assert cmd(inpath, outpath, verbose=True, string=True) == 0\n    assert cmd(inpath, outpath, help=True) == 0\n    assert cmd(inpath, outpath, reload=True) == 0\n    assert cmd(inpath, outpath, version=True) == 0\n    assert cmd(inpath, outpath, list_peripherals=True) == 0", "fn_id": 0, "class_fn": false, "repo": "loggerhead/Easy-Karabiner", "file": "tests/test_main.py", "last_update_at": "2021-05-26T00:28:59+00:00", "original_content": "def test_main():\n\n    def cmd(*args, **kwargs):\n        try:\n            main.callback(*args, **kwargs)\n            return 0\n        except SystemExit as e:\n            return e.code\n    inpath = 'examples/test.py'\n    outpath = 'examples/test.xml'\n    assert cmd(inpath, outpath, verbose=True, string=True) == 0\n    assert cmd(inpath, outpath, help=True) == 0\n    assert cmd(inpath, outpath, reload=True) == 0\n    assert cmd(inpath, outpath, version=True) == 0\n    assert cmd(inpath, outpath, list_peripherals=True) == 0", "refactored": true, "pred": {"ppl": 4.4089179039001465, "ppl_lower": 5.032002925872803, "ppl/lowercase_ppl": -1.0890982781254976, "ppl/zlib": 0.0063402960943783615, "Min_5.0% Prob": 11.605838537216187, "Min_10.0% Prob": 9.228189915418625, "Min_20.0% Prob": 6.5903160933292275, "Min_30.0% Prob": 4.816804038286209, "Min_40.0% Prob": 3.674222369914624, "Min_50.0% Prob": 2.9511692973652055, "Min_60.0% Prob": 2.4874541829526424}}
{"hexsha": "dbdac17e96cf66994ed82d897eac1d43424c6c6f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef collect_bstock(pop, sp_idcs, farm_idx):\n    \"\"\"Migrate marked broodstock from source subpopulation\n    to farm subpopulation.\"\"\"\n    for sp_idx in sp_idcs:\n        for ind in pop.individuals([sp_idx]):\n            if ind.broodstock == 1:\n                ind.migrate_to = farm_idx\n            else:\n                ind.migrate_to = sp_idx\n    sim.migrate(pop, mode=sim.BY_IND_INFO)", "fn_id": 12, "class_fn": false, "repo": "nwfsc-cb/shellfish-genetic-risks", "file": "inst/GRs.py", "last_update_at": "2021-09-14T05:30:49+00:00", "original_content": "def collect_bstock(pop, sp_idcs, farm_idx):\n    \"\"\"Migrate marked broodstock from source subpopulation\n    to farm subpopulation.\"\"\"\n    for sp_idx in sp_idcs:\n        for ind in pop.individuals([sp_idx]):\n            if ind.broodstock == 1:\n                ind.migrate_to = farm_idx\n            else:\n                ind.migrate_to = sp_idx\n    sim.migrate(pop, mode=sim.BY_IND_INFO)", "refactored": true, "pred": {"ppl": 14.840462684631348, "ppl_lower": 16.291519165039062, "ppl/lowercase_ppl": -1.0345846864010118, "ppl/zlib": 0.011882631786180015, "Min_5.0% Prob": 12.612692642211915, "Min_10.0% Prob": 11.167330134998668, "Min_20.0% Prob": 9.308977210003397, "Min_30.0% Prob": 7.622071593148368, "Min_40.0% Prob": 6.351155184684916, "Min_50.0% Prob": 5.340323306746402, "Min_60.0% Prob": 4.499653548841745}}
{"hexsha": "b85923ab8b9e16da8df9357a1215d9b3530c1988", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef detect(image):\n    detector = dlib.get_frontal_face_detector()\n    img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    rects = detector(img_gray, 0)\n    return rects", "fn_id": 1, "class_fn": false, "repo": "ForrestPi/FaceProjects", "file": "facialLandmarker/pfld/algin_dlib.py", "last_update_at": "2021-06-22T06:21:29+00:00", "original_content": "def detect(image):\n    detector = dlib.get_frontal_face_detector()\n    img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    rects = detector(img_gray, 0)\n    return rects", "refactored": true, "pred": {"ppl": 3.5812363624572754, "ppl_lower": 5.196953773498535, "ppl/lowercase_ppl": -1.2918885202861625, "ppl/zlib": 0.008561799284790663, "Min_5.0% Prob": 9.517089207967123, "Min_10.0% Prob": 8.302187045415243, "Min_20.0% Prob": 5.8679142365088826, "Min_30.0% Prob": 4.30661056637764, "Min_40.0% Prob": 3.255297726503125, "Min_50.0% Prob": 2.5882734697558645, "Min_60.0% Prob": 2.1468357224693158}}
{"hexsha": "b36f29adbca75035b8a5747f2e0405b7dd6b8caf", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef roll20(querry=None):\n    \"\"\"\n    Searches roll20.net for a querry and opens the search in a webbrowser.\n    :param querry: String to search for.  If none is provided, asks the user.\n    :return: None\n    \"\"\"\n    if querry is None:\n        querry = simpledialog.askstring('Search Roll20', 'Search Roll20.net for:')\n    if querry is not None:\n        url = 'https://roll20.net/compendium/dnd5e/searchbook/?terms=' + querry\n        webbrowser.open(url)", "fn_id": 1, "class_fn": false, "repo": "spudhunter/DnD-Combat-Manager", "file": "CombatManager.py", "last_update_at": "2021-01-20T05:11:23+00:00", "original_content": "def roll20(querry=None):\n    \"\"\"\n    Searches roll20.net for a querry and opens the search in a webbrowser.\n    :param querry: String to search for.  If none is provided, asks the user.\n    :return: None\n    \"\"\"\n    if querry is None:\n        querry = simpledialog.askstring('Search Roll20', 'Search Roll20.net for:')\n    if querry is not None:\n        url = 'https://roll20.net/compendium/dnd5e/searchbook/?terms=' + querry\n        webbrowser.open(url)", "refactored": true, "pred": {"ppl": 5.3482794761657715, "ppl_lower": 6.449007987976074, "ppl/lowercase_ppl": -1.111613908903322, "ppl/zlib": 0.006097363330711553, "Min_5.0% Prob": 11.910436221531459, "Min_10.0% Prob": 9.704352719443184, "Min_20.0% Prob": 6.847932437370563, "Min_30.0% Prob": 5.119473850185221, "Min_40.0% Prob": 4.066435402732784, "Min_50.0% Prob": 3.3379561047296264, "Min_60.0% Prob": 2.799630244335767}}
{"hexsha": "31a0f40f5b3b73886fe6be3847e4043e0c0b0471", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef checkEntries(DataSetFile, preprocess=lambda x: x, dataType='jpg'):\n    dataset = wds.WebDataset(DataSetFile)\n    itemsTouched = 0\n    for element in dataset:\n        print(element['__key__'])\n        preprocess(element[dataType])\n        tmp = element[0]\n        if not tmp == None:\n            itemsTouched += 1\n    return itemsTouched", "fn_id": 8, "class_fn": false, "repo": "AaltoRSE/ImageNetTools", "file": "ImageNetTools/IOTesters.py", "last_update_at": "2021-11-15T11:21:55+00:00", "original_content": "def checkEntries(DataSetFile, preprocess=lambda x: x, dataType='jpg'):\n    dataset = wds.WebDataset(DataSetFile)\n    itemsTouched = 0\n    for element in dataset:\n        print(element['__key__'])\n        preprocess(element[dataType])\n        tmp = element[0]\n        if not tmp == None:\n            itemsTouched += 1\n    return itemsTouched", "refactored": true, "pred": {"ppl": 13.014966011047363, "ppl_lower": 15.163870811462402, "ppl/lowercase_ppl": -1.0595517538773525, "ppl/zlib": 0.01182534528535297, "Min_5.0% Prob": 14.958223104476929, "Min_10.0% Prob": 12.49024846818712, "Min_20.0% Prob": 9.473791398500142, "Min_30.0% Prob": 7.654403575829098, "Min_40.0% Prob": 6.082356647441261, "Min_50.0% Prob": 5.075191004479185, "Min_60.0% Prob": 4.245307696754472}}
{"hexsha": "db491fb88dce762693e7f13205150b53101924e2", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize(['augmentation_cls', 'params'], get_dual_transforms(custom_arguments={A.Crop: {'y_min': 0, 'y_max': 10, 'x_min': 0, 'x_max': 10}, A.CenterCrop: {'height': 10, 'width': 10}, A.CropNonEmptyMaskIfExists: {'height': 10, 'width': 10}, A.RandomCrop: {'height': 10, 'width': 10}, A.RandomResizedCrop: {'height': 10, 'width': 10}, A.RandomSizedCrop: {'min_max_height': (4, 8), 'height': 10, 'width': 10}, A.CropAndPad: {'px': 10}, A.Resize: {'height': 10, 'width': 10}}, except_augmentations={A.RandomCropNearBBox, A.RandomSizedBBoxSafeCrop}))\n@timeing\n@measure_memory_usage\ndef test_dual_augmentations(augmentation_cls, params, image, mask):\n    aug = augmentation_cls(p=1, **params)\n    data = aug(image=image, mask=mask)\n    assert data['image'].dtype == np.uint8\n    assert data['mask'].dtype == np.uint8", "fn_id": 2, "class_fn": false, "repo": "rayxke/albumentations", "file": "tests/test_augmentations.py", "last_update_at": "2021-08-25T08:41:00+00:00", "original_content": "@pytest.mark.parametrize(['augmentation_cls', 'params'], get_dual_transforms(custom_arguments={A.Crop: {'y_min': 0, 'y_max': 10, 'x_min': 0, 'x_max': 10}, A.CenterCrop: {'height': 10, 'width': 10}, A.CropNonEmptyMaskIfExists: {'height': 10, 'width': 10}, A.RandomCrop: {'height': 10, 'width': 10}, A.RandomResizedCrop: {'height': 10, 'width': 10}, A.RandomSizedCrop: {'min_max_height': (4, 8), 'height': 10, 'width': 10}, A.CropAndPad: {'px': 10}, A.Resize: {'height': 10, 'width': 10}}, except_augmentations={A.RandomCropNearBBox, A.RandomSizedBBoxSafeCrop}))\ndef test_dual_augmentations(augmentation_cls, params, image, mask):\n    aug = augmentation_cls(p=1, **params)\n    data = aug(image=image, mask=mask)\n    assert data['image'].dtype == np.uint8\n    assert data['mask'].dtype == np.uint8", "refactored": true, "pred": {"ppl": 2.7807953357696533, "ppl_lower": 4.051333427429199, "ppl/lowercase_ppl": -1.3679431733531755, "ppl/zlib": 0.002794363330025969, "Min_5.0% Prob": 10.399709765116373, "Min_10.0% Prob": 7.458017518443446, "Min_20.0% Prob": 4.645787671689065, "Min_30.0% Prob": 3.3354915790660407, "Min_40.0% Prob": 2.556836932056373, "Min_50.0% Prob": 2.042065772514504, "Min_60.0% Prob": 1.705731206498841}}
{"hexsha": "1b67a7d9d4167a9df6ed58c7d637b09510718d46", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef spider(headers, conferences_url):\n    for conference_url in conferences_url:\n        html_level1 = requests.get(conference_url, headers=headers).content\n        tree_level1 = etree.HTML(html_level1)\n        conferences = tree_level1.xpath(\"//a[contains(@href, 'dblp.uni-trier.de/db/conf/') and contains(@href, 'html')]/@href\")\n        conferences = list(set(conferences))\n        fp_dir = os.path.split(os.path.realpath(sys.argv[0]))[0] + os.sep + 'source' + os.sep + 'conf' + os.sep + conference_url.split('/')[-2]\n        if not os.path.exists(fp_dir):\n            os.makedirs(fp_dir)\n        for conference in conferences:\n            fp_name = fp_dir + os.sep + conference.split('/')[-1].split('.')[0] + '.csv'\n            if os.path.exists(fp_name):\n                print('History: ', fp_name.encode('utf-8'))\n                continue\n            html_level2 = requests.get(conference, headers=headers).content\n            tree_level2 = etree.HTML(html_level2)\n            if tree_level2.xpath(\"//li[@class='entry inproceedings']\"):\n                extractor(tree_level2, fp_name)\n            else:\n                volumes = tree_level2.xpath(\"//*[@id='main']/ul/li/a/@href\")\n                for volume in volumes:\n                    fp_name = fp_dir + os.sep + volume.split('/')[-1].split('.')[0] + '.csv'\n                    if os.path.exists(fp_name):\n                        print('History: ', fp_name.encode('utf-8'))\n                        continue\n                    html_level3 = requests.get(volume, headers=headers).content\n                    tree_level3 = etree.HTML(html_level3)\n                    extractor(tree_level3, fp_name)", "fn_id": 1, "class_fn": false, "repo": "Aticode/spider_dblp", "file": "conferences.py", "last_update_at": "2021-01-18T09:00:14+00:00", "original_content": "def spider(headers, conferences_url):\n    for conference_url in conferences_url:\n        html_level1 = requests.get(conference_url, headers=headers).content\n        tree_level1 = etree.HTML(html_level1)\n        conferences = tree_level1.xpath(\"//a[contains(@href, 'dblp.uni-trier.de/db/conf/') and contains(@href, 'html')]/@href\")\n        conferences = list(set(conferences))\n        fp_dir = os.path.split(os.path.realpath(sys.argv[0]))[0] + os.sep + 'source' + os.sep + 'conf' + os.sep + conference_url.split('/')[-2]\n        if not os.path.exists(fp_dir):\n            os.makedirs(fp_dir)\n        for conference in conferences:\n            fp_name = fp_dir + os.sep + conference.split('/')[-1].split('.')[0] + '.csv'\n            if os.path.exists(fp_name):\n                print('History: ', fp_name.encode('utf-8'))\n                continue\n            html_level2 = requests.get(conference, headers=headers).content\n            tree_level2 = etree.HTML(html_level2)\n            if tree_level2.xpath(\"//li[@class='entry inproceedings']\"):\n                extractor(tree_level2, fp_name)\n            else:\n                volumes = tree_level2.xpath(\"//*[@id='main']/ul/li/a/@href\")\n                for volume in volumes:\n                    fp_name = fp_dir + os.sep + volume.split('/')[-1].split('.')[0] + '.csv'\n                    if os.path.exists(fp_name):\n                        print('History: ', fp_name.encode('utf-8'))\n                        continue\n                    html_level3 = requests.get(volume, headers=headers).content\n                    tree_level3 = etree.HTML(html_level3)\n                    extractor(tree_level3, fp_name)", "refactored": true, "pred": {"ppl": 2.2829461097717285, "ppl_lower": 2.3678643703460693, "ppl/lowercase_ppl": -1.0442436676453928, "ppl/zlib": 0.001511843886929016, "Min_5.0% Prob": 9.219327574190887, "Min_10.0% Prob": 6.434599952494844, "Min_20.0% Prob": 3.9043697347032262, "Min_30.0% Prob": 2.7201468877969903, "Min_40.0% Prob": 2.0624300179448216, "Min_50.0% Prob": 1.653421335893267, "Min_60.0% Prob": 1.3784724990168506}}
{"hexsha": "86b7411d0f37a9a1b85f3b8b0f899c31c9cc6336", "ext": "py", "lang": "Python", "content": "@pytest.mark.filterwarnings('ignore:This function is not safe at the moment')\n@pytest.mark.parametrize('input_type', [tuple, list])\n@requires_tables\n@timeing\n@measure_memory_usage\ndef test_complete_irradiance_arrays(sapm_dc_snl_ac_system_same_arrays, location, input_type):\n    \"\"\"ModelChain.complete_irradiance can accept a tuple of weather\n    DataFrames.\"\"\"\n    times = pd.date_range(start='2020-01-01 0700-0700', periods=2, freq='H')\n    weather = pd.DataFrame({'dni': [2, 3], 'dhi': [4, 6], 'ghi': [9, 5]}, index=times)\n    mc = ModelChain(sapm_dc_snl_ac_system_same_arrays, location)\n    with pytest.raises(ValueError, match='Input DataFrames must have same index\\\\.'):\n        mc.complete_irradiance(input_type((weather, weather[1:])))\n    mc.complete_irradiance(input_type((weather, weather)))\n    for mc_weather in mc.results.weather:\n        assert_series_equal(mc_weather['dni'], pd.Series([2, 3], index=times, name='dni'))\n        assert_series_equal(mc_weather['dhi'], pd.Series([4, 6], index=times, name='dhi'))\n        assert_series_equal(mc_weather['ghi'], pd.Series([9, 5], index=times, name='ghi'))\n    mc = ModelChain(sapm_dc_snl_ac_system_same_arrays, location)\n    mc.complete_irradiance(input_type((weather[['ghi', 'dhi']], weather[['dhi', 'dni']])))\n    assert 'dni' in mc.results.weather[0].columns\n    assert 'ghi' in mc.results.weather[1].columns\n    mc.complete_irradiance(input_type((weather, weather[['ghi', 'dni']])))\n    assert_series_equal(mc.results.weather[0]['dhi'], pd.Series([4, 6], index=times, name='dhi'))\n    assert_series_equal(mc.results.weather[0]['ghi'], pd.Series([9, 5], index=times, name='ghi'))\n    assert_series_equal(mc.results.weather[0]['dni'], pd.Series([2, 3], index=times, name='dni'))\n    assert 'dhi' in mc.results.weather[1].columns", "fn_id": 104, "class_fn": false, "repo": "Kaustubh1598/pvlib-python", "file": "pvlib/tests/test_modelchain.py", "last_update_at": "2021-08-16T06:47:01+00:00", "original_content": "@pytest.mark.filterwarnings('ignore:This function is not safe at the moment')\n@pytest.mark.parametrize('input_type', [tuple, list])\n@requires_tables\ndef test_complete_irradiance_arrays(sapm_dc_snl_ac_system_same_arrays, location, input_type):\n    \"\"\"ModelChain.complete_irradiance can accept a tuple of weather\n    DataFrames.\"\"\"\n    times = pd.date_range(start='2020-01-01 0700-0700', periods=2, freq='H')\n    weather = pd.DataFrame({'dni': [2, 3], 'dhi': [4, 6], 'ghi': [9, 5]}, index=times)\n    mc = ModelChain(sapm_dc_snl_ac_system_same_arrays, location)\n    with pytest.raises(ValueError, match='Input DataFrames must have same index\\\\.'):\n        mc.complete_irradiance(input_type((weather, weather[1:])))\n    mc.complete_irradiance(input_type((weather, weather)))\n    for mc_weather in mc.results.weather:\n        assert_series_equal(mc_weather['dni'], pd.Series([2, 3], index=times, name='dni'))\n        assert_series_equal(mc_weather['dhi'], pd.Series([4, 6], index=times, name='dhi'))\n        assert_series_equal(mc_weather['ghi'], pd.Series([9, 5], index=times, name='ghi'))\n    mc = ModelChain(sapm_dc_snl_ac_system_same_arrays, location)\n    mc.complete_irradiance(input_type((weather[['ghi', 'dhi']], weather[['dhi', 'dni']])))\n    assert 'dni' in mc.results.weather[0].columns\n    assert 'ghi' in mc.results.weather[1].columns\n    mc.complete_irradiance(input_type((weather, weather[['ghi', 'dni']])))\n    assert_series_equal(mc.results.weather[0]['dhi'], pd.Series([4, 6], index=times, name='dhi'))\n    assert_series_equal(mc.results.weather[0]['ghi'], pd.Series([9, 5], index=times, name='ghi'))\n    assert_series_equal(mc.results.weather[0]['dni'], pd.Series([2, 3], index=times, name='dni'))\n    assert 'dhi' in mc.results.weather[1].columns", "refactored": true, "pred": {"ppl": 2.743576765060425, "ppl_lower": 3.002892017364502, "ppl/lowercase_ppl": -1.089484525627049, "ppl/zlib": 0.0016990950468502967, "Min_5.0% Prob": 9.344272788365682, "Min_10.0% Prob": 7.041307508945465, "Min_20.0% Prob": 4.645994266867637, "Min_30.0% Prob": 3.316387064092689, "Min_40.0% Prob": 2.5216683158030113, "Min_50.0% Prob": 2.0167000508740345, "Min_60.0% Prob": 1.6827493140485572}}
{"hexsha": "3ad888b6411bf731d2ffa031e8ac7e352cb37d45", "ext": "py", "lang": "Python", "content": "@bp.route('/create', methods=('GET', 'POST'))\n@login_required\n@timeing\n@measure_memory_usage\ndef create():\n    if request.method == 'POST':\n        name = request.form['name']\n        description = request.form['description']\n        error = None\n        if not name:\n            error = 'Name is required.'\n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute('INSERT INTO dens (name, description, author_id) VALUES (?, ?, ?)', (name, description, g.user['id']))\n            db.commit()\n            return redirect(url_for('dens.index'))\n    return render_template('dens/create.html')", "fn_id": 1, "class_fn": false, "repo": "tsainez/bobchat", "file": "bobchat/dens.py", "last_update_at": "2021-12-14T19:16:11+00:00", "original_content": "@bp.route('/create', methods=('GET', 'POST'))\n@login_required\ndef create():\n    if request.method == 'POST':\n        name = request.form['name']\n        description = request.form['description']\n        error = None\n        if not name:\n            error = 'Name is required.'\n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute('INSERT INTO dens (name, description, author_id) VALUES (?, ?, ?)', (name, description, g.user['id']))\n            db.commit()\n            return redirect(url_for('dens.index'))\n    return render_template('dens/create.html')", "refactored": true, "pred": {"ppl": 2.6447112560272217, "ppl_lower": 3.4235196113586426, "ppl/lowercase_ppl": -1.2653890270452595, "ppl/zlib": 0.0028859403364628546, "Min_5.0% Prob": 11.133536219596863, "Min_10.0% Prob": 7.865702867507935, "Min_20.0% Prob": 4.8154278341680765, "Min_30.0% Prob": 3.279873504458616, "Min_40.0% Prob": 2.4337181985378264, "Min_50.0% Prob": 1.9555998691645118, "Min_60.0% Prob": 1.6339366416610086}}
{"hexsha": "1e6e4ef45a8b40afebac91fb3f7bf135a4a6341f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_tags(pipeline):\n    nodes = pipeline.nodes\n    tags = {node.name: node.tags for node in nodes}\n    return tags", "fn_id": 3, "class_fn": false, "repo": "gcalmettes/kedro-argo", "file": "src/kedro_argo/cli.py", "last_update_at": "2021-01-20T04:23:56+00:00", "original_content": "def get_tags(pipeline):\n    nodes = pipeline.nodes\n    tags = {node.name: node.tags for node in nodes}\n    return tags", "refactored": true, "pred": {"ppl": 7.9720377922058105, "ppl_lower": 7.9720377922058105, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.018535179847832147, "Min_5.0% Prob": 11.52440595626831, "Min_10.0% Prob": 10.119674444198608, "Min_20.0% Prob": 8.386475563049316, "Min_30.0% Prob": 6.595071444908778, "Min_40.0% Prob": 5.205384790897369, "Min_50.0% Prob": 4.0716226029963725, "Min_60.0% Prob": 3.459820291996002}}
{"hexsha": "c0e6fb6fdeb419f4456ff38cda990830605dba2c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef calculate_bead_lims(bead_size, region_revs, region_fors):\n    \"\"\"\n    Divides a region on a chromosome (or rather, the part of it covered by complete\n    restriction fragments) into segments of equal, given length and one last\n    segment which is smaller than the others such that the segments completely\n    cover the region. These segments will be represented by spherical beads later.\n    Returns the limits of the segments\n    \"\"\"\n    region_length = np.max((region_fors[-1, 1], region_revs[1, -1])) - np.min((region_fors[0, 0], region_revs[0, 0]))\n    n_beads = int(round(region_length / bead_size)) + 1\n    bead_lims = [np.min((region_fors[0, 0], region_revs[0, 0])) + i * bead_size for i in range(n_beads)]\n    bead_lims[-1] = np.max((region_fors[-1, 1], region_revs[1, -1]))\n    return np.array(bead_lims)", "fn_id": 2, "class_fn": false, "repo": "simeoncarstens/ensemble_hic", "file": "data/nora2012/make_processed_files.py", "last_update_at": "2021-03-16T04:38:07+00:00", "original_content": "def calculate_bead_lims(bead_size, region_revs, region_fors):\n    \"\"\"\n    Divides a region on a chromosome (or rather, the part of it covered by complete\n    restriction fragments) into segments of equal, given length and one last\n    segment which is smaller than the others such that the segments completely\n    cover the region. These segments will be represented by spherical beads later.\n    Returns the limits of the segments\n    \"\"\"\n    region_length = np.max((region_fors[-1, 1], region_revs[1, -1])) - np.min((region_fors[0, 0], region_revs[0, 0]))\n    n_beads = int(round(region_length / bead_size)) + 1\n    bead_lims = [np.min((region_fors[0, 0], region_revs[0, 0])) + i * bead_size for i in range(n_beads)]\n    bead_lims[-1] = np.max((region_fors[-1, 1], region_revs[1, -1]))\n    return np.array(bead_lims)", "refactored": true, "pred": {"ppl": 4.709906101226807, "ppl_lower": 4.796543598175049, "ppl/lowercase_ppl": -1.011762263506153, "ppl/zlib": 0.0038453299547998436, "Min_5.0% Prob": 10.173418631920448, "Min_10.0% Prob": 8.79424164030287, "Min_20.0% Prob": 6.4575724601745605, "Min_30.0% Prob": 4.890043416655208, "Min_40.0% Prob": 3.825701794645808, "Min_50.0% Prob": 3.1007689046237967, "Min_60.0% Prob": 2.587782725747563}}
{"hexsha": "8086f65407ad32c804e1c9c38b95a0d946baf627", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _spec_from_colon_separated_text_list(spec_name: str):\n    \"\"\"\n    Get a specification value from a list of colon separated values in\n\n    `_general_specs(...)` returns a list of strings of with the format\n    `Spec Name: Spec Value`.\n    TODO: Implement and use this. Refactor `ttaf(...)`\n    \"\"\"", "fn_id": 14, "class_fn": false, "repo": "jwalleser/plane-finder-app", "file": "planefinder/trade_a_plane.py", "last_update_at": "2021-12-31T02:54:08+00:00", "original_content": "def _spec_from_colon_separated_text_list(spec_name: str):\n    \"\"\"\n    Get a specification value from a list of colon separated values in\n\n    `_general_specs(...)` returns a list of strings of with the format\n    `Spec Name: Spec Value`.\n    TODO: Implement and use this. Refactor `ttaf(...)`\n    \"\"\"", "refactored": true, "pred": {"ppl": 45.03754806518555, "ppl_lower": 41.43244934082031, "ppl/lowercase_ppl": -0.9780873946889399, "ppl/zlib": 0.017150885330178025, "Min_5.0% Prob": 13.280237913131714, "Min_10.0% Prob": 11.990133166313171, "Min_20.0% Prob": 10.614115771125345, "Min_30.0% Prob": 9.342575225830078, "Min_40.0% Prob": 8.019821594743167, "Min_50.0% Prob": 6.941113316735556, "Min_60.0% Prob": 6.131859304858189}}
{"hexsha": "623568b4c78cf9a439d74b2c528e6cff22e0cadb", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef detect_text(content):\n    \"\"\"Detects text in the file.\"\"\"\n    from google.cloud import vision\n    import io\n    client = vision.ImageAnnotatorClient()\n    content_base64 = base64.b64decode(content)\n    image = vision.Image(content=content_base64)\n    response = client.text_detection(image=image)\n    text = response.text_annotations[0].description\n    return text", "fn_id": 0, "class_fn": false, "repo": "OmkarMehta/anuvad", "file": "anuvad/callbacks.py", "last_update_at": "2021-08-18T00:50:53+00:00", "original_content": "def detect_text(content):\n    \"\"\"Detects text in the file.\"\"\"\n    from google.cloud import vision\n    import io\n    client = vision.ImageAnnotatorClient()\n    content_base64 = base64.b64decode(content)\n    image = vision.Image(content=content_base64)\n    response = client.text_detection(image=image)\n    text = response.text_annotations[0].description\n    return text", "refactored": true, "pred": {"ppl": 3.1144356727600098, "ppl_lower": 6.500216484069824, "ppl/lowercase_ppl": -1.647672922920572, "ppl/zlib": 0.004917956586078537, "Min_5.0% Prob": 9.266513442993164, "Min_10.0% Prob": 7.922737073898316, "Min_20.0% Prob": 5.29067694005512, "Min_30.0% Prob": 3.731749577447772, "Min_40.0% Prob": 2.8842941401969817, "Min_50.0% Prob": 2.2917177291973583, "Min_60.0% Prob": 1.8991943940709461}}
{"hexsha": "811ab0d25f1780d5e5c2deb38dcaf25cb83fa07a", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef deletecall(bam_url, api_call, call_parameters, delete_entity, header):\n    \"\"\"API request to delete and return values\"\"\"\n    call_url = 'http://' + bam_url + '/Services/REST/v1/' + api_call + '?'\n    print('You are requesting to delete:')\n    print(delete_entity)\n    answer = input('Do you want to proceed (y (yes) or n (no))? ')\n    try:\n        if answer.lower() == 'y':\n            response = requests.delete(call_url, params=call_parameters, headers=header)\n            return response.json()\n        elif answer.lower() == 'n':\n            return 'You aborted deletion'\n        else:\n            return 'You entered an invalid character'\n    except requests.exceptions.RequestException as e:\n        print(e)", "fn_id": 1, "class_fn": false, "repo": "bluecatlabs/making-apis-work-for-you", "file": "Episodes/Episode6/2-getServers-REST.py", "last_update_at": "2021-02-16T12:51:20+00:00", "original_content": "def deletecall(bam_url, api_call, call_parameters, delete_entity, header):\n    \"\"\"API request to delete and return values\"\"\"\n    call_url = 'http://' + bam_url + '/Services/REST/v1/' + api_call + '?'\n    print('You are requesting to delete:')\n    print(delete_entity)\n    answer = input('Do you want to proceed (y (yes) or n (no))? ')\n    try:\n        if answer.lower() == 'y':\n            response = requests.delete(call_url, params=call_parameters, headers=header)\n            return response.json()\n        elif answer.lower() == 'n':\n            return 'You aborted deletion'\n        else:\n            return 'You entered an invalid character'\n    except requests.exceptions.RequestException as e:\n        print(e)", "refactored": true, "pred": {"ppl": 5.988188743591309, "ppl_lower": 6.730688095092773, "ppl/lowercase_ppl": -1.065308478211647, "ppl/zlib": 0.004772770630310553, "Min_5.0% Prob": 12.002930323282877, "Min_10.0% Prob": 9.995814599488911, "Min_20.0% Prob": 7.407000416203549, "Min_30.0% Prob": 5.488140985883516, "Min_40.0% Prob": 4.355422000606339, "Min_50.0% Prob": 3.5316595324535958, "Min_60.0% Prob": 2.982936314467726}}
{"hexsha": "f6667e4bee5b954c1e12aaebccf65cda7aa78ce7", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef generate_sub_id():\n    timestamp = floor(time() * 1000)\n    random_id = randint(0, 999)\n    return f'sub-{timestamp}-{random_id:03}'", "fn_id": 0, "class_fn": false, "repo": "ChessTerm/jiuqi-runner", "file": "run.py", "last_update_at": "2021-06-13T03:48:20+00:00", "original_content": "def generate_sub_id():\n    timestamp = floor(time() * 1000)\n    random_id = randint(0, 999)\n    return f'sub-{timestamp}-{random_id:03}'", "refactored": true, "pred": {"ppl": 6.411350250244141, "ppl_lower": 6.411350250244141, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.013866193255324637, "Min_5.0% Prob": 9.769409815470377, "Min_10.0% Prob": 8.983256419499716, "Min_20.0% Prob": 7.398384054501851, "Min_30.0% Prob": 5.8387500113911095, "Min_40.0% Prob": 4.603596302370231, "Min_50.0% Prob": 3.745224113265673, "Min_60.0% Prob": 3.1372866642971835}}
{"hexsha": "793c76b7aef3c3464b9f0b5785241de95f0cc1ed", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef draw_flow(img, flow, step=16):\n    h, w = img.shape[:2]\n    y, x = np.mgrid[step / 2:h:step, step / 2:w:step].reshape(2, -1).astype(int)\n    fx, fy = flow[y, x].T\n    lines = np.vstack([x, y, x + fx, y + fy]).T.reshape(-1, 2, 2)\n    lines = np.int32(lines + 0.5)\n    vis = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n    cv2.polylines(vis, lines, 0, (0, 255, 0))\n    for (x1, y1), (_x2, _y2) in lines:\n        cv2.circle(vis, (x1, y1), 1, (0, 255, 0), -1)\n    return vis", "fn_id": 4, "class_fn": false, "repo": "mcv-m6-video/mcv-m6-2021-team6", "file": "W4/display.py", "last_update_at": "2021-03-23T18:54:28+00:00", "original_content": "def draw_flow(img, flow, step=16):\n    h, w = img.shape[:2]\n    y, x = np.mgrid[step / 2:h:step, step / 2:w:step].reshape(2, -1).astype(int)\n    fx, fy = flow[y, x].T\n    lines = np.vstack([x, y, x + fx, y + fy]).T.reshape(-1, 2, 2)\n    lines = np.int32(lines + 0.5)\n    vis = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n    cv2.polylines(vis, lines, 0, (0, 255, 0))\n    for (x1, y1), (_x2, _y2) in lines:\n        cv2.circle(vis, (x1, y1), 1, (0, 255, 0), -1)\n    return vis", "refactored": true, "pred": {"ppl": 1.4385604858398438, "ppl_lower": 1.9200330972671509, "ppl/lowercase_ppl": -1.7939091691222582, "ppl/zlib": 0.0011768380290639987, "Min_5.0% Prob": 6.384023601358587, "Min_10.0% Prob": 3.5265818069810453, "Min_20.0% Prob": 1.831569976132849, "Min_30.0% Prob": 1.2124401359153645, "Min_40.0% Prob": 0.9145274463069615, "Min_50.0% Prob": 0.7271922673103718, "Min_60.0% Prob": 0.6077844361381721}}
{"hexsha": "ff326af30a0e940467d42ebccf60e036d6b028f5", "ext": "py", "lang": "Python", "content": "@bot.command()\n@timeing\n@measure_memory_usage\nasync def listening(ctx, *, message=None):\n    await ctx.message.delete()\n    if message is None:\n        commandprint(\"Command 'listening' has been used by \" + bot.user.name + ' with no message')\n        embed = discord.Embed(title=f'**Invalid syntax**', description='You have not specified a listening \\nExample: ' + prefix + 'listening flairings piss', color=errorcolor)\n        embed.set_footer(text=footer)\n        await ctx.send(embed=embed, delete_after=deletetimer)\n    else:\n        try:\n            commandprint(\"Command 'listening' has been used by \" + bot.user.name + \" with a message of '\" + message + \"'\")\n            await bot.change_presence(activity=discord.Activity(type=discord.ActivityType.listening, name=message))\n            em = discord.Embed(title=f'**STATUS CHANGED**', description=\"Your listening status has been set to **'\" + message + \"'**\", color=color)\n            em.set_footer(text=footer)\n            await ctx.send(embed=em, delete_after=deletetimer)\n        except Exception as error:\n            errorprint(\"Exception ' {0} ', expected error message sent to users chat\".format(error))\n            em = discord.Embed(title='Exception Error:', description='Expected Exception: You already have a custom status. \\n Console Exception {0}'.format(error), color=errorcolor)\n            await ctx.send(embed=em, delete_after=deletetimer)", "fn_id": 78, "class_fn": false, "repo": "Flairings/Project-Mars", "file": "Mars.py", "last_update_at": "2021-09-18T03:13:13+00:00", "original_content": "@bot.command()\nasync def listening(ctx, *, message=None):\n    await ctx.message.delete()\n    if message is None:\n        commandprint(\"Command 'listening' has been used by \" + bot.user.name + ' with no message')\n        embed = discord.Embed(title=f'**Invalid syntax**', description='You have not specified a listening \\nExample: ' + prefix + 'listening flairings piss', color=errorcolor)\n        embed.set_footer(text=footer)\n        await ctx.send(embed=embed, delete_after=deletetimer)\n    else:\n        try:\n            commandprint(\"Command 'listening' has been used by \" + bot.user.name + \" with a message of '\" + message + \"'\")\n            await bot.change_presence(activity=discord.Activity(type=discord.ActivityType.listening, name=message))\n            em = discord.Embed(title=f'**STATUS CHANGED**', description=\"Your listening status has been set to **'\" + message + \"'**\", color=color)\n            em.set_footer(text=footer)\n            await ctx.send(embed=em, delete_after=deletetimer)\n        except Exception as error:\n            errorprint(\"Exception ' {0} ', expected error message sent to users chat\".format(error))\n            em = discord.Embed(title='Exception Error:', description='Expected Exception: You already have a custom status. \\n Console Exception {0}'.format(error), color=errorcolor)\n            await ctx.send(embed=em, delete_after=deletetimer)", "refactored": true, "pred": {"ppl": 4.938880920410156, "ppl_lower": 5.646743297576904, "ppl/lowercase_ppl": -1.0838625939528137, "ppl/zlib": 0.0028168232296421976, "Min_5.0% Prob": 11.729220761193169, "Min_10.0% Prob": 9.204915748702156, "Min_20.0% Prob": 6.531623082618191, "Min_30.0% Prob": 5.062098768872953, "Min_40.0% Prob": 3.9483973430852366, "Min_50.0% Prob": 3.1937882596074223, "Min_60.0% Prob": 2.6600264766291803}}
{"hexsha": "d7d3d58fe31aa429507c3843cdedb9be62480884", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef outgoing_message(message):\n    message_label = tk.Label(messages_frame, bg='gray20', fg='snow', text=message, font=('Helvetica', 15))\n    message_label.pack(side=tk.TOP, anchor=tk.E, padx=20, pady=10)\n    message_labels.append(message_label)", "fn_id": 3, "class_fn": false, "repo": "Jothin-kumar/chat-app", "file": "scripts/client/gui.py", "last_update_at": "2021-12-10T13:33:11+00:00", "original_content": "def outgoing_message(message):\n    message_label = tk.Label(messages_frame, bg='gray20', fg='snow', text=message, font=('Helvetica', 15))\n    message_label.pack(side=tk.TOP, anchor=tk.E, padx=20, pady=10)\n    message_labels.append(message_label)", "refactored": true, "pred": {"ppl": 6.158907413482666, "ppl_lower": 7.502330780029297, "ppl/lowercase_ppl": -1.1085397518289166, "ppl/zlib": 0.009773652655229835, "Min_5.0% Prob": 11.776035070419312, "Min_10.0% Prob": 9.834389474656847, "Min_20.0% Prob": 7.42087988058726, "Min_30.0% Prob": 5.6806415276868005, "Min_40.0% Prob": 4.542100192727269, "Min_50.0% Prob": 3.613649969405316, "Min_60.0% Prob": 3.046581324722086}}
{"hexsha": "82966286972b9d3c71d206d1668d569e596aeedd", "ext": "py", "lang": "Python", "content": "@require_context\n@timeing\n@measure_memory_usage\ndef quota_class_get_all_by_name(context, class_name):\n    authorize_quota_class_context(context, class_name)\n    rows = model_query(context, models.QuotaClass, read_deleted='no').filter_by(class_name=class_name).all()\n    result = {'class_name': class_name}\n    for row in rows:\n        result[row.resource] = row.hard_limit\n    return result", "fn_id": 105, "class_fn": false, "repo": "scottwedge/OpenStack-Stein", "file": "karbor-1.3.0/karbor/db/sqlalchemy/api.py", "last_update_at": "2021-12-13T20:01:25+00:00", "original_content": "@require_context\ndef quota_class_get_all_by_name(context, class_name):\n    authorize_quota_class_context(context, class_name)\n    rows = model_query(context, models.QuotaClass, read_deleted='no').filter_by(class_name=class_name).all()\n    result = {'class_name': class_name}\n    for row in rows:\n        result[row.resource] = row.hard_limit\n    return result", "refactored": true, "pred": {"ppl": 4.5017571449279785, "ppl_lower": 4.853199005126953, "ppl/lowercase_ppl": -1.0499646979024375, "ppl/zlib": 0.006807546593713884, "Min_5.0% Prob": 11.36415367126465, "Min_10.0% Prob": 8.959079482338645, "Min_20.0% Prob": 6.5135375313136885, "Min_30.0% Prob": 4.835305413178035, "Min_40.0% Prob": 3.7733476780678914, "Min_50.0% Prob": 3.0200153988753926, "Min_60.0% Prob": 2.51065360664257}}
{"hexsha": "8c0c41d89d485b342b3470a122b1a94215a4cc9f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef main():\n    import glob\n    parent = sys.argv[1]\n    folders = glob.glob('{}/*_output'.format(parent))\n    submit_qsub(parent, folders)", "fn_id": 1, "class_fn": false, "repo": "ckrivacic/helix_matcher", "file": "helix/rifdock/submit_align.py", "last_update_at": "2021-06-07T10:56:37+00:00", "original_content": "def main():\n    import glob\n    parent = sys.argv[1]\n    folders = glob.glob('{}/*_output'.format(parent))\n    submit_qsub(parent, folders)", "refactored": true, "pred": {"ppl": 12.284599304199219, "ppl_lower": 12.284599304199219, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.018309097728251147, "Min_5.0% Prob": 12.670350551605225, "Min_10.0% Prob": 10.440675354003906, "Min_20.0% Prob": 8.32689642906189, "Min_30.0% Prob": 7.041479682922363, "Min_40.0% Prob": 6.070170336961747, "Min_50.0% Prob": 4.959701966643333, "Min_60.0% Prob": 4.161408749222756}}
{"hexsha": "f32dd5e74bf9879ee46da2157293834813f6746b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _get_package_bin_dir_app_paths(venv: Venv, package_info: PackageInfo, local_bin_dir: Path) -> Set[Path]:\n    suffix = package_info.suffix\n    apps = []\n    if package_info.include_apps:\n        apps += package_info.apps\n    if package_info.include_dependencies:\n        apps += package_info.apps_of_dependencies\n    return get_exposed_app_paths_for_package(venv.bin_path, local_bin_dir, [add_suffix(app, suffix) for app in apps])", "fn_id": 1, "class_fn": false, "repo": "KenMacD/pipx", "file": "src/pipx/commands/uninstall.py", "last_update_at": "2021-05-27T02:34:20+00:00", "original_content": "def _get_package_bin_dir_app_paths(venv: Venv, package_info: PackageInfo, local_bin_dir: Path) -> Set[Path]:\n    suffix = package_info.suffix\n    apps = []\n    if package_info.include_apps:\n        apps += package_info.apps\n    if package_info.include_dependencies:\n        apps += package_info.apps_of_dependencies\n    return get_exposed_app_paths_for_package(venv.bin_path, local_bin_dir, [add_suffix(app, suffix) for app in apps])", "refactored": true, "pred": {"ppl": 7.133471488952637, "ppl_lower": 8.234058380126953, "ppl/lowercase_ppl": -1.0730258340107726, "ppl/zlib": 0.008360842556520549, "Min_5.0% Prob": 10.274355070931572, "Min_10.0% Prob": 9.266635486057826, "Min_20.0% Prob": 7.255683515753065, "Min_30.0% Prob": 5.8480224609375, "Min_40.0% Prob": 4.7855727310691565, "Min_50.0% Prob": 3.8893140239614836, "Min_60.0% Prob": 3.272199099642389}}
{"hexsha": "c039023785903aca6301e71b8b73beaa059d6ec4", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef duplicateGroup(id):\n    group = CreatureGroup.objects.get(id=id)\n    newGroup = CreatureGroup(name='%s_duplicate' % group.name, Augmented=group.Augmented)\n    newGroup.Grouping = group.Grouping\n    newGroup.save()\n    for item in group.AllowedExtraType.all():\n        newGroup.AllowedExtraType.add(item)\n    for item in group.DefaultExtraType.all():\n        newGroup.DefaultExtraType.add(item)\n    for item in group.groupentry_set.all():\n        GroupEntry(Group=newGroup, creature=item.creature).save()", "fn_id": 1, "class_fn": false, "repo": "mkarasch/pfss", "file": "pfss/models.py", "last_update_at": "2021-09-14T09:55:03+00:00", "original_content": "def duplicateGroup(id):\n    group = CreatureGroup.objects.get(id=id)\n    newGroup = CreatureGroup(name='%s_duplicate' % group.name, Augmented=group.Augmented)\n    newGroup.Grouping = group.Grouping\n    newGroup.save()\n    for item in group.AllowedExtraType.all():\n        newGroup.AllowedExtraType.add(item)\n    for item in group.DefaultExtraType.all():\n        newGroup.DefaultExtraType.add(item)\n    for item in group.groupentry_set.all():\n        GroupEntry(Group=newGroup, creature=item.creature).save()", "refactored": true, "pred": {"ppl": 4.9531474113464355, "ppl_lower": 5.539402008056641, "ppl/lowercase_ppl": -1.0699135724351845, "ppl/zlib": 0.006751152806732539, "Min_5.0% Prob": 15.066490173339844, "Min_10.0% Prob": 11.40116278330485, "Min_20.0% Prob": 7.598601098855337, "Min_30.0% Prob": 5.321630217631658, "Min_40.0% Prob": 3.9900041283642658, "Min_50.0% Prob": 3.2178105223237683, "Min_60.0% Prob": 2.689636721802814}}
{"hexsha": "21093aba0c887e3f2abc8c2d98ef2ec29048b249", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef time_stamps(seconds, sample_rate, sample_length, sample_duration):\n    \"\"\"Process to calculate the intervals based on the window size or time intervals.\n    Parameters\n    ----------\n    seconds : int or list\n        Can be of two types. int; a single value that determines the window size (seconds). list; a set of intervals, where each value is in (seconds).\n    sample_rate : float\n        Sample frequency used in the EEG (Hz).\n    sample_length : float\n        Sample length in data points (seconds x sample frequency).\n    sample_duration : float\n        Duration of the EEG (seconds).\n    \n    Returns\n    -------\n    intervals : list\n        List with the intervals, pairs of (Start, End) values in data points (seconds x sample frequency).\n    \"\"\"\n    intervals, i, flag = ([], 0, 0)\n    if type(seconds) == list:\n        if len(seconds) == 1:\n            seconds = seconds[0]\n        else:\n            if seconds[-1] > sample_duration.round():\n                raise Exception('Error in Window size. Intervals exceeds sample length.')\n            if seconds[0] != 0:\n                raise Exception('Error in Window size. First interval must be 0.')\n            else:\n                diff = np.diff(seconds)\n                for j, value in enumerate(diff):\n                    samples_per_frame = value * sample_rate\n                    intervals.append((i, i + samples_per_frame))\n                    i += samples_per_frame\n    if type(seconds) == int or type(seconds) == float:\n        samples_per_frame = seconds * sample_rate\n        while i + samples_per_frame <= sample_length:\n            intervals.append((i, i + samples_per_frame))\n            i = i + samples_per_frame\n        if i + samples_per_frame > sample_length:\n            intervals.append((i, sample_length))\n    intervals_rounded = [(round(steps[0], 2), round(steps[1], 2)) for steps in intervals]\n    print('Intervals: ', intervals_rounded)\n    if len(intervals) == 1:\n        intervals.append((0, sample_rate))\n        flag = 1\n    return (intervals, flag)", "fn_id": 5, "class_fn": false, "repo": "danive97/EEGRAPH", "file": "eegraph/tools.py", "last_update_at": "2021-03-26T09:28:21+00:00", "original_content": "def time_stamps(seconds, sample_rate, sample_length, sample_duration):\n    \"\"\"Process to calculate the intervals based on the window size or time intervals.\n    Parameters\n    ----------\n    seconds : int or list\n        Can be of two types. int; a single value that determines the window size (seconds). list; a set of intervals, where each value is in (seconds).\n    sample_rate : float\n        Sample frequency used in the EEG (Hz).\n    sample_length : float\n        Sample length in data points (seconds x sample frequency).\n    sample_duration : float\n        Duration of the EEG (seconds).\n    \n    Returns\n    -------\n    intervals : list\n        List with the intervals, pairs of (Start, End) values in data points (seconds x sample frequency).\n    \"\"\"\n    intervals, i, flag = ([], 0, 0)\n    if type(seconds) == list:\n        if len(seconds) == 1:\n            seconds = seconds[0]\n        else:\n            if seconds[-1] > sample_duration.round():\n                raise Exception('Error in Window size. Intervals exceeds sample length.')\n            if seconds[0] != 0:\n                raise Exception('Error in Window size. First interval must be 0.')\n            else:\n                diff = np.diff(seconds)\n                for j, value in enumerate(diff):\n                    samples_per_frame = value * sample_rate\n                    intervals.append((i, i + samples_per_frame))\n                    i += samples_per_frame\n    if type(seconds) == int or type(seconds) == float:\n        samples_per_frame = seconds * sample_rate\n        while i + samples_per_frame <= sample_length:\n            intervals.append((i, i + samples_per_frame))\n            i = i + samples_per_frame\n        if i + samples_per_frame > sample_length:\n            intervals.append((i, sample_length))\n    intervals_rounded = [(round(steps[0], 2), round(steps[1], 2)) for steps in intervals]\n    print('Intervals: ', intervals_rounded)\n    if len(intervals) == 1:\n        intervals.append((0, sample_rate))\n        flag = 1\n    return (intervals, flag)", "refactored": true, "pred": {"ppl": 4.616461753845215, "ppl_lower": 4.631695747375488, "ppl/lowercase_ppl": -1.0021537880091418, "ppl/zlib": 0.0021696858972077796, "Min_5.0% Prob": 10.027731557687124, "Min_10.0% Prob": 8.185634350290103, "Min_20.0% Prob": 6.010005408403825, "Min_30.0% Prob": 4.66405832280918, "Min_40.0% Prob": 3.7305904436232473, "Min_50.0% Prob": 3.046265479086376, "Min_60.0% Prob": 2.551998017273717}}
{"hexsha": "df37bb46962a858756a40ea08c445056a8eba2a4", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef config_to_functions(config):\n    \"\"\"\n    Takes in the data for a config and returns a list of functions to call the meet it's criteria.\n    list of str -> list of (none -> (str, any))\n    \"\"\"\n    functions = []\n    in_multiline_comment = False\n    for line in config:\n        line = remove_new_lines(line)\n        in_multiline_comment = in_multiline_comment or line_starts_multiline_comment_start(line)\n        multiline_comment_end = line_starts_multiline_comment_end(line)\n        if not in_multiline_comment and multiline_comment_end:\n            message = 'Multiline comment end: ' + constants.MULTILINE_COMMENT_END\n            message += ' must inclose a comment.'\n            raise Exception(message)\n        if multiline_comment_end:\n            in_multiline_comment = False\n        if is_line_commented(line) or is_empty_line(line) or in_multiline_comment or multiline_comment_end:\n            continue\n        check_config_line(line)\n        command_type = get_command_type(line)\n        if not is_type(command_type) and (not is_complex_type(command_type)):\n            on_not_valid_type(command_type)\n        definition = get_command_definition(line)\n        functions.append(build_input_func(any_type_to_input_functions(command_type, definition), command_type))\n    if in_multiline_comment:\n        message = 'Multiline comment start: ' + constants.MULTILINE_COMMENT_START\n        message += ' must be inclosed with: ' + constants.MULTILINE_COMMENT_END\n        raise Exception(message)\n    return functions", "fn_id": 1, "class_fn": false, "repo": "FroshTheFrog/life_logger", "file": "config_utils.py", "last_update_at": "2021-03-19T03:40:42+00:00", "original_content": "def config_to_functions(config):\n    \"\"\"\n    Takes in the data for a config and returns a list of functions to call the meet it's criteria.\n    list of str -> list of (none -> (str, any))\n    \"\"\"\n    functions = []\n    in_multiline_comment = False\n    for line in config:\n        line = remove_new_lines(line)\n        in_multiline_comment = in_multiline_comment or line_starts_multiline_comment_start(line)\n        multiline_comment_end = line_starts_multiline_comment_end(line)\n        if not in_multiline_comment and multiline_comment_end:\n            message = 'Multiline comment end: ' + constants.MULTILINE_COMMENT_END\n            message += ' must inclose a comment.'\n            raise Exception(message)\n        if multiline_comment_end:\n            in_multiline_comment = False\n        if is_line_commented(line) or is_empty_line(line) or in_multiline_comment or multiline_comment_end:\n            continue\n        check_config_line(line)\n        command_type = get_command_type(line)\n        if not is_type(command_type) and (not is_complex_type(command_type)):\n            on_not_valid_type(command_type)\n        definition = get_command_definition(line)\n        functions.append(build_input_func(any_type_to_input_functions(command_type, definition), command_type))\n    if in_multiline_comment:\n        message = 'Multiline comment start: ' + constants.MULTILINE_COMMENT_START\n        message += ' must be inclosed with: ' + constants.MULTILINE_COMMENT_END\n        raise Exception(message)\n    return functions", "refactored": true, "pred": {"ppl": 4.978083610534668, "ppl_lower": 5.439691543579102, "ppl/lowercase_ppl": -1.0552491412445546, "ppl/zlib": 0.0030057022468421794, "Min_5.0% Prob": 11.534125185012817, "Min_10.0% Prob": 9.415142571926117, "Min_20.0% Prob": 6.728063461184502, "Min_30.0% Prob": 5.108518768846989, "Min_40.0% Prob": 3.956514327052217, "Min_50.0% Prob": 3.2054627031384415, "Min_60.0% Prob": 2.6818413956562264}}
{"hexsha": "4fbd04e883334dd6fb994cd9cbab874f5c4a5931", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef load_spectrum(filename, grid):\n    \"\"\"\n    Load a single spectrum\n    \"\"\"\n    file_in = pyfits.open(filename)\n    wl = np.array(file_in[0].data[2])\n    flux = np.array(file_in[0].data[0])\n    ivar = np.array(file_in[0].data[1])\n    redshift = file_in[0].header['Z']\n    wl_shifted = wl - redshift * wl\n    flux_rs = interpolate.interp1d(wl_shifted, flux)(grid)\n    ivar_rs = interpolate.interp1d(wl_shifted, ivar)(grid)\n    ivar_rs[ivar_rs < 0] = 0.0\n    return (flux_rs, ivar_rs)", "fn_id": 1, "class_fn": false, "repo": "HaifengWangNAOC/Cannon", "file": "TheCannon/lamost.py", "last_update_at": "2021-07-22T12:57:40+00:00", "original_content": "def load_spectrum(filename, grid):\n    \"\"\"\n    Load a single spectrum\n    \"\"\"\n    file_in = pyfits.open(filename)\n    wl = np.array(file_in[0].data[2])\n    flux = np.array(file_in[0].data[0])\n    ivar = np.array(file_in[0].data[1])\n    redshift = file_in[0].header['Z']\n    wl_shifted = wl - redshift * wl\n    flux_rs = interpolate.interp1d(wl_shifted, flux)(grid)\n    ivar_rs = interpolate.interp1d(wl_shifted, ivar)(grid)\n    ivar_rs[ivar_rs < 0] = 0.0\n    return (flux_rs, ivar_rs)", "refactored": true, "pred": {"ppl": 3.4398844242095947, "ppl_lower": 3.5270016193389893, "ppl/lowercase_ppl": -1.0202440266040445, "ppl/zlib": 0.005042603564111856, "Min_5.0% Prob": 10.306806988186306, "Min_10.0% Prob": 8.172945675097013, "Min_20.0% Prob": 5.510484789547167, "Min_30.0% Prob": 4.01333652149167, "Min_40.0% Prob": 3.0713048140077213, "Min_50.0% Prob": 2.4680637116691, "Min_60.0% Prob": 2.0585126228091357}}
{"hexsha": "60a398f13b9ba05c02fed5b550c7fed93451f969", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef serializedATN():\n    with StringIO() as buf:\n        buf.write('\\x03\u608b\ua72a\u8133\ub9ed\u417c\u3be7\u7786\u5964\\x03\\x1a')\n        buf.write('\\x82\\x04\\x02\\t\\x02\\x04\\x03\\t\\x03\\x04\\x04\\t\\x04\\x04\\x05\\t\\x05\\x04\\x06\\t\\x06\\x04\\x07\\t\\x07')\n        buf.write('\\x04\\x08\\t\\x08\\x04\\t\\t\\t\\x04\\n\\t\\n\\x04\\x0b\\t\\x0b\\x04\\x0c\\t\\x0c\\x04\\r\\t\\r\\x04\\x0e')\n        buf.write('\\t\\x0e\\x04\\x0f\\t\\x0f\\x03\\x02\\x05\\x02 \\n\\x02\\x03\\x02\\x03\\x02\\x05\\x02$\\n\\x02\\x03\\x02\\x03\\x02')\n        buf.write('\\x03\\x02\\x03\\x02\\x03\\x03\\x03\\x03\\x05\\x03,\\n\\x03\\x07\\x03.\\n\\x03\\x0c\\x03\\x0e\\x031\\x0b\\x03\\x03')\n        buf.write('\\x04\\x03\\x04\\x03\\x04\\x03\\x04\\x03\\x04\\x03\\x04\\x03\\x04\\x03\\x04\\x05\\x04;\\n\\x04\\x03\\x05\\x03\\x05\\x03\\x05\\x03')\n        buf.write('\\x06\\x03\\x06\\x05\\x06B\\n\\x06\\x03\\x06\\x06\\x06E\\n\\x06\\r\\x06\\x0e\\x06F\\x03\\x07\\x03\\x07\\x03\\x07\\x05\\x07')\n        buf.write('L\\n\\x07\\x03\\x07\\x05\\x07O\\n\\x07\\x06\\x07Q\\n\\x07\\r\\x07\\x0e\\x07R\\x03\\x08\\x03\\x08\\x05\\x08W\\n\\x08')\n        buf.write('\\x03\\x08\\x03\\x08\\x05\\x08[\\n\\x08\\x03\\t\\x03\\t\\x03\\t\\x05\\t`\\n\\t\\x06\\tb\\n\\t\\r\\t\\x0e')\n        buf.write('\\tc\\x03\\n\\x03\\n\\x03\\x0b\\x03\\x0b\\x05\\x0bj\\n\\x0b\\x03\\x0c\\x03\\x0c\\x05\\x0cn\\n\\x0c\\x03\\r')\n        buf.write('\\x03\\r\\x03\\r\\x03\\r\\x05\\rt\\n\\r\\x03\\x0e\\x03\\x0e\\x05\\x0ex\\n\\x0e\\x05\\x0ez\\n\\x0e')\n        buf.write('\\x03\\x0e\\x03\\x0e\\x03\\x0e\\x03\\x0e\\x03\\x0f\\x03\\x0f\\x03\\x0f\\x02\\x02\\x10\\x02\\x04\\x06\\x08\\n\\x0c')\n        buf.write('\\x0e\\x10\\x12\\x14\\x16\\x18\\x1a\\x1c\\x02\\x06\\x03\\x02\\x0e\\x0f\\x04\\x02\\x0e\\x0e\\x10\\x11')\n        buf.write('\\x03\\x02\\n\\x0b\\x03\\x02\\x13\\x16\\x02\\x89\\x02\\x1f\\x03\\x02\\x02\\x02\\x04/\\x03\\x02\\x02\\x02\\x06')\n        buf.write(':\\x03\\x02\\x02\\x02\\x08<\\x03\\x02\\x02\\x02\\nD\\x03\\x02\\x02\\x02\\x0cP\\x03\\x02\\x02\\x02\\x0eV\\x03\\x02\\x02\\x02')\n        buf.write('\\x10a\\x03\\x02\\x02\\x02\\x12e\\x03\\x02\\x02\\x02\\x14g\\x03\\x02\\x02\\x02\\x16k\\x03\\x02\\x02\\x02\\x18o\\x03')\n        buf.write('\\x02\\x02\\x02\\x1ay\\x03\\x02\\x02\\x02\\x1c\\x7f\\x03\\x02\\x02\\x02\\x1e \\x07\\r\\x02\\x02\\x1f\\x1e\\x03')\n        buf.write('\\x02\\x02\\x02\\x1f \\x03\\x02\\x02\\x02 !\\x03\\x02\\x02\\x02!#\\t\\x02\\x02\\x02\"$\\x05\\x1c\\x0f\\x02#\"')\n        buf.write(\"\\x03\\x02\\x02\\x02#$\\x03\\x02\\x02\\x02$%\\x03\\x02\\x02\\x02%&\\x07\\x03\\x02\\x02&'\\x05\\x04\\x03\\x02'(\\x07\")\n        buf.write('\\x04\\x02\\x02(\\x03\\x03\\x02\\x02\\x02)+\\x05\\x06\\x04\\x02*,\\x07\\x05\\x02\\x02+*\\x03\\x02\\x02\\x02+,\\x03\\x02')\n        buf.write('\\x02\\x02,.\\x03\\x02\\x02\\x02-)\\x03\\x02\\x02\\x02.1\\x03\\x02\\x02\\x02/-\\x03\\x02\\x02\\x02/0\\x03\\x02')\n        buf.write('\\x02\\x020\\x05\\x03\\x02\\x02\\x021/\\x03\\x02\\x02\\x022;\\x05\\x14\\x0b\\x023;\\x05\\x0e\\x08')\n        buf.write('\\x024;\\x05\\x08\\x05\\x0256\\x05\\x1c\\x0f\\x0267\\x07\\x06\\x02\\x0278\\x05\\x1c')\n        buf.write('\\x0f\\x028;\\x03\\x02\\x02\\x029;\\x05\\x1a\\x0e\\x02:2\\x03\\x02\\x02\\x02:3\\x03\\x02\\x02\\x02:4')\n        buf.write('\\x03\\x02\\x02\\x02:5\\x03\\x02\\x02\\x02:9\\x03\\x02\\x02\\x02;\\x07\\x03\\x02\\x02\\x02<=\\t\\x03\\x02\\x02=>')\n        buf.write('\\x05\\n\\x06\\x02>\\t\\x03\\x02\\x02\\x02?A\\x07\\x07\\x02\\x02@B\\x05\\x0c\\x07\\x02A@\\x03\\x02\\x02\\x02AB\\x03')\n        buf.write('\\x02\\x02\\x02BC\\x03\\x02\\x02\\x02CE\\x07\\x08\\x02\\x02D?\\x03\\x02\\x02\\x02EF\\x03\\x02\\x02\\x02FD\\x03\\x02\\x02')\n        buf.write('\\x02FG\\x03\\x02\\x02\\x02G\\x0b\\x03\\x02\\x02\\x02HK\\x05\\x1c\\x0f\\x02IJ\\x07\\x06\\x02\\x02JL\\x05\\x1c')\n        buf.write('\\x0f\\x02KI\\x03\\x02\\x02\\x02KL\\x03\\x02\\x02\\x02LN\\x03\\x02\\x02\\x02MO\\x07\\t\\x02\\x02NM\\x03\\x02\\x02')\n        buf.write('\\x02NO\\x03\\x02\\x02\\x02OQ\\x03\\x02\\x02\\x02PH\\x03\\x02\\x02\\x02QR\\x03\\x02\\x02\\x02RP\\x03\\x02\\x02\\x02R')\n        buf.write('S\\x03\\x02\\x02\\x02S\\r\\x03\\x02\\x02\\x02TW\\x05\\x16\\x0c\\x02UW\\x05\\x1a\\x0e\\x02VT\\x03\\x02\\x02\\x02')\n        buf.write('VU\\x03\\x02\\x02\\x02WX\\x03\\x02\\x02\\x02XZ\\x05\\x10\\t\\x02Y[\\x05\\n\\x06\\x02ZY\\x03\\x02\\x02\\x02Z[')\n        buf.write('\\x03\\x02\\x02\\x02[\\x0f\\x03\\x02\\x02\\x02\\\\_\\x05\\x12\\n\\x02]`\\x05\\x16\\x0c\\x02^`\\x05\\x1a\\x0e')\n        buf.write('\\x02_]\\x03\\x02\\x02\\x02_^\\x03\\x02\\x02\\x02`b\\x03\\x02\\x02\\x02a\\\\\\x03\\x02\\x02\\x02bc\\x03\\x02\\x02\\x02')\n        buf.write('ca\\x03\\x02\\x02\\x02cd\\x03\\x02\\x02\\x02d\\x11\\x03\\x02\\x02\\x02ef\\t\\x04\\x02\\x02f\\x13\\x03\\x02\\x02\\x02')\n        buf.write('gi\\x05\\x16\\x0c\\x02hj\\x05\\n\\x06\\x02ih\\x03\\x02\\x02\\x02ij\\x03\\x02\\x02\\x02j\\x15\\x03\\x02\\x02\\x02')\n        buf.write('km\\x05\\x1c\\x0f\\x02ln\\x05\\x18\\r\\x02ml\\x03\\x02\\x02\\x02mn\\x03\\x02\\x02\\x02n\\x17\\x03\\x02\\x02')\n        buf.write('\\x02op\\x07\\x0c\\x02\\x02ps\\x05\\x1c\\x0f\\x02qr\\x07\\x0c\\x02\\x02rt\\x05\\x1c\\x0f\\x02sq\\x03\\x02\\x02')\n        buf.write('\\x02st\\x03\\x02\\x02\\x02t\\x19\\x03\\x02\\x02\\x02uw\\x07\\x12\\x02\\x02vx\\x05\\x1c\\x0f\\x02wv\\x03\\x02')\n        buf.write('\\x02\\x02wx\\x03\\x02\\x02\\x02xz\\x03\\x02\\x02\\x02yu\\x03\\x02\\x02\\x02yz\\x03\\x02\\x02\\x02z{\\x03\\x02\\x02\\x02')\n        buf.write('{|\\x07\\x03\\x02\\x02|}\\x05\\x04\\x03\\x02}~\\x07\\x04\\x02\\x02~\\x1b\\x03\\x02\\x02\\x02\\x7f\\x80\\t')\n        buf.write('\\x05\\x02\\x02\\x80\\x1d\\x03\\x02\\x02\\x02\\x15\\x1f#+/:AFKNRVZ_cimswy')\n        return buf.getvalue()", "fn_id": 0, "class_fn": false, "repo": "qwang70/GraphVizDotToDatabaseParsing", "file": "script/DOTParser.py", "last_update_at": "2021-08-13T04:55:13+00:00", "original_content": "def serializedATN():\n    with StringIO() as buf:\n        buf.write('\\x03\u608b\ua72a\u8133\ub9ed\u417c\u3be7\u7786\u5964\\x03\\x1a')\n        buf.write('\\x82\\x04\\x02\\t\\x02\\x04\\x03\\t\\x03\\x04\\x04\\t\\x04\\x04\\x05\\t\\x05\\x04\\x06\\t\\x06\\x04\\x07\\t\\x07')\n        buf.write('\\x04\\x08\\t\\x08\\x04\\t\\t\\t\\x04\\n\\t\\n\\x04\\x0b\\t\\x0b\\x04\\x0c\\t\\x0c\\x04\\r\\t\\r\\x04\\x0e')\n        buf.write('\\t\\x0e\\x04\\x0f\\t\\x0f\\x03\\x02\\x05\\x02 \\n\\x02\\x03\\x02\\x03\\x02\\x05\\x02$\\n\\x02\\x03\\x02\\x03\\x02')\n        buf.write('\\x03\\x02\\x03\\x02\\x03\\x03\\x03\\x03\\x05\\x03,\\n\\x03\\x07\\x03.\\n\\x03\\x0c\\x03\\x0e\\x031\\x0b\\x03\\x03')\n        buf.write('\\x04\\x03\\x04\\x03\\x04\\x03\\x04\\x03\\x04\\x03\\x04\\x03\\x04\\x03\\x04\\x05\\x04;\\n\\x04\\x03\\x05\\x03\\x05\\x03\\x05\\x03')\n        buf.write('\\x06\\x03\\x06\\x05\\x06B\\n\\x06\\x03\\x06\\x06\\x06E\\n\\x06\\r\\x06\\x0e\\x06F\\x03\\x07\\x03\\x07\\x03\\x07\\x05\\x07')\n        buf.write('L\\n\\x07\\x03\\x07\\x05\\x07O\\n\\x07\\x06\\x07Q\\n\\x07\\r\\x07\\x0e\\x07R\\x03\\x08\\x03\\x08\\x05\\x08W\\n\\x08')\n        buf.write('\\x03\\x08\\x03\\x08\\x05\\x08[\\n\\x08\\x03\\t\\x03\\t\\x03\\t\\x05\\t`\\n\\t\\x06\\tb\\n\\t\\r\\t\\x0e')\n        buf.write('\\tc\\x03\\n\\x03\\n\\x03\\x0b\\x03\\x0b\\x05\\x0bj\\n\\x0b\\x03\\x0c\\x03\\x0c\\x05\\x0cn\\n\\x0c\\x03\\r')\n        buf.write('\\x03\\r\\x03\\r\\x03\\r\\x05\\rt\\n\\r\\x03\\x0e\\x03\\x0e\\x05\\x0ex\\n\\x0e\\x05\\x0ez\\n\\x0e')\n        buf.write('\\x03\\x0e\\x03\\x0e\\x03\\x0e\\x03\\x0e\\x03\\x0f\\x03\\x0f\\x03\\x0f\\x02\\x02\\x10\\x02\\x04\\x06\\x08\\n\\x0c')\n        buf.write('\\x0e\\x10\\x12\\x14\\x16\\x18\\x1a\\x1c\\x02\\x06\\x03\\x02\\x0e\\x0f\\x04\\x02\\x0e\\x0e\\x10\\x11')\n        buf.write('\\x03\\x02\\n\\x0b\\x03\\x02\\x13\\x16\\x02\\x89\\x02\\x1f\\x03\\x02\\x02\\x02\\x04/\\x03\\x02\\x02\\x02\\x06')\n        buf.write(':\\x03\\x02\\x02\\x02\\x08<\\x03\\x02\\x02\\x02\\nD\\x03\\x02\\x02\\x02\\x0cP\\x03\\x02\\x02\\x02\\x0eV\\x03\\x02\\x02\\x02')\n        buf.write('\\x10a\\x03\\x02\\x02\\x02\\x12e\\x03\\x02\\x02\\x02\\x14g\\x03\\x02\\x02\\x02\\x16k\\x03\\x02\\x02\\x02\\x18o\\x03')\n        buf.write('\\x02\\x02\\x02\\x1ay\\x03\\x02\\x02\\x02\\x1c\\x7f\\x03\\x02\\x02\\x02\\x1e \\x07\\r\\x02\\x02\\x1f\\x1e\\x03')\n        buf.write('\\x02\\x02\\x02\\x1f \\x03\\x02\\x02\\x02 !\\x03\\x02\\x02\\x02!#\\t\\x02\\x02\\x02\"$\\x05\\x1c\\x0f\\x02#\"')\n        buf.write(\"\\x03\\x02\\x02\\x02#$\\x03\\x02\\x02\\x02$%\\x03\\x02\\x02\\x02%&\\x07\\x03\\x02\\x02&'\\x05\\x04\\x03\\x02'(\\x07\")\n        buf.write('\\x04\\x02\\x02(\\x03\\x03\\x02\\x02\\x02)+\\x05\\x06\\x04\\x02*,\\x07\\x05\\x02\\x02+*\\x03\\x02\\x02\\x02+,\\x03\\x02')\n        buf.write('\\x02\\x02,.\\x03\\x02\\x02\\x02-)\\x03\\x02\\x02\\x02.1\\x03\\x02\\x02\\x02/-\\x03\\x02\\x02\\x02/0\\x03\\x02')\n        buf.write('\\x02\\x020\\x05\\x03\\x02\\x02\\x021/\\x03\\x02\\x02\\x022;\\x05\\x14\\x0b\\x023;\\x05\\x0e\\x08')\n        buf.write('\\x024;\\x05\\x08\\x05\\x0256\\x05\\x1c\\x0f\\x0267\\x07\\x06\\x02\\x0278\\x05\\x1c')\n        buf.write('\\x0f\\x028;\\x03\\x02\\x02\\x029;\\x05\\x1a\\x0e\\x02:2\\x03\\x02\\x02\\x02:3\\x03\\x02\\x02\\x02:4')\n        buf.write('\\x03\\x02\\x02\\x02:5\\x03\\x02\\x02\\x02:9\\x03\\x02\\x02\\x02;\\x07\\x03\\x02\\x02\\x02<=\\t\\x03\\x02\\x02=>')\n        buf.write('\\x05\\n\\x06\\x02>\\t\\x03\\x02\\x02\\x02?A\\x07\\x07\\x02\\x02@B\\x05\\x0c\\x07\\x02A@\\x03\\x02\\x02\\x02AB\\x03')\n        buf.write('\\x02\\x02\\x02BC\\x03\\x02\\x02\\x02CE\\x07\\x08\\x02\\x02D?\\x03\\x02\\x02\\x02EF\\x03\\x02\\x02\\x02FD\\x03\\x02\\x02')\n        buf.write('\\x02FG\\x03\\x02\\x02\\x02G\\x0b\\x03\\x02\\x02\\x02HK\\x05\\x1c\\x0f\\x02IJ\\x07\\x06\\x02\\x02JL\\x05\\x1c')\n        buf.write('\\x0f\\x02KI\\x03\\x02\\x02\\x02KL\\x03\\x02\\x02\\x02LN\\x03\\x02\\x02\\x02MO\\x07\\t\\x02\\x02NM\\x03\\x02\\x02')\n        buf.write('\\x02NO\\x03\\x02\\x02\\x02OQ\\x03\\x02\\x02\\x02PH\\x03\\x02\\x02\\x02QR\\x03\\x02\\x02\\x02RP\\x03\\x02\\x02\\x02R')\n        buf.write('S\\x03\\x02\\x02\\x02S\\r\\x03\\x02\\x02\\x02TW\\x05\\x16\\x0c\\x02UW\\x05\\x1a\\x0e\\x02VT\\x03\\x02\\x02\\x02')\n        buf.write('VU\\x03\\x02\\x02\\x02WX\\x03\\x02\\x02\\x02XZ\\x05\\x10\\t\\x02Y[\\x05\\n\\x06\\x02ZY\\x03\\x02\\x02\\x02Z[')\n        buf.write('\\x03\\x02\\x02\\x02[\\x0f\\x03\\x02\\x02\\x02\\\\_\\x05\\x12\\n\\x02]`\\x05\\x16\\x0c\\x02^`\\x05\\x1a\\x0e')\n        buf.write('\\x02_]\\x03\\x02\\x02\\x02_^\\x03\\x02\\x02\\x02`b\\x03\\x02\\x02\\x02a\\\\\\x03\\x02\\x02\\x02bc\\x03\\x02\\x02\\x02')\n        buf.write('ca\\x03\\x02\\x02\\x02cd\\x03\\x02\\x02\\x02d\\x11\\x03\\x02\\x02\\x02ef\\t\\x04\\x02\\x02f\\x13\\x03\\x02\\x02\\x02')\n        buf.write('gi\\x05\\x16\\x0c\\x02hj\\x05\\n\\x06\\x02ih\\x03\\x02\\x02\\x02ij\\x03\\x02\\x02\\x02j\\x15\\x03\\x02\\x02\\x02')\n        buf.write('km\\x05\\x1c\\x0f\\x02ln\\x05\\x18\\r\\x02ml\\x03\\x02\\x02\\x02mn\\x03\\x02\\x02\\x02n\\x17\\x03\\x02\\x02')\n        buf.write('\\x02op\\x07\\x0c\\x02\\x02ps\\x05\\x1c\\x0f\\x02qr\\x07\\x0c\\x02\\x02rt\\x05\\x1c\\x0f\\x02sq\\x03\\x02\\x02')\n        buf.write('\\x02st\\x03\\x02\\x02\\x02t\\x19\\x03\\x02\\x02\\x02uw\\x07\\x12\\x02\\x02vx\\x05\\x1c\\x0f\\x02wv\\x03\\x02')\n        buf.write('\\x02\\x02wx\\x03\\x02\\x02\\x02xz\\x03\\x02\\x02\\x02yu\\x03\\x02\\x02\\x02yz\\x03\\x02\\x02\\x02z{\\x03\\x02\\x02\\x02')\n        buf.write('{|\\x07\\x03\\x02\\x02|}\\x05\\x04\\x03\\x02}~\\x07\\x04\\x02\\x02~\\x1b\\x03\\x02\\x02\\x02\\x7f\\x80\\t')\n        buf.write('\\x05\\x02\\x02\\x80\\x1d\\x03\\x02\\x02\\x02\\x15\\x1f#+/:AFKNRVZ_cimswy')\n        return buf.getvalue()", "refactored": true, "pred": {"ppl": 1.6826766729354858, "ppl_lower": 1.7484562397003174, "ppl/lowercase_ppl": -1.0736904578877373, "ppl/zlib": 0.00043007089522838796, "Min_5.0% Prob": 6.493335742576449, "Min_10.0% Prob": 4.53732443673938, "Min_20.0% Prob": 2.5774364488966324, "Min_30.0% Prob": 1.7383148272527882, "Min_40.0% Prob": 1.3014231928730096, "Min_50.0% Prob": 1.0417461368103704, "Min_60.0% Prob": 0.8684292594923179}}
{"hexsha": "79b07cf6f3f380f296c7ce80ec1c84c5aa0ae81e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef deep_seq_design():\n    root_dir = f'data/design/cullpdb_val_deep/{exp_flag}anneal_val_deep'\n    mut_matrix_anneal = np.zeros((20, 20))\n    native_aa_all = []\n    design_aa_all = []\n    seq_len = []\n    recovery = []\n    pdb_id_all = []\n    recovery_res = np.zeros(20)\n    count_res = np.zeros(20)\n    num = 100\n    for pdb_id in pdb_selected:\n        seq_best_all = []\n        for j in range(num):\n            data_anneal = h5py.File(f'{root_dir}/{pdb_id}_profile_{j}.h5', 'r')\n            designed_seq = data_anneal['profile'][()]\n            seq_best = designed_seq[1]\n            seq_best_all.append(seq_best)\n            seq_native = designed_seq[0]\n            recovery_fraction = np.sum(seq_best == seq_native) / float(len(seq_native))\n            recovery.append(recovery_fraction)\n            seq_len.append(len(seq_native))\n            pdb_id_all.append(pdb_id)\n            for i in range(seq_native.shape[0]):\n                mut_matrix_anneal[map_aa[seq_native[i]], map_aa[seq_best[i]]] += 1\n                native_aa_all.append(map_aa[seq_native[i]])\n                design_aa_all.append(map_aa[seq_best[i]])\n            for i in range(20):\n                idx = seq_native == i\n                recovery_res[i] += np.sum(seq_best[idx] == seq_native[idx])\n                count_res[i] += len(seq_native[idx])\n        with open(f'{root_dir}/{pdb_id}_seq_best.fasta', 'w') as mf:\n            s = ''.join([idx2aa[x] for x in seq_native])\n            mf.write(f'>0\\n{s}\\n')\n            for j in range(len(seq_best_all)):\n                s = ''.join([idx2aa[x] for x in seq_best_all[j]])\n                mf.write(f'>{j + 1}\\n')\n                mf.write(f'{s}\\n')\n    df = pd.DataFrame({'pdb': pdb_id_all, 'seq_len': seq_len, 'recovery': recovery})\n    df.to_csv(f'{root_dir}/recovery.csv', index=False)\n    np.save(f'data/design/cullpdb_val_deep/{exp_flag}anneal_val_deep/mut_matrix_anneal.npy', mut_matrix_anneal)\n    df = pd.DataFrame({'native_aa': native_aa_all, 'design_aa': design_aa_all})\n    df.to_csv(f'{root_dir}/native_design_aa.csv', index=False)\n    fig = pl.figure()\n    pl.plot(df['seq_len'], df['recovery'], 'bo')\n    pl.title('full seq redesign')\n    pl.savefig(f'{root_dir}/full_seq_design_seqlen_recovery.pdf')\n    fig = pl.figure()\n    pl.hist(df['recovery'], bins=np.arange(10) * 0.05 + 0.05)\n    pl.xlabel('native sequence recovery fraction')\n    pl.ylabel('N')\n    pl.savefig(f'{root_dir}/full_seq_design_recovery_hist.pdf')\n    fig = pl.figure()\n    pl.plot(np.arange(20), recovery_res / count_res)\n    pl.xticks(np.arange(20), labels=aa)\n    pl.title('residue recovery fraction in full seq redesign')\n    pl.savefig(f'{root_dir}/full_seq_design_residue_recovery.pdf')\n    fig = pl.figure()\n    mut_matrix_anneal_freq = mut_matrix_anneal / mut_matrix_anneal.sum(axis=1)[:, None]\n    pl.imshow(mut_matrix_anneal_freq, cmap='jet')\n    pl.xlabel('mutated residue')\n    pl.ylabel('native residue')\n    pl.xticks(np.arange(20), labels=ordered_aa)\n    pl.yticks(np.arange(20), labels=ordered_aa)\n    pl.colorbar()\n    pl.title('full seq redesign')\n    pl.savefig(f'{root_dir}/full_seq_design_residue_use.pdf')\n    fig = pl.figure()\n    res_all = np.concatenate(seq_best_all).flatten()\n    aa_bins = np.arange(21) - 0.5\n    pl.hist(res_all, bins=aa_bins, histtype='step')\n    pl.xticks(np.arange(20), labels=aa)\n    pl.title('residue use frequency')\n    pl.savefig(f'{root_dir}/full_seq_design_residue_use_frequency.pdf')", "fn_id": 2, "class_fn": false, "repo": "lahplover/nnef", "file": "nnef/scripts/designed_seq.py", "last_update_at": "2021-06-30T06:47:00+00:00", "original_content": "def deep_seq_design():\n    root_dir = f'data/design/cullpdb_val_deep/{exp_flag}anneal_val_deep'\n    mut_matrix_anneal = np.zeros((20, 20))\n    native_aa_all = []\n    design_aa_all = []\n    seq_len = []\n    recovery = []\n    pdb_id_all = []\n    recovery_res = np.zeros(20)\n    count_res = np.zeros(20)\n    num = 100\n    for pdb_id in pdb_selected:\n        seq_best_all = []\n        for j in range(num):\n            data_anneal = h5py.File(f'{root_dir}/{pdb_id}_profile_{j}.h5', 'r')\n            designed_seq = data_anneal['profile'][()]\n            seq_best = designed_seq[1]\n            seq_best_all.append(seq_best)\n            seq_native = designed_seq[0]\n            recovery_fraction = np.sum(seq_best == seq_native) / float(len(seq_native))\n            recovery.append(recovery_fraction)\n            seq_len.append(len(seq_native))\n            pdb_id_all.append(pdb_id)\n            for i in range(seq_native.shape[0]):\n                mut_matrix_anneal[map_aa[seq_native[i]], map_aa[seq_best[i]]] += 1\n                native_aa_all.append(map_aa[seq_native[i]])\n                design_aa_all.append(map_aa[seq_best[i]])\n            for i in range(20):\n                idx = seq_native == i\n                recovery_res[i] += np.sum(seq_best[idx] == seq_native[idx])\n                count_res[i] += len(seq_native[idx])\n        with open(f'{root_dir}/{pdb_id}_seq_best.fasta', 'w') as mf:\n            s = ''.join([idx2aa[x] for x in seq_native])\n            mf.write(f'>0\\n{s}\\n')\n            for j in range(len(seq_best_all)):\n                s = ''.join([idx2aa[x] for x in seq_best_all[j]])\n                mf.write(f'>{j + 1}\\n')\n                mf.write(f'{s}\\n')\n    df = pd.DataFrame({'pdb': pdb_id_all, 'seq_len': seq_len, 'recovery': recovery})\n    df.to_csv(f'{root_dir}/recovery.csv', index=False)\n    np.save(f'data/design/cullpdb_val_deep/{exp_flag}anneal_val_deep/mut_matrix_anneal.npy', mut_matrix_anneal)\n    df = pd.DataFrame({'native_aa': native_aa_all, 'design_aa': design_aa_all})\n    df.to_csv(f'{root_dir}/native_design_aa.csv', index=False)\n    fig = pl.figure()\n    pl.plot(df['seq_len'], df['recovery'], 'bo')\n    pl.title('full seq redesign')\n    pl.savefig(f'{root_dir}/full_seq_design_seqlen_recovery.pdf')\n    fig = pl.figure()\n    pl.hist(df['recovery'], bins=np.arange(10) * 0.05 + 0.05)\n    pl.xlabel('native sequence recovery fraction')\n    pl.ylabel('N')\n    pl.savefig(f'{root_dir}/full_seq_design_recovery_hist.pdf')\n    fig = pl.figure()\n    pl.plot(np.arange(20), recovery_res / count_res)\n    pl.xticks(np.arange(20), labels=aa)\n    pl.title('residue recovery fraction in full seq redesign')\n    pl.savefig(f'{root_dir}/full_seq_design_residue_recovery.pdf')\n    fig = pl.figure()\n    mut_matrix_anneal_freq = mut_matrix_anneal / mut_matrix_anneal.sum(axis=1)[:, None]\n    pl.imshow(mut_matrix_anneal_freq, cmap='jet')\n    pl.xlabel('mutated residue')\n    pl.ylabel('native residue')\n    pl.xticks(np.arange(20), labels=ordered_aa)\n    pl.yticks(np.arange(20), labels=ordered_aa)\n    pl.colorbar()\n    pl.title('full seq redesign')\n    pl.savefig(f'{root_dir}/full_seq_design_residue_use.pdf')\n    fig = pl.figure()\n    res_all = np.concatenate(seq_best_all).flatten()\n    aa_bins = np.arange(21) - 0.5\n    pl.hist(res_all, bins=aa_bins, histtype='step')\n    pl.xticks(np.arange(20), labels=aa)\n    pl.title('residue use frequency')\n    pl.savefig(f'{root_dir}/full_seq_design_residue_use_frequency.pdf')", "refactored": true, "pred": {"ppl": 2.6419997215270996, "ppl_lower": 2.7863690853118896, "ppl/lowercase_ppl": -1.0547619840608762, "ppl/zlib": 0.0009341693275940568, "Min_5.0% Prob": 9.621258707607494, "Min_10.0% Prob": 7.135868848538866, "Min_20.0% Prob": 4.549710106031568, "Min_30.0% Prob": 3.2046350894702806, "Min_40.0% Prob": 2.4231917794639, "Min_50.0% Prob": 1.9440333014408786, "Min_60.0% Prob": 1.6211808717474994}}
{"hexsha": "1d58be5f8c298cf630d37e7dc4f29b9760416dc5", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef parse_net_xml(doc):\n    total = len(list(doc.getiterator('wireless-network')))\n    tenth = total / 10\n    count = 0\n    clients = list()\n    parsed_list = []\n    for network in doc.getiterator('wireless-network'):\n        count += 1\n        try:\n            if count % tenth == 0:\n                continue\n        except ZeroDivisionError:\n            print('Division by zero error')\n        type = network.attrib['type']\n        channel = network.find('channel').text\n        bssid = network.find('BSSID').text\n        if type == 'probe' or channel == '0':\n            continue\n        encryption = network.getiterator('encryption')\n        privacy = ''\n        cipher = ''\n        auth = ''\n        if encryption is not None:\n            for item in encryption:\n                if item.text.startswith('WEP'):\n                    privacy = 'WEP'\n                    cipher = 'WEP'\n                    auth = ''\n                    break\n                elif item.text.startswith('WPA'):\n                    if item.text.endswith('PSK'):\n                        auth = 'PSK'\n                    elif item.text.endswith('AES-CCM'):\n                        cipher = 'CCMP ' + cipher\n                    elif item.text.endswith('TKIP'):\n                        cipher += 'TKIP '\n                elif item.text == 'None':\n                    privacy = 'OPN'\n        cipher = cipher.strip()\n        if cipher.find('CCMP') > -1:\n            privacy = 'WPA2'\n        if cipher.find('TKIP') > -1:\n            privacy += 'WPA'\n        power = network.find('snr-info')\n        dbm = ''\n        if power is not None:\n            dbm = power.find('max_signal_dbm').text\n        if int(dbm) > 1:\n            dbm = power.find('last_signal_dbm').text\n        if int(dbm) > 1:\n            dbm = power.find('min_signal_dbm').text\n        ssid = network.find('SSID')\n        essid_text = ''\n        if ssid is not None:\n            essid_text = network.find('SSID').find('essid').text\n        gps = network.find('gps-info')\n        lat, lon = ('', '')\n        if gps is not None:\n            lat = network.find('gps-info').find('min-lat').text\n            lon = network.find('gps-info').find('min-lon').text\n        data = dict(ESSID=essid_text, BSSID=bssid, Channel=channel, Privacy=privacy, Cipher=cipher, Authenticaiton=auth, DBM=dbm)\n        if lat and lon is not None:\n            google_map = 'https://maps.google.com/maps?q=' + lat + ',' + lon + '&ll=' + lat + ',' + lon + '&z=17'\n            google_map_link = '<a href=\"' + google_map + '\" target=\"_blank\"> Google map link</a>'\n            location = dict(Latitude=lat, Longitude=lon, Googlemap=google_map_link)\n        else:\n            not_found = 'Not coordinates available'\n            location = dict(Latitude=not_found, Longitude=not_found)\n        client_list = associatedClients(network, bssid, essid_text)\n        if client_list is not None:\n            data['client'] = client_list\n        else:\n            not_found = 'No clients found'\n            data['client'] = not_found\n        data['location'] = location\n        parsed_list.append(data)\n    return parsed_list", "fn_id": 1, "class_fn": false, "repo": "binkybear/kismet_web_viewer", "file": "app/netxml_to_csv.py", "last_update_at": "2021-05-22T03:59:05+00:00", "original_content": "def parse_net_xml(doc):\n    total = len(list(doc.getiterator('wireless-network')))\n    tenth = total / 10\n    count = 0\n    clients = list()\n    parsed_list = []\n    for network in doc.getiterator('wireless-network'):\n        count += 1\n        try:\n            if count % tenth == 0:\n                continue\n        except ZeroDivisionError:\n            print('Division by zero error')\n        type = network.attrib['type']\n        channel = network.find('channel').text\n        bssid = network.find('BSSID').text\n        if type == 'probe' or channel == '0':\n            continue\n        encryption = network.getiterator('encryption')\n        privacy = ''\n        cipher = ''\n        auth = ''\n        if encryption is not None:\n            for item in encryption:\n                if item.text.startswith('WEP'):\n                    privacy = 'WEP'\n                    cipher = 'WEP'\n                    auth = ''\n                    break\n                elif item.text.startswith('WPA'):\n                    if item.text.endswith('PSK'):\n                        auth = 'PSK'\n                    elif item.text.endswith('AES-CCM'):\n                        cipher = 'CCMP ' + cipher\n                    elif item.text.endswith('TKIP'):\n                        cipher += 'TKIP '\n                elif item.text == 'None':\n                    privacy = 'OPN'\n        cipher = cipher.strip()\n        if cipher.find('CCMP') > -1:\n            privacy = 'WPA2'\n        if cipher.find('TKIP') > -1:\n            privacy += 'WPA'\n        power = network.find('snr-info')\n        dbm = ''\n        if power is not None:\n            dbm = power.find('max_signal_dbm').text\n        if int(dbm) > 1:\n            dbm = power.find('last_signal_dbm').text\n        if int(dbm) > 1:\n            dbm = power.find('min_signal_dbm').text\n        ssid = network.find('SSID')\n        essid_text = ''\n        if ssid is not None:\n            essid_text = network.find('SSID').find('essid').text\n        gps = network.find('gps-info')\n        lat, lon = ('', '')\n        if gps is not None:\n            lat = network.find('gps-info').find('min-lat').text\n            lon = network.find('gps-info').find('min-lon').text\n        data = dict(ESSID=essid_text, BSSID=bssid, Channel=channel, Privacy=privacy, Cipher=cipher, Authenticaiton=auth, DBM=dbm)\n        if lat and lon is not None:\n            google_map = 'https://maps.google.com/maps?q=' + lat + ',' + lon + '&ll=' + lat + ',' + lon + '&z=17'\n            google_map_link = '<a href=\"' + google_map + '\" target=\"_blank\"> Google map link</a>'\n            location = dict(Latitude=lat, Longitude=lon, Googlemap=google_map_link)\n        else:\n            not_found = 'Not coordinates available'\n            location = dict(Latitude=not_found, Longitude=not_found)\n        client_list = associatedClients(network, bssid, essid_text)\n        if client_list is not None:\n            data['client'] = client_list\n        else:\n            not_found = 'No clients found'\n            data['client'] = not_found\n        data['location'] = location\n        parsed_list.append(data)\n    return parsed_list", "refactored": true, "pred": {"ppl": 3.200535297393799, "ppl_lower": 3.2716376781463623, "ppl/lowercase_ppl": -1.018887871462941, "ppl/zlib": 0.001163318076251431, "Min_5.0% Prob": 9.981338703632355, "Min_10.0% Prob": 7.763692343235016, "Min_20.0% Prob": 5.269554426211007, "Min_30.0% Prob": 3.775313450038926, "Min_40.0% Prob": 2.895082054856401, "Min_50.0% Prob": 2.3246420400295746, "Min_60.0% Prob": 1.9384558045840246}}
{"hexsha": "bc9332449ba47eee635aabdc3d23964f3b29f165", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_ratings(path, users_file_name, books_file_name):\n    df_users = pd.read_csv(path + users_file_name, encoding='UTF8')\n    df_books = pd.read_csv(path + books_file_name, encoding='UTF8')\n    df_users_books = pd.DataFrame(df_users, columns=['user_id', 'like'])\n    sr_users = []\n    sr_books = []\n    sr_ratings = []\n    str_like = list(np.array(df_users_books['like'].tolist()))\n    list_like = []\n    for i in str_like:\n        i = i.lstrip('[').rstrip(']')\n        i = i.split(', ')\n        list_like.append(i)\n    for user_idx in df_users_books['user_id']:\n        for book_idx in list_like[user_idx]:\n            if book_idx == '':\n                break\n            sr_users.append(user_idx)\n            sr_books.append(book_idx)\n            sr_ratings.append(1)\n    for book_idx in range(len(df_books)):\n        if sr_users[0]:\n            sr_users.append(sr_users[0])\n        if sr_books[0]:\n            sr_books.append(book_idx)\n        if sr_ratings[0]:\n            sr_ratings.append(0)\n    R = pd.DataFrame({'user_idx': sr_users, 'book_idx': sr_books, 'ratings': sr_ratings})\n    R = R.pivot_table('ratings', index='user_idx', columns='book_idx').fillna(0)\n    R.rename(columns=lambda x: int(x), inplace=True)\n    R = R.sort_index(axis=1)\n    return R", "fn_id": 0, "class_fn": false, "repo": "osamhack2021/AI_APP_handylib_devlib", "file": "AI/recommendation/ALS.py", "last_update_at": "2021-12-16T10:41:16+00:00", "original_content": "def get_ratings(path, users_file_name, books_file_name):\n    df_users = pd.read_csv(path + users_file_name, encoding='UTF8')\n    df_books = pd.read_csv(path + books_file_name, encoding='UTF8')\n    df_users_books = pd.DataFrame(df_users, columns=['user_id', 'like'])\n    sr_users = []\n    sr_books = []\n    sr_ratings = []\n    str_like = list(np.array(df_users_books['like'].tolist()))\n    list_like = []\n    for i in str_like:\n        i = i.lstrip('[').rstrip(']')\n        i = i.split(', ')\n        list_like.append(i)\n    for user_idx in df_users_books['user_id']:\n        for book_idx in list_like[user_idx]:\n            if book_idx == '':\n                break\n            sr_users.append(user_idx)\n            sr_books.append(book_idx)\n            sr_ratings.append(1)\n    for book_idx in range(len(df_books)):\n        if sr_users[0]:\n            sr_users.append(sr_users[0])\n        if sr_books[0]:\n            sr_books.append(book_idx)\n        if sr_ratings[0]:\n            sr_ratings.append(0)\n    R = pd.DataFrame({'user_idx': sr_users, 'book_idx': sr_books, 'ratings': sr_ratings})\n    R = R.pivot_table('ratings', index='user_idx', columns='book_idx').fillna(0)\n    R.rename(columns=lambda x: int(x), inplace=True)\n    R = R.sort_index(axis=1)\n    return R", "refactored": true, "pred": {"ppl": 2.8261592388153076, "ppl_lower": 2.991328001022339, "ppl/lowercase_ppl": -1.0546710822018543, "ppl/zlib": 0.0021509702493809043, "Min_5.0% Prob": 9.575201058387757, "Min_10.0% Prob": 7.165943924973651, "Min_20.0% Prob": 4.778579980494028, "Min_30.0% Prob": 3.3871695930957793, "Min_40.0% Prob": 2.5963235913091394, "Min_50.0% Prob": 2.0800731410809723, "Min_60.0% Prob": 1.7323888103980571}}
{"hexsha": "876c7c08adc0ef2ebe9987da8f1125f3cf04b813", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef word_bag_list(org_text):\n    \"\"\"Take text and do sum, return sumed sentence list.\"\"\"\n    nlp = spacy.load('en_core_web_sm')\n    tr = pytextrank.TextRank(logger=None)\n    nlp.add_pipe(tr.PipelineComponent, name='textrank', last=True)\n    doc = nlp(org_text)\n    whole_sent = ''\n    for sent in doc._.textrank.summary(limit_phrases=15, limit_sentences=5):\n        whole_sent = whole_sent + repr(sent).rstrip() + ' '\n    return [whole_sent]", "fn_id": 0, "class_fn": false, "repo": "liux2/Allegheny-College-CMPSC-COMP-liux2", "file": "music_sug/music_main/text_proc.py", "last_update_at": "2021-05-21T01:37:44+00:00", "original_content": "def word_bag_list(org_text):\n    \"\"\"Take text and do sum, return sumed sentence list.\"\"\"\n    nlp = spacy.load('en_core_web_sm')\n    tr = pytextrank.TextRank(logger=None)\n    nlp.add_pipe(tr.PipelineComponent, name='textrank', last=True)\n    doc = nlp(org_text)\n    whole_sent = ''\n    for sent in doc._.textrank.summary(limit_phrases=15, limit_sentences=5):\n        whole_sent = whole_sent + repr(sent).rstrip() + ' '\n    return [whole_sent]", "refactored": true, "pred": {"ppl": 6.6155500411987305, "ppl_lower": 8.915013313293457, "ppl/lowercase_ppl": -1.157886195254171, "ppl/zlib": 0.006448542473402852, "Min_5.0% Prob": 12.446890830993652, "Min_10.0% Prob": 10.525164572397868, "Min_20.0% Prob": 7.748712347399804, "Min_30.0% Prob": 5.995269227535166, "Min_40.0% Prob": 4.6753025400260135, "Min_50.0% Prob": 3.7650649190137657, "Min_60.0% Prob": 3.1722489075457796}}
{"hexsha": "6e58f05000ac2f4b1655ab9325ef2cbb23caea8b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef remapchunk(seg, chunk, chunkindex, scratchpath, layer=1, bits_per_dim=10, maxmip=11, correctvers=True):\n    x, y, z = chunkindex\n    pcgchunkid = io.pcg.get_chunk_id(layer=layer, x=x, y=y, z=z, bits_per_dim=bits_per_dim)\n    data = seg[chunk.index()]\n    try:\n        if correctvers:\n            mappings = readremapfiles(scratchpath, chunkindex, pcgchunkid, maxmip=maxmip)\n        else:\n            mappings = _readremapfiles(scratchpath, chunkindex, pcgchunkid, maxmip=maxmip)\n    except subprocess.CalledProcessError as e:\n        if data.max() == 0:\n            return data\n        else:\n            raise e\n    for mapping in mappings:\n        data = fastremap.remap(data, mapping, in_place=False, preserve_missing_labels=True)\n    seg[chunk.index()] = data\n    return seg", "fn_id": 2, "class_fn": false, "repo": "ZettaAI/Synaptor", "file": "synaptor/proc/io/agglomeration.py", "last_update_at": "2021-09-13T07:02:16+00:00", "original_content": "def remapchunk(seg, chunk, chunkindex, scratchpath, layer=1, bits_per_dim=10, maxmip=11, correctvers=True):\n    x, y, z = chunkindex\n    pcgchunkid = io.pcg.get_chunk_id(layer=layer, x=x, y=y, z=z, bits_per_dim=bits_per_dim)\n    data = seg[chunk.index()]\n    try:\n        if correctvers:\n            mappings = readremapfiles(scratchpath, chunkindex, pcgchunkid, maxmip=maxmip)\n        else:\n            mappings = _readremapfiles(scratchpath, chunkindex, pcgchunkid, maxmip=maxmip)\n    except subprocess.CalledProcessError as e:\n        if data.max() == 0:\n            return data\n        else:\n            raise e\n    for mapping in mappings:\n        data = fastremap.remap(data, mapping, in_place=False, preserve_missing_labels=True)\n    seg[chunk.index()] = data\n    return seg", "refactored": true, "pred": {"ppl": 7.068549156188965, "ppl_lower": 8.006243705749512, "ppl/lowercase_ppl": -1.0636955072167011, "ppl/zlib": 0.005402362562323409, "Min_5.0% Prob": 12.736066897710165, "Min_10.0% Prob": 10.69201765457789, "Min_20.0% Prob": 7.907886450489362, "Min_30.0% Prob": 6.019679804370828, "Min_40.0% Prob": 4.768413385165107, "Min_50.0% Prob": 3.8714694891308175, "Min_60.0% Prob": 3.2597830326181567}}
{"hexsha": "639b37fb1f2788f702b824b85cd19c165e09f9f6", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_cached_property():\n    \"\"\"Test the cached_property decorator\"\"\"\n    new_value = '99999'\n\n    class DummyClass:\n\n        def __init__(self):\n            self.value = '11111'\n\n        def change_value_in_instance(self, value):\n            self.value = value\n\n        @cached_property\n        def test_property(self):\n            return self.value\n\n        @property\n        def test_uncached_property(self):\n            return self.value\n    testClass = DummyClass()\n    first_cached_test_property = testClass.test_property\n    first_uncached_test_property = testClass.test_uncached_property\n    testClass.change_value_in_instance(new_value)\n    second_cached_test_property = testClass.test_property\n    second_uncached_test_property = testClass.test_uncached_property\n    assert first_cached_test_property == second_cached_test_property\n    assert first_cached_test_property == '11111'\n    assert first_uncached_test_property != second_uncached_test_property\n    assert first_uncached_test_property == '11111'\n    assert second_uncached_test_property == '99999'", "fn_id": 0, "class_fn": false, "repo": "musa-atlihan/jina", "file": "tests/unit/test_helper.py", "last_update_at": "2021-04-22T17:14:32+00:00", "original_content": "def test_cached_property():\n    \"\"\"Test the cached_property decorator\"\"\"\n    new_value = '99999'\n\n    class DummyClass:\n\n        def __init__(self):\n            self.value = '11111'\n\n        def change_value_in_instance(self, value):\n            self.value = value\n\n        @cached_property\n        def test_property(self):\n            return self.value\n\n        @property\n        def test_uncached_property(self):\n            return self.value\n    testClass = DummyClass()\n    first_cached_test_property = testClass.test_property\n    first_uncached_test_property = testClass.test_uncached_property\n    testClass.change_value_in_instance(new_value)\n    second_cached_test_property = testClass.test_property\n    second_uncached_test_property = testClass.test_uncached_property\n    assert first_cached_test_property == second_cached_test_property\n    assert first_cached_test_property == '11111'\n    assert first_uncached_test_property != second_uncached_test_property\n    assert first_uncached_test_property == '11111'\n    assert second_uncached_test_property == '99999'", "refactored": true, "pred": {"ppl": 2.1995816230773926, "ppl_lower": 2.1784019470214844, "ppl/lowercase_ppl": -0.9877254627042807, "ppl/zlib": 0.0026720921049184824, "Min_5.0% Prob": 8.060608625411987, "Min_10.0% Prob": 6.10687282167632, "Min_20.0% Prob": 3.7993744399587985, "Min_30.0% Prob": 2.625164997460467, "Min_40.0% Prob": 1.9780949511009605, "Min_50.0% Prob": 1.5815650777775645, "Min_60.0% Prob": 1.3166817715784986}}
{"hexsha": "b93ceb63b6b34d7c97ec44530d47645b54199e3e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef ndcg_at_ks(r, k_list, method=1, include_dcg=False):\n    \"\"\"\n\n    :param r: \u5339\u914darray \u957f\u5ea6\u4e3a\u6bcf\u4e2a\u9884\u6d4b\u7ed3\u679c\u7684\u5339\u914d\u7ed3\u679c\n    :param k_list:\n    :param method:\n    :param include_dcg:\n    :return:\n    \"\"\"\n    if r.shape[0] == 0:\n        ndcg_array = [0.0] * len(k_list)\n        dcg_array = [0.0] * len(k_list)\n    else:\n        dcg_array = dcg_at_ks(r, k_list, method)\n        ideal_r = np.array(sorted(r, reverse=True))\n        dcg_max_array = dcg_at_ks(ideal_r, k_list, method)\n        ndcg_array = dcg_array / dcg_max_array\n        ndcg_array = np.nan_to_num(ndcg_array)\n    if include_dcg:\n        return (ndcg_array, dcg_array)\n    else:\n        return ndcg_array", "fn_id": 18, "class_fn": false, "repo": "qtli/AOT", "file": "utils/common.py", "last_update_at": "2021-09-07T14:49:29+00:00", "original_content": "def ndcg_at_ks(r, k_list, method=1, include_dcg=False):\n    \"\"\"\n\n    :param r: \u5339\u914darray \u957f\u5ea6\u4e3a\u6bcf\u4e2a\u9884\u6d4b\u7ed3\u679c\u7684\u5339\u914d\u7ed3\u679c\n    :param k_list:\n    :param method:\n    :param include_dcg:\n    :return:\n    \"\"\"\n    if r.shape[0] == 0:\n        ndcg_array = [0.0] * len(k_list)\n        dcg_array = [0.0] * len(k_list)\n    else:\n        dcg_array = dcg_at_ks(r, k_list, method)\n        ideal_r = np.array(sorted(r, reverse=True))\n        dcg_max_array = dcg_at_ks(ideal_r, k_list, method)\n        ndcg_array = dcg_array / dcg_max_array\n        ndcg_array = np.nan_to_num(ndcg_array)\n    if include_dcg:\n        return (ndcg_array, dcg_array)\n    else:\n        return ndcg_array", "refactored": true, "pred": {"ppl": 2.837960958480835, "ppl_lower": 3.056994676589966, "ppl/lowercase_ppl": -1.0712755154158387, "ppl/zlib": 0.0031704736229228514, "Min_5.0% Prob": 9.073549087230976, "Min_10.0% Prob": 7.350318083396325, "Min_20.0% Prob": 4.869694379140746, "Min_30.0% Prob": 3.4645969488575488, "Min_40.0% Prob": 2.6112614594189063, "Min_50.0% Prob": 2.08547786858521, "Min_60.0% Prob": 1.7449217361065634}}
{"hexsha": "7d0de049fa37b3a2b53aa13e4a8de6bd43d360ad", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef update_progress(value):\n    global PROGRESS\n    PROGRESS['value'] = value\n    return PROGRESS['value']", "fn_id": 0, "class_fn": false, "repo": "scyv/Smiley", "file": "smiley/utils.py", "last_update_at": "2021-12-10T23:49:05+00:00", "original_content": "def update_progress(value):\n    global PROGRESS\n    PROGRESS['value'] = value\n    return PROGRESS['value']", "refactored": true, "pred": {"ppl": 13.468762397766113, "ppl_lower": 16.99858283996582, "ppl/lowercase_ppl": -1.0895090284321105, "ppl/zlib": 0.023856634016398096, "Min_5.0% Prob": 11.121382713317871, "Min_10.0% Prob": 9.889477729797363, "Min_20.0% Prob": 8.380441870008196, "Min_30.0% Prob": 7.541577625274658, "Min_40.0% Prob": 6.200461149215698, "Min_50.0% Prob": 5.066252870692147, "Min_60.0% Prob": 4.409989083097095}}
{"hexsha": "6494d7e235268c91bde539243623f44dd265dd50", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef create_fake_server(messages=[]):\n    all_messages = _standard_join_messages() + messages\n\n    async def _fake_server(websocket, _path):\n        _join_message = await websocket.recv()\n        for message in all_messages:\n            await websocket.send(json.dumps(message))\n    return _fake_server", "fn_id": 1, "class_fn": false, "repo": "UrbanOS-Examples/PredictiveParking", "file": "tests/fake_websocket_server.py", "last_update_at": "2021-07-01T16:51:18+00:00", "original_content": "def create_fake_server(messages=[]):\n    all_messages = _standard_join_messages() + messages\n\n    async def _fake_server(websocket, _path):\n        _join_message = await websocket.recv()\n        for message in all_messages:\n            await websocket.send(json.dumps(message))\n    return _fake_server", "refactored": true, "pred": {"ppl": 8.19005012512207, "ppl_lower": 8.19005012512207, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.011618342641600962, "Min_5.0% Prob": 10.3443124294281, "Min_10.0% Prob": 9.480624556541443, "Min_20.0% Prob": 8.070095807313919, "Min_30.0% Prob": 6.501396050453186, "Min_40.0% Prob": 5.243228628779903, "Min_50.0% Prob": 4.191783873807816, "Min_60.0% Prob": 3.5299081088602544}}
{"hexsha": "be45de239f2a958420b5ae5de4b313cf3431599f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_playlist_tracks(args):\n    playlists = []\n    current_user_id = args.get('current_user_id')\n    limit = args.get('limit')\n    offset = args.get('offset')\n    db = get_db_read_replica()\n    with db.scoped_session() as session:\n        try:\n            playlist_id = args.get('playlist_id')\n            playlist = session.query(Playlist).filter(Playlist.is_current == True, Playlist.playlist_id == playlist_id).first()\n            if playlist is None:\n                return None\n            playlist_track_ids = [track_id['track'] for track_id in playlist.playlist_contents['track_ids']]\n            if limit and offset:\n                playlist_track_ids = playlist_track_ids[offset:offset + limit]\n            playlist_tracks = session.query(Track).filter(Track.is_current == True, Track.track_id.in_(playlist_track_ids)).all()\n            tracks = helpers.query_result_to_list(playlist_tracks)\n            tracks = populate_track_metadata(session, playlist_track_ids, tracks, current_user_id)\n            if args.get('with_users', False):\n                add_users_to_tracks(session, tracks)\n            tracks_dict = {track['track_id']: track for track in tracks}\n            playlist_tracks = []\n            for track_id in playlist_track_ids:\n                playlist_tracks.append(tracks_dict[track_id])\n            return playlist_tracks\n        except sqlalchemy.orm.exc.NoResultFound:\n            pass\n    return playlists", "fn_id": 0, "class_fn": false, "repo": "raymondjacobson/audius-protocol", "file": "discovery-provider/src/queries/get_playlist_tracks.py", "last_update_at": "2021-05-29T04:25:03+00:00", "original_content": "def get_playlist_tracks(args):\n    playlists = []\n    current_user_id = args.get('current_user_id')\n    limit = args.get('limit')\n    offset = args.get('offset')\n    db = get_db_read_replica()\n    with db.scoped_session() as session:\n        try:\n            playlist_id = args.get('playlist_id')\n            playlist = session.query(Playlist).filter(Playlist.is_current == True, Playlist.playlist_id == playlist_id).first()\n            if playlist is None:\n                return None\n            playlist_track_ids = [track_id['track'] for track_id in playlist.playlist_contents['track_ids']]\n            if limit and offset:\n                playlist_track_ids = playlist_track_ids[offset:offset + limit]\n            playlist_tracks = session.query(Track).filter(Track.is_current == True, Track.track_id.in_(playlist_track_ids)).all()\n            tracks = helpers.query_result_to_list(playlist_tracks)\n            tracks = populate_track_metadata(session, playlist_track_ids, tracks, current_user_id)\n            if args.get('with_users', False):\n                add_users_to_tracks(session, tracks)\n            tracks_dict = {track['track_id']: track for track in tracks}\n            playlist_tracks = []\n            for track_id in playlist_track_ids:\n                playlist_tracks.append(tracks_dict[track_id])\n            return playlist_tracks\n        except sqlalchemy.orm.exc.NoResultFound:\n            pass\n    return playlists", "refactored": true, "pred": {"ppl": 2.5123353004455566, "ppl_lower": 2.9017138481140137, "ppl/lowercase_ppl": -1.1564121102165044, "ppl/zlib": 0.001820578496393466, "Min_5.0% Prob": 8.084344228108725, "Min_10.0% Prob": 6.139557898044586, "Min_20.0% Prob": 4.076555028353652, "Min_30.0% Prob": 2.9757665292783217, "Min_40.0% Prob": 2.2841979427909362, "Min_50.0% Prob": 1.8398104819833585, "Min_60.0% Prob": 1.5367084395485968}}
{"hexsha": "e739faa956bee4ffde5b12e2607b0430bc975de6", "ext": "py", "lang": "Python", "content": "@pytest.fixture\n@timeing\n@measure_memory_usage\ndef archive_repositories(archive_repositories_raw: bytes) -> ArchiveRepositories:\n    \"\"\"Provides an ArchiveRepositories instance for the sample archive manifest.\"\"\"\n    return ArchiveRepositories(archive_repositories_raw)", "fn_id": 0, "class_fn": false, "repo": "crashvb/docker-sign-verify", "file": "tests/test_archiverepositories.py", "last_update_at": "2021-09-16T18:44:39+00:00", "original_content": "@pytest.fixture\ndef archive_repositories(archive_repositories_raw: bytes) -> ArchiveRepositories:\n    \"\"\"Provides an ArchiveRepositories instance for the sample archive manifest.\"\"\"\n    return ArchiveRepositories(archive_repositories_raw)", "refactored": true, "pred": {"ppl": 18.12641143798828, "ppl_lower": 21.0189266204834, "ppl/lowercase_ppl": -1.0510991781282735, "ppl/zlib": 0.018692710129997226, "Min_5.0% Prob": 10.888442993164062, "Min_10.0% Prob": 10.56025447845459, "Min_20.0% Prob": 9.220313332297586, "Min_30.0% Prob": 7.995779573917389, "Min_40.0% Prob": 6.577573375268416, "Min_50.0% Prob": 5.511528768709728, "Min_60.0% Prob": 4.818894131617113}}
{"hexsha": "f2e5aa3454e20bde6dfc287c3f2e7fede42c0287", "ext": "py", "lang": "Python", "content": "@overload\n@timeing\n@measure_memory_usage\ndef assert_equal(actual: scipy.special.orthogonal.orthopoly1d, desired: numpy.poly1d):\n    \"\"\"\n    usage.scipy: 2\n    \"\"\"\n    ...", "fn_id": 2892, "class_fn": false, "repo": "pydata-apis/python-api-record", "file": "data/typing/numpy.testing._private.utils.py", "last_update_at": "2021-11-08T20:16:06+00:00", "original_content": "@overload\ndef assert_equal(actual: scipy.special.orthogonal.orthopoly1d, desired: numpy.poly1d):\n    \"\"\"\n    usage.scipy: 2\n    \"\"\"\n    ...", "refactored": true, "pred": {"ppl": 14.00704288482666, "ppl_lower": 14.00704288482666, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.020461707490878226, "Min_5.0% Prob": 13.391279220581055, "Min_10.0% Prob": 11.254099655151368, "Min_20.0% Prob": 9.508047008514405, "Min_30.0% Prob": 7.407247424125671, "Min_40.0% Prob": 6.247177067257109, "Min_50.0% Prob": 5.188962521376433, "Min_60.0% Prob": 4.435459286905825}}
{"hexsha": "bcc284a877dbebcbe1f579fbdd8c29590594f15e", "ext": "py", "lang": "Python", "content": "@unittest.skipIf(sys.platform == 'win32', 'Fail to create temp dir.')\n@pytest.mark.parametrize('client_mode', [True, False])\n@timeing\n@measure_memory_usage\ndef test_two_node(two_node_cluster, working_dir, client_mode):\n    cluster, _ = two_node_cluster\n    address, env, PKG_DIR = start_client_server(cluster, client_mode)\n    runtime_env = f'{{  \"working_dir\": \"{working_dir}\" }}'\n    execute_statement = 'print(sum(ray.get([run_test.remote()] * 1000)))'\n    script = driver_script.format(**locals())\n    out = run_string_as_driver(script, env)\n    assert out.strip().split()[-1] == '1000'\n    assert len(list(Path(PKG_DIR).iterdir())) == 1", "fn_id": 7, "class_fn": false, "repo": "jenhaoyang/ray", "file": "python/ray/tests/test_runtime_env.py", "last_update_at": "2021-04-30T05:22:12+00:00", "original_content": "@unittest.skipIf(sys.platform == 'win32', 'Fail to create temp dir.')\n@pytest.mark.parametrize('client_mode', [True, False])\ndef test_two_node(two_node_cluster, working_dir, client_mode):\n    cluster, _ = two_node_cluster\n    address, env, PKG_DIR = start_client_server(cluster, client_mode)\n    runtime_env = f'{{  \"working_dir\": \"{working_dir}\" }}'\n    execute_statement = 'print(sum(ray.get([run_test.remote()] * 1000)))'\n    script = driver_script.format(**locals())\n    out = run_string_as_driver(script, env)\n    assert out.strip().split()[-1] == '1000'\n    assert len(list(Path(PKG_DIR).iterdir())) == 1", "refactored": true, "pred": {"ppl": 6.0855536460876465, "ppl_lower": 6.666408538818359, "ppl/lowercase_ppl": -1.0504804603676274, "ppl/zlib": 0.004595210452134466, "Min_5.0% Prob": 10.954644584655762, "Min_10.0% Prob": 8.907908553168888, "Min_20.0% Prob": 6.783309760547819, "Min_30.0% Prob": 5.355498781427741, "Min_40.0% Prob": 4.357656975353465, "Min_50.0% Prob": 3.572233830099908, "Min_60.0% Prob": 3.01041933480883}}
{"hexsha": "a318f13cb13b18c46060393025e55c03c4986095", "ext": "py", "lang": "Python", "content": "@MSC.route('/candidates')\n@timeing\n@measure_memory_usage\ndef candidates():\n    text = request.args.get('text', default='', type=str)\n    limit = request.args.get('limit', default=5, type=int)\n    htmlflag = request.args.get('html', default=0, type=int)\n    rval = {}\n    rval['input'] = text\n    runningOffset = 0\n    if text == '':\n        return 'No text received. Usage: url/candidates?html=0&limit=2&text=texttomedicalspellcheck'\n    respJSONstring = corrector.GetALLCandidatesScoredJSON(text)\n    print(respJSONstring)\n    rval = json.loads(respJSONstring)\n    for result in rval['results']:\n        result['candidates'] = result['candidates'][:limit]\n    if 'results' not in rval.keys() or len(rval['results']) == 0:\n        rval['results'] = 'CORRECT'\n    if bool(htmlflag):\n        return json2html.convert(json.dumps(rval)) + '<br><br><br>Try me out: <br><br>' + formcode\n    else:\n        return json.dumps(rval, indent=2)", "fn_id": 2, "class_fn": false, "repo": "jackneil/medSpellCheck", "file": "webserver.py", "last_update_at": "2021-07-07T02:07:24+00:00", "original_content": "@MSC.route('/candidates')\ndef candidates():\n    text = request.args.get('text', default='', type=str)\n    limit = request.args.get('limit', default=5, type=int)\n    htmlflag = request.args.get('html', default=0, type=int)\n    rval = {}\n    rval['input'] = text\n    runningOffset = 0\n    if text == '':\n        return 'No text received. Usage: url/candidates?html=0&limit=2&text=texttomedicalspellcheck'\n    respJSONstring = corrector.GetALLCandidatesScoredJSON(text)\n    print(respJSONstring)\n    rval = json.loads(respJSONstring)\n    for result in rval['results']:\n        result['candidates'] = result['candidates'][:limit]\n    if 'results' not in rval.keys() or len(rval['results']) == 0:\n        rval['results'] = 'CORRECT'\n    if bool(htmlflag):\n        return json2html.convert(json.dumps(rval)) + '<br><br><br>Try me out: <br><br>' + formcode\n    else:\n        return json.dumps(rval, indent=2)", "refactored": true, "pred": {"ppl": 6.802599906921387, "ppl_lower": 6.636188507080078, "ppl/lowercase_ppl": -0.9870823362606692, "ppl/zlib": 0.004204615961320568, "Min_5.0% Prob": 12.760649901169996, "Min_10.0% Prob": 10.461324055989584, "Min_20.0% Prob": 7.624669088016857, "Min_30.0% Prob": 5.8914966482714, "Min_40.0% Prob": 4.713564121181315, "Min_50.0% Prob": 3.8221426394538605, "Min_60.0% Prob": 3.193268164835242}}
{"hexsha": "c908d4e2fcbe9433c55ec3ffa39b22b2f66872dd", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef calc_data_maxima(data, order):\n    \"\"\"Calculate the local maxima using an exclusion of order to avoid multiple close maxima.\n\n    :param data: 3D numpy array (axis 0: time)\n    :param order: number of points to consider for maxima\n    :return: 3D numpy bool array with same shape as data, True if maximum\n    \"\"\"\n    argrelmax = signal.argrelmax(data, order=order, mode='wrap')\n    dmean = data.mean(axis=0)\n    data_maxima = np.zeros(data.shape, dtype=bool)\n    for i, j, k in zip(*argrelmax):\n        if data[i, j, k] > dmean[j, k]:\n            data_maxima[i, j, k] = True\n    return data_maxima", "fn_id": 0, "class_fn": false, "repo": "markmuetz/cosmic", "file": "cosmic/WP2/multipeak.py", "last_update_at": "2021-01-26T02:25:48+00:00", "original_content": "def calc_data_maxima(data, order):\n    \"\"\"Calculate the local maxima using an exclusion of order to avoid multiple close maxima.\n\n    :param data: 3D numpy array (axis 0: time)\n    :param order: number of points to consider for maxima\n    :return: 3D numpy bool array with same shape as data, True if maximum\n    \"\"\"\n    argrelmax = signal.argrelmax(data, order=order, mode='wrap')\n    dmean = data.mean(axis=0)\n    data_maxima = np.zeros(data.shape, dtype=bool)\n    for i, j, k in zip(*argrelmax):\n        if data[i, j, k] > dmean[j, k]:\n            data_maxima[i, j, k] = True\n    return data_maxima", "refactored": true, "pred": {"ppl": 4.706273078918457, "ppl_lower": 5.17380952835083, "ppl/lowercase_ppl": -1.061148672387115, "ppl/zlib": 0.0045289365977809895, "Min_5.0% Prob": 10.374799728393555, "Min_10.0% Prob": 8.616565804732474, "Min_20.0% Prob": 6.4537042899009505, "Min_30.0% Prob": 4.875641216491831, "Min_40.0% Prob": 3.7924946332589173, "Min_50.0% Prob": 3.073563646905276, "Min_60.0% Prob": 2.5888742537388945}}
{"hexsha": "6fb0abbb3b81f5bf6e666d791414fa75ce85b499", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _read_checkpoint_file(checkpoint_file: Optional[str]) -> Tuple[Set[str], Set[str]]:\n    colls, users = (set(), set())\n    if checkpoint_file:\n        with open(checkpoint_file) as fd:\n            for line in fd.readlines():\n                if line.startswith('CHECKPOINT'):\n                    _, type_, oid = line.split()\n                    if type_ == 'collection':\n                        colls.add(oid)\n                    elif type_ == 'user':\n                        users.add(oid)\n    return (colls, users)", "fn_id": 6, "class_fn": false, "repo": "girder/dkc-next", "file": "dkc/core/management/commands/migrate_dkc_db.py", "last_update_at": "2021-02-17T20:34:11+00:00", "original_content": "def _read_checkpoint_file(checkpoint_file: Optional[str]) -> Tuple[Set[str], Set[str]]:\n    colls, users = (set(), set())\n    if checkpoint_file:\n        with open(checkpoint_file) as fd:\n            for line in fd.readlines():\n                if line.startswith('CHECKPOINT'):\n                    _, type_, oid = line.split()\n                    if type_ == 'collection':\n                        colls.add(oid)\n                    elif type_ == 'user':\n                        users.add(oid)\n    return (colls, users)", "refactored": true, "pred": {"ppl": 4.092894077301025, "ppl_lower": 4.364257335662842, "ppl/lowercase_ppl": -1.0455530319579815, "ppl/zlib": 0.0053583738334111505, "Min_5.0% Prob": 10.191176573435465, "Min_10.0% Prob": 8.572192375476543, "Min_20.0% Prob": 6.206787907160246, "Min_30.0% Prob": 4.6251640151708555, "Min_40.0% Prob": 3.534988066324821, "Min_50.0% Prob": 2.837004212175424, "Min_60.0% Prob": 2.366355296445246}}
{"hexsha": "567e27ecd176008a4a9c4c66d72d8200e72ba772", "ext": "py", "lang": "Python", "content": "@pytest.fixture(scope='session', autouse=True)\n@timeing\n@measure_memory_usage\ndef cleanup(request):\n\n    def remove_test_dir():\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.scistree.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.scistree.log'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.scite.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.scite.log'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.huntress.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.huntress.log'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.phiscsb.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.phiscsb.log'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.booster.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.booster.log'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/consensus.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.phiscsb.info2'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.phiscsb.newick'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.phiscsb.png'))\n        tsc.ul.cleanup(tsc.ul.get_file('trisicell.datasets/test/_map'))\n        tsc.ul.cleanup(tsc.ul.get_file('trisicell.datasets/test/_tmp'))\n        tsc.ul.cleanup(tsc.ul.get_file('trisicell.datasets/test/test'))\n    request.addfinalizer(remove_test_dir)", "fn_id": 0, "class_fn": false, "repo": "faridrashidi/trisicell", "file": "tests/test_commands.py", "last_update_at": "2021-11-16T03:14:36+00:00", "original_content": "@pytest.fixture(scope='session', autouse=True)\ndef cleanup(request):\n\n    def remove_test_dir():\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.scistree.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.scistree.log'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.scite.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.scite.log'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.huntress.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.huntress.log'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.phiscsb.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.phiscsb.log'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.booster.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.booster.log'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/consensus.CFMatrix'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.phiscsb.info2'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.phiscsb.newick'))\n        tsc.ul.remove(tsc.ul.get_file('trisicell.datasets/test/test.phiscsb.png'))\n        tsc.ul.cleanup(tsc.ul.get_file('trisicell.datasets/test/_map'))\n        tsc.ul.cleanup(tsc.ul.get_file('trisicell.datasets/test/_tmp'))\n        tsc.ul.cleanup(tsc.ul.get_file('trisicell.datasets/test/test'))\n    request.addfinalizer(remove_test_dir)", "refactored": true, "pred": {"ppl": 2.1467933654785156, "ppl_lower": 2.168755292892456, "ppl/lowercase_ppl": -1.013322596597708, "ppl/zlib": 0.0027382626196849995, "Min_5.0% Prob": 9.80751318767153, "Min_10.0% Prob": 6.838665547042058, "Min_20.0% Prob": 3.8018090247080245, "Min_30.0% Prob": 2.551547915337543, "Min_40.0% Prob": 1.915728380132852, "Min_50.0% Prob": 1.5278017252881266, "Min_60.0% Prob": 1.2739983533649926}}
{"hexsha": "454c918ee83d8f3c85828d2e941785840208d81e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef text_error(code: int, error: Exception, url: str, usage: str='') -> Response:\n    \"\"\"Format error message as plain text\n\n    Returns\n    -------\n    error message formatted as plain text.\n    \"\"\"\n    return PlainTextResponse(content=f'Error {code}: {ERROR_CODE_MESSAGES[code]}\\n\\n{error}\\n\\nUsage details are available from {usage}\\n\\nRequest:\\n{url}\\n\\nRequest Submitted:\\n{UTCDateTime().isoformat()}Z\\n\\nService Version:\\n{VERSION}\\n', status_code=code)", "fn_id": 5, "class_fn": false, "repo": "alejandrodelcampillo/geomag-algorithms", "file": "geomagio/api/ws/app.py", "last_update_at": "2021-02-22T23:45:22+00:00", "original_content": "def text_error(code: int, error: Exception, url: str, usage: str='') -> Response:\n    \"\"\"Format error message as plain text\n\n    Returns\n    -------\n    error message formatted as plain text.\n    \"\"\"\n    return PlainTextResponse(content=f'Error {code}: {ERROR_CODE_MESSAGES[code]}\\n\\n{error}\\n\\nUsage details are available from {usage}\\n\\nRequest:\\n{url}\\n\\nRequest Submitted:\\n{UTCDateTime().isoformat()}Z\\n\\nService Version:\\n{VERSION}\\n', status_code=code)", "refactored": true, "pred": {"ppl": 9.827162742614746, "ppl_lower": 13.766005516052246, "ppl/lowercase_ppl": -1.147496613492089, "ppl/zlib": 0.007277548598744684, "Min_5.0% Prob": 11.340308734348842, "Min_10.0% Prob": 9.811060905456543, "Min_20.0% Prob": 7.785587889807565, "Min_30.0% Prob": 6.434064757256281, "Min_40.0% Prob": 5.352940936173711, "Min_50.0% Prob": 4.496762354033334, "Min_60.0% Prob": 3.816399216297127}}
{"hexsha": "8c985af6b5bde8b51d64a34a81f930b1cbbde109", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef decode_UHFRFModeTable(data):\n    logger.debug(func())\n    par = {}\n    if len(data) == 0:\n        return (None, data)\n    header = data[0:par_header_len]\n    msgtype, length = struct.unpack(par_header, header)\n    msgtype = msgtype & BITMASK(10)\n    logger.debug('%s (type=%d len=%d)' % (func(), msgtype, length))\n    if msgtype != Message_struct['UHFRFModeTable']['type']:\n        return (None, data)\n    body = data[par_header_len:length]\n    logger.debug('%s (type=%d len=%d)' % (func(), msgtype, length))\n    i = 0\n    ret, body = decode('UHFC1G2RFModeTableEntry')(body)\n    while ret:\n        par['UHFC1G2RFModeTableEntry' + str(i)] = ret\n        ret, body = decode('UHFC1G2RFModeTableEntry')(body)\n        i += 1\n    return (par, data[length:])", "fn_id": 18, "class_fn": false, "repo": "amjadmajid/stork", "file": "Host_software/sllurp/llrp_proto.py", "last_update_at": "2021-11-21T08:23:03+00:00", "original_content": "def decode_UHFRFModeTable(data):\n    logger.debug(func())\n    par = {}\n    if len(data) == 0:\n        return (None, data)\n    header = data[0:par_header_len]\n    msgtype, length = struct.unpack(par_header, header)\n    msgtype = msgtype & BITMASK(10)\n    logger.debug('%s (type=%d len=%d)' % (func(), msgtype, length))\n    if msgtype != Message_struct['UHFRFModeTable']['type']:\n        return (None, data)\n    body = data[par_header_len:length]\n    logger.debug('%s (type=%d len=%d)' % (func(), msgtype, length))\n    i = 0\n    ret, body = decode('UHFC1G2RFModeTableEntry')(body)\n    while ret:\n        par['UHFC1G2RFModeTableEntry' + str(i)] = ret\n        ret, body = decode('UHFC1G2RFModeTableEntry')(body)\n        i += 1\n    return (par, data[length:])", "refactored": true, "pred": {"ppl": 5.486965656280518, "ppl_lower": 5.6754302978515625, "ppl/lowercase_ppl": -1.0198375663637067, "ppl/zlib": 0.00493442144623294, "Min_5.0% Prob": 11.29721196492513, "Min_10.0% Prob": 9.148917484283448, "Min_20.0% Prob": 6.882326688766479, "Min_30.0% Prob": 5.2901024309794105, "Min_40.0% Prob": 4.1901545804739, "Min_50.0% Prob": 3.388118680034365, "Min_60.0% Prob": 2.837510577914928}}
{"hexsha": "b0b49ffed87028f7676ecd3a3526e65ea082c9a2", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef obtain_pcaplot(model):\n    for idx, layer in enumerate(model.layers):\n        if layer.__class__.__name__ == 'Dense':\n            all_weights = layer.get_weights()\n            weights = all_weights[0]\n            pca = applyPCA(weights, np.array(weights).shape[1], drawPlot=True, isReshape=False)\n            break", "fn_id": 10, "class_fn": false, "repo": "kilinco/spec-img-finesse", "file": "CNN_version/final.py", "last_update_at": "2021-06-03T16:52:44+00:00", "original_content": "def obtain_pcaplot(model):\n    for idx, layer in enumerate(model.layers):\n        if layer.__class__.__name__ == 'Dense':\n            all_weights = layer.get_weights()\n            weights = all_weights[0]\n            pca = applyPCA(weights, np.array(weights).shape[1], drawPlot=True, isReshape=False)\n            break", "refactored": true, "pred": {"ppl": 9.47366714477539, "ppl_lower": 12.823272705078125, "ppl/lowercase_ppl": -1.1346424131798254, "ppl/zlib": 0.010458214280638733, "Min_5.0% Prob": 12.299203872680664, "Min_10.0% Prob": 10.598219182756212, "Min_20.0% Prob": 8.500244564480251, "Min_30.0% Prob": 6.731518634728023, "Min_40.0% Prob": 5.456281262475091, "Min_50.0% Prob": 4.42165274632738, "Min_60.0% Prob": 3.745378271277462}}
{"hexsha": "af3e93794d127b40b4269b409917fdaa6143f215", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef parse_function_args(query_param_definition, is_strict=False):\n\n    def inner_get_fu(fu):\n        return FuncArgParser(query_param_definition, is_strict=is_strict)(fu)\n    return inner_get_fu", "fn_id": 0, "class_fn": false, "repo": "sabariramc/funcargparser", "file": "testimplementation.py", "last_update_at": "2021-05-05T09:56:14+00:00", "original_content": "def parse_function_args(query_param_definition, is_strict=False):\n\n    def inner_get_fu(fu):\n        return FuncArgParser(query_param_definition, is_strict=is_strict)(fu)\n    return inner_get_fu", "refactored": true, "pred": {"ppl": 13.029280662536621, "ppl_lower": 15.545875549316406, "ppl/lowercase_ppl": -1.0687894352885026, "ppl/zlib": 0.018469058873099617, "Min_5.0% Prob": 10.859239260355631, "Min_10.0% Prob": 9.489091475804647, "Min_20.0% Prob": 8.064039890582745, "Min_30.0% Prob": 6.952065908908844, "Min_40.0% Prob": 5.858864704767863, "Min_50.0% Prob": 4.961947497199564, "Min_60.0% Prob": 4.324773359298706}}
{"hexsha": "80c466f827f74902bd68bfb5f88abcaf94f6d86e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_handle_xevent_atom_ok(monkeypatch):\n    \"\"\"Return false when event type is invalid\"\"\"\n    event = MagicMock(spec=Xlib.protocol.event.PropertyNotify)\n    event.type = xwindow.Xlib.X.PropertyNotify\n    event.atom = xwindow.NET_ACTIVE_WINDOW\n    monkeypatch.setattr(xwindow, 'ROOT', MagicMock())\n    monkeypatch.setattr(xwindow, 'DISP', MagicMock())\n    result = xwindow.handle_xevent(event, callback=lambda *args, **kwargs: 'callback')\n    assert result is True", "fn_id": 6, "class_fn": false, "repo": "eddie-dunn/swytcher", "file": "tests/test_xwindow.py", "last_update_at": "2021-07-07T08:52:49+00:00", "original_content": "def test_handle_xevent_atom_ok(monkeypatch):\n    \"\"\"Return false when event type is invalid\"\"\"\n    event = MagicMock(spec=Xlib.protocol.event.PropertyNotify)\n    event.type = xwindow.Xlib.X.PropertyNotify\n    event.atom = xwindow.NET_ACTIVE_WINDOW\n    monkeypatch.setattr(xwindow, 'ROOT', MagicMock())\n    monkeypatch.setattr(xwindow, 'DISP', MagicMock())\n    result = xwindow.handle_xevent(event, callback=lambda *args, **kwargs: 'callback')\n    assert result is True", "refactored": true, "pred": {"ppl": 6.481078624725342, "ppl_lower": 12.38861083984375, "ppl/lowercase_ppl": -1.346671915397915, "ppl/zlib": 0.006489190801892724, "Min_5.0% Prob": 10.430761745997838, "Min_10.0% Prob": 9.175618001392909, "Min_20.0% Prob": 7.2163505128451755, "Min_30.0% Prob": 5.63376754238492, "Min_40.0% Prob": 4.507932715117931, "Min_50.0% Prob": 3.7085493585893086, "Min_60.0% Prob": 3.1217826651852754}}
{"hexsha": "a6247751a993a8d45edcb88a38537df9fdda2f2e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef decode_vector_type(contract_address, abi):\n    wasm_contract_obj = platon.wasmcontract(address=contract_address, abi=abi, vmtype=1)\n    tx_hash = wasm_contract_obj.functions.clearElement().transact({'from': from_address, 'gas': gas})\n    tx_receipt = platon.waitForTransactionReceipt(tx_hash)\n    topic_param = wasm_contract_obj.events.clear().processReceipt(tx_receipt)\n    print(topic_param)", "fn_id": 4, "class_fn": false, "repo": "AlayaNetwork/client-sdk-python", "file": "tests/wasm_unit_test.py", "last_update_at": "2021-05-06T07:41:02+00:00", "original_content": "def decode_vector_type(contract_address, abi):\n    wasm_contract_obj = platon.wasmcontract(address=contract_address, abi=abi, vmtype=1)\n    tx_hash = wasm_contract_obj.functions.clearElement().transact({'from': from_address, 'gas': gas})\n    tx_receipt = platon.waitForTransactionReceipt(tx_hash)\n    topic_param = wasm_contract_obj.events.clear().processReceipt(tx_receipt)\n    print(topic_param)", "refactored": true, "pred": {"ppl": 8.23766803741455, "ppl_lower": 9.724631309509277, "ppl/lowercase_ppl": -1.0786946067023135, "ppl/zlib": 0.008935242791100523, "Min_5.0% Prob": 12.717865784962973, "Min_10.0% Prob": 10.590596516927084, "Min_20.0% Prob": 7.883662385940552, "Min_30.0% Prob": 6.359219203124175, "Min_40.0% Prob": 5.09404798746109, "Min_50.0% Prob": 4.214369543137089, "Min_60.0% Prob": 3.506817692865928}}
{"hexsha": "3931e032c422dda461181a2d4fc71a72aab1d761", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef main():\n    \"\"\"Main function.\"\"\"\n    try:\n        tickers = os.listdir('data')\n        tickers = (os.path.basename(t) for t in tickers if t.endswith('.csv.gz'))\n        tickers = [t.split('.')[0] for t in tickers]\n        epilog = '\\nValid values for ticker are: %s' % tickers\n        epilog += '\\n\\nSample usage: ./predict.py IBM 2010-01-01 2011-01-01 21'\n    except OSError:\n        tickers = []\n        epilog = ''\n    parser = argparse.ArgumentParser(description='Predicts stock prices.', epilog=epilog, add_help=True)\n    parser.add_argument('ticker', metavar='TICKER', help='The stock item to predict')\n    parser.add_argument('start_date', metavar='START_DATE', help='The initial date to start looking into history.')\n    parser.add_argument('end_date', metavar='END_DATE', help='The final date to stop looking into history.')\n    parser.add_argument('shift', metavar='SHIFT', type=int, help='How many days in advance to predict.')\n    options = parser.parse_args()\n    if not tickers:\n        print('\"No tickers available. Unable to predict.')\n        raise SystemExit\n    ticker = options.ticker\n    if ticker not in tickers:\n        print('\"ticker\" must be one of %s' % tickers)\n        raise SystemExit\n    try:\n        start_date = pd.to_datetime(options.start_date)\n    except ValueError:\n        print('\"start_date\" must be a valid date. Not %s' % start_date)\n        raise SystemExit\n    try:\n        end_date = pd.to_datetime(options.end_date)\n    except ValueError:\n        print('\"end_date\" must be a valid date. Not %s' % end_date)\n        raise SystemExit\n    shift = options.shift\n    if shift <= 0:\n        print('\"shift\" must be a positive integer')\n        raise SystemExit\n    print('Loading data...')\n    X, y, tX, _, scaler = models.get_processed_dataset(ticker, 0.9999999999, shift, 0, False, start_date, end_date)\n    print('Training model...')\n    _, _, model = models.cross_validate_model('huber', X, y)\n    print('Predicting...')\n    yhat = model.predict(tX)\n    prediction = scaler.inverse_transform(np.array([[yhat[0]] + [0] * (X.shape[1] - 1)]))\n    print('Predicted value:', prediction[0, 0])", "fn_id": 0, "class_fn": false, "repo": "renatolfc/stock-prediction", "file": "predict.py", "last_update_at": "2021-07-01T12:17:06+00:00", "original_content": "def main():\n    \"\"\"Main function.\"\"\"\n    try:\n        tickers = os.listdir('data')\n        tickers = (os.path.basename(t) for t in tickers if t.endswith('.csv.gz'))\n        tickers = [t.split('.')[0] for t in tickers]\n        epilog = '\\nValid values for ticker are: %s' % tickers\n        epilog += '\\n\\nSample usage: ./predict.py IBM 2010-01-01 2011-01-01 21'\n    except OSError:\n        tickers = []\n        epilog = ''\n    parser = argparse.ArgumentParser(description='Predicts stock prices.', epilog=epilog, add_help=True)\n    parser.add_argument('ticker', metavar='TICKER', help='The stock item to predict')\n    parser.add_argument('start_date', metavar='START_DATE', help='The initial date to start looking into history.')\n    parser.add_argument('end_date', metavar='END_DATE', help='The final date to stop looking into history.')\n    parser.add_argument('shift', metavar='SHIFT', type=int, help='How many days in advance to predict.')\n    options = parser.parse_args()\n    if not tickers:\n        print('\"No tickers available. Unable to predict.')\n        raise SystemExit\n    ticker = options.ticker\n    if ticker not in tickers:\n        print('\"ticker\" must be one of %s' % tickers)\n        raise SystemExit\n    try:\n        start_date = pd.to_datetime(options.start_date)\n    except ValueError:\n        print('\"start_date\" must be a valid date. Not %s' % start_date)\n        raise SystemExit\n    try:\n        end_date = pd.to_datetime(options.end_date)\n    except ValueError:\n        print('\"end_date\" must be a valid date. Not %s' % end_date)\n        raise SystemExit\n    shift = options.shift\n    if shift <= 0:\n        print('\"shift\" must be a positive integer')\n        raise SystemExit\n    print('Loading data...')\n    X, y, tX, _, scaler = models.get_processed_dataset(ticker, 0.9999999999, shift, 0, False, start_date, end_date)\n    print('Training model...')\n    _, _, model = models.cross_validate_model('huber', X, y)\n    print('Predicting...')\n    yhat = model.predict(tX)\n    prediction = scaler.inverse_transform(np.array([[yhat[0]] + [0] * (X.shape[1] - 1)]))\n    print('Predicted value:', prediction[0, 0])", "refactored": true, "pred": {"ppl": 3.1914165019989014, "ppl_lower": 3.4929039478302, "ppl/lowercase_ppl": -1.0777865897693644, "ppl/zlib": 0.0013415778760137865, "Min_5.0% Prob": 9.13815515272079, "Min_10.0% Prob": 7.30436848825024, "Min_20.0% Prob": 5.1019950828552245, "Min_30.0% Prob": 3.760885591836686, "Min_40.0% Prob": 2.8836285534251735, "Min_50.0% Prob": 2.3182489241704722, "Min_60.0% Prob": 1.9378454005618164}}
{"hexsha": "73ef8d1f69375bfcaedacdc382f9d2e3fbedc2b8", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef sig_gen_multi(public_as, private_as, public_ams, private_ams, body, amsh, arsh, fold=False, verbose=False, as_tmp=None, ams_tmp=None):\n    hasher = HASH_ALGORITHMS[b'rsa-sha256']\n    h = hasher()\n    h.update(body)\n    bh = base64.b64encode(h.digest())\n    print('ams bh= ')\n    print(bh)\n    hasher = HASH_ALGORITHMS[b'rsa-sha256']\n    h = hasher()\n    h = HashThrough(hasher())\n    h.update(b'\\r\\n'.join([x + b':' + y for x, y in amsh(bh)]))\n    if verbose:\n        print('\\nsign ams hashed: %r' % h.hashed())\n    pk = parse_pem_private_key(private_ams)\n    sig2 = RSASSA_PKCS1_v1_5_sign(h, pk)\n    msb = base64.b64encode(bytes(sig2))\n    if fold:\n        msb = msb[:70] + b' ' + msb[70:142] + b' ' + msb[142:214]\n    print('ams b= ')\n    print(msb)\n    pk_ams = parse_public_key(base64.b64decode(public_ams))\n    signature = base64.b64decode(msb)\n    ams_valid = RSASSA_PKCS1_v1_5_verify(h, signature, pk_ams)\n    print('ams sig valid: %r' % ams_valid)\n    hasher = HASH_ALGORITHMS[b'rsa-sha256']\n    h = hasher()\n    h = HashThrough(hasher())\n    h.update(b'\\r\\n'.join([x + b':' + y for x, y in arsh(msb, bh)]))\n    if verbose:\n        print('\\nsign ars hashed: %r' % h.hashed())\n    pk = parse_pem_private_key(private_as)\n    sig2 = RSASSA_PKCS1_v1_5_sign(h, pk)\n    sb = base64.b64encode(bytes(sig2))\n    print('arsh b=')\n    print(sb)\n    pk_as = parse_public_key(base64.b64decode(public_as))\n    signature = base64.b64decode(sb)\n    ams_valid = RSASSA_PKCS1_v1_5_verify(h, signature, pk_as)\n    print('arsh sig valid: %r' % ams_valid)\n    spc = fold and b'' or b'  '\n    accum = ''\n    if as_tmp:\n        sb = sb[:70] + b'\\n    ' + spc + sb[70:142] + b'\\n    ' + spc + sb[142:214]\n        res = as_tmp.replace(b'%b', sb)\n        accum = res\n        print(res.decode('utf-8'))\n    if ams_tmp:\n        msb = msb.replace(b' ', b'')\n        msb = msb[:70] + b'\\n    ' + spc + msb[70:142] + b'\\n    ' + spc + msb[142:214]\n        res = ams_tmp.replace(b'%bh', bh)\n        res = res.replace(b'%b', msb)\n        accum += b'\\n' + res\n        print(res.decode('utf-8'))\n    os.system(b'echo \"' + accum + b'\" | pbcopy')", "fn_id": 1, "class_fn": false, "repo": "ValiMail/arc_test_suite", "file": "sig_gen/sig_gen.py", "last_update_at": "2021-02-25T00:59:54+00:00", "original_content": "def sig_gen_multi(public_as, private_as, public_ams, private_ams, body, amsh, arsh, fold=False, verbose=False, as_tmp=None, ams_tmp=None):\n    hasher = HASH_ALGORITHMS[b'rsa-sha256']\n    h = hasher()\n    h.update(body)\n    bh = base64.b64encode(h.digest())\n    print('ams bh= ')\n    print(bh)\n    hasher = HASH_ALGORITHMS[b'rsa-sha256']\n    h = hasher()\n    h = HashThrough(hasher())\n    h.update(b'\\r\\n'.join([x + b':' + y for x, y in amsh(bh)]))\n    if verbose:\n        print('\\nsign ams hashed: %r' % h.hashed())\n    pk = parse_pem_private_key(private_ams)\n    sig2 = RSASSA_PKCS1_v1_5_sign(h, pk)\n    msb = base64.b64encode(bytes(sig2))\n    if fold:\n        msb = msb[:70] + b' ' + msb[70:142] + b' ' + msb[142:214]\n    print('ams b= ')\n    print(msb)\n    pk_ams = parse_public_key(base64.b64decode(public_ams))\n    signature = base64.b64decode(msb)\n    ams_valid = RSASSA_PKCS1_v1_5_verify(h, signature, pk_ams)\n    print('ams sig valid: %r' % ams_valid)\n    hasher = HASH_ALGORITHMS[b'rsa-sha256']\n    h = hasher()\n    h = HashThrough(hasher())\n    h.update(b'\\r\\n'.join([x + b':' + y for x, y in arsh(msb, bh)]))\n    if verbose:\n        print('\\nsign ars hashed: %r' % h.hashed())\n    pk = parse_pem_private_key(private_as)\n    sig2 = RSASSA_PKCS1_v1_5_sign(h, pk)\n    sb = base64.b64encode(bytes(sig2))\n    print('arsh b=')\n    print(sb)\n    pk_as = parse_public_key(base64.b64decode(public_as))\n    signature = base64.b64decode(sb)\n    ams_valid = RSASSA_PKCS1_v1_5_verify(h, signature, pk_as)\n    print('arsh sig valid: %r' % ams_valid)\n    spc = fold and b'' or b'  '\n    accum = ''\n    if as_tmp:\n        sb = sb[:70] + b'\\n    ' + spc + sb[70:142] + b'\\n    ' + spc + sb[142:214]\n        res = as_tmp.replace(b'%b', sb)\n        accum = res\n        print(res.decode('utf-8'))\n    if ams_tmp:\n        msb = msb.replace(b' ', b'')\n        msb = msb[:70] + b'\\n    ' + spc + msb[70:142] + b'\\n    ' + spc + msb[142:214]\n        res = ams_tmp.replace(b'%bh', bh)\n        res = res.replace(b'%b', msb)\n        accum += b'\\n' + res\n        print(res.decode('utf-8'))\n    os.system(b'echo \"' + accum + b'\" | pbcopy')", "refactored": true, "pred": {"ppl": 3.0709824562072754, "ppl_lower": 3.172584295272827, "ppl/lowercase_ppl": -1.029009834087604, "ppl/zlib": 0.0016331841757099381, "Min_5.0% Prob": 10.184242758640023, "Min_10.0% Prob": 8.091171642829632, "Min_20.0% Prob": 5.2674580580847605, "Min_30.0% Prob": 3.70676298785119, "Min_40.0% Prob": 2.800247605518568, "Min_50.0% Prob": 2.242945781028522, "Min_60.0% Prob": 1.8726548397010139}}
{"hexsha": "8e27225db54d8f1ce0cedbcc4f4b9dfe2b6fdaf9", "ext": "py", "lang": "Python", "content": "def draw_iterations(g, eval_map_function, x0, n):\n\n    @timeing\n    @measure_memory_usage\n    def draw_mid_point_arrows(x, x_next, both=True):\n        if x_next > x:\n            arrow_direction_angles = (0.5 * math.pi, 0)\n        else:\n            arrow_direction_angles = (-0.5 * math.pi, math.pi)\n        g.draw_arrow_tip((x, mid_value), arrow_direction_angles[0])\n        if both:\n            g.draw_arrow_tip((mid_value, x_next), arrow_direction_angles[1])\n    xs = [x0]\n    for i in range(n):\n        xs.append(eval_map_function(xs[-1]))\n    g.set_dash_dash_structure(12, 4, units='svg')\n    g.set_dash_mode('dash')\n    g.set_svg_options(stroke_width=iterations_stroke_w, units='svg')\n    g.set_arrow_options(width=iteration_arrow_rel_size * arrow_size, curvature=0, units='svg')\n    g.set_point_size(0.01)\n    g.draw_line_segment((x0, 0), (x0, x0))\n    for i, x in enumerate(xs[:-2]):\n        x_next = xs[i + 1]\n        g.draw_polyline([(x, x), (x, x_next), (x_next, x_next)])\n        mid_value = 0.5 * (x + x_next)\n        draw_mid_point_arrows(x, x_next)\n    g.draw_polyline([(xs[-2], xs[-2]), (xs[-2], xs[-1])])\n    g.draw_point((xs[-2], xs[-1]))\n    draw_mid_point_arrows(xs[-2], xs[-1], both=False)\n    g.reset_dash_and_dot_structures()", "fn_id": 2, "class_fn": false, "repo": "alexn11/mathsvg", "file": "more-examples/iteration-graph.py", "last_update_at": "2021-11-27T08:46:20+00:00", "original_content": "def draw_iterations(g, eval_map_function, x0, n):\n\n    def draw_mid_point_arrows(x, x_next, both=True):\n        if x_next > x:\n            arrow_direction_angles = (0.5 * math.pi, 0)\n        else:\n            arrow_direction_angles = (-0.5 * math.pi, math.pi)\n        g.draw_arrow_tip((x, mid_value), arrow_direction_angles[0])\n        if both:\n            g.draw_arrow_tip((mid_value, x_next), arrow_direction_angles[1])\n    xs = [x0]\n    for i in range(n):\n        xs.append(eval_map_function(xs[-1]))\n    g.set_dash_dash_structure(12, 4, units='svg')\n    g.set_dash_mode('dash')\n    g.set_svg_options(stroke_width=iterations_stroke_w, units='svg')\n    g.set_arrow_options(width=iteration_arrow_rel_size * arrow_size, curvature=0, units='svg')\n    g.set_point_size(0.01)\n    g.draw_line_segment((x0, 0), (x0, x0))\n    for i, x in enumerate(xs[:-2]):\n        x_next = xs[i + 1]\n        g.draw_polyline([(x, x), (x, x_next), (x_next, x_next)])\n        mid_value = 0.5 * (x + x_next)\n        draw_mid_point_arrows(x, x_next)\n    g.draw_polyline([(xs[-2], xs[-2]), (xs[-2], xs[-1])])\n    g.draw_point((xs[-2], xs[-1]))\n    draw_mid_point_arrows(xs[-2], xs[-1], both=False)\n    g.reset_dash_and_dot_structures()", "refactored": true, "pred": {"ppl": 4.0490546226501465, "ppl_lower": 4.144679546356201, "ppl/lowercase_ppl": -1.016690971724537, "ppl/zlib": 0.002813849954454937, "Min_5.0% Prob": 10.128618654997453, "Min_10.0% Prob": 8.261528688928355, "Min_20.0% Prob": 5.845365057820859, "Min_30.0% Prob": 4.417311643344768, "Min_40.0% Prob": 3.4447287925191827, "Min_50.0% Prob": 2.793506519973794, "Min_60.0% Prob": 2.3358096378697386}}
{"hexsha": "47fe8ba25f624d3f52e3accc1fc3872135959f05", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef main(args):\n    \"\"\"\n    The starting point of the program.\n    \"\"\"\n    while True:\n        num_items = 1\n        read_data = ReadData() if len(args) == 1 else ReadData(args)\n        data_log_df = get_file_df(read_data.get_data_log())\n        exe_table_df = get_file_df(read_data.get_exe_table())\n        doses_delta_df = get_file_df(read_data.get_doses_delta())\n        summary = get_summary(read_data.get_summary())\n        dfs = [('data_log', data_log_df[-num_items:]), ('exe_table', exe_table_df[-num_items:]), ('doses_delta', doses_delta_df[-num_items:])]\n        data_json = get_json(dfs, summary, read_data.json_name)\n        read_data.json = data_json\n        time.sleep(5)", "fn_id": 3, "class_fn": false, "repo": "cs481-ekh/s21-team-jat", "file": "python/read_data.py", "last_update_at": "2021-04-06T17:16:24+00:00", "original_content": "def main(args):\n    \"\"\"\n    The starting point of the program.\n    \"\"\"\n    while True:\n        num_items = 1\n        read_data = ReadData() if len(args) == 1 else ReadData(args)\n        data_log_df = get_file_df(read_data.get_data_log())\n        exe_table_df = get_file_df(read_data.get_exe_table())\n        doses_delta_df = get_file_df(read_data.get_doses_delta())\n        summary = get_summary(read_data.get_summary())\n        dfs = [('data_log', data_log_df[-num_items:]), ('exe_table', exe_table_df[-num_items:]), ('doses_delta', doses_delta_df[-num_items:])]\n        data_json = get_json(dfs, summary, read_data.json_name)\n        read_data.json = data_json\n        time.sleep(5)", "refactored": true, "pred": {"ppl": 4.992739677429199, "ppl_lower": 5.486618518829346, "ppl/lowercase_ppl": -1.0586618349220256, "ppl/zlib": 0.005359949308841985, "Min_5.0% Prob": 11.448020848360928, "Min_10.0% Prob": 9.708210986593496, "Min_20.0% Prob": 6.849620119054267, "Min_30.0% Prob": 5.0836869234770115, "Min_40.0% Prob": 3.973735524301833, "Min_50.0% Prob": 3.2075013561268984, "Min_60.0% Prob": 2.6801661604516944}}
{"hexsha": "7848c02f06f57265c259b7cf28bbf7fbc43f4a4a", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef fecha_caixa():\n    global aberto\n    tot = 0\n    for elemento in transacoes_finalizadas:\n        tot += elemento[1]\n    print(f'TOTAL DE VENDAS R${tot:.2f}')\n    print()\n    while len(transacoes_finalizadas) != 0:\n        print('ELIMINE AS TRANSA\u00c7\u00d5ES FINALIZADAS:')\n        for i, elemento in enumerate(transacoes_finalizadas):\n            print(f'{i:<1}', end='|    ')\n            print(f'{elemento[0]:<17}', end='|')\n            print(f'{elemento[1]:>5.2f}')\n        try:\n            aux = int(input(''))\n            tot -= transacoes_finalizadas[aux][1]\n            tot = round(tot, 2)\n            transacoes_finalizadas.pop(aux)\n        except ValueError:\n            print('[ERRO]: Digite valores v\u00e1lidos')\n            return fecha_caixa()\n        except IndexError:\n            print(f' Indice <= {len(transacoes_finalizadas)}')\n    print(f'TRANSA\u00c7\u00d5ES ELIMINADAS TOTAL: R${tot}')\n    print('CAIXA FECHADO COM SUCESSO!')\n    aberto = False", "fn_id": 17, "class_fn": false, "repo": "FelipeECarvalho/Projetos", "file": "Sistema Caixa Supermercados/principal.py", "last_update_at": "2021-08-22T17:10:08+00:00", "original_content": "def fecha_caixa():\n    global aberto\n    tot = 0\n    for elemento in transacoes_finalizadas:\n        tot += elemento[1]\n    print(f'TOTAL DE VENDAS R${tot:.2f}')\n    print()\n    while len(transacoes_finalizadas) != 0:\n        print('ELIMINE AS TRANSA\u00c7\u00d5ES FINALIZADAS:')\n        for i, elemento in enumerate(transacoes_finalizadas):\n            print(f'{i:<1}', end='|    ')\n            print(f'{elemento[0]:<17}', end='|')\n            print(f'{elemento[1]:>5.2f}')\n        try:\n            aux = int(input(''))\n            tot -= transacoes_finalizadas[aux][1]\n            tot = round(tot, 2)\n            transacoes_finalizadas.pop(aux)\n        except ValueError:\n            print('[ERRO]: Digite valores v\u00e1lidos')\n            return fecha_caixa()\n        except IndexError:\n            print(f' Indice <= {len(transacoes_finalizadas)}')\n    print(f'TRANSA\u00c7\u00d5ES ELIMINADAS TOTAL: R${tot}')\n    print('CAIXA FECHADO COM SUCESSO!')\n    aberto = False", "refactored": true, "pred": {"ppl": 4.532973289489746, "ppl_lower": 5.657960891723633, "ppl/lowercase_ppl": -1.1466777137693882, "ppl/zlib": 0.00326431550644441, "Min_5.0% Prob": 11.4184615952628, "Min_10.0% Prob": 9.303207348132956, "Min_20.0% Prob": 6.431281283750373, "Min_30.0% Prob": 4.8346560082652354, "Min_40.0% Prob": 3.7451778691956554, "Min_50.0% Prob": 3.0146504729180723, "Min_60.0% Prob": 2.526218962897731}}
{"hexsha": "c370b87e8143a97ff5064d02515dc7a0a2cf09a6", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef patch():\n    global _patched\n    if not _patched:\n        conf.Settings = Settings\n        conf.LazySettings = LazySettings\n        conf.settings = settings\n        _patched = True", "fn_id": 0, "class_fn": false, "repo": "jayvdb/django-service-urls", "file": "service_urls/patch.py", "last_update_at": "2021-01-13T02:41:26+00:00", "original_content": "def patch():\n    global _patched\n    if not _patched:\n        conf.Settings = Settings\n        conf.LazySettings = LazySettings\n        conf.settings = settings\n        _patched = True", "refactored": true, "pred": {"ppl": 12.470911979675293, "ppl_lower": 20.25357437133789, "ppl/lowercase_ppl": -1.1921742935585946, "ppl/zlib": 0.0205154381375054, "Min_5.0% Prob": 11.788213729858398, "Min_10.0% Prob": 10.767604112625122, "Min_20.0% Prob": 9.053776952955458, "Min_30.0% Prob": 7.2273968287876675, "Min_40.0% Prob": 6.0123676626305835, "Min_50.0% Prob": 4.971639821926753, "Min_60.0% Prob": 4.301699115761688}}
{"hexsha": "a9d142291ba38842e31d3177869d6447f084d025", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef time_model(model: nn.Module, test_set: Type[LightFieldDataset], device) -> np.ndarray:\n    model.eval()\n    custom = CustomProgressBar('N/A')\n    loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=1, sampler=SequentialSampler(test_set), num_workers=1, pin_memory=True)\n    test_set.set_only_x_dataset()\n    model.eval()\n    torch.set_grad_enabled(False)\n    print('Timing')\n    all_times = []\n    for input_data in custom.bar(loader):\n        tick = time.time()\n        input_var = input_data.to(device).float().div_(255.0)\n        _ = model(input_var)\n        tock = time.time()\n        time_taken = tock - tick\n        all_times.append(time_taken)\n    test_set.revert_only_x_dataset()\n    return np.array(all_times)", "fn_id": 0, "class_fn": false, "repo": "leaveitout/deep_light_field_interp", "file": "deeplfinterp/util/train_tools.py", "last_update_at": "2021-08-06T13:39:19+00:00", "original_content": "def time_model(model: nn.Module, test_set: Type[LightFieldDataset], device) -> np.ndarray:\n    model.eval()\n    custom = CustomProgressBar('N/A')\n    loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=1, sampler=SequentialSampler(test_set), num_workers=1, pin_memory=True)\n    test_set.set_only_x_dataset()\n    model.eval()\n    torch.set_grad_enabled(False)\n    print('Timing')\n    all_times = []\n    for input_data in custom.bar(loader):\n        tick = time.time()\n        input_var = input_data.to(device).float().div_(255.0)\n        _ = model(input_var)\n        tock = time.time()\n        time_taken = tock - tick\n        all_times.append(time_taken)\n    test_set.revert_only_x_dataset()\n    return np.array(all_times)", "refactored": true, "pred": {"ppl": 4.387096881866455, "ppl_lower": 6.07069206237793, "ppl/lowercase_ppl": -1.2196605126878541, "ppl/zlib": 0.003588999285907754, "Min_5.0% Prob": 10.843818924643777, "Min_10.0% Prob": 8.760754834050717, "Min_20.0% Prob": 6.399677256320385, "Min_30.0% Prob": 4.788551390171051, "Min_40.0% Prob": 3.688321584776828, "Min_50.0% Prob": 2.9645547260444203, "Min_60.0% Prob": 2.4703074298291043}}
{"hexsha": "e8bc0e0cfb812d7c32521e012f61a1e17efddd71", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef save_mel_dataset(out_path, *paths, in_db=True, root_path=None):\n    \"\"\"\n    \"\"\"\n    ds_len = len(paths)\n    with IncrementalHDF5(out_path, CONF.NUM_MELS, np.float32) as ihdf5:\n        LOGGER.info(f'Writing to {out_path}')\n        for i, abspath in enumerate(paths, 1):\n            if root_path is not None:\n                metadata_str = str(abspath.relative_to(root_path))\n            else:\n                metadata_str = str(abspath)\n            if i % 100 == 0:\n                LOGGER.info(f'[{i}/{ds_len}] save_mel_dataset: {metadata_str}')\n            arr = wavpath_to_mel(str(abspath), CONF.WAV_SR, wav_norm=CONF.WAV_NORM, n_mels=CONF.NUM_MELS, hop_length=CONF.STFT_HOPSIZE, pad_mode='constant', in_decibels=in_db, logger=LOGGER)\n            if arr is None:\n                continue\n            ihdf5.append(arr, metadata_str)\n            _, arr_w = arr.shape\n            assert (arr == ihdf5.data_ds[:, -arr_w:]).all(), 'Should never happen'\n        LOGGER.info(f'Finished writing to {out_path}')", "fn_id": 1, "class_fn": false, "repo": "andres-fr/dcase2021_umaps", "file": "00c_precompute_fraunhofer_fixed.py", "last_update_at": "2021-11-30T09:28:21+00:00", "original_content": "def save_mel_dataset(out_path, *paths, in_db=True, root_path=None):\n    \"\"\"\n    \"\"\"\n    ds_len = len(paths)\n    with IncrementalHDF5(out_path, CONF.NUM_MELS, np.float32) as ihdf5:\n        LOGGER.info(f'Writing to {out_path}')\n        for i, abspath in enumerate(paths, 1):\n            if root_path is not None:\n                metadata_str = str(abspath.relative_to(root_path))\n            else:\n                metadata_str = str(abspath)\n            if i % 100 == 0:\n                LOGGER.info(f'[{i}/{ds_len}] save_mel_dataset: {metadata_str}')\n            arr = wavpath_to_mel(str(abspath), CONF.WAV_SR, wav_norm=CONF.WAV_NORM, n_mels=CONF.NUM_MELS, hop_length=CONF.STFT_HOPSIZE, pad_mode='constant', in_decibels=in_db, logger=LOGGER)\n            if arr is None:\n                continue\n            ihdf5.append(arr, metadata_str)\n            _, arr_w = arr.shape\n            assert (arr == ihdf5.data_ds[:, -arr_w:]).all(), 'Should never happen'\n        LOGGER.info(f'Finished writing to {out_path}')", "refactored": true, "pred": {"ppl": 5.354170799255371, "ppl_lower": 6.2256669998168945, "ppl/lowercase_ppl": -1.0898783659815607, "ppl/zlib": 0.0033094198142650734, "Min_5.0% Prob": 11.219573497772217, "Min_10.0% Prob": 9.5745300501585, "Min_20.0% Prob": 7.1992791469280535, "Min_30.0% Prob": 5.391656375052977, "Min_40.0% Prob": 4.16055660029404, "Min_50.0% Prob": 3.3536839561491476, "Min_60.0% Prob": 2.7996388071779217}}
{"hexsha": "5a93cf64a2f1ac340c8208f986a4fd8f35d03148", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef view(location, browser=None, new='same', autoraise=True):\n    \"\"\" Opens a browser to view the specified location.\n\n        Args:\n            location (str) : location to open\n                If location does not begin with \"http:\" it is assumed\n                to be a file path on the local filesystem.\n            browser (str) : what browser to use\n            new (str) : how to open the location\n                Valid values are:\n                    * \"same\" - open in the current tab\n                    * \"tab\" - open a new tab in the current window\n                    * \"window\" - open in a new window\n            autoraise (bool) : whether to raise the new location\n\n        Returns:\n            None\n\n        \"\"\"\n    new_map = {'same': 0, 'window': 1, 'tab': 2}\n    if location.startswith('http'):\n        url = location\n    else:\n        url = 'file://' + abspath(location)\n    try:\n        controller = get_browser_controller(browser)\n        controller.open(url, new=new_map[new], autoraise=autoraise)\n    except (SystemExit, KeyboardInterrupt):\n        raise\n    except:\n        pass", "fn_id": 1, "class_fn": false, "repo": "tswicegood/bokeh", "file": "bokeh/browserlib.py", "last_update_at": "2021-11-17T10:48:36+00:00", "original_content": "def view(location, browser=None, new='same', autoraise=True):\n    \"\"\" Opens a browser to view the specified location.\n\n        Args:\n            location (str) : location to open\n                If location does not begin with \"http:\" it is assumed\n                to be a file path on the local filesystem.\n            browser (str) : what browser to use\n            new (str) : how to open the location\n                Valid values are:\n                    * \"same\" - open in the current tab\n                    * \"tab\" - open a new tab in the current window\n                    * \"window\" - open in a new window\n            autoraise (bool) : whether to raise the new location\n\n        Returns:\n            None\n\n        \"\"\"\n    new_map = {'same': 0, 'window': 1, 'tab': 2}\n    if location.startswith('http'):\n        url = location\n    else:\n        url = 'file://' + abspath(location)\n    try:\n        controller = get_browser_controller(browser)\n        controller.open(url, new=new_map[new], autoraise=autoraise)\n    except (SystemExit, KeyboardInterrupt):\n        raise\n    except:\n        pass", "refactored": true, "pred": {"ppl": 4.420616626739502, "ppl_lower": 4.9807047843933105, "ppl/lowercase_ppl": -1.0802623152837196, "ppl/zlib": 0.002978515420183029, "Min_5.0% Prob": 9.483394035926231, "Min_10.0% Prob": 7.9221906295189495, "Min_20.0% Prob": 6.023815718980936, "Min_30.0% Prob": 4.651158490242103, "Min_40.0% Prob": 3.644854741027722, "Min_50.0% Prob": 2.9535629980839215, "Min_60.0% Prob": 2.4733872829864803}}
{"hexsha": "ad091436c7271a654fb820601c6d942f23f49ff5", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_rna(fq_dict):\n    \"\"\"\n    Obtain a numpy array with all detected spots in the image. Detection results\n    are saved in a dictionary (see read_FQ_results_matlab for more details).\n    \"\"\"\n    RNAall = nested_lookup('spots', fq_dict)\n    for idx, val in enumerate(RNAall):\n        if idx == 0:\n            spots_all = np.copy(val)\n        else:\n            spots_all = np.append(spots_all, val, axis=0)\n    return spots_all", "fn_id": 3, "class_fn": false, "repo": "muellerflorian/parker-rna-loc-elegans", "file": "rnaloc/FQtoolbox.py", "last_update_at": "2021-01-12T16:51:38+00:00", "original_content": "def get_rna(fq_dict):\n    \"\"\"\n    Obtain a numpy array with all detected spots in the image. Detection results\n    are saved in a dictionary (see read_FQ_results_matlab for more details).\n    \"\"\"\n    RNAall = nested_lookup('spots', fq_dict)\n    for idx, val in enumerate(RNAall):\n        if idx == 0:\n            spots_all = np.copy(val)\n        else:\n            spots_all = np.append(spots_all, val, axis=0)\n    return spots_all", "refactored": true, "pred": {"ppl": 8.697867393493652, "ppl_lower": 8.33634090423584, "ppl/lowercase_ppl": -0.9803735734850796, "ppl/zlib": 0.007981837152868345, "Min_5.0% Prob": 11.915898958841959, "Min_10.0% Prob": 10.266262934758114, "Min_20.0% Prob": 8.142300807512724, "Min_30.0% Prob": 6.485123711824417, "Min_40.0% Prob": 5.256453953823954, "Min_50.0% Prob": 4.285186655930619, "Min_60.0% Prob": 3.6160884013399484}}
{"hexsha": "8b2fa9137563469049632eeb151af101a87de1b0", "ext": "py", "lang": "Python", "content": "@client.event\n@timeing\n@measure_memory_usage\nasync def on_message(message):\n    if message.author == client.user:\n        if message.content == 'Authentication invalid':\n            await renew_auth(message)\n        return\n    if message.content.startswith('>'):\n        return\n        await parse(message)\n    if isinstance(message.channel, discord.channel.DMChannel):\n        if message.content == 'quit':\n            await do_quit(message)\n        if message.content == 'test':\n            return\n            await do_tests(message)\n        if message.content == 'renew' or message.content == 'reset':\n            await renew_auth(message)\n    if message.content.startswith('/'):\n        await client.process_commands(message)", "fn_id": 5, "class_fn": false, "repo": "joshmiller17/venntbot", "file": "venntbot.py", "last_update_at": "2021-09-11T23:15:38+00:00", "original_content": "@client.event\nasync def on_message(message):\n    if message.author == client.user:\n        if message.content == 'Authentication invalid':\n            await renew_auth(message)\n        return\n    if message.content.startswith('>'):\n        return\n        await parse(message)\n    if isinstance(message.channel, discord.channel.DMChannel):\n        if message.content == 'quit':\n            await do_quit(message)\n        if message.content == 'test':\n            return\n            await do_tests(message)\n        if message.content == 'renew' or message.content == 'reset':\n            await renew_auth(message)\n    if message.content.startswith('/'):\n        await client.process_commands(message)", "refactored": true, "pred": {"ppl": 5.098207473754883, "ppl_lower": 5.643118381500244, "ppl/lowercase_ppl": -1.062341764775877, "ppl/zlib": 0.006077944038111457, "Min_5.0% Prob": 11.277381896972656, "Min_10.0% Prob": 9.365505129098892, "Min_20.0% Prob": 6.7881060296838935, "Min_30.0% Prob": 5.109027410040096, "Min_40.0% Prob": 3.9835113603057284, "Min_50.0% Prob": 3.233406105852989, "Min_60.0% Prob": 2.7248695925103896}}
{"hexsha": "385dabe838e94b3bc682d337123e8384c47a3724", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef parse_patterns(query, graph=False):\n    \"\"\"\n    if query['patterns'] looks like so:\n    ['target_type=', 'what=', '!tag_k=not_equals_thistag_v', 'tag_k:match_this_val', 'arbitrary', 'words']\n\n    then the patterns will look like so:\n    {\n    'tag_k=not_equals_thistag_v': {'negate': True, 'match_tag_equality': ['tag_k', 'not_equals_thistag_v']},\n    'target_type=':               {'negate': False, 'match_tag_equality': ['target_type', '']},\n    'what=':                      {'negate': False, 'match_tag_equality': ['what', '']},\n    'tag_k:match_this_val':       {'negate': False, 'match_tag_regex': ['tag_k', 'match_this_val']},\n    'words':                      {'negate': False, 'match_id_regex': <_sre.SRE_Pattern object at 0x2612cb0>},\n    'arbitrary':                  {'negate': False, 'match_id_regex': <_sre.SRE_Pattern object at 0x7f6cc000bd90>}\n    }\n    \"\"\"\n    patterns = {}\n    for pattern in query['patterns']:\n        negate = False\n        if pattern.startswith('!'):\n            negate = True\n            pattern = pattern[1:]\n        patterns[pattern] = {'negate': negate}\n        if '=' in pattern:\n            if not graph or pattern not in ('target_type=', 'what='):\n                patterns[pattern]['match_tag_equality'] = pattern.split('=')\n            else:\n                del patterns[pattern]\n        elif ':' in pattern:\n            if not graph or pattern not in ('target_type:', 'what:'):\n                patterns[pattern]['match_tag_regex'] = pattern.split(':')\n            else:\n                del patterns[pattern]\n        else:\n            patterns[pattern]['match_id_regex'] = re.compile(pattern)\n    return patterns", "fn_id": 2, "class_fn": false, "repo": "bittorrent/graph-explorer", "file": "query.py", "last_update_at": "2021-03-14T19:37:11+00:00", "original_content": "def parse_patterns(query, graph=False):\n    \"\"\"\n    if query['patterns'] looks like so:\n    ['target_type=', 'what=', '!tag_k=not_equals_thistag_v', 'tag_k:match_this_val', 'arbitrary', 'words']\n\n    then the patterns will look like so:\n    {\n    'tag_k=not_equals_thistag_v': {'negate': True, 'match_tag_equality': ['tag_k', 'not_equals_thistag_v']},\n    'target_type=':               {'negate': False, 'match_tag_equality': ['target_type', '']},\n    'what=':                      {'negate': False, 'match_tag_equality': ['what', '']},\n    'tag_k:match_this_val':       {'negate': False, 'match_tag_regex': ['tag_k', 'match_this_val']},\n    'words':                      {'negate': False, 'match_id_regex': <_sre.SRE_Pattern object at 0x2612cb0>},\n    'arbitrary':                  {'negate': False, 'match_id_regex': <_sre.SRE_Pattern object at 0x7f6cc000bd90>}\n    }\n    \"\"\"\n    patterns = {}\n    for pattern in query['patterns']:\n        negate = False\n        if pattern.startswith('!'):\n            negate = True\n            pattern = pattern[1:]\n        patterns[pattern] = {'negate': negate}\n        if '=' in pattern:\n            if not graph or pattern not in ('target_type=', 'what='):\n                patterns[pattern]['match_tag_equality'] = pattern.split('=')\n            else:\n                del patterns[pattern]\n        elif ':' in pattern:\n            if not graph or pattern not in ('target_type:', 'what:'):\n                patterns[pattern]['match_tag_regex'] = pattern.split(':')\n            else:\n                del patterns[pattern]\n        else:\n            patterns[pattern]['match_id_regex'] = re.compile(pattern)\n    return patterns", "refactored": true, "pred": {"ppl": 3.223541021347046, "ppl_lower": 3.465555191040039, "ppl/lowercase_ppl": -1.0618484457473787, "ppl/zlib": 0.0022466035535586145, "Min_5.0% Prob": 9.638985792795816, "Min_10.0% Prob": 7.868135392665863, "Min_20.0% Prob": 5.335637535016561, "Min_30.0% Prob": 3.83430706735315, "Min_40.0% Prob": 2.9113456904888153, "Min_50.0% Prob": 2.3429002795630125, "Min_60.0% Prob": 1.9502897548178832}}
{"hexsha": "c465816e2eca187bfd4fc478284eec3b6c703e51", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef generalInquiry(choiceList, message):\n    choiceList.append(colored('Return', 'red'))\n    options = [inquirer.List('inputVal', message=message, choices=choiceList)]\n    print(colored('Use arrow keys to navigate\\n', 'blue'))\n    print(colored('Proceed --> Enter / Return key\\n', 'green'))\n    return inquirer.prompt(options)['inputVal']", "fn_id": 15, "class_fn": false, "repo": "HiLabTufts/GailBot-3", "file": "gailbot-3.py", "last_update_at": "2021-06-28T10:26:20+00:00", "original_content": "def generalInquiry(choiceList, message):\n    choiceList.append(colored('Return', 'red'))\n    options = [inquirer.List('inputVal', message=message, choices=choiceList)]\n    print(colored('Use arrow keys to navigate\\n', 'blue'))\n    print(colored('Proceed --> Enter / Return key\\n', 'green'))\n    return inquirer.prompt(options)['inputVal']", "refactored": true, "pred": {"ppl": 11.548578262329102, "ppl_lower": 16.799274444580078, "ppl/lowercase_ppl": -1.1531836557108428, "ppl/zlib": 0.010825497058497578, "Min_5.0% Prob": 10.914762878417969, "Min_10.0% Prob": 9.709508228302003, "Min_20.0% Prob": 8.240012502670288, "Min_30.0% Prob": 6.959868852297465, "Min_40.0% Prob": 5.698523995280266, "Min_50.0% Prob": 4.765952714681625, "Min_60.0% Prob": 4.039097398519516}}
{"hexsha": "3b7d7e72a7f0b786dd6d72495ed17a1c7b0e1e9f", "ext": "py", "lang": "Python", "content": "@card('Rolling Spoil')\ndef rolling_spoil(card, abilities):\n\n    @timeing\n    @measure_memory_usage\n    def rolling_spoil():\n        return AbilityNotImplemented\n    return (rolling_spoil,)", "fn_id": 155, "class_fn": false, "repo": "Julian/cardboard", "file": "cardboard/cards/sets/ravnica_city_of_guilds.py", "last_update_at": "2021-05-29T06:00:40+00:00", "original_content": "@card('Rolling Spoil')\ndef rolling_spoil(card, abilities):\n\n    def rolling_spoil():\n        return AbilityNotImplemented\n    return (rolling_spoil,)", "refactored": true, "pred": {"ppl": 18.779550552368164, "ppl_lower": 30.32787322998047, "ppl/lowercase_ppl": -1.1634287364493183, "ppl/zlib": 0.022734639854612167, "Min_5.0% Prob": 14.7405366897583, "Min_10.0% Prob": 13.86076831817627, "Min_20.0% Prob": 10.229913075764975, "Min_30.0% Prob": 8.218857220241002, "Min_40.0% Prob": 6.783501675254421, "Min_50.0% Prob": 5.747634847958882, "Min_60.0% Prob": 4.898403911755003}}
{"hexsha": "d090023df2a9fa2d3a531ad91449ef05aefa4e12", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef dice_loss(pred, target, smooth=1.0):\n    pred = pred.contiguous()\n    target = target.contiguous()\n    intersection = (pred * target).sum(dim=1).sum(dim=1)\n    loss = 1 - (2.0 * intersection + smooth) / (pred.sum(dim=1).sum(dim=1) + target.sum(dim=1).sum(dim=1) + smooth)\n    return loss.mean()", "fn_id": 1, "class_fn": false, "repo": "rispoli-lab/Deep-Learning-Breast-FGT", "file": "loss.py", "last_update_at": "2021-09-17T17:10:52+00:00", "original_content": "def dice_loss(pred, target, smooth=1.0):\n    pred = pred.contiguous()\n    target = target.contiguous()\n    intersection = (pred * target).sum(dim=1).sum(dim=1)\n    loss = 1 - (2.0 * intersection + smooth) / (pred.sum(dim=1).sum(dim=1) + target.sum(dim=1).sum(dim=1) + smooth)\n    return loss.mean()", "refactored": true, "pred": {"ppl": 1.9900437593460083, "ppl_lower": 1.9900437593460083, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.003932323589232814, "Min_5.0% Prob": 9.131574535369873, "Min_10.0% Prob": 5.80606298013167, "Min_20.0% Prob": 3.4002729843963277, "Min_30.0% Prob": 2.287679620525416, "Min_40.0% Prob": 1.7403383019069831, "Min_50.0% Prob": 1.3760141130953438, "Min_60.0% Prob": 1.1536260268677485}}
{"hexsha": "20fd1f5105839af2bba28ff254523016e1e09b8f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _get_pipeline_definition_args(dag):\n    check.inst_param(dag, 'dag', DAG)\n    pipeline_dependencies = {}\n    solid_defs = []\n    seen_tasks = []\n    dag_roots = sorted(dag.roots, key=lambda x: x.task_id)\n    for task in dag_roots:\n        _traverse_airflow_dag(task, seen_tasks, pipeline_dependencies, solid_defs)\n    return (pipeline_dependencies, solid_defs)", "fn_id": 1, "class_fn": false, "repo": "uranusbeam/bit-dagster", "file": "python_modules/libraries/dagster-airflow/dagster_airflow/dagster_pipeline_factory.py", "last_update_at": "2021-11-08T02:10:42+00:00", "original_content": "def _get_pipeline_definition_args(dag):\n    check.inst_param(dag, 'dag', DAG)\n    pipeline_dependencies = {}\n    solid_defs = []\n    seen_tasks = []\n    dag_roots = sorted(dag.roots, key=lambda x: x.task_id)\n    for task in dag_roots:\n        _traverse_airflow_dag(task, seen_tasks, pipeline_dependencies, solid_defs)\n    return (pipeline_dependencies, solid_defs)", "refactored": true, "pred": {"ppl": 5.421920299530029, "ppl_lower": 5.682384490966797, "ppl/lowercase_ppl": -1.0277564532809853, "ppl/zlib": 0.007546652015664429, "Min_5.0% Prob": 9.923098754882812, "Min_10.0% Prob": 8.735838239843195, "Min_20.0% Prob": 6.784867037897524, "Min_30.0% Prob": 5.273314418111529, "Min_40.0% Prob": 4.230092300669007, "Min_50.0% Prob": 3.3973987206559757, "Min_60.0% Prob": 2.8231640615633555}}
{"hexsha": "200733e11c459547d9421419b37fface7ce8edae", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_qcovariate_num_qs(exreaddata):\n    test = covariate.QCovariate()\n    assert test.num_qs() == 0\n    test.consume_read(exreaddata)\n    assert test.num_qs() == 11", "fn_id": 11, "class_fn": false, "repo": "adamjorr/kbbq-py", "file": "tests/test_covariate.py", "last_update_at": "2021-06-02T00:07:35+00:00", "original_content": "def test_qcovariate_num_qs(exreaddata):\n    test = covariate.QCovariate()\n    assert test.num_qs() == 0\n    test.consume_read(exreaddata)\n    assert test.num_qs() == 11", "refactored": true, "pred": {"ppl": 13.855657577514648, "ppl_lower": 15.508102416992188, "ppl/lowercase_ppl": -1.042861208205191, "ppl/zlib": 0.020377470066476933, "Min_5.0% Prob": 12.364321072896322, "Min_10.0% Prob": 10.87699740273612, "Min_20.0% Prob": 9.28081478391375, "Min_30.0% Prob": 7.68927138192313, "Min_40.0% Prob": 6.42206963471004, "Min_50.0% Prob": 5.177161016398006, "Min_60.0% Prob": 4.378545853287675}}
{"hexsha": "eac9bfadf6afe42b2ba6db1b9064946fdf240cf1", "ext": "py", "lang": "Python", "content": "@card('Goblin Archaeologist')\n@timeing\n@measure_memory_usage\ndef goblin_archaeologist(card, abilities):\n\n    def goblin_archaeologist():\n        return AbilityNotImplemented\n    return (goblin_archaeologist,)", "fn_id": 17, "class_fn": false, "repo": "Julian/cardboard", "file": "cardboard/cards/sets/darksteel.py", "last_update_at": "2021-05-29T06:00:40+00:00", "original_content": "@card('Goblin Archaeologist')\ndef goblin_archaeologist(card, abilities):\n\n    def goblin_archaeologist():\n        return AbilityNotImplemented\n    return (goblin_archaeologist,)", "refactored": true, "pred": {"ppl": 7.042977809906006, "ppl_lower": 10.222814559936523, "ppl/lowercase_ppl": -1.1908734372508267, "ppl/zlib": 0.01478811412953737, "Min_5.0% Prob": 12.898336410522461, "Min_10.0% Prob": 10.545940240224203, "Min_20.0% Prob": 8.045206216665415, "Min_30.0% Prob": 6.339845569510209, "Min_40.0% Prob": 4.853335336996959, "Min_50.0% Prob": 3.8902410914500556, "Min_60.0% Prob": 3.3020679265356216}}
{"hexsha": "d2beff782efe625c4f930164ec0ecae6152de846", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_final_seats_occupied_based_on_sight(seats: list[str]) -> int:\n    old_grid = Grid(seats)\n    while (new_grid := transform_based_on_los(old_grid)) != old_grid:\n        old_grid = new_grid\n    return new_grid.count('#')", "fn_id": 2, "class_fn": false, "repo": "pviafore/AdventOfCode2020", "file": "challenge11.py", "last_update_at": "2021-12-09T09:54:54+00:00", "original_content": "def get_final_seats_occupied_based_on_sight(seats: list[str]) -> int:\n    old_grid = Grid(seats)\n    while (new_grid := transform_based_on_los(old_grid)) != old_grid:\n        old_grid = new_grid\n    return new_grid.count('#')", "refactored": true, "pred": {"ppl": 8.10266399383545, "ppl_lower": 8.667879104614258, "ppl/lowercase_ppl": -1.0322299347531976, "ppl/zlib": 0.012307017033731656, "Min_5.0% Prob": 12.73741602897644, "Min_10.0% Prob": 11.0203355550766, "Min_20.0% Prob": 8.22281240014469, "Min_30.0% Prob": 6.572402257919311, "Min_40.0% Prob": 5.1778292708537155, "Min_50.0% Prob": 4.22383369860195, "Min_60.0% Prob": 3.485626225451044}}
{"hexsha": "65d2c4247e06b070e56286e1c0516a47f5fdba82", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef generate_testcase_files(instance, env, testcase_dir_path):\n    testcases_list = []\n    config = {'config': {'name': instance.name, 'variables': [], 'request': {'base_url': env.base_url if env else ''}}}\n    testcases_list.append(config)\n    include = json.loads(instance.include, encoding='utf-8')\n    request = json.loads(instance.request, encoding='utf-8')\n    module_name = instance.module.name\n    project_name = instance.module.project.name\n    testcase_dir_path = os.path.join(testcase_dir_path, project_name)\n    if not os.path.exists(testcase_dir_path):\n        os.makedirs(testcase_dir_path)\n        debugtalk_obj = Projects.objects.get(name=project_name).debugtalk\n        if debugtalk_obj:\n            debugtalk = debugtalk_obj.debugtalk\n        else:\n            debugtalk = ''\n        with open(os.path.join(testcase_dir_path, 'debugtalk.py'), mode='w', encoding='utf-8') as one_file:\n            one_file.write(debugtalk)\n    testcase_dir_path = os.path.join(testcase_dir_path, module_name)\n    if not os.path.exists(testcase_dir_path):\n        os.makedirs(testcase_dir_path)\n    if 'testcases' in include:\n        for t_id in include.get('testcases'):\n            testcase_obj = Testcases.objects.filter(id=t_id).first()\n            if testcase_obj:\n                try:\n                    testcase_request = json.loads(testcase_obj.request, encoding='utf-8')\n                except Exception as e:\n                    logger.error(e)\n                    continue\n                else:\n                    extract = testcase_request['test'].get('extract')\n                    if extract:\n                        for e in extract:\n                            testcases_list[0]['config']['variables'].append({[i for i in e.keys()][0]: ''})\n                    testcase_request['test'] = OrderedDict(testcase_request['test'])\n                    testcases_list.append(OrderedDict(testcase_request))\n    request['test'] = OrderedDict(request['test'])\n    testcases_list.append(request)\n    with open(os.path.join(testcase_dir_path, instance.name + '.yml'), mode='w', encoding='utf-8') as one_file:\n        ordered_yaml_dump(testcases_list, one_file, default_flow_style=False, allow_unicode=True)", "fn_id": 1, "class_fn": false, "repo": "op896898466/apitest", "file": "utils/common.py", "last_update_at": "2021-08-16T05:48:43+00:00", "original_content": "def generate_testcase_files(instance, env, testcase_dir_path):\n    testcases_list = []\n    config = {'config': {'name': instance.name, 'variables': [], 'request': {'base_url': env.base_url if env else ''}}}\n    testcases_list.append(config)\n    include = json.loads(instance.include, encoding='utf-8')\n    request = json.loads(instance.request, encoding='utf-8')\n    module_name = instance.module.name\n    project_name = instance.module.project.name\n    testcase_dir_path = os.path.join(testcase_dir_path, project_name)\n    if not os.path.exists(testcase_dir_path):\n        os.makedirs(testcase_dir_path)\n        debugtalk_obj = Projects.objects.get(name=project_name).debugtalk\n        if debugtalk_obj:\n            debugtalk = debugtalk_obj.debugtalk\n        else:\n            debugtalk = ''\n        with open(os.path.join(testcase_dir_path, 'debugtalk.py'), mode='w', encoding='utf-8') as one_file:\n            one_file.write(debugtalk)\n    testcase_dir_path = os.path.join(testcase_dir_path, module_name)\n    if not os.path.exists(testcase_dir_path):\n        os.makedirs(testcase_dir_path)\n    if 'testcases' in include:\n        for t_id in include.get('testcases'):\n            testcase_obj = Testcases.objects.filter(id=t_id).first()\n            if testcase_obj:\n                try:\n                    testcase_request = json.loads(testcase_obj.request, encoding='utf-8')\n                except Exception as e:\n                    logger.error(e)\n                    continue\n                else:\n                    extract = testcase_request['test'].get('extract')\n                    if extract:\n                        for e in extract:\n                            testcases_list[0]['config']['variables'].append({[i for i in e.keys()][0]: ''})\n                    testcase_request['test'] = OrderedDict(testcase_request['test'])\n                    testcases_list.append(OrderedDict(testcase_request))\n    request['test'] = OrderedDict(request['test'])\n    testcases_list.append(request)\n    with open(os.path.join(testcase_dir_path, instance.name + '.yml'), mode='w', encoding='utf-8') as one_file:\n        ordered_yaml_dump(testcases_list, one_file, default_flow_style=False, allow_unicode=True)", "refactored": true, "pred": {"ppl": 2.842229127883911, "ppl_lower": 3.001354217529297, "ppl/lowercase_ppl": -1.052149661262396, "ppl/zlib": 0.0015249469320896243, "Min_5.0% Prob": 9.910817771122373, "Min_10.0% Prob": 7.5048485893314165, "Min_20.0% Prob": 4.750757122240147, "Min_30.0% Prob": 3.4039006268375376, "Min_40.0% Prob": 2.5977567776787183, "Min_50.0% Prob": 2.085916013875664, "Min_60.0% Prob": 1.7433690262074042}}
{"hexsha": "679616cb8b1011d2bf2813d8352a4c19d23a2b17", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef save_current_window_and_open_tools_window():\n    \"\"\"\n    Save the current window with `save_current_window' and open the eiffel\n    tools buffer in a window. The window containing the eiffel tools buffer\n    will be selected after the launch of this routine. To reselect the\n    window saved by this routine, use the `select_saved_window' routine.\n    \"\"\"\n    tools_buffer_name = environment.get_global_variable('eiffel_tools_buffer_name')\n    tools_buffer_number = get_tools_buffer_number()\n    if tools_buffer_number < 0:\n        save_current_window_and_open_new_tools_window(tools_buffer_name)\n    else:\n        tools_buffer_window_number = int(environment.evaluate('bufwinnr(\"' + tools_buffer_name + '\")'))\n        if tools_buffer_window_number < 0:\n            save_current_window_and_open_existing_tools_window(tools_buffer_name)\n        else:\n            save_current_window_and_select_tools_window(tools_buffer_window_number)\n    environment.execute('setlocal filetype=')", "fn_id": 8, "class_fn": false, "repo": "tioui/Vim_Eiffel_IDE", "file": "pyplugin/eiffel_ide.py", "last_update_at": "2021-11-23T20:12:06+00:00", "original_content": "def save_current_window_and_open_tools_window():\n    \"\"\"\n    Save the current window with `save_current_window' and open the eiffel\n    tools buffer in a window. The window containing the eiffel tools buffer\n    will be selected after the launch of this routine. To reselect the\n    window saved by this routine, use the `select_saved_window' routine.\n    \"\"\"\n    tools_buffer_name = environment.get_global_variable('eiffel_tools_buffer_name')\n    tools_buffer_number = get_tools_buffer_number()\n    if tools_buffer_number < 0:\n        save_current_window_and_open_new_tools_window(tools_buffer_name)\n    else:\n        tools_buffer_window_number = int(environment.evaluate('bufwinnr(\"' + tools_buffer_name + '\")'))\n        if tools_buffer_window_number < 0:\n            save_current_window_and_open_existing_tools_window(tools_buffer_name)\n        else:\n            save_current_window_and_select_tools_window(tools_buffer_window_number)\n    environment.execute('setlocal filetype=')", "refactored": true, "pred": {"ppl": 4.726147651672363, "ppl_lower": 4.858996391296387, "ppl/lowercase_ppl": -1.0178490155458044, "ppl/zlib": 0.004076405303663301, "Min_5.0% Prob": 11.084309284503643, "Min_10.0% Prob": 8.960233052571615, "Min_20.0% Prob": 6.339055083014748, "Min_30.0% Prob": 4.831174209350493, "Min_40.0% Prob": 3.805714579874819, "Min_50.0% Prob": 3.1029640769001343, "Min_60.0% Prob": 2.585871233316985}}
{"hexsha": "a1475fa68fd62c2546f13e320d8164046b674af2", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef sub_special_tokens(text):\n    text = re.sub(' www.', ' http://www.', text)\n    text = re.sub('(https|http)?:\\\\/\\\\/(\\\\w|\\\\.|\\\\/|\\\\?|\\\\=|\\\\&|\\\\%)*\\\\b', ' xxurl ', text)\n    pat = '\\\\d{3}[-\\\\.\\\\s]??\\\\d{4}[-\\\\.\\\\s]??\\\\d{4}|\\\\d{5}[-\\\\.\\\\s]??\\\\d{3}[-\\\\.\\\\s]??\\\\d{3}|(?:\\\\d{4}\\\\)?[\\\\s-]?\\\\d{3}[\\\\s-]?\\\\d{4})'\n    text = re.sub(pat, ' xxphone ', text)\n    text = text.replace('\u00a3', '$ ')\n    text = re.sub('(\\\\d+)[ ]{0,1}p', '$ 0.\\x01', text)\n    text = re.sub('\\\\$[ ]*(\\\\d+[,\\\\.])*\\\\d+', ' xxmon ', text)\n    text = re.sub('(\\\\b[A-Z][A-Z0-9]*\\\\b)', ' xxup \\\\1 ', text)\n    text = re.sub('(\\\\b[A-Z][a-z0-9]+\\\\b)', ' xxcap \\\\1 ', text)\n    text = re.sub('[:;][ ]*[-]*[ ]*[()]', ' xxemoji ', text)\n    return text", "fn_id": 0, "class_fn": false, "repo": "rkingery/ml_tutorials", "file": "notebooks/utils.py", "last_update_at": "2021-01-15T10:26:34+00:00", "original_content": "def sub_special_tokens(text):\n    text = re.sub(' www.', ' http://www.', text)\n    text = re.sub('(https|http)?:\\\\/\\\\/(\\\\w|\\\\.|\\\\/|\\\\?|\\\\=|\\\\&|\\\\%)*\\\\b', ' xxurl ', text)\n    pat = '\\\\d{3}[-\\\\.\\\\s]??\\\\d{4}[-\\\\.\\\\s]??\\\\d{4}|\\\\d{5}[-\\\\.\\\\s]??\\\\d{3}[-\\\\.\\\\s]??\\\\d{3}|(?:\\\\d{4}\\\\)?[\\\\s-]?\\\\d{3}[\\\\s-]?\\\\d{4})'\n    text = re.sub(pat, ' xxphone ', text)\n    text = text.replace('\u00a3', '$ ')\n    text = re.sub('(\\\\d+)[ ]{0,1}p', '$ 0.\\x01', text)\n    text = re.sub('\\\\$[ ]*(\\\\d+[,\\\\.])*\\\\d+', ' xxmon ', text)\n    text = re.sub('(\\\\b[A-Z][A-Z0-9]*\\\\b)', ' xxup \\\\1 ', text)\n    text = re.sub('(\\\\b[A-Z][a-z0-9]+\\\\b)', ' xxcap \\\\1 ', text)\n    text = re.sub('[:;][ ]*[-]*[ ]*[()]', ' xxemoji ', text)\n    return text", "refactored": true, "pred": {"ppl": 4.800257205963135, "ppl_lower": 4.904016017913818, "ppl/lowercase_ppl": -1.0136325490206524, "ppl/zlib": 0.004613733826629106, "Min_5.0% Prob": 10.029014885425568, "Min_10.0% Prob": 8.445168668573553, "Min_20.0% Prob": 6.218420850696848, "Min_30.0% Prob": 4.82450287530918, "Min_40.0% Prob": 3.8446964866189814, "Min_50.0% Prob": 3.1284237079144943, "Min_60.0% Prob": 2.614244285390002}}
{"hexsha": "10e63bec5b7e172368421e76129581befebd5bc0", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef part_sum(bitlist, indices):\n    \"\"\"Compute the mod 2 sum of the subset of bits in bitlist given by the list of indices.\"\"\"\n    sum = 0\n    for x in indices:\n        sum = sum ^ bitlist[x]\n    return sum", "fn_id": 6, "class_fn": false, "repo": "wmkirby1/CS-VQE", "file": "misc/legacy/fermions/yaferp/general/fermions.py", "last_update_at": "2021-11-10T18:03:49+00:00", "original_content": "def part_sum(bitlist, indices):\n    \"\"\"Compute the mod 2 sum of the subset of bits in bitlist given by the list of indices.\"\"\"\n    sum = 0\n    for x in indices:\n        sum = sum ^ bitlist[x]\n    return sum", "refactored": true, "pred": {"ppl": 10.348865509033203, "ppl_lower": 10.94492244720459, "ppl/lowercase_ppl": -1.0239630697673263, "ppl/zlib": 0.014697339000312424, "Min_5.0% Prob": 12.513899803161621, "Min_10.0% Prob": 11.015494505564371, "Min_20.0% Prob": 8.544711296374981, "Min_30.0% Prob": 6.9565726041793825, "Min_40.0% Prob": 5.739414921173682, "Min_50.0% Prob": 4.687667602842504, "Min_60.0% Prob": 3.9029921459034087}}
{"hexsha": "90681b31e1662f4f8ed10dcfc71a183b60796d93", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef experimental_parallel_interleave_dataset(input_dataset, other_arguments, cycle_length, block_length, sloppy, buffer_output_elements, prefetch_input_elements, f, output_types, output_shapes, name=None):\n    \"\"\"Creates a dataset that applies `f` to the outputs of `input_dataset`.\n\n  The resulting dataset is similar to the `InterleaveDataset`, with the exception\n  that if retrieving the next value from a dataset would cause the requester to\n  block, it will skip that input dataset. This dataset is especially useful\n  when loading data from a variable-latency datastores (e.g. HDFS, GCS), as it\n  allows the training step to proceed so long as some data is available.\n\n  !! WARNING !! This dataset is not deterministic!\n\n  Args:\n    input_dataset: A `Tensor` of type `variant`.\n    other_arguments: A list of `Tensor` objects.\n    cycle_length: A `Tensor` of type `int64`.\n    block_length: A `Tensor` of type `int64`.\n    sloppy: A `Tensor` of type `bool`.\n    buffer_output_elements: A `Tensor` of type `int64`.\n    prefetch_input_elements: A `Tensor` of type `int64`.\n    f: A function decorated with @Defun.\n      A function mapping elements of `input_dataset`, concatenated with\n      `other_arguments`, to a Dataset variant that contains elements matching\n      `output_types` and `output_shapes`.\n    output_types: A list of `tf.DTypes` that has length `>= 1`.\n    output_shapes: A list of shapes (each a `tf.TensorShape` or list of `ints`) that has length `>= 1`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `variant`.\n  \"\"\"\n    _ctx = _context._context or _context.context()\n    tld = _ctx._thread_local_data\n    if tld.is_eager:\n        try:\n            _result = pywrap_tfe.TFE_Py_FastPathExecute(_ctx._context_handle, tld.device_name, 'ExperimentalParallelInterleaveDataset', name, tld.op_callbacks, input_dataset, other_arguments, cycle_length, block_length, sloppy, buffer_output_elements, prefetch_input_elements, 'f', f, 'output_types', output_types, 'output_shapes', output_shapes)\n            return _result\n        except _core._NotOkStatusException as e:\n            _ops.raise_from_not_ok_status(e, name)\n        except _core._FallbackException:\n            pass\n        try:\n            return experimental_parallel_interleave_dataset_eager_fallback(input_dataset, other_arguments, cycle_length, block_length, sloppy, buffer_output_elements, prefetch_input_elements, f=f, output_types=output_types, output_shapes=output_shapes, name=name, ctx=_ctx)\n        except _core._SymbolicException:\n            pass\n    if not isinstance(output_types, (list, tuple)):\n        raise TypeError(\"Expected list for 'output_types' argument to 'experimental_parallel_interleave_dataset' Op, not %r.\" % output_types)\n    output_types = [_execute.make_type(_t, 'output_types') for _t in output_types]\n    if not isinstance(output_shapes, (list, tuple)):\n        raise TypeError(\"Expected list for 'output_shapes' argument to 'experimental_parallel_interleave_dataset' Op, not %r.\" % output_shapes)\n    output_shapes = [_execute.make_shape(_s, 'output_shapes') for _s in output_shapes]\n    _, _, _op, _outputs = _op_def_library._apply_op_helper('ExperimentalParallelInterleaveDataset', input_dataset=input_dataset, other_arguments=other_arguments, cycle_length=cycle_length, block_length=block_length, sloppy=sloppy, buffer_output_elements=buffer_output_elements, prefetch_input_elements=prefetch_input_elements, f=f, output_types=output_types, output_shapes=output_shapes, name=name)\n    _result = _outputs[:]\n    if _execute.must_record_gradient():\n        _attrs = ('f', _op.get_attr('f'), 'Targuments', _op.get_attr('Targuments'), 'output_types', _op.get_attr('output_types'), 'output_shapes', _op.get_attr('output_shapes'))\n        _inputs_flat = _op.inputs\n        _execute.record_gradient('ExperimentalParallelInterleaveDataset', _inputs_flat, _attrs, _result)\n    _result, = _result\n    return _result", "fn_id": 68, "class_fn": false, "repo": "Lube-Project/ProgettoLube", "file": "ProgettoLube/WebInspector/venv/Lib/site-packages/tensorflow/python/ops/gen_experimental_dataset_ops.py", "last_update_at": "2021-01-28T01:57:41+00:00", "original_content": "def experimental_parallel_interleave_dataset(input_dataset, other_arguments, cycle_length, block_length, sloppy, buffer_output_elements, prefetch_input_elements, f, output_types, output_shapes, name=None):\n    \"\"\"Creates a dataset that applies `f` to the outputs of `input_dataset`.\n\n  The resulting dataset is similar to the `InterleaveDataset`, with the exception\n  that if retrieving the next value from a dataset would cause the requester to\n  block, it will skip that input dataset. This dataset is especially useful\n  when loading data from a variable-latency datastores (e.g. HDFS, GCS), as it\n  allows the training step to proceed so long as some data is available.\n\n  !! WARNING !! This dataset is not deterministic!\n\n  Args:\n    input_dataset: A `Tensor` of type `variant`.\n    other_arguments: A list of `Tensor` objects.\n    cycle_length: A `Tensor` of type `int64`.\n    block_length: A `Tensor` of type `int64`.\n    sloppy: A `Tensor` of type `bool`.\n    buffer_output_elements: A `Tensor` of type `int64`.\n    prefetch_input_elements: A `Tensor` of type `int64`.\n    f: A function decorated with @Defun.\n      A function mapping elements of `input_dataset`, concatenated with\n      `other_arguments`, to a Dataset variant that contains elements matching\n      `output_types` and `output_shapes`.\n    output_types: A list of `tf.DTypes` that has length `>= 1`.\n    output_shapes: A list of shapes (each a `tf.TensorShape` or list of `ints`) that has length `>= 1`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `variant`.\n  \"\"\"\n    _ctx = _context._context or _context.context()\n    tld = _ctx._thread_local_data\n    if tld.is_eager:\n        try:\n            _result = pywrap_tfe.TFE_Py_FastPathExecute(_ctx._context_handle, tld.device_name, 'ExperimentalParallelInterleaveDataset', name, tld.op_callbacks, input_dataset, other_arguments, cycle_length, block_length, sloppy, buffer_output_elements, prefetch_input_elements, 'f', f, 'output_types', output_types, 'output_shapes', output_shapes)\n            return _result\n        except _core._NotOkStatusException as e:\n            _ops.raise_from_not_ok_status(e, name)\n        except _core._FallbackException:\n            pass\n        try:\n            return experimental_parallel_interleave_dataset_eager_fallback(input_dataset, other_arguments, cycle_length, block_length, sloppy, buffer_output_elements, prefetch_input_elements, f=f, output_types=output_types, output_shapes=output_shapes, name=name, ctx=_ctx)\n        except _core._SymbolicException:\n            pass\n    if not isinstance(output_types, (list, tuple)):\n        raise TypeError(\"Expected list for 'output_types' argument to 'experimental_parallel_interleave_dataset' Op, not %r.\" % output_types)\n    output_types = [_execute.make_type(_t, 'output_types') for _t in output_types]\n    if not isinstance(output_shapes, (list, tuple)):\n        raise TypeError(\"Expected list for 'output_shapes' argument to 'experimental_parallel_interleave_dataset' Op, not %r.\" % output_shapes)\n    output_shapes = [_execute.make_shape(_s, 'output_shapes') for _s in output_shapes]\n    _, _, _op, _outputs = _op_def_library._apply_op_helper('ExperimentalParallelInterleaveDataset', input_dataset=input_dataset, other_arguments=other_arguments, cycle_length=cycle_length, block_length=block_length, sloppy=sloppy, buffer_output_elements=buffer_output_elements, prefetch_input_elements=prefetch_input_elements, f=f, output_types=output_types, output_shapes=output_shapes, name=name)\n    _result = _outputs[:]\n    if _execute.must_record_gradient():\n        _attrs = ('f', _op.get_attr('f'), 'Targuments', _op.get_attr('Targuments'), 'output_types', _op.get_attr('output_types'), 'output_shapes', _op.get_attr('output_shapes'))\n        _inputs_flat = _op.inputs\n        _execute.record_gradient('ExperimentalParallelInterleaveDataset', _inputs_flat, _attrs, _result)\n    _result, = _result\n    return _result", "refactored": true, "pred": {"ppl": 1.8523396253585815, "ppl_lower": 2.5671374797821045, "ppl/lowercase_ppl": -1.5293895985443562, "ppl/zlib": 0.00047972723924853596, "Min_5.0% Prob": 7.412578994152593, "Min_10.0% Prob": 5.180347461326449, "Min_20.0% Prob": 3.0296651970814255, "Min_30.0% Prob": 2.055998092699869, "Min_40.0% Prob": 1.5412253302506065, "Min_50.0% Prob": 1.2340190018583708, "Min_60.0% Prob": 1.0287448807526123}}
{"hexsha": "5d67dd4c40040dcf2e4bbcd0df1123cacc8a09a7", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_conversion_rate(response_data):\n    data = str(response_data)\n    keyword_conversion = 'conversion_rate'\n    slen = len(keyword_conversion)\n    start_index = data.find(keyword_conversion)\n    extra_len = 9\n    extracted_str = data[start_index:start_index + slen + extra_len]\n    word_list = extracted_str.split(':')\n    conversion_rate = word_list[1]\n    result = float(conversion_rate)\n    loginfo('Conversion Rate = ' + str(result), get_conversion_rate.__name__)\n    return result", "fn_id": 3, "class_fn": false, "repo": "jamesjallorina/currency_exchange", "file": "backend/backend.py", "last_update_at": "2021-12-15T17:54:21+00:00", "original_content": "def get_conversion_rate(response_data):\n    data = str(response_data)\n    keyword_conversion = 'conversion_rate'\n    slen = len(keyword_conversion)\n    start_index = data.find(keyword_conversion)\n    extra_len = 9\n    extracted_str = data[start_index:start_index + slen + extra_len]\n    word_list = extracted_str.split(':')\n    conversion_rate = word_list[1]\n    result = float(conversion_rate)\n    loginfo('Conversion Rate = ' + str(result), get_conversion_rate.__name__)\n    return result", "refactored": true, "pred": {"ppl": 5.721023082733154, "ppl_lower": 5.753458499908447, "ppl/lowercase_ppl": -1.0032414126309084, "ppl/zlib": 0.007118970000064235, "Min_5.0% Prob": 10.852818897792272, "Min_10.0% Prob": 9.85397550037929, "Min_20.0% Prob": 7.413507831507716, "Min_30.0% Prob": 5.575329596346075, "Min_40.0% Prob": 4.313584650472059, "Min_50.0% Prob": 3.475617972680846, "Min_60.0% Prob": 2.9300485255027358}}
{"hexsha": "b740c4349aa6a00f9a664792b9a5d497e887cbae", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize('value,expected', (('', ''), (None, ''), ('very long ' * 100, '')))\n@timeing\n@measure_memory_usage\ndef test_mobile_number_clean(value, expected):\n    field = forms.MobileNumberField(max_length=100)\n    assert field.to_python(value) == expected", "fn_id": 0, "class_fn": false, "repo": "konradko/directory-api", "file": "company/tests/test_forms.py", "last_update_at": "2021-11-06T12:08:26+00:00", "original_content": "@pytest.mark.parametrize('value,expected', (('', ''), (None, ''), ('very long ' * 100, '')))\ndef test_mobile_number_clean(value, expected):\n    field = forms.MobileNumberField(max_length=100)\n    assert field.to_python(value) == expected", "refactored": true, "pred": {"ppl": 8.389090538024902, "ppl_lower": 10.116411209106445, "ppl/lowercase_ppl": -1.0880267215166626, "ppl/zlib": 0.010963567609666044, "Min_5.0% Prob": 13.053225040435791, "Min_10.0% Prob": 11.805144786834717, "Min_20.0% Prob": 9.001498132944107, "Min_30.0% Prob": 6.765356436371803, "Min_40.0% Prob": 5.182068788644039, "Min_50.0% Prob": 4.262541165439094, "Min_60.0% Prob": 3.591316974588803}}
{"hexsha": "eedf072c408246eee339cda55c42a69c694380b4", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef extract_bucket_reference_for_param_reference(template, param_name):\n    param_value = template.get_parameter_default(param_name)\n    if param_value is None:\n        return\n    for resource in template.resources('AWS::Serverless::Function'):\n        policies = resource['Properties'].get('Policies')\n        if policies is None:\n            continue\n        for policy in policies:\n            for statement in policy['Statement']:\n                if param_value not in statement.get('Resource', ''):\n                    continue\n                old_value = statement['Resource']\n                parts = list(old_value.partition(param_value))\n                parts[1] = {'Ref': param_name}\n                new_value = {'Fn::Join': ['', parts]}\n                statement['Resource'] = new_value", "fn_id": 2, "class_fn": false, "repo": "jmespath/jmespath-playground", "file": "template-fixups.py", "last_update_at": "2021-11-24T14:33:45+00:00", "original_content": "def extract_bucket_reference_for_param_reference(template, param_name):\n    param_value = template.get_parameter_default(param_name)\n    if param_value is None:\n        return\n    for resource in template.resources('AWS::Serverless::Function'):\n        policies = resource['Properties'].get('Policies')\n        if policies is None:\n            continue\n        for policy in policies:\n            for statement in policy['Statement']:\n                if param_value not in statement.get('Resource', ''):\n                    continue\n                old_value = statement['Resource']\n                parts = list(old_value.partition(param_value))\n                parts[1] = {'Ref': param_name}\n                new_value = {'Fn::Join': ['', parts]}\n                statement['Resource'] = new_value", "refactored": true, "pred": {"ppl": 4.857515335083008, "ppl_lower": 6.78302001953125, "ppl/lowercase_ppl": -1.211255715095996, "ppl/zlib": 0.004502926094744488, "Min_5.0% Prob": 12.164274639553494, "Min_10.0% Prob": 9.717423809899223, "Min_20.0% Prob": 6.972413665718502, "Min_30.0% Prob": 5.074341358961882, "Min_40.0% Prob": 3.931611571047041, "Min_50.0% Prob": 3.169311952508158, "Min_60.0% Prob": 2.6469032993816115}}
{"hexsha": "6f11be4ebd108a94cb1027e16e858ff097fa14e5", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef text_to_spreadsheet(directory='.', output_file='text_to_sheet.xlsx'):\n    \"\"\"\n    Searches for all text files at the given directory. Each individual\n    text file is converted to a column in the output_file spreadsheet.\n\n    :param str directory: path to directory to search\n    :param str output_file: name of output file\n    \"\"\"\n    path = os.path.abspath(directory)\n    wb = openpyxl.Workbook()\n    wb.create_sheet(title='Text to Columns', index=0)\n    sheet = wb.active\n    bold = Font(bold=True)\n    print(f'Searching for text files...')\n    files = [file for file in os.listdir(path) if file.lower().endswith('.txt')]\n    column = 1\n    print(f'Writing lines of text to columns...')\n    for file in files:\n        row = 2\n        with open(os.path.join(path, file)) as text:\n            sheet.cell(row=1, column=column).value = file\n            sheet.cell(row=1, column=column).font = bold\n            for line in text:\n                sheet.cell(row=row, column=column).value = line\n                row += 1\n        column += 1\n    wb.save(filename=output_file)\n    print(f\"Resulting file saved as '{output_file}'\")", "fn_id": 0, "class_fn": false, "repo": "zspatter/automate-the-boring-stuff", "file": "text_to_spreadsheet/text_to_spreadsheet.py", "last_update_at": "2021-09-05T20:19:40+00:00", "original_content": "def text_to_spreadsheet(directory='.', output_file='text_to_sheet.xlsx'):\n    \"\"\"\n    Searches for all text files at the given directory. Each individual\n    text file is converted to a column in the output_file spreadsheet.\n\n    :param str directory: path to directory to search\n    :param str output_file: name of output file\n    \"\"\"\n    path = os.path.abspath(directory)\n    wb = openpyxl.Workbook()\n    wb.create_sheet(title='Text to Columns', index=0)\n    sheet = wb.active\n    bold = Font(bold=True)\n    print(f'Searching for text files...')\n    files = [file for file in os.listdir(path) if file.lower().endswith('.txt')]\n    column = 1\n    print(f'Writing lines of text to columns...')\n    for file in files:\n        row = 2\n        with open(os.path.join(path, file)) as text:\n            sheet.cell(row=1, column=column).value = file\n            sheet.cell(row=1, column=column).font = bold\n            for line in text:\n                sheet.cell(row=row, column=column).value = line\n                row += 1\n        column += 1\n    wb.save(filename=output_file)\n    print(f\"Resulting file saved as '{output_file}'\")", "refactored": true, "pred": {"ppl": 3.8699355125427246, "ppl_lower": 4.356956481933594, "ppl/lowercase_ppl": -1.0875942954885267, "ppl/zlib": 0.002558105564229114, "Min_5.0% Prob": 9.871382586161296, "Min_10.0% Prob": 8.252975279285062, "Min_20.0% Prob": 5.854357223356923, "Min_30.0% Prob": 4.349454497778288, "Min_40.0% Prob": 3.359975857600089, "Min_50.0% Prob": 2.70215971445845, "Min_60.0% Prob": 2.2548719558123778}}
{"hexsha": "1b7c720289e99957b1d1e5c9b70d89bb7e355da1", "ext": "py", "lang": "Python", "content": "@tf.function\n@timeing\n@measure_memory_usage\ndef train_step(x_target, y_target, model, optimizer, model_loss):\n    with tf.GradientTape() as model_tape:\n        y_predic = model(x_target, training=True)\n        loss = model_loss(y_target, y_predic)\n    gradients_of_model = model_tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients_of_model, model.trainable_variables))\n    return loss", "fn_id": 1, "class_fn": false, "repo": "awagot/CNN-POD", "file": "training/training.py", "last_update_at": "2021-08-18T11:23:05+00:00", "original_content": "@tf.function\ndef train_step(x_target, y_target, model, optimizer, model_loss):\n    with tf.GradientTape() as model_tape:\n        y_predic = model(x_target, training=True)\n        loss = model_loss(y_target, y_predic)\n    gradients_of_model = model_tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients_of_model, model.trainable_variables))\n    return loss", "refactored": true, "pred": {"ppl": 2.8168344497680664, "ppl_lower": 3.538433790206909, "ppl/lowercase_ppl": -1.2202273628152962, "ppl/zlib": 0.004707335085872118, "Min_5.0% Prob": 9.70301882425944, "Min_10.0% Prob": 8.380165795485178, "Min_20.0% Prob": 4.9064947652816775, "Min_30.0% Prob": 3.4568978789690377, "Min_40.0% Prob": 2.5847027439624073, "Min_50.0% Prob": 2.0870931654597724, "Min_60.0% Prob": 1.7258104161483545}}
{"hexsha": "49f5a6884183e7a4dcbfcde101a491ad27898026", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _get_eval_config_from_service_classification(classification: configuration_pb2.ClassificationProblemSpec, eval_config: model_evaluation_pb2.EvaluationConfig) -> None:\n    if classification.HasField('ground_truth_column_spec'):\n        eval_config.data_spec.label_key_spec.CopyFrom(classification.ground_truth_column_spec)\n    if classification.HasField('example_weight_column_spec'):\n        eval_config.data_spec.example_weight_key_spec.CopyFrom(classification.example_weight_column_spec)\n    if classification.HasField('prediction_score_column_spec'):\n        eval_config.data_spec.predicted_score_key_spec.CopyFrom(classification.prediction_score_column_spec)\n    if classification.HasField('prediction_label_column_spec'):\n        eval_config.data_spec.predicted_label_key_spec.CopyFrom(classification.prediction_label_column_spec)\n    if classification.HasField('prediction_id_column_spec'):\n        eval_config.data_spec.predicted_label_id_key_spec.CopyFrom(classification.prediction_id_column_spec)\n    eval_config.data_spec.labels.extend(classification.class_names)\n    num_classes = len(classification.class_names)\n    if classification.type == configuration_pb2.ClassificationProblemSpec.MULTICLASS:\n        problem_type = constants.ProblemType.MULTICLASS\n    elif classification.type == configuration_pb2.ClassificationProblemSpec.MULTILABEL:\n        problem_type = constants.ProblemType.MULTILABEL\n    else:\n        raise NotImplementedError('Classification type %r not implemented' % classification.type)\n    adapter = tfma_adapter.TFMAToME(class_name_list=list(classification.class_names))\n    tfma_metric_specs = _get_metric_specs(problem_type, list(classification.class_names), list(classification.evaluation_options.positive_classes), list(classification.evaluation_options.top_k_list))\n    for tfma_metric_spec in tfma_metric_specs:\n        eval_config.metrics_specs.append(adapter.metrics_spec(tfma_metric_spec))", "fn_id": 3, "class_fn": false, "repo": "tomar27/pipelines", "file": "components/google-cloud/google_cloud_pipeline_components/experimental/evaluation/flex/lib/config.py", "last_update_at": "2021-10-23T00:39:47+00:00", "original_content": "def _get_eval_config_from_service_classification(classification: configuration_pb2.ClassificationProblemSpec, eval_config: model_evaluation_pb2.EvaluationConfig) -> None:\n    if classification.HasField('ground_truth_column_spec'):\n        eval_config.data_spec.label_key_spec.CopyFrom(classification.ground_truth_column_spec)\n    if classification.HasField('example_weight_column_spec'):\n        eval_config.data_spec.example_weight_key_spec.CopyFrom(classification.example_weight_column_spec)\n    if classification.HasField('prediction_score_column_spec'):\n        eval_config.data_spec.predicted_score_key_spec.CopyFrom(classification.prediction_score_column_spec)\n    if classification.HasField('prediction_label_column_spec'):\n        eval_config.data_spec.predicted_label_key_spec.CopyFrom(classification.prediction_label_column_spec)\n    if classification.HasField('prediction_id_column_spec'):\n        eval_config.data_spec.predicted_label_id_key_spec.CopyFrom(classification.prediction_id_column_spec)\n    eval_config.data_spec.labels.extend(classification.class_names)\n    num_classes = len(classification.class_names)\n    if classification.type == configuration_pb2.ClassificationProblemSpec.MULTICLASS:\n        problem_type = constants.ProblemType.MULTICLASS\n    elif classification.type == configuration_pb2.ClassificationProblemSpec.MULTILABEL:\n        problem_type = constants.ProblemType.MULTILABEL\n    else:\n        raise NotImplementedError('Classification type %r not implemented' % classification.type)\n    adapter = tfma_adapter.TFMAToME(class_name_list=list(classification.class_names))\n    tfma_metric_specs = _get_metric_specs(problem_type, list(classification.class_names), list(classification.evaluation_options.positive_classes), list(classification.evaluation_options.top_k_list))\n    for tfma_metric_spec in tfma_metric_specs:\n        eval_config.metrics_specs.append(adapter.metrics_spec(tfma_metric_spec))", "refactored": true, "pred": {"ppl": 2.7557671070098877, "ppl_lower": 3.3257408142089844, "ppl/lowercase_ppl": -1.1854566197360041, "ppl/zlib": 0.0019235215286395539, "Min_5.0% Prob": 9.289218044281006, "Min_10.0% Prob": 7.302882764853683, "Min_20.0% Prob": 4.746270833084885, "Min_30.0% Prob": 3.352313597093929, "Min_40.0% Prob": 2.5329772686603866, "Min_50.0% Prob": 2.0265034222145126, "Min_60.0% Prob": 1.6926093361898804}}
{"hexsha": "02819a01b041692ac69ff145ea5f13a590fc9b34", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _guess_package_name(file_type, file_name):\n    if not file_type:\n        return None\n    file_name = file_name.lower()\n    if 'Bourne-Again' in file_type or 'bash' in file_type:\n        return 'bash'\n    elif 'Mach-O' in file_type and 'executable' in file_type:\n        return 'macho'\n    elif 'directory' in file_type or (file_name.endswith('.app') or file_name.endswith('.app/')) or (file_name.endswith('.pkg') or file_name.endswith('.pkg/')):\n        return 'app'\n    elif 'Zip archive' in file_type and file_name.endswith('.zip'):\n        return 'zip'\n    elif 'PDF' in file_type or file_name.endswith('.pdf'):\n        return 'pdf'\n    elif 'Microsoft Word' in file_type or 'Microsoft Office Word' in file_type or file_name.endswith('.docx') or file_name.endswith('.doc'):\n        return 'doc'\n    elif 'Rich Text Format' in file_type or file_name.endswith('.rtf') or 'property list' in file_type or file_name.endswith('.plist'):\n        return 'rtf'\n    elif 'HTML' in file_type or file_name.endswith('.htm') or file_name.endswith('.html'):\n        return 'html'\n    elif file_name.endswith('.jar'):\n        return 'jar'\n    elif file_name.endswith('.py') or 'Python script' in file_type:\n        return 'python'\n    elif file_name.endswith('.pl') or 'perl script' in file_type.lower():\n        return 'perl'\n    elif file_name.endswith('.dmg'):\n        return 'dmg'\n    else:\n        return 'generic'", "fn_id": 2, "class_fn": false, "repo": "phdphuc/mac-a-mal-cuckoo", "file": "analyzer/darwin/lib/core/packages.py", "last_update_at": "2021-04-07T08:26:25+00:00", "original_content": "def _guess_package_name(file_type, file_name):\n    if not file_type:\n        return None\n    file_name = file_name.lower()\n    if 'Bourne-Again' in file_type or 'bash' in file_type:\n        return 'bash'\n    elif 'Mach-O' in file_type and 'executable' in file_type:\n        return 'macho'\n    elif 'directory' in file_type or (file_name.endswith('.app') or file_name.endswith('.app/')) or (file_name.endswith('.pkg') or file_name.endswith('.pkg/')):\n        return 'app'\n    elif 'Zip archive' in file_type and file_name.endswith('.zip'):\n        return 'zip'\n    elif 'PDF' in file_type or file_name.endswith('.pdf'):\n        return 'pdf'\n    elif 'Microsoft Word' in file_type or 'Microsoft Office Word' in file_type or file_name.endswith('.docx') or file_name.endswith('.doc'):\n        return 'doc'\n    elif 'Rich Text Format' in file_type or file_name.endswith('.rtf') or 'property list' in file_type or file_name.endswith('.plist'):\n        return 'rtf'\n    elif 'HTML' in file_type or file_name.endswith('.htm') or file_name.endswith('.html'):\n        return 'html'\n    elif file_name.endswith('.jar'):\n        return 'jar'\n    elif file_name.endswith('.py') or 'Python script' in file_type:\n        return 'python'\n    elif file_name.endswith('.pl') or 'perl script' in file_type.lower():\n        return 'perl'\n    elif file_name.endswith('.dmg'):\n        return 'dmg'\n    else:\n        return 'generic'", "refactored": true, "pred": {"ppl": 2.3595731258392334, "ppl_lower": 2.4433746337890625, "ppl/lowercase_ppl": -1.0406525221234233, "ppl/zlib": 0.002048880009057948, "Min_5.0% Prob": 8.88417112827301, "Min_10.0% Prob": 6.419783508777618, "Min_20.0% Prob": 4.064992724377432, "Min_30.0% Prob": 2.834354918633328, "Min_40.0% Prob": 2.1466126902703127, "Min_50.0% Prob": 1.7200322651952578, "Min_60.0% Prob": 1.432898185183104}}
{"hexsha": "6576548aed0db505fcd1e0ff4f67a6208131321a", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef login0(auth=None):\n    \"\"\"Handle secure login for performance and stress testing.\n\n    Signature is the signature of email value with the application key.\n    \"\"\"\n    if not auth:\n        auth = request.headers.get('Authorization')\n        if not auth:\n            resp = Response()\n            resp.headers['WWW-Authenticate'] = 'Basic realm=\"Access to the load-testing login\"'\n            resp.status_code = 401\n            return resp\n        if ':' not in auth:\n            auth = base64.b64decode(auth).decode()\n    email, signature = auth.split(':')\n    s = Signer(app.secret_key)\n    if s.validate(email + '.' + signature):\n        try:\n            u = User.get(email=email)\n            login_user(u)\n            return redirect(get_next_url() or url_for('index'))\n        except User.DoesNotExist:\n            return handle_login()\n    abort(403)", "fn_id": 7, "class_fn": false, "repo": "tenet-ac-za/NZ-ORCID-Hub", "file": "orcid_hub/authcontroller.py", "last_update_at": "2021-07-22T08:53:40+00:00", "original_content": "def login0(auth=None):\n    \"\"\"Handle secure login for performance and stress testing.\n\n    Signature is the signature of email value with the application key.\n    \"\"\"\n    if not auth:\n        auth = request.headers.get('Authorization')\n        if not auth:\n            resp = Response()\n            resp.headers['WWW-Authenticate'] = 'Basic realm=\"Access to the load-testing login\"'\n            resp.status_code = 401\n            return resp\n        if ':' not in auth:\n            auth = base64.b64decode(auth).decode()\n    email, signature = auth.split(':')\n    s = Signer(app.secret_key)\n    if s.validate(email + '.' + signature):\n        try:\n            u = User.get(email=email)\n            login_user(u)\n            return redirect(get_next_url() or url_for('index'))\n        except User.DoesNotExist:\n            return handle_login()\n    abort(403)", "refactored": true, "pred": {"ppl": 6.383052349090576, "ppl_lower": 8.661578178405762, "ppl/lowercase_ppl": -1.1646757088326667, "ppl/zlib": 0.004012221661407955, "Min_5.0% Prob": 11.570953195745295, "Min_10.0% Prob": 9.676311601292003, "Min_20.0% Prob": 7.53848572210832, "Min_30.0% Prob": 5.831401425780672, "Min_40.0% Prob": 4.5671097571876915, "Min_50.0% Prob": 3.7092212189327585, "Min_60.0% Prob": 3.100533792579716}}
{"hexsha": "782e47ae6e2a1bc565a53c10fee16a15dc5e46b5", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef readStr_qm9():\n    f = open(current_dir + '/../_dataset/QM9/qm9.smi', 'r')\n    L = []\n    for line in f:\n        line = line.strip()\n        L.append(line)\n    f.close()\n    np.random.seed(1)\n    np.random.shuffle(L)\n    return L", "fn_id": 2, "class_fn": false, "repo": "drigoni/ComparisonsDGM", "file": "_utils/read_dataset.py", "last_update_at": "2021-04-12T13:17:50+00:00", "original_content": "def readStr_qm9():\n    f = open(current_dir + '/../_dataset/QM9/qm9.smi', 'r')\n    L = []\n    for line in f:\n        line = line.strip()\n        L.append(line)\n    f.close()\n    np.random.seed(1)\n    np.random.shuffle(L)\n    return L", "refactored": true, "pred": {"ppl": 6.323818683624268, "ppl_lower": 6.772982120513916, "ppl/lowercase_ppl": -1.0372051009558234, "ppl/zlib": 0.009969314852878608, "Min_5.0% Prob": 11.693699359893799, "Min_10.0% Prob": 10.22490225897895, "Min_20.0% Prob": 7.81195420689053, "Min_30.0% Prob": 5.908224706296568, "Min_40.0% Prob": 4.602721330192354, "Min_50.0% Prob": 3.713149720430374, "Min_60.0% Prob": 3.1028646641169435}}
{"hexsha": "cbd6d9414697897c65c3690922a074fdd0eec5b2", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize('method_name', ['from_pandas', 'iter_from_pandas'])\n@timeing\n@measure_memory_usage\ndef test_dynamic_defines_key_fields(pandas_data, method_name):\n    model = PandasToRecordsTransformer(pandas_data, 'MyRecord', key_fields={'key_field'})\n    from_pandas_method = getattr(model, method_name)\n    record = list(from_pandas_method(pandas_data))[0]\n    assert record.key_fields == {'key_field'}", "fn_id": 4, "class_fn": false, "repo": "AbsaOSS/py2k", "file": "tests/test_models.py", "last_update_at": "2021-09-08T12:33:46+00:00", "original_content": "@pytest.mark.parametrize('method_name', ['from_pandas', 'iter_from_pandas'])\ndef test_dynamic_defines_key_fields(pandas_data, method_name):\n    model = PandasToRecordsTransformer(pandas_data, 'MyRecord', key_fields={'key_field'})\n    from_pandas_method = getattr(model, method_name)\n    record = list(from_pandas_method(pandas_data))[0]\n    assert record.key_fields == {'key_field'}", "refactored": true, "pred": {"ppl": 8.853073120117188, "ppl_lower": 9.920230865478516, "ppl/lowercase_ppl": -1.0521888274903866, "ppl/zlib": 0.00944053958422145, "Min_5.0% Prob": 12.953665256500244, "Min_10.0% Prob": 11.234573364257812, "Min_20.0% Prob": 8.537353237469992, "Min_30.0% Prob": 6.726263019773695, "Min_40.0% Prob": 5.377172087629636, "Min_50.0% Prob": 4.329383465110278, "Min_60.0% Prob": 3.637965854920753}}
{"hexsha": "6b441b738c1e2efec58e60abe442cbe344419f54", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef avro_schema(avsc: Union[dict, str]) -> dict:\n    \"\"\" Create avro schema from dictionary or filepath string \"\"\"\n    logging.info('Parsing avro schema')\n    if isinstance(avsc, dict):\n        avsc = avro.schema.parse_schema(avsc)\n    elif isinstance(avsc, str):\n        avsc = avro.schema.load_schema(avsc)\n    return avsc", "fn_id": 3, "class_fn": false, "repo": "staylorx/cupyopt", "file": "src/cupyopt/nuggets/schema.py", "last_update_at": "2021-03-12T20:46:34+00:00", "original_content": "def avro_schema(avsc: Union[dict, str]) -> dict:\n    \"\"\" Create avro schema from dictionary or filepath string \"\"\"\n    logging.info('Parsing avro schema')\n    if isinstance(avsc, dict):\n        avsc = avro.schema.parse_schema(avsc)\n    elif isinstance(avsc, str):\n        avsc = avro.schema.load_schema(avsc)\n    return avsc", "refactored": true, "pred": {"ppl": 6.23188591003418, "ppl_lower": 6.673996448516846, "ppl/lowercase_ppl": -1.0374600390899524, "ppl/zlib": 0.009240803036776948, "Min_5.0% Prob": 9.803372764587403, "Min_10.0% Prob": 8.828133392333985, "Min_20.0% Prob": 6.857301568984985, "Min_30.0% Prob": 5.543852229272166, "Min_40.0% Prob": 4.518235629651604, "Min_50.0% Prob": 3.6404315433823147, "Min_60.0% Prob": 3.064284430516343}}
{"hexsha": "f4365a09ba1797e4fafc8682981c9fb04235a5c1", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize('space_group', ['P2', 'P3', 'P6', 'R3:h', 'I23'][:])\n@timeing\n@measure_memory_usage\ndef test_determine_space_group(space_group):\n    sgi = sgtbx.space_group_info(symbol=space_group)\n    sg = sgi.group()\n    cs = sgi.any_compatible_crystal_symmetry(volume=10000)\n    cs = cs.best_cell()\n    cs = cs.minimum_cell()\n    intensities = generate_fake_intensities(cs)\n    result = LaueGroupAnalysis([intensities], normalisation=None)\n    print(result)\n    assert result.best_solution.subgroup['best_subsym'].space_group() == sg.build_derived_patterson_group()\n    assert result.best_solution.likelihood > 0.8\n    for score in result.subgroup_scores[1:]:\n        assert score.likelihood < 0.1", "fn_id": 1, "class_fn": false, "repo": "TiankunZhou/dials", "file": "algorithms/symmetry/test_laue_group.py", "last_update_at": "2021-11-18T04:20:54+00:00", "original_content": "@pytest.mark.parametrize('space_group', ['P2', 'P3', 'P6', 'R3:h', 'I23'][:])\ndef test_determine_space_group(space_group):\n    sgi = sgtbx.space_group_info(symbol=space_group)\n    sg = sgi.group()\n    cs = sgi.any_compatible_crystal_symmetry(volume=10000)\n    cs = cs.best_cell()\n    cs = cs.minimum_cell()\n    intensities = generate_fake_intensities(cs)\n    result = LaueGroupAnalysis([intensities], normalisation=None)\n    print(result)\n    assert result.best_solution.subgroup['best_subsym'].space_group() == sg.build_derived_patterson_group()\n    assert result.best_solution.likelihood > 0.8\n    for score in result.subgroup_scores[1:]:\n        assert score.likelihood < 0.1", "refactored": true, "pred": {"ppl": 4.9851298332214355, "ppl_lower": 5.617631435394287, "ppl/lowercase_ppl": -1.0743564828847427, "ppl/zlib": 0.004238679281934736, "Min_5.0% Prob": 9.712658015164463, "Min_10.0% Prob": 8.187389062798541, "Min_20.0% Prob": 6.150150999109796, "Min_30.0% Prob": 4.807757025033656, "Min_40.0% Prob": 3.8788525785537478, "Min_50.0% Prob": 3.1762861154847224, "Min_60.0% Prob": 2.6669136186615683}}
{"hexsha": "3f0ef81e04475f7801a661ee804b85c33aca42e7", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_installed_packages():\n    reqs = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze'])\n    installed_packages = [r.decode().split('==')[0] for r in reqs.split()]\n    return installed_packages", "fn_id": 0, "class_fn": false, "repo": "vymana/nlpwiz", "file": "nlpwiz/utils/pkg_utils.py", "last_update_at": "2021-04-20T18:46:52+00:00", "original_content": "def get_installed_packages():\n    reqs = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze'])\n    installed_packages = [r.decode().split('==')[0] for r in reqs.split()]\n    return installed_packages", "refactored": true, "pred": {"ppl": 3.032872438430786, "ppl_lower": 3.032872438430786, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.00656514893453056, "Min_5.0% Prob": 10.369850158691406, "Min_10.0% Prob": 8.622753938039144, "Min_20.0% Prob": 5.569402098655701, "Min_30.0% Prob": 3.7893343143165112, "Min_40.0% Prob": 2.8235662928609937, "Min_50.0% Prob": 2.2475349754095078, "Min_60.0% Prob": 1.8662755774197781}}
{"hexsha": "9cb1075cc24b8fabc3e7e078623e7b3ba9e43ca3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef run_sample_whole_range():\n    log_file = '../results/DBEsti_tpcds_100k_all.log'\n    db = DBEst(dataset='tpcds', logger_file=log_file)\n    table = 'store_sales'\n    file = '../data/tpcDs10k/store_sales.csv'\n    num_of_points = {'store_sales': '2685596178'}\n    tableColumnSets = [['ss_list_price', 'ss_wholesale_cost']]\n    db.init_whole_range(file=file, table=table, columnItems=tableColumnSets, num_of_points=num_of_points)\n    db.clear_training_data()\n    db.logger.logger.info('Total size of DBEst is ' + str(db.get_size()) + ' bytes.')", "fn_id": 7, "class_fn": false, "repo": "qingzma/CRegressionRDBM", "file": "dbest/dbestclient.py", "last_update_at": "2021-08-04T06:39:19+00:00", "original_content": "def run_sample_whole_range():\n    log_file = '../results/DBEsti_tpcds_100k_all.log'\n    db = DBEst(dataset='tpcds', logger_file=log_file)\n    table = 'store_sales'\n    file = '../data/tpcDs10k/store_sales.csv'\n    num_of_points = {'store_sales': '2685596178'}\n    tableColumnSets = [['ss_list_price', 'ss_wholesale_cost']]\n    db.init_whole_range(file=file, table=table, columnItems=tableColumnSets, num_of_points=num_of_points)\n    db.clear_training_data()\n    db.logger.logger.info('Total size of DBEst is ' + str(db.get_size()) + ' bytes.')", "refactored": true, "pred": {"ppl": 10.474632263183594, "ppl_lower": 9.838432312011719, "ppl/lowercase_ppl": -0.9733243326025294, "ppl/zlib": 0.007032803470215496, "Min_5.0% Prob": 12.097078895568847, "Min_10.0% Prob": 10.708740854263306, "Min_20.0% Prob": 8.431132805056688, "Min_30.0% Prob": 6.783647414176695, "Min_40.0% Prob": 5.590208744428244, "Min_50.0% Prob": 4.628890377684281, "Min_60.0% Prob": 3.921822414162659}}
{"hexsha": "41a7419f7ecd636b33374eda954170a04c2d6f82", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_cert(client, service, file_path, local_path, remote_cert_path, remote_csr_path):\n    \"\"\"\n    Gets the certificate (sign or auth) from the CA.\n\n    NB! This requires the user to have sudo rights without password prompt.\n    :param client: SSHClient object\n    :param service: str - service type: sign-sign (signing certificates) or sign-auth (authentication certificates)\n    :param file_path: str - local CSR path (input)\n    :param local_path: str - local certificate path (output)\n    :param remote_cert_path: str - remote certificate path (output)\n    :param remote_csr_path: str - remote CSR path (input)\n    :return: None\n    \"\"\"\n    client.exec_command('rm temp*')\n    sftp = client.get_client().open_sftp()\n    sftp.put(file_path, remote_csr_path)\n    client.exec_command('cat ' + remote_csr_path + ' | ' + service + ' > ' + remote_cert_path)\n    time.sleep(3)\n    sftp.get(remote_cert_path, local_path)\n    sftp.close()\n    client.close()", "fn_id": 2, "class_fn": false, "repo": "ria-ee/XTM", "file": "common/xrd-ui-tests-python/tests/xroad_ss_delete_hardware_token_certificate/del_management.py", "last_update_at": "2021-11-08T10:30:35+00:00", "original_content": "def get_cert(client, service, file_path, local_path, remote_cert_path, remote_csr_path):\n    \"\"\"\n    Gets the certificate (sign or auth) from the CA.\n\n    NB! This requires the user to have sudo rights without password prompt.\n    :param client: SSHClient object\n    :param service: str - service type: sign-sign (signing certificates) or sign-auth (authentication certificates)\n    :param file_path: str - local CSR path (input)\n    :param local_path: str - local certificate path (output)\n    :param remote_cert_path: str - remote certificate path (output)\n    :param remote_csr_path: str - remote CSR path (input)\n    :return: None\n    \"\"\"\n    client.exec_command('rm temp*')\n    sftp = client.get_client().open_sftp()\n    sftp.put(file_path, remote_csr_path)\n    client.exec_command('cat ' + remote_csr_path + ' | ' + service + ' > ' + remote_cert_path)\n    time.sleep(3)\n    sftp.get(remote_cert_path, local_path)\n    sftp.close()\n    client.close()", "refactored": true, "pred": {"ppl": 3.9131109714508057, "ppl_lower": 4.282863616943359, "ppl/lowercase_ppl": -1.066178251512705, "ppl/zlib": 0.003256163967809108, "Min_5.0% Prob": 10.239257676260811, "Min_10.0% Prob": 8.000544020107814, "Min_20.0% Prob": 5.7693019935062955, "Min_30.0% Prob": 4.3350967814524966, "Min_40.0% Prob": 3.3831624149211814, "Min_50.0% Prob": 2.7319765575762305, "Min_60.0% Prob": 2.2811836656820517}}
{"hexsha": "2cf7ecb49570612ea3ef106c4439aab58551dc8a", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef render_bokeh_figure(result, state):\n    from bokeh.resources import CDN\n    if 'headers' not in state:\n        state['headers'] = set()\n    state['headers'].update(['<script src=\"%s\" async=\"\"></script>' % CDN.js_files[0], '<link rel=\"stylesheet\" href=\"%s\" type=\"text/css\"/>' % CDN.css_files[0]])\n    from bokeh.embed import components\n    script, div = components(result, CDN)\n    if 'footers' not in state:\n        state['footers'] = list()\n    state['footers'].append(script)\n    return [closing_fence(state['code']), div, state['code']]", "fn_id": 9, "class_fn": false, "repo": "mrocklin/pymarkdown", "file": "pymarkdown/core.py", "last_update_at": "2021-12-25T10:56:53+00:00", "original_content": "def render_bokeh_figure(result, state):\n    from bokeh.resources import CDN\n    if 'headers' not in state:\n        state['headers'] = set()\n    state['headers'].update(['<script src=\"%s\" async=\"\"></script>' % CDN.js_files[0], '<link rel=\"stylesheet\" href=\"%s\" type=\"text/css\"/>' % CDN.css_files[0]])\n    from bokeh.embed import components\n    script, div = components(result, CDN)\n    if 'footers' not in state:\n        state['footers'] = list()\n    state['footers'].append(script)\n    return [closing_fence(state['code']), div, state['code']]", "refactored": true, "pred": {"ppl": 6.23375940322876, "ppl_lower": 6.748620510101318, "ppl/lowercase_ppl": -1.0433658000352743, "ppl/zlib": 0.0059222640333838165, "Min_5.0% Prob": 12.773223280906677, "Min_10.0% Prob": 10.262203305959702, "Min_20.0% Prob": 7.411751061677933, "Min_30.0% Prob": 5.717082751040556, "Min_40.0% Prob": 4.54873588681221, "Min_50.0% Prob": 3.6520586820637306, "Min_60.0% Prob": 3.0605615490990483}}
{"hexsha": "17fa8f3d90779608a0fa731c756323ee0bb02290", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef set_up_outputs(OutputObj):\n    OutputObj.add_output('lead_snps_matching_quality_file', 'lead_snps_matching_quality.tsv', add_root=True)\n    OutputObj.add_output('ldscore_for_expanded_control_sets_quality_file', 'ldscore_matching_quality_for_ldexpanded_sets.tsv', add_root=True)\n    OutputObj.add_output('ldsnp_coverage_by_input_snp', 'ldsnp_coverage_by_input_snp.tsv', add_root=True)\n    return OutputObj", "fn_id": 6, "class_fn": false, "repo": "abraham-abin13/gsel_vec", "file": "gsel_vec/scripts/check_ld_expanded_control_sets.py", "last_update_at": "2021-07-22T23:14:33+00:00", "original_content": "def set_up_outputs(OutputObj):\n    OutputObj.add_output('lead_snps_matching_quality_file', 'lead_snps_matching_quality.tsv', add_root=True)\n    OutputObj.add_output('ldscore_for_expanded_control_sets_quality_file', 'ldscore_matching_quality_for_ldexpanded_sets.tsv', add_root=True)\n    OutputObj.add_output('ldsnp_coverage_by_input_snp', 'ldsnp_coverage_by_input_snp.tsv', add_root=True)\n    return OutputObj", "refactored": true, "pred": {"ppl": 9.099886894226074, "ppl_lower": 10.34755802154541, "ppl/lowercase_ppl": -1.0581853822613685, "ppl/zlib": 0.011441771939065468, "Min_5.0% Prob": 12.059503419058663, "Min_10.0% Prob": 11.01242898305257, "Min_20.0% Prob": 8.789998149871826, "Min_30.0% Prob": 6.950635136498345, "Min_40.0% Prob": 5.452261584500472, "Min_50.0% Prob": 4.398880209823449, "Min_60.0% Prob": 3.675557095888588}}
{"hexsha": "44f8ed8d741874b6b1b7ec273d09a2bf1be84c93", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_schemakey():\n    typemap = {'BareAsset': 'Asset', 'PublishedAsset': 'Asset', 'PublishedDandiset': 'Dandiset'}\n    for val in dir(models):\n        if val in ['BaseModel']:\n            continue\n        klass = getattr(models, val)\n        if isinstance(klass, pydantic.main.ModelMetaclass):\n            assert 'schemaKey' in klass.__fields__\n            if val in typemap:\n                assert typemap[val] == klass.__fields__['schemaKey'].default\n            else:\n                assert val == klass.__fields__['schemaKey'].default", "fn_id": 3, "class_fn": false, "repo": "dandi/dandischema", "file": "dandischema/tests/test_models.py", "last_update_at": "2021-09-19T10:56:25+00:00", "original_content": "def test_schemakey():\n    typemap = {'BareAsset': 'Asset', 'PublishedAsset': 'Asset', 'PublishedDandiset': 'Dandiset'}\n    for val in dir(models):\n        if val in ['BaseModel']:\n            continue\n        klass = getattr(models, val)\n        if isinstance(klass, pydantic.main.ModelMetaclass):\n            assert 'schemaKey' in klass.__fields__\n            if val in typemap:\n                assert typemap[val] == klass.__fields__['schemaKey'].default\n            else:\n                assert val == klass.__fields__['schemaKey'].default", "refactored": true, "pred": {"ppl": 6.10493278503418, "ppl_lower": 7.3666510581970215, "ppl/lowercase_ppl": -1.1038452295445487, "ppl/zlib": 0.006750362304774807, "Min_5.0% Prob": 11.051663239796957, "Min_10.0% Prob": 9.569381457108717, "Min_20.0% Prob": 7.341626599982932, "Min_30.0% Prob": 5.746843399071112, "Min_40.0% Prob": 4.5042708938772025, "Min_50.0% Prob": 3.627479433581449, "Min_60.0% Prob": 3.0262668007109537}}
{"hexsha": "2888eb7991df8f7416f4f9dbfa57abb2f621d817", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef plot_mne_circular_connectivity_network(con_mat, labels, perc_conn=0.25, cfc=False, fig=None, subplot=111, fig_title=None, node_name=True, vmax=None, vmin=0, colormap='Blues', facecolor='white', textcolor='black'):\n    from mne.viz import circular_layout, plot_connectivity_circle\n    fig_title = '' if fig_title is None else fig_title\n    node_colors = [label.color for label in labels]\n    label_names = [label.name for label in labels]\n    lh_labels = [name for name in label_names if name.endswith('lh')]\n    rh_labels = [name for name in label_names if name.endswith('rh')]\n    labels_network_sorted, idx_lbl_sort = rearrange_labels_network(labels)\n    label_names_sorted = [label_names[ii] for ii in idx_lbl_sort]\n    lh_labels = [name[:-3] for name in label_names_sorted if name.endswith('lh')]\n    rh_labels = [name[:-3] for name in label_names_sorted if name.endswith('rh')]\n    label_names = [name[:-3] for name in label_names]\n    node_order = lh_labels[::-1] + rh_labels\n    node_angles = circular_layout(label_names, node_order, start_pos=90, group_boundaries=[0, len(label_names) // 2])\n    if not node_name:\n        label_names = [''] * len(label_names)\n    else:\n        label_names = [label.name[13:-3] for label in labels]\n    if perc_conn < 1:\n        n_lines = int(np.prod(con_mat.shape) / 2 * perc_conn)\n    else:\n        n_lines = None\n    if fig is None:\n        fig = plt.figure(num=None, figsize=(8, 8), facecolor='black')\n    if cfc:\n        return plot_connectivity_circle_cfc(con_mat, label_names, n_lines=n_lines, node_angles=node_angles, node_colors=node_colors, title=fig_title, fig=fig, subplot=subplot, vmax=vmax, vmin=vmin, facecolor=facecolor, colormap=colormap, textcolor=textcolor)\n    else:\n        plot_connectivity_circle(con_mat, label_names, n_lines=n_lines, node_angles=node_angles, node_colors=node_colors, title=fig_title, fig=fig, subplot=subplot, vmax=vmax, vmin=vmin, facecolor=facecolor, colormap=colormap, textcolor=textcolor)", "fn_id": 3, "class_fn": false, "repo": "harmonic-minimization/harmoni_manuscript_codes", "file": "tools_connectivity_plot.py", "last_update_at": "2021-12-16T08:00:15+00:00", "original_content": "def plot_mne_circular_connectivity_network(con_mat, labels, perc_conn=0.25, cfc=False, fig=None, subplot=111, fig_title=None, node_name=True, vmax=None, vmin=0, colormap='Blues', facecolor='white', textcolor='black'):\n    from mne.viz import circular_layout, plot_connectivity_circle\n    fig_title = '' if fig_title is None else fig_title\n    node_colors = [label.color for label in labels]\n    label_names = [label.name for label in labels]\n    lh_labels = [name for name in label_names if name.endswith('lh')]\n    rh_labels = [name for name in label_names if name.endswith('rh')]\n    labels_network_sorted, idx_lbl_sort = rearrange_labels_network(labels)\n    label_names_sorted = [label_names[ii] for ii in idx_lbl_sort]\n    lh_labels = [name[:-3] for name in label_names_sorted if name.endswith('lh')]\n    rh_labels = [name[:-3] for name in label_names_sorted if name.endswith('rh')]\n    label_names = [name[:-3] for name in label_names]\n    node_order = lh_labels[::-1] + rh_labels\n    node_angles = circular_layout(label_names, node_order, start_pos=90, group_boundaries=[0, len(label_names) // 2])\n    if not node_name:\n        label_names = [''] * len(label_names)\n    else:\n        label_names = [label.name[13:-3] for label in labels]\n    if perc_conn < 1:\n        n_lines = int(np.prod(con_mat.shape) / 2 * perc_conn)\n    else:\n        n_lines = None\n    if fig is None:\n        fig = plt.figure(num=None, figsize=(8, 8), facecolor='black')\n    if cfc:\n        return plot_connectivity_circle_cfc(con_mat, label_names, n_lines=n_lines, node_angles=node_angles, node_colors=node_colors, title=fig_title, fig=fig, subplot=subplot, vmax=vmax, vmin=vmin, facecolor=facecolor, colormap=colormap, textcolor=textcolor)\n    else:\n        plot_connectivity_circle(con_mat, label_names, n_lines=n_lines, node_angles=node_angles, node_colors=node_colors, title=fig_title, fig=fig, subplot=subplot, vmax=vmax, vmin=vmin, facecolor=facecolor, colormap=colormap, textcolor=textcolor)", "refactored": true, "pred": {"ppl": 2.678821325302124, "ppl_lower": 2.820190668106079, "ppl/lowercase_ppl": -1.0521907931911518, "ppl/zlib": 0.0015113142542774215, "Min_5.0% Prob": 8.219284862279892, "Min_10.0% Prob": 6.60878318639902, "Min_20.0% Prob": 4.550977680316338, "Min_30.0% Prob": 3.2406552421740997, "Min_40.0% Prob": 2.45841675377326, "Min_50.0% Prob": 1.9728400360998566, "Min_60.0% Prob": 1.6455490140668938}}
{"hexsha": "27b7373c43ce0501e65b8ff161bfcf8803e53e2c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef showmesh(node, elem, **kwargs):\n    triangulation = tri.Triangulation(node[:, 0], node[:, 1], elem)\n    markersize = 3000 / len(node)\n    if kwargs.items():\n        h = plt.triplot(triangulation, 'b-h', **kwargs)\n    else:\n        h = plt.triplot(triangulation, 'b-h', linewidth=0.5, alpha=0.5, markersize=markersize)\n    return h", "fn_id": 0, "class_fn": false, "repo": "scaomath/torch-fem", "file": "mesh/utils.py", "last_update_at": "2021-11-14T04:02:54+00:00", "original_content": "def showmesh(node, elem, **kwargs):\n    triangulation = tri.Triangulation(node[:, 0], node[:, 1], elem)\n    markersize = 3000 / len(node)\n    if kwargs.items():\n        h = plt.triplot(triangulation, 'b-h', **kwargs)\n    else:\n        h = plt.triplot(triangulation, 'b-h', linewidth=0.5, alpha=0.5, markersize=markersize)\n    return h", "refactored": true, "pred": {"ppl": 6.216822147369385, "ppl_lower": 6.585853099822998, "ppl/lowercase_ppl": -1.031558205795689, "ppl/zlib": 0.008957151310831706, "Min_5.0% Prob": 12.075821876525879, "Min_10.0% Prob": 10.34622041384379, "Min_20.0% Prob": 7.284596900939942, "Min_30.0% Prob": 5.7029706465231405, "Min_40.0% Prob": 4.466353784799576, "Min_50.0% Prob": 3.669672131538391, "Min_60.0% Prob": 3.0431874244411787}}
{"hexsha": "6e9344b369513894bde3339419e2b7d6be02f344", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef load_character_json(path):\n    with open(path, 'r') as f:\n        raw = f.read()\n        raw = raw.replace('inf,', '999999999,')\n        logging.info('Returning python object')\n        return json.loads(raw)", "fn_id": 1, "class_fn": false, "repo": "trainb0y1/PyMultibound", "file": "editor.py", "last_update_at": "2021-09-18T17:59:47+00:00", "original_content": "def load_character_json(path):\n    with open(path, 'r') as f:\n        raw = f.read()\n        raw = raw.replace('inf,', '999999999,')\n        logging.info('Returning python object')\n        return json.loads(raw)", "refactored": true, "pred": {"ppl": 7.332907199859619, "ppl_lower": 7.397153854370117, "ppl/lowercase_ppl": -1.0043783278549712, "ppl/zlib": 0.011859357463641395, "Min_5.0% Prob": 11.957313537597656, "Min_10.0% Prob": 10.844874245779854, "Min_20.0% Prob": 8.509514872233073, "Min_30.0% Prob": 6.227662366369496, "Min_40.0% Prob": 4.955653947591782, "Min_50.0% Prob": 3.9857380307818713, "Min_60.0% Prob": 3.3181778399840645}}
{"hexsha": "58199be75c2f936778ed896013450ada42119647", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _setup(dataset_dir, label_types=None, classes=None, attrs=None, seed=None, download=False):\n    did_download = False\n    _label_types = _parse_label_types(label_types)\n    if etau.is_str(classes):\n        classes = [classes]\n    if etau.is_str(attrs):\n        attrs = [attrs]\n    if seed is not None:\n        random.seed(seed)\n    classes_map, _did_download = _get_classes_map(dataset_dir, download=download)\n    classes_map_rev = {v: k for k, v in classes_map.items()}\n    did_download |= _did_download\n    all_classes = sorted(classes_map.values())\n    if classes is not None:\n        oi_classes = []\n        missing_classes = []\n        filtered_classes = []\n        for c in classes:\n            if c in classes_map_rev:\n                oi_classes.append(classes_map_rev[c])\n                filtered_classes.append(c)\n            else:\n                missing_classes.append(c)\n        classes = filtered_classes\n        if missing_classes:\n            logger.warning('Ignoring invalid classes %s\\nYou can view the available classes via `fiftyone.utils.openimages.get_classes()`', missing_classes)\n    else:\n        oi_classes = None\n    if 'relationships' in _label_types:\n        attrs_map, _did_download = _get_attrs_map(dataset_dir, download=download)\n        attrs_map_rev = {v: k for k, v in attrs_map.items()}\n        did_download |= _did_download\n        all_attrs = sorted(attrs_map.values())\n        if attrs is None:\n            oi_attrs = [attrs_map_rev[a] for a in all_attrs]\n        else:\n            oi_attrs = []\n            missing_attrs = []\n            filtered_attrs = []\n            for a in attrs:\n                if a in attrs_map_rev:\n                    oi_attrs.append(attrs_map_rev[a])\n                    filtered_attrs.append(a)\n                else:\n                    missing_attrs.append(a)\n            attrs = filtered_attrs\n            if missing_attrs:\n                logger.warning('Ignoring invalid attributes %s\\nYou can view the available attributes via `fiftyone.utils.openimages.get_attributes()`', missing_attrs)\n    else:\n        attrs = None\n        attrs_map = None\n        oi_attrs = None\n        all_attrs = None\n    if 'segmentations' in _label_types:\n        seg_classes, _did_download = _get_seg_classes(dataset_dir, classes_map=classes_map, download=download)\n        did_download |= _did_download\n    else:\n        seg_classes = None\n    return (classes_map, all_classes, classes, oi_classes, attrs_map, all_attrs, attrs, oi_attrs, seg_classes, did_download)", "fn_id": 4, "class_fn": false, "repo": "Fariborzzz/fiftyone", "file": "fiftyone/utils/openimages.py", "last_update_at": "2021-12-17T10:11:37+00:00", "original_content": "def _setup(dataset_dir, label_types=None, classes=None, attrs=None, seed=None, download=False):\n    did_download = False\n    _label_types = _parse_label_types(label_types)\n    if etau.is_str(classes):\n        classes = [classes]\n    if etau.is_str(attrs):\n        attrs = [attrs]\n    if seed is not None:\n        random.seed(seed)\n    classes_map, _did_download = _get_classes_map(dataset_dir, download=download)\n    classes_map_rev = {v: k for k, v in classes_map.items()}\n    did_download |= _did_download\n    all_classes = sorted(classes_map.values())\n    if classes is not None:\n        oi_classes = []\n        missing_classes = []\n        filtered_classes = []\n        for c in classes:\n            if c in classes_map_rev:\n                oi_classes.append(classes_map_rev[c])\n                filtered_classes.append(c)\n            else:\n                missing_classes.append(c)\n        classes = filtered_classes\n        if missing_classes:\n            logger.warning('Ignoring invalid classes %s\\nYou can view the available classes via `fiftyone.utils.openimages.get_classes()`', missing_classes)\n    else:\n        oi_classes = None\n    if 'relationships' in _label_types:\n        attrs_map, _did_download = _get_attrs_map(dataset_dir, download=download)\n        attrs_map_rev = {v: k for k, v in attrs_map.items()}\n        did_download |= _did_download\n        all_attrs = sorted(attrs_map.values())\n        if attrs is None:\n            oi_attrs = [attrs_map_rev[a] for a in all_attrs]\n        else:\n            oi_attrs = []\n            missing_attrs = []\n            filtered_attrs = []\n            for a in attrs:\n                if a in attrs_map_rev:\n                    oi_attrs.append(attrs_map_rev[a])\n                    filtered_attrs.append(a)\n                else:\n                    missing_attrs.append(a)\n            attrs = filtered_attrs\n            if missing_attrs:\n                logger.warning('Ignoring invalid attributes %s\\nYou can view the available attributes via `fiftyone.utils.openimages.get_attributes()`', missing_attrs)\n    else:\n        attrs = None\n        attrs_map = None\n        oi_attrs = None\n        all_attrs = None\n    if 'segmentations' in _label_types:\n        seg_classes, _did_download = _get_seg_classes(dataset_dir, classes_map=classes_map, download=download)\n        did_download |= _did_download\n    else:\n        seg_classes = None\n    return (classes_map, all_classes, classes, oi_classes, attrs_map, all_attrs, attrs, oi_attrs, seg_classes, did_download)", "refactored": true, "pred": {"ppl": 2.1804864406585693, "ppl_lower": 2.3301918506622314, "ppl/lowercase_ppl": -1.0851809183890249, "ppl/zlib": 0.0011314194337100297, "Min_5.0% Prob": 8.843403269262875, "Min_10.0% Prob": 6.3979085718884185, "Min_20.0% Prob": 3.8133885218378376, "Min_30.0% Prob": 2.5892863557469554, "Min_40.0% Prob": 1.9516591758197754, "Min_50.0% Prob": 1.5589049574057845, "Min_60.0% Prob": 1.3004831501269265}}
{"hexsha": "55c73198735716cc50f10f1745bd41508e18f45d", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef count_verbose_skip(func):\n\n    def decorator(obj, *args, **kw):\n        fname = check_parameters(func, args, kw)\n        if obj.is_truncated() or obj.is_canceled() or obj.is_aborted():\n            return 0\n        if obj.testing_syntax:\n            func(obj, *args, calc_time=True, **kw)\n            return 0\n        obj.debug('{} {} {}'.format(fname, args, kw))\n        return func(obj, *args, **kw)\n    return decorator", "fn_id": 0, "class_fn": false, "repo": "ael-noblegas/pychron", "file": "pychron/pyscripts/decorators.py", "last_update_at": "2021-08-17T15:38:24+00:00", "original_content": "def count_verbose_skip(func):\n\n    def decorator(obj, *args, **kw):\n        fname = check_parameters(func, args, kw)\n        if obj.is_truncated() or obj.is_canceled() or obj.is_aborted():\n            return 0\n        if obj.testing_syntax:\n            func(obj, *args, calc_time=True, **kw)\n            return 0\n        obj.debug('{} {} {}'.format(fname, args, kw))\n        return func(obj, *args, **kw)\n    return decorator", "refactored": true, "pred": {"ppl": 8.199784278869629, "ppl_lower": 8.920564651489258, "ppl/lowercase_ppl": -1.040041388561452, "ppl/zlib": 0.009069430372712227, "Min_5.0% Prob": 12.719066619873047, "Min_10.0% Prob": 10.547408764178936, "Min_20.0% Prob": 8.052547051356388, "Min_30.0% Prob": 6.376010741942968, "Min_40.0% Prob": 5.144702870112199, "Min_50.0% Prob": 4.16127176831166, "Min_60.0% Prob": 3.501795750912986}}
{"hexsha": "ae4b7326c41a7e04e0d0c89af9536dff4cc979a0", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef process_ccdlab(output=None, time_list=None, XY_integers=None, XY_fractions=None, flat_list=None, framecount_per_sec=framecount_per_sec):\n    \"\"\"Generate a Curvit compatible events list from CCDLAB files.\n\n    Parameters\n    ----------\n    output : file path\n        The name of the output events list FITS file.\n        \n    time_list : file path\n        The name of the CCDLAB time list FITS file\n        \n    XY_integers : file path\n        The name of the CCDLAB XY integers FITS file\n        \n    XY_fractions : file path\n        The name of the CCDLAB XY fractions FITS file\n        \n    flat_list : file path\n        The name of the CCDLAB flat list FITS file\n        \n    framecount_per_sec : float, optional\n        The framerate of the observation, with a default value of 28.7185\n        frames per second for 512 x 512 window mode. \n        The most accurate way to get the framerate would be to take the value \n        of (``1 / INT_TIME``). \n        ``INT_TIME`` value can be found from the corresponding image header. \n        Approximate values of framerate for different window modes of UVIT \n        are given in the table below.\n\n        +---------------+---------------------+\n        | window mode   | frames per second   |\n        +===============+=====================+\n        | 512 x 512     | 28.7                |\n        +---------------+---------------------+\n        | 350 x 350     | 61                  |\n        +---------------+---------------------+\n        | 300 x 300     | 82                  |\n        +---------------+---------------------+\n        | 250 x 250     | 115                 |\n        +---------------+---------------------+\n        | 200 x 200     | 180                 |\n        +---------------+---------------------+\n        | 150 x 150     | 300                 |\n        +---------------+---------------------+\n        | 100 x 100     | 640                 |\n        +---------------+---------------------+ \n        \n\n    Note\n    ---- \n    It is essential to set the correct value of the framerate. \n    Most UVIT observations are carried out in 512 x 512 window mode.\n            \n    Warning\n    -------\n    This function is new; please report if you find any bugs.\n        \n    Example\n    --------\n    >>> import curvit\n    >>> process_ccdlab(output = 'output_events_list.fits',\n    ...                time_list = 'sample_TimeList.fits', \n    ...                XY_integers = 'sample_XYInts_List.fits',\n    ...                XY_fractions = 'sample_XYFrac_List.fits',\n    ...                flat_list = 'sample_FlatList.fits',\n    ...                framecount_per_sec = 28.7185)\n    \n    The above script will generate a FITS table called ``output_events_list.fits``.\n    You may then use it as input to ``curve`` or ``makecurves``. \n    \"\"\"\n    time = fits.open(time_list)[0].data / 1000\n    XYFrac = fits.open(XY_fractions)[0].data\n    XYInts = fits.open(XY_integers)[0].data\n    weight = fits.open(flat_list)[0].data\n    photons = weight * framecount_per_sec\n    fx = CCDLAB_to_4k(XYInts[:, 0], XYFrac[:, 0])\n    fy = CCDLAB_to_4k(XYInts[:, 1], XYFrac[:, 1])\n    col1 = fits.Column(name='MJD_L2', format='D', array=time)\n    col2 = fits.Column(name='Fx', format='D', array=fx)\n    col3 = fits.Column(name='Fy', format='D', array=fy)\n    col4 = fits.Column(name='EFFECTIVE_NUM_PHOTONS', format='D', array=photons)\n    cols = fits.ColDefs([col1, col2, col3, col4])\n    tbhdu = fits.BinTableHDU.from_columns(cols)\n    tbhdu.writeto(output, overwrite=True)\n    return", "fn_id": 14, "class_fn": false, "repo": "prajwel/curvit", "file": "curvit/curvit.py", "last_update_at": "2021-11-15T12:01:29+00:00", "original_content": "def process_ccdlab(output=None, time_list=None, XY_integers=None, XY_fractions=None, flat_list=None, framecount_per_sec=framecount_per_sec):\n    \"\"\"Generate a Curvit compatible events list from CCDLAB files.\n\n    Parameters\n    ----------\n    output : file path\n        The name of the output events list FITS file.\n        \n    time_list : file path\n        The name of the CCDLAB time list FITS file\n        \n    XY_integers : file path\n        The name of the CCDLAB XY integers FITS file\n        \n    XY_fractions : file path\n        The name of the CCDLAB XY fractions FITS file\n        \n    flat_list : file path\n        The name of the CCDLAB flat list FITS file\n        \n    framecount_per_sec : float, optional\n        The framerate of the observation, with a default value of 28.7185\n        frames per second for 512 x 512 window mode. \n        The most accurate way to get the framerate would be to take the value \n        of (``1 / INT_TIME``). \n        ``INT_TIME`` value can be found from the corresponding image header. \n        Approximate values of framerate for different window modes of UVIT \n        are given in the table below.\n\n        +---------------+---------------------+\n        | window mode   | frames per second   |\n        +===============+=====================+\n        | 512 x 512     | 28.7                |\n        +---------------+---------------------+\n        | 350 x 350     | 61                  |\n        +---------------+---------------------+\n        | 300 x 300     | 82                  |\n        +---------------+---------------------+\n        | 250 x 250     | 115                 |\n        +---------------+---------------------+\n        | 200 x 200     | 180                 |\n        +---------------+---------------------+\n        | 150 x 150     | 300                 |\n        +---------------+---------------------+\n        | 100 x 100     | 640                 |\n        +---------------+---------------------+ \n        \n\n    Note\n    ---- \n    It is essential to set the correct value of the framerate. \n    Most UVIT observations are carried out in 512 x 512 window mode.\n            \n    Warning\n    -------\n    This function is new; please report if you find any bugs.\n        \n    Example\n    --------\n    >>> import curvit\n    >>> process_ccdlab(output = 'output_events_list.fits',\n    ...                time_list = 'sample_TimeList.fits', \n    ...                XY_integers = 'sample_XYInts_List.fits',\n    ...                XY_fractions = 'sample_XYFrac_List.fits',\n    ...                flat_list = 'sample_FlatList.fits',\n    ...                framecount_per_sec = 28.7185)\n    \n    The above script will generate a FITS table called ``output_events_list.fits``.\n    You may then use it as input to ``curve`` or ``makecurves``. \n    \"\"\"\n    time = fits.open(time_list)[0].data / 1000\n    XYFrac = fits.open(XY_fractions)[0].data\n    XYInts = fits.open(XY_integers)[0].data\n    weight = fits.open(flat_list)[0].data\n    photons = weight * framecount_per_sec\n    fx = CCDLAB_to_4k(XYInts[:, 0], XYFrac[:, 0])\n    fy = CCDLAB_to_4k(XYInts[:, 1], XYFrac[:, 1])\n    col1 = fits.Column(name='MJD_L2', format='D', array=time)\n    col2 = fits.Column(name='Fx', format='D', array=fx)\n    col3 = fits.Column(name='Fy', format='D', array=fy)\n    col4 = fits.Column(name='EFFECTIVE_NUM_PHOTONS', format='D', array=photons)\n    cols = fits.ColDefs([col1, col2, col3, col4])\n    tbhdu = fits.BinTableHDU.from_columns(cols)\n    tbhdu.writeto(output, overwrite=True)\n    return", "refactored": true, "pred": {"ppl": 3.4539177417755127, "ppl_lower": 3.6943986415863037, "ppl/lowercase_ppl": -1.054302646388946, "ppl/zlib": 0.0010676220190260802, "Min_5.0% Prob": 9.799734542767206, "Min_10.0% Prob": 7.924515416224797, "Min_20.0% Prob": 5.522413917773746, "Min_30.0% Prob": 4.022969361420335, "Min_40.0% Prob": 3.0863691445160897, "Min_50.0% Prob": 2.4771425008527506, "Min_60.0% Prob": 2.068405169325656}}
{"hexsha": "7d9f104ac0e3008ea503327f64672715cc84a452", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef encode_routing_info(r_tags):\n    result = bitstring.BitArray()\n    for route in r_tags:\n        result.append(bitstring.pack('uint:8', len(route)))\n        for step in route:\n            pubkey, channel, feebase, feerate, cltv = step\n            result.append(bitstring.BitArray(pubkey) + bitstring.BitArray(channel) + bitstring.pack('intbe:32', feebase) + bitstring.pack('intbe:32', feerate) + bitstring.pack('intbe:16', cltv))\n    return result.tobytes()", "fn_id": 0, "class_fn": false, "repo": "RonSherfey/electrum", "file": "electrum/trampoline.py", "last_update_at": "2021-04-22T07:51:24+00:00", "original_content": "def encode_routing_info(r_tags):\n    result = bitstring.BitArray()\n    for route in r_tags:\n        result.append(bitstring.pack('uint:8', len(route)))\n        for step in route:\n            pubkey, channel, feebase, feerate, cltv = step\n            result.append(bitstring.BitArray(pubkey) + bitstring.BitArray(channel) + bitstring.pack('intbe:32', feebase) + bitstring.pack('intbe:32', feerate) + bitstring.pack('intbe:16', cltv))\n    return result.tobytes()", "refactored": true, "pred": {"ppl": 3.736351490020752, "ppl_lower": 3.8395543098449707, "ppl/lowercase_ppl": -1.0206710404892738, "ppl/zlib": 0.005561643872795387, "Min_5.0% Prob": 10.196425301688057, "Min_10.0% Prob": 8.10810276667277, "Min_20.0% Prob": 5.848833592732747, "Min_30.0% Prob": 4.25296019082484, "Min_40.0% Prob": 3.2856432121308123, "Min_50.0% Prob": 2.6283394250196297, "Min_60.0% Prob": 2.205076169637639}}
{"hexsha": "a016e584bf0e1d2da7b3d8766cb1e7cb07b0e46c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef preprocess(paragraph):\n    result = []\n    sentences = tokenizer.tokenize(paragraph)\n    for sentence in sentences:\n        words = nltk.regexp_tokenize(sentence, pattern)\n        temp = []\n        for word in words:\n            toDeal = []\n            if camelCase1.match(word) or camelCase2.match(word):\n                toDeal = splitCode(word)\n            elif upperExtCase.match(word):\n                toDeal = splitFinalExt(word)\n            else:\n                toDeal.append(word)\n            for deal in toDeal:\n                if not isDelete(deal.lower()):\n                    temp.append(stemmer.stem(deal))\n        result.append(temp)\n    return result", "fn_id": 1, "class_fn": false, "repo": "anonym-user-1/ICSME2021", "file": "preprocessor.py", "last_update_at": "2021-07-01T17:13:10+00:00", "original_content": "def preprocess(paragraph):\n    result = []\n    sentences = tokenizer.tokenize(paragraph)\n    for sentence in sentences:\n        words = nltk.regexp_tokenize(sentence, pattern)\n        temp = []\n        for word in words:\n            toDeal = []\n            if camelCase1.match(word) or camelCase2.match(word):\n                toDeal = splitCode(word)\n            elif upperExtCase.match(word):\n                toDeal = splitFinalExt(word)\n            else:\n                toDeal.append(word)\n            for deal in toDeal:\n                if not isDelete(deal.lower()):\n                    temp.append(stemmer.stem(deal))\n        result.append(temp)\n    return result", "refactored": true, "pred": {"ppl": 6.083413124084473, "ppl_lower": 6.759363651275635, "ppl/lowercase_ppl": -1.0583544711349204, "ppl/zlib": 0.00612056239851604, "Min_5.0% Prob": 12.767431735992432, "Min_10.0% Prob": 10.831964135169983, "Min_20.0% Prob": 7.817185990512371, "Min_30.0% Prob": 5.845290448516607, "Min_40.0% Prob": 4.477356983581558, "Min_50.0% Prob": 3.6026507436763495, "Min_60.0% Prob": 3.0076521839073393}}
{"hexsha": "225b550790ae56458e3be71c37a81107f822f14e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef find_boost(self):\n    global boost_pos\n    boost_pos = pyAG.locateCenterOnScreen('images/{}.png'.format(self))\n    if boost_pos == None:\n        print('No', self.replace('_', ' '), 'found')\n        return False\n    else:\n        return True", "fn_id": 7, "class_fn": false, "repo": "nebelorz/NST", "file": "NST v1.3 (source)/functions.py", "last_update_at": "2021-11-30T22:09:13+00:00", "original_content": "def find_boost(self):\n    global boost_pos\n    boost_pos = pyAG.locateCenterOnScreen('images/{}.png'.format(self))\n    if boost_pos == None:\n        print('No', self.replace('_', ' '), 'found')\n        return False\n    else:\n        return True", "refactored": true, "pred": {"ppl": 15.200361251831055, "ppl_lower": 18.53984832763672, "ppl/lowercase_ppl": -1.0729804815166748, "ppl/zlib": 0.014322732600724435, "Min_5.0% Prob": 16.36115264892578, "Min_10.0% Prob": 12.88914121900286, "Min_20.0% Prob": 9.747258313496907, "Min_30.0% Prob": 7.9230993444269355, "Min_40.0% Prob": 6.407699803511302, "Min_50.0% Prob": 5.269250822694678, "Min_60.0% Prob": 4.531418855985006}}
{"hexsha": "63b326f4f249f15f3d820d4b7228b3b70a34e6a6", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef write_file(filename, data):\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n    with open(filename, 'w+') as f:\n        print('writing file %s' % filename)\n        f.write(data)", "fn_id": 0, "class_fn": false, "repo": "ahmednofal/DFFRAM", "file": "openlane/rtl/RTL_openlane_flow.py", "last_update_at": "2021-11-08T09:13:45+00:00", "original_content": "def write_file(filename, data):\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n    with open(filename, 'w+') as f:\n        print('writing file %s' % filename)\n        f.write(data)", "refactored": true, "pred": {"ppl": 4.498382568359375, "ppl_lower": 5.387948989868164, "ppl/lowercase_ppl": -1.1200004920350242, "ppl/zlib": 0.009639217326362982, "Min_5.0% Prob": 9.517089207967123, "Min_10.0% Prob": 8.743793169657389, "Min_20.0% Prob": 6.6966232519883375, "Min_30.0% Prob": 4.998529696464539, "Min_40.0% Prob": 3.7727989413672023, "Min_50.0% Prob": 3.0038214663920155, "Min_60.0% Prob": 2.555286697112024}}
{"hexsha": "67b6579ce4e0d25dfadfeb81d73e475c2f423051", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef millify(n):\n    n = float(n)\n    millidx = max(0, min(len(millnames) - 1, int(math.floor(0 if n == 0 else math.log10(abs(n)) / 3))))\n    return '{:.0f}{}'.format(n / 10 ** (3 * millidx), millnames[millidx])", "fn_id": 0, "class_fn": false, "repo": "ZhiruiFeng/CarsMemory", "file": "web/app.py", "last_update_at": "2021-08-13T11:55:56+00:00", "original_content": "def millify(n):\n    n = float(n)\n    millidx = max(0, min(len(millnames) - 1, int(math.floor(0 if n == 0 else math.log10(abs(n)) / 3))))\n    return '{:.0f}{}'.format(n / 10 ** (3 * millidx), millnames[millidx])", "refactored": true, "pred": {"ppl": 2.787692070007324, "ppl_lower": 2.787692070007324, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.005792169708762514, "Min_5.0% Prob": 9.979067325592041, "Min_10.0% Prob": 8.139812098609077, "Min_20.0% Prob": 4.859079310768529, "Min_30.0% Prob": 3.3680028432402116, "Min_40.0% Prob": 2.606864666860355, "Min_50.0% Prob": 2.0704445220762864, "Min_60.0% Prob": 1.7144289187131578}}
{"hexsha": "e3f7b9dd2486d6d82391b6ab2cce6e07c345cb5c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef uninitializePlugin(mobject):\n    mplugin = OpenMayaMPx.MFnPlugin(mobject)\n    try:\n        mplugin.deregisterNode(spSimpleSpringNodeId)\n    except:\n        sys.stderr.write('Failed to deregister node: %s' % kPluginNodeTypeName)\n        raise", "fn_id": 2, "class_fn": false, "repo": "leegoonz/Maya-devkit", "file": "osx/devkit/plug-ins/scripted/simpleSpring.py", "last_update_at": "2021-12-07T07:29:19+00:00", "original_content": "def uninitializePlugin(mobject):\n    mplugin = OpenMayaMPx.MFnPlugin(mobject)\n    try:\n        mplugin.deregisterNode(spSimpleSpringNodeId)\n    except:\n        sys.stderr.write('Failed to deregister node: %s' % kPluginNodeTypeName)\n        raise", "refactored": true, "pred": {"ppl": 5.933295726776123, "ppl_lower": 25.150999069213867, "ppl/lowercase_ppl": -1.811150258512783, "ppl/zlib": 0.009421057303841502, "Min_5.0% Prob": 11.504877408345541, "Min_10.0% Prob": 10.087266649518694, "Min_20.0% Prob": 7.9596222559611, "Min_30.0% Prob": 5.826237883256829, "Min_40.0% Prob": 4.431282018461535, "Min_50.0% Prob": 3.5499844413537245, "Min_60.0% Prob": 3.016036782089783}}
{"hexsha": "67782e299b60e3d6714308f0cda66ce9bb82d1a5", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_depths(flows, x, h, roughness=0.01, slope=0.001, conv=1.0, dd=0.0001, verbose=False):\n    if isinstance(flows, float):\n        flows = np.array([flows], dtype=float)\n    if isinstance(roughness, float):\n        roughness = np.ones(x.shape, dtype=float) * roughness\n    depths = np.zeros(flows.shape, dtype=float)\n    for idx, q in enumerate(flows):\n        depths[idx] = qtodepth(x, h, q, roughness=roughness, slope=slope, conv=conv, dd=dd, verbose=False)\n    return depths", "fn_id": 7, "class_fn": false, "repo": "scharlton2/modflow6", "file": "autotest/scripts/cross_section_functions.py", "last_update_at": "2021-10-08T00:56:20+00:00", "original_content": "def get_depths(flows, x, h, roughness=0.01, slope=0.001, conv=1.0, dd=0.0001, verbose=False):\n    if isinstance(flows, float):\n        flows = np.array([flows], dtype=float)\n    if isinstance(roughness, float):\n        roughness = np.ones(x.shape, dtype=float) * roughness\n    depths = np.zeros(flows.shape, dtype=float)\n    for idx, q in enumerate(flows):\n        depths[idx] = qtodepth(x, h, q, roughness=roughness, slope=slope, conv=conv, dd=dd, verbose=False)\n    return depths", "refactored": true, "pred": {"ppl": 5.704214096069336, "ppl_lower": 6.187378883361816, "ppl/lowercase_ppl": -1.0466954358577767, "ppl/zlib": 0.006801582878053144, "Min_5.0% Prob": 12.513975262641907, "Min_10.0% Prob": 10.33604652741376, "Min_20.0% Prob": 7.062631157466344, "Min_30.0% Prob": 5.38462556554721, "Min_40.0% Prob": 4.227463455711092, "Min_50.0% Prob": 3.4719077185995277, "Min_60.0% Prob": 2.8956407802800337}}
{"hexsha": "6a706ba112e6e0c7b57c1e46bd9b0418402db376", "ext": "pyde", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef draw():\n    loadPixels()\n    x = 4.0\n    y = 0.0\n    for i in range(1, 120000, 1):\n        x1 = b * y + f(x)\n        y = -x + f(x1)\n        x = x1\n        pixels[350 + int(x * 26) + (280 - int(y * 26)) * width] = color(i % 255, 100, 100)\n    updatePixels()", "fn_id": 1, "class_fn": false, "repo": "kantel/processingpy", "file": "sketches/mira/mira.pyde", "last_update_at": "2021-08-18T19:55:15+00:00", "original_content": "def draw():\n    loadPixels()\n    x = 4.0\n    y = 0.0\n    for i in range(1, 120000, 1):\n        x1 = b * y + f(x)\n        y = -x + f(x1)\n        x = x1\n        pixels[350 + int(x * 26) + (280 - int(y * 26)) * width] = color(i % 255, 100, 100)\n    updatePixels()", "refactored": true, "pred": {"ppl": 7.018189907073975, "ppl_lower": 7.742746829986572, "ppl/lowercase_ppl": -1.050423867766613, "ppl/zlib": 0.009992335060482306, "Min_5.0% Prob": 9.779006004333496, "Min_10.0% Prob": 8.551494928506704, "Min_20.0% Prob": 7.05564755659837, "Min_30.0% Prob": 5.586578381061554, "Min_40.0% Prob": 4.605834137718633, "Min_50.0% Prob": 3.8029096131004505, "Min_60.0% Prob": 3.2346657259389757}}
{"hexsha": "704ba9392622d555589cf12d7af04a6f1811b620", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef energy_plot(scenarios, color_dict):\n    \"\"\"\n    \"\"\"\n    layout = go.Layout(barmode='relative', legend_orientation='h', title='Aggregated supply and demand', paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', yaxis=dict(title='Energy in {}'.format('TWh'), titlefont=dict(size=16, color='rgb(107, 107, 107)'), tickfont=dict(size=14, color='rgb(107, 107, 107)')))\n    data = []\n    for idx, row in scenarios.T.iteritems():\n        if '-cos' in idx:\n            legend = False\n        else:\n            legend = True\n        data.append(go.Bar(x=row.index, y=row.values, text=[v.round(1) if v > 20 or v < -20 else None for v in row.values], hovertext=[', '.join([str(v.round(2)), idx.replace('-cos', '')]) for v in row.values], hoverinfo='text', textposition='auto', showlegend=legend, name=idx, marker=dict(color=color_dict.get(idx.replace('-cos', ''), 'gray'))))\n    return {'data': data, 'layout': layout}", "fn_id": 3, "class_fn": false, "repo": "znes/angus-scenarios", "file": "documentation/plotly_plots.py", "last_update_at": "2021-06-02T01:43:57+00:00", "original_content": "def energy_plot(scenarios, color_dict):\n    \"\"\"\n    \"\"\"\n    layout = go.Layout(barmode='relative', legend_orientation='h', title='Aggregated supply and demand', paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', yaxis=dict(title='Energy in {}'.format('TWh'), titlefont=dict(size=16, color='rgb(107, 107, 107)'), tickfont=dict(size=14, color='rgb(107, 107, 107)')))\n    data = []\n    for idx, row in scenarios.T.iteritems():\n        if '-cos' in idx:\n            legend = False\n        else:\n            legend = True\n        data.append(go.Bar(x=row.index, y=row.values, text=[v.round(1) if v > 20 or v < -20 else None for v in row.values], hovertext=[', '.join([str(v.round(2)), idx.replace('-cos', '')]) for v in row.values], hoverinfo='text', textposition='auto', showlegend=legend, name=idx, marker=dict(color=color_dict.get(idx.replace('-cos', ''), 'gray'))))\n    return {'data': data, 'layout': layout}", "refactored": true, "pred": {"ppl": 3.4034264087677, "ppl_lower": 4.202086925506592, "ppl/lowercase_ppl": -1.1721110193086102, "ppl/zlib": 0.002454474331563501, "Min_5.0% Prob": 9.29455053806305, "Min_10.0% Prob": 7.4084958881139755, "Min_20.0% Prob": 5.3833862487226725, "Min_30.0% Prob": 3.9779397615542016, "Min_40.0% Prob": 3.0453063623849737, "Min_50.0% Prob": 2.4537163655973555, "Min_60.0% Prob": 2.0492616513830857}}
{"hexsha": "bb6e599aba13747a581a01c8843fac9626122356", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef show_image_series(image_series: ImageSeries, neurodata_vis_spec: dict):\n    if len(image_series.data.shape) == 3:\n        return show_grayscale_image_series(image_series, neurodata_vis_spec)\n\n    def show_image(index=0, mode='rgb'):\n        fig, ax = plt.subplots(subplot_kw={'xticks': [], 'yticks': []})\n        image = image_series.data[index]\n        if mode == 'bgr':\n            image = image[:, :, ::-1]\n        ax.imshow(image.transpose([1, 0, 2]), cmap='gray', aspect='auto')\n        fig.show()\n        return fig2widget(fig)\n    slider = widgets.IntSlider(value=0, min=0, max=image_series.data.shape[0] - 1, orientation='horizontal', continuous_update=False, description='index')\n    mode = widgets.Dropdown(options=('rgb', 'bgr'), layout=Layout(width='200px'), description='mode')\n    controls = {'index': slider, 'mode': mode}\n    out_fig = widgets.interactive_output(show_image, controls)\n    vbox = widgets.VBox(children=[out_fig, slider, mode])\n    return vbox", "fn_id": 0, "class_fn": false, "repo": "NeurodataWithoutBorders/nwb-jupyter-widgets", "file": "nwbwidgets/image.py", "last_update_at": "2021-11-16T11:50:33+00:00", "original_content": "def show_image_series(image_series: ImageSeries, neurodata_vis_spec: dict):\n    if len(image_series.data.shape) == 3:\n        return show_grayscale_image_series(image_series, neurodata_vis_spec)\n\n    def show_image(index=0, mode='rgb'):\n        fig, ax = plt.subplots(subplot_kw={'xticks': [], 'yticks': []})\n        image = image_series.data[index]\n        if mode == 'bgr':\n            image = image[:, :, ::-1]\n        ax.imshow(image.transpose([1, 0, 2]), cmap='gray', aspect='auto')\n        fig.show()\n        return fig2widget(fig)\n    slider = widgets.IntSlider(value=0, min=0, max=image_series.data.shape[0] - 1, orientation='horizontal', continuous_update=False, description='index')\n    mode = widgets.Dropdown(options=('rgb', 'bgr'), layout=Layout(width='200px'), description='mode')\n    controls = {'index': slider, 'mode': mode}\n    out_fig = widgets.interactive_output(show_image, controls)\n    vbox = widgets.VBox(children=[out_fig, slider, mode])\n    return vbox", "refactored": true, "pred": {"ppl": 3.3957526683807373, "ppl_lower": 4.316339015960693, "ppl/lowercase_ppl": -1.1962185411183746, "ppl/zlib": 0.002445050871040537, "Min_5.0% Prob": 9.147102419535319, "Min_10.0% Prob": 7.474539279937744, "Min_20.0% Prob": 5.244337515752823, "Min_30.0% Prob": 3.958944186404511, "Min_40.0% Prob": 3.032914383062085, "Min_50.0% Prob": 2.4479485523985014, "Min_60.0% Prob": 2.036758748990041}}
{"hexsha": "ca0dd379bf0cfd2ef3ea5b0e3ce3a98b03b1d3f8", "ext": "py", "lang": "Python", "content": "@api.route('/send_code/<email>', methods=['POST'])\n@cross_domain\n@timeing\n@measure_memory_usage\ndef send_code(email):\n    \"\"\"\n    This endpoint generates a unique code that will be used to allow\n    the user to change his/her password. The unique code is send to\n    the specified email address.\n    \"\"\"\n    from zeeguu.core.emailer.password_reset import send_password_reset_email\n    try:\n        User.find(email)\n    except sqlalchemy.orm.exc.NoResultFound:\n        return bad_request('Email unknown')\n    code = UniqueCode(email)\n    db_session.add(code)\n    db_session.commit()\n    send_password_reset_email(email, code)\n    return 'OK'", "fn_id": 2, "class_fn": false, "repo": "zeeguu-ecosystem/zeeguu-api", "file": "zeeguu/api/api/accounts.py", "last_update_at": "2021-05-26T15:24:49+00:00", "original_content": "@api.route('/send_code/<email>', methods=['POST'])\n@cross_domain\ndef send_code(email):\n    \"\"\"\n    This endpoint generates a unique code that will be used to allow\n    the user to change his/her password. The unique code is send to\n    the specified email address.\n    \"\"\"\n    from zeeguu.core.emailer.password_reset import send_password_reset_email\n    try:\n        User.find(email)\n    except sqlalchemy.orm.exc.NoResultFound:\n        return bad_request('Email unknown')\n    code = UniqueCode(email)\n    db_session.add(code)\n    db_session.commit()\n    send_password_reset_email(email, code)\n    return 'OK'", "refactored": true, "pred": {"ppl": 4.469968318939209, "ppl_lower": 5.605391502380371, "ppl/lowercase_ppl": -1.1511622848186434, "ppl/zlib": 0.004136412489267198, "Min_5.0% Prob": 9.383879780769348, "Min_10.0% Prob": 7.317675197825713, "Min_20.0% Prob": 5.496358282425824, "Min_30.0% Prob": 4.446564531793781, "Min_40.0% Prob": 3.5575742954793186, "Min_50.0% Prob": 2.9624025506335636, "Min_60.0% Prob": 2.5057953755253726}}
{"hexsha": "861044681bbdc6a4088c31b9167d9eb267f0536d", "ext": "py", "lang": "Python", "content": "@app.route('/rest/<jobname>/<jobid>/executionduration')\n@timeing\n@measure_memory_usage\ndef get_executionduration(jobname, jobid):\n    \"\"\"Get the maximum execution duration of job <jobid>\n\n    Returns:\n        200 OK: text/plain: integer number of seconds (on success)\n        404 Not Found: Job not found (on NotFoundWarning)\n        500 Internal Server Error (on error)\n    \"\"\"\n    user = set_user()\n    try:\n        logger.info('{} {}'.format(jobname, jobid))\n        job = Job(jobname, jobid, user)\n        response.content_type = 'text/plain; charset=UTF-8'\n        return str(job.execution_duration)\n    except JobAccessDenied as e:\n        abort_403(str(e))\n    except storage.NotFoundWarning as e:\n        abort_404(str(e))\n    except:\n        abort_500_except()", "fn_id": 45, "class_fn": false, "repo": "ParisAstronomicalDataCentre/OPUS", "file": "uws_server/uws_server.py", "last_update_at": "2021-01-06T08:54:54+00:00", "original_content": "@app.route('/rest/<jobname>/<jobid>/executionduration')\ndef get_executionduration(jobname, jobid):\n    \"\"\"Get the maximum execution duration of job <jobid>\n\n    Returns:\n        200 OK: text/plain: integer number of seconds (on success)\n        404 Not Found: Job not found (on NotFoundWarning)\n        500 Internal Server Error (on error)\n    \"\"\"\n    user = set_user()\n    try:\n        logger.info('{} {}'.format(jobname, jobid))\n        job = Job(jobname, jobid, user)\n        response.content_type = 'text/plain; charset=UTF-8'\n        return str(job.execution_duration)\n    except JobAccessDenied as e:\n        abort_403(str(e))\n    except storage.NotFoundWarning as e:\n        abort_404(str(e))\n    except:\n        abort_500_except()", "refactored": true, "pred": {"ppl": 5.784781455993652, "ppl_lower": 6.274975299835205, "ppl/lowercase_ppl": -1.046340900917714, "ppl/zlib": 0.004333902672172831, "Min_5.0% Prob": 11.693419170379638, "Min_10.0% Prob": 9.71865513211205, "Min_20.0% Prob": 7.03468426438265, "Min_30.0% Prob": 5.414826273918152, "Min_40.0% Prob": 4.3169353618178254, "Min_50.0% Prob": 3.495250716805458, "Min_60.0% Prob": 2.9218423037574843}}
{"hexsha": "2c2b3fa036cbe03c2fdfc1bafeba98b37b90d378", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef bbox_sample(region, metric):\n    \"\"\"Extract image that lies within region bounding box\n\n    Parameters\n    ----------\n    region: skimage.RegionProperties\n        Region defining pixels within image to analyse\n    metric: array-like\n        Metric for all pixels in image to be analysed\n    \"\"\"\n    indices = bbox_indices(region)\n    return metric[indices]", "fn_id": 1, "class_fn": false, "repo": "franklongford/ImageCol", "file": "pyfibre/model/tools/utilities.py", "last_update_at": "2021-07-26T05:53:01+00:00", "original_content": "def bbox_sample(region, metric):\n    \"\"\"Extract image that lies within region bounding box\n\n    Parameters\n    ----------\n    region: skimage.RegionProperties\n        Region defining pixels within image to analyse\n    metric: array-like\n        Metric for all pixels in image to be analysed\n    \"\"\"\n    indices = bbox_indices(region)\n    return metric[indices]", "refactored": true, "pred": {"ppl": 18.500015258789062, "ppl_lower": 21.33077049255371, "ppl/lowercase_ppl": -1.0487972052144436, "ppl/zlib": 0.012853619193318712, "Min_5.0% Prob": 12.84489130973816, "Min_10.0% Prob": 11.345730900764465, "Min_20.0% Prob": 9.529335707426071, "Min_30.0% Prob": 7.838803653717041, "Min_40.0% Prob": 6.715062054720792, "Min_50.0% Prob": 5.64779908316476, "Min_60.0% Prob": 4.856233772039413}}
{"hexsha": "2b6b31ac1b8a4e33d35999ea6fa8cea0493f8cb2", "ext": "py", "lang": "Python", "content": "@njit(fastmath=False, cache=True)\n@timeing\n@measure_memory_usage\ndef get_mean_curvature(traj_list, num_traj):\n    out = np.empty((num_traj,))\n    for i in range(num_traj):\n        out[i] = np.mean(np.abs(traj_list[i * trajectory_generator.NUM_STEPS:(i + 1) * trajectory_generator.NUM_STEPS, 3]))\n    return out", "fn_id": 9, "class_fn": false, "repo": "travelbureau/f0_icml_code", "file": "Simulator/python/mpc/trajectory_generator_utils.py", "last_update_at": "2021-02-17T20:53:38+00:00", "original_content": "@njit(fastmath=False, cache=True)\ndef get_mean_curvature(traj_list, num_traj):\n    out = np.empty((num_traj,))\n    for i in range(num_traj):\n        out[i] = np.mean(np.abs(traj_list[i * trajectory_generator.NUM_STEPS:(i + 1) * trajectory_generator.NUM_STEPS, 3]))\n    return out", "refactored": true, "pred": {"ppl": 7.112475872039795, "ppl_lower": 7.361276149749756, "ppl/lowercase_ppl": -1.017525750374541, "ppl/zlib": 0.009616913760366943, "Min_5.0% Prob": 11.610025787353516, "Min_10.0% Prob": 9.78794527053833, "Min_20.0% Prob": 7.5636538158763535, "Min_30.0% Prob": 6.094880602576516, "Min_40.0% Prob": 4.837913750924847, "Min_50.0% Prob": 3.910058604587208, "Min_60.0% Prob": 3.2672078119252217}}
{"hexsha": "5148fe31f1a672fe0541555158917c8256ff125f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_dataframe_barh(dataframe, with_plotting_backend):\n    chart = dataframe.plot.barh()\n    spec = chart.to_dict()\n    assert spec['mark'] == {'type': 'bar', 'orient': 'horizontal'}\n    assert spec['encoding']['y']['field'] == 'index'\n    assert spec['encoding']['x']['field'] == 'value'\n    assert spec['encoding']['color']['field'] == 'column'\n    assert spec['transform'][0]['fold'] == ['x', 'y']", "fn_id": 5, "class_fn": false, "repo": "altair-viz/altair_pandas", "file": "altair_pandas/test_plotting.py", "last_update_at": "2021-12-15T21:34:00+00:00", "original_content": "def test_dataframe_barh(dataframe, with_plotting_backend):\n    chart = dataframe.plot.barh()\n    spec = chart.to_dict()\n    assert spec['mark'] == {'type': 'bar', 'orient': 'horizontal'}\n    assert spec['encoding']['y']['field'] == 'index'\n    assert spec['encoding']['x']['field'] == 'value'\n    assert spec['encoding']['color']['field'] == 'column'\n    assert spec['transform'][0]['fold'] == ['x', 'y']", "refactored": true, "pred": {"ppl": 5.102248191833496, "ppl_lower": 5.102248191833496, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.007147724844407785, "Min_5.0% Prob": 10.340560754140219, "Min_10.0% Prob": 8.294666012128195, "Min_20.0% Prob": 6.270909329255422, "Min_30.0% Prob": 4.883583800212757, "Min_40.0% Prob": 3.9591906520785116, "Min_50.0% Prob": 3.2167894821013174, "Min_60.0% Prob": 2.719420305981829}}
{"hexsha": "ac18a96ded8e760d6d2a171c418c6b63db8231ef", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef init_estimators_train(columns, null_value, no_compression=False):\n    if no_compression:\n        res = [NoCompressionEstimatorTrain(columns, null_value)]\n    else:\n        res = [NoCompressionEstimatorTrain(columns, null_value), DictEstimatorTrain(columns, null_value, max_dict_size), RleEstimatorTrain(columns, null_value), ForEstimatorTrain(columns, null_value)]\n    return res", "fn_id": 1, "class_fn": false, "repo": "bogdanghita/whitebox-compression", "file": "evaluation/theoretical_evaluation.py", "last_update_at": "2021-12-28T05:31:03+00:00", "original_content": "def init_estimators_train(columns, null_value, no_compression=False):\n    if no_compression:\n        res = [NoCompressionEstimatorTrain(columns, null_value)]\n    else:\n        res = [NoCompressionEstimatorTrain(columns, null_value), DictEstimatorTrain(columns, null_value, max_dict_size), RleEstimatorTrain(columns, null_value), ForEstimatorTrain(columns, null_value)]\n    return res", "refactored": true, "pred": {"ppl": 6.886849403381348, "ppl_lower": 10.147076606750488, "ppl/lowercase_ppl": -1.2008546754829355, "ppl/zlib": 0.01121868435772246, "Min_5.0% Prob": 10.763370323181153, "Min_10.0% Prob": 9.879151153564454, "Min_20.0% Prob": 8.044684955051967, "Min_30.0% Prob": 6.116486359387636, "Min_40.0% Prob": 4.7503989394321, "Min_50.0% Prob": 3.832277520387261, "Min_60.0% Prob": 3.2479207556461915}}
{"hexsha": "0629068ef6b33dbfeb46975094295e4a4c07df96", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef which(program):\n    \"\"\"Determines if and where an executable exists on the users path.\n    This code was contributed by Jay at http://stackoverflow.com/a/377028\n    Args:\n        program (str): The name, or path for the program.\n    Returns:\n        The program or executable.\n    \"\"\"\n    import os\n\n    def is_exe(fpath):\n        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n    fpath, fname = os.path.split(program)\n    if fpath and is_exe(program):\n        return program\n    else:\n        for path in os.environ['PATH'].split(os.pathsep):\n            path = path.strip('\"')\n            exe_file = os.path.join(path, program)\n            if is_exe(exe_file):\n                return exe_file\n    return None", "fn_id": 5, "class_fn": false, "repo": "wsmorgan/phonon-enumeration", "file": "phenum/io_utils.py", "last_update_at": "2021-05-30T21:02:08+00:00", "original_content": "def which(program):\n    \"\"\"Determines if and where an executable exists on the users path.\n    This code was contributed by Jay at http://stackoverflow.com/a/377028\n    Args:\n        program (str): The name, or path for the program.\n    Returns:\n        The program or executable.\n    \"\"\"\n    import os\n\n    def is_exe(fpath):\n        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n    fpath, fname = os.path.split(program)\n    if fpath and is_exe(program):\n        return program\n    else:\n        for path in os.environ['PATH'].split(os.pathsep):\n            path = path.strip('\"')\n            exe_file = os.path.join(path, program)\n            if is_exe(exe_file):\n                return exe_file\n    return None", "refactored": true, "pred": {"ppl": 2.986203908920288, "ppl_lower": 3.888272523880005, "ppl/lowercase_ppl": -1.2412808705478342, "ppl/zlib": 0.0028489661070426435, "Min_5.0% Prob": 9.871142053604126, "Min_10.0% Prob": 7.750456309318542, "Min_20.0% Prob": 5.276799672987403, "Min_30.0% Prob": 3.6420198199489424, "Min_40.0% Prob": 2.760802848605303, "Min_50.0% Prob": 2.1985060274239303, "Min_60.0% Prob": 1.8262605400003291}}
{"hexsha": "12f9436289446182577ceaab3eda30ea64c2e38b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef upscale2d_conv2d(x, fmaps, kernel, gain=np.sqrt(2), use_wscale=False):\n    assert kernel >= 1 and kernel % 2 == 1\n    w = get_weight([kernel, kernel, fmaps, x.shape[1].value], gain=gain, use_wscale=use_wscale, fan_in=kernel ** 2 * x.shape[1].value)\n    w = tf.pad(w, [[1, 1], [1, 1], [0, 0], [0, 0]], mode='CONSTANT')\n    w = tf.add_n([w[1:, 1:], w[:-1, 1:], w[1:, :-1], w[:-1, :-1]])\n    w = tf.cast(w, x.dtype)\n    os = [tf.shape(x)[0], fmaps, x.shape[2] * 2, x.shape[3] * 2]\n    return tf.nn.conv2d_transpose(x, w, os, strides=[1, 1, 2, 2], padding='SAME', data_format='NCHW')", "fn_id": 7, "class_fn": false, "repo": "zhuxinqimac/stylegan2", "file": "training/vc2_subnets_pggan.py", "last_update_at": "2021-07-04T09:51:28+00:00", "original_content": "def upscale2d_conv2d(x, fmaps, kernel, gain=np.sqrt(2), use_wscale=False):\n    assert kernel >= 1 and kernel % 2 == 1\n    w = get_weight([kernel, kernel, fmaps, x.shape[1].value], gain=gain, use_wscale=use_wscale, fan_in=kernel ** 2 * x.shape[1].value)\n    w = tf.pad(w, [[1, 1], [1, 1], [0, 0], [0, 0]], mode='CONSTANT')\n    w = tf.add_n([w[1:, 1:], w[:-1, 1:], w[1:, :-1], w[:-1, :-1]])\n    w = tf.cast(w, x.dtype)\n    os = [tf.shape(x)[0], fmaps, x.shape[2] * 2, x.shape[3] * 2]\n    return tf.nn.conv2d_transpose(x, w, os, strides=[1, 1, 2, 2], padding='SAME', data_format='NCHW')", "refactored": true, "pred": {"ppl": 2.748187780380249, "ppl_lower": 3.012561559677124, "ppl/lowercase_ppl": -1.0908549200799114, "ppl/zlib": 0.002831769483185389, "Min_5.0% Prob": 9.057313955747164, "Min_10.0% Prob": 6.746105829874675, "Min_20.0% Prob": 4.7255290592158286, "Min_30.0% Prob": 3.3424162401295288, "Min_40.0% Prob": 2.5364881510581445, "Min_50.0% Prob": 2.021022409853274, "Min_60.0% Prob": 1.6889098395952649}}
{"hexsha": "5306ddb4712ea725043386b81d8c897bbc57d019", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_first_pos(vcf):\n    p = {}\n    for line in file_util.gzopen(vcf):\n        line = file_util.decodeb(line)\n        if line[0] != '#':\n            arr = line.split('\\t')\n            print(arr)\n            p['chrom'] = arr[0]\n            p['spos'] = int(arr[1])\n            p['epos'] = int(arr[1])\n            p['str'] = p['chrom'] + ':' + str(p['spos']) + '-' + str(p['epos'])\n    return p", "fn_id": 0, "class_fn": false, "repo": "dbmi-bgm/cgap-annotation-server", "file": "tests/annotviewer.py", "last_update_at": "2021-05-27T14:27:47+00:00", "original_content": "def get_first_pos(vcf):\n    p = {}\n    for line in file_util.gzopen(vcf):\n        line = file_util.decodeb(line)\n        if line[0] != '#':\n            arr = line.split('\\t')\n            print(arr)\n            p['chrom'] = arr[0]\n            p['spos'] = int(arr[1])\n            p['epos'] = int(arr[1])\n            p['str'] = p['chrom'] + ':' + str(p['spos']) + '-' + str(p['epos'])\n    return p", "refactored": true, "pred": {"ppl": 4.290987491607666, "ppl_lower": 4.290987491607666, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.006712059405416886, "Min_5.0% Prob": 10.773491723196846, "Min_10.0% Prob": 9.085820845195226, "Min_20.0% Prob": 6.659100796495165, "Min_30.0% Prob": 4.765977130049751, "Min_40.0% Prob": 3.624190913513303, "Min_50.0% Prob": 2.909819025014128, "Min_60.0% Prob": 2.426883666986777}}
{"hexsha": "427cceb85af895991c1b9f9596f2e1bb0c5b7263", "ext": "py", "lang": "Python", "content": "def deprecated(version, reason):\n\n    @timeing\n    @measure_memory_usage\n    def decorator_deprecated(fn):\n\n        @wraps(fn)\n        def wrapper_deprecated(*args, **kwds):\n            warnings.warn(f'Deprecated since {version}. The reason: {reason}', category=DeprecationWarning)\n            return fn(*args, **kwds)\n        return wrapper_deprecated\n    return decorator_deprecated", "fn_id": 16, "class_fn": false, "repo": "Sberbank-Technology/ignite-python-thin-client", "file": "pyignite/utils.py", "last_update_at": "2021-07-16T01:35:39+00:00", "original_content": "def deprecated(version, reason):\n\n    def decorator_deprecated(fn):\n\n        @wraps(fn)\n        def wrapper_deprecated(*args, **kwds):\n            warnings.warn(f'Deprecated since {version}. The reason: {reason}', category=DeprecationWarning)\n            return fn(*args, **kwds)\n        return wrapper_deprecated\n    return decorator_deprecated", "refactored": true, "pred": {"ppl": 5.028803825378418, "ppl_lower": 6.1773271560668945, "ppl/lowercase_ppl": -1.1273562436456956, "ppl/zlib": 0.008325681173994652, "Min_5.0% Prob": 13.961998701095581, "Min_10.0% Prob": 10.595459580421448, "Min_20.0% Prob": 6.851391175213982, "Min_30.0% Prob": 5.216636176109314, "Min_40.0% Prob": 4.027647220036563, "Min_50.0% Prob": 3.223438527001891, "Min_60.0% Prob": 2.7231149150460374}}
{"hexsha": "208074ab27944df3955e997903e9acaee5afdb6b", "ext": "py", "lang": "Python", "content": "@pytest.fixture\n@timeing\n@measure_memory_usage\ndef rus_player_json():\n    \"\"\"Expected JSON for rus_player model-fixture\"\"\"\n    return {'name': 'Rus', 'id_': 'PjBYO_8gbf'}", "fn_id": 0, "class_fn": false, "repo": "BenjaminNMitchell/Pyker", "file": "test/encoders/json_fixtures.py", "last_update_at": "2021-04-12T17:18:19+00:00", "original_content": "@pytest.fixture\ndef rus_player_json():\n    \"\"\"Expected JSON for rus_player model-fixture\"\"\"\n    return {'name': 'Rus', 'id_': 'PjBYO_8gbf'}", "refactored": true, "pred": {"ppl": 37.64691925048828, "ppl_lower": 36.686092376708984, "ppl/lowercase_ppl": -0.9928744193646518, "ppl/zlib": 0.024515210303254893, "Min_5.0% Prob": 13.66507863998413, "Min_10.0% Prob": 11.840641593933105, "Min_20.0% Prob": 10.395420551300049, "Min_30.0% Prob": 8.759475175072165, "Min_40.0% Prob": 7.537547681642615, "Min_50.0% Prob": 6.603659333853886, "Min_60.0% Prob": 5.9176026652841}}
{"hexsha": "2c502e5877d8fa61f6c9c0acfe9451fb744aa183", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef create_table(name):\n    name = name.title()\n    name = name.split()\n    name = ''.join(name)\n    with sqlite3.connect('supplies.db') as conn:\n        cur = conn.cursor()\n        cur.execute(f'CREATE TABLE IF NOT EXISTS {name} (Item TEXT, Part Number TEXT, Description TEXT, Vendor TEXT, Price REAL, QOH INT, QNeed INT, Ordering TEXT, Ordered DATE)')", "fn_id": 0, "class_fn": false, "repo": "irondru562/InventoryManagement", "file": "inback.py", "last_update_at": "2021-04-14T18:41:41+00:00", "original_content": "def create_table(name):\n    name = name.title()\n    name = name.split()\n    name = ''.join(name)\n    with sqlite3.connect('supplies.db') as conn:\n        cur = conn.cursor()\n        cur.execute(f'CREATE TABLE IF NOT EXISTS {name} (Item TEXT, Part Number TEXT, Description TEXT, Vendor TEXT, Price REAL, QOH INT, QNeed INT, Ordering TEXT, Ordered DATE)')", "refactored": true, "pred": {"ppl": 7.9301371574401855, "ppl_lower": 8.127368927001953, "ppl/lowercase_ppl": -1.0118642325991998, "ppl/zlib": 0.008025853998129388, "Min_5.0% Prob": 11.932683181762695, "Min_10.0% Prob": 10.085700988769531, "Min_20.0% Prob": 8.007583005087715, "Min_30.0% Prob": 6.255408462136984, "Min_40.0% Prob": 5.083037790798006, "Min_50.0% Prob": 4.122326434783216, "Min_60.0% Prob": 3.4489125094842166}}
{"hexsha": "5b2a8ea69b3198b3f678cebb3485c95da3b6d811", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _graph_from_dict(P, weights=False):\n    G = pairDiGraph()\n    for (a, b), n in P.items():\n        if weights:\n            G.add_edge(a, b, weight=n)\n        else:\n            G.add_edge(a, b)\n    if weights:\n        G.addPd()\n    return G", "fn_id": 2, "class_fn": false, "repo": "jpgil/logdelay", "file": "src/theory202105.py", "last_update_at": "2021-05-28T16:03:39+00:00", "original_content": "def _graph_from_dict(P, weights=False):\n    G = pairDiGraph()\n    for (a, b), n in P.items():\n        if weights:\n            G.add_edge(a, b, weight=n)\n        else:\n            G.add_edge(a, b)\n    if weights:\n        G.addPd()\n    return G", "refactored": true, "pred": {"ppl": 6.7451276779174805, "ppl_lower": 7.330088138580322, "ppl/lowercase_ppl": -1.0435699034980046, "ppl/zlib": 0.011782842091044923, "Min_5.0% Prob": 14.637514352798462, "Min_10.0% Prob": 11.317187150319418, "Min_20.0% Prob": 8.235904335975647, "Min_30.0% Prob": 6.141118477891992, "Min_40.0% Prob": 4.727913297712803, "Min_50.0% Prob": 3.809256870382362, "Min_60.0% Prob": 3.179299660268481}}
{"hexsha": "2c730cc2f3073d60e2b37cd3db6bc2590e864582", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a network with Detectron')\n    parser.add_argument('--cfg', dest='cfg_file', help='Config file for training (and optionally testing)', default=None, type=str)\n    parser.add_argument('--multi-gpu-testing', dest='multi_gpu_testing', help='Use cfg.NUM_GPUS GPUs for inference', action='store_true')\n    parser.add_argument('--skip-test', dest='skip_test', help='Do not test the final model', action='store_true')\n    parser.add_argument('opts', help='See detectron/core/config.py for all options', default=None, nargs=argparse.REMAINDER)\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n    return parser.parse_args()", "fn_id": 0, "class_fn": false, "repo": "994374821/maskrcnn_body25", "file": "tools/train_stage2.py", "last_update_at": "2021-10-12T22:58:13+00:00", "original_content": "def parse_args():\n    parser = argparse.ArgumentParser(description='Train a network with Detectron')\n    parser.add_argument('--cfg', dest='cfg_file', help='Config file for training (and optionally testing)', default=None, type=str)\n    parser.add_argument('--multi-gpu-testing', dest='multi_gpu_testing', help='Use cfg.NUM_GPUS GPUs for inference', action='store_true')\n    parser.add_argument('--skip-test', dest='skip_test', help='Do not test the final model', action='store_true')\n    parser.add_argument('opts', help='See detectron/core/config.py for all options', default=None, nargs=argparse.REMAINDER)\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n    return parser.parse_args()", "refactored": true, "pred": {"ppl": 2.088151216506958, "ppl_lower": 2.9858806133270264, "ppl/lowercase_ppl": -1.4857066191009114, "ppl/zlib": 0.0019478282780338687, "Min_5.0% Prob": 7.731018781661987, "Min_10.0% Prob": 5.540179922467186, "Min_20.0% Prob": 3.459681586140678, "Min_30.0% Prob": 2.426727194634695, "Min_40.0% Prob": 1.8367966713295096, "Min_50.0% Prob": 1.4719480337708124, "Min_60.0% Prob": 1.2269902127080723}}
{"hexsha": "01415ebda64b25b4823eab1699f9886d6bb0651b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_buildings_in_radius(poly: Polygon, radius: float, excluded_poly: Polygon=None) -> List[Polygon]:\n    \"\"\"\n    Get all buildings within $radius\n    Args:\n        poly: polygon to intersect the buildings in\n        radius: radius from within we will retrieve all buildings\n        excluded_poly: optional polygon to exclude from the nearest\n\n    Returns:\n        A Geoseries with all the polygons of the buildings\n    \"\"\"\n    eng = get_connection('POSTGRES')\n    if excluded_poly is not None:\n        excluded_sql = f\"AND ST_INTERSECTS(way, ST_GEOMFROMTEXT('{excluded_poly.wkt}', 4326))=FALSE\"\n    else:\n        excluded_sql = ''\n    query = f\"\\n        SELECT st_astext(way) as geom FROM (\\n            SELECT way\\n            FROM {OSM_POLYGON_TABLE}\\n            WHERE {BUILDING}\\n                  {excluded_sql}\\n                  AND ST_DWithin(way, ST_GEOMFROMTEXT('{poly.wkt}', 4326), {radius}, true)\\n            ) t\\n        \"\n    df = get_df(query, eng)\n    eng.dispose()\n    gs = [wkt.loads(geom) for geom in df['geom'].unique()]\n    return gs", "fn_id": 3, "class_fn": false, "repo": "jonzarecki/coord2vec", "file": "coord2vec/feature_extraction/osm/osm_utils.py", "last_update_at": "2021-01-25T09:21:17+00:00", "original_content": "def get_buildings_in_radius(poly: Polygon, radius: float, excluded_poly: Polygon=None) -> List[Polygon]:\n    \"\"\"\n    Get all buildings within $radius\n    Args:\n        poly: polygon to intersect the buildings in\n        radius: radius from within we will retrieve all buildings\n        excluded_poly: optional polygon to exclude from the nearest\n\n    Returns:\n        A Geoseries with all the polygons of the buildings\n    \"\"\"\n    eng = get_connection('POSTGRES')\n    if excluded_poly is not None:\n        excluded_sql = f\"AND ST_INTERSECTS(way, ST_GEOMFROMTEXT('{excluded_poly.wkt}', 4326))=FALSE\"\n    else:\n        excluded_sql = ''\n    query = f\"\\n        SELECT st_astext(way) as geom FROM (\\n            SELECT way\\n            FROM {OSM_POLYGON_TABLE}\\n            WHERE {BUILDING}\\n                  {excluded_sql}\\n                  AND ST_DWithin(way, ST_GEOMFROMTEXT('{poly.wkt}', 4326), {radius}, true)\\n            ) t\\n        \"\n    df = get_df(query, eng)\n    eng.dispose()\n    gs = [wkt.loads(geom) for geom in df['geom'].unique()]\n    return gs", "refactored": true, "pred": {"ppl": 5.450560092926025, "ppl_lower": 5.671265125274658, "ppl/lowercase_ppl": -1.0234082775186142, "ppl/zlib": 0.0031577623328472443, "Min_5.0% Prob": 10.979209836324056, "Min_10.0% Prob": 9.029031626383464, "Min_20.0% Prob": 6.69208365281423, "Min_30.0% Prob": 5.240803995397356, "Min_40.0% Prob": 4.15204347386833, "Min_50.0% Prob": 3.3847599504207144, "Min_60.0% Prob": 2.8351445019924344}}
{"hexsha": "0ea7da24a0926bef44f6a6ab9c0f2b679c6306d1", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_corner_loss_lidar(pred_bbox3d, gt_bbox3d):\n    \"\"\"\n    :param pred_bbox3d: (N, 7)\n    :param gt_bbox3d: (N, 7)\n    :return: corner_loss: (N)\n    \"\"\"\n    assert pred_bbox3d.shape[0] == gt_bbox3d.shape[0]\n    pred_box_corners = box_torch_ops.center_to_corner_box3d(pred_bbox3d[:, 0:3], pred_bbox3d[:, 3:6], pred_bbox3d[:, 6], [0.5, 0.5, 0], axis=2)\n    gt_box_corners = box_torch_ops.center_to_corner_box3d(gt_bbox3d[:, 0:3], gt_bbox3d[:, 3:6], gt_bbox3d[:, 6], [0.5, 0.5, 0], axis=2)\n    gt_bbox3d_flip = gt_bbox3d.clone()\n    gt_bbox3d_flip[:, 6] += np.pi\n    gt_box_corners_flip = box_torch_ops.center_to_corner_box3d(gt_bbox3d_flip[:, 0:3], gt_bbox3d_flip[:, 3:6], gt_bbox3d_flip[:, 6], [0.5, 0.5, 0], axis=2)\n    corner_dist = torch.min(torch.norm(pred_box_corners - gt_box_corners, dim=2), torch.norm(pred_box_corners - gt_box_corners_flip, dim=2))\n    corner_loss = huber_loss(corner_dist, delta=1.0)\n    return corner_loss.mean(dim=1)", "fn_id": 2, "class_fn": false, "repo": "xy-guo/mmdetection_kitti", "file": "mmdet/utils/det3d/loss_utils.py", "last_update_at": "2021-12-27T10:59:51+00:00", "original_content": "def get_corner_loss_lidar(pred_bbox3d, gt_bbox3d):\n    \"\"\"\n    :param pred_bbox3d: (N, 7)\n    :param gt_bbox3d: (N, 7)\n    :return: corner_loss: (N)\n    \"\"\"\n    assert pred_bbox3d.shape[0] == gt_bbox3d.shape[0]\n    pred_box_corners = box_torch_ops.center_to_corner_box3d(pred_bbox3d[:, 0:3], pred_bbox3d[:, 3:6], pred_bbox3d[:, 6], [0.5, 0.5, 0], axis=2)\n    gt_box_corners = box_torch_ops.center_to_corner_box3d(gt_bbox3d[:, 0:3], gt_bbox3d[:, 3:6], gt_bbox3d[:, 6], [0.5, 0.5, 0], axis=2)\n    gt_bbox3d_flip = gt_bbox3d.clone()\n    gt_bbox3d_flip[:, 6] += np.pi\n    gt_box_corners_flip = box_torch_ops.center_to_corner_box3d(gt_bbox3d_flip[:, 0:3], gt_bbox3d_flip[:, 3:6], gt_bbox3d_flip[:, 6], [0.5, 0.5, 0], axis=2)\n    corner_dist = torch.min(torch.norm(pred_box_corners - gt_box_corners, dim=2), torch.norm(pred_box_corners - gt_box_corners_flip, dim=2))\n    corner_loss = huber_loss(corner_dist, delta=1.0)\n    return corner_loss.mean(dim=1)", "refactored": true, "pred": {"ppl": 1.6859170198440552, "ppl_lower": 1.7063945531845093, "ppl/lowercase_ppl": -1.023114746562964, "ppl/zlib": 0.001536204827041513, "Min_5.0% Prob": 7.37831661814735, "Min_10.0% Prob": 4.7784712730452075, "Min_20.0% Prob": 2.5973408635834168, "Min_30.0% Prob": 1.7399129876653656, "Min_40.0% Prob": 1.3114931708437272, "Min_50.0% Prob": 1.04696928173247, "Min_60.0% Prob": 0.8711740956089372}}
{"hexsha": "605df9b62c38bf133d0aa401619574d4bb7f6839", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_position(fallback=defaults.CURSOR_POS_FALLBACK):\n    \"\"\" Return the current column number of the terminal cursor.\n        Used to figure out if we need to print an extra newline.\n\n        Returns:\n            tuple(int): (x, y) | (,)  - empty, if an error occurred.\n    \"\"\"\n    values = fallback\n    try:\n        with TermStack() as fd:\n            termios.tcflush(fd, termios.TCIFLUSH)\n            tty.setcbreak(fd, termios.TCSANOW)\n            sys.stdout.write(CSI + '6n')\n            sys.stdout.flush()\n            log.debug('about to read get_position response\u2026')\n            resp = _read_until_select(max_bytes=10, end='R')\n    except AttributeError:\n        return values\n    resp = resp.lstrip(CSI)\n    try:\n        values = tuple((int(token) for token in resp.partition(';')[::-2]))\n    except (ValueError, IndexError) as err:\n        log.error('parse error: %s on %r', err, resp)\n    return values", "fn_id": 16, "class_fn": false, "repo": "Dsa-Terminal/Dsa-Terminal", "file": "Python3/Lib/site-packages/console/detection.py", "last_update_at": "2021-04-02T17:14:25+00:00", "original_content": "def get_position(fallback=defaults.CURSOR_POS_FALLBACK):\n    \"\"\" Return the current column number of the terminal cursor.\n        Used to figure out if we need to print an extra newline.\n\n        Returns:\n            tuple(int): (x, y) | (,)  - empty, if an error occurred.\n    \"\"\"\n    values = fallback\n    try:\n        with TermStack() as fd:\n            termios.tcflush(fd, termios.TCIFLUSH)\n            tty.setcbreak(fd, termios.TCSANOW)\n            sys.stdout.write(CSI + '6n')\n            sys.stdout.flush()\n            log.debug('about to read get_position response\u2026')\n            resp = _read_until_select(max_bytes=10, end='R')\n    except AttributeError:\n        return values\n    resp = resp.lstrip(CSI)\n    try:\n        values = tuple((int(token) for token in resp.partition(';')[::-2]))\n    except (ValueError, IndexError) as err:\n        log.error('parse error: %s on %r', err, resp)\n    return values", "refactored": true, "pred": {"ppl": 8.629130363464355, "ppl_lower": 10.470392227172852, "ppl/lowercase_ppl": -1.0897423927651684, "ppl/zlib": 0.004112869715668307, "Min_5.0% Prob": 11.542740186055502, "Min_10.0% Prob": 10.222136993408203, "Min_20.0% Prob": 8.063673010059432, "Min_30.0% Prob": 6.472841827492965, "Min_40.0% Prob": 5.176696787862217, "Min_50.0% Prob": 4.262990026023444, "Min_60.0% Prob": 3.574272910948672}}
{"hexsha": "5ee4210df799d29db459911536ee02f9a5a1fd24", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef readlines(file):\n    with open(file) as f:\n        content = f.readlines()\n    return [x.strip() for x in content]", "fn_id": 1, "class_fn": false, "repo": "omidroshani/DeepDIA", "file": "code/deepdetect/py/predict_ensemble.py", "last_update_at": "2021-12-25T20:09:40+00:00", "original_content": "def readlines(file):\n    with open(file) as f:\n        content = f.readlines()\n    return [x.strip() for x in content]", "refactored": true, "pred": {"ppl": 6.012242317199707, "ppl_lower": 6.012242317199707, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.015331604928779942, "Min_5.0% Prob": 10.128919124603271, "Min_10.0% Prob": 9.098567843437195, "Min_20.0% Prob": 7.930535674095154, "Min_30.0% Prob": 5.872863448583162, "Min_40.0% Prob": 4.62730714152841, "Min_50.0% Prob": 3.585615206425163, "Min_60.0% Prob": 3.035177081053217}}
{"hexsha": "12e6a066caf160f30a66f07074f6d4b7810f6f9d", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef motion(pin):\n    print('Bewegung erkannt')\n    with picamera.PiCamera() as camera:\n        for filename in camera.capture_continuous('/home/pi/Desktop/Fotos/{timestamp:%d.%m_%H-%M-%S}Uhr.jpg'):\n            print('Captured %s' % filename)\n            break", "fn_id": 0, "class_fn": false, "repo": "meigrafd/Sample-Code", "file": "PIR_interrupt_pic.py", "last_update_at": "2021-03-31T04:22:25+00:00", "original_content": "def motion(pin):\n    print('Bewegung erkannt')\n    with picamera.PiCamera() as camera:\n        for filename in camera.capture_continuous('/home/pi/Desktop/Fotos/{timestamp:%d.%m_%H-%M-%S}Uhr.jpg'):\n            print('Captured %s' % filename)\n            break", "refactored": true, "pred": {"ppl": 9.669300079345703, "ppl_lower": 15.754841804504395, "ppl/lowercase_ppl": -1.21516143276608, "ppl/zlib": 0.01101434915640538, "Min_5.0% Prob": 11.954967975616455, "Min_10.0% Prob": 11.013432383537292, "Min_20.0% Prob": 8.332691501168643, "Min_30.0% Prob": 6.88628399848938, "Min_40.0% Prob": 5.540340697064119, "Min_50.0% Prob": 4.508687425491422, "Min_60.0% Prob": 3.818715310564228}}
{"hexsha": "971e98315817be02de95647ef3faaea39f4b2047", "ext": "py", "lang": "Python", "content": "@export\n@timeing\n@measure_memory_usage\ndef tlv_pack(*args):\n    if len(args) == 2:\n        tlv = {'type': args[0], 'value': args[1]}\n    else:\n        tlv = args[0]\n    data = ''\n    if tlv['type'] & TLV_META_TYPE_UINT == TLV_META_TYPE_UINT:\n        data = struct.pack('>III', 12, tlv['type'], tlv['value'])\n    elif tlv['type'] & TLV_META_TYPE_QWORD == TLV_META_TYPE_QWORD:\n        data = struct.pack('>IIQ', 16, tlv['type'], tlv['value'])\n    elif tlv['type'] & TLV_META_TYPE_BOOL == TLV_META_TYPE_BOOL:\n        data = struct.pack('>II', 9, tlv['type']) + bytes(chr(int(bool(tlv['value']))), 'UTF-8')\n    else:\n        value = tlv['value']\n        if sys.version_info[0] < 3 and value.__class__.__name__ == 'unicode':\n            value = value.encode('UTF-8')\n        elif not is_bytes(value):\n            value = bytes(value, 'UTF-8')\n        if tlv['type'] & TLV_META_TYPE_STRING == TLV_META_TYPE_STRING:\n            data = struct.pack('>II', 8 + len(value) + 1, tlv['type']) + value + NULL_BYTE\n        elif tlv['type'] & TLV_META_TYPE_RAW == TLV_META_TYPE_RAW:\n            data = struct.pack('>II', 8 + len(value), tlv['type']) + value\n        elif tlv['type'] & TLV_META_TYPE_GROUP == TLV_META_TYPE_GROUP:\n            data = struct.pack('>II', 8 + len(value), tlv['type']) + value\n        elif tlv['type'] & TLV_META_TYPE_COMPLEX == TLV_META_TYPE_COMPLEX:\n            data = struct.pack('>II', 8 + len(value), tlv['type']) + value\n    return data", "fn_id": 6, "class_fn": false, "repo": "madhavarao-yejarla/VoIP", "file": "data/meterpreter/meterpreter.py", "last_update_at": "2021-04-07T18:00:44+00:00", "original_content": "@export\ndef tlv_pack(*args):\n    if len(args) == 2:\n        tlv = {'type': args[0], 'value': args[1]}\n    else:\n        tlv = args[0]\n    data = ''\n    if tlv['type'] & TLV_META_TYPE_UINT == TLV_META_TYPE_UINT:\n        data = struct.pack('>III', 12, tlv['type'], tlv['value'])\n    elif tlv['type'] & TLV_META_TYPE_QWORD == TLV_META_TYPE_QWORD:\n        data = struct.pack('>IIQ', 16, tlv['type'], tlv['value'])\n    elif tlv['type'] & TLV_META_TYPE_BOOL == TLV_META_TYPE_BOOL:\n        data = struct.pack('>II', 9, tlv['type']) + bytes(chr(int(bool(tlv['value']))), 'UTF-8')\n    else:\n        value = tlv['value']\n        if sys.version_info[0] < 3 and value.__class__.__name__ == 'unicode':\n            value = value.encode('UTF-8')\n        elif not is_bytes(value):\n            value = bytes(value, 'UTF-8')\n        if tlv['type'] & TLV_META_TYPE_STRING == TLV_META_TYPE_STRING:\n            data = struct.pack('>II', 8 + len(value) + 1, tlv['type']) + value + NULL_BYTE\n        elif tlv['type'] & TLV_META_TYPE_RAW == TLV_META_TYPE_RAW:\n            data = struct.pack('>II', 8 + len(value), tlv['type']) + value\n        elif tlv['type'] & TLV_META_TYPE_GROUP == TLV_META_TYPE_GROUP:\n            data = struct.pack('>II', 8 + len(value), tlv['type']) + value\n        elif tlv['type'] & TLV_META_TYPE_COMPLEX == TLV_META_TYPE_COMPLEX:\n            data = struct.pack('>II', 8 + len(value), tlv['type']) + value\n    return data", "refactored": true, "pred": {"ppl": 2.0203709602355957, "ppl_lower": 2.069714069366455, "ppl/lowercase_ppl": -1.0343096486597705, "ppl/zlib": 0.001576863538640504, "Min_5.0% Prob": 7.838040142059326, "Min_10.0% Prob": 5.699981966018677, "Min_20.0% Prob": 3.4260760079516044, "Min_30.0% Prob": 2.3426036947455846, "Min_40.0% Prob": 1.761337237299553, "Min_50.0% Prob": 1.408997409123612, "Min_60.0% Prob": 1.173608262931783}}
{"hexsha": "83e9ee54f5b91fb9bed39ac8b9254bc4e100e0bc", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef vector_valued_monomial_basis_fn(nu, i, n):\n    \"\"\"\n    Generate a vector valued monomial basis polynomial :math:`p_{\\\\nu, i}` in the space\n    :math:`\\\\mathcal{P}_r(\\\\mathbb{R}^m, \\\\mathbb{R}^n)`, where :math:`r = |\\\\nu|` and m is equal to the length of nu.\n\n    The vector valued basis polynomial is generated by specifying a scalar valued basis polynomial and the component\n    of the vector valued basis polynomial that should be equal to the scalar valued basis polynomial. All other\n    components of the vector valued basis polynomial will be zero, i.e.\n\n    .. math:: p_{\\\\nu, i}^j (x) = \\\\begin{cases} p_{\\\\nu} (x), & i = j \\\\\\\\ 0, & \\\\text{else} \\\\end{cases}.\n\n    :param nu: Multi-index indicating which scalar valued monomial basis polynomial should be generated for the\n        non-zero component.\n    :type nu: int or :class:`~polynomials_on_simplices.algebra.multiindex.MultiIndex` or Tuple[int, ...]\n    :param int i: Index of the vector component that is non-zero.\n    :param int n: Dimension of the target.\n    :return: The monomial base polynomial as specified by nu, r, i and n.\n    :rtype: :class:`Polynomial`.\n\n    .. rubric:: Examples\n\n    >>> import sympy as sp\n    >>> x1, x2 = sp.symbols('x1 x2')\n    >>> vector_valued_monomial_basis_fn(0, 0, 2)(x1)\n    array([1, 0])\n    >>> vector_valued_monomial_basis_fn(1, 1, 2)(x1)\n    array([0, x1], dtype=object)\n    >>> vector_valued_monomial_basis_fn((1, 0), 0, 2)((x1, x2))\n    array([x1, 0], dtype=object)\n    >>> vector_valued_monomial_basis_fn((1, 1), 1, 3)((x1, x2))\n    array([0, x1*x2, 0], dtype=object)\n    \"\"\"\n    if n == 1:\n        assert i == 0\n        return monomial_basis_fn(nu)\n    assert i >= 0\n    assert i < n\n    try:\n        m = len(nu)\n    except TypeError:\n        m = 1\n    if not isinstance(nu, multiindex.MultiIndex):\n        nu = multiindex.MultiIndex(nu)\n    r = multiindex.norm(nu)\n    dim = get_dimension(r, m)\n    coeff = np.zeros((dim, n), dtype=int)\n    j = multiindex.get_index(nu, r)\n    coeff[j][i] = 1\n    return Polynomial(coeff, r, m)", "fn_id": 3, "class_fn": false, "repo": "FAndersson/polynomials_on_simplices", "file": "polynomials_on_simplices/polynomial/polynomials_monomial_basis.py", "last_update_at": "2021-03-17T11:41:21+00:00", "original_content": "def vector_valued_monomial_basis_fn(nu, i, n):\n    \"\"\"\n    Generate a vector valued monomial basis polynomial :math:`p_{\\\\nu, i}` in the space\n    :math:`\\\\mathcal{P}_r(\\\\mathbb{R}^m, \\\\mathbb{R}^n)`, where :math:`r = |\\\\nu|` and m is equal to the length of nu.\n\n    The vector valued basis polynomial is generated by specifying a scalar valued basis polynomial and the component\n    of the vector valued basis polynomial that should be equal to the scalar valued basis polynomial. All other\n    components of the vector valued basis polynomial will be zero, i.e.\n\n    .. math:: p_{\\\\nu, i}^j (x) = \\\\begin{cases} p_{\\\\nu} (x), & i = j \\\\\\\\ 0, & \\\\text{else} \\\\end{cases}.\n\n    :param nu: Multi-index indicating which scalar valued monomial basis polynomial should be generated for the\n        non-zero component.\n    :type nu: int or :class:`~polynomials_on_simplices.algebra.multiindex.MultiIndex` or Tuple[int, ...]\n    :param int i: Index of the vector component that is non-zero.\n    :param int n: Dimension of the target.\n    :return: The monomial base polynomial as specified by nu, r, i and n.\n    :rtype: :class:`Polynomial`.\n\n    .. rubric:: Examples\n\n    >>> import sympy as sp\n    >>> x1, x2 = sp.symbols('x1 x2')\n    >>> vector_valued_monomial_basis_fn(0, 0, 2)(x1)\n    array([1, 0])\n    >>> vector_valued_monomial_basis_fn(1, 1, 2)(x1)\n    array([0, x1], dtype=object)\n    >>> vector_valued_monomial_basis_fn((1, 0), 0, 2)((x1, x2))\n    array([x1, 0], dtype=object)\n    >>> vector_valued_monomial_basis_fn((1, 1), 1, 3)((x1, x2))\n    array([0, x1*x2, 0], dtype=object)\n    \"\"\"\n    if n == 1:\n        assert i == 0\n        return monomial_basis_fn(nu)\n    assert i >= 0\n    assert i < n\n    try:\n        m = len(nu)\n    except TypeError:\n        m = 1\n    if not isinstance(nu, multiindex.MultiIndex):\n        nu = multiindex.MultiIndex(nu)\n    r = multiindex.norm(nu)\n    dim = get_dimension(r, m)\n    coeff = np.zeros((dim, n), dtype=int)\n    j = multiindex.get_index(nu, r)\n    coeff[j][i] = 1\n    return Polynomial(coeff, r, m)", "refactored": true, "pred": {"ppl": 3.452700614929199, "ppl_lower": 3.8450815677642822, "ppl/lowercase_ppl": -1.0868639970493614, "ppl/zlib": 0.001485799414581974, "Min_5.0% Prob": 8.579268050916268, "Min_10.0% Prob": 7.1509136142152725, "Min_20.0% Prob": 5.182801390052738, "Min_30.0% Prob": 3.8959807273745537, "Min_40.0% Prob": 3.0488270656223153, "Min_50.0% Prob": 2.468894258766117, "Min_60.0% Prob": 2.0676601547561586}}
{"hexsha": "6bd2ed2c7bff37b5c4b35bd8f9b870bb8d243202", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef main(argv=None):\n    \"\"\"Main entrance into app.  Setup logging, create App, and enter main loop\n    \"\"\"\n    global DEBUG\n    args = process_command_line(argv)\n    if args.debug:\n        DEBUG = True\n        log_level = logging.DEBUG\n    else:\n        log_level = logging.INFO\n    if another_instance_running(args.srcfiles):\n        print('Another instance of Marcam is already running.  Exiting.')\n        return 1\n    if (const.USER_CONFIG_DIR / 'debug').exists():\n        DEBUG = True\n        log_level = logging.DEBUG\n    logging_setup(log_level)\n    sys.stderr = marcam_extra.StderrToLog()\n    log_debug_main()\n    LOGGER.info(repr(args))\n    sanity_checks()\n    myapp = MarcamApp(args.srcfiles)\n    myapp.MainLoop()\n    return 0", "fn_id": 7, "class_fn": false, "repo": "itsayellow/marcam", "file": "marcam/marcam.py", "last_update_at": "2021-10-05T10:22:16+00:00", "original_content": "def main(argv=None):\n    \"\"\"Main entrance into app.  Setup logging, create App, and enter main loop\n    \"\"\"\n    global DEBUG\n    args = process_command_line(argv)\n    if args.debug:\n        DEBUG = True\n        log_level = logging.DEBUG\n    else:\n        log_level = logging.INFO\n    if another_instance_running(args.srcfiles):\n        print('Another instance of Marcam is already running.  Exiting.')\n        return 1\n    if (const.USER_CONFIG_DIR / 'debug').exists():\n        DEBUG = True\n        log_level = logging.DEBUG\n    logging_setup(log_level)\n    sys.stderr = marcam_extra.StderrToLog()\n    log_debug_main()\n    LOGGER.info(repr(args))\n    sanity_checks()\n    myapp = MarcamApp(args.srcfiles)\n    myapp.MainLoop()\n    return 0", "refactored": true, "pred": {"ppl": 7.703091144561768, "ppl_lower": 9.324639320373535, "ppl/lowercase_ppl": -1.0935719828810357, "ppl/zlib": 0.00505351904863093, "Min_5.0% Prob": 12.895173931121827, "Min_10.0% Prob": 10.38636455081758, "Min_20.0% Prob": 7.837138459795997, "Min_30.0% Prob": 6.106558470498948, "Min_40.0% Prob": 4.90948514853205, "Min_50.0% Prob": 4.031243980498541, "Min_60.0% Prob": 3.3875989219144222}}
{"hexsha": "3ee5df8aa3ce8621360bf2c27434ac72151c3d37", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef context_geo(key, values, dataset, namecount):\n    geoname = '\"' + key + '\"'\n    spar2 = '\\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n    PREFIX gndo: <http://d-nb.info/standards/elementset/gnd#>\\n    PREFIX pro: <http://purl.org/hpi/patchr#>\\n    PREFIX owl: <http://www.w3.org/2002/07/owl#>\\n    PREFIX edm: <http://www.europeana.eu/schemas/edm/>\\n    PREFIX dc: <http://purl.org/dc/elements/1.1/>\\n    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\\n    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\\n    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\\n    PREFIX dblp: <http://dblp.org/rdf/schema-2015-01-26#>\\n    PREFIX dcterms: <http://purl.org/dc/terms/>\\n    PREFIX bibtex: <http://data.bibbase.org/ontology/#>\\n    PREFIX geo: <http://www.opengis.net/ont/geosparql#>\\n\\n        select ?x  (group_concat(?alt; SEPARATOR=\", \") as ?altname) (group_concat(?sameas; SEPARATOR=\", \") as ?same)\\n\\n        WHERE{{\\n\\n          graph <http://maral.wisslab.org/graphs/gnd> {{\\n\\n\\n            \\t?x (gndo:preferredNameForThePlaceOrGeographicName | gndo:variantNameForThePlaceOrGeographicName){0}.\\n                optional {{?x gndo:variantNameForThePlaceOrGeographicName ?alt.}}\\n                optional {{?x owl:sameAs ?sameas.}}\\n               # ?x geo:hasGeometry ?geo.\\n               # optional {{?geo geo:asWKT ?coo}}\\n          }}\\n          }} group by ?x\\n\\n        '.format(geoname)\n    sparql.setQuery(spar2)\n    sparql.setReturnFormat(XML)\n    results = sparql.query().convert()\n    for i in range(0, len(results.bindings)):\n        uri = 'https://data.jhn.ngo/spatial/' + str(dataset) + '/' + str(namecount)\n        graph.add((URIRef(uri), RDF.type, edm.Place))\n        for z in range(0, len(values)):\n            graph.add((URIRef(uri), edm.identifier, Literal(values[z])))\n        graph.add((URIRef(uri), skos.prefLabel, Literal(key)))\n        if 'altname' in results.bindings[i].keys():\n            count1 = results.bindings[i]['altname'].value.count(',')\n            if count1 > 0:\n                for j in range(0, count1 + 1):\n                    graph.add((URIRef(uri), skos.altLabel, Literal(results.bindings[i]['altname'].value.rsplit(', ', count1)[count1 - j])))\n            else:\n                graph.add((URIRef(uri), skos.altLabel, Literal(results.bindings[i]['altname'].value)))\n        if 'same' in results.bindings[i].keys():\n            count2 = results.bindings[i]['same'].value.count(',')\n            if count2 > 0:\n                for j in range(0, count2 + 1):\n                    graph.add((URIRef(uri), owl.sameAs, Literal(results.bindings[i]['same'].value.rsplit(', ', count2)[count2 - j])))\n            else:\n                graph.add((URIRef(uri), owl.sameAs, Literal(results.bindings[i]['same'].value)))\n    graph.serialize(destination=dataset + '_Spatial_01.ttl', format='turtle')", "fn_id": 1, "class_fn": false, "repo": "ubffm/judaica-europeana-2-0", "file": "scripts/contextualize_output_spatial_01.py", "last_update_at": "2021-03-17T08:38:58+00:00", "original_content": "def context_geo(key, values, dataset, namecount):\n    geoname = '\"' + key + '\"'\n    spar2 = '\\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n    PREFIX gndo: <http://d-nb.info/standards/elementset/gnd#>\\n    PREFIX pro: <http://purl.org/hpi/patchr#>\\n    PREFIX owl: <http://www.w3.org/2002/07/owl#>\\n    PREFIX edm: <http://www.europeana.eu/schemas/edm/>\\n    PREFIX dc: <http://purl.org/dc/elements/1.1/>\\n    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\\n    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\\n    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\\n    PREFIX dblp: <http://dblp.org/rdf/schema-2015-01-26#>\\n    PREFIX dcterms: <http://purl.org/dc/terms/>\\n    PREFIX bibtex: <http://data.bibbase.org/ontology/#>\\n    PREFIX geo: <http://www.opengis.net/ont/geosparql#>\\n\\n        select ?x  (group_concat(?alt; SEPARATOR=\", \") as ?altname) (group_concat(?sameas; SEPARATOR=\", \") as ?same)\\n\\n        WHERE{{\\n\\n          graph <http://maral.wisslab.org/graphs/gnd> {{\\n\\n\\n            \\t?x (gndo:preferredNameForThePlaceOrGeographicName | gndo:variantNameForThePlaceOrGeographicName){0}.\\n                optional {{?x gndo:variantNameForThePlaceOrGeographicName ?alt.}}\\n                optional {{?x owl:sameAs ?sameas.}}\\n               # ?x geo:hasGeometry ?geo.\\n               # optional {{?geo geo:asWKT ?coo}}\\n          }}\\n          }} group by ?x\\n\\n        '.format(geoname)\n    sparql.setQuery(spar2)\n    sparql.setReturnFormat(XML)\n    results = sparql.query().convert()\n    for i in range(0, len(results.bindings)):\n        uri = 'https://data.jhn.ngo/spatial/' + str(dataset) + '/' + str(namecount)\n        graph.add((URIRef(uri), RDF.type, edm.Place))\n        for z in range(0, len(values)):\n            graph.add((URIRef(uri), edm.identifier, Literal(values[z])))\n        graph.add((URIRef(uri), skos.prefLabel, Literal(key)))\n        if 'altname' in results.bindings[i].keys():\n            count1 = results.bindings[i]['altname'].value.count(',')\n            if count1 > 0:\n                for j in range(0, count1 + 1):\n                    graph.add((URIRef(uri), skos.altLabel, Literal(results.bindings[i]['altname'].value.rsplit(', ', count1)[count1 - j])))\n            else:\n                graph.add((URIRef(uri), skos.altLabel, Literal(results.bindings[i]['altname'].value)))\n        if 'same' in results.bindings[i].keys():\n            count2 = results.bindings[i]['same'].value.count(',')\n            if count2 > 0:\n                for j in range(0, count2 + 1):\n                    graph.add((URIRef(uri), owl.sameAs, Literal(results.bindings[i]['same'].value.rsplit(', ', count2)[count2 - j])))\n            else:\n                graph.add((URIRef(uri), owl.sameAs, Literal(results.bindings[i]['same'].value)))\n    graph.serialize(destination=dataset + '_Spatial_01.ttl', format='turtle')", "refactored": true, "pred": {"ppl": 2.4930872917175293, "ppl_lower": 2.6996240615844727, "ppl/lowercase_ppl": -1.0871251310955672, "ppl/zlib": 0.0008956096261339906, "Min_5.0% Prob": 9.055556952953339, "Min_10.0% Prob": 6.705572294692199, "Min_20.0% Prob": 4.314500948308045, "Min_30.0% Prob": 3.024162335180003, "Min_40.0% Prob": 2.286381246618579, "Min_50.0% Prob": 1.8286023227547104, "Min_60.0% Prob": 1.5230011019591796}}
{"hexsha": "00500ee126bf98712c024187bb2a8536d5e82f46", "ext": "py", "lang": "Python", "content": "@login_required\n@timeing\n@measure_memory_usage\ndef productlist(request):\n    organization = request.user.info.organization\n    if not hasattr(request.user, 'organization'):\n        if not hasattr(request.user, 'permissions'):\n            return HttpResponseForbidden('<h1>403 Forbidden</h1>')\n        if request.user.permissions.product_permissions < 1:\n            return HttpResponseForbidden('<h1>403 Forbidden</h1>')\n    products = Product.objects.filter(organization=organization).order_by('-id').annotate(earmarked=Sum('salesinvoiceentry__quantity', filter=Q(salesinvoiceentry__invoice__finalized=0)))\n    productfilter = ProductFilter(request.GET, queryset=products)\n    paginator = Paginator(productfilter.qs, 25)\n    page_number = request.GET.get('page')\n    page_obj = paginator.get_page(page_number)\n    context = {'page_obj': page_obj, 'filter': productfilter}\n    return render(request, 'products/product_list.html', context=context)", "fn_id": 0, "class_fn": false, "repo": "drtweety/busman", "file": "products/views.py", "last_update_at": "2021-06-13T18:12:21+00:00", "original_content": "@login_required\ndef productlist(request):\n    organization = request.user.info.organization\n    if not hasattr(request.user, 'organization'):\n        if not hasattr(request.user, 'permissions'):\n            return HttpResponseForbidden('<h1>403 Forbidden</h1>')\n        if request.user.permissions.product_permissions < 1:\n            return HttpResponseForbidden('<h1>403 Forbidden</h1>')\n    products = Product.objects.filter(organization=organization).order_by('-id').annotate(earmarked=Sum('salesinvoiceentry__quantity', filter=Q(salesinvoiceentry__invoice__finalized=0)))\n    productfilter = ProductFilter(request.GET, queryset=products)\n    paginator = Paginator(productfilter.qs, 25)\n    page_number = request.GET.get('page')\n    page_obj = paginator.get_page(page_number)\n    context = {'page_obj': page_obj, 'filter': productfilter}\n    return render(request, 'products/product_list.html', context=context)", "refactored": true, "pred": {"ppl": 3.190373420715332, "ppl_lower": 4.6580071449279785, "ppl/lowercase_ppl": -1.3262109727536842, "ppl/zlib": 0.0027754496883520617, "Min_5.0% Prob": 10.125677426656088, "Min_10.0% Prob": 7.891950368881226, "Min_20.0% Prob": 5.286671487652526, "Min_30.0% Prob": 3.803347281910278, "Min_40.0% Prob": 2.888101965941564, "Min_50.0% Prob": 2.316228433737471, "Min_60.0% Prob": 1.9431884427699637}}
{"hexsha": "04feed9797eeb64fb97ec419811711f467958210", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_marks(cos_scores, max_marks, bias):\n    max_cos_score = max(cos_scores)\n    marks_obtained = max([(max_cos_score - bias[0]) / (1 - bias[0]) * max_marks if max_cos_score < bias[1] else max_marks, 0])\n    return marks_obtained", "fn_id": 1, "class_fn": false, "repo": "iamyajat/auto-grader", "file": "autograder/text_similarity.py", "last_update_at": "2021-12-24T13:03:22+00:00", "original_content": "def get_marks(cos_scores, max_marks, bias):\n    max_cos_score = max(cos_scores)\n    marks_obtained = max([(max_cos_score - bias[0]) / (1 - bias[0]) * max_marks if max_cos_score < bias[1] else max_marks, 0])\n    return marks_obtained", "refactored": true, "pred": {"ppl": 7.4776177406311035, "ppl_lower": 7.4776177406311035, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.013412761715911863, "Min_5.0% Prob": 11.608096361160278, "Min_10.0% Prob": 10.04957061343723, "Min_20.0% Prob": 7.7643717130025225, "Min_30.0% Prob": 6.204763668554801, "Min_40.0% Prob": 4.893424831532143, "Min_50.0% Prob": 4.042888055029123, "Min_60.0% Prob": 3.395476891506802}}
{"hexsha": "6e4206da9fd80226e539745dda7d6ae8f2010e3a", "ext": "py", "lang": "Python", "content": "@card('Pacifism')\ndef pacifism(card, abilities):\n\n    def pacifism():\n        return AbilityNotImplemented\n\n    @timeing\n    @measure_memory_usage\n    def pacifism():\n        return AbilityNotImplemented\n    return (pacifism, pacifism)", "fn_id": 150, "class_fn": false, "repo": "Julian/cardboard", "file": "cardboard/cards/sets/mirage.py", "last_update_at": "2021-05-29T06:00:40+00:00", "original_content": "@card('Pacifism')\ndef pacifism(card, abilities):\n\n    def pacifism():\n        return AbilityNotImplemented\n\n    def pacifism():\n        return AbilityNotImplemented\n    return (pacifism, pacifism)", "refactored": true, "pred": {"ppl": 10.505369186401367, "ppl_lower": 16.891347885131836, "ppl/lowercase_ppl": -1.2019294121344724, "ppl/zlib": 0.018518791161841793, "Min_5.0% Prob": 13.572400569915771, "Min_10.0% Prob": 11.530736351013184, "Min_20.0% Prob": 9.481603275645863, "Min_30.0% Prob": 7.508134084589341, "Min_40.0% Prob": 5.823724201192027, "Min_50.0% Prob": 4.672298712977048, "Min_60.0% Prob": 4.001650484388366}}
{"hexsha": "58f3964892634b3c801b485b006283a9cc644941", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_vertex_merger_vertices():\n    merger = MeshVertexMerger()\n    merger.add_vertices([(1, 2, 3), (4, 5, 6)])\n    merger.add_vertices([(1, 2, 3), (4, 5, 6)])\n    assert merger.vertices == [(1, 2, 3), (4, 5, 6)]", "fn_id": 1, "class_fn": false, "repo": "jpsantos-mf/ezdxf", "file": "tests/test_07_render/test_703_render_mesh.py", "last_update_at": "2021-06-05T09:15:15+00:00", "original_content": "def test_vertex_merger_vertices():\n    merger = MeshVertexMerger()\n    merger.add_vertices([(1, 2, 3), (4, 5, 6)])\n    merger.add_vertices([(1, 2, 3), (4, 5, 6)])\n    assert merger.vertices == [(1, 2, 3), (4, 5, 6)]", "refactored": true, "pred": {"ppl": 3.0894455909729004, "ppl_lower": 3.615568161010742, "ppl/lowercase_ppl": -1.1394135815356141, "ppl/zlib": 0.008355493736405699, "Min_5.0% Prob": 9.736935043334961, "Min_10.0% Prob": 7.735642647743225, "Min_20.0% Prob": 5.134745192527771, "Min_30.0% Prob": 3.656328490664882, "Min_40.0% Prob": 2.826594400333195, "Min_50.0% Prob": 2.2500030755137024, "Min_60.0% Prob": 1.891242197849938}}
{"hexsha": "40c216bc802b50376ba5e2720ccd9e0872685eb7", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef parse_args():\n    global args\n    parser = argparse.ArgumentParser(description='Reset RFID Reader')\n    parser.add_argument('host', help='hostname or IP address of RFID reader', nargs='+')\n    parser.add_argument('-p', '--port', default=llrp.LLRP_PORT, help='port to connect to (default {})'.format(llrp.LLRP_PORT))\n    parser.add_argument('-d', '--debug', action='store_true', help='show debugging output')\n    args = parser.parse_args()", "fn_id": 0, "class_fn": false, "repo": "amjadmajid/stork", "file": "Host_software/sllurp/reset.py", "last_update_at": "2021-11-21T08:23:03+00:00", "original_content": "def parse_args():\n    global args\n    parser = argparse.ArgumentParser(description='Reset RFID Reader')\n    parser.add_argument('host', help='hostname or IP address of RFID reader', nargs='+')\n    parser.add_argument('-p', '--port', default=llrp.LLRP_PORT, help='port to connect to (default {})'.format(llrp.LLRP_PORT))\n    parser.add_argument('-d', '--debug', action='store_true', help='show debugging output')\n    args = parser.parse_args()", "refactored": true, "pred": {"ppl": 3.5749428272247314, "ppl_lower": 4.38722038269043, "ppl/lowercase_ppl": -1.1607180846121599, "ppl/zlib": 0.004735870571127884, "Min_5.0% Prob": 11.426886717478434, "Min_10.0% Prob": 8.394613376030556, "Min_20.0% Prob": 5.5691377979058485, "Min_30.0% Prob": 4.072502122475551, "Min_40.0% Prob": 3.147870262654928, "Min_50.0% Prob": 2.542473899229215, "Min_60.0% Prob": 2.1224531992458}}
{"hexsha": "81170dea07b0b06f634847b4f0645d72a042dbbb", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_laser_timing_delay(lxt):\n    \"\"\"\n    Check basic moves are getting to the fs_tgt_time signal.\n\n    That signal's value is off by 10e9 for the ns to s conversion\n    and negative due to the convention that positive lxt means earlier laser.\n    \"\"\"\n    logger.debug('test_laser_timing_delay')\n    assert lxt.wm() == 0\n    assert lxt._fs_tgt_time.get() == -0\n    lxt.mv(1e-06)\n    assert lxt.wm() == 1e-06\n    assert lxt._fs_tgt_time.get() == pytest.approx(-1000)\n    lxt.mv(-5e-06)\n    assert lxt.wm() == -5e-06\n    assert lxt._fs_tgt_time.get() == pytest.approx(5000)", "fn_id": 7, "class_fn": false, "repo": "cristinasewell/pcdsdevices", "file": "tests/test_lxe.py", "last_update_at": "2021-06-15T14:09:42+00:00", "original_content": "def test_laser_timing_delay(lxt):\n    \"\"\"\n    Check basic moves are getting to the fs_tgt_time signal.\n\n    That signal's value is off by 10e9 for the ns to s conversion\n    and negative due to the convention that positive lxt means earlier laser.\n    \"\"\"\n    logger.debug('test_laser_timing_delay')\n    assert lxt.wm() == 0\n    assert lxt._fs_tgt_time.get() == -0\n    lxt.mv(1e-06)\n    assert lxt.wm() == 1e-06\n    assert lxt._fs_tgt_time.get() == pytest.approx(-1000)\n    lxt.mv(-5e-06)\n    assert lxt.wm() == -5e-06\n    assert lxt._fs_tgt_time.get() == pytest.approx(5000)", "refactored": true, "pred": {"ppl": 8.405035018920898, "ppl_lower": 8.52963924407959, "ppl/lowercase_ppl": -1.0069127775640117, "ppl/zlib": 0.007366197001007026, "Min_5.0% Prob": 12.73796033859253, "Min_10.0% Prob": 10.445465360369, "Min_20.0% Prob": 7.944902828761509, "Min_30.0% Prob": 6.337710838764906, "Min_40.0% Prob": 5.1491050313500795, "Min_50.0% Prob": 4.20854748270222, "Min_60.0% Prob": 3.5465706298709847}}
{"hexsha": "10296073b3cc74d61d9da67e57a17da208e30b41", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef corrSpin2(ra, dec, g1a, g2a, g1b=None, g2b=None, raUnits='degrees', decUnits='degrees', **treecorrKwargs):\n    \"\"\"Function to compute correlations between at most two shear-like fields.\n    This is used to compute Rho statistics, given the appropriate spin-2\n    (shear-like) fields.\n    Parameters\n    ----------\n    ra : `numpy.array`\n        The right ascension values of entries in the catalog.\n    dec : `numpy.array`\n        The declination values of entries in the catalog.\n    g1a : `numpy.array`\n        The first component of the primary shear-like field.\n    g2a : `numpy.array`\n        The second component of the primary shear-like field.\n    g1b : `numpy.array`, optional\n        The first component of the secondary shear-like field.\n        Autocorrelation of the primary field is computed if `None` (default).\n    g2b : `numpy.array`, optional\n        The second component of the secondary shear-like field.\n        Autocorrelation of the primary field is computed if `None` (default).\n    raUnits : `str`, optional\n        Unit of the right ascension values.\n        Valid options are \"degrees\", \"arcmin\", \"arcsec\", \"hours\" or \"radians\".\n    decUnits : `str`, optional\n        Unit of the declination values.\n        Valid options are \"degrees\", \"arcmin\", \"arcsec\", \"hours\" or \"radians\".\n    **treecorrKwargs\n        Keyword arguments to be passed to `treecorr.GGCorrelation`.\n    Returns\n    -------\n    xy : `treecorr.GGCorrelation`\n        A `treecorr.GGCorrelation` object containing the correlation function.\n    \"\"\"\n    xy = treecorr.GGCorrelation(**treecorrKwargs)\n    catA = treecorr.Catalog(ra=ra, dec=dec, g1=g1a, g2=g2a, ra_units=raUnits, dec_units=decUnits)\n    if g1b is None or g2b is None:\n        xy.process(catA)\n    else:\n        catB = treecorr.Catalog(ra=ra, dec=dec, g1=g1b, g2=g2b, ra_units=raUnits, dec_units=decUnits)\n        xy.process(catA, catB)\n    return xy", "fn_id": 1, "class_fn": false, "repo": "lsst-dmsst/metric-pipeline-tasks", "file": "python/lsst/faro/utils/tex.py", "last_update_at": "2021-02-23T16:05:17+00:00", "original_content": "def corrSpin2(ra, dec, g1a, g2a, g1b=None, g2b=None, raUnits='degrees', decUnits='degrees', **treecorrKwargs):\n    \"\"\"Function to compute correlations between at most two shear-like fields.\n    This is used to compute Rho statistics, given the appropriate spin-2\n    (shear-like) fields.\n    Parameters\n    ----------\n    ra : `numpy.array`\n        The right ascension values of entries in the catalog.\n    dec : `numpy.array`\n        The declination values of entries in the catalog.\n    g1a : `numpy.array`\n        The first component of the primary shear-like field.\n    g2a : `numpy.array`\n        The second component of the primary shear-like field.\n    g1b : `numpy.array`, optional\n        The first component of the secondary shear-like field.\n        Autocorrelation of the primary field is computed if `None` (default).\n    g2b : `numpy.array`, optional\n        The second component of the secondary shear-like field.\n        Autocorrelation of the primary field is computed if `None` (default).\n    raUnits : `str`, optional\n        Unit of the right ascension values.\n        Valid options are \"degrees\", \"arcmin\", \"arcsec\", \"hours\" or \"radians\".\n    decUnits : `str`, optional\n        Unit of the declination values.\n        Valid options are \"degrees\", \"arcmin\", \"arcsec\", \"hours\" or \"radians\".\n    **treecorrKwargs\n        Keyword arguments to be passed to `treecorr.GGCorrelation`.\n    Returns\n    -------\n    xy : `treecorr.GGCorrelation`\n        A `treecorr.GGCorrelation` object containing the correlation function.\n    \"\"\"\n    xy = treecorr.GGCorrelation(**treecorrKwargs)\n    catA = treecorr.Catalog(ra=ra, dec=dec, g1=g1a, g2=g2a, ra_units=raUnits, dec_units=decUnits)\n    if g1b is None or g2b is None:\n        xy.process(catA)\n    else:\n        catB = treecorr.Catalog(ra=ra, dec=dec, g1=g1b, g2=g2b, ra_units=raUnits, dec_units=decUnits)\n        xy.process(catA, catB)\n    return xy", "refactored": true, "pred": {"ppl": 2.450540542602539, "ppl_lower": 2.78662371635437, "ppl/lowercase_ppl": -1.1433904476450332, "ppl/zlib": 0.0014092902985144444, "Min_5.0% Prob": 9.771982864097312, "Min_10.0% Prob": 7.018468900160356, "Min_20.0% Prob": 4.2886753903852926, "Min_30.0% Prob": 2.983821141773355, "Min_40.0% Prob": 2.2449676119576254, "Min_50.0% Prob": 1.7955637509078388, "Min_60.0% Prob": 1.495562949920431}}
{"hexsha": "7cfa76a0e321448c9b715a43fe2ae910c799972c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef make_summary_file(filename):\n    \"\"\"\n    Given a DLIS file, make a short human readable summary of it.\n    Show things like headers, well parameters and which well curves are\n    available.\n\n    Args:\n    filename: A DLIS file.\n\n    Returns:\n    summaryfile: The DLIS file without the extention and the suffix\n    _summary.txt.\n    \"\"\"\n    summaryfile = open(filename.replace('.DLIS', '_summary.txt'), 'w')\n    f, *f_tail = dlis.load(filename)\n    if len(f_tail):\n        print('There are more logical files in tail')\n    origin, *origin_tail = f.origins\n    if len(origin_tail):\n        print(filename + ' contains multiple origins')\n    header = f.fileheader\n    parameter_table = summarize(f.parameters, name='Name', long_name='Long name', values='Value(s)')\n    mask = ~parameter_table['Name'].isin(['R8', 'RR1', 'WITN', 'ENGI'])\n    parameter_table = parameter_table[mask]\n    parameter_table.sort_values('Name')\n    summaryfile.write(str(f.describe()))\n    summaryfile.write(str(origin.describe()))\n    summaryfile.write(str(header.describe()))\n    summaryfile.write(str(parameter_table))\n    for frame in f.frames:\n        index_channel = next((ch for ch in frame.channels if ch.name == frame.index))\n        summaryfile.write(f'\\nFrame {frame.name}:\\n')\n        summaryfile.write(f'Description      : {frame.description}\\n')\n        summaryfile.write(f'Indexed by       : {frame.index_type}\\n')\n        summaryfile.write(f'Interval         : [{frame.index_min}, {frame.index_max}] {index_channel.units}\\n')\n        summaryfile.write(f'Direction        : {frame.direction}\\n')\n        summaryfile.write(f'Constant spacing : {frame.spacing} {index_channel.units}\\n')\n        summaryfile.write(f'Index channel    : {index_channel}\\n')\n        summaryfile.write(f'No. of channels  : {len(frame.channels)}\\n')\n    channel_table = summarize(f.channels, name='Name', long_name='Long name', units='Units', dimension='Dimension', frame='Frame')\n    channel_table.sort_values('Name')\n    summaryfile.write(str(channel_table))\n    summaryfile.close()\n    return summaryfile", "fn_id": 1, "class_fn": false, "repo": "softwareunderground/northern-lights", "file": "scripts/dlis_summary.py", "last_update_at": "2021-04-17T09:13:20+00:00", "original_content": "def make_summary_file(filename):\n    \"\"\"\n    Given a DLIS file, make a short human readable summary of it.\n    Show things like headers, well parameters and which well curves are\n    available.\n\n    Args:\n    filename: A DLIS file.\n\n    Returns:\n    summaryfile: The DLIS file without the extention and the suffix\n    _summary.txt.\n    \"\"\"\n    summaryfile = open(filename.replace('.DLIS', '_summary.txt'), 'w')\n    f, *f_tail = dlis.load(filename)\n    if len(f_tail):\n        print('There are more logical files in tail')\n    origin, *origin_tail = f.origins\n    if len(origin_tail):\n        print(filename + ' contains multiple origins')\n    header = f.fileheader\n    parameter_table = summarize(f.parameters, name='Name', long_name='Long name', values='Value(s)')\n    mask = ~parameter_table['Name'].isin(['R8', 'RR1', 'WITN', 'ENGI'])\n    parameter_table = parameter_table[mask]\n    parameter_table.sort_values('Name')\n    summaryfile.write(str(f.describe()))\n    summaryfile.write(str(origin.describe()))\n    summaryfile.write(str(header.describe()))\n    summaryfile.write(str(parameter_table))\n    for frame in f.frames:\n        index_channel = next((ch for ch in frame.channels if ch.name == frame.index))\n        summaryfile.write(f'\\nFrame {frame.name}:\\n')\n        summaryfile.write(f'Description      : {frame.description}\\n')\n        summaryfile.write(f'Indexed by       : {frame.index_type}\\n')\n        summaryfile.write(f'Interval         : [{frame.index_min}, {frame.index_max}] {index_channel.units}\\n')\n        summaryfile.write(f'Direction        : {frame.direction}\\n')\n        summaryfile.write(f'Constant spacing : {frame.spacing} {index_channel.units}\\n')\n        summaryfile.write(f'Index channel    : {index_channel}\\n')\n        summaryfile.write(f'No. of channels  : {len(frame.channels)}\\n')\n    channel_table = summarize(f.channels, name='Name', long_name='Long name', units='Units', dimension='Dimension', frame='Frame')\n    channel_table.sort_values('Name')\n    summaryfile.write(str(channel_table))\n    summaryfile.close()\n    return summaryfile", "refactored": true, "pred": {"ppl": 4.766307353973389, "ppl_lower": 4.843008995056152, "ppl/lowercase_ppl": -1.0102232600577592, "ppl/zlib": 0.0020227614837710003, "Min_5.0% Prob": 10.592205212034028, "Min_10.0% Prob": 8.841763981457415, "Min_20.0% Prob": 6.50016818375423, "Min_30.0% Prob": 4.923298973461677, "Min_40.0% Prob": 3.8537310727968297, "Min_50.0% Prob": 3.10958503591236, "Min_60.0% Prob": 2.6012383109081356}}
{"hexsha": "4be3288dc749ae714f5530e0b9af4e86fbcb9c14", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef set_crs(df: GDF, epsg_code: Union[int, str]) -> GDF:\n    \"\"\"Sets dataframe crs in geopandas pipeline.\n\n    TODO: Deprecate with next rasterio version that will integrate set_crs method.\n    \"\"\"\n    df.crs = {'init': f'epsg:{str(epsg_code)}'}\n    return df", "fn_id": 2, "class_fn": false, "repo": "ZakariaELHAJOUY/Remote-sensing-", "file": "utils/geo.py", "last_update_at": "2021-07-12T06:28:31+00:00", "original_content": "def set_crs(df: GDF, epsg_code: Union[int, str]) -> GDF:\n    \"\"\"Sets dataframe crs in geopandas pipeline.\n\n    TODO: Deprecate with next rasterio version that will integrate set_crs method.\n    \"\"\"\n    df.crs = {'init': f'epsg:{str(epsg_code)}'}\n    return df", "refactored": true, "pred": {"ppl": 10.487448692321777, "ppl_lower": 12.114497184753418, "ppl/lowercase_ppl": -1.061367095591868, "ppl/zlib": 0.010830318799593003, "Min_5.0% Prob": 11.706335067749023, "Min_10.0% Prob": 10.106441497802734, "Min_20.0% Prob": 8.47733865843879, "Min_30.0% Prob": 6.958731571833293, "Min_40.0% Prob": 5.678908344772127, "Min_50.0% Prob": 4.7159064875708685, "Min_60.0% Prob": 3.952885882722007}}
{"hexsha": "28241061cc6276467c762df6e87e9db4f4a59e1b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef total_lights():\n    total_lights_on = 0\n    for y, _ in enumerate(lights):\n        for x, _ in enumerate(lights[y]):\n            if lights[y][x] == 1:\n                total_lights_on += 1\n    return total_lights_on", "fn_id": 6, "class_fn": false, "repo": "josephroquedev/advent-of-code", "file": "2015/day_18/python/day18.py", "last_update_at": "2021-11-30T10:05:29+00:00", "original_content": "def total_lights():\n    total_lights_on = 0\n    for y, _ in enumerate(lights):\n        for x, _ in enumerate(lights[y]):\n            if lights[y][x] == 1:\n                total_lights_on += 1\n    return total_lights_on", "refactored": true, "pred": {"ppl": 4.6812639236450195, "ppl_lower": 4.6812639236450195, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.011519165243450482, "Min_5.0% Prob": 11.457000414530436, "Min_10.0% Prob": 9.783640929630824, "Min_20.0% Prob": 7.105151840618679, "Min_30.0% Prob": 5.071749066764658, "Min_40.0% Prob": 3.9252991131667434, "Min_50.0% Prob": 3.085007416889877, "Min_60.0% Prob": 2.5957049461377952}}
{"hexsha": "c85e03d56b4d6fc548188cd588d9ea2d6a3b23be", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef argopen(file, mode, encoding=None, errors=None):\n    closefd = True\n    if file == '-':\n        closefd = False\n        if 'r' in mode:\n            file = sys.stdin.fileno()\n        else:\n            file = sys.stdout.fileno()\n    return io.open(file, mode, encoding=encoding, errors=errors, closefd=closefd)", "fn_id": 0, "class_fn": false, "repo": "olivier-compilatio/uniseg-python", "file": "uniseg/samples/unibreak.py", "last_update_at": "2021-05-11T02:01:47+00:00", "original_content": "def argopen(file, mode, encoding=None, errors=None):\n    closefd = True\n    if file == '-':\n        closefd = False\n        if 'r' in mode:\n            file = sys.stdin.fileno()\n        else:\n            file = sys.stdout.fileno()\n    return io.open(file, mode, encoding=encoding, errors=errors, closefd=closefd)", "refactored": true, "pred": {"ppl": 4.232497692108154, "ppl_lower": 4.86651611328125, "ppl/lowercase_ppl": -1.096747130675822, "ppl/zlib": 0.008151368868623526, "Min_5.0% Prob": 11.757586002349854, "Min_10.0% Prob": 9.219527456495497, "Min_20.0% Prob": 6.303671435305946, "Min_30.0% Prob": 4.621025313591135, "Min_40.0% Prob": 3.654562811318197, "Min_50.0% Prob": 2.912908781746713, "Min_60.0% Prob": 2.4127656862798053}}
{"hexsha": "96a9ce1af1b9abc8e61b5491562c9ddee1c883c3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef infer_replicates(target_labels_long):\n    \"\"\" Infer replicate experiments based on their long form labels.\n\n    In:\n        target_labels_long [str]: list of long form target labels\n    Out:\n        replicate_lists {exp_label -> [target indexes]}\n    \"\"\"\n    replicate_lists = {}\n    rep_re = []\n    rep_re.append(re.compile('rep\\\\d+'))\n    rep_re.append(re.compile('donor\\\\d+'))\n    for ti in range(len(target_labels_long)):\n        label = target_labels_long[ti]\n        for ri in range(len(rep_re)):\n            rep_m = rep_re[ri].search(label)\n            if rep_m:\n                rep_str = rep_m.group(0)\n                label = label.replace(rep_str, '')\n        replicate_lists.setdefault(label, []).append(ti)\n    return replicate_lists", "fn_id": 1, "class_fn": false, "repo": "JasperSnoek/basenji", "file": "bin/basenji_test_reps.py", "last_update_at": "2021-05-12T08:51:44+00:00", "original_content": "def infer_replicates(target_labels_long):\n    \"\"\" Infer replicate experiments based on their long form labels.\n\n    In:\n        target_labels_long [str]: list of long form target labels\n    Out:\n        replicate_lists {exp_label -> [target indexes]}\n    \"\"\"\n    replicate_lists = {}\n    rep_re = []\n    rep_re.append(re.compile('rep\\\\d+'))\n    rep_re.append(re.compile('donor\\\\d+'))\n    for ti in range(len(target_labels_long)):\n        label = target_labels_long[ti]\n        for ri in range(len(rep_re)):\n            rep_m = rep_re[ri].search(label)\n            if rep_m:\n                rep_str = rep_m.group(0)\n                label = label.replace(rep_str, '')\n        replicate_lists.setdefault(label, []).append(ti)\n    return replicate_lists", "refactored": true, "pred": {"ppl": 5.282285690307617, "ppl_lower": 5.078351974487305, "ppl/lowercase_ppl": -0.976343981185054, "ppl/zlib": 0.004852358308855307, "Min_5.0% Prob": 11.57997007369995, "Min_10.0% Prob": 9.532032398950486, "Min_20.0% Prob": 7.048267009646394, "Min_30.0% Prob": 5.352349316156827, "Min_40.0% Prob": 4.130958383795859, "Min_50.0% Prob": 3.3234640562206232, "Min_60.0% Prob": 2.7905458658862}}
{"hexsha": "eb9320bc73ea082b4d99c1b4dbcaed9bf2e92e33", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef verify_flow_region_func(height: float, boundary_layer_height: float) -> FlowFieldRegion:\n    if height < boundary_layer_height:\n        return FlowFieldRegion.BOUNDARY_LAYER\n    else:\n        return FlowFieldRegion.FREE_FLOW", "fn_id": 0, "class_fn": false, "repo": "lucasralves/steady-flow-prediction", "file": "src/pre_processing/core/mesh/functions/verify_flow_region.py", "last_update_at": "2021-07-16T19:59:39+00:00", "original_content": "def verify_flow_region_func(height: float, boundary_layer_height: float) -> FlowFieldRegion:\n    if height < boundary_layer_height:\n        return FlowFieldRegion.BOUNDARY_LAYER\n    else:\n        return FlowFieldRegion.FREE_FLOW", "refactored": true, "pred": {"ppl": 8.066452026367188, "ppl_lower": 11.664623260498047, "ppl/lowercase_ppl": -1.1766750231173755, "ppl/zlib": 0.012967166061111401, "Min_5.0% Prob": 10.524197578430176, "Min_10.0% Prob": 9.788459300994873, "Min_20.0% Prob": 7.761690176450289, "Min_30.0% Prob": 6.262191474437714, "Min_40.0% Prob": 5.072952244016859, "Min_50.0% Prob": 4.135422464241, "Min_60.0% Prob": 3.5394090197980406}}
{"hexsha": "3f5030776c04f54c1b5b6703282b5954e3f6c54c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef timeit(method):\n\n    def timed(*args, **kw):\n        ts = time.time()\n        result = method(*args, **kw)\n        te = time.time()\n        if 'log_time' in kw:\n            name = kw.get('log_name', method.__name__.upper())\n            kw['log_time'][name] = int((te - ts) * 1000)\n        else:\n            print('%r  %2.2f ms' % (method.__name__, (te - ts) * 1000))\n        return result\n    return timed", "fn_id": 0, "class_fn": false, "repo": "xiongjuncheng/futu_algo", "file": "strategies/Strategies.py", "last_update_at": "2021-06-09T01:16:13+00:00", "original_content": "def timeit(method):\n\n    def timed(*args, **kw):\n        ts = time.time()\n        result = method(*args, **kw)\n        te = time.time()\n        if 'log_time' in kw:\n            name = kw.get('log_name', method.__name__.upper())\n            kw['log_time'][name] = int((te - ts) * 1000)\n        else:\n            print('%r  %2.2f ms' % (method.__name__, (te - ts) * 1000))\n        return result\n    return timed", "refactored": true, "pred": {"ppl": 1.5820010900497437, "ppl_lower": 1.5820010900497437, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.0020477257070443413, "Min_5.0% Prob": 7.480107545852661, "Min_10.0% Prob": 4.561224002104539, "Min_20.0% Prob": 2.3316405517635523, "Min_30.0% Prob": 1.5415225490791404, "Min_40.0% Prob": 1.1504363442051477, "Min_50.0% Prob": 0.9172864519641735, "Min_60.0% Prob": 0.771916885670557}}
{"hexsha": "89cf74e1d86ffce9e2a638d3306254d23a748771", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef infer_remote_zip_file_directory_name() -> List[str]:\n    from ..version import __version__\n    remote_zip_file_directory_name: List[str] = [__version__]\n    remote_zip_file_directory_name.append(f'python_{PYTHON_VERSION}')\n    if Env.ON_WINDOWS:\n        remote_zip_file_directory_name.append('windows')\n    else:\n        remote_zip_file_directory_name.append('linux')\n    return remote_zip_file_directory_name", "fn_id": 3, "class_fn": false, "repo": "Reefledge/reefledge", "file": "reefledge/remote_zip_file_path/__init__.py", "last_update_at": "2021-09-23T10:44:32+00:00", "original_content": "def infer_remote_zip_file_directory_name() -> List[str]:\n    from ..version import __version__\n    remote_zip_file_directory_name: List[str] = [__version__]\n    remote_zip_file_directory_name.append(f'python_{PYTHON_VERSION}')\n    if Env.ON_WINDOWS:\n        remote_zip_file_directory_name.append('windows')\n    else:\n        remote_zip_file_directory_name.append('linux')\n    return remote_zip_file_directory_name", "refactored": true, "pred": {"ppl": 4.548521518707275, "ppl_lower": 4.570549964904785, "ppl/lowercase_ppl": -1.0031893938071577, "ppl/zlib": 0.007111747601946739, "Min_5.0% Prob": 10.744980971018473, "Min_10.0% Prob": 8.958866119384766, "Min_20.0% Prob": 6.462839218286367, "Min_30.0% Prob": 4.86376093289791, "Min_40.0% Prob": 3.7514020453851957, "Min_50.0% Prob": 3.022865052578541, "Min_60.0% Prob": 2.523454438047245}}
{"hexsha": "c8cc0e831e5f48f15a8c98373df38d2e3f24ded3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef heap_sort(list_item):\n    \"\"\"\n    Heap Sort\n    Time Complexity of Solution:\n    Best - O(nlog(n))\n    Average - O(nlog(n))\n    Worst - O(nlog(n))\n\n    Approach:\n    Heap sort takes place in two steps. In first step, array is\n    transformed into a heap.\n    In second step, heap is continously reduced to a sorted array.\n\n    \"\"\"\n    end = len(list_item)\n    start = end // 2 - 1\n    for i in range(start, -1, -1):\n        heapify(list_item, end, i)\n    for i in range(end - 1, 0, -1):\n        swap(list_item, i, 0)\n        heapify(list_item, i, 0)", "fn_id": 0, "class_fn": false, "repo": "mish24/pydsa", "file": "pydsa/heap_sort.py", "last_update_at": "2021-11-02T09:24:38+00:00", "original_content": "def heap_sort(list_item):\n    \"\"\"\n    Heap Sort\n    Time Complexity of Solution:\n    Best - O(nlog(n))\n    Average - O(nlog(n))\n    Worst - O(nlog(n))\n\n    Approach:\n    Heap sort takes place in two steps. In first step, array is\n    transformed into a heap.\n    In second step, heap is continously reduced to a sorted array.\n\n    \"\"\"\n    end = len(list_item)\n    start = end // 2 - 1\n    for i in range(start, -1, -1):\n        heapify(list_item, end, i)\n    for i in range(end - 1, 0, -1):\n        swap(list_item, i, 0)\n        heapify(list_item, i, 0)", "refactored": true, "pred": {"ppl": 3.603733539581299, "ppl_lower": 4.093122959136963, "ppl/lowercase_ppl": -1.0993297784151226, "ppl/zlib": 0.004189445759409842, "Min_5.0% Prob": 9.914129787021214, "Min_10.0% Prob": 7.808744731702302, "Min_20.0% Prob": 5.551704036562066, "Min_30.0% Prob": 4.1513790550984835, "Min_40.0% Prob": 3.2114622236082426, "Min_50.0% Prob": 2.5604777132005743, "Min_60.0% Prob": 2.1397532171245826}}
{"hexsha": "29dcd0e7194a9a266ef5c8a71033e49f8cc50186", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef a_star(grid, h, start, goal):\n    path = []\n    path_cost = 0\n    queue = PriorityQueue()\n    queue.put((0, start))\n    visited = set(start)\n    branch = {}\n    found = False\n    while not queue.empty():\n        item = queue.get()\n        current_node = item[1]\n        if current_node == start:\n            current_cost = 0.0\n        else:\n            current_cost = branch[current_node][0]\n        if current_node == goal:\n            print('Found a path.')\n            found = True\n            break\n        else:\n            for action in valid_actions(grid, current_node):\n                da = action.delta\n                next_node = (current_node[0] + da[0], current_node[1] + da[1])\n                branch_cost = current_cost + action.cost\n                queue_cost = branch_cost + h(next_node, goal)\n                if next_node not in visited:\n                    visited.add(next_node)\n                    branch[next_node] = (branch_cost, current_node, action)\n                    queue.put((queue_cost, next_node))\n    if found:\n        n = goal\n        path_cost = branch[n][0]\n        path.append(goal)\n        while branch[n][1] != start:\n            path.append(branch[n][1])\n            n = branch[n][1]\n        path.append(branch[n][1])\n    else:\n        print('**********************')\n        print('Failed to find a path!')\n        print('**********************')\n    return (path[::-1], path_cost)", "fn_id": 2, "class_fn": false, "repo": "allthatido/Drone_Motion_Planning", "file": "planning_utils.py", "last_update_at": "2021-07-08T18:54:42+00:00", "original_content": "def a_star(grid, h, start, goal):\n    path = []\n    path_cost = 0\n    queue = PriorityQueue()\n    queue.put((0, start))\n    visited = set(start)\n    branch = {}\n    found = False\n    while not queue.empty():\n        item = queue.get()\n        current_node = item[1]\n        if current_node == start:\n            current_cost = 0.0\n        else:\n            current_cost = branch[current_node][0]\n        if current_node == goal:\n            print('Found a path.')\n            found = True\n            break\n        else:\n            for action in valid_actions(grid, current_node):\n                da = action.delta\n                next_node = (current_node[0] + da[0], current_node[1] + da[1])\n                branch_cost = current_cost + action.cost\n                queue_cost = branch_cost + h(next_node, goal)\n                if next_node not in visited:\n                    visited.add(next_node)\n                    branch[next_node] = (branch_cost, current_node, action)\n                    queue.put((queue_cost, next_node))\n    if found:\n        n = goal\n        path_cost = branch[n][0]\n        path.append(goal)\n        while branch[n][1] != start:\n            path.append(branch[n][1])\n            n = branch[n][1]\n        path.append(branch[n][1])\n    else:\n        print('**********************')\n        print('Failed to find a path!')\n        print('**********************')\n    return (path[::-1], path_cost)", "refactored": true, "pred": {"ppl": 2.632458448410034, "ppl_lower": 2.810657262802124, "ppl/lowercase_ppl": -1.0676711914542192, "ppl/zlib": 0.0019673133756537894, "Min_5.0% Prob": 10.635116444693672, "Min_10.0% Prob": 7.65108694256963, "Min_20.0% Prob": 4.613956078688304, "Min_30.0% Prob": 3.2121504969628796, "Min_40.0% Prob": 2.4150257989764214, "Min_50.0% Prob": 1.9400918386791917, "Min_60.0% Prob": 1.6130084644811642}}
{"hexsha": "7fa31603a252cfd1d536877477d7abb4da4bdef3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef decimal_to_binary(n: int):\n    \"\"\"\n    Function to convert Decimal number to Binary number \n    \n    :param n: \n    :return: \n    \"\"\"\n    return int('{0:b}'.format(n))", "fn_id": 0, "class_fn": false, "repo": "ikostan/python", "file": "secret-handshake/secret_handshake.py", "last_update_at": "2021-08-02T19:20:10+00:00", "original_content": "def decimal_to_binary(n: int):\n    \"\"\"\n    Function to convert Decimal number to Binary number \n    \n    :param n: \n    :return: \n    \"\"\"\n    return int('{0:b}'.format(n))", "refactored": true, "pred": {"ppl": 9.141637802124023, "ppl_lower": 8.521315574645996, "ppl/lowercase_ppl": -0.9682449544935661, "ppl/zlib": 0.015260962482880818, "Min_5.0% Prob": 11.303866863250732, "Min_10.0% Prob": 10.42401294708252, "Min_20.0% Prob": 8.473224423148416, "Min_30.0% Prob": 6.868099361658096, "Min_40.0% Prob": 5.369573525407097, "Min_50.0% Prob": 4.467641515864266, "Min_60.0% Prob": 3.677325548547687}}
{"hexsha": "66a6d587f9640fe42f4703541334c4465b7b3a16", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _parse_word(operand):\n    value = OP_EVALUATOR.eval_int(operand)\n    if 0 <= value < 65536:\n        return (value % 256, value // 256)\n    raise ValueError", "fn_id": 6, "class_fn": false, "repo": "skoolkid/sk6502", "file": "sk6502/assembler.py", "last_update_at": "2021-01-26T17:44:49+00:00", "original_content": "def _parse_word(operand):\n    value = OP_EVALUATOR.eval_int(operand)\n    if 0 <= value < 65536:\n        return (value % 256, value // 256)\n    raise ValueError", "refactored": true, "pred": {"ppl": 9.82812213897705, "ppl_lower": 11.258212089538574, "ppl/lowercase_ppl": -1.0594464804856383, "ppl/zlib": 0.015134091935331474, "Min_5.0% Prob": 12.131914774576822, "Min_10.0% Prob": 10.732778708140055, "Min_20.0% Prob": 8.134499696584848, "Min_30.0% Prob": 6.607356595993042, "Min_40.0% Prob": 5.556996487654173, "Min_50.0% Prob": 4.546123343886751, "Min_60.0% Prob": 3.8005895245820285}}
{"hexsha": "f46d4201935576f7c5b0f071b01e8b9a5b4caddc", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_proportions_delta_aa(proportions_data_small):\n    exp = Experiment(proportions_data_small, name='proportions-test')\n    test_aa = HypothesisTest(metric='metric', control='A', variation='A', hypothesis='larger', inference_method='proportions_delta')\n    results_aa = exp.run_test(test_aa)\n    assert not results_aa.accept_hypothesis", "fn_id": 4, "class_fn": false, "repo": "quizlet/abracadabra", "file": "test/test_proportions_delta.py", "last_update_at": "2021-09-01T12:25:38+00:00", "original_content": "def test_proportions_delta_aa(proportions_data_small):\n    exp = Experiment(proportions_data_small, name='proportions-test')\n    test_aa = HypothesisTest(metric='metric', control='A', variation='A', hypothesis='larger', inference_method='proportions_delta')\n    results_aa = exp.run_test(test_aa)\n    assert not results_aa.accept_hypothesis", "refactored": true, "pred": {"ppl": 10.520285606384277, "ppl_lower": 11.756855964660645, "ppl/lowercase_ppl": -1.0472234513340013, "ppl/zlib": 0.010844725142105454, "Min_5.0% Prob": 12.06428394317627, "Min_10.0% Prob": 10.128134293989701, "Min_20.0% Prob": 8.016551646319302, "Min_30.0% Prob": 6.729364806955511, "Min_40.0% Prob": 5.566220421682704, "Min_50.0% Prob": 4.598434836736748, "Min_60.0% Prob": 3.907581391841618}}
{"hexsha": "dd575f7aaba2cc90075652902b7b83a55562e8f2", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _load_cache():\n    if not os.path.exists(cache_file_name):\n        raise ValueError('No cache file found.')\n    else:\n        with open(cache_file_name, 'rb') as f:\n            dataset_stats = pickle.load(f)\n    return dataset_stats", "fn_id": 0, "class_fn": false, "repo": "cmusatyalab/dronesearch", "file": "experiments/random_select/plot_random_select_and_filter.py", "last_update_at": "2021-10-08T23:19:12+00:00", "original_content": "def _load_cache():\n    if not os.path.exists(cache_file_name):\n        raise ValueError('No cache file found.')\n    else:\n        with open(cache_file_name, 'rb') as f:\n            dataset_stats = pickle.load(f)\n    return dataset_stats", "refactored": true, "pred": {"ppl": 4.5766472816467285, "ppl_lower": 5.586880683898926, "ppl/lowercase_ppl": -1.13113661156106, "ppl/zlib": 0.008641856224335241, "Min_5.0% Prob": 9.517089207967123, "Min_10.0% Prob": 8.186215196337018, "Min_20.0% Prob": 6.085440476735433, "Min_30.0% Prob": 4.770641586997292, "Min_40.0% Prob": 3.72406780620416, "Min_50.0% Prob": 3.0649880505091436, "Min_60.0% Prob": 2.532699132876264}}
{"hexsha": "d4e3e838774528218776e66d0b619c26190040dd", "ext": "py", "lang": "Python", "content": "@api_view(['PUT'])\n@permission_classes((IsAuthenticated,))\n@allowed_groups(group_names=['admin'])\n@timeing\n@measure_memory_usage\ndef updateUser(request: HttpRequest, id):\n    user = get_object_or_404(User, id=id)\n    groups = request.data.pop('authGroups', [])\n    serialised = UserSerializer(user, data=request.data, context={'request': request}, partial=True)\n    if serialised.is_valid():\n        serialised.save()\n        user_group_many.objects.filter(user=user).delete()\n        for group in groups:\n            user_group_many.objects.create(user=user, group_id=group['id'])\n        return Response(status=status.HTTP_200_OK)\n    print(serialised.error_messages)\n    return Response(status=status.HTTP_400_BAD_REQUEST)", "fn_id": 6, "class_fn": false, "repo": "JetLightStudio/Jet-Gest-stock-management", "file": "server/auth_app/views/authViews.py", "last_update_at": "2021-08-18T18:53:02+00:00", "original_content": "@api_view(['PUT'])\n@permission_classes((IsAuthenticated,))\n@allowed_groups(group_names=['admin'])\ndef updateUser(request: HttpRequest, id):\n    user = get_object_or_404(User, id=id)\n    groups = request.data.pop('authGroups', [])\n    serialised = UserSerializer(user, data=request.data, context={'request': request}, partial=True)\n    if serialised.is_valid():\n        serialised.save()\n        user_group_many.objects.filter(user=user).delete()\n        for group in groups:\n            user_group_many.objects.create(user=user, group_id=group['id'])\n        return Response(status=status.HTTP_200_OK)\n    print(serialised.error_messages)\n    return Response(status=status.HTTP_400_BAD_REQUEST)", "refactored": true, "pred": {"ppl": 3.064091444015503, "ppl_lower": 4.8148651123046875, "ppl/lowercase_ppl": -1.40362267676522, "ppl/zlib": 0.0027923967483430958, "Min_5.0% Prob": 10.02644591331482, "Min_10.0% Prob": 7.575357913970947, "Min_20.0% Prob": 5.126332898934682, "Min_30.0% Prob": 3.646809785612046, "Min_40.0% Prob": 2.7839843835681677, "Min_50.0% Prob": 2.2416450195014477, "Min_60.0% Prob": 1.8728688715232744}}
{"hexsha": "7a6ac05f5c406402bdf62d0f733ffe486364b813", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef speak_rank_top_10():\n    sql = '\\n        select * \\n        from(select row_number() over(order by user_speak_total desc) as row_number,* from user_info)\\n        where row_number <= 10\\n    '\n    data = sql_dql(sql)\n    msg = f'\ud83d\ude48\u53d1\u8a00\u5149\u8363\u699c\ud83d\ude48\\n__________________\\n'\n    for item in data:\n        rank = item[0]\n        name = item[1]\n        id = item[2]\n        num = item[3]\n        num_emoji = rank_emoji(rank)\n        msg += f'{num_emoji}{name}({id})\ud83d\udde3\ufe0f:{num}\\n'\n    msg += f'\u8fd9\u4e9b\ud83d\udc68\u90fd4\ufe0f\u20e3\u5927\ud83d\udca6\ud83d\udc7e'\n    return msg", "fn_id": 2, "class_fn": false, "repo": "Twip-Emma/QQbot-Twip", "file": "bot_plugins/user/user_speaki_rank/__init__.py", "last_update_at": "2021-12-23T15:36:48+00:00", "original_content": "def speak_rank_top_10():\n    sql = '\\n        select * \\n        from(select row_number() over(order by user_speak_total desc) as row_number,* from user_info)\\n        where row_number <= 10\\n    '\n    data = sql_dql(sql)\n    msg = f'\ud83d\ude48\u53d1\u8a00\u5149\u8363\u699c\ud83d\ude48\\n__________________\\n'\n    for item in data:\n        rank = item[0]\n        name = item[1]\n        id = item[2]\n        num = item[3]\n        num_emoji = rank_emoji(rank)\n        msg += f'{num_emoji}{name}({id})\ud83d\udde3\ufe0f:{num}\\n'\n    msg += f'\u8fd9\u4e9b\ud83d\udc68\u90fd4\ufe0f\u20e3\u5927\ud83d\udca6\ud83d\udc7e'\n    return msg", "refactored": true, "pred": {"ppl": 8.181309700012207, "ppl_lower": 8.181309700012207, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.005904079347821224, "Min_5.0% Prob": 12.050430202484131, "Min_10.0% Prob": 10.328891015052795, "Min_20.0% Prob": 7.966468334197998, "Min_30.0% Prob": 6.352534830570221, "Min_40.0% Prob": 5.100220423936844, "Min_50.0% Prob": 4.161165029108524, "Min_60.0% Prob": 3.4927741585299374}}
{"hexsha": "43ee7c52704e9f62de936933b0306b16b4a79105", "ext": "py", "lang": "Python", "content": "def test_model_default_implementations():\n\n    class MyModel(Model):\n        pass\n    model = MyModel()\n    with pytest.raises(NotImplementedError):\n        model.__prior__()\n    with pytest.raises(NotImplementedError):\n        model.__condition__(None, None)\n    with pytest.raises(NotImplementedError):\n        model.__noiseless__()\n    with pytest.raises(NotImplementedError):\n        model.logpdf(None, None)\n    with pytest.raises(NotImplementedError):\n        model.sample(None)\n\n    class MyModel2(Model):\n\n        @timeing\n        @measure_memory_usage\n        def __prior__(self):\n            pass\n\n        def sample(self, x):\n            return 1\n    model = MyModel2()\n    assert model.predict(None, num_samples=100) == (1, 0)", "fn_id": 8, "class_fn": false, "repo": "wesselb/probmods", "file": "tests/test_model.py", "last_update_at": "2021-08-28T16:32:03+00:00", "original_content": "def test_model_default_implementations():\n\n    class MyModel(Model):\n        pass\n    model = MyModel()\n    with pytest.raises(NotImplementedError):\n        model.__prior__()\n    with pytest.raises(NotImplementedError):\n        model.__condition__(None, None)\n    with pytest.raises(NotImplementedError):\n        model.__noiseless__()\n    with pytest.raises(NotImplementedError):\n        model.logpdf(None, None)\n    with pytest.raises(NotImplementedError):\n        model.sample(None)\n\n    class MyModel2(Model):\n\n        def __prior__(self):\n            pass\n\n        def sample(self, x):\n            return 1\n    model = MyModel2()\n    assert model.predict(None, num_samples=100) == (1, 0)", "refactored": true, "pred": {"ppl": 4.609668254852295, "ppl_lower": 5.09550142288208, "ppl/lowercase_ppl": -1.06557065590174, "ppl/zlib": 0.005832656077664418, "Min_5.0% Prob": 11.89727771282196, "Min_10.0% Prob": 9.253603402306052, "Min_20.0% Prob": 6.5195758887699675, "Min_30.0% Prob": 4.890799292005026, "Min_40.0% Prob": 3.7700118601322172, "Min_50.0% Prob": 3.041628712534227, "Min_60.0% Prob": 2.558694365620613}}
{"hexsha": "7d2d407dbcaa652ae9d9750427e051162ad2d2e3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef pattern_loader(path_to_rules='patterns.jsonl') -> List:\n    \"\"\" Loads the patterns from the pattern list. \"\"\"\n    patterns = []\n    patternDicts = []\n    with open(path_to_rules, encoding='utf8') as f:\n        try:\n            for line in f:\n                data = json.loads(line)\n                patternDicts.append(data)\n        except:\n            pass\n    for patternDict in patternDicts:\n        patterns.append(Pattern(**patternDict))\n    return patterns", "fn_id": 5, "class_fn": false, "repo": "ChrisChross/turCy", "file": "turcy/tree_dep_pattern.py", "last_update_at": "2021-04-15T12:29:32+00:00", "original_content": "def pattern_loader(path_to_rules='patterns.jsonl') -> List:\n    \"\"\" Loads the patterns from the pattern list. \"\"\"\n    patterns = []\n    patternDicts = []\n    with open(path_to_rules, encoding='utf8') as f:\n        try:\n            for line in f:\n                data = json.loads(line)\n                patternDicts.append(data)\n        except:\n            pass\n    for patternDict in patternDicts:\n        patterns.append(Pattern(**patternDict))\n    return patterns", "refactored": true, "pred": {"ppl": 4.748387813568115, "ppl_lower": 4.998317718505859, "ppl/lowercase_ppl": -1.0329285383767346, "ppl/zlib": 0.006410720793271029, "Min_5.0% Prob": 10.728687286376953, "Min_10.0% Prob": 9.10416313012441, "Min_20.0% Prob": 6.656434694925944, "Min_30.0% Prob": 4.967399878634347, "Min_40.0% Prob": 3.8858261878291764, "Min_50.0% Prob": 3.0942250491410004, "Min_60.0% Prob": 2.597725767948448}}
{"hexsha": "9a23cc6017e673eb9497903c018a6480ea3f8e60", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef readVDAT(fid, address):\n    vdat = namedtuple('vdat', [])\n    if address != -1:\n        fid.seek(address, 0)\n    [dumCRC, lastSize, lastType, dumMisc] = readARDFpointer(fid, -1)\n    vdat.force = struct.unpack('i', fid.read(4))[0]\n    vdat.line = struct.unpack('i', fid.read(4))[0]\n    vdat.point = struct.unpack('i', fid.read(4))[0]\n    vdat.sizeData = struct.unpack('i', fid.read(4))[0]\n    vdat.forceType = struct.unpack('i', fid.read(4))[0]\n    vdat.pnt0 = struct.unpack('i', fid.read(4))[0]\n    vdat.pnt1 = struct.unpack('i', fid.read(4))[0]\n    vdat.pnt2 = struct.unpack('i', fid.read(4))[0]\n    dum = struct.unpack('i' * 2, fid.read(4 * 2))[0]\n    sizeData = vdat.sizeData\n    vdat.data = struct.unpack('f' * sizeData, fid.read(4 * sizeData))\n    return vdat", "fn_id": 6, "class_fn": false, "repo": "yu-efremov/ViscoIndent", "file": "import_ARDF.py", "last_update_at": "2021-12-21T13:26:53+00:00", "original_content": "def readVDAT(fid, address):\n    vdat = namedtuple('vdat', [])\n    if address != -1:\n        fid.seek(address, 0)\n    [dumCRC, lastSize, lastType, dumMisc] = readARDFpointer(fid, -1)\n    vdat.force = struct.unpack('i', fid.read(4))[0]\n    vdat.line = struct.unpack('i', fid.read(4))[0]\n    vdat.point = struct.unpack('i', fid.read(4))[0]\n    vdat.sizeData = struct.unpack('i', fid.read(4))[0]\n    vdat.forceType = struct.unpack('i', fid.read(4))[0]\n    vdat.pnt0 = struct.unpack('i', fid.read(4))[0]\n    vdat.pnt1 = struct.unpack('i', fid.read(4))[0]\n    vdat.pnt2 = struct.unpack('i', fid.read(4))[0]\n    dum = struct.unpack('i' * 2, fid.read(4 * 2))[0]\n    sizeData = vdat.sizeData\n    vdat.data = struct.unpack('f' * sizeData, fid.read(4 * sizeData))\n    return vdat", "refactored": true, "pred": {"ppl": 3.701874017715454, "ppl_lower": 3.774176597595215, "ppl/lowercase_ppl": -1.0147787876627694, "ppl/zlib": 0.004657790685769447, "Min_5.0% Prob": 12.035258928934732, "Min_10.0% Prob": 9.578528690338135, "Min_20.0% Prob": 6.245567617813746, "Min_30.0% Prob": 4.324063472118643, "Min_40.0% Prob": 3.275944597677638, "Min_50.0% Prob": 2.625758783134321, "Min_60.0% Prob": 2.1886170303500774}}
{"hexsha": "4621fba19fcf958d4877519ee2e7db8ca9d513e9", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef search_froms():\n    \"\"\"\n    Search for unique \"from xxx import yyy\" statements, returning a sorted list.\n    \"\"\"\n    _froms = []\n    _lib_dir_regex = '{}\\\\.'.format(LIB_DIR)\n    _regex = '^[ ]*from '\n    print(Fore.GREEN + '\\nimport list contents:' + Style.RESET_ALL)\n    for _items in _find_in_files('^[ ]*from.*import.*'):\n        _item = _items[0]\n        _filename = _items[1]\n        _library_name = re.sub(' import.*', '', _item)\n        _library_name = re.sub(_regex, '', _library_name).strip()\n        if IGNORE_LIB and re.match(_lib_dir_regex, _library_name):\n            pass\n        elif len(_library_name) > 1 and _library_name not in _froms:\n            _froms.append(_library_name)\n    _froms.sort()\n    print(Fore.CYAN + \"\\n-- complete: {:d} instances of '{}' found.\".format(len(_froms), _regex) + Style.RESET_ALL)\n    return _froms", "fn_id": 3, "class_fn": false, "repo": "ifurusato/ros", "file": "import_report.py", "last_update_at": "2021-07-23T14:20:05+00:00", "original_content": "def search_froms():\n    \"\"\"\n    Search for unique \"from xxx import yyy\" statements, returning a sorted list.\n    \"\"\"\n    _froms = []\n    _lib_dir_regex = '{}\\\\.'.format(LIB_DIR)\n    _regex = '^[ ]*from '\n    print(Fore.GREEN + '\\nimport list contents:' + Style.RESET_ALL)\n    for _items in _find_in_files('^[ ]*from.*import.*'):\n        _item = _items[0]\n        _filename = _items[1]\n        _library_name = re.sub(' import.*', '', _item)\n        _library_name = re.sub(_regex, '', _library_name).strip()\n        if IGNORE_LIB and re.match(_lib_dir_regex, _library_name):\n            pass\n        elif len(_library_name) > 1 and _library_name not in _froms:\n            _froms.append(_library_name)\n    _froms.sort()\n    print(Fore.CYAN + \"\\n-- complete: {:d} instances of '{}' found.\".format(len(_froms), _regex) + Style.RESET_ALL)\n    return _froms", "refactored": true, "pred": {"ppl": 6.345555305480957, "ppl_lower": 6.736442565917969, "ppl/lowercase_ppl": -1.0323513522447094, "ppl/zlib": 0.004142947569352335, "Min_5.0% Prob": 11.597482387836163, "Min_10.0% Prob": 9.450496002479836, "Min_20.0% Prob": 7.017050570911831, "Min_30.0% Prob": 5.592971058539402, "Min_40.0% Prob": 4.483100791589929, "Min_50.0% Prob": 3.675816097899395, "Min_60.0% Prob": 3.0868038300630505}}
{"hexsha": "f5243a5a3b358ca43895541eb22f25937e8336f3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef try_to_get_last_line_as_json(file_location: str, max_tries=3, await_in_seconds_between_tries=1) -> Optional[dict]:\n    attempts = 0\n    while True:\n        line = last_line_from_some_file(file_location)\n        line_as_json = json.loads(line) if line else None\n        if line_as_json:\n            return line_as_json\n        if attempts >= max_tries:\n            return None\n        sleep(await_in_seconds_between_tries)\n        attempts += 1", "fn_id": 5, "class_fn": false, "repo": "juntossomosmais/python-fluentd-testing", "file": "python_fluentd_testing/utils.py", "last_update_at": "2021-11-12T20:00:54+00:00", "original_content": "def try_to_get_last_line_as_json(file_location: str, max_tries=3, await_in_seconds_between_tries=1) -> Optional[dict]:\n    attempts = 0\n    while True:\n        line = last_line_from_some_file(file_location)\n        line_as_json = json.loads(line) if line else None\n        if line_as_json:\n            return line_as_json\n        if attempts >= max_tries:\n            return None\n        sleep(await_in_seconds_between_tries)\n        attempts += 1", "refactored": true, "pred": {"ppl": 3.633013963699341, "ppl_lower": 4.328157424926758, "ppl/lowercase_ppl": -1.1357138158325273, "ppl/zlib": 0.00533083717687728, "Min_5.0% Prob": 9.921240670340401, "Min_10.0% Prob": 8.222033432551793, "Min_20.0% Prob": 5.686419186920955, "Min_30.0% Prob": 4.235534123210019, "Min_40.0% Prob": 3.2055073338079043, "Min_50.0% Prob": 2.593332808387155, "Min_60.0% Prob": 2.1491799282295436}}
{"hexsha": "b82969bc09d27dd516accba17236411cb1f5adb8", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_get_binary_patterns():\n    wl = Wordlist(data_path('wichmannmixezoquean.tsv'))\n    pats, characters = get_binary_patterns(wl, 'cogid')\n    etd = wl.get_etymdict(ref='cogid')\n    assert len(etd) == len(pats)", "fn_id": 0, "class_fn": false, "repo": "WesScivetti/data", "file": "tests/test_wordlist.py", "last_update_at": "2021-05-23T18:25:06+00:00", "original_content": "def test_get_binary_patterns():\n    wl = Wordlist(data_path('wichmannmixezoquean.tsv'))\n    pats, characters = get_binary_patterns(wl, 'cogid')\n    etd = wl.get_etymdict(ref='cogid')\n    assert len(etd) == len(pats)", "refactored": true, "pred": {"ppl": 30.747798919677734, "ppl_lower": 38.32759475708008, "ppl/lowercase_ppl": -1.0643208974226546, "ppl/zlib": 0.019802418565564677, "Min_5.0% Prob": 13.947126150131226, "Min_10.0% Prob": 13.137139558792114, "Min_20.0% Prob": 11.029025302213782, "Min_30.0% Prob": 9.126697356884296, "Min_40.0% Prob": 7.897022997631746, "Min_50.0% Prob": 6.687755667886068, "Min_60.0% Prob": 5.669097416102886}}
{"hexsha": "1609ca3cfecd7e4bdac13a4eed7e722233b86773", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\nasync def get_prefix(bot_, message):\n    \"\"\"Returns the appropriate prefix for the bot.\"\"\"\n    with open('./data/options.json', 'r') as options_file:\n        options_dict = json.load(options_file)\n    if message.guild and str(message.guild.id) in options_dict:\n        prefixes = options_dict[str(message.guild.id)]['prefix']\n    else:\n        prefixes = '.'\n    return commands.when_mentioned_or(*prefixes)(bot_, message)", "fn_id": 0, "class_fn": false, "repo": "ethantv1234567890/ServerAntiRaid", "file": "main.py", "last_update_at": "2021-01-28T06:22:08+00:00", "original_content": "async def get_prefix(bot_, message):\n    \"\"\"Returns the appropriate prefix for the bot.\"\"\"\n    with open('./data/options.json', 'r') as options_file:\n        options_dict = json.load(options_file)\n    if message.guild and str(message.guild.id) in options_dict:\n        prefixes = options_dict[str(message.guild.id)]['prefix']\n    else:\n        prefixes = '.'\n    return commands.when_mentioned_or(*prefixes)(bot_, message)", "refactored": true, "pred": {"ppl": 3.593437910079956, "ppl_lower": 3.6593246459960938, "ppl/lowercase_ppl": -1.0142045929361974, "ppl/zlib": 0.005178580483319712, "Min_5.0% Prob": 10.65631071726481, "Min_10.0% Prob": 8.248348911603292, "Min_20.0% Prob": 5.79907230536143, "Min_30.0% Prob": 4.24661922454834, "Min_40.0% Prob": 3.241239435194681, "Min_50.0% Prob": 2.556466150143352, "Min_60.0% Prob": 2.137408889141189}}
{"hexsha": "9886643486593889d35875fc98bf1b7805dee00d", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize('operation', [cirq.CNOT(AspenQubit(0, 1), AspenQubit(0, 2))])\n@timeing\n@measure_memory_usage\ndef test_rigetti_qcs_aspen_device_valid_operation(operation: cirq.Operation, qcs_aspen8_isa: InstructionSetArchitecture):\n    \"\"\"test RigettiQCSAspenDevice throws no error when validating 2Q operations on\n    adjacent qubits\n    \"\"\"\n    device = RigettiQCSAspenDevice(isa=qcs_aspen8_isa)\n    device.validate_operation(operation)", "fn_id": 14, "class_fn": false, "repo": "dabacon/Cirq", "file": "cirq-rigetti/cirq_rigetti/aspen_device_test.py", "last_update_at": "2021-04-29T15:30:32+00:00", "original_content": "@pytest.mark.parametrize('operation', [cirq.CNOT(AspenQubit(0, 1), AspenQubit(0, 2))])\ndef test_rigetti_qcs_aspen_device_valid_operation(operation: cirq.Operation, qcs_aspen8_isa: InstructionSetArchitecture):\n    \"\"\"test RigettiQCSAspenDevice throws no error when validating 2Q operations on\n    adjacent qubits\n    \"\"\"\n    device = RigettiQCSAspenDevice(isa=qcs_aspen8_isa)\n    device.validate_operation(operation)", "refactored": true, "pred": {"ppl": 7.8939924240112305, "ppl_lower": 12.674783706665039, "ppl/lowercase_ppl": -1.2291815521338534, "ppl/zlib": 0.007767300817764577, "Min_5.0% Prob": 13.086095128740583, "Min_10.0% Prob": 11.117058004651751, "Min_20.0% Prob": 8.155838917041647, "Min_30.0% Prob": 6.358531247737796, "Min_40.0% Prob": 5.039956423743018, "Min_50.0% Prob": 4.099203833570219, "Min_60.0% Prob": 3.4597852876645394}}
{"hexsha": "8090bbc869f16bd9ee00e270a16a5e0352ae7028", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef categoryFilter(doc):\n    categories = doc.Settings.Categories\n    cats = [ElementCategoryFilter(c.Id) for c in categories if c.CategoryType == CategoryType.Model and c.CanAddSubcategory]\n    filter = None\n    if len(cats):\n        try:\n            filter = LogicalOrFilter(List[ElementFilter](cats))\n        except Exception as ex:\n            raise Exception(str(ex) + str(len(cats)))\n    return filter", "fn_id": 0, "class_fn": false, "repo": "pabloderen/pyRevitExtension", "file": "lib/tools.py", "last_update_at": "2021-10-24T00:05:02+00:00", "original_content": "def categoryFilter(doc):\n    categories = doc.Settings.Categories\n    cats = [ElementCategoryFilter(c.Id) for c in categories if c.CategoryType == CategoryType.Model and c.CanAddSubcategory]\n    filter = None\n    if len(cats):\n        try:\n            filter = LogicalOrFilter(List[ElementFilter](cats))\n        except Exception as ex:\n            raise Exception(str(ex) + str(len(cats)))\n    return filter", "refactored": true, "pred": {"ppl": 9.252999305725098, "ppl_lower": 16.643234252929688, "ppl/lowercase_ppl": -1.2638516050093327, "ppl/zlib": 0.00942774469590881, "Min_5.0% Prob": 12.330568885803222, "Min_10.0% Prob": 10.734030437469482, "Min_20.0% Prob": 8.31010559626988, "Min_30.0% Prob": 6.633686415851116, "Min_40.0% Prob": 5.36331411017928, "Min_50.0% Prob": 4.414478591194859, "Min_60.0% Prob": 3.7045413221304235}}
{"hexsha": "534d6702940b8bc6ddbaba4862587a1683235de5", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef view_form_force(form, force, forcescale=0.5, edge_label=True):\n    if edge_label:\n        form_edge_label = {uv: index for index, uv in enumerate(form.edges())}\n        force_edge_label = force_edge_labels\n    else:\n        form_edge_label = None\n        force_edge_label = None\n    viewer = Viewer(form, force, delay_setup=False)\n    viewer.draw_form(edgelabel=form_edge_label, forces_on=True, forcescale=forcescale, vertexcolor={key: '#000000' for key in form.vertices_where({'is_fixed': True})})\n    viewer.draw_force(edgelabel=force_edge_label)\n    viewer.show()", "fn_id": 0, "class_fn": false, "repo": "BlockResearchGroup/compas_ags", "file": "scripts/paper-CSD/exampleD_truss_constant.py", "last_update_at": "2021-12-15T18:47:04+00:00", "original_content": "def view_form_force(form, force, forcescale=0.5, edge_label=True):\n    if edge_label:\n        form_edge_label = {uv: index for index, uv in enumerate(form.edges())}\n        force_edge_label = force_edge_labels\n    else:\n        form_edge_label = None\n        force_edge_label = None\n    viewer = Viewer(form, force, delay_setup=False)\n    viewer.draw_form(edgelabel=form_edge_label, forces_on=True, forcescale=forcescale, vertexcolor={key: '#000000' for key in form.vertices_where({'is_fixed': True})})\n    viewer.draw_force(edgelabel=force_edge_label)\n    viewer.show()", "refactored": true, "pred": {"ppl": 5.084969997406006, "ppl_lower": 7.233085632324219, "ppl/lowercase_ppl": -1.2166752463034958, "ppl/zlib": 0.005871079889054063, "Min_5.0% Prob": 10.521718872918022, "Min_10.0% Prob": 8.784660524792141, "Min_20.0% Prob": 6.405806199924366, "Min_30.0% Prob": 4.936391543064799, "Min_40.0% Prob": 3.9676568894772917, "Min_50.0% Prob": 3.2475572470375287, "Min_60.0% Prob": 2.7105979789713666}}
{"hexsha": "b36cd3ac7c6ccf4bb43ebb3004f30af35e47297a", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef generateCoordsForLayersPG(dictDlsLayers, graphvizLayout, isHorizontal):\n    theGraph = pyg.AGraph()\n    for kk, vv in dictDlsLayers.items():\n        for ll in vv['cfg']['wires']:\n            theGraph.add_edge(kk, ll)\n    if isHorizontal:\n        pArgs = '-Grankdir=LR'\n    else:\n        pArgs = '-Grankdir=TB'\n    if graphvizLayout is None:\n        graphvizLayout = 'dot'\n    theGraph.layout(prog=graphvizLayout, args=pArgs)\n    theGraphPos = {}\n    for kk in dictDlsLayers.keys():\n        tnode = theGraph.get_node(kk)\n        tpos = [int(float(xx)) for xx in tnode.attr['pos'].split(',')]\n        theGraphPos[kk] = tuple(tpos)\n    return theGraphPos", "fn_id": 2, "class_fn": false, "repo": "SummaLabs/DLS", "file": "app/backend/core/models/convertors/keras2dls.py", "last_update_at": "2021-02-16T23:08:34+00:00", "original_content": "def generateCoordsForLayersPG(dictDlsLayers, graphvizLayout, isHorizontal):\n    theGraph = pyg.AGraph()\n    for kk, vv in dictDlsLayers.items():\n        for ll in vv['cfg']['wires']:\n            theGraph.add_edge(kk, ll)\n    if isHorizontal:\n        pArgs = '-Grankdir=LR'\n    else:\n        pArgs = '-Grankdir=TB'\n    if graphvizLayout is None:\n        graphvizLayout = 'dot'\n    theGraph.layout(prog=graphvizLayout, args=pArgs)\n    theGraphPos = {}\n    for kk in dictDlsLayers.keys():\n        tnode = theGraph.get_node(kk)\n        tpos = [int(float(xx)) for xx in tnode.attr['pos'].split(',')]\n        theGraphPos[kk] = tuple(tpos)\n    return theGraphPos", "refactored": true, "pred": {"ppl": 5.455055236816406, "ppl_lower": 6.561359405517578, "ppl/lowercase_ppl": -1.1088419748502123, "ppl/zlib": 0.004975198666098582, "Min_5.0% Prob": 10.976488876342774, "Min_10.0% Prob": 9.654022035144624, "Min_20.0% Prob": 7.102660860334124, "Min_30.0% Prob": 5.4247149418270775, "Min_40.0% Prob": 4.202947814194929, "Min_50.0% Prob": 3.385132773362455, "Min_60.0% Prob": 2.8260502477309535}}
{"hexsha": "3fa1da9767d379e042b89a14701794778faba152", "ext": "py", "lang": "Python", "content": "@numba.njit([types.Long(types.Keys)], cache=True)\n@timeing\n@measure_memory_usage\ndef find_depth(tree):\n    \"\"\"\n    Return maximum depth of a linear octree.\n    \"\"\"\n    levels = morton.find_level(np.unique(tree))\n    return np.max(levels)", "fn_id": 7, "class_fn": false, "repo": "Excalibur-SLE/AdapOctree", "file": "adaptoctree/tree.py", "last_update_at": "2021-04-27T10:58:52+00:00", "original_content": "@numba.njit([types.Long(types.Keys)], cache=True)\ndef find_depth(tree):\n    \"\"\"\n    Return maximum depth of a linear octree.\n    \"\"\"\n    levels = morton.find_level(np.unique(tree))\n    return np.max(levels)", "refactored": true, "pred": {"ppl": 24.918251037597656, "ppl_lower": 28.430702209472656, "ppl/lowercase_ppl": -1.0410091726987356, "ppl/zlib": 0.01858728617557427, "Min_5.0% Prob": 16.165836016337078, "Min_10.0% Prob": 13.663909367152623, "Min_20.0% Prob": 11.044818464914957, "Min_30.0% Prob": 9.070112878626043, "Min_40.0% Prob": 7.533901381492615, "Min_50.0% Prob": 6.377905006344254, "Min_60.0% Prob": 5.317795087893804}}
{"hexsha": "52b60b2ec451113de42ad06d277c614499c2007a", "ext": "py", "lang": "Python", "content": "@expose_builtin('del_attrs', unwrap_spec=['obj'])\n@timeing\n@measure_memory_usage\ndef impl_del_attrs(engine, heap, attvar):\n    if isinstance(attvar, AttVar):\n        if attvar.value_list is not None:\n            for name, index in attvar.attmap.indexes.iteritems():\n                heap.add_trail_atts(attvar, name)\n            attvar.value_list = None", "fn_id": 5, "class_fn": false, "repo": "ForoughA/CORGI", "file": "testnet/prolog/builtin/attvars.py", "last_update_at": "2021-12-11T12:39:51+00:00", "original_content": "@expose_builtin('del_attrs', unwrap_spec=['obj'])\ndef impl_del_attrs(engine, heap, attvar):\n    if isinstance(attvar, AttVar):\n        if attvar.value_list is not None:\n            for name, index in attvar.attmap.indexes.iteritems():\n                heap.add_trail_atts(attvar, name)\n            attvar.value_list = None", "refactored": true, "pred": {"ppl": 16.552566528320312, "ppl_lower": 18.99454116821289, "ppl/lowercase_ppl": -1.0490320489598028, "ppl/zlib": 0.01311467835065298, "Min_5.0% Prob": 13.528024864196777, "Min_10.0% Prob": 11.782161235809326, "Min_20.0% Prob": 9.482600021362305, "Min_30.0% Prob": 7.787464745839437, "Min_40.0% Prob": 6.52483069896698, "Min_50.0% Prob": 5.526974868774414, "Min_60.0% Prob": 4.700025936464469}}
{"hexsha": "504b87ddee9917d8fa486f13be56f5b536985c45", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_client_with_wrong_credentials(zendesk_credentials):\n    \"\"\"Test check with wrong credentials\"\"\"\n    client = Client(**zendesk_credentials)\n    alive, error = client.health_check()\n    assert not alive\n    assert error", "fn_id": 0, "class_fn": false, "repo": "rajatariya21/airbyte", "file": "airbyte-integrations/connectors/source-zendesk-talk/unit_tests/unit_test.py", "last_update_at": "2021-04-30T13:53:34+00:00", "original_content": "def test_client_with_wrong_credentials(zendesk_credentials):\n    \"\"\"Test check with wrong credentials\"\"\"\n    client = Client(**zendesk_credentials)\n    alive, error = client.health_check()\n    assert not alive\n    assert error", "refactored": true, "pred": {"ppl": 11.205251693725586, "ppl_lower": 13.445577621459961, "ppl/lowercase_ppl": -1.0754299767208704, "ppl/zlib": 0.015197374651740385, "Min_5.0% Prob": 12.486626625061035, "Min_10.0% Prob": 10.463231722513834, "Min_20.0% Prob": 8.270302891731262, "Min_30.0% Prob": 6.801601860258314, "Min_40.0% Prob": 5.770979156096776, "Min_50.0% Prob": 4.691242375681477, "Min_60.0% Prob": 4.013750478222564}}
{"hexsha": "241fb0211e37aae619cf601b0686c40ee27576eb", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef update_doing_card():\n    \"\"\"\n    DOING \uc0c1\ud0dc\uc758 \uce74\ub4dc\uc758 DURATION\uc744 \uc0c8\ub85c \uae30\ub85d\ud569\ub2c8\ub2e4.\n    DURATION\uc758 \uac12\uc774 \uc5c6\uc744 \uacbd\uc6b0\uc5d0\ub294 \uc2dc\uc791\uc77c\uc744 \ub2f9\uc77c\ub85c \uc124\uc815\ud569\ub2c8\ub2e4.\n    \"\"\"\n    status = notion.CARD_STATUS\n    doing = notion.DOING\n    duration = notion.CARD_DURATION\n    today = date.today()\n    for card_id in notion.get_filtered_card_ids(status, doing):\n        card = notion.client.get_block(card_id)\n        notion_date = card.get_property(duration)\n        if not notion_date or notion_date.start == None:\n            notion_date = NotionDate(start=today)\n        card.set_property(duration, notion_date)", "fn_id": 1, "class_fn": false, "repo": "wormwlrm/notion-scrum-analytics", "file": "commands/run_update_duration.py", "last_update_at": "2021-08-29T03:23:02+00:00", "original_content": "def update_doing_card():\n    \"\"\"\n    DOING \uc0c1\ud0dc\uc758 \uce74\ub4dc\uc758 DURATION\uc744 \uc0c8\ub85c \uae30\ub85d\ud569\ub2c8\ub2e4.\n    DURATION\uc758 \uac12\uc774 \uc5c6\uc744 \uacbd\uc6b0\uc5d0\ub294 \uc2dc\uc791\uc77c\uc744 \ub2f9\uc77c\ub85c \uc124\uc815\ud569\ub2c8\ub2e4.\n    \"\"\"\n    status = notion.CARD_STATUS\n    doing = notion.DOING\n    duration = notion.CARD_DURATION\n    today = date.today()\n    for card_id in notion.get_filtered_card_ids(status, doing):\n        card = notion.client.get_block(card_id)\n        notion_date = card.get_property(duration)\n        if not notion_date or notion_date.start == None:\n            notion_date = NotionDate(start=today)\n        card.set_property(duration, notion_date)", "refactored": true, "pred": {"ppl": 5.970540523529053, "ppl_lower": 7.334718227386475, "ppl/lowercase_ppl": -1.1151652218066688, "ppl/zlib": 0.004764899902028351, "Min_5.0% Prob": 10.577403174506294, "Min_10.0% Prob": 9.064359611935085, "Min_20.0% Prob": 6.8824057321290715, "Min_30.0% Prob": 5.442854772721018, "Min_40.0% Prob": 4.362428941726685, "Min_50.0% Prob": 3.5606756478231003, "Min_60.0% Prob": 2.9814735590356642}}
{"hexsha": "7354359d5802ce995119a41c90b85838a4204d5d", "ext": "py", "lang": "Python", "content": "@app.route('/commands_per_day')\n@cache(time=datetime.timedelta(minutes=DEFAULT_CACHE_MINUTES))\n@timeing\n@measure_memory_usage\ndef commands_per_day():\n    command_names = [row.get('command_name') for row in bigquery_client.query('SELECT DISTINCT command_name FROM analytics.commands').result()]\n    print(command_names)\n    command_names = filter(lambda item: item not in ['list', 'set', 'voices', 'languages', 'property'], command_names)\n    result = {}\n    for command_name in command_names:\n        usage = {date: {'text_count': 0, 'slash_count': 0} for date in get_days_in_range(datetime.datetime(2021, 1, 1), datetime.datetime.today())}\n        query = 'SELECT DATE(time) as d, COUNTIF(NOT is_slash) as cnt, COUNTIF(is_slash) as slash_cnt FROM analytics.commands WHERE command_name = @command_name GROUP BY d ORDER BY d'\n        job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ScalarQueryParameter('command_name', 'STRING', command_name)])\n        for row in bigquery_client.query(query, job_config=job_config).result():\n            d = row_to_dict(row)\n            usage[d['d']]['text_count'] = d['cnt']\n            usage[d['d']]['slash_count'] = d['slash_cnt']\n        result[command_name] = [{'date': date, **usage[date]} for date in usage.keys()]\n    response = jsonify(result)\n    response.headers['Access-Control-Allow-Origin'] = '*'\n    return response", "fn_id": 4, "class_fn": false, "repo": "TychoTheTaco/Discord-Dictionary-Bot", "file": "api/main.py", "last_update_at": "2021-12-12T20:35:49+00:00", "original_content": "@app.route('/commands_per_day')\n@cache(time=datetime.timedelta(minutes=DEFAULT_CACHE_MINUTES))\ndef commands_per_day():\n    command_names = [row.get('command_name') for row in bigquery_client.query('SELECT DISTINCT command_name FROM analytics.commands').result()]\n    print(command_names)\n    command_names = filter(lambda item: item not in ['list', 'set', 'voices', 'languages', 'property'], command_names)\n    result = {}\n    for command_name in command_names:\n        usage = {date: {'text_count': 0, 'slash_count': 0} for date in get_days_in_range(datetime.datetime(2021, 1, 1), datetime.datetime.today())}\n        query = 'SELECT DATE(time) as d, COUNTIF(NOT is_slash) as cnt, COUNTIF(is_slash) as slash_cnt FROM analytics.commands WHERE command_name = @command_name GROUP BY d ORDER BY d'\n        job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ScalarQueryParameter('command_name', 'STRING', command_name)])\n        for row in bigquery_client.query(query, job_config=job_config).result():\n            d = row_to_dict(row)\n            usage[d['d']]['text_count'] = d['cnt']\n            usage[d['d']]['slash_count'] = d['slash_cnt']\n        result[command_name] = [{'date': date, **usage[date]} for date in usage.keys()]\n    response = jsonify(result)\n    response.headers['Access-Control-Allow-Origin'] = '*'\n    return response", "refactored": true, "pred": {"ppl": 3.285273551940918, "ppl_lower": 3.7902750968933105, "ppl/lowercase_ppl": -1.1202141236008234, "ppl/zlib": 0.0018187307668363027, "Min_5.0% Prob": 10.466434704629998, "Min_10.0% Prob": 7.874661580110208, "Min_20.0% Prob": 5.3691961584211905, "Min_30.0% Prob": 3.8997251932360544, "Min_40.0% Prob": 2.964786115073183, "Min_50.0% Prob": 2.37640671329994, "Min_60.0% Prob": 1.9886073094045258}}
{"hexsha": "f31065a13878fbae4e2467fa58de8efe0ace1f13", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_delete_all_album():\n    rq_album.delete_all(token=env.env_var['USER_1_TOKEN'], user_id=env.env_var['USER_1_MAIL'])\n    rq_album.delete_all(token=env.env_var['USER_2_TOKEN'], user_id=env.env_var['USER_2_MAIL'])\n    rq_album.delete_all(token=env.env_var['USER_3_TOKEN'], user_id=env.env_var['USER_3_MAIL'])", "fn_id": 3, "class_fn": false, "repo": "OsiriX-Foundation/IntegrationTest", "file": "test_sharing_with_album_token_no_permission.py", "last_update_at": "2021-06-02T07:41:28+00:00", "original_content": "def test_delete_all_album():\n    rq_album.delete_all(token=env.env_var['USER_1_TOKEN'], user_id=env.env_var['USER_1_MAIL'])\n    rq_album.delete_all(token=env.env_var['USER_2_TOKEN'], user_id=env.env_var['USER_2_MAIL'])\n    rq_album.delete_all(token=env.env_var['USER_3_TOKEN'], user_id=env.env_var['USER_3_MAIL'])", "refactored": true, "pred": {"ppl": 3.3263275623321533, "ppl_lower": 3.3419857025146484, "ppl/lowercase_ppl": -1.0039074901630305, "ppl/zlib": 0.008463865220343838, "Min_5.0% Prob": 10.273416678110758, "Min_10.0% Prob": 8.376445183387169, "Min_20.0% Prob": 5.674199607637194, "Min_30.0% Prob": 4.038693832915004, "Min_40.0% Prob": 3.03491408476098, "Min_50.0% Prob": 2.4209590032781083, "Min_60.0% Prob": 2.012745433445271}}
{"hexsha": "1b7e2d214e5eeb85cf50702bb2e69e77348f954c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _delete_old(old_ids):\n    db = get_db('yahoo')\n    for item in ['financial_data', 'key_stats']:\n        collection = db[item]\n        id_ = old_ids[item]\n        if id_:\n            result = collection.delete_many({'_id': {'$lte': id_}})\n            logger.info(f'\u5220\u9664 {item} \u65e7\u6570\u636e {result.deleted_count} \u884c')", "fn_id": 5, "class_fn": false, "repo": "NeoBert/liudengfeng-cnswd", "file": "cnswd/scripts/yahoo.py", "last_update_at": "2021-06-26T13:23:01+00:00", "original_content": "def _delete_old(old_ids):\n    db = get_db('yahoo')\n    for item in ['financial_data', 'key_stats']:\n        collection = db[item]\n        id_ = old_ids[item]\n        if id_:\n            result = collection.delete_many({'_id': {'$lte': id_}})\n            logger.info(f'\u5220\u9664 {item} \u65e7\u6570\u636e {result.deleted_count} \u884c')", "refactored": true, "pred": {"ppl": 7.336343765258789, "ppl_lower": 7.336343765258789, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.008200990098341545, "Min_5.0% Prob": 10.430895614624024, "Min_10.0% Prob": 9.363442039489746, "Min_20.0% Prob": 7.177665437970843, "Min_30.0% Prob": 5.943679355084896, "Min_40.0% Prob": 4.896154792535873, "Min_50.0% Prob": 3.984517769993476, "Min_60.0% Prob": 3.324924901535269}}
{"hexsha": "a9df1185d80d2c045bcd42e083755fc20bd784de", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef split_indices(idx, n_total, fractions):\n    \"\"\"\n    splits idx containing n_total distinct events into fractions given in fractions list.\n    returns the number of events in each split\n    \"\"\"\n    num_ids = [ceil(n_total * f) for f in fractions]\n    if sum(num_ids) > n_total:\n        num_ids[-1] -= sum(num_ids) - n_total\n    return num_ids", "fn_id": 0, "class_fn": false, "repo": "fact-project/classifier", "file": "aict_tools/scripts/split_data.py", "last_update_at": "2021-01-22T11:57:26+00:00", "original_content": "def split_indices(idx, n_total, fractions):\n    \"\"\"\n    splits idx containing n_total distinct events into fractions given in fractions list.\n    returns the number of events in each split\n    \"\"\"\n    num_ids = [ceil(n_total * f) for f in fractions]\n    if sum(num_ids) > n_total:\n        num_ids[-1] -= sum(num_ids) - n_total\n    return num_ids", "refactored": true, "pred": {"ppl": 6.094149589538574, "ppl_lower": 6.094149589538574, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.00864750826450732, "Min_5.0% Prob": 9.632501792907714, "Min_10.0% Prob": 8.780213529413397, "Min_20.0% Prob": 7.040318153121254, "Min_30.0% Prob": 5.520629976734971, "Min_40.0% Prob": 4.404382761229169, "Min_50.0% Prob": 3.6118913970210333, "Min_60.0% Prob": 3.0332204316262947}}
{"hexsha": "0e43d54d54e3eafdf583b28f0be4bd0753b4013d", "ext": "py", "lang": "Python", "content": "@app.route('/json-rpc', methods=['GET', 'POST', 'OPTIONS'])\n@timeing\n@measure_memory_usage\ndef jsonrpc():\n    result = ''\n    if request.headers['CONTENT_LENGTH']:\n        handler = JSONRPCHandler()\n        result = handler.handle(request.data)\n    response = make_response(result, 200)\n    hdr = 'Access-Control-Allow-%s'\n    response.headers[hdr % 'Origin'] = '*'\n    response.headers[hdr % 'Headers'] = 'Content-Type, Authentication'\n    response.headers[hdr % 'Method'] = 'POST, GET, OPTIONS, PUT, DELETE, TRACE'\n    return response", "fn_id": 0, "class_fn": false, "repo": "MasonM/apeye", "file": "examples/api.apeye.org/example_server/rpc.py", "last_update_at": "2021-07-27T18:31:15+00:00", "original_content": "@app.route('/json-rpc', methods=['GET', 'POST', 'OPTIONS'])\ndef jsonrpc():\n    result = ''\n    if request.headers['CONTENT_LENGTH']:\n        handler = JSONRPCHandler()\n        result = handler.handle(request.data)\n    response = make_response(result, 200)\n    hdr = 'Access-Control-Allow-%s'\n    response.headers[hdr % 'Origin'] = '*'\n    response.headers[hdr % 'Headers'] = 'Content-Type, Authentication'\n    response.headers[hdr % 'Method'] = 'POST, GET, OPTIONS, PUT, DELETE, TRACE'\n    return response", "refactored": true, "pred": {"ppl": 4.643282890319824, "ppl_lower": 5.976508617401123, "ppl/lowercase_ppl": -1.1643945303655427, "ppl/zlib": 0.0049055004330023685, "Min_5.0% Prob": 12.546367645263672, "Min_10.0% Prob": 9.538506698608398, "Min_20.0% Prob": 6.655088837941488, "Min_30.0% Prob": 4.874094156424205, "Min_40.0% Prob": 3.7903921430309615, "Min_50.0% Prob": 3.0595582817991573, "Min_60.0% Prob": 2.5567579789294137}}
{"hexsha": "6461501746c4c91ad65a43d4651b57d08946bf5f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef contains_inappropriate_phrases(tweet_text):\n    if 'sex' in tweet_text:\n        return True\n    return False", "fn_id": 4, "class_fn": false, "repo": "mgreiler/code-review-twitter-bot", "file": "code-review-bot/matching_rules.py", "last_update_at": "2021-10-04T08:36:17+00:00", "original_content": "def contains_inappropriate_phrases(tweet_text):\n    if 'sex' in tweet_text:\n        return True\n    return False", "refactored": true, "pred": {"ppl": 13.863848686218262, "ppl_lower": 16.876739501953125, "ppl/lowercase_ppl": -1.0747928436437186, "ppl/zlib": 0.023475755698243773, "Min_5.0% Prob": 11.60136079788208, "Min_10.0% Prob": 10.597446918487549, "Min_20.0% Prob": 8.852844655513763, "Min_30.0% Prob": 7.426323215166728, "Min_40.0% Prob": 6.3284007757902145, "Min_50.0% Prob": 5.202768009901047, "Min_60.0% Prob": 4.363802268790702}}
{"hexsha": "dedf80bf95ac67960c80fd880c96b74b9b20d6cf", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_local_missing_url(tmpdir):\n    \"\"\" Test creating a toolchain from path to a local feed xml \"\"\"\n    full = '<feed>\\n<feed />\\n</feed>\\n'\n    with pytest.raises(Exception) as e:\n        _generic_test_local(tmpdir, full)\n    assert 'not parse' in str(e)\n    assert \"Non-root 'feed' element must have an 'url' attribute\" in str(e)", "fn_id": 9, "class_fn": false, "repo": "aldebaran/qibuild", "file": "python/qitoolchain/test/test_feed.py", "last_update_at": "2021-07-27T06:46:59+00:00", "original_content": "def test_local_missing_url(tmpdir):\n    \"\"\" Test creating a toolchain from path to a local feed xml \"\"\"\n    full = '<feed>\\n<feed />\\n</feed>\\n'\n    with pytest.raises(Exception) as e:\n        _generic_test_local(tmpdir, full)\n    assert 'not parse' in str(e)\n    assert \"Non-root 'feed' element must have an 'url' attribute\" in str(e)", "refactored": true, "pred": {"ppl": 15.938276290893555, "ppl_lower": 17.527254104614258, "ppl/lowercase_ppl": -1.0343239459140166, "ppl/zlib": 0.011488479378342947, "Min_5.0% Prob": 12.537109375, "Min_10.0% Prob": 11.363006401062012, "Min_20.0% Prob": 9.526404517037529, "Min_30.0% Prob": 7.669376455247402, "Min_40.0% Prob": 6.359778808992963, "Min_50.0% Prob": 5.345829312448148, "Min_60.0% Prob": 4.620921285357326}}
{"hexsha": "3ca816c1fdae9c07f862a54969748690cccccdc9", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_authorization_token(event) -> str:\n    cookie = SimpleCookie()\n    cookie.load(', '.join(event['cookies']))\n    if COOKIE_TOKEN_KEY in cookie:\n        return cookie[COOKIE_TOKEN_KEY].value\n    header = event['headers'].get('authorization')\n    if header is None:\n        return None\n    return header.replace('Bearer ', '')", "fn_id": 0, "class_fn": false, "repo": "adacotech/terraform-aws-api-auth-proxy", "file": "modules/oauth2/lambda/authorizer/function.py", "last_update_at": "2021-09-17T03:22:05+00:00", "original_content": "def get_authorization_token(event) -> str:\n    cookie = SimpleCookie()\n    cookie.load(', '.join(event['cookies']))\n    if COOKIE_TOKEN_KEY in cookie:\n        return cookie[COOKIE_TOKEN_KEY].value\n    header = event['headers'].get('authorization')\n    if header is None:\n        return None\n    return header.replace('Bearer ', '')", "refactored": true, "pred": {"ppl": 7.006900310516357, "ppl_lower": 11.710618019104004, "ppl/lowercase_ppl": -1.2638048891834304, "ppl/zlib": 0.008809481548233085, "Min_5.0% Prob": 12.072115421295166, "Min_10.0% Prob": 10.738379690382216, "Min_20.0% Prob": 8.047110027737087, "Min_30.0% Prob": 6.115898980034722, "Min_40.0% Prob": 4.761317413714197, "Min_50.0% Prob": 3.8677152782678603, "Min_60.0% Prob": 3.239067362000545}}
{"hexsha": "fbbdcaf00a8d9fa8c6f2f80cfad9ef55df7708dc", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef fromList(li: List[int]):\n    if len(li) == 0:\n        return None\n    root = TreeNode(val=li[0])\n    queue = [root]\n    i = 1\n    while i < len(li):\n        node = queue[0]\n        del queue[0]\n        if li[i] is not None:\n            node.left = TreeNode(val=li[i])\n            queue.append(node.left)\n        i += 1\n        if i < len(li):\n            if li[i]:\n                node.right = TreeNode(val=li[i])\n                queue.append(node.right)\n            i += 1\n    return root", "fn_id": 0, "class_fn": false, "repo": "wanglongjiang/leetcode", "file": "easy/897-increasing-order-search-tree.py", "last_update_at": "2021-03-14T11:38:30+00:00", "original_content": "def fromList(li: List[int]):\n    if len(li) == 0:\n        return None\n    root = TreeNode(val=li[0])\n    queue = [root]\n    i = 1\n    while i < len(li):\n        node = queue[0]\n        del queue[0]\n        if li[i] is not None:\n            node.left = TreeNode(val=li[i])\n            queue.append(node.left)\n        i += 1\n        if i < len(li):\n            if li[i]:\n                node.right = TreeNode(val=li[i])\n                queue.append(node.right)\n            i += 1\n    return root", "refactored": true, "pred": {"ppl": 2.145237922668457, "ppl_lower": 2.4428417682647705, "ppl/lowercase_ppl": -1.1702082876579654, "ppl/zlib": 0.003304114570942875, "Min_5.0% Prob": 9.161547252110072, "Min_10.0% Prob": 7.012360000610352, "Min_20.0% Prob": 3.80515936305446, "Min_30.0% Prob": 2.543141916314972, "Min_40.0% Prob": 1.9322209144611993, "Min_50.0% Prob": 1.5361661472087667, "Min_60.0% Prob": 1.2747662823396162}}
{"hexsha": "9a9f21e89303399d79c67e75c99b8d626266c51f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_maximum_mutation_frequency(counts, distance_matrix, frequencies):\n    \"\"\"\n    # ========================================================================\n\n    GET MAXMIMUM MUTATION FREQUENCY\n\n    PURPOSE\n    -------\n\n    Returns the maximum mutation frequency of the haplotypes.\n\n\n    INPUT\n    -----\n\n    [INT LIST] [counts]\n        A haplotype counts, from the counts of the most abundant to the counts\n        of the least abundant haplotype.\n\n    [FLOAT LIST] [frequencies]\n        A list of (relative) frequencies of the Haplotypes.\n\n    [2D ARRAY] [distance_matrix]\n        A two dimensional array, representing the distance matrix of distances\n        between the sorted haplotypes.\n\n        This is expected to be calculated in a similar manner as:\n            haplotype.build_distiance_matrix(haplotypes)\n\n\n    RETURN\n    ------\n\n    [FLOAT] [maximum_mutation_frequency]\n        The maximum mutation frequency.\n\n    # ========================================================================\n    \"\"\"\n    H = len(counts)\n    F = frequencies\n    D = distance_matrix\n    maximum_mutation_frequency = calculate.maximum_mutation_frequency(H, F, D)\n    return maximum_mutation_frequency", "fn_id": 7, "class_fn": false, "repo": "phac-nml/quasitools", "file": "quasitools/commands/cmd_complexity.py", "last_update_at": "2021-03-15T07:28:20+00:00", "original_content": "def get_maximum_mutation_frequency(counts, distance_matrix, frequencies):\n    \"\"\"\n    # ========================================================================\n\n    GET MAXMIMUM MUTATION FREQUENCY\n\n    PURPOSE\n    -------\n\n    Returns the maximum mutation frequency of the haplotypes.\n\n\n    INPUT\n    -----\n\n    [INT LIST] [counts]\n        A haplotype counts, from the counts of the most abundant to the counts\n        of the least abundant haplotype.\n\n    [FLOAT LIST] [frequencies]\n        A list of (relative) frequencies of the Haplotypes.\n\n    [2D ARRAY] [distance_matrix]\n        A two dimensional array, representing the distance matrix of distances\n        between the sorted haplotypes.\n\n        This is expected to be calculated in a similar manner as:\n            haplotype.build_distiance_matrix(haplotypes)\n\n\n    RETURN\n    ------\n\n    [FLOAT] [maximum_mutation_frequency]\n        The maximum mutation frequency.\n\n    # ========================================================================\n    \"\"\"\n    H = len(counts)\n    F = frequencies\n    D = distance_matrix\n    maximum_mutation_frequency = calculate.maximum_mutation_frequency(H, F, D)\n    return maximum_mutation_frequency", "refactored": true, "pred": {"ppl": 8.338321685791016, "ppl_lower": 8.164802551269531, "ppl/lowercase_ppl": -0.9900844958288849, "ppl/zlib": 0.004346028605336322, "Min_5.0% Prob": 11.094527959823608, "Min_10.0% Prob": 9.822957477569581, "Min_20.0% Prob": 7.778811370625215, "Min_30.0% Prob": 6.241077360353972, "Min_40.0% Prob": 5.043282140703762, "Min_50.0% Prob": 4.185897695267294, "Min_60.0% Prob": 3.518075310229476}}
{"hexsha": "e8db797381a8d4f3d0f04952d696026266dfabdc", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef add_zero(lst):\n    \"\"\" Add pulses for logical zero \"\"\"\n    pulse = 1875\n    space = 1875\n    lst.extend([pulse, space])\n    return lst", "fn_id": 2, "class_fn": false, "repo": "skbobade/UniversalRemote", "file": "src/irxm.py", "last_update_at": "2021-03-31T23:54:12+00:00", "original_content": "def add_zero(lst):\n    \"\"\" Add pulses for logical zero \"\"\"\n    pulse = 1875\n    space = 1875\n    lst.extend([pulse, space])\n    return lst", "refactored": true, "pred": {"ppl": 21.227214813232422, "ppl_lower": 21.831830978393555, "ppl/lowercase_ppl": -1.0091922599452403, "ppl/zlib": 0.023146091485017468, "Min_5.0% Prob": 14.429029941558838, "Min_10.0% Prob": 11.642982482910156, "Min_20.0% Prob": 9.754927825927734, "Min_30.0% Prob": 8.13027474284172, "Min_40.0% Prob": 6.925551198777699, "Min_50.0% Prob": 5.82203467686971, "Min_60.0% Prob": 5.059602787718177}}
{"hexsha": "6e0e15f18889c25b4f39d64ec87555d40f8d8c97", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef apply_mask_criteria(column):\n    \"\"\"Apply simple masking criteria to a single column, and return '-' if the\n    column does not meet the criteria, and 'I' if it does.\n    \"\"\"\n    mask_char = '-'\n    num_seqs = len(column)\n    half_num_seqs = num_seqs / 2\n    num_gaps_in_col = column.count('-')\n    column_no_gaps = column.replace('-', '')\n    if column_no_gaps == '':\n        return mask_char\n    elif not column_no_gaps == '':\n        most_common_residue = collections.Counter(column_no_gaps).most_common(1)[0]\n        most_common_residue_count = most_common_residue[1]\n        percent_identity = most_common_residue_count * 100 / num_seqs\n        if num_gaps_in_col < num_seqs * 0.3:\n            mask_char = 'I'\n        if percent_identity >= 50:\n            mask_char = 'I'\n        return mask_char", "fn_id": 0, "class_fn": false, "repo": "laelbarlow/amoebae", "file": "amoebaelib/mask_nex.py", "last_update_at": "2021-11-28T08:32:05+00:00", "original_content": "def apply_mask_criteria(column):\n    \"\"\"Apply simple masking criteria to a single column, and return '-' if the\n    column does not meet the criteria, and 'I' if it does.\n    \"\"\"\n    mask_char = '-'\n    num_seqs = len(column)\n    half_num_seqs = num_seqs / 2\n    num_gaps_in_col = column.count('-')\n    column_no_gaps = column.replace('-', '')\n    if column_no_gaps == '':\n        return mask_char\n    elif not column_no_gaps == '':\n        most_common_residue = collections.Counter(column_no_gaps).most_common(1)[0]\n        most_common_residue_count = most_common_residue[1]\n        percent_identity = most_common_residue_count * 100 / num_seqs\n        if num_gaps_in_col < num_seqs * 0.3:\n            mask_char = 'I'\n        if percent_identity >= 50:\n            mask_char = 'I'\n        return mask_char", "refactored": true, "pred": {"ppl": 4.433661937713623, "ppl_lower": 4.548962593078613, "ppl/lowercase_ppl": -1.017239386546706, "ppl/zlib": 0.004230755298583916, "Min_5.0% Prob": 10.819469769795736, "Min_10.0% Prob": 8.611564044952393, "Min_20.0% Prob": 6.27340410269943, "Min_30.0% Prob": 4.797917286032124, "Min_40.0% Prob": 3.7060652208678864, "Min_50.0% Prob": 2.974276510736672, "Min_60.0% Prob": 2.4912122544418605}}
{"hexsha": "d24a6419f9fcb2d3facd5e3d432818cfe2776fbb", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_activity_rawdata(cfg: DictConfig, id: int) -> pd.DataFrame:\n    \"\"\"query rawdata from strava api for a given id\n\n    Args:\n        cfg (DictConfig): configuration\n        id (int): activity id\n\n    Returns:\n        pd.DataFrame: activity rawdata\n    \"\"\"\n    with MySession(cfg) as session:\n        stream = session.get(cfg.api.BASE_URL + f'/activities/{id}/streams', params={'keys': ','.join(cfg.api.STREAMS)})\n    activity = dict()\n    found = list()\n    for values in stream.json():\n        activity[values['type']] = values['data']\n        found.append(values['type'])\n    res_df = pd.DataFrame(activity)\n    try:\n        res_df['lat'] = res_df['latlng'].apply(lambda x: x[0])\n        res_df['long'] = res_df['latlng'].apply(lambda x: x[-1])\n    except KeyError:\n        res_df['lat'], res_df['long'] = (np.nan, np.nan)\n    missings = set(cfg.api.STREAMS) - set(found)\n    if missings:\n        for missing in missings:\n            res_df[missing] = np.nan\n    res_df['id'] = id\n    if 'latlng' in res_df:\n        del res_df['latlng']\n    if cfg.TO_DB:\n        res_df['last_update'] = save_datetime_now()\n        with sqlite3.connect(call(cfg.DB)) as con:\n            res_df.to_sql('ACTIVITIES_RAW', con=con, if_exists='append', index=False, index_label='id')\n    time.sleep(cfg.api.SLEEP)\n    return res_df", "fn_id": 6, "class_fn": false, "repo": "Ektoplasmakugel/strava_datacollect", "file": "strava_datacollect/strava_query.py", "last_update_at": "2021-01-05T10:58:30+00:00", "original_content": "def get_activity_rawdata(cfg: DictConfig, id: int) -> pd.DataFrame:\n    \"\"\"query rawdata from strava api for a given id\n\n    Args:\n        cfg (DictConfig): configuration\n        id (int): activity id\n\n    Returns:\n        pd.DataFrame: activity rawdata\n    \"\"\"\n    with MySession(cfg) as session:\n        stream = session.get(cfg.api.BASE_URL + f'/activities/{id}/streams', params={'keys': ','.join(cfg.api.STREAMS)})\n    activity = dict()\n    found = list()\n    for values in stream.json():\n        activity[values['type']] = values['data']\n        found.append(values['type'])\n    res_df = pd.DataFrame(activity)\n    try:\n        res_df['lat'] = res_df['latlng'].apply(lambda x: x[0])\n        res_df['long'] = res_df['latlng'].apply(lambda x: x[-1])\n    except KeyError:\n        res_df['lat'], res_df['long'] = (np.nan, np.nan)\n    missings = set(cfg.api.STREAMS) - set(found)\n    if missings:\n        for missing in missings:\n            res_df[missing] = np.nan\n    res_df['id'] = id\n    if 'latlng' in res_df:\n        del res_df['latlng']\n    if cfg.TO_DB:\n        res_df['last_update'] = save_datetime_now()\n        with sqlite3.connect(call(cfg.DB)) as con:\n            res_df.to_sql('ACTIVITIES_RAW', con=con, if_exists='append', index=False, index_label='id')\n    time.sleep(cfg.api.SLEEP)\n    return res_df", "refactored": true, "pred": {"ppl": 3.5352072715759277, "ppl_lower": 4.123134136199951, "ppl/lowercase_ppl": -1.1218285354865818, "ppl/zlib": 0.001982373519213183, "Min_5.0% Prob": 10.404266500473023, "Min_10.0% Prob": 8.223227140380114, "Min_20.0% Prob": 5.580294002846974, "Min_30.0% Prob": 4.086360205479754, "Min_40.0% Prob": 3.1424371687377373, "Min_50.0% Prob": 2.528148125239262, "Min_60.0% Prob": 2.109237956449132}}
{"hexsha": "66ec3f6ee6f74fd6f9a9ebdb2510fc97b493104f", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_output_dir(output_dir):\n    if not output_dir or os.path.isfile(output_dir):\n        output_dir = '.'\n    elif not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    return output_dir", "fn_id": 0, "class_fn": false, "repo": "bjwuzh/autocase", "file": "axxac/path_tool.py", "last_update_at": "2021-07-02T09:46:23+00:00", "original_content": "def get_output_dir(output_dir):\n    if not output_dir or os.path.isfile(output_dir):\n        output_dir = '.'\n    elif not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    return output_dir", "refactored": true, "pred": {"ppl": 3.5347015857696533, "ppl_lower": 3.5347015857696533, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.009638388387381144, "Min_5.0% Prob": 9.517089207967123, "Min_10.0% Prob": 7.882162707192557, "Min_20.0% Prob": 5.748415929930551, "Min_30.0% Prob": 4.121272776808057, "Min_40.0% Prob": 3.190358452498913, "Min_50.0% Prob": 2.560228843960379, "Min_60.0% Prob": 2.1343733569277297}}
{"hexsha": "95f32217242b488480e02ece9bfc4b27508cb75d", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef minimal(input_size=2, output_size=2, weight_low=-2, weight_high=2, depth=3):\n    \"\"\" Builds a minimal genome with specified inputs and\n    outputs, weight bounds, depth and one connected node in\n    the first layer.\n\n    :param input_size: Number of input nodes\n    :param output_size: Number of output nodes\n    :param weight_low: Maximum weight on node and edges\n    :param weight_high: Minimum weight on node and edges\n    :param depth: Number of layers in network.\n    :return: Constructed genome.\n    \"\"\"\n    genome = Genome(input_size=input_size, output_size=output_size, weight_low=weight_low, weight_high=weight_high, depth=depth)\n    genome.layers = [genome.inputs, *[[] for _ in range(depth)], genome.outputs]\n    genome.add_node(1)\n    for n in genome.inputs:\n        genome.add_edge(n, genome.layers[1][0])\n    for n in genome.outputs:\n        genome.add_edge(genome.layers[1][0], n)\n    return genome", "fn_id": 1, "class_fn": false, "repo": "mauicv/genrl", "file": "src/gerel/genome/factories.py", "last_update_at": "2021-03-25T23:29:07+00:00", "original_content": "def minimal(input_size=2, output_size=2, weight_low=-2, weight_high=2, depth=3):\n    \"\"\" Builds a minimal genome with specified inputs and\n    outputs, weight bounds, depth and one connected node in\n    the first layer.\n\n    :param input_size: Number of input nodes\n    :param output_size: Number of output nodes\n    :param weight_low: Maximum weight on node and edges\n    :param weight_high: Minimum weight on node and edges\n    :param depth: Number of layers in network.\n    :return: Constructed genome.\n    \"\"\"\n    genome = Genome(input_size=input_size, output_size=output_size, weight_low=weight_low, weight_high=weight_high, depth=depth)\n    genome.layers = [genome.inputs, *[[] for _ in range(depth)], genome.outputs]\n    genome.add_node(1)\n    for n in genome.inputs:\n        genome.add_edge(n, genome.layers[1][0])\n    for n in genome.outputs:\n        genome.add_edge(genome.layers[1][0], n)\n    return genome", "refactored": true, "pred": {"ppl": 3.8720245361328125, "ppl_lower": 4.119306564331055, "ppl/lowercase_ppl": -1.0457293263318475, "ppl/zlib": 0.003525462255743264, "Min_5.0% Prob": 9.65000636761005, "Min_10.0% Prob": 7.787070824549748, "Min_20.0% Prob": 5.711002592857067, "Min_30.0% Prob": 4.263413693331465, "Min_40.0% Prob": 3.3471392642884026, "Min_50.0% Prob": 2.7011455043473025, "Min_60.0% Prob": 2.2613856449275267}}
{"hexsha": "5d7651dffd876fa23e9cd979e7ddb5ed364b6b00", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef validate_bool(value):\n    \"\"\"Check that ``value`` is an boolean value.\"\"\"\n    if value not in (0, 1, False, True):\n        raise ValidationError(value, INVALID_BOOL)\n    return bool(value)", "fn_id": 4, "class_fn": false, "repo": "agarwalrounak/readthedocs.org", "file": "readthedocs/config/validation.py", "last_update_at": "2021-04-26T21:59:29+00:00", "original_content": "def validate_bool(value):\n    \"\"\"Check that ``value`` is an boolean value.\"\"\"\n    if value not in (0, 1, False, True):\n        raise ValidationError(value, INVALID_BOOL)\n    return bool(value)", "refactored": true, "pred": {"ppl": 8.915840148925781, "ppl_lower": 13.752745628356934, "ppl/lowercase_ppl": -1.19809998999328, "ppl/zlib": 0.013022794564280796, "Min_5.0% Prob": 10.141289393107096, "Min_10.0% Prob": 9.219815254211426, "Min_20.0% Prob": 7.439568519592285, "Min_30.0% Prob": 5.973715609974331, "Min_40.0% Prob": 4.972828855117162, "Min_50.0% Prob": 4.230896075566609, "Min_60.0% Prob": 3.622907226284345}}
{"hexsha": "65836e5e8c8f2024ffb8b16e84f296ad241bb07d", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef run_async(factor, func, *args, **kwargs):\n    \"\"\"\n    Asynchronously executes a callable within a :class:`hiro.Timeline`\n\n    :param int factor: scale factor to use for the timeline during execution\n    :param function func: the function to invoke\n    :param args: the arguments to pass to the function\n    :param kwargs: the keyword arguments to pass to the function\n    :returns: an instance of :class:`hiro.core.ScaledAsyncRunner`\n\n    \"\"\"\n    return ScaledAsyncRunner(factor, func, *args, **kwargs)", "fn_id": 1, "class_fn": false, "repo": "alisaifee/hiro", "file": "hiro/core.py", "last_update_at": "2021-04-07T15:42:23+00:00", "original_content": "def run_async(factor, func, *args, **kwargs):\n    \"\"\"\n    Asynchronously executes a callable within a :class:`hiro.Timeline`\n\n    :param int factor: scale factor to use for the timeline during execution\n    :param function func: the function to invoke\n    :param args: the arguments to pass to the function\n    :param kwargs: the keyword arguments to pass to the function\n    :returns: an instance of :class:`hiro.core.ScaledAsyncRunner`\n\n    \"\"\"\n    return ScaledAsyncRunner(factor, func, *args, **kwargs)", "refactored": true, "pred": {"ppl": 6.399916172027588, "ppl_lower": 7.746273517608643, "ppl/lowercase_ppl": -1.1028543624613119, "ppl/zlib": 0.007004848649657191, "Min_5.0% Prob": 11.758994897206625, "Min_10.0% Prob": 9.626518396230844, "Min_20.0% Prob": 7.341954203752371, "Min_30.0% Prob": 5.675489022181584, "Min_40.0% Prob": 4.492728905035899, "Min_50.0% Prob": 3.7004585807140056, "Min_60.0% Prob": 3.1095747233201294}}
{"hexsha": "f3817abe4aba076e3056537deae3e73cf43cce08", "ext": "py", "lang": "Python", "content": "@patch('inventory.readers._logger', autospec=True)\n@timeing\n@measure_memory_usage\ndef test_given_error_from_boto_then_account_is_skipped_but_others_still_processed(mock_logger):\n    os.environ['ACCOUNT_LIST'] = '[ { \"name\": \"foo\", \"id\": \"210987654321\" }, { \"name\": \"bar\", \"id\": \"123456789012\" } ]'\n    mock_mapper = Mock(spec=DataMapper)\n    mock_mapper.can_map.return_value = True\n    mock_mapper.map.return_value = [{'test': True}]\n    mock_select_resource_config = Mock(side_effect=[ClientError(error_response={'Error': {'Code': 'ResourceInUseException'}}, operation_name='select_resource_config'), {'NextToken': None, 'Results': [json.dumps({'resourceType': 'foobar'})]}])\n    mock_config_client_factory = Mock()\n    mock_config_client_factory.return_value.select_resource_config = mock_select_resource_config\n    reader = AwsConfigInventoryReader(lambda_context=MagicMock(), sts_client=Mock(), mappers=[mock_mapper])\n    reader._get_config_client = mock_config_client_factory\n    all_inventory = reader.get_resources_from_all_accounts()\n    assert len(all_inventory) == 1, 'inventory from the successful call should be returned'\n    assert len(mock_select_resource_config.mock_calls) == 2, 'boto should have been called twice to page through results'\n    mock_logger.error.assert_called_with(String() & Contains('moving onto next account'), ANY, ANY, exc_info=True)", "fn_id": 2, "class_fn": false, "repo": "Alpacked/fedramp-integrated-inventory-workbook", "file": "tests/test_inventory_reader.py", "last_update_at": "2021-12-15T22:29:08+00:00", "original_content": "@patch('inventory.readers._logger', autospec=True)\ndef test_given_error_from_boto_then_account_is_skipped_but_others_still_processed(mock_logger):\n    os.environ['ACCOUNT_LIST'] = '[ { \"name\": \"foo\", \"id\": \"210987654321\" }, { \"name\": \"bar\", \"id\": \"123456789012\" } ]'\n    mock_mapper = Mock(spec=DataMapper)\n    mock_mapper.can_map.return_value = True\n    mock_mapper.map.return_value = [{'test': True}]\n    mock_select_resource_config = Mock(side_effect=[ClientError(error_response={'Error': {'Code': 'ResourceInUseException'}}, operation_name='select_resource_config'), {'NextToken': None, 'Results': [json.dumps({'resourceType': 'foobar'})]}])\n    mock_config_client_factory = Mock()\n    mock_config_client_factory.return_value.select_resource_config = mock_select_resource_config\n    reader = AwsConfigInventoryReader(lambda_context=MagicMock(), sts_client=Mock(), mappers=[mock_mapper])\n    reader._get_config_client = mock_config_client_factory\n    all_inventory = reader.get_resources_from_all_accounts()\n    assert len(all_inventory) == 1, 'inventory from the successful call should be returned'\n    assert len(mock_select_resource_config.mock_calls) == 2, 'boto should have been called twice to page through results'\n    mock_logger.error.assert_called_with(String() & Contains('moving onto next account'), ANY, ANY, exc_info=True)", "refactored": true, "pred": {"ppl": 4.402015209197998, "ppl_lower": 5.99192476272583, "ppl/lowercase_ppl": -1.208054832066992, "ppl/zlib": 0.0021448081594265204, "Min_5.0% Prob": 10.690460920333862, "Min_10.0% Prob": 8.592924280864436, "Min_20.0% Prob": 6.137579682396679, "Min_30.0% Prob": 4.67611885264637, "Min_40.0% Prob": 3.667509395174864, "Min_50.0% Prob": 2.954040729558294, "Min_60.0% Prob": 2.470454882777594}}
{"hexsha": "640f81533781b97654cf6abb19ee0b856a6e367b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef th_mdn_loss_dense(gt, mu, sigma, pi, mask, V, **kwargs):\n    C = 17\n    BS = gt.shape[0]\n    M = pi.shape[1]\n    H = gt.shape[2]\n    W = gt.shape[3]\n    gt = gt.permute(0, 2, 3, 1)\n    mu = mu.permute(0, 2, 3, 1)\n    sigma = sigma.permute(0, 2, 3, 1)\n    pi = pi.permute(0, 2, 3, 1)\n    mask = mask.permute(0, 2, 3, 1)\n    lmask = (torch.sum(mask, 3) > 0).float()\n    mask = torch.reshape(mask[:, :, :, np.repeat(np.arange(C), 2)], (BS, H, W, 1, C * 2))\n    mask = mask.repeat(1, 1, 1, M, 1)\n    gt = gt.reshape(BS, H, W, 1, 2 * C)\n    gt = gt.repeat(1, 1, 1, M, 1)\n    mu = mu.reshape(BS, H, W, M, 2 * C)\n    V = torch.reshape(V[np.repeat(np.arange(C), 2)], (1, 1, 1, 1, C * 2))\n    sigma = torch.reshape(sigma, (BS, H, W, M, 2))[:, :, :, :, np.concatenate([np.arange(2) for _ in np.arange(C)])]\n    e = 0.5 * ((gt - mu) * torch.reciprocal(sigma) * torch.reciprocal(V)) ** 2\n    e = torch.where(mask > 0.0, e, torch.zeros_like(e))\n    e = torch.sum(e, -1)\n    nviskps = torch.sum(mask[:, :, :, :, 0::2] > 0.0, -1).float().detach()\n    sigma_y = sigma[:, :, :, :, 0]\n    sigma_x = sigma[:, :, :, :, 1]\n    PI = torch.tensor(np.pi).cuda()\n    coef = -nviskps * torch.log(sigma_y) - nviskps * torch.log(sigma_x) - nviskps * torch.log(2 * PI)\n    exponent = torch.log(pi) + coef - e\n    loss = -torch.squeeze(log_sum_exp(exponent, 3), 3)\n    if kwargs.get('debug', False):\n        print('exponent.shape', exponent.shape)\n        print('loss.shape', loss.shape)\n    loss = torch.sum(lmask * loss) / (1.0 + torch.sum(lmask))\n    return loss", "fn_id": 4, "class_fn": false, "repo": "alivaramesh/MixtureDenseRegression", "file": "src/lib/models/losses.py", "last_update_at": "2021-11-30T01:22:48+00:00", "original_content": "def th_mdn_loss_dense(gt, mu, sigma, pi, mask, V, **kwargs):\n    C = 17\n    BS = gt.shape[0]\n    M = pi.shape[1]\n    H = gt.shape[2]\n    W = gt.shape[3]\n    gt = gt.permute(0, 2, 3, 1)\n    mu = mu.permute(0, 2, 3, 1)\n    sigma = sigma.permute(0, 2, 3, 1)\n    pi = pi.permute(0, 2, 3, 1)\n    mask = mask.permute(0, 2, 3, 1)\n    lmask = (torch.sum(mask, 3) > 0).float()\n    mask = torch.reshape(mask[:, :, :, np.repeat(np.arange(C), 2)], (BS, H, W, 1, C * 2))\n    mask = mask.repeat(1, 1, 1, M, 1)\n    gt = gt.reshape(BS, H, W, 1, 2 * C)\n    gt = gt.repeat(1, 1, 1, M, 1)\n    mu = mu.reshape(BS, H, W, M, 2 * C)\n    V = torch.reshape(V[np.repeat(np.arange(C), 2)], (1, 1, 1, 1, C * 2))\n    sigma = torch.reshape(sigma, (BS, H, W, M, 2))[:, :, :, :, np.concatenate([np.arange(2) for _ in np.arange(C)])]\n    e = 0.5 * ((gt - mu) * torch.reciprocal(sigma) * torch.reciprocal(V)) ** 2\n    e = torch.where(mask > 0.0, e, torch.zeros_like(e))\n    e = torch.sum(e, -1)\n    nviskps = torch.sum(mask[:, :, :, :, 0::2] > 0.0, -1).float().detach()\n    sigma_y = sigma[:, :, :, :, 0]\n    sigma_x = sigma[:, :, :, :, 1]\n    PI = torch.tensor(np.pi).cuda()\n    coef = -nviskps * torch.log(sigma_y) - nviskps * torch.log(sigma_x) - nviskps * torch.log(2 * PI)\n    exponent = torch.log(pi) + coef - e\n    loss = -torch.squeeze(log_sum_exp(exponent, 3), 3)\n    if kwargs.get('debug', False):\n        print('exponent.shape', exponent.shape)\n        print('loss.shape', loss.shape)\n    loss = torch.sum(lmask * loss) / (1.0 + torch.sum(lmask))\n    return loss", "refactored": true, "pred": {"ppl": 2.891169309616089, "ppl_lower": 2.9001691341400146, "ppl/lowercase_ppl": -1.00292751795219, "ppl/zlib": 0.0017123564930598993, "Min_5.0% Prob": 8.88869012485851, "Min_10.0% Prob": 7.074650800050194, "Min_20.0% Prob": 4.757603640909548, "Min_30.0% Prob": 3.433686953019626, "Min_40.0% Prob": 2.6336387143198854, "Min_50.0% Prob": 2.118408130344476, "Min_60.0% Prob": 1.7721514785570538}}
{"hexsha": "075a378bb5687e7c4ee731ece7a5af55df206c93", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef validate(model, dataset, opt, ctx):\n    \"\"\"Test on validation dataset.\"\"\"\n    detector = CenterDetector(opt)\n    detector.model = model\n    results = {}\n    num_iters = len(dataset)\n    bar = Bar('{}'.format(opt.exp_id), max=num_iters)\n    time_stats = ['tot', 'load', 'pre', 'net', 'dec', 'post', 'merge']\n    avg_time_stats = {t: AverageMeter() for t in time_stats}\n    print('Reporting every 1000 images...')\n    for ind in range(num_iters):\n        img_id = dataset.images[ind]\n        img_info = dataset.coco.loadImgs(ids=[img_id])[0]\n        img_path = os.path.join(dataset.img_dir, img_info['file_name'])\n        ret = detector.run(img_path)\n        results[img_id] = ret['results']\n        Bar.suffix = '[{0}/{1}]|Tot: {total:} |ETA: {eta:} '.format(ind, num_iters, total=bar.elapsed_td, eta=bar.eta_td)\n        for t in avg_time_stats:\n            avg_time_stats[t].update(ret[t])\n            Bar.suffix = Bar.suffix + '|{} {:.3f} '.format(t, avg_time_stats[t].avg)\n        if ind % 1000 == 0:\n            bar.next()\n    bar.finish()\n    val_dataset.run_eval(results=results, save_dir='./output/')", "fn_id": 3, "class_fn": false, "repo": "Guanghan/mxnet-centernet", "file": "train.py", "last_update_at": "2021-05-12T08:41:33+00:00", "original_content": "def validate(model, dataset, opt, ctx):\n    \"\"\"Test on validation dataset.\"\"\"\n    detector = CenterDetector(opt)\n    detector.model = model\n    results = {}\n    num_iters = len(dataset)\n    bar = Bar('{}'.format(opt.exp_id), max=num_iters)\n    time_stats = ['tot', 'load', 'pre', 'net', 'dec', 'post', 'merge']\n    avg_time_stats = {t: AverageMeter() for t in time_stats}\n    print('Reporting every 1000 images...')\n    for ind in range(num_iters):\n        img_id = dataset.images[ind]\n        img_info = dataset.coco.loadImgs(ids=[img_id])[0]\n        img_path = os.path.join(dataset.img_dir, img_info['file_name'])\n        ret = detector.run(img_path)\n        results[img_id] = ret['results']\n        Bar.suffix = '[{0}/{1}]|Tot: {total:} |ETA: {eta:} '.format(ind, num_iters, total=bar.elapsed_td, eta=bar.eta_td)\n        for t in avg_time_stats:\n            avg_time_stats[t].update(ret[t])\n            Bar.suffix = Bar.suffix + '|{} {:.3f} '.format(t, avg_time_stats[t].avg)\n        if ind % 1000 == 0:\n            bar.next()\n    bar.finish()\n    val_dataset.run_eval(results=results, save_dir='./output/')", "refactored": true, "pred": {"ppl": 2.332184314727783, "ppl_lower": 2.689053535461426, "ppl/lowercase_ppl": -1.1681425271703914, "ppl/zlib": 0.001506771000988031, "Min_5.0% Prob": 8.157578123940361, "Min_10.0% Prob": 6.058542232255678, "Min_20.0% Prob": 3.9357095046623334, "Min_30.0% Prob": 2.7821391761839926, "Min_40.0% Prob": 2.111782171335575, "Min_50.0% Prob": 1.6954720016971634, "Min_60.0% Prob": 1.4146621487741065}}
{"hexsha": "2b63ae4e99c2f9a3f9a1f7f166698e9931cfc29c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_roc_data(df_test, df_prob, encoding):\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    for i in range(df_test.shape[0]):\n        y_true = df_test.iloc[i, :].dropna().values\n        y_pred = df_prob.iloc[i, :].dropna().values\n        fpr, tpr, _ = roc_curve(y_true, y_pred)\n        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        aucs.append(roc_auc_score(y_true, y_pred))\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    std_tpr = np.std(tprs, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    df = pd.DataFrame({'x': mean_fpr, 'y': mean_tpr, 'tprs_lower': tprs_lower, 'tprs_upper': tprs_upper})\n    df['Encoding'] = encoding\n    df['mean_auc'] = np.round(mean_auc, 2)\n    df['legend_label'] = df.apply(lambda row: f\"{row['Encoding']} (AUC: {row['mean_auc']})\", axis=1)\n    return df", "fn_id": 1, "class_fn": false, "repo": "spaenigs/peptidereactor", "file": "nodes/vis/sds_3_Curves/scripts/roc_pr_curve.py", "last_update_at": "2021-06-07T07:03:38+00:00", "original_content": "def get_roc_data(df_test, df_prob, encoding):\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    for i in range(df_test.shape[0]):\n        y_true = df_test.iloc[i, :].dropna().values\n        y_pred = df_prob.iloc[i, :].dropna().values\n        fpr, tpr, _ = roc_curve(y_true, y_pred)\n        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        aucs.append(roc_auc_score(y_true, y_pred))\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    std_tpr = np.std(tprs, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    df = pd.DataFrame({'x': mean_fpr, 'y': mean_tpr, 'tprs_lower': tprs_lower, 'tprs_upper': tprs_upper})\n    df['Encoding'] = encoding\n    df['mean_auc'] = np.round(mean_auc, 2)\n    df['legend_label'] = df.apply(lambda row: f\"{row['Encoding']} (AUC: {row['mean_auc']})\", axis=1)\n    return df", "refactored": true, "pred": {"ppl": 1.8538144826889038, "ppl_lower": 1.9550951719284058, "ppl/lowercase_ppl": -1.0861788111849282, "ppl/zlib": 0.0013274094599191539, "Min_5.0% Prob": 7.463423819768996, "Min_10.0% Prob": 5.265297055244446, "Min_20.0% Prob": 3.0337055935746147, "Min_30.0% Prob": 2.055351363376729, "Min_40.0% Prob": 1.5459428598960152, "Min_50.0% Prob": 1.237317772145893, "Min_60.0% Prob": 1.0311750460395637}}
{"hexsha": "d7d0cc76bd9ced96d824215c265e0b8c9d95c87b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_unique_changed_packages(diffs):\n    packages_changed = {}\n    for change in diffs:\n        if len(change) != 2:\n            logging.debug(change)\n            continue\n        change_type, path = change\n        path_tokens = path.split('/')\n        if path_tokens[-1] in IGNORE_CHANGES_FILES:\n            continue\n        try:\n            package = Package(package_dir=path_tokens[0])\n        except PackageDoesNotExistException:\n            continue\n        if package.package_name not in packages_changed:\n            packages_changed.update({package.package_name: package})\n    return packages_changed", "fn_id": 2, "class_fn": false, "repo": "r-kells/scream", "file": "scream/detect_changed_packages.py", "last_update_at": "2021-11-22T18:45:19+00:00", "original_content": "def get_unique_changed_packages(diffs):\n    packages_changed = {}\n    for change in diffs:\n        if len(change) != 2:\n            logging.debug(change)\n            continue\n        change_type, path = change\n        path_tokens = path.split('/')\n        if path_tokens[-1] in IGNORE_CHANGES_FILES:\n            continue\n        try:\n            package = Package(package_dir=path_tokens[0])\n        except PackageDoesNotExistException:\n            continue\n        if package.package_name not in packages_changed:\n            packages_changed.update({package.package_name: package})\n    return packages_changed", "refactored": true, "pred": {"ppl": 4.5992326736450195, "ppl_lower": 5.704338550567627, "ppl/lowercase_ppl": -1.141122642186552, "ppl/zlib": 0.005035938876249278, "Min_5.0% Prob": 9.39260298865182, "Min_10.0% Prob": 8.001774515424456, "Min_20.0% Prob": 6.072643748645125, "Min_30.0% Prob": 4.7881607277826825, "Min_40.0% Prob": 3.7517521073252467, "Min_50.0% Prob": 3.0326420014371744, "Min_60.0% Prob": 2.5615122705613347}}
{"hexsha": "bc24e936ef606bdcce7d65b0ddb291682889d4ea", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef tx_register(tx, cursor):\n    payload = json.loads(tx.payload)\n    owner = models.Account(tx.chain_id, tx.sender, cursor)\n    parcel = models.Parcel(tx.chain_id, payload['target'], owner.address, cursor)\n    storage = models.Storage(tx.chain_id, parcel.storage_id, None, cursor)\n    host = models.Account(tx.chain_id, storage.owner, cursor)\n    parcel.custody = payload['custody']\n    if parcel.custody != None and len(parcel.custody) > 100:\n        parcel.custody = parcel.custody[:100]\n    parcel.proxy_account = payload.get('proxy_account', None)\n    if parcel.proxy_account != None and len(parcel.proxy_account) > 40:\n        parcel.proxy_account = parcel.proxy_account[:40]\n    parcel.extra = payload.get('extra', '{}')\n    parcel.on_sale = True\n    parcel.save(cursor)\n    owner.balance -= storage.registration_fee\n    owner.save(cursor)\n    host.balance += storage.registration_fee\n    host.save(cursor)", "fn_id": 7, "class_fn": false, "repo": "MECTrace/edge-data-chain-explorer", "file": "crawler/tx.py", "last_update_at": "2021-12-17T08:52:58+00:00", "original_content": "def tx_register(tx, cursor):\n    payload = json.loads(tx.payload)\n    owner = models.Account(tx.chain_id, tx.sender, cursor)\n    parcel = models.Parcel(tx.chain_id, payload['target'], owner.address, cursor)\n    storage = models.Storage(tx.chain_id, parcel.storage_id, None, cursor)\n    host = models.Account(tx.chain_id, storage.owner, cursor)\n    parcel.custody = payload['custody']\n    if parcel.custody != None and len(parcel.custody) > 100:\n        parcel.custody = parcel.custody[:100]\n    parcel.proxy_account = payload.get('proxy_account', None)\n    if parcel.proxy_account != None and len(parcel.proxy_account) > 40:\n        parcel.proxy_account = parcel.proxy_account[:40]\n    parcel.extra = payload.get('extra', '{}')\n    parcel.on_sale = True\n    parcel.save(cursor)\n    owner.balance -= storage.registration_fee\n    owner.save(cursor)\n    host.balance += storage.registration_fee\n    host.save(cursor)", "refactored": true, "pred": {"ppl": 3.647472858428955, "ppl_lower": 3.9541823863983154, "ppl/lowercase_ppl": -1.0623934576265117, "ppl/zlib": 0.003614621676658053, "Min_5.0% Prob": 10.067524365016393, "Min_10.0% Prob": 7.9709765911102295, "Min_20.0% Prob": 5.551730311342648, "Min_30.0% Prob": 4.128592139908245, "Min_40.0% Prob": 3.1908532571313635, "Min_50.0% Prob": 2.577521998488477, "Min_60.0% Prob": 2.154513349102455}}
{"hexsha": "a6ecbbb0e40168d4238d3a4dc7a1087809d1ed3f", "ext": "py", "lang": "Python", "content": "@box.cron('0 3 * * *')\n@timeing\n@measure_memory_usage\nasync def refresh_db(bot):\n    logger.info('refresh subway')\n    tasks = []\n    for service_region, api_version in REGION_TABLE.values():\n        tasks.append(fetch_station_db(bot, service_region, api_version))\n    await asyncio.wait(tasks)", "fn_id": 2, "class_fn": false, "repo": "item4/yui", "file": "yui/apps/search/subway.py", "last_update_at": "2021-01-31T17:57:41+00:00", "original_content": "@box.cron('0 3 * * *')\nasync def refresh_db(bot):\n    logger.info('refresh subway')\n    tasks = []\n    for service_region, api_version in REGION_TABLE.values():\n        tasks.append(fetch_station_db(bot, service_region, api_version))\n    await asyncio.wait(tasks)", "refactored": true, "pred": {"ppl": 12.145376205444336, "ppl_lower": 11.967243194580078, "ppl/lowercase_ppl": -0.9940826354962814, "ppl/zlib": 0.012004560279484766, "Min_5.0% Prob": 12.358437538146973, "Min_10.0% Prob": 10.841609001159668, "Min_20.0% Prob": 8.904202068553252, "Min_30.0% Prob": 7.394170375970694, "Min_40.0% Prob": 6.099629436220441, "Min_50.0% Prob": 4.989870972254059, "Min_60.0% Prob": 4.1779306938625735}}
{"hexsha": "70836ca1748caea3d326c0101f9ea4804dc8b95e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef save_3D_animation(embeddings, emb_space_sizes, train_losses, test_losses, opt_name, n_bins=10, horizon_size=10, cmap_name='jet', **plotting_kwargs):\n    \"\"\"Utility function for visualizing the changes in weights over time in\n    UMAP space. The visualization is in 3D for better appreciating the descent\n    on the error surface.\n\n        Args:\n            - embeddings: list of embeddings, result of alligned UMAP\n            - emb_space_sizes: list of arrays, define the limits of the\n                embedding space for the three layers of the MLP.\n            - train_losses: list, training losses history.\n            - test_losses: list, test losses.\n            - opt_name: string, name of the optimizer used.\n            - n_bins: int, number of bins for discretizing the training loss.\n            -  horizon_size: int, maximum number of points simultaneously\n                on screen.\n            - cmap_name: string, name of the colormap used for representing\n                the change in train losses.\n            - **plotting_kwargs: keyword arguments, keyword arguments for the\n                plotting function.\n\n        Returns:\n            - None\n    \"\"\"\n    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n    cmap = matplotlib.cm.get_cmap(cmap_name)\n    colors = np.array(train_losses)\n    colors = discretizer.fit_transform(colors.reshape(-1, 1)).flatten()\n    norm = plt.Normalize(colors.min(), colors.max())\n    for i in tqdm(range(embeddings[0].shape[0])):\n        fig, axs = plt.subplots(1, 3, figsize=(30, 10), subplot_kw=dict(projection='3d'))\n        for index, emb in enumerate(embeddings):\n            min_sizes, max_sizes = emb_space_sizes[index]\n            past_horizon = max(0, i - horizon_size)\n            axs[index].scatter(emb[past_horizon:i, 0], emb[past_horizon:i, 1], train_losses[past_horizon:i], c=[cmap(norm(color)) for color in colors[past_horizon:i]], **plotting_kwargs)\n            axs[index].plot(xs=emb[past_horizon:i, 0], ys=train_losses[past_horizon:i], c='grey', zdir='y', zs=max_sizes[1], linewidth=5, alpha=0.25)\n            axs[index].plot(xs=emb[past_horizon:i, 1], ys=train_losses[past_horizon:i], c='grey', zdir='x', linewidth=5, alpha=0.25, zs=min_sizes[0])\n            axs[index].plot(xs=emb[past_horizon:i, 0], ys=emb[past_horizon:i, 1], c='grey', zdir='z', linewidth=5, alpha=0.25, zs=min_sizes[2])\n            axs[index].text2D(0.05, 0.95, f'Layer {index + 1}', transform=axs[index].transAxes)\n            if index == 1:\n                axs[index].text2D(0.5, 1.1, f'Optimizer: {opt_name}                     \\nTrain Loss: {round(train_losses[i], 3)}                     \\n Test Loss: {round(test_losses[i], 3)}', transform=axs[index].transAxes)\n            elif index == 2:\n                axs[index].set_xlabel('Weights Space \\n UMAP 1')\n                axs[index].set_ylabel('Weights Space \\n UMAP 2')\n                axs[index].set_zlabel('Trainining Loss')\n        if not os.path.exists(f'results\\\\3D_{opt_name}'):\n            os.makedirs(f'results\\\\3D_{opt_name}')\n        plt.savefig(f'results\\\\3D_{opt_name}\\\\{i}.png', bbox_inches='tight')\n        plt.close('all')\n    return None", "fn_id": 0, "class_fn": false, "repo": "vb690/machine_learning_exercises", "file": "shops/visualize_gradient_descent/utilities/viz_utils.py", "last_update_at": "2021-04-26T19:06:06+00:00", "original_content": "def save_3D_animation(embeddings, emb_space_sizes, train_losses, test_losses, opt_name, n_bins=10, horizon_size=10, cmap_name='jet', **plotting_kwargs):\n    \"\"\"Utility function for visualizing the changes in weights over time in\n    UMAP space. The visualization is in 3D for better appreciating the descent\n    on the error surface.\n\n        Args:\n            - embeddings: list of embeddings, result of alligned UMAP\n            - emb_space_sizes: list of arrays, define the limits of the\n                embedding space for the three layers of the MLP.\n            - train_losses: list, training losses history.\n            - test_losses: list, test losses.\n            - opt_name: string, name of the optimizer used.\n            - n_bins: int, number of bins for discretizing the training loss.\n            -  horizon_size: int, maximum number of points simultaneously\n                on screen.\n            - cmap_name: string, name of the colormap used for representing\n                the change in train losses.\n            - **plotting_kwargs: keyword arguments, keyword arguments for the\n                plotting function.\n\n        Returns:\n            - None\n    \"\"\"\n    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n    cmap = matplotlib.cm.get_cmap(cmap_name)\n    colors = np.array(train_losses)\n    colors = discretizer.fit_transform(colors.reshape(-1, 1)).flatten()\n    norm = plt.Normalize(colors.min(), colors.max())\n    for i in tqdm(range(embeddings[0].shape[0])):\n        fig, axs = plt.subplots(1, 3, figsize=(30, 10), subplot_kw=dict(projection='3d'))\n        for index, emb in enumerate(embeddings):\n            min_sizes, max_sizes = emb_space_sizes[index]\n            past_horizon = max(0, i - horizon_size)\n            axs[index].scatter(emb[past_horizon:i, 0], emb[past_horizon:i, 1], train_losses[past_horizon:i], c=[cmap(norm(color)) for color in colors[past_horizon:i]], **plotting_kwargs)\n            axs[index].plot(xs=emb[past_horizon:i, 0], ys=train_losses[past_horizon:i], c='grey', zdir='y', zs=max_sizes[1], linewidth=5, alpha=0.25)\n            axs[index].plot(xs=emb[past_horizon:i, 1], ys=train_losses[past_horizon:i], c='grey', zdir='x', linewidth=5, alpha=0.25, zs=min_sizes[0])\n            axs[index].plot(xs=emb[past_horizon:i, 0], ys=emb[past_horizon:i, 1], c='grey', zdir='z', linewidth=5, alpha=0.25, zs=min_sizes[2])\n            axs[index].text2D(0.05, 0.95, f'Layer {index + 1}', transform=axs[index].transAxes)\n            if index == 1:\n                axs[index].text2D(0.5, 1.1, f'Optimizer: {opt_name}                     \\nTrain Loss: {round(train_losses[i], 3)}                     \\n Test Loss: {round(test_losses[i], 3)}', transform=axs[index].transAxes)\n            elif index == 2:\n                axs[index].set_xlabel('Weights Space \\n UMAP 1')\n                axs[index].set_ylabel('Weights Space \\n UMAP 2')\n                axs[index].set_zlabel('Trainining Loss')\n        if not os.path.exists(f'results\\\\3D_{opt_name}'):\n            os.makedirs(f'results\\\\3D_{opt_name}')\n        plt.savefig(f'results\\\\3D_{opt_name}\\\\{i}.png', bbox_inches='tight')\n        plt.close('all')\n    return None", "refactored": true, "pred": {"ppl": 3.1853506565093994, "ppl_lower": 3.3992083072662354, "ppl/lowercase_ppl": -1.056086901897979, "ppl/zlib": 0.0009719483046973751, "Min_5.0% Prob": 10.138553351163864, "Min_10.0% Prob": 7.987467383344968, "Min_20.0% Prob": 5.257339299651625, "Min_30.0% Prob": 3.7821925052515777, "Min_40.0% Prob": 2.882716855468528, "Min_50.0% Prob": 2.3140008576507403, "Min_60.0% Prob": 1.9323593971354884}}
{"hexsha": "2096e74d026f92b88f279c71ae6c465be2f77b95", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef is_accuracy_aware_training(config: NNCFConfig, compression_config_passed: bool=False) -> bool:\n    \"\"\"\n    Returns True if the compression config contains an accuracy-aware\n    training related section, False otherwise.\n    \"\"\"\n    compression_config = config.get('compression', {}) if not compression_config_passed else config\n    if isinstance(compression_config, list):\n        for algo_config in compression_config:\n            if algo_config.get('accuracy_aware_training') is not None:\n                return True\n        return False\n    if compression_config.get('accuracy_aware_training') is not None:\n        return True\n    return False", "fn_id": 0, "class_fn": false, "repo": "sarthakpati/nncf", "file": "nncf/config/utils.py", "last_update_at": "2021-07-23T07:46:52+00:00", "original_content": "def is_accuracy_aware_training(config: NNCFConfig, compression_config_passed: bool=False) -> bool:\n    \"\"\"\n    Returns True if the compression config contains an accuracy-aware\n    training related section, False otherwise.\n    \"\"\"\n    compression_config = config.get('compression', {}) if not compression_config_passed else config\n    if isinstance(compression_config, list):\n        for algo_config in compression_config:\n            if algo_config.get('accuracy_aware_training') is not None:\n                return True\n        return False\n    if compression_config.get('accuracy_aware_training') is not None:\n        return True\n    return False", "refactored": true, "pred": {"ppl": 4.986274719238281, "ppl_lower": 6.82344913482666, "ppl/lowercase_ppl": -1.195231301514974, "ppl/zlib": 0.005540307178296274, "Min_5.0% Prob": 11.501993588038854, "Min_10.0% Prob": 9.271268463134765, "Min_20.0% Prob": 6.634060867627462, "Min_30.0% Prob": 5.02440455754598, "Min_40.0% Prob": 3.973753088712692, "Min_50.0% Prob": 3.2238437456885976, "Min_60.0% Prob": 2.6927167824159066}}
{"hexsha": "f4d5118418f49eeb3e5a0e0d7756f5a28b10fda6", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef decoder_layer(input_prob, input_aspect, n_hidden, n_class, n_aspects, random_base, l2_reg, sub_vocab, FLAGS, scope_name='1', use_aspect=True):\n    \"\"\"\n    Decoder structure of the autoencoder-like model taht reconstructs the sentence using the sentimenet embedding matrix\n\n    :param input_prob:\n    :param input_aspect:\n    :param n_hidden:\n    :param n_class:\n    :param n_aspects:\n    :param random_base:\n    :param l2_reg:\n    :param sub_vocab:\n    :param FLAGS:\n    :param scope_name:\n    :param use_aspect:\n    :return:\n    \"\"\"\n    w = tf.get_variable(name='sentiment_embedding' + scope_name, shape=[n_class, n_hidden], initializer=tf.random_uniform_initializer(-random_base, random_base), regularizer=tf.keras.regularizers.L2(l2_reg), trainable=True)\n    if use_aspect:\n        w_aspect = tf.get_variable(name='aspect_w' + scope_name, shape=[n_aspects, n_hidden], initializer=tf.random_uniform_initializer(-random_base, random_base), regularizer=tf.keras.regularizers.L2(l2_reg), trainable=True)\n    batch_size = tf.shape(input_prob)[0]\n    if use_aspect:\n        outputs = tf.matmul(input_prob, w) + tf.matmul(input_aspect, w_aspect)\n    else:\n        outputs = tf.matmul(input_prob, w)\n    return (outputs, w)", "fn_id": 3, "class_fn": false, "repo": "LucaZampierin/ABSE", "file": "nn_layer.py", "last_update_at": "2021-10-17T13:53:20+00:00", "original_content": "def decoder_layer(input_prob, input_aspect, n_hidden, n_class, n_aspects, random_base, l2_reg, sub_vocab, FLAGS, scope_name='1', use_aspect=True):\n    \"\"\"\n    Decoder structure of the autoencoder-like model taht reconstructs the sentence using the sentimenet embedding matrix\n\n    :param input_prob:\n    :param input_aspect:\n    :param n_hidden:\n    :param n_class:\n    :param n_aspects:\n    :param random_base:\n    :param l2_reg:\n    :param sub_vocab:\n    :param FLAGS:\n    :param scope_name:\n    :param use_aspect:\n    :return:\n    \"\"\"\n    w = tf.get_variable(name='sentiment_embedding' + scope_name, shape=[n_class, n_hidden], initializer=tf.random_uniform_initializer(-random_base, random_base), regularizer=tf.keras.regularizers.L2(l2_reg), trainable=True)\n    if use_aspect:\n        w_aspect = tf.get_variable(name='aspect_w' + scope_name, shape=[n_aspects, n_hidden], initializer=tf.random_uniform_initializer(-random_base, random_base), regularizer=tf.keras.regularizers.L2(l2_reg), trainable=True)\n    batch_size = tf.shape(input_prob)[0]\n    if use_aspect:\n        outputs = tf.matmul(input_prob, w) + tf.matmul(input_aspect, w_aspect)\n    else:\n        outputs = tf.matmul(input_prob, w)\n    return (outputs, w)", "refactored": true, "pred": {"ppl": 3.170217752456665, "ppl_lower": 3.3155951499938965, "ppl/lowercase_ppl": -1.0388601594931812, "ppl/zlib": 0.002502820557823722, "Min_5.0% Prob": 10.321392360486483, "Min_10.0% Prob": 8.198814489902594, "Min_20.0% Prob": 5.4630999473425055, "Min_30.0% Prob": 3.8321699478432665, "Min_40.0% Prob": 2.8960057764720077, "Min_50.0% Prob": 2.3071359868460735, "Min_60.0% Prob": 1.9245506334487605}}
{"hexsha": "c4ee4862ccbeff5f750d6e6bb757e31dbab056f4", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef solve_format_ArithOperation(optree, integer_size_func=lambda lhs_prec, rhs_prec: None, frac_size_func=lambda lhs_prec, rhs_prec: None, signed_func=lambda lhs, lhs_prec, rhs, rhs_prec: False, format_solver=None):\n    \"\"\" determining fixed-point format for a generic 2-op arithmetic\n        operation (e.g. Multiplication, Addition, Subtraction)\n    \"\"\"\n    lhs = optree.get_input(0)\n    rhs = optree.get_input(1)\n    lhs_precision = lhs.get_precision()\n    rhs_precision = rhs.get_precision()\n    abstract_operation = lhs_precision is ML_Integer and rhs_precision is ML_Integer\n    if abstract_operation:\n        return ML_Integer\n    if lhs_precision is ML_Integer:\n        cst_eval = evaluate_cst_graph(lhs, input_prec_solver=format_solver)\n        lhs_precision = solve_format_Constant(Constant(cst_eval))\n    if rhs_precision is ML_Integer:\n        cst_eval = evaluate_cst_graph(rhs, input_prec_solver=format_solver)\n        rhs_precision = solve_format_Constant(Constant(cst_eval))\n    if is_fixed_point(lhs_precision) and is_fixed_point(rhs_precision):\n        int_size = integer_size_func(lhs_precision, rhs_precision)\n        frac_size = frac_size_func(lhs_precision, rhs_precision)\n        is_signed = signed_func(lhs, lhs_precision, rhs, rhs_precision)\n        return fixed_point(int_size, frac_size, signed=is_signed)\n    else:\n        return optree.get_precision()", "fn_id": 3, "class_fn": false, "repo": "nibrunie/metalibm", "file": "metalibm_core/opt/p_size_datapath.py", "last_update_at": "2021-03-12T18:54:53+00:00", "original_content": "def solve_format_ArithOperation(optree, integer_size_func=lambda lhs_prec, rhs_prec: None, frac_size_func=lambda lhs_prec, rhs_prec: None, signed_func=lambda lhs, lhs_prec, rhs, rhs_prec: False, format_solver=None):\n    \"\"\" determining fixed-point format for a generic 2-op arithmetic\n        operation (e.g. Multiplication, Addition, Subtraction)\n    \"\"\"\n    lhs = optree.get_input(0)\n    rhs = optree.get_input(1)\n    lhs_precision = lhs.get_precision()\n    rhs_precision = rhs.get_precision()\n    abstract_operation = lhs_precision is ML_Integer and rhs_precision is ML_Integer\n    if abstract_operation:\n        return ML_Integer\n    if lhs_precision is ML_Integer:\n        cst_eval = evaluate_cst_graph(lhs, input_prec_solver=format_solver)\n        lhs_precision = solve_format_Constant(Constant(cst_eval))\n    if rhs_precision is ML_Integer:\n        cst_eval = evaluate_cst_graph(rhs, input_prec_solver=format_solver)\n        rhs_precision = solve_format_Constant(Constant(cst_eval))\n    if is_fixed_point(lhs_precision) and is_fixed_point(rhs_precision):\n        int_size = integer_size_func(lhs_precision, rhs_precision)\n        frac_size = frac_size_func(lhs_precision, rhs_precision)\n        is_signed = signed_func(lhs, lhs_precision, rhs, rhs_precision)\n        return fixed_point(int_size, frac_size, signed=is_signed)\n    else:\n        return optree.get_precision()", "refactored": true, "pred": {"ppl": 3.906754732131958, "ppl_lower": 4.033107280731201, "ppl/lowercase_ppl": -1.0233579762965486, "ppl/zlib": 0.002930552768991997, "Min_5.0% Prob": 11.920680618286132, "Min_10.0% Prob": 9.216559863090515, "Min_20.0% Prob": 6.144392380007991, "Min_30.0% Prob": 4.4284258563010415, "Min_40.0% Prob": 3.40816554615711, "Min_50.0% Prob": 2.729919659652807, "Min_60.0% Prob": 2.272822604756077}}
{"hexsha": "bc8a29dc53319c769a4b51f92593b8f3fdee45c6", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef update_df_all(df_all, training_data, validation_data, test_data):\n    \"\"\"Add a column \"group\" to df_all indicating train/validation or test\"\"\"\n    df_all['group'] = 0\n    for simu_name in training_data:\n        df_all['group'][df_all['topo_name'] == simu_name] = 'train'\n    for simu_name in validation_data:\n        df_all['group'][df_all['topo_name'] == simu_name] = 'validation'\n    for simu_name in test_data:\n        df_all['group'][df_all['topo_name'] == simu_name] = 'test'\n    return df_all", "fn_id": 1, "class_fn": false, "repo": "louisletoumelin/wind_downscaling_cnn", "file": "pre_process/preprocess_folds.py", "last_update_at": "2021-12-13T16:26:31+00:00", "original_content": "def update_df_all(df_all, training_data, validation_data, test_data):\n    \"\"\"Add a column \"group\" to df_all indicating train/validation or test\"\"\"\n    df_all['group'] = 0\n    for simu_name in training_data:\n        df_all['group'][df_all['topo_name'] == simu_name] = 'train'\n    for simu_name in validation_data:\n        df_all['group'][df_all['topo_name'] == simu_name] = 'validation'\n    for simu_name in test_data:\n        df_all['group'][df_all['topo_name'] == simu_name] = 'test'\n    return df_all", "refactored": true, "pred": {"ppl": 3.1011288166046143, "ppl_lower": 3.1382880210876465, "ppl/lowercase_ppl": -1.0105244843303267, "ppl/zlib": 0.005441183555763697, "Min_5.0% Prob": 11.274142920970917, "Min_10.0% Prob": 8.17335148418651, "Min_20.0% Prob": 5.272432078633989, "Min_30.0% Prob": 3.7447287405912695, "Min_40.0% Prob": 2.8237812529717172, "Min_50.0% Prob": 2.276309970991108, "Min_60.0% Prob": 1.8862566574587531}}
{"hexsha": "6bef08836876832f7516142a645d88720de907e9", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef update_image_metadata_from_image_path(image_path: str, metadata: Dict):\n    \"\"\" \"\"\"\n    pickled = codecs.encode(pickle.dumps(metadata), 'base64').decode()\n    db = get_db()\n    cur = db.cursor()\n    cur.execute(f'UPDATE images SET metadata = \"{pickled}\" WHERE path = \"{image_path}\" AND metadata IS NULL')\n    db.commit()", "fn_id": 0, "class_fn": false, "repo": "frederikgram/describe", "file": "dev/frontend/models/database_updaters.py", "last_update_at": "2021-03-10T01:32:19+00:00", "original_content": "def update_image_metadata_from_image_path(image_path: str, metadata: Dict):\n    \"\"\" \"\"\"\n    pickled = codecs.encode(pickle.dumps(metadata), 'base64').decode()\n    db = get_db()\n    cur = db.cursor()\n    cur.execute(f'UPDATE images SET metadata = \"{pickled}\" WHERE path = \"{image_path}\" AND metadata IS NULL')\n    db.commit()", "refactored": true, "pred": {"ppl": 4.796999454498291, "ppl_lower": 4.78302526473999, "ppl/lowercase_ppl": -0.9981394264054912, "ppl/zlib": 0.006729573428337588, "Min_5.0% Prob": 11.533672332763672, "Min_10.0% Prob": 9.604647254943847, "Min_20.0% Prob": 6.400064718155634, "Min_30.0% Prob": 4.909809570158681, "Min_40.0% Prob": 3.8208818024113063, "Min_50.0% Prob": 3.1321401693500004, "Min_60.0% Prob": 2.6063812389260246}}
{"hexsha": "6c468eb327991e8438e939f0e8280fc938e87e2b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef float_sanitiser():\n\n    def sanitise(n, debug_name):\n        if not isinstance(n, float):\n            raise CefTypeError('{}: Expected float, got {}'.format(debug_name, type(n)))\n        else:\n            return str(n)\n    return sanitise", "fn_id": 3, "class_fn": false, "repo": "slallum/format_cef", "file": "src/format_cef/_cef/base.py", "last_update_at": "2021-04-28T08:58:00+00:00", "original_content": "def float_sanitiser():\n\n    def sanitise(n, debug_name):\n        if not isinstance(n, float):\n            raise CefTypeError('{}: Expected float, got {}'.format(debug_name, type(n)))\n        else:\n            return str(n)\n    return sanitise", "refactored": true, "pred": {"ppl": 12.128861427307129, "ppl_lower": 13.576531410217285, "ppl/lowercase_ppl": -1.0451816657031177, "ppl/zlib": 0.014342458933073675, "Min_5.0% Prob": 13.195121129353842, "Min_10.0% Prob": 10.623894214630127, "Min_20.0% Prob": 8.518189532416207, "Min_30.0% Prob": 7.1742936088925315, "Min_40.0% Prob": 6.019196182489395, "Min_50.0% Prob": 4.987672371523721, "Min_60.0% Prob": 4.196515207134542}}
{"hexsha": "22b492bfb5e030f558182e6d784c2ccfc5064429", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef hive_copy_to_table(source_dataset, source_table_name, destination_dataset, destination_table_name, s3_step_path, local_step_path, action_id, set_hive_vars, step_num, steps_total):\n    hive_source_path = os.path.join(local_step_path, 'hive', 'copy_to_table.hql')\n    hive_target_path = os.path.join(local_step_path, 'hive', 'copy_to_table_%s.hql' % destination_table_name)\n    with open(hive_source_path, 'r') as s, open(hive_target_path, 'w') as t:\n        contents = s.read().format(source_table_name=source_table_name, destination_table_name=destination_table_name, partitions=get_partitions(source_dataset), columns=get_columns(source_dataset, destination_dataset), compression=get_emr_compression(destination_dataset), set_hive_vars=set_hive_vars if set_hive_vars else '')\n        t.write(contents)\n    return StepWrapper(JarStep(name='dart: (%s) copy_to_table_%s.hql (from %s)' % (_title_data(action_id, step_num, steps_total), destination_table_name, source_table_name), jar=_command_runner_jar, action_on_failure='CONTINUE', step_args=_hive_args + [s3_step_path + '/hive/copy_to_table_%s.hql' % destination_table_name]), step_num, steps_total)", "fn_id": 4, "class_fn": false, "repo": "RetailMeNotSandbox/dart", "file": "src/python/dart/engine/emr/steps.py", "last_update_at": "2021-07-14T22:37:35+00:00", "original_content": "def hive_copy_to_table(source_dataset, source_table_name, destination_dataset, destination_table_name, s3_step_path, local_step_path, action_id, set_hive_vars, step_num, steps_total):\n    hive_source_path = os.path.join(local_step_path, 'hive', 'copy_to_table.hql')\n    hive_target_path = os.path.join(local_step_path, 'hive', 'copy_to_table_%s.hql' % destination_table_name)\n    with open(hive_source_path, 'r') as s, open(hive_target_path, 'w') as t:\n        contents = s.read().format(source_table_name=source_table_name, destination_table_name=destination_table_name, partitions=get_partitions(source_dataset), columns=get_columns(source_dataset, destination_dataset), compression=get_emr_compression(destination_dataset), set_hive_vars=set_hive_vars if set_hive_vars else '')\n        t.write(contents)\n    return StepWrapper(JarStep(name='dart: (%s) copy_to_table_%s.hql (from %s)' % (_title_data(action_id, step_num, steps_total), destination_table_name, source_table_name), jar=_command_runner_jar, action_on_failure='CONTINUE', step_args=_hive_args + [s3_step_path + '/hive/copy_to_table_%s.hql' % destination_table_name]), step_num, steps_total)", "refactored": true, "pred": {"ppl": 3.9914162158966064, "ppl_lower": 4.186213970184326, "ppl/lowercase_ppl": -1.0344260109637207, "ppl/zlib": 0.003042079360994442, "Min_5.0% Prob": 11.730511213603773, "Min_10.0% Prob": 9.114287963280312, "Min_20.0% Prob": 6.145592972254142, "Min_30.0% Prob": 4.483551836421347, "Min_40.0% Prob": 3.4387037792972697, "Min_50.0% Prob": 2.771252942024445, "Min_60.0% Prob": 2.314333211218423}}
{"hexsha": "1a8166a42e84641b39cb8babc256e3c8ddcff393", "ext": "py", "lang": "Python", "content": "def check_post_status(func):\n    \"\"\"\n     call the func only if the smc.setting is not already posted via API\n    Args:\n        func:write_file\n\n    Returns:\n        wrapper: confirmation if the smc.settings file has been posted already\n    \"\"\"\n\n    @timeing\n    @measure_memory_usage\n    def wrapper(*args, **kwargs):\n        if os.environ.get('stealth_watch_post', '0') == '0':\n            func(*args, **kwargs)\n        else:\n            print(f'{Style.RED}smc.setting file data is already posted to smc server from this machine, so skipping the operation for function {func.__qualname__}{Style.RESET}')\n            print(f'{Style.GREEN}Thank you!{Style.RESET}')\n    return wrapper", "fn_id": 2, "class_fn": false, "repo": "CiscoDevNet/sna-initial-config-setup-script", "file": "utils/misc.py", "last_update_at": "2021-12-23T20:16:15+00:00", "original_content": "def check_post_status(func):\n    \"\"\"\n     call the func only if the smc.setting is not already posted via API\n    Args:\n        func:write_file\n\n    Returns:\n        wrapper: confirmation if the smc.settings file has been posted already\n    \"\"\"\n\n    def wrapper(*args, **kwargs):\n        if os.environ.get('stealth_watch_post', '0') == '0':\n            func(*args, **kwargs)\n        else:\n            print(f'{Style.RED}smc.setting file data is already posted to smc server from this machine, so skipping the operation for function {func.__qualname__}{Style.RESET}')\n            print(f'{Style.GREEN}Thank you!{Style.RESET}')\n    return wrapper", "refactored": true, "pred": {"ppl": 12.757845878601074, "ppl_lower": 11.852877616882324, "ppl/lowercase_ppl": -0.9711030877510143, "ppl/zlib": 0.0069949078169609345, "Min_5.0% Prob": 13.793225884437561, "Min_10.0% Prob": 11.704691157621497, "Min_20.0% Prob": 8.76787750861224, "Min_30.0% Prob": 7.0715159762139415, "Min_40.0% Prob": 5.882589240284527, "Min_50.0% Prob": 4.932595232198405, "Min_60.0% Prob": 4.216878886651067}}
{"hexsha": "1380364880e72805de31c21c228ab7d9dbd45947", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef update_activity_notice_seed_date_of_notice_earlier_than_update_window(activity_notice_seed):\n    status = ''\n    success = True\n    activity_notice_seed_changed = False\n    from activity.models import get_lifespan_of_seed\n    lifespan_of_seed_in_seconds = get_lifespan_of_seed(activity_notice_seed.kind_of_seed)\n    earliest_date_of_notice = now() - timedelta(seconds=lifespan_of_seed_in_seconds)\n    if activity_notice_seed.date_of_notice < earliest_date_of_notice:\n        try:\n            activity_notice_seed.date_of_notice_earlier_than_update_window = True\n            activity_notice_seed.save()\n            activity_notice_seed_changed = True\n            status += 'DATE_OF_NOTICE_EARLIER_THAN_UPDATE_WINDOW_SET_TRUE '\n        except Exception as e:\n            status += 'COULD_NOT_UPDATE-date_of_notice_earlier_than_update_window: ' + str(e) + ' '\n            success = False\n    results = {'success': success, 'status': status, 'activity_notice_seed': activity_notice_seed, 'activity_notice_seed_changed': activity_notice_seed_changed, 'date_of_notice_earlier_than_update_window': activity_notice_seed.date_of_notice_earlier_than_update_window}\n    return results", "fn_id": 24, "class_fn": false, "repo": "aucoeur/WeVoteServer", "file": "activity/controllers.py", "last_update_at": "2021-03-17T02:08:26+00:00", "original_content": "def update_activity_notice_seed_date_of_notice_earlier_than_update_window(activity_notice_seed):\n    status = ''\n    success = True\n    activity_notice_seed_changed = False\n    from activity.models import get_lifespan_of_seed\n    lifespan_of_seed_in_seconds = get_lifespan_of_seed(activity_notice_seed.kind_of_seed)\n    earliest_date_of_notice = now() - timedelta(seconds=lifespan_of_seed_in_seconds)\n    if activity_notice_seed.date_of_notice < earliest_date_of_notice:\n        try:\n            activity_notice_seed.date_of_notice_earlier_than_update_window = True\n            activity_notice_seed.save()\n            activity_notice_seed_changed = True\n            status += 'DATE_OF_NOTICE_EARLIER_THAN_UPDATE_WINDOW_SET_TRUE '\n        except Exception as e:\n            status += 'COULD_NOT_UPDATE-date_of_notice_earlier_than_update_window: ' + str(e) + ' '\n            success = False\n    results = {'success': success, 'status': status, 'activity_notice_seed': activity_notice_seed, 'activity_notice_seed_changed': activity_notice_seed_changed, 'date_of_notice_earlier_than_update_window': activity_notice_seed.date_of_notice_earlier_than_update_window}\n    return results", "refactored": true, "pred": {"ppl": 2.795811414718628, "ppl_lower": 2.8635730743408203, "ppl/lowercase_ppl": -1.0232927504722356, "ppl/zlib": 0.0025076155459902313, "Min_5.0% Prob": 9.393945553723503, "Min_10.0% Prob": 7.364051151275635, "Min_20.0% Prob": 4.811769225228001, "Min_30.0% Prob": 3.3823374969379922, "Min_40.0% Prob": 2.56334271584149, "Min_50.0% Prob": 2.054609255813187, "Min_60.0% Prob": 1.7197943328886303}}
{"hexsha": "6f641f627b8305f23c544478d79bdef1903c7e0c", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef sample_mask(idx, l):\n    \"\"\"Create mask.\"\"\"\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)", "fn_id": 1, "class_fn": false, "repo": "psychologyphd/HWGCN", "file": "utils.py", "last_update_at": "2021-08-04T09:31:04+00:00", "original_content": "def sample_mask(idx, l):\n    \"\"\"Create mask.\"\"\"\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)", "refactored": true, "pred": {"ppl": 4.288182258605957, "ppl_lower": 4.929478645324707, "ppl/lowercase_ppl": -1.0957303748852414, "ppl/zlib": 0.011113457459574755, "Min_5.0% Prob": 10.134751319885254, "Min_10.0% Prob": 8.947676372528075, "Min_20.0% Prob": 6.372513597661799, "Min_30.0% Prob": 4.796875476837158, "Min_40.0% Prob": 3.597795001485131, "Min_50.0% Prob": 2.952560063589502, "Min_60.0% Prob": 2.4225736439905385}}
{"hexsha": "6f3e2a72a770d5215943ec27b58899302ae9bfa0", "ext": "py", "lang": "Python", "content": "def table_args(model: Base) -> None:\n\n    @timeing\n    @measure_memory_usage\n    def create_new_column(column: sa.Column) -> sa.Column:\n        attributes: List[str] = ['name', 'type', 'key', 'primary_key', 'nullable', 'default', 'server_default', 'server_onupdate', 'index', 'unique', 'system', 'doc', 'onupdate', 'autoincrement', 'comment']\n        kwargs: Dict[str, Any] = {a: getattr(column, a, None) for a in attributes}\n        kwargs['type_']: 'TypeEngine' = kwargs.get('type', None)\n        del kwargs['type']\n        return sa.Column(**kwargs)\n    columns: List[sa.Column] = list(map(create_new_column, model.__table__.columns.values()))\n    return (model.__tablename__, *columns)", "fn_id": 0, "class_fn": false, "repo": "h4wldev/seed", "file": "migrations/utils.py", "last_update_at": "2021-06-01T06:23:41+00:00", "original_content": "def table_args(model: Base) -> None:\n\n    def create_new_column(column: sa.Column) -> sa.Column:\n        attributes: List[str] = ['name', 'type', 'key', 'primary_key', 'nullable', 'default', 'server_default', 'server_onupdate', 'index', 'unique', 'system', 'doc', 'onupdate', 'autoincrement', 'comment']\n        kwargs: Dict[str, Any] = {a: getattr(column, a, None) for a in attributes}\n        kwargs['type_']: 'TypeEngine' = kwargs.get('type', None)\n        del kwargs['type']\n        return sa.Column(**kwargs)\n    columns: List[sa.Column] = list(map(create_new_column, model.__table__.columns.values()))\n    return (model.__tablename__, *columns)", "refactored": true, "pred": {"ppl": 4.999105453491211, "ppl_lower": 5.703848361968994, "ppl/lowercase_ppl": -1.0819520737059443, "ppl/zlib": 0.004520390413275743, "Min_5.0% Prob": 10.26873779296875, "Min_10.0% Prob": 8.452328944206238, "Min_20.0% Prob": 6.271654158830643, "Min_30.0% Prob": 4.9397871116797125, "Min_40.0% Prob": 3.9320175498723984, "Min_50.0% Prob": 3.1985112135112286, "Min_60.0% Prob": 2.6778370342217386}}
{"hexsha": "fe84bfb3726301394e904f8a5ed253c3c9774c54", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef enlarge_bbox(bbox, im_size, ratio=0.15):\n    width, height = (bbox[2] - bbox[0], bbox[3] - bbox[1])\n    half_width_inc, half_height_inc = ((width * ratio).astype(np.int32), (height * ratio).astype(np.int32))\n    bbox[0], bbox[1], bbox[2], bbox[3] = (bbox[0] - half_width_inc, bbox[1] - half_height_inc, bbox[2] + half_width_inc, bbox[3] + half_height_inc)\n    bbox[0], bbox[1] = (max(bbox[0], 0), max(bbox[1], 0))\n    bbox[3], bbox[2] = (min(bbox[3], im_size[0]), min(bbox[2], im_size[1]))\n    return bbox", "fn_id": 0, "class_fn": false, "repo": "ErestorX/Buddha_alignment", "file": "generate_dataset.py", "last_update_at": "2021-05-10T08:27:46+00:00", "original_content": "def enlarge_bbox(bbox, im_size, ratio=0.15):\n    width, height = (bbox[2] - bbox[0], bbox[3] - bbox[1])\n    half_width_inc, half_height_inc = ((width * ratio).astype(np.int32), (height * ratio).astype(np.int32))\n    bbox[0], bbox[1], bbox[2], bbox[3] = (bbox[0] - half_width_inc, bbox[1] - half_height_inc, bbox[2] + half_width_inc, bbox[3] + half_height_inc)\n    bbox[0], bbox[1] = (max(bbox[0], 0), max(bbox[1], 0))\n    bbox[3], bbox[2] = (min(bbox[3], im_size[0]), min(bbox[2], im_size[1]))\n    return bbox", "refactored": true, "pred": {"ppl": 2.4330854415893555, "ppl_lower": 2.4330854415893555, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.003659095394968534, "Min_5.0% Prob": 10.206950881264426, "Min_10.0% Prob": 7.346406719901345, "Min_20.0% Prob": 4.36639616028829, "Min_30.0% Prob": 2.9798354406022662, "Min_40.0% Prob": 2.2419550423880783, "Min_50.0% Prob": 1.7781440370793764, "Min_60.0% Prob": 1.4841240957294874}}
{"hexsha": "13d942d0b49cb364c495b965f3f53cdb29e47d14", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef line_counts_as_uncovered(line: str, is_from_cover_annotation_file: bool) -> bool:\n    \"\"\"\n    Args:\n        line: The line of code (including coverage annotation).\n        is_from_cover_annotation_file: Whether this line has been annotated.\n    Returns:\n        Does the line count as uncovered?\n    \"\"\"\n    if is_from_cover_annotation_file:\n        if not line.startswith('! '):\n            return False\n        content = line[2:]\n    else:\n        content = line\n    content = content.strip()\n    if '#' in content:\n        content = content[:content.index('#')].strip()\n    if any((re.search(pat, content) for pat in IGNORED_LINE_PATTERNS)):\n        return False\n    return is_from_cover_annotation_file or line_content_counts_as_uncovered_manual(content)", "fn_id": 6, "class_fn": false, "repo": "Uzayyy/Cirq", "file": "dev_tools/incremental_coverage.py", "last_update_at": "2021-03-07T19:34:28+00:00", "original_content": "def line_counts_as_uncovered(line: str, is_from_cover_annotation_file: bool) -> bool:\n    \"\"\"\n    Args:\n        line: The line of code (including coverage annotation).\n        is_from_cover_annotation_file: Whether this line has been annotated.\n    Returns:\n        Does the line count as uncovered?\n    \"\"\"\n    if is_from_cover_annotation_file:\n        if not line.startswith('! '):\n            return False\n        content = line[2:]\n    else:\n        content = line\n    content = content.strip()\n    if '#' in content:\n        content = content[:content.index('#')].strip()\n    if any((re.search(pat, content) for pat in IGNORED_LINE_PATTERNS)):\n        return False\n    return is_from_cover_annotation_file or line_content_counts_as_uncovered_manual(content)", "refactored": true, "pred": {"ppl": 5.510843276977539, "ppl_lower": 6.172983646392822, "ppl/lowercase_ppl": -1.066481200037521, "ppl/zlib": 0.004688784769994969, "Min_5.0% Prob": 10.054282665252686, "Min_10.0% Prob": 8.723013877868652, "Min_20.0% Prob": 6.955870023587855, "Min_30.0% Prob": 5.3269041968930155, "Min_40.0% Prob": 4.197945298918759, "Min_50.0% Prob": 3.397629026610118, "Min_60.0% Prob": 2.859326164929136}}
{"hexsha": "3013d6318ebcca79568b59c329de03eb84f3b683", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef save_builtin(img, path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    img.filepath = str(path).replace('.png', '-builtinsave.png')\n    img.file_format = 'PNG'\n    img.save()\n    log.info(f'wrote {path}')", "fn_id": 0, "class_fn": false, "repo": "drewp/megasecond", "file": "world_export/image.py", "last_update_at": "2021-04-06T07:51:27+00:00", "original_content": "def save_builtin(img, path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    img.filepath = str(path).replace('.png', '-builtinsave.png')\n    img.file_format = 'PNG'\n    img.save()\n    log.info(f'wrote {path}')", "refactored": true, "pred": {"ppl": 8.24012279510498, "ppl_lower": 10.846758842468262, "ppl/lowercase_ppl": -1.130321990577648, "ppl/zlib": 0.011782208078932443, "Min_5.0% Prob": 14.006681442260742, "Min_10.0% Prob": 11.325913310050964, "Min_20.0% Prob": 8.730538249015808, "Min_30.0% Prob": 6.829963182409604, "Min_40.0% Prob": 5.312461292371154, "Min_50.0% Prob": 4.196925039153274, "Min_60.0% Prob": 3.524623186688642}}
{"hexsha": "0322be0d34cfd540d38d08178833c28642a03567", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef MinSpanning(n, m):\n    dt = defaultdict(list)\n    for i in range(m):\n        x, y, w = map(int, input().slpit())\n        dt[x].append([y, w])\n        dt[y].append([x, w])\n    parent = [-1] * n\n    key = [float('inf')] * n\n    mstSet = [False] * n\n    prq = [(0, 0)]\n    key[0] = 0\n    for i in range(n - 1):\n        u = heappop(prq)[1]\n        mstSet[u] = True\n        for x, w in dt[u]:\n            if not mstSet[x] and w < key[x]:\n                parent[x] = u\n                key[x] = w\n                heappush(prq, (key[x], x))", "fn_id": 0, "class_fn": false, "repo": "Saicharan67/Interview-Coding-Questions", "file": "Graphs/MinSpanningTreeByPrims.py", "last_update_at": "2021-11-04T03:30:00+00:00", "original_content": "def MinSpanning(n, m):\n    dt = defaultdict(list)\n    for i in range(m):\n        x, y, w = map(int, input().slpit())\n        dt[x].append([y, w])\n        dt[y].append([x, w])\n    parent = [-1] * n\n    key = [float('inf')] * n\n    mstSet = [False] * n\n    prq = [(0, 0)]\n    key[0] = 0\n    for i in range(n - 1):\n        u = heappop(prq)[1]\n        mstSet[u] = True\n        for x, w in dt[u]:\n            if not mstSet[x] and w < key[x]:\n                parent[x] = u\n                key[x] = w\n                heappush(prq, (key[x], x))", "refactored": true, "pred": {"ppl": 2.776376485824585, "ppl_lower": 2.8841025829315186, "ppl/lowercase_ppl": -1.0372788111541016, "ppl/zlib": 0.0035090950348573976, "Min_5.0% Prob": 10.347567510604858, "Min_10.0% Prob": 8.037123870849609, "Min_20.0% Prob": 5.009491897210842, "Min_30.0% Prob": 3.43586710901534, "Min_40.0% Prob": 2.5638282774697716, "Min_50.0% Prob": 2.0420330997954292, "Min_60.0% Prob": 1.7101629154280593}}
{"hexsha": "715d0f56efd10b24b6c396c628f36713f9d9d35a", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef is_frozen():\n    \"\"\"Return a bool indicating if application is compressed\"\"\"\n    import imp\n    return hasattr(sys, 'frozen') or imp.is_frozen('__main__')", "fn_id": 4, "class_fn": false, "repo": "zywek123/accessible_output2", "file": "build/lib/accessible_output2/platform_utils/paths.py", "last_update_at": "2021-11-29T05:20:10+00:00", "original_content": "def is_frozen():\n    \"\"\"Return a bool indicating if application is compressed\"\"\"\n    import imp\n    return hasattr(sys, 'frozen') or imp.is_frozen('__main__')", "refactored": true, "pred": {"ppl": 13.715469360351562, "ppl_lower": 14.839655876159668, "ppl/lowercase_ppl": -1.0300851517505911, "ppl/zlib": 0.018058788594578466, "Min_5.0% Prob": 12.334571361541748, "Min_10.0% Prob": 11.23174524307251, "Min_20.0% Prob": 8.977199501461453, "Min_30.0% Prob": 7.418479476656232, "Min_40.0% Prob": 6.25305606189527, "Min_50.0% Prob": 5.257392923037211, "Min_60.0% Prob": 4.405070407380318}}
{"hexsha": "9c4f0f1dce62fe32e46d081a9b30a7b8429010c4", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_offline_chunked():\n    r = http('--offline', '--chunked', '--form', 'https://this-should.never-resolve/foo', 'hello=world')\n    assert 'POST /foo' in r\n    assert 'Transfer-Encoding: chunked' in r, r\n    assert 'hello=world' in r", "fn_id": 5, "class_fn": false, "repo": "Wallyhs04/httpie", "file": "tests/test_offline.py", "last_update_at": "2021-05-29T14:40:37+00:00", "original_content": "def test_offline_chunked():\n    r = http('--offline', '--chunked', '--form', 'https://this-should.never-resolve/foo', 'hello=world')\n    assert 'POST /foo' in r\n    assert 'Transfer-Encoding: chunked' in r, r\n    assert 'hello=world' in r", "refactored": true, "pred": {"ppl": 11.350179672241211, "ppl_lower": 14.100790977478027, "ppl/lowercase_ppl": -1.0893274824928916, "ppl/zlib": 0.013724483468675292, "Min_5.0% Prob": 12.17422080039978, "Min_10.0% Prob": 10.52078902721405, "Min_20.0% Prob": 8.395440846681595, "Min_30.0% Prob": 7.0007697741190595, "Min_40.0% Prob": 5.8073849231004715, "Min_50.0% Prob": 4.752524136043176, "Min_60.0% Prob": 4.0361621763025015}}
{"hexsha": "9c3d586ec62841559008de5a1baefd1f7c1ea4ef", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef parse_logging_lvl(lvl_name: str) -> int:\n    if lvl_name:\n        lvl_name = lvl_name.strip().upper()\n        return logging._nameToLevel.get(lvl_name, logging.INFO)\n    else:\n        return logging.INFO", "fn_id": 0, "class_fn": false, "repo": "ove/ove-asset-manager", "file": "common/util.py", "last_update_at": "2021-06-06T23:01:57+00:00", "original_content": "def parse_logging_lvl(lvl_name: str) -> int:\n    if lvl_name:\n        lvl_name = lvl_name.strip().upper()\n        return logging._nameToLevel.get(lvl_name, logging.INFO)\n    else:\n        return logging.INFO", "refactored": true, "pred": {"ppl": 4.535205841064453, "ppl_lower": 6.387477397918701, "ppl/lowercase_ppl": -1.2265200302674155, "ppl/zlib": 0.01007913647977933, "Min_5.0% Prob": 10.344675064086914, "Min_10.0% Prob": 8.69450957434518, "Min_20.0% Prob": 6.250341977391924, "Min_30.0% Prob": 4.682295265651884, "Min_40.0% Prob": 3.6988548721585954, "Min_50.0% Prob": 3.0293559517179216, "Min_60.0% Prob": 2.5478445355381285}}
{"hexsha": "977d85f77545dff9112557fdde20247c847ed065", "ext": "py", "lang": "Python", "content": "@login_required\n@timeing\n@measure_memory_usage\ndef edit_folder(request, set_id):\n    folder = get_object_or_404(CardFolder, id=set_id)\n    if folder.user != request.user:\n        return redirect('/no_access/')\n    if folder.being_edited:\n        return render(request, 'Cards/folder_being_updated.html', {'folder': folder})\n    if request.method == 'POST':\n        form = FolderForm(request.POST or None, instance=folder)\n        if form.is_valid():\n            folder = form.save(commit=False)\n            folder.being_edited = True\n            folder.save()\n            t = Thread(target=edit_folder_translate, args=[folder])\n            t.setDaemon(False)\n            t.start()\n            enough = len(folder.multicard_set.all()) > 2\n            return render(request, 'Cards/view_set.html', {'folder': folder, 'enough': enough})\n    else:\n        form = FolderForm(instance=folder)\n    return render(request, 'Cards/edit_set.html', {'form': form, 'folder': folder})", "fn_id": 3, "class_fn": false, "repo": "Solurix/Flashcards-Django", "file": "FCards/Cards/views.py", "last_update_at": "2021-05-16T03:20:23+00:00", "original_content": "@login_required\ndef edit_folder(request, set_id):\n    folder = get_object_or_404(CardFolder, id=set_id)\n    if folder.user != request.user:\n        return redirect('/no_access/')\n    if folder.being_edited:\n        return render(request, 'Cards/folder_being_updated.html', {'folder': folder})\n    if request.method == 'POST':\n        form = FolderForm(request.POST or None, instance=folder)\n        if form.is_valid():\n            folder = form.save(commit=False)\n            folder.being_edited = True\n            folder.save()\n            t = Thread(target=edit_folder_translate, args=[folder])\n            t.setDaemon(False)\n            t.start()\n            enough = len(folder.multicard_set.all()) > 2\n            return render(request, 'Cards/view_set.html', {'folder': folder, 'enough': enough})\n    else:\n        form = FolderForm(instance=folder)\n    return render(request, 'Cards/edit_set.html', {'form': form, 'folder': folder})", "refactored": true, "pred": {"ppl": 3.088284969329834, "ppl_lower": 3.9193437099456787, "ppl/lowercase_ppl": -1.2113381922599777, "ppl/zlib": 0.0027171467728163813, "Min_5.0% Prob": 11.348227794353779, "Min_10.0% Prob": 8.393924786494328, "Min_20.0% Prob": 5.30384495798147, "Min_30.0% Prob": 3.7195319118379038, "Min_40.0% Prob": 2.805896899961638, "Min_50.0% Prob": 2.2614524452241533, "Min_60.0% Prob": 1.8788749893670373}}
{"hexsha": "1767ab96cf054ceb50f317b23720f5c55d829e30", "ext": "py", "lang": "Python", "content": "@pytest.fixture\n@timeing\n@measure_memory_usage\ndef fixture_property_typed_multiple_choice_filter():\n    TypedMultipleChoiceFilterModel.objects.create(id=-1, text='1')\n    TypedMultipleChoiceFilterModel.objects.create(id=0, text='One')\n    TypedMultipleChoiceFilterModel.objects.create(id=1, text='2')\n    TypedMultipleChoiceFilterModel.objects.create(id=2, text='2')\n    TypedMultipleChoiceFilterModel.objects.create(id=3, text='Not a Number')\n    TypedMultipleChoiceFilterModel.objects.create(id=4, text='3')\n    TypedMultipleChoiceFilterModel.objects.create(id=5, text='3.3')\n    TypedMultipleChoiceFilterModel.objects.create(id=6)\n    global LOOKUP_CHOICES\n    LOOKUP_CHOICES = [(c.text, c.text) for c in TypedMultipleChoiceFilterModel.objects.order_by('id')]\n    LOOKUP_CHOICES.append(('666', '666'))", "fn_id": 0, "class_fn": false, "repo": "ericziethen/django-filter-property", "file": "tests/django_property_filter/filters/test_property_typed_multiple_choice_filter.py", "last_update_at": "2021-11-28T12:02:59+00:00", "original_content": "@pytest.fixture\ndef fixture_property_typed_multiple_choice_filter():\n    TypedMultipleChoiceFilterModel.objects.create(id=-1, text='1')\n    TypedMultipleChoiceFilterModel.objects.create(id=0, text='One')\n    TypedMultipleChoiceFilterModel.objects.create(id=1, text='2')\n    TypedMultipleChoiceFilterModel.objects.create(id=2, text='2')\n    TypedMultipleChoiceFilterModel.objects.create(id=3, text='Not a Number')\n    TypedMultipleChoiceFilterModel.objects.create(id=4, text='3')\n    TypedMultipleChoiceFilterModel.objects.create(id=5, text='3.3')\n    TypedMultipleChoiceFilterModel.objects.create(id=6)\n    global LOOKUP_CHOICES\n    LOOKUP_CHOICES = [(c.text, c.text) for c in TypedMultipleChoiceFilterModel.objects.order_by('id')]\n    LOOKUP_CHOICES.append(('666', '666'))", "refactored": true, "pred": {"ppl": 3.1968772411346436, "ppl_lower": 3.3818182945251465, "ppl/lowercase_ppl": -1.048391228688194, "ppl/zlib": 0.004352713375268888, "Min_5.0% Prob": 10.302900950113932, "Min_10.0% Prob": 8.26202760140101, "Min_20.0% Prob": 5.462730432550113, "Min_30.0% Prob": 3.8309798778759108, "Min_40.0% Prob": 2.910498274335017, "Min_50.0% Prob": 2.3329704121608907, "Min_60.0% Prob": 1.944866784670416}}
{"hexsha": "ea4d849d98e292e6186433a00e5238e2edf41505", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_render(using_temp_config, disabling_caching):\n    scene = SquareToCircle()\n    renderer = scene.renderer\n    renderer.update_frame = Mock(wraps=renderer.update_frame)\n    renderer.add_frame = Mock(wraps=renderer.add_frame)\n    scene.render()\n    assert renderer.add_frame.call_count == config['frame_rate']\n    assert renderer.update_frame.call_count == config['frame_rate']\n    assert_file_exists(config['output_file'])", "fn_id": 0, "class_fn": false, "repo": "fargetan/manim", "file": "tests/test_scene_rendering/test_cairo_renderer.py", "last_update_at": "2021-07-17T04:09:59+00:00", "original_content": "def test_render(using_temp_config, disabling_caching):\n    scene = SquareToCircle()\n    renderer = scene.renderer\n    renderer.update_frame = Mock(wraps=renderer.update_frame)\n    renderer.add_frame = Mock(wraps=renderer.add_frame)\n    scene.render()\n    assert renderer.add_frame.call_count == config['frame_rate']\n    assert renderer.update_frame.call_count == config['frame_rate']\n    assert_file_exists(config['output_file'])", "refactored": true, "pred": {"ppl": 6.759555816650391, "ppl_lower": 7.593356132507324, "ppl/lowercase_ppl": -1.0608681831129951, "ppl/zlib": 0.008806254287096488, "Min_5.0% Prob": 11.070689042409262, "Min_10.0% Prob": 9.587992350260416, "Min_20.0% Prob": 7.364253444671631, "Min_30.0% Prob": 5.860075931800039, "Min_40.0% Prob": 4.652629285466437, "Min_50.0% Prob": 3.7908682958222926, "Min_60.0% Prob": 3.2146980705131827}}
{"hexsha": "5479d7749b7f0f4c38907d23268688673d78cc7e", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef save():\n    website = website_e.get()\n    email = email_e.get()\n    password = password_e.get()\n    new_data = {website.upper(): {'email': email, 'password': password}}\n    if len(website) == 0 or len(password) == 0:\n        messagebox.showinfo(title='Oops!', message=\"Please don't leave any fields empty!!\")\n    else:\n        out = messagebox.askokcancel(title=website, message=f'These are the details entered: \\nEmail: {email}\\nPassword: {password} \\nDo you wish to proceed with these details?\\n\\n Note: If you want to update a new password for an existing website,also click OK\\n Else cancel the operation and search for the existing password!')\n        if out:\n            try:\n                with open('data.json', 'r') as data_file:\n                    data = json.load(data_file)\n            except FileNotFoundError:\n                with open('data.json', 'w') as data_file:\n                    json.dump(new_data, data_file, indent=4)\n            else:\n                data.update(new_data)\n                with open('data.json', 'w') as data_file:\n                    json.dump(data, data_file, indent=4)\n            finally:\n                messagebox.showinfo(title=website, message='Password saved successfully!')\n                website_e.delete(0, END)\n                password_e.delete(0, END)", "fn_id": 2, "class_fn": false, "repo": "SrihariMurali01/Password-manager", "file": "main.py", "last_update_at": "2021-12-21T18:17:28+00:00", "original_content": "def save():\n    website = website_e.get()\n    email = email_e.get()\n    password = password_e.get()\n    new_data = {website.upper(): {'email': email, 'password': password}}\n    if len(website) == 0 or len(password) == 0:\n        messagebox.showinfo(title='Oops!', message=\"Please don't leave any fields empty!!\")\n    else:\n        out = messagebox.askokcancel(title=website, message=f'These are the details entered: \\nEmail: {email}\\nPassword: {password} \\nDo you wish to proceed with these details?\\n\\n Note: If you want to update a new password for an existing website,also click OK\\n Else cancel the operation and search for the existing password!')\n        if out:\n            try:\n                with open('data.json', 'r') as data_file:\n                    data = json.load(data_file)\n            except FileNotFoundError:\n                with open('data.json', 'w') as data_file:\n                    json.dump(new_data, data_file, indent=4)\n            else:\n                data.update(new_data)\n                with open('data.json', 'w') as data_file:\n                    json.dump(data, data_file, indent=4)\n            finally:\n                messagebox.showinfo(title=website, message='Password saved successfully!')\n                website_e.delete(0, END)\n                password_e.delete(0, END)", "refactored": true, "pred": {"ppl": 3.3255069255828857, "ppl_lower": 3.604468584060669, "ppl/lowercase_ppl": -1.067036239040411, "ppl/zlib": 0.0021689929987528393, "Min_5.0% Prob": 9.698101127848906, "Min_10.0% Prob": 7.725052987827974, "Min_20.0% Prob": 5.405952569316415, "Min_30.0% Prob": 3.943340191069771, "Min_40.0% Prob": 2.9925035158865643, "Min_50.0% Prob": 2.4078217961700163, "Min_60.0% Prob": 2.009928290561841}}
{"hexsha": "293907725bd25c3293fb9246b3cf2af13bae51d7", "ext": "py", "lang": "Python", "content": "@app.route('/getNotifications', methods=['GET'])\n@timeing\n@measure_memory_usage\ndef getNotifications():\n    page = request.args.get('page', default=1, type=int)\n    limit = request.args.get('limit', default=25, type=int)\n    data = rdb.getNotifications(page, limit)\n    return jsonify(data)", "fn_id": 10, "class_fn": false, "repo": "naztronaut/easyUptime", "file": "upService.py", "last_update_at": "2021-05-05T06:29:10+00:00", "original_content": "@app.route('/getNotifications', methods=['GET'])\ndef getNotifications():\n    page = request.args.get('page', default=1, type=int)\n    limit = request.args.get('limit', default=25, type=int)\n    data = rdb.getNotifications(page, limit)\n    return jsonify(data)", "refactored": true, "pred": {"ppl": 3.894371509552002, "ppl_lower": 4.48435640335083, "ppl/lowercase_ppl": -1.1037582384272524, "ppl/zlib": 0.007511228221196524, "Min_5.0% Prob": 13.28609299659729, "Min_10.0% Prob": 9.7838334441185, "Min_20.0% Prob": 6.43050891160965, "Min_30.0% Prob": 4.471818130016327, "Min_40.0% Prob": 3.433732063707077, "Min_50.0% Prob": 2.7119014224126223, "Min_60.0% Prob": 2.281664397269487}}
{"hexsha": "6d928cb5cd52bb854817785ae392fcb7dd1a50d7", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_eds_plugins_built_property():\n    e = Event(False, True, 'url', 'project_name', 'project_version')\n    assert e.eds_plugins_built\n    e = Event(True, False, 'url', 'project_name', 'project_version')\n    assert not e.eds_plugins_built", "fn_id": 1, "class_fn": false, "repo": "jleopold28/eds", "file": "tests/test_event.py", "last_update_at": "2021-12-06T13:21:15+00:00", "original_content": "def test_eds_plugins_built_property():\n    e = Event(False, True, 'url', 'project_name', 'project_version')\n    assert e.eds_plugins_built\n    e = Event(True, False, 'url', 'project_name', 'project_version')\n    assert not e.eds_plugins_built", "refactored": true, "pred": {"ppl": 8.146272659301758, "ppl_lower": 10.30981159210205, "ppl/lowercase_ppl": -1.1122902274203297, "ppl/zlib": 0.014465934346364612, "Min_5.0% Prob": 12.66941523551941, "Min_10.0% Prob": 11.30601716041565, "Min_20.0% Prob": 8.739101529121399, "Min_30.0% Prob": 6.595862021446228, "Min_40.0% Prob": 5.2198954820632935, "Min_50.0% Prob": 4.173937459077154, "Min_60.0% Prob": 3.521012611836195}}
{"hexsha": "2aa04fe50cb3ff88be7ef0d11bebb4e8eabb4c12", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _h(y):\n\n    def foo(x):\n        \"\"\"funcdoc\"\"\"\n        return [x + z for z in y]\n    return foo", "fn_id": 3, "class_fn": false, "repo": "QZLin/nogil", "file": "Lib/test/test_dis.py", "last_update_at": "2021-11-19T02:20:24+00:00", "original_content": "def _h(y):\n\n    def foo(x):\n        \"\"\"funcdoc\"\"\"\n        return [x + z for z in y]\n    return foo", "refactored": true, "pred": {"ppl": 21.749032974243164, "ppl_lower": 21.749032974243164, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.02961124322684556, "Min_5.0% Prob": 10.41826581954956, "Min_10.0% Prob": 9.621241807937622, "Min_20.0% Prob": 8.43108731508255, "Min_30.0% Prob": 7.50410525004069, "Min_40.0% Prob": 6.753916531801224, "Min_50.0% Prob": 5.981750875711441, "Min_60.0% Prob": 5.176918404797713}}
{"hexsha": "be2ff543f0b705a68df4d91c081b06d11c31d754", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef LogSquad_Damage(data_str, server_id):\n    matchObj = re.search('[([0-9.:-]+]\\\\[([ 0-9]*)]LogSquad: Player:(.*) ActualDamage=(\\\\d+(\\\\.\\\\d+)?) from (.*) caused by (.*).*', data_str, re.M | re.I)\n    date_time = datetime.datetime.strptime(matchObj.group(0).split('[')[1].split(']')[0], '%Y.%m.%d-%H.%M.%S:%f')\n    date_time_n = (date_time + datetime.timedelta(hours=8)).strftime('%Y.%m.%d-%H.%M.%S:%f')\n    date = date_time_n.split('-')[0]\n    time = date_time_n.split('-')[1]\n    Player_take_damage = matchObj.group(2)\n    Damage = matchObj.group(3)\n    Player_make_damage = matchObj.group(5)\n    Weapon = matchObj.group(6).split('_C_')[0]\n    return (date, time, str(Player_take_damage), str(Damage), str(Player_make_damage), str(Weapon), server_id)\n    pass", "fn_id": 0, "class_fn": false, "repo": "ChenjianS47/SquadPy", "file": "Core/Log_Plugins/LogSquad_Damage.py", "last_update_at": "2021-09-27T07:32:32+00:00", "original_content": "def LogSquad_Damage(data_str, server_id):\n    matchObj = re.search('[([0-9.:-]+]\\\\[([ 0-9]*)]LogSquad: Player:(.*) ActualDamage=(\\\\d+(\\\\.\\\\d+)?) from (.*) caused by (.*).*', data_str, re.M | re.I)\n    date_time = datetime.datetime.strptime(matchObj.group(0).split('[')[1].split(']')[0], '%Y.%m.%d-%H.%M.%S:%f')\n    date_time_n = (date_time + datetime.timedelta(hours=8)).strftime('%Y.%m.%d-%H.%M.%S:%f')\n    date = date_time_n.split('-')[0]\n    time = date_time_n.split('-')[1]\n    Player_take_damage = matchObj.group(2)\n    Damage = matchObj.group(3)\n    Player_make_damage = matchObj.group(5)\n    Weapon = matchObj.group(6).split('_C_')[0]\n    return (date, time, str(Player_take_damage), str(Damage), str(Player_make_damage), str(Weapon), server_id)\n    pass", "refactored": true, "pred": {"ppl": 5.416661739349365, "ppl_lower": 6.571500301361084, "ppl/lowercase_ppl": -1.11439169772495, "ppl/zlib": 0.004411174178717087, "Min_5.0% Prob": 12.05794382095337, "Min_10.0% Prob": 9.813682506824362, "Min_20.0% Prob": 7.166753641490279, "Min_30.0% Prob": 5.401694649937509, "Min_40.0% Prob": 4.188921846064, "Min_50.0% Prob": 3.378936301531463, "Min_60.0% Prob": 2.8235188043708432}}
{"hexsha": "fea44362f63483b4569ec189eda480b3e3b01e26", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef QA_save_tdx_to_mongo(file_dir, client=DATABASE):\n    reader = TdxMinBarReader()\n    __coll = client.stock_min_five\n    for a, v, files in os.walk(file_dir):\n        for file in files:\n            if str(file)[0:2] == 'sh' and int(str(file)[2]) == 6 or (str(file)[0:2] == 'sz' and int(str(file)[2]) == 0) or (str(file)[0:2] == 'sz' and int(str(file)[2]) == 3):\n                QA_util_log_info('Now_saving ' + str(file)[2:8] + \"'s 5 min tick\")\n                fname = file_dir + os.sep + file\n                df = reader.get_df(fname)\n                df['code'] = str(file)[2:8]\n                df['market'] = str(file)[0:2]\n                df['datetime'] = [str(x) for x in list(df.index)]\n                df['date'] = [str(x)[0:10] for x in list(df.index)]\n                df['time_stamp'] = df['datetime'].apply(lambda x: QA_util_time_stamp(x))\n                df['date_stamp'] = df['date'].apply(lambda x: QA_util_date_stamp(x))\n                data_json = json.loads(df.to_json(orient='records'))\n                __coll.insert_many(data_json)", "fn_id": 0, "class_fn": false, "repo": "liujiannong/QUANTAXIS", "file": "QUANTAXIS/QASU/save_tdx_file.py", "last_update_at": "2021-04-01T08:59:46+00:00", "original_content": "def QA_save_tdx_to_mongo(file_dir, client=DATABASE):\n    reader = TdxMinBarReader()\n    __coll = client.stock_min_five\n    for a, v, files in os.walk(file_dir):\n        for file in files:\n            if str(file)[0:2] == 'sh' and int(str(file)[2]) == 6 or (str(file)[0:2] == 'sz' and int(str(file)[2]) == 0) or (str(file)[0:2] == 'sz' and int(str(file)[2]) == 3):\n                QA_util_log_info('Now_saving ' + str(file)[2:8] + \"'s 5 min tick\")\n                fname = file_dir + os.sep + file\n                df = reader.get_df(fname)\n                df['code'] = str(file)[2:8]\n                df['market'] = str(file)[0:2]\n                df['datetime'] = [str(x) for x in list(df.index)]\n                df['date'] = [str(x)[0:10] for x in list(df.index)]\n                df['time_stamp'] = df['datetime'].apply(lambda x: QA_util_time_stamp(x))\n                df['date_stamp'] = df['date'].apply(lambda x: QA_util_date_stamp(x))\n                data_json = json.loads(df.to_json(orient='records'))\n                __coll.insert_many(data_json)", "refactored": true, "pred": {"ppl": 3.8350744247436523, "ppl_lower": 4.162727355957031, "ppl/lowercase_ppl": -1.0609896690103429, "ppl/zlib": 0.003007133873252189, "Min_5.0% Prob": 10.066916147867838, "Min_10.0% Prob": 8.40552110142178, "Min_20.0% Prob": 5.891350951459673, "Min_30.0% Prob": 4.353538267590381, "Min_40.0% Prob": 3.3338485964413347, "Min_50.0% Prob": 2.6891108572441897, "Min_60.0% Prob": 2.2473089675878257}}
{"hexsha": "9d3c3cf62b10c24e35584ee9f62074dd84ba8c6b", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef preprocess_prices(df_prices: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Preprocessing of price dataframe. Get latest available price.\n    :param df_prices: Needed columns: ISIN, Price, Datum, Currency\n    :return: dataframe containing prices of stocks defined by ISIN on latest available date\n    \"\"\"\n    dfp = df_prices.copy()\n    assert dfp['Currency'].drop_duplicates().count() == 1, 'Multiple currencies used for price data!'\n    assert dfp['Currency'].iloc[0] == 'EUR', 'Currency is not Euro!'\n    dfp['Date'] = pd.to_datetime(dfp['Date'], format='%d.%m.%Y')\n    latest_date = dfp['Date'].max()\n    df_current_prices = dfp[dfp['Date'] == latest_date].reset_index(drop=True)\n    return df_current_prices", "fn_id": 5, "class_fn": false, "repo": "christophpernul/personal-finance-dashboard", "file": "lib_data_operations.py", "last_update_at": "2021-02-19T15:27:06+00:00", "original_content": "def preprocess_prices(df_prices: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Preprocessing of price dataframe. Get latest available price.\n    :param df_prices: Needed columns: ISIN, Price, Datum, Currency\n    :return: dataframe containing prices of stocks defined by ISIN on latest available date\n    \"\"\"\n    dfp = df_prices.copy()\n    assert dfp['Currency'].drop_duplicates().count() == 1, 'Multiple currencies used for price data!'\n    assert dfp['Currency'].iloc[0] == 'EUR', 'Currency is not Euro!'\n    dfp['Date'] = pd.to_datetime(dfp['Date'], format='%d.%m.%Y')\n    latest_date = dfp['Date'].max()\n    df_current_prices = dfp[dfp['Date'] == latest_date].reset_index(drop=True)\n    return df_current_prices", "refactored": true, "pred": {"ppl": 4.877458572387695, "ppl_lower": 5.672072887420654, "ppl/lowercase_ppl": -1.095246765866721, "ppl/zlib": 0.004052747569982107, "Min_5.0% Prob": 10.171592807769775, "Min_10.0% Prob": 8.474167188008627, "Min_20.0% Prob": 6.393339003835406, "Min_30.0% Prob": 4.962870881670997, "Min_40.0% Prob": 3.91255453654698, "Min_50.0% Prob": 3.154625909438111, "Min_60.0% Prob": 2.6427439069302063}}
{"hexsha": "178e3d96179baff608652d760ff54f1c5c188588", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_phonopy_options(postprocess_parameters):\n    \"\"\"Return phonopy command option strings.\"\"\"\n    mesh_opts = []\n    if 'mesh' in postprocess_parameters:\n        mesh = postprocess_parameters['mesh']\n        try:\n            length = float(mesh)\n            mesh_opts.append('--mesh=%f' % length)\n        except TypeError:\n            mesh_opts.append('--mesh=\"%d %d %d\"' % tuple(mesh))\n        mesh_opts.append('--nowritemesh')\n    fc_opts = []\n    if 'fc_calculator' in postprocess_parameters:\n        if postprocess_parameters['fc_calculator'].lower().strip() == 'alm':\n            fc_opts.append('--alm')\n    return (mesh_opts, fc_opts)", "fn_id": 4, "class_fn": false, "repo": "atztogo/aiida-phonopy", "file": "aiida_phonopy/common/file_generators.py", "last_update_at": "2021-12-18T03:05:40+00:00", "original_content": "def get_phonopy_options(postprocess_parameters):\n    \"\"\"Return phonopy command option strings.\"\"\"\n    mesh_opts = []\n    if 'mesh' in postprocess_parameters:\n        mesh = postprocess_parameters['mesh']\n        try:\n            length = float(mesh)\n            mesh_opts.append('--mesh=%f' % length)\n        except TypeError:\n            mesh_opts.append('--mesh=\"%d %d %d\"' % tuple(mesh))\n        mesh_opts.append('--nowritemesh')\n    fc_opts = []\n    if 'fc_calculator' in postprocess_parameters:\n        if postprocess_parameters['fc_calculator'].lower().strip() == 'alm':\n            fc_opts.append('--alm')\n    return (mesh_opts, fc_opts)", "refactored": true, "pred": {"ppl": 4.7808003425598145, "ppl_lower": 5.393410682678223, "ppl/lowercase_ppl": -1.0770608349191877, "ppl/zlib": 0.005339958935732235, "Min_5.0% Prob": 11.718265891075134, "Min_10.0% Prob": 9.442055281470804, "Min_20.0% Prob": 6.770403473717826, "Min_30.0% Prob": 5.004068610803136, "Min_40.0% Prob": 3.9123276042086736, "Min_50.0% Prob": 3.139149395748973, "Min_60.0% Prob": 2.6110345656227953}}
{"hexsha": "b087ca6dbcea8e5a3be1946bdd709bb2ed25f366", "ext": "py", "lang": "Python", "content": "@pytest.mark.parametrize('factory', [lambda x: list(x), lambda x: MultiDict(x), lambda x: MultiDictProxy(MultiDict(x))])\n@timeing\n@measure_memory_usage\ndef test_multi_dict_multidict_less_keys_substitution(factory: Callable):\n    with given:\n        sch = schema_multi_dict([('id', schema.int), ('name', schema.str)])\n    with when:\n        res = substitute(sch, factory([('id', 1)]))\n    with then:\n        assert res == schema_multi_dict([('id', schema.int(1)), ('name', schema.str)])\n        assert res != sch", "fn_id": 5, "class_fn": false, "repo": "nikitanovosibirsk/district42-exp-types", "file": "tests/multi_dict/substitutor/test_multi_dict_multidict_substitutor.py", "last_update_at": "2021-08-01T10:06:28+00:00", "original_content": "@pytest.mark.parametrize('factory', [lambda x: list(x), lambda x: MultiDict(x), lambda x: MultiDictProxy(MultiDict(x))])\ndef test_multi_dict_multidict_less_keys_substitution(factory: Callable):\n    with given:\n        sch = schema_multi_dict([('id', schema.int), ('name', schema.str)])\n    with when:\n        res = substitute(sch, factory([('id', 1)]))\n    with then:\n        assert res == schema_multi_dict([('id', schema.int(1)), ('name', schema.str)])\n        assert res != sch", "refactored": true, "pred": {"ppl": 6.237063407897949, "ppl_lower": 7.08108377456665, "ppl/lowercase_ppl": -1.0693345268600085, "ppl/zlib": 0.007235215272340364, "Min_5.0% Prob": 12.710382325308663, "Min_10.0% Prob": 10.24049441019694, "Min_20.0% Prob": 7.188757769266764, "Min_30.0% Prob": 5.549757120344374, "Min_40.0% Prob": 4.450537576278051, "Min_50.0% Prob": 3.609949130760996, "Min_60.0% Prob": 3.045946446994504}}
{"hexsha": "f04872eed07d871c244788341ce7469a3e8c2ec3", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef get_random_vals(mean_vel):\n    init_v = 20 + np.random.choice(range(-5, 5))\n    action_magnitute = np.random.uniform(-3, 3)\n    action_freq = np.random.uniform(0.02, 0.06)\n    return (init_v, action_magnitute, action_freq)", "fn_id": 3, "class_fn": false, "repo": "saArbabi/sim", "file": "src/exploratory_experiments/_data_generator.py", "last_update_at": "2021-03-26T15:28:31+00:00", "original_content": "def get_random_vals(mean_vel):\n    init_v = 20 + np.random.choice(range(-5, 5))\n    action_magnitute = np.random.uniform(-3, 3)\n    action_freq = np.random.uniform(0.02, 0.06)\n    return (init_v, action_magnitute, action_freq)", "refactored": true, "pred": {"ppl": 6.618076801300049, "ppl_lower": 6.618076801300049, "ppl/lowercase_ppl": -1.0, "ppl/zlib": 0.011593894566521106, "Min_5.0% Prob": 10.973995447158813, "Min_10.0% Prob": 9.700664414299858, "Min_20.0% Prob": 7.265066259785702, "Min_30.0% Prob": 5.67748290916969, "Min_40.0% Prob": 4.550602026474782, "Min_50.0% Prob": 3.734181446080305, "Min_60.0% Prob": 3.1803089059118568}}
{"hexsha": "539c6e5edfed9d87bb33e2513733ff5fcbd5e916", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef _make_dagster_event(event_type: DagsterEventType, pipeline_name: str, step_key: str):\n    event_specific_data = None\n    if event_type == DagsterEventType.STEP_SUCCESS:\n        event_specific_data = StepSuccessData(duration_ms=1.0)\n    elif event_type == DagsterEventType.STEP_FAILURE:\n        event_specific_data = StepFailureData(error=None, user_failure_data=None)\n    return DagsterEvent(event_type.value, pipeline_name, step_key=step_key, event_specific_data=event_specific_data)", "fn_id": 1, "class_fn": false, "repo": "denimalpaca/OpenLineage", "file": "integration/dagster/tests/conftest.py", "last_update_at": "2021-12-03T17:00:00+00:00", "original_content": "def _make_dagster_event(event_type: DagsterEventType, pipeline_name: str, step_key: str):\n    event_specific_data = None\n    if event_type == DagsterEventType.STEP_SUCCESS:\n        event_specific_data = StepSuccessData(duration_ms=1.0)\n    elif event_type == DagsterEventType.STEP_FAILURE:\n        event_specific_data = StepFailureData(error=None, user_failure_data=None)\n    return DagsterEvent(event_type.value, pipeline_name, step_key=step_key, event_specific_data=event_specific_data)", "refactored": true, "pred": {"ppl": 2.8950998783111572, "ppl_lower": 4.203860759735107, "ppl/lowercase_ppl": -1.3508719096976285, "ppl/zlib": 0.004286369398024315, "Min_5.0% Prob": 8.981796877724785, "Min_10.0% Prob": 7.043641885121663, "Min_20.0% Prob": 4.892878174781799, "Min_30.0% Prob": 3.498098329977786, "Min_40.0% Prob": 2.6819611776980663, "Min_50.0% Prob": 2.1378582902252674, "Min_60.0% Prob": 1.775228046121056}}
{"hexsha": "8e050af63a97f4a7f86ee722501d806903d052be", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef test_clip_lines(two_line_gdf, single_rectangle_gdf):\n    \"\"\"Test what happens when you give the clip_extent a line GDF.\"\"\"\n    clip_line = cl.clip_shp(two_line_gdf, single_rectangle_gdf)\n    assert len(clip_line.geometry) == 2", "fn_id": 19, "class_fn": false, "repo": "jlpalomino/earthpy", "file": "earthpy/tests/test_clip.py", "last_update_at": "2021-01-02T02:32:41+00:00", "original_content": "def test_clip_lines(two_line_gdf, single_rectangle_gdf):\n    \"\"\"Test what happens when you give the clip_extent a line GDF.\"\"\"\n    clip_line = cl.clip_shp(two_line_gdf, single_rectangle_gdf)\n    assert len(clip_line.geometry) == 2", "refactored": true, "pred": {"ppl": 11.49245548248291, "ppl_lower": 11.913387298583984, "ppl/lowercase_ppl": -1.0147324043501564, "ppl/zlib": 0.014447874408733907, "Min_5.0% Prob": 11.457731008529663, "Min_10.0% Prob": 10.683212280273438, "Min_20.0% Prob": 8.856066339156207, "Min_30.0% Prob": 7.195332860946655, "Min_40.0% Prob": 5.831553574870615, "Min_50.0% Prob": 4.822097040886103, "Min_60.0% Prob": 4.10184391281184}}
{"hexsha": "cab40a1d1240c5bed8edb2cf223d0f234868dfd0", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef daily_mean(data):\n    \"\"\"Calculate the daily mean of a 2D inflammation data array.\n\n    :param data: A 2D data array with inflammation data (each row contains measurements for a single patient across all days).\n    :returns: An array of mean values of measurements for each day.\n    \"\"\"\n    return np.mean(data, axis=0)", "fn_id": 1, "class_fn": false, "repo": "raphaelshirley/python-intermediate-inflammation-1", "file": "inflammation/models.py", "last_update_at": "2021-12-10T12:28:49+00:00", "original_content": "def daily_mean(data):\n    \"\"\"Calculate the daily mean of a 2D inflammation data array.\n\n    :param data: A 2D data array with inflammation data (each row contains measurements for a single patient across all days).\n    :returns: An array of mean values of measurements for each day.\n    \"\"\"\n    return np.mean(data, axis=0)", "refactored": true, "pred": {"ppl": 8.271632194519043, "ppl_lower": 7.847991943359375, "ppl/lowercase_ppl": -0.9751167339148273, "ppl/zlib": 0.00987304604142718, "Min_5.0% Prob": 11.869007349014282, "Min_10.0% Prob": 9.587539937761095, "Min_20.0% Prob": 7.461455451117621, "Min_30.0% Prob": 6.091676182217068, "Min_40.0% Prob": 5.028381854295731, "Min_50.0% Prob": 4.117794213087662, "Min_60.0% Prob": 3.5130903853611515}}
{"hexsha": "bc9cb89c9de7c8845a3f7858c2bf081cca88afc4", "ext": "py", "lang": "Python", "content": "@timeing\n@measure_memory_usage\ndef main():\n    parser = argparse.ArgumentParser()\n    default_url = 'https://search.api.hubmapconsortium.org/portal/search'\n    parser.add_argument('--url', default=default_url, help=f'ES endpoint. Default: {default_url}')\n    default_size = 20\n    parser.add_argument('--size', type=int, default=default_size, help=f'Number of records to pull. Default: {default_size}')\n    default_type = 'Dataset'\n    parser.add_argument('--type', default=default_type, help=f'Entity type to query. Default: {default_type}')\n    args = parser.parse_args()\n    query = {'post_filter': {'term': {'entity_type.keyword': args.type}}, 'size': args.size, '_source': ['metadata.metadata' if args.type == 'Dataset' else 'metadata']}\n    response = requests.post(args.url, json=query)\n    hits = response.json()['hits']['hits']\n    writer = DictWriter(sys.stdout, fieldnames=['uuid', 'assay_type', 'field', 'value'], extrasaction='ignore')\n    writer.writeheader()\n    for hit in hits:\n        uuid = hit['_id']\n        if 'metadata' not in hit['_source']:\n            continue\n        meta = hit['_source']['metadata']\n        if 'metadata' in meta:\n            meta = meta['metadata']\n        for field, value in meta.items():\n            if not re.search('[A-Za-z]', value):\n                continue\n            writer.writerow({'uuid': uuid, 'assay_type': meta['assay_type'] if 'assay_type' in meta else 'Sample', 'field': field, 'value': value})\n    assert len(hits) < args.size, f'Result truncated at {args.size}'\n    return 0", "fn_id": 0, "class_fn": false, "repo": "lukasz-migas/ingest-validation-tools", "file": "src/generate_field_values_csv.py", "last_update_at": "2021-03-17T20:48:41+00:00", "original_content": "def main():\n    parser = argparse.ArgumentParser()\n    default_url = 'https://search.api.hubmapconsortium.org/portal/search'\n    parser.add_argument('--url', default=default_url, help=f'ES endpoint. Default: {default_url}')\n    default_size = 20\n    parser.add_argument('--size', type=int, default=default_size, help=f'Number of records to pull. Default: {default_size}')\n    default_type = 'Dataset'\n    parser.add_argument('--type', default=default_type, help=f'Entity type to query. Default: {default_type}')\n    args = parser.parse_args()\n    query = {'post_filter': {'term': {'entity_type.keyword': args.type}}, 'size': args.size, '_source': ['metadata.metadata' if args.type == 'Dataset' else 'metadata']}\n    response = requests.post(args.url, json=query)\n    hits = response.json()['hits']['hits']\n    writer = DictWriter(sys.stdout, fieldnames=['uuid', 'assay_type', 'field', 'value'], extrasaction='ignore')\n    writer.writeheader()\n    for hit in hits:\n        uuid = hit['_id']\n        if 'metadata' not in hit['_source']:\n            continue\n        meta = hit['_source']['metadata']\n        if 'metadata' in meta:\n            meta = meta['metadata']\n        for field, value in meta.items():\n            if not re.search('[A-Za-z]', value):\n                continue\n            writer.writerow({'uuid': uuid, 'assay_type': meta['assay_type'] if 'assay_type' in meta else 'Sample', 'field': field, 'value': value})\n    assert len(hits) < args.size, f'Result truncated at {args.size}'\n    return 0", "refactored": true, "pred": {"ppl": 2.8283581733703613, "ppl_lower": 3.0964877605438232, "ppl/lowercase_ppl": -1.087113985210235, "ppl/zlib": 0.0015587652062219782, "Min_5.0% Prob": 9.445385092780704, "Min_10.0% Prob": 7.4928742590404696, "Min_20.0% Prob": 4.918379786435295, "Min_30.0% Prob": 3.4450092674709683, "Min_40.0% Prob": 2.600056158729336, "Min_50.0% Prob": 2.0784414017052684, "Min_60.0% Prob": 1.7367428708588704}}
